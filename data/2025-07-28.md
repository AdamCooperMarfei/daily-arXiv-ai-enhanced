<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 15]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Perpetua: Multi-Hypothesis Persistence Modeling for Semi-Static Environments](https://arxiv.org/abs/2507.18808)
*Miguel Saavedra-Ruiz,Samer B. Nashed,Charlie Gauthier,Liam Paull*

Main category: cs.RO

TL;DR: Perpetua是一种用于建模半静态特征动态的方法，能够结合先验知识、跟踪多假设并预测未来特征状态。


<details>
  <summary>Details</summary>
Motivation: 复杂动态环境中，机器人系统需要处理环境变化，现有算法无法有效预测动态特征的未来状态。

Method: 通过链式混合“持久性”和“涌现性”滤波器，在贝叶斯框架中建模特征消失或重现的概率。

Result: 实验表明，Perpetua在模拟和真实数据中比类似方法更准确，且具有在线适应性和鲁棒性。

Conclusion: Perpetua是一种高效、可扩展且通用的方法，适用于动态环境中的特征状态估计。

Abstract: Many robotic systems require extended deployments in complex, dynamic
environments. In such deployments, parts of the environment may change between
subsequent robot observations. Most robotic mapping or environment modeling
algorithms are incapable of representing dynamic features in a way that enables
predicting their future state. Instead, they opt to filter certain state
observations, either by removing them or some form of weighted averaging. This
paper introduces Perpetua, a method for modeling the dynamics of semi-static
features. Perpetua is able to: incorporate prior knowledge about the dynamics
of the feature if it exists, track multiple hypotheses, and adapt over time to
enable predicting of future feature states. Specifically, we chain together
mixtures of "persistence" and "emergence" filters to model the probability that
features will disappear or reappear in a formal Bayesian framework. The
approach is an efficient, scalable, general, and robust method for estimating
the states of features in an environment, both in the present as well as at
arbitrary future times. Through experiments on simulated and real-world data,
we find that Perpetua yields better accuracy than similar approaches while also
being online adaptable and robust to missing observations.

</details>


### [2] [Probabilistic Collision Risk Estimation through Gauss-Legendre Cubature and Non-Homogeneous Poisson Processes](https://arxiv.org/abs/2507.18819)
*Trent Weiss,Madhur Behl*

Main category: cs.RO

TL;DR: 论文提出了一种名为Gauss-Legendre Rectangle (GLR)的算法，用于高速自动驾驶赛车中的碰撞风险实时估计，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在高速自动驾驶赛车中，精确的实时碰撞风险估计至关重要，尤其是在车轮对车轮的极小安全距离场景中。现有方法要么过于简化，要么过于保守。

Method: GLR算法结合Gauss-Legendre积分和非齐次泊松过程，分两阶段估计碰撞风险，同时考虑车辆几何形状和轨迹不确定性。

Result: 在446个超车场景的高保真F1赛车模拟中，GLR平均误差减少77%，优于其他五种先进方法，且运行频率达1000Hz。

Conclusion: GLR算法不仅适用于自动驾驶赛车，还可推广到更广泛的运动规划场景。

Abstract: Overtaking in high-speed autonomous racing demands precise, real-time
estimation of collision risk; particularly in wheel-to-wheel scenarios where
safety margins are minimal. Existing methods for collision risk estimation
either rely on simplified geometric approximations, like bounding circles, or
perform Monte Carlo sampling which leads to overly conservative motion planning
behavior at racing speeds. We introduce the Gauss-Legendre Rectangle (GLR)
algorithm, a principled two-stage integration method that estimates collision
risk by combining Gauss-Legendre with a non-homogeneous Poisson process over
time. GLR produces accurate risk estimates that account for vehicle geometry
and trajectory uncertainty. In experiments across 446 overtaking scenarios in a
high-fidelity Formula One racing simulation, GLR outperforms five
state-of-the-art baselines achieving an average error reduction of 77% and
surpassing the next-best method by 52%, all while running at 1000 Hz. The
framework is general and applicable to broader motion planning contexts beyond
autonomous racing.

</details>


### [3] [MetaMorph -- A Metamodelling Approach For Robot Morphology](https://arxiv.org/abs/2507.18820)
*Rachel Ringe,Robin Nolte,Nima Zargham,Robert Porzel,Rainer Malaka*

Main category: cs.RO

TL;DR: MetaMorph框架通过元建模方法分类机器人形态，弥补了现有分类方法的不足，支持跨类型机器人设计比较。


<details>
  <summary>Details</summary>
Motivation: 现有机器人外观分类方法过于宽泛或仅关注拟人化特征，无法全面分类所有类型机器人，限制了设计与交互效果的研究。

Method: 从IEEE Robots Guide中提取222个机器人数据，采用元建模方法构建MetaMorph框架。

Result: MetaMorph提供了结构化方法，支持机器人视觉特征比较和设计优化。

Conclusion: MetaMorph为机器人设计提供了更全面的分类工具，有助于探索设计与交互的关系。

Abstract: Robot appearance crucially shapes Human-Robot Interaction (HRI) but is
typically described via broad categories like anthropomorphic, zoomorphic, or
technical. More precise approaches focus almost exclusively on anthropomorphic
features, which fail to classify robots across all types, limiting the ability
to draw meaningful connections between robot design and its effect on
interaction. In response, we present MetaMorph, a comprehensive framework for
classifying robot morphology. Using a metamodeling approach, MetaMorph was
synthesized from 222 robots in the IEEE Robots Guide, offering a structured
method for comparing visual features. This model allows researchers to assess
the visual distances between robot models and explore optimal design traits
tailored to different tasks and contexts.

</details>


### [4] [Equivariant Volumetric Grasping](https://arxiv.org/abs/2507.18847)
*Pinhao Song,Yutong Hu,Pengteng Li,Renaud Detry*

Main category: cs.RO

TL;DR: 提出了一种新的体积抓取模型，具有垂直轴旋转等变性，显著提高了样本效率。采用三平面体积特征表示，结合可变形可操纵卷积，实现了局部几何适应性和等变性。


<details>
  <summary>Details</summary>
Motivation: 提高抓取模型的样本效率和计算效率，同时保持对旋转的等变性。

Method: 使用三平面特征表示，设计水平面特征对90度旋转等变，其他两平面特征不变。引入可变形可操纵卷积，结合局部几何适应性和等变性。

Result: 模型显著降低了计算和内存成本，并在模拟和实际实验中表现优于非等变模型。

Conclusion: 提出的三平面特征设计和等变模型在抓取任务中表现出高效性和优越性。

Abstract: We propose a new volumetric grasp model that is equivariant to rotations
around the vertical axis, leading to a significant improvement in sample
efficiency. Our model employs a tri-plane volumetric feature representation --
i.e., the projection of 3D features onto three canonical planes. We introduce a
novel tri-plane feature design in which features on the horizontal plane are
equivariant to 90{\deg} rotations, while the sum of features from the other two
planes remains invariant to the same transformations. This design is enabled by
a new deformable steerable convolution, which combines the adaptability of
deformable convolutions with the rotational equivariance of steerable ones.
This allows the receptive field to adapt to local object geometry while
preserving equivariance properties. We further develop equivariant adaptations
of two state-of-the-art volumetric grasp planners, GIGA and IGD. Specifically,
we derive a new equivariant formulation of IGD's deformable attention mechanism
and propose an equivariant generative model of grasp orientations based on flow
matching. We provide a detailed analytical justification of the proposed
equivariance properties and validate our approach through extensive simulated
and real-world experiments. Our results demonstrate that the proposed
projection-based design significantly reduces both computational and memory
costs. Moreover, the equivariant grasp models built on top of our tri-plane
features consistently outperform their non-equivariant counterparts, achieving
higher performance with only a modest computational overhead. Video and code
can be viewed in: https://mousecpn.github.io/evg-page/

</details>


### [5] [A Fast and Light-weight Non-Iterative Visual Odometry with RGB-D Cameras](https://arxiv.org/abs/2507.18886)
*Zheng Yang,Kuan Xu,Shenghai Yuan,Lihua Xie*

Main category: cs.RO

TL;DR: 提出了一种高效估计6自由度机器人位姿的非迭代解耦方法，利用重叠平面元素，避免了传统RGB-D视觉里程计的计算负担。


<details>
  <summary>Details</summary>
Motivation: 传统RGB-D视觉里程计依赖迭代优化和特征提取，计算量大且耗时，需改进。

Method: 通过分离旋转和平移估计，利用平面特征计算旋转矩阵，再使用核互相关器确定平移，避免迭代优化和特征提取。

Result: 在低端i5 CPU上实现71Hz性能，在低纹理退化环境中优于现有方法。

Conclusion: 该方法显著提高了计算效率，适用于低纹理环境。

Abstract: In this paper, we introduce a novel approach for efficiently estimating the
6-Degree-of-Freedom (DoF) robot pose with a decoupled, non-iterative method
that capitalizes on overlapping planar elements. Conventional RGB-D visual
odometry(RGBD-VO) often relies on iterative optimization solvers to estimate
pose and involves a process of feature extraction and matching. This results in
significant computational burden and time delays. To address this, our
innovative method for RGBD-VO separates the estimation of rotation and
translation. Initially, we exploit the overlaid planar characteristics within
the scene to calculate the rotation matrix. Following this, we utilize a kernel
cross-correlator (KCC) to ascertain the translation. By sidestepping the
resource-intensive iterative optimization and feature extraction and alignment
procedures, our methodology offers improved computational efficacy, achieving a
performance of 71Hz on a lower-end i5 CPU. When the RGBD-VO does not rely on
feature points, our technique exhibits enhanced performance in low-texture
degenerative environments compared to state-of-the-art methods.

</details>


### [6] [GEAR: Gaze-Enabled Human-Robot Collaborative Assembly](https://arxiv.org/abs/2507.18947)
*Asad Ali Shahid,Angelo Moroncelli,Drazen Brscic,Takayuki Kanda,Loris Roveda*

Main category: cs.RO

TL;DR: GEAR是一种基于视线交互的机器人辅助系统，用于复杂装配任务，相比触摸屏界面，减少了用户的体力需求并提升了体验。


<details>
  <summary>Details</summary>
Motivation: 复杂装配任务因任务多变性和精确性需求而具有挑战性，需要探索机器人辅助的新方法。

Method: 提出GEAR系统，利用用户视线交互辅助装配任务，并与触摸屏界面进行对比实验。

Result: GEAR在复杂任务中显著减少体力需求，提升用户体验，同时保持高效性能。

Conclusion: 视线交互的GEAR系统在复杂装配任务中优于传统触摸屏界面，具有实际应用潜力。

Abstract: Recent progress in robot autonomy and safety has significantly improved
human-robot interactions, enabling robots to work alongside humans on various
tasks. However, complex assembly tasks still present significant challenges due
to inherent task variability and the need for precise operations. This work
explores deploying robots in an assistive role for such tasks, where the robot
assists by fetching parts while the skilled worker provides high-level guidance
and performs the assembly. We introduce GEAR, a gaze-enabled system designed to
enhance human-robot collaboration by allowing robots to respond to the user's
gaze. We evaluate GEAR against a touch-based interface where users interact
with the robot through a touchscreen. The experimental study involved 30
participants working on two distinct assembly scenarios of varying complexity.
Results demonstrated that GEAR enabled participants to accomplish the assembly
with reduced physical demand and effort compared to the touchscreen interface,
especially for complex tasks, maintaining great performance, and receiving
objects effectively. Participants also reported enhanced user experience while
performing assembly tasks. Project page: sites.google.com/view/gear-hri

</details>


### [7] [Frequency Response Data-Driven Disturbance Observer Design for Flexible Joint Robots](https://arxiv.org/abs/2507.18979)
*Deokjin Lee,Junho Song,Alireza Karimi,Sehoon Oh*

Main category: cs.RO

TL;DR: 提出了一种基于频率响应函数（FRF）的优化方法，用于提升柔性关节机器人（FJR）的扰动观测器（DOB）性能，增强系统鲁棒性和运动性能。


<details>
  <summary>Details</summary>
Motivation: 柔性关节机器人（FJR）的动态特性因关节弹性和系统参数变化而复杂化，传统扰动观测器（DOB）设计保守，性能受限。

Method: 采用频率响应函数（FRF）优化方法，最大化控制带宽并抑制振动，同时通过奈奎斯特稳定性准则验证闭环稳定性。

Result: 实验验证表明，该方法显著提升了系统鲁棒性和运动性能，即使在关节弹性和系统变化条件下。

Conclusion: FRF优化方法有效解决了柔性关节机器人中DOB性能受限的问题，为复杂动态系统控制提供了新思路。

Abstract: Motion control of flexible joint robots (FJR) is challenged by inherent
flexibility and configuration-dependent variations in system dynamics. While
disturbance observers (DOB) can enhance system robustness, their performance is
often limited by the elasticity of the joints and the variations in system
parameters, which leads to a conservative design of the DOB. This paper
presents a novel frequency response function (FRF)-based optimization method
aimed at improving DOB performance, even in the presence of flexibility and
system variability. The proposed method maximizes control bandwidth and
effectively suppresses vibrations, thus enhancing overall system performance.
Closed-loop stability is rigorously proven using the Nyquist stability
criterion. Experimental validation on a FJR demonstrates that the proposed
approach significantly improves robustness and motion performance, even under
conditions of joint flexibility and system variation.

</details>


### [8] [SmartPNT-MSF: A Multi-Sensor Fusion Dataset for Positioning and Navigation Research](https://arxiv.org/abs/2507.19079)
*Feng Zhu,Zihang Zhang,Kangcheng Teng,Abduhelil Yakup,Xiaohong Zhang*

Main category: cs.RO

TL;DR: 论文介绍了SmartPNT多源集成导航、定位和姿态数据集，旨在解决现有数据集在传感器多样性和环境覆盖方面的不足，为多传感器融合和高精度导航研究提供丰富资源。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在传感器多样性和环境覆盖方面存在局限，阻碍了高精度导航和定位技术的发展。

Method: 数据集整合了GNSS、IMU、光学相机和LiDAR等多传感器数据，并详细记录了传感器配置、坐标系定义和校准过程。

Result: 通过VINS-Mono和LIO-SAM等SLAM算法验证，数据集适用于复杂环境中的高级导航研究。

Conclusion: 该数据集填补了传感器多样性、数据可访问性和环境代表性方面的空白，推动了导航技术的创新。

Abstract: High-precision navigation and positioning systems are critical for
applications in autonomous vehicles and mobile mapping, where robust and
continuous localization is essential. To test and enhance the performance of
algorithms, some research institutions and companies have successively
constructed and publicly released datasets. However, existing datasets still
suffer from limitations in sensor diversity and environmental coverage. To
address these shortcomings and advance development in related fields, the
SmartPNT Multisource Integrated Navigation, Positioning, and Attitude Dataset
has been developed. This dataset integrates data from multiple sensors,
including Global Navigation Satellite Systems (GNSS), Inertial Measurement
Units (IMU), optical cameras, and LiDAR, to provide a rich and versatile
resource for research in multi-sensor fusion and high-precision navigation. The
dataset construction process is thoroughly documented, encompassing sensor
configurations, coordinate system definitions, and calibration procedures for
both cameras and LiDAR. A standardized framework for data collection and
processing ensures consistency and scalability, enabling large-scale analysis.
Validation using state-of-the-art Simultaneous Localization and Mapping (SLAM)
algorithms, such as VINS-Mono and LIO-SAM, demonstrates the dataset's
applicability for advanced navigation research. Covering a wide range of
real-world scenarios, including urban areas, campuses, tunnels, and suburban
environments, the dataset offers a valuable tool for advancing navigation
technologies and addressing challenges in complex environments. By providing a
publicly accessible, high-quality dataset, this work aims to bridge gaps in
sensor diversity, data accessibility, and environmental representation,
fostering further innovation in the field.

</details>


### [9] [Bot Appétit! Exploring how Robot Morphology Shapes Perceived Affordances via a Mise en Place Scenario in a VR Kitchen](https://arxiv.org/abs/2507.19082)
*Rachel Ringe,Leandra Thiele,Mihai Pomarlan,Nima Zargham,Robin Nolte,Lars Hurrelbrink,Rainer Malaka*

Main category: cs.RO

TL;DR: 研究探讨机器人视觉设计因素如何影响人类在协作烹饪场景中的布局及任务分配，发现人类偏好与生物形态机器人合作，机器人形态对其感知能力信念影响较小，且纤细机器人减少空间共享时的回避策略。


<details>
  <summary>Details</summary>
Motivation: 探索机器人视觉设计对人类协作行为的影响，特别是在烹饪场景中，以优化人机协作体验。

Method: 在VR环境中，参与者与不同形态的机器人共同布置厨房，收集布局数据、口头思考和问卷回答，进行多模态分析。

Result: 提出假设：人类偏好生物形态机器人；机器人形态对感知能力信念影响较小；纤细机器人减少空间共享时的回避策略。

Conclusion: 研究为后续验证假设奠定了基础，未来将进一步验证这些发现。

Abstract: This study explores which factors of the visual design of a robot may
influence how humans would place it in a collaborative cooking scenario and how
these features may influence task delegation. Human participants were placed in
a Virtual Reality (VR) environment and asked to set up a kitchen for cooking
alongside a robot companion while considering the robot's morphology. We
collected multimodal data for the arrangements created by the participants,
transcripts of their think-aloud as they were performing the task, and
transcripts of their answers to structured post-task questionnaires. Based on
analyzing this data, we formulate several hypotheses: humans prefer to
collaborate with biomorphic robots; human beliefs about the sensory
capabilities of robots are less influenced by the morphology of the robot than
beliefs about action capabilities; and humans will implement fewer avoidance
strategies when sharing space with gracile robots. We intend to verify these
hypotheses in follow-up studies.

</details>


### [10] [Monocular Vision-Based Swarm Robot Localization Using Equilateral Triangular Formations](https://arxiv.org/abs/2507.19100)
*Taewon Kang,Ji-Wook Kwon,Il Bae,Jin Hyo Kim*

Main category: cs.RO

TL;DR: 提出了一种基于等边三角形构型的低成本单目视觉传感器定位方法，适用于开放环境中的群体机器人。


<details>
  <summary>Details</summary>
Motivation: 为低成本单目视觉传感器的群体机器人开发一种在开放空间中无需地标或定位基础设施的精确定位系统。

Method: 利用等边三角形的几何特性，通过机器人间的一维横向距离信息估计二维位置。

Result: 实验和仿真表明，随着时间推移，该方法的定位误差显著低于传统的航位推算法。

Conclusion: 该方法在开放环境中为低成本群体机器人提供了有效的定位解决方案。

Abstract: Localization of mobile robots is crucial for deploying robots in real-world
applications such as search and rescue missions. This work aims to develop an
accurate localization system applicable to swarm robots equipped only with
low-cost monocular vision sensors and visual markers. The system is designed to
operate in fully open spaces, without landmarks or support from positioning
infrastructures. To achieve this, we propose a localization method based on
equilateral triangular formations. By leveraging the geometric properties of
equilateral triangles, the accurate two-dimensional position of each
participating robot is estimated using one-dimensional lateral distance
information between robots, which can be reliably and accurately obtained with
a low-cost monocular vision sensor. Experimental and simulation results
demonstrate that, as travel time increases, the positioning error of the
proposed method becomes significantly smaller than that of a conventional
dead-reckoning system, another low-cost localization approach applicable to
open environments.

</details>


### [11] [Diverse and Adaptive Behavior Curriculum for Autonomous Driving: A Student-Teacher Framework with Multi-Agent RL](https://arxiv.org/abs/2507.19146)
*Ahmed Abouelazm,Johannes Ratz,Philip Schörner,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 论文提出了一种基于学生-教师框架的自动课程学习方法，用于提升强化学习在自动驾驶中的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在复杂交通环境中面临挑战，现有强化学习方法依赖规则生成的交通场景，泛化能力有限，且缺乏对常规驾驶行为的平衡。

Method: 采用学生-教师框架，教师通过基于图的多智能体强化学习动态生成不同难度的交通行为，学生则通过深度强化学习在部分可观测条件下训练。

Result: 实验表明，教师能生成多样化的交通行为，学生通过自动课程学习在奖励和驾驶表现上优于基于规则训练的智能体。

Conclusion: 自动课程学习方法有效提升了自动驾驶策略的鲁棒性和覆盖范围，为复杂交通场景下的强化学习提供了新思路。

Abstract: Autonomous driving faces challenges in navigating complex real-world traffic,
requiring safe handling of both common and critical scenarios. Reinforcement
learning (RL), a prominent method in end-to-end driving, enables agents to
learn through trial and error in simulation. However, RL training often relies
on rule-based traffic scenarios, limiting generalization. Additionally, current
scenario generation methods focus heavily on critical scenarios, neglecting a
balance with routine driving behaviors. Curriculum learning, which
progressively trains agents on increasingly complex tasks, is a promising
approach to improving the robustness and coverage of RL driving policies.
However, existing research mainly emphasizes manually designed curricula,
focusing on scenery and actor placement rather than traffic behavior dynamics.
This work introduces a novel student-teacher framework for automatic curriculum
learning. The teacher, a graph-based multi-agent RL component, adaptively
generates traffic behaviors across diverse difficulty levels. An adaptive
mechanism adjusts task difficulty based on student performance, ensuring
exposure to behaviors ranging from common to critical. The student, though
exchangeable, is realized as a deep RL agent with partial observability,
reflecting real-world perception constraints. Results demonstrate the teacher's
ability to generate diverse traffic behaviors. The student, trained with
automatic curricula, outperformed agents trained on rule-based traffic,
achieving higher rewards and exhibiting balanced, assertive driving.

</details>


### [12] [ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination](https://arxiv.org/abs/2507.19151)
*Michael Amir,Guang Yang,Zhan Gao,Keisuke Okumura,Heedo Woo,Amanda Prorok*

Main category: cs.RO

TL;DR: ReCoDe是一个结合优化控制器和多智能体强化学习的混合框架，通过学习动态约束改进专家控制器，提升多智能体导航任务的性能。


<details>
  <summary>Details</summary>
Motivation: 在多智能体环境中，手工设计的约束可能无法满足复杂协调需求，因此需要一种更灵活的方法来改进现有控制器。

Method: ReCoDe通过局部通信学习动态约束，结合优化控制器和强化学习的优势，动态调整对专家控制器的依赖。

Result: 在需要复杂协调的多智能体导航任务中，ReCoDe优于手工控制器、其他混合方法和标准MARL基线。

Conclusion: 保留用户定义的控制器并动态调整其依赖程度，比从头学习更高效，ReCoDe为此提供了理论和实证支持。

Abstract: Constraint-based optimization is a cornerstone of robotics, enabling the
design of controllers that reliably encode task and safety requirements such as
collision avoidance or formation adherence. However, handcrafted constraints
can fail in multi-agent settings that demand complex coordination. We introduce
ReCoDe--Reinforcement-based Constraint Design--a decentralized, hybrid
framework that merges the reliability of optimization-based controllers with
the adaptability of multi-agent reinforcement learning. Rather than discarding
expert controllers, ReCoDe improves them by learning additional, dynamic
constraints that capture subtler behaviors, for example, by constraining agent
movements to prevent congestion in cluttered scenarios. Through local
communication, agents collectively constrain their allowed actions to
coordinate more effectively under changing conditions. In this work, we focus
on applications of ReCoDe to multi-agent navigation tasks requiring intricate,
context-based movements and consensus, where we show that it outperforms purely
handcrafted controllers, other hybrid approaches, and standard MARL baselines.
We give empirical (real robot) and theoretical evidence that retaining a
user-defined controller, even when it is imperfect, is more efficient than
learning from scratch, especially because ReCoDe can dynamically change the
degree to which it relies on this controller.

</details>


### [13] [Towards Multimodal Social Conversations with Robots: Using Vision-Language Models](https://arxiv.org/abs/2507.19196)
*Ruben Janssens,Tony Belpaeme*

Main category: cs.RO

TL;DR: 论文探讨了如何利用视觉语言模型增强社交机器人的多模态对话能力，以解决当前大语言模型在社交互动中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在社交机器人中缺乏多模态交互能力，无法充分利用视觉等模态信息进行社交对话。

Method: 提出利用视觉语言模型处理广泛的视觉信息，并描述其适应社交机器人场景的方法及技术挑战。

Result: 视觉语言模型能够为自主社交机器人提供足够通用的多模态处理能力。

Conclusion: 论文总结了视觉语言模型在社交机器人中的应用潜力，并讨论了评估实践。

Abstract: Large language models have given social robots the ability to autonomously
engage in open-domain conversations. However, they are still missing a
fundamental social skill: making use of the multiple modalities that carry
social interactions. While previous work has focused on task-oriented
interactions that require referencing the environment or specific phenomena in
social interactions such as dialogue breakdowns, we outline the overall needs
of a multimodal system for social conversations with robots. We then argue that
vision-language models are able to process this wide range of visual
information in a sufficiently general manner for autonomous social robots. We
describe how to adapt them to this setting, which technical challenges remain,
and briefly discuss evaluation practices.

</details>


### [14] [Foundation Model-Driven Grasping of Unknown Objects via Center of Gravity Estimation](https://arxiv.org/abs/2507.19242)
*Kang Xiangli,Yage He,Xianwu Gong,Zehan Liu,Yuru Bai*

Main category: cs.RO

TL;DR: 提出了一种基于扩散模型的方法，用于定位质量分布不均物体的重心（CoG），以提高机器人抓取的稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于关键点或功能驱动的方法在抓取质量分布不均物体时存在局限性，导致姿态不稳定。

Method: 构建了包含790张图像的数据集，标注了CoG关键点，并开发了基于基础模型的视觉驱动框架。

Result: 实验表明，该方法比传统关键点方法成功率提高49%，比最新功能驱动方法提高11%，且在未见物体上CoG定位准确率达76%。

Conclusion: 该方法为精确稳定的抓取任务提供了新解决方案。

Abstract: This study presents a grasping method for objects with uneven mass
distribution by leveraging diffusion models to localize the center of gravity
(CoG) on unknown objects. In robotic grasping, CoG deviation often leads to
postural instability, where existing keypoint-based or affordance-driven
methods exhibit limitations. We constructed a dataset of 790 images featuring
unevenly distributed objects with keypoint annotations for CoG localization. A
vision-driven framework based on foundation models was developed to achieve
CoG-aware grasping. Experimental evaluations across real-world scenarios
demonstrate that our method achieves a 49\% higher success rate compared to
conventional keypoint-based approaches and an 11\% improvement over
state-of-the-art affordance-driven methods. The system exhibits strong
generalization with a 76\% CoG localization accuracy on unseen objects,
providing a novel solution for precise and stable grasping tasks.

</details>


### [15] [How Age Influences the Interpretation of Emotional Body Language in Humanoid Robots -- long paper version](https://arxiv.org/abs/2507.19335)
*Ilaria Consoli,Claudio Mattutino,Cristina Gena,Berardina de Carolis,Giuseppe Palestra*

Main category: cs.RO

TL;DR: 研究不同年龄组（儿童、年轻人和老年人）如何理解人形机器人NAO表达的情感肢体语言，揭示用户对机器人情感线索的感知差异。


<details>
  <summary>Details</summary>
Motivation: 探讨用户如何感知和响应机器人情感线索，评估机器人对不同年龄组的情感传达效果。

Method: 通过收集老年人数据并与年轻人和儿童数据对比，分析年龄组间的异同。

Result: 年轻人和老年人对情感的理解更相似，但与年轻人存在差异。

Conclusion: 年龄影响对机器人情感肢体语言的解读，年轻人和老年人表现相似，但与年轻人不同。

Abstract: This paper presents an empirical study investigating how individuals across
different age groups, children, young and older adults, interpret emotional
body language expressed by the humanoid robot NAO. The aim is to offer insights
into how users perceive and respond to emotional cues from robotic agents,
through an empirical evaluation of the robot's effectiveness in conveying
emotions to different groups of users. By analyzing data collected from elderly
participants and comparing these findings with previously gathered data from
young adults and children, the study highlights similarities and differences
between the groups, with younger and older users more similar but different
from young adults.

</details>
