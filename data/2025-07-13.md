<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [g2o vs. Ceres: Optimizing Scan Matching in Cartographer SLAM](https://arxiv.org/abs/2507.07142)
*Quanjie Qiu,MengCheng Lau*

Main category: cs.RO

TL;DR: 比较g2o和Ceres求解器在Cartographer框架中提升扫描匹配性能的表现，发现Ceres在速度、收敛效率和地图清晰度上优于g2o，而g2o在局部障碍物检测中表现更好。


<details>
  <summary>Details</summary>
Motivation: 评估g2o与Ceres求解器在Cartographer中的性能差异，以优化SLAM的精度和效率。

Method: 在Cartographer框架中对比g2o和Ceres求解器的性能，使用AgileX LIMO机器人进行实验。

Result: Ceres在速度、收敛效率和地图清晰度上优于g2o，但g2o在局部障碍物检测中表现更佳。

Conclusion: Ceres更适合全局优化任务，而g2o在特定场景（如局部障碍物检测）中有优势。

Abstract: This article presents a comparative analysis of g2o and Ceres solvers in
enhancing scan matching performance within the Cartographer framework.
Cartographer, a widely-used library for Simultaneous Localization and Mapping
(SLAM), relies on optimization algorithms to refine pose estimates and improve
map accuracy. The research aims to evaluate the performance, efficiency, and
accuracy of the g2o solver in comparison to the Ceres solver, which is the
default in Cartographer. In our experiments comparing Ceres and g2o within
Cartographer, Ceres outperformed g2o in terms of speed, convergence efficiency,
and overall map clarity. Ceres required fewer iterations and less time to
converge, producing more accurate and well-defined maps, especially in
real-world mapping scenarios with the AgileX LIMO robot. However, g2o excelled
in localized obstacle detection, highlighting its value in specific situations.

</details>


### [2] [Self-Wearing Adaptive Garments via Soft Robotic Unfurling](https://arxiv.org/abs/2507.07221)
*Nam Gyun Kim,William E. Heap,Yimeng Qin,Elvy B. Yao,Jee-Hwan Ryu,Allison M. Okamura*

Main category: cs.RO

TL;DR: 提出了一种新型软机器人穿衣系统SWAG，通过展开和生长机制实现自主穿衣，解决了传统刚性机器人处理可变形衣物和确保安全交互的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有机器人穿衣方案依赖刚性机械臂，存在操作时间长、控制复杂、用户姿势受限等问题，限制了实用性和适应性。

Method: 采用基于展开的部署方法，使SWAG贴合人体，减少皮肤与衣物摩擦，实现更安全高效的穿衣过程。

Result: SWAG在不同衣物配置下表现出色，为传统机器人穿衣辅助提供了有前景的替代方案。

Conclusion: SWAG系统通过软机器人技术，显著提升了穿衣辅助的效率与安全性，具有广泛应用潜力。

Abstract: Robotic dressing assistance has the potential to improve the quality of life
for individuals with limited mobility. Existing solutions predominantly rely on
rigid robotic manipulators, which have challenges in handling deformable
garments and ensuring safe physical interaction with the human body. Prior
robotic dressing methods require excessive operation times, complex control
strategies, and constrained user postures, limiting their practicality and
adaptability. This paper proposes a novel soft robotic dressing system, the
Self-Wearing Adaptive Garment (SWAG), which uses an unfurling and growth
mechanism to facilitate autonomous dressing. Unlike traditional approaches,the
SWAG conforms to the human body through an unfurling based deployment method,
eliminating skin-garment friction and enabling a safer and more efficient
dressing process. We present the working principles of the SWAG, introduce its
design and fabrication, and demonstrate its performance in dressing assistance.
The proposed system demonstrates effective garment application across various
garment configurations, presenting a promising alternative to conventional
robotic dressing assistance.

</details>


### [3] [3D Steering and Localization in Pipes and Burrows using an Externally Steered Soft Growing Robot](https://arxiv.org/abs/2507.07225)
*Yimeng Qin,Jared Grinberg,William Heap,Allison M. Okamura*

Main category: cs.RO

TL;DR: 提出了一种可转向的藤蔓机器人，用于在管道和洞穴等狭窄环境中导航，具备主动分支选择、小半径管道导航、锐角转向和实时3D定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有藤蔓机器人在人造和自然通道中导航困难，尤其是分支和锐角3D转弯场景。

Method: 设计了具有简单管状体和外部尖端支架的机器人，通过改变生长方向和必要时支撑管道壁实现三自由度转向。

Result: 实现了51.7°的最大转向角、2.5厘米小半径管道导航、锐角转向能力及GPS缺失环境下的实时3D定位。

Conclusion: 该机器人设计在复杂管道和洞穴环境中表现出优越的导航和适应性。

Abstract: Navigation and inspection in confined environments, such as tunnels and
pipes, pose significant challenges for existing robots due to limitations in
maneuverability and adaptability to varying geometries. Vine robots, which are
soft growing continuum robots that extend their length through soft material
eversion at their tip, offer unique advantages due to their ability to navigate
tight spaces, adapt to complex paths, and minimize friction. However, existing
vine robot designs struggle with navigation in manmade and natural passageways,
with branches and sharp 3D turns. In this letter, we introduce a steerable vine
robot specifically designed for pipe and burrow environments. The robot
features a simple tubular body and an external tip mount that steers the vine
robot in three degrees of freedom by changing the growth direction and, when
necessary, bracing against the wall of the pipe or burrow. Our external tip
steering approach enables: (1) active branch selection in 3D space with a
maximum steerable angle of 51.7{\deg}, (2) navigation of pipe networks with
radii as small as 2.5 cm, (3) a compliant tip enabling navigation of sharp
turns, and (4) real-time 3D localization in GPS-denied environments using
tip-mounted sensors and continuum body odometry. We describe the forward
kinematics, characterize steerability, and demonstrate the system in a 3D pipe
system as well as a natural animal burrow.

</details>


### [4] [LangNavBench: Evaluation of Natural Language Understanding in Semantic Navigation](https://arxiv.org/abs/2507.07299)
*Sonia Raychaudhuri,Enrico Cancelli,Tommaso Campari,Lamberto Ballan,Manolis Savva,Angel X. Chang*

Main category: cs.RO

TL;DR: LangNav是一个新开放的数据集，用于测试语言导航代理对自然语言指令的理解能力，并提供了LangNavBench基准和MLFM方法。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个专注于语言理解的导航基准，LangNav填补了这一空白，旨在评估代理对不同层次语言描述的理解能力。

Method: 提出了LangNav数据集和LangNavBench基准，并开发了Multi-Layered Feature Map (MLFM)方法，用于构建可查询的多层语义地图。

Result: MLFM在LangNav数据集上优于现有的基于地图的导航基线。

Conclusion: LangNav和LangNavBench为语言导航提供了系统化的评估工具，MLFM方法在复杂语言指令下表现优异。

Abstract: Recent progress in large vision-language models has driven improvements in
language-based semantic navigation, where an embodied agent must reach a target
object described in natural language. Despite these advances, we still lack a
clear, language-focused benchmark for testing how well such agents ground the
words in their instructions. We address this gap with LangNav, an open-set
dataset specifically created to test an agent's ability to locate objects
described at different levels of detail, from broad category names to fine
attributes and object-object relations. Every description in LangNav was
manually checked, yielding a lower error rate than existing lifelong- and
semantic-navigation datasets. On top of LangNav we build LangNavBench, a
benchmark that measures how well current semantic-navigation methods understand
and act on these descriptions while moving toward their targets. LangNavBench
allows us to systematically compare models on their handling of attributes,
spatial and relational cues, and category hierarchies, offering the first
thorough, language-centric evaluation of embodied navigation systems. We also
present Multi-Layered Feature Map (MLFM), a method that builds a queryable
multi-layered semantic map, particularly effective when dealing with small
objects or instructions involving spatial relations. MLFM outperforms
state-of-the-art mapping-based navigation baselines on the LangNav dataset.

</details>


### [5] [Classifying Emergence in Robot Swarms: An Observer-Dependent Approach](https://arxiv.org/abs/2507.07315)
*Ricardo Vega,Cameron Nowzari*

Main category: cs.RO

TL;DR: 论文探讨了‘涌现’和‘群体’的正式定义问题，提出一个框架以区分可观测和不可观测状态，强调这些概念的主观性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏对‘涌现’和‘群体’的共识定义，新老研究者难以交流，论文旨在提供一个严谨的讨论框架。

Method: 提出一个区分可观测和不可观测状态的框架，用于比较现有定义。

Result: 认为‘群体’不仅由群体行为定义，更由其生成过程决定，概念主观性源于观察者视角。

Conclusion: 论文支持机器人群体系统的设计与部署，强调多机器人系统与真正群体的区别。

Abstract: Emergence and swarms are widely discussed topics, yet no consensus exists on
their formal definitions. This lack of agreement makes it difficult not only
for new researchers to grasp these concepts, but also for experts who may use
the same terms to mean different things. Many attempts have been made to
objectively define 'swarm' or 'emergence,' with recent work highlighting the
role of the external observer. Still, several researchers argue that once an
observer's vantage point (e.g., scope, resolution, context) is established, the
terms can be made objective or measured quantitatively. In this note, we
propose a framework to discuss these ideas rigorously by separating externally
observable states from latent, unobservable ones. This allows us to compare and
contrast existing definitions of swarms and emergence on common ground. We
argue that these concepts are ultimately subjective-shaped less by the system
itself than by the perception and tacit knowledge of the observer.
Specifically, we suggest that a 'swarm' is not defined by its group behavior
alone, but by the process generating that behavior. Our broader goal is to
support the design and deployment of robotic swarm systems, highlighting the
critical distinction between multi-robot systems and true swarms.

</details>


### [6] [Effects of Wrist-Worn Haptic Feedback on Force Accuracy and Task Speed during a Teleoperated Robotic Surgery Task](https://arxiv.org/abs/2507.07327)
*Brian B. Vuong,Josie Davidson,Sangheui Cheon,Kyujin Cho,Allison M. Okamura*

Main category: cs.RO

TL;DR: 研究探讨了将触觉反馈从手部转移到腕部对机器人辅助微创手术中力应用准确性的影响，发现腕部触觉反馈显著降低了力误差。


<details>
  <summary>Details</summary>
Motivation: 手部触觉反馈会干扰手术机器人的操作，因此研究腕部触觉反馈的可行性及其效果。

Method: 使用软气动手腕穿戴设备在da Vinci手术机器人上测试腕部触觉反馈对力应用准确性的影响。

Result: 腕部触觉反馈显著降低了力误差，但增加了操作时间。

Conclusion: 腕部触觉反馈有效，但可能影响操作速度与准确性的权衡。

Abstract: Previous work has shown that the addition of haptic feedback to the hands can
improve awareness of tool-tissue interactions and enhance performance of
teleoperated tasks in robot-assisted minimally invasive surgery. However,
hand-based haptic feedback occludes direct interaction with the manipulanda of
surgeon console in teleoperated surgical robots. We propose relocating haptic
feedback to the wrist using a wearable haptic device so that haptic feedback
mechanisms do not need to be integrated into the manipulanda. However, it is
unknown if such feedback will be effective, given that it is not co-located
with the finger movements used for manipulation. To test if relocated haptic
feedback improves force application during teleoperated tasks using da Vinci
Research Kit (dVRK) surgical robot, participants learned to palpate a phantom
tissue to desired forces. A soft pneumatic wrist-worn haptic device with an
anchoring system renders tool-tissue interaction forces to the wrist of the
user. Participants performed the palpation task with and without wrist-worn
haptic feedback and were evaluated for the accuracy of applied forces.
Participants demonstrated statistically significant lower force error when
wrist-worn haptic feedback was provided. Participants also performed the
palpation task with longer movement times when provided wrist-worn haptic
feedback, indicating that the haptic feedback may have caused participants to
operate at a different point in the speed-accuracy tradeoff curve.

</details>


### [7] [UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid Robots](https://arxiv.org/abs/2507.07356)
*Kangning Yin,Weishuai Zeng,Ke Fan,Zirui Wang,Qiang Zhang,Zheng Tian,Jingbo Wang,Jiangmiao Pang,Weinan Zhang*

Main category: cs.RO

TL;DR: UniTracker是一个基于CVAE的框架，用于提升人形机器人全身控制的多样性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在策略蒸馏过程中会损失运动多样性，且对新行为的泛化能力有限。

Method: 通过将CVAE整合到学生策略中，显式建模人类运动的潜在多样性。

Result: UniTracker在运动质量、对新参考的泛化能力和部署鲁棒性上显著优于基线方法。

Conclusion: UniTracker为表达性人形控制提供了一种实用且可扩展的解决方案。

Abstract: Humanoid robots must achieve diverse, robust, and generalizable whole-body
control to operate effectively in complex, human-centric environments. However,
existing methods, particularly those based on teacher-student frameworks often
suffer from a loss of motion diversity during policy distillation and exhibit
limited generalization to unseen behaviors. In this work, we present
UniTracker, a simplified yet powerful framework that integrates a Conditional
Variational Autoencoder (CVAE) into the student policy to explicitly model the
latent diversity of human motion. By leveraging a learned CVAE prior, our
method enables the student to retain expressive motion characteristics while
improving robustness and adaptability under partial observations. The result is
a single policy capable of tracking a wide spectrum of whole-body motions with
high fidelity and stability. Comprehensive experiments in both simulation and
real-world deployments demonstrate that UniTracker significantly outperforms
MLP-based DAgger baselines in motion quality, generalization to unseen
references, and deployment robustness, offering a practical and scalable
solution for expressive humanoid control.

</details>


### [8] [Data-driven Kinematic Modeling in Soft Robots: System Identification and Uncertainty Quantification](https://arxiv.org/abs/2507.07370)
*Zhanhong Jiang,Dylan Shah,Hsin-Jung Yang,Soumik Sarkar*

Main category: cs.RO

TL;DR: 论文提出了一种基于共形预测的软体机器人运动学建模框架，用于量化预测不确定性，提升建模精度。


<details>
  <summary>Details</summary>
Motivation: 软体机器人运动学建模因高度非线性和复杂行为而具有挑战性，现有数据驱动模型存在预测不确定性，影响精度。

Method: 研究多种线性和非线性机器学习模型，发现非线性集成方法泛化性能最佳；提出共形运动学建模框架，利用分裂共形预测量化位置不确定性。

Result: 非线性集成方法表现最稳健；共形预测框架提供理论保证的分布无关预测区间。

Conclusion: 共形预测框架有效解决了软体机器人运动学建模中的不确定性量化问题。

Abstract: Precise kinematic modeling is critical in calibration and controller design
for soft robots, yet remains a challenging issue due to their highly nonlinear
and complex behaviors. To tackle the issue, numerous data-driven machine
learning approaches have been proposed for modeling nonlinear dynamics.
However, these models suffer from prediction uncertainty that can negatively
affect modeling accuracy, and uncertainty quantification for kinematic modeling
in soft robots is underexplored. In this work, using limited simulation and
real-world data, we first investigate multiple linear and nonlinear machine
learning models commonly used for kinematic modeling of soft robots. The
results reveal that nonlinear ensemble methods exhibit the most robust
generalization performance. We then develop a conformal kinematic modeling
framework for soft robots by utilizing split conformal prediction to quantify
predictive position uncertainty, ensuring distribution-free prediction
intervals with a theoretical guarantee.

</details>


### [9] [PILOC: A Pheromone Inverse Guidance Mechanism and Local-Communication Framework for Dynamic Target Search of Multi-Agent in Unknown Environments](https://arxiv.org/abs/2507.07376)
*Hengrui Liu,Yi Feng,Qilong Zhang*

Main category: cs.RO

TL;DR: PILOC框架通过局部感知和通信解决多智能体搜索救援中的动态和未知环境问题，结合信息素机制和深度强化学习提升效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 动态和未知环境中的目标不可预测性和环境不确定性对多智能体搜索救援（MASAR）提出了重大挑战。

Method: 提出PILOC框架，利用局部感知和通信，引入信息素逆向引导机制，并将其嵌入深度强化学习的观察空间。

Result: 实验表明，PILOC在动态和通信受限场景下表现优于现有方法，显著提升搜索效率、适应性和系统鲁棒性。

Conclusion: PILOC为未来MASAR应用提供了有前景的方向，特别是在动态和通信受限的环境中。

Abstract: Multi-Agent Search and Rescue (MASAR) plays a vital role in disaster
response, exploration, and reconnaissance. However, dynamic and unknown
environments pose significant challenges due to target unpredictability and
environmental uncertainty. To tackle these issues, we propose PILOC, a
framework that operates without global prior knowledge, leveraging local
perception and communication. It introduces a pheromone inverse guidance
mechanism to enable efficient coordination and dynamic target localization.
PILOC promotes decentralized cooperation through local communication,
significantly reducing reliance on global channels. Unlike conventional
heuristics, the pheromone mechanism is embedded into the observation space of
Deep Reinforcement Learning (DRL), supporting indirect agent coordination based
on environmental cues. We further integrate this strategy into a DRL-based
multi-agent architecture and conduct extensive experiments. Results show that
combining local communication with pheromone-based guidance significantly
boosts search efficiency, adaptability, and system robustness. Compared to
existing methods, PILOC performs better under dynamic and
communication-constrained scenarios, offering promising directions for future
MASAR applications.

</details>


### [10] [Towards Safe Autonomous Driving: A Real-Time Safeguarding Concept for Motion Planning Algorithms](https://arxiv.org/abs/2507.07444)
*Korbinian Moller,Rafael Neher,Marvin Seegert,Johannes Betz*

Main category: cs.RO

TL;DR: 该论文提出了一种用于自动驾驶车辆运动规划模块的时间保护机制，扩展了现有方法，通过监控规划输出的时间一致性来确保系统响应及时。原型实现表明该模块在实时范围内运行并能有效检测不安全轨迹。


<details>
  <summary>Details</summary>
Motivation: 确保自动驾驶车辆运动规划模块的功能安全性是一个关键挑战，尤其是在处理复杂或基于学习的软件时。在线验证是一种有前景的方法，但其在嵌入式实时环境中的集成仍有限。

Method: 提出了一种时间保护机制，通过约束可行性检查和基于成本的合理性指标评估轨迹候选。原型在实时操作系统上实现。

Result: 初步结果显示，保护模块在实时范围内运行，并能有效检测不安全轨迹。

Conclusion: 该研究为运行时轨迹验证提供了一个模块化和可扩展的框架，并强调了在汽车级硬件上部署的关键方面。未来工作包括完成保护逻辑并通过硬件在环仿真和车辆测试验证其有效性。

Abstract: Ensuring the functional safety of motion planning modules in autonomous
vehicles remains a critical challenge, especially when dealing with complex or
learning-based software. Online verification has emerged as a promising
approach to monitor such systems at runtime, yet its integration into embedded
real-time environments remains limited. This work presents a safeguarding
concept for motion planning that extends prior approaches by introducing a time
safeguard. While existing methods focus on geometric and dynamic feasibility,
our approach additionally monitors the temporal consistency of planning outputs
to ensure timely system response. A prototypical implementation on a real-time
operating system evaluates trajectory candidates using constraint-based
feasibility checks and cost-based plausibility metrics. Preliminary results
show that the safeguarding module operates within real-time bounds and
effectively detects unsafe trajectories. However, the full integration of the
time safeguard logic and fallback strategies is ongoing. This study contributes
a modular and extensible framework for runtime trajectory verification and
highlights key aspects for deployment on automotive-grade hardware. Future work
includes completing the safeguarding logic and validating its effectiveness
through hardware-in-the-loop simulations and vehicle-based testing. The code is
available at: https://github.com/TUM-AVS/motion-planning-supervisor

</details>


### [11] [SCREP: Scene Coordinate Regression and Evidential Learning-based Perception-Aware Trajectory Generation](https://arxiv.org/abs/2507.07467)
*Juyeop Han,Lukas Lao Beyer,Guilherme V. Cavalheiro,Sertac Karaman*

Main category: cs.RO

TL;DR: 提出了一种感知感知框架，结合了基于证据学习的SCR姿态估计器和后退水平轨迹优化器，用于GPS拒绝的室内空间自主飞行。


<details>
  <summary>Details</summary>
Motivation: 解决GPS拒绝室内空间中视觉定位误差累积的问题，提高自主飞行的精确性。

Method: 结合SCR姿态估计器和轨迹优化器，通过优化相机视角选择可靠像素，并融合SCR与IMU数据实现实时闭环控制。

Result: 仿真中，平移和旋转误差分别减少了54%/15%和40%/31%，硬件实验验证了框架的可行性。

Conclusion: 该框架显著降低了定位误差，适用于GPS拒绝环境中的自主飞行。

Abstract: Autonomous flight in GPS denied indoor spaces requires trajectories that keep
visual localization error tightly bounded across varied missions. Whereas
visual inertial odometry (VIO) accumulates drift over time, scene coordinate
regression (SCR) yields drift-free, high accuracy absolute pose estimation. We
present a perception-aware framework that couples an evidential learning-based
SCR pose estimator with a receding horizon trajectory optimizer. The optimizer
steers the onboard camera toward pixels whose uncertainty predicts reliable
scene coordinates, while a fixed-lag smoother fuses the low rate SCR stream
with high rate IMU data to close the perception control loop in real time. In
simulation, our planner reduces translation (rotation) mean error by 54% / 15%
(40% / 31%) relative to yaw fixed and forward-looking baselines, respectively.
Moreover, hardware in the loop experiment validates the feasibility of our
proposed framework.

</details>


### [12] [FiDTouch: A 3D Wearable Haptic Display for the Finger Pad](https://arxiv.org/abs/2507.07661)
*Daria Trinitatova,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: FiDTouch是一种3D可穿戴触觉设备，通过微型倒置Delta机器人提供精确的触觉反馈，提升人机交互体验。


<details>
  <summary>Details</summary>
Motivation: 指尖触觉设备在虚拟现实、医疗培训和远程机器人操作等领域具有广泛应用潜力，但现有设备难以提供精确的动态触觉反馈。

Method: 设计了一种基于微型倒置Delta机器人的3D可穿戴触觉设备FiDTouch，能够提供接触、压力、皮肤拉伸和振动等触觉反馈。

Result: 通过两阶段用户研究验证了设备在静态空间接触和皮肤拉伸刺激感知方面的性能，显示其能提供精确的触觉反馈。

Conclusion: FiDTouch通过精确的触觉反馈，提升了人机交互的沉浸感和效率。

Abstract: The applications of fingertip haptic devices have spread to various fields
from revolutionizing virtual reality and medical training simulations to
facilitating remote robotic operations, proposing great potential for enhancing
user experiences, improving training outcomes, and new forms of interaction. In
this work, we present FiDTouch, a 3D wearable haptic device that delivers
cutaneous stimuli to the finger pad, such as contact, pressure, encounter, skin
stretch, and vibrotactile feedback. The application of a tiny inverted Delta
robot in the mechanism design allows providing accurate contact and fast
changing dynamic stimuli to the finger pad surface. The performance of the
developed display was evaluated in a two-stage user study of the perception of
static spatial contact stimuli and skin stretch stimuli generated on the finger
pad. The proposed display, by providing users with precise touch and force
stimuli, can enhance user immersion and efficiency in the fields of
human-computer and human-robot interactions.

</details>


### [13] [Adaptive Gaussian Mixture Models-based Anomaly Detection for under-constrained Cable-Driven Parallel Robots](https://arxiv.org/abs/2507.07714)
*Julio Garrido,Javier Vales,Diego Silva-Muñiz,Enrique Riveiro,Pablo López-Matencio,Josué Rivera-Andrade*

Main category: cs.RO

TL;DR: 论文提出了一种基于高斯混合模型（GMM）的自适应无监督异常检测算法，仅利用电机扭矩数据检测电缆驱动并联机器人（CDPRs）中的异常。


<details>
  <summary>Details</summary>
Motivation: 在CDPRs的负载操纵任务中，需在固定姿态下检测异常（如风或电缆碰撞），但传统方法依赖额外传感器。本文探索仅用扭矩数据实现异常检测的可能性。

Method: 通过GMM拟合无异常数据，利用马氏距离实时评估扭矩信号，动态更新模型参数以适应环境变化。

Result: 在14次长时间测试中，方法实现了100%真阳性率和95.4%平均真阴性率，检测延迟为1秒。

Conclusion: 该方法无需额外传感器，对漂移和环境变化具有更高鲁棒性，优于功率阈值和非自适应GMM方法。

Abstract: Cable-Driven Parallel Robots (CDPRs) are increasingly used for load
manipulation tasks involving predefined toolpaths with intermediate stops. At
each stop, where the platform maintains a fixed pose and the motors keep the
cables under tension, the system must evaluate whether it is safe to proceed by
detecting anomalies that could compromise performance (e.g., wind gusts or
cable impacts). This paper investigates whether anomalies can be detected using
only motor torque data, without additional sensors. It introduces an adaptive,
unsupervised outlier detection algorithm based on Gaussian Mixture Models
(GMMs) to identify anomalies from torque signals. The method starts with a
brief calibration period, just a few seconds, during which a GMM is fit on
known anomaly-free data. Real-time torque measurements are then evaluated using
Mahalanobis distance from the GMM, with statistically derived thresholds
triggering anomaly flags. Model parameters are periodically updated using the
latest segments identified as anomaly-free to adapt to changing conditions.
Validation includes 14 long-duration test sessions simulating varied wind
intensities. The proposed method achieves a 100% true positive rate and 95.4%
average true negative rate, with 1-second detection latency. Comparative
evaluation against power threshold and non-adaptive GMM methods indicates
higher robustness to drift and environmental variation.

</details>


### [14] [Implementation and Assessment of an Augmented Training Curriculum for Surgical Robotics](https://arxiv.org/abs/2507.07718)
*Alberto Rota,Ke Fan,Elena De Momi*

Main category: cs.RO

TL;DR: 研究开发了一种触觉增强的虚拟现实手术机器人训练模拟器，通过引入高级触觉辅助算法，显著提升了训练效果和技能迁移能力。


<details>
  <summary>Details</summary>
Motivation: 通过整合高级辅助算法到手术机器人训练课程中，帮助外科医生建立更全面和稳健的技能，从而提升临床表现。

Method: 开发并验证了一种触觉增强的虚拟现实模拟器，包含8个手术任务，通过物理引擎实现交互，并引入高级触觉接口提供运动引导和性能评分。

Result: 实验表明，引入增强的机器人辅助显著提升了训练过程中的表现，并促进了技能向无辅助手术场景的迁移。

Conclusion: 触觉增强的虚拟现实模拟器结合高级辅助算法，能有效提升手术机器人训练的效果和技能迁移能力。

Abstract: The integration of high-level assistance algorithms in surgical robotics
training curricula may be beneficial in establishing a more comprehensive and
robust skillset for aspiring surgeons, improving their clinical performance as
a consequence. This work presents the development and validation of a
haptic-enhanced Virtual Reality simulator for surgical robotics training,
featuring 8 surgical tasks that the trainee can interact with thanks to the
embedded physics engine. This virtual simulated environment is augmented by the
introduction of high-level haptic interfaces for robotic assistance that aim at
re-directing the motion of the trainee's hands and wrists toward targets or
away from obstacles, and providing a quantitative performance score after the
execution of each training exercise.An experimental study shows that the
introduction of enhanced robotic assistance into a surgical robotics training
curriculum improves performance during the training process and, crucially,
promotes the transfer of the acquired skills to an unassisted surgical
scenario, like the clinical one.

</details>


### [15] [Distributed Surface Inspection via Operational Modal Analysis by a Swarm of Miniaturized Vibration-Sensing Robots](https://arxiv.org/abs/2507.07724)
*Thiemen Siemensma,Niels de Boer,Bahar Haghighat*

Main category: cs.RO

TL;DR: 论文研究了利用微型振动传感机器人群体在仿真环境中检测和定位结构损伤的方法，结合高斯过程估计和模态分析，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统静态传感器网络在结构覆盖上存在局限性，机器人群体为分布式传感应用提供了新可能，尤其是在结构监测领域。

Method: 在仿真环境中，使用有限元分析获取结构振动数据，结合高斯过程估计引导机器人探索，并通过模态分析检测损伤。

Result: 模拟研究验证了微型机器人群体在振动检测中的有效性，分析了探索半径对估计不确定性的影响。

Conclusion: 该方法在仿真中表现出对结构损伤检测的高效性，为实际应用提供了潜在解决方案。

Abstract: Robot swarms offer the potential to serve a variety of distributed sensing
applications. An interesting real-world application that stands to benefit
significantly from deployment of swarms is structural monitoring, where
traditional sensor networks face challenges in structural coverage due to their
static nature. This paper investigates the deployment of a swarm of
miniaturized vibration sensing robots to inspect and localize structural
damages on a surface section within a high-fidelity simulation environment. In
particular, we consider a 1 m x 1 m x 3 mm steel surface section and utilize
finite element analysis using Abaqus to obtain realistic structural vibration
data. The resulting vibration data is imported into the physics-based robotic
simulator Webots, where we simulate the dynamics of our surface inspecting
robot swarm. We employ (i) Gaussian process estimators to guide the robots'
exploration as they collect vibration samples across the surface and (ii)
operational modal analysis to detect structural damages by estimating and
comparing existing and intact structural vibration patterns. We analyze the
influence of exploration radii on estimation uncertainty and assess the
effectiveness of our method across 10 randomized scenarios, where the number,
locations, surface area, and depth of structural damages vary. Our simulation
studies validate the efficacy of our miniaturized robot swarm for
vibration-based structural inspection.

</details>


### [16] [On the capabilities of LLMs for classifying and segmenting time series of fruit picking motions into primitive actions](https://arxiv.org/abs/2507.07745)
*Eleni Konstantinidou,Nikolaos Kounalakis,Nikolaos Efstathopoulos,Dimitrios Papageorgiou*

Main category: cs.RO

TL;DR: 研究探讨了大型语言模型（LLMs）在水果采摘任务中分类和分割复杂动作的能力，比较了三种微调方法。


<details>
  <summary>Details</summary>
Motivation: LLMs如ChatGPT已显著减轻认知负担，本研究旨在探索其在学习示范（LbD）中分类和分割动作的潜力，以简化任务编码。

Method: 使用UR10e机器人采集数据，比较三种不同的LLMs微调方法，以分类和分割水果采摘中的基本动作。

Result: 未明确提及具体结果，但目标是使方法更易应用于实际场景。

Conclusion: LLMs在动作分类和分割任务中具有潜力，可能简化实际应用中的部署。

Abstract: Despite their recent introduction to human society, Large Language Models
(LLMs) have significantly affected the way we tackle mental challenges in our
everyday lives. From optimizing our linguistic communication to assisting us in
making important decisions, LLMs, such as ChatGPT, are notably reducing our
cognitive load by gradually taking on an increasing share of our mental
activities. In the context of Learning by Demonstration (LbD), classifying and
segmenting complex motions into primitive actions, such as pushing, pulling,
twisting etc, is considered to be a key-step towards encoding a task. In this
work, we investigate the capabilities of LLMs to undertake this task,
considering a finite set of predefined primitive actions found in fruit picking
operations. By utilizing LLMs instead of simple supervised learning or analytic
methods, we aim at making the method easily applicable and deployable in a
real-life scenario. Three different fine-tuning approaches are investigated,
compared on datasets captured kinesthetically, using a UR10e robot, during a
fruit-picking scenario.

</details>


### [17] [IRAF-SLAM: An Illumination-Robust and Adaptive Feature-Culling Front-End for Visual SLAM in Challenging Environments](https://arxiv.org/abs/2507.07752)
*Thanh Nguyen Canh,Bao Nguyen Quoc,Haolan Zhang,Bupesh Rethinam Veeraiah,Xiem HoangVan,Nak Young Chong*

Main category: cs.RO

TL;DR: IRAF-SLAM是一种针对复杂光照条件下视觉SLAM的鲁棒性改进方法，通过图像增强、自适应特征提取和特征筛选策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM系统在动态物体、低纹理和光照变化下性能下降，需要一种自适应前端以增强鲁棒性。

Method: 提出图像增强方案、自适应特征提取机制和特征筛选策略，动态调整参数以适应光照变化。

Result: 在TUM-VI和EuRoC数据集上，IRAF-SLAM显著减少跟踪失败并提高轨迹精度。

Conclusion: 自适应前端策略能有效提升vSLAM鲁棒性，且计算开销低。

Abstract: Robust Visual SLAM (vSLAM) is essential for autonomous systems operating in
real-world environments, where challenges such as dynamic objects, low texture,
and critically, varying illumination conditions often degrade performance.
Existing feature-based SLAM systems rely on fixed front-end parameters, making
them vulnerable to sudden lighting changes and unstable feature tracking. To
address these challenges, we propose ``IRAF-SLAM'', an Illumination-Robust and
Adaptive Feature-Culling front-end designed to enhance vSLAM resilience in
complex and challenging environments. Our approach introduces: (1) an image
enhancement scheme to preprocess and adjust image quality under varying
lighting conditions; (2) an adaptive feature extraction mechanism that
dynamically adjusts detection sensitivity based on image entropy, pixel
intensity, and gradient analysis; and (3) a feature culling strategy that
filters out unreliable feature points using density distribution analysis and a
lighting impact factor. Comprehensive evaluations on the TUM-VI and European
Robotics Challenge (EuRoC) datasets demonstrate that IRAF-SLAM significantly
reduces tracking failures and achieves superior trajectory accuracy compared to
state-of-the-art vSLAM methods under adverse illumination conditions. These
results highlight the effectiveness of adaptive front-end strategies in
improving vSLAM robustness without incurring significant computational
overhead. The implementation of IRAF-SLAM is publicly available at
https://thanhnguyencanh. github.io/IRAF-SLAM/.

</details>


### [18] [Collaborative Human-Robot Surgery for Mandibular Angle Split Osteotomy: Optical Tracking based Approach](https://arxiv.org/abs/2507.07794)
*Zhe Han,Huanyu Tian,Tom Vercauteren,Da Liu,Changsheng Li,Xingguang Duan*

Main category: cs.RO

TL;DR: 提出了一种人机协作系统用于下颌角劈开截骨术（MASO），通过任务分解和光学跟踪系统提高手术精度和安全性。


<details>
  <summary>Details</summary>
Motivation: 尽管技术和器械有所进步，MASO的成功仍高度依赖外科医生的经验，因此需要一种更精确和安全的方法。

Method: 将手术任务分解为机器人主导的位置和方向控制，以及外科医生主导的力控制；使用光学跟踪系统实现患者追踪。

Result: 在模型实验中，计划与实际钻孔点的平均误差为1.85毫米。

Conclusion: 该系统能够有效提高MASO的精确性和安全性，减少对医生经验的依赖。

Abstract: Mandibular Angle Split Osteotomy (MASO) is a significant procedure in oral
and maxillofacial surgery. Despite advances in technique and instrumentation,
its success still relies heavily on the surgeon's experience. In this work, a
human-robot collaborative system is proposed to perform MASO according to a
preoperative plan and under guidance of a surgeon. A task decomposition
methodology is used to divide the collaborative surgical procedure into three
subtasks: (1) positional control and (2) orientation control, both led by the
robot for precise alignment; and (3) force-control, managed by surgeon to
ensure safety. Additionally, to achieve patient tracking without the need for a
skull clamp, an optical tracking system (OTS) is utilized. Movement of the
patient mandibular is measured with an optical-based tracker mounted on a
dental occlusal splint. A registration method and Robot-OTS calibration method
are introduced to achieve reliable navigation within our framework. The
experiments of drilling were conducted on the realistic phantom model, which
demonstrated that the average error between the planned and actual drilling
points is 1.85mm.

</details>


### [19] [Beyond Robustness: Learning Unknown Dynamic Load Adaptation for Quadruped Locomotion on Rough Terrain](https://arxiv.org/abs/2507.07825)
*Leixin Chang,Yuxuan Nai,Hua Chen,Liangjing Yang*

Main category: cs.RO

TL;DR: 该论文提出了一种通用的负载特性建模方法，结合强化学习控制，使四足机器人能够推断负载动态并间接稳定负载，验证了其在真实场景中的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决四足机器人在未知动态负载下的三大挑战：负载动态建模、无外部感知的动态捕获以及负载交互与稳定。

Method: 提出负载特性建模方法，结合强化学习控制技术，实现负载动态推断与间接交互。

Result: 仿真实验表明，该方法在突发负载抵抗、负载稳定及复杂地形重载运动方面优于其他方法。

Conclusion: 该方法有效解决了四足机器人在未知动态负载下的控制问题，并验证了其实际应用潜力。

Abstract: Unknown dynamic load carrying is one important practical application for
quadruped robots. Such a problem is non-trivial, posing three major challenges
in quadruped locomotion control. First, how to model or represent the dynamics
of the load in a generic manner. Second, how to make the robot capture the
dynamics without any external sensing. Third, how to enable the robot to
interact with load handling the mutual effect and stabilizing the load. In this
work, we propose a general load modeling approach called load characteristics
modeling to capture the dynamics of the load. We integrate this proposed
modeling technique and leverage recent advances in Reinforcement Learning (RL)
based locomotion control to enable the robot to infer the dynamics of load
movement and interact with the load indirectly to stabilize it and realize the
sim-to-real deployment to verify its effectiveness in real scenarios. We
conduct extensive comparative simulation experiments to validate the
effectiveness and superiority of our proposed method. Results show that our
method outperforms other methods in sudden load resistance, load stabilizing
and locomotion with heavy load on rough terrain.
\href{https://leixinjonaschang.github.io/leggedloadadapt.github.io/}{Project
Page}.

</details>


### [20] [Perceptual Distortions and Autonomous Representation Learning in a Minimal Robotic System](https://arxiv.org/abs/2507.07845)
*David Warutumo,Ciira wa Maina*

Main category: cs.RO

TL;DR: 论文研究了感知失真如何影响自主机器人的表示学习，通过模拟实验展示了机器人如何在感知失真下自主构建导航表示。


<details>
  <summary>Details</summary>
Motivation: 探索感知失真对自主机器人内部世界表示的影响，以及机器人如何在不依赖显式空间信息的情况下学习导航。

Method: 使用配备距离传感器和指南针的模拟两轮机器人，在方形环境中随机探索，分析其感知数据。

Result: 发现感知失真下仍能形成与物理环境相关的结构，机器人自主学习了导航表示。

Conclusion: 研究为具身认知、最小代理和感知在人工生命导航策略中的作用提供了新见解。

Abstract: Autonomous agents, particularly in the field of robotics, rely on sensory
information to perceive and navigate their environment. However, these sensory
inputs are often imperfect, leading to distortions in the agent's internal
representation of the world. This paper investigates the nature of these
perceptual distortions and how they influence autonomous representation
learning using a minimal robotic system. We utilize a simulated two-wheeled
robot equipped with distance sensors and a compass, operating within a simple
square environment. Through analysis of the robot's sensor data during random
exploration, we demonstrate how a distorted perceptual space emerges. Despite
these distortions, we identify emergent structures within the perceptual space
that correlate with the physical environment, revealing how the robot
autonomously learns a structured representation for navigation without explicit
spatial information. This work contributes to the understanding of embodied
cognition, minimal agency, and the role of perception in self-generated
navigation strategies in artificial life.

</details>


### [21] [ROS Help Desk: GenAI Powered, User-Centric Framework for ROS Error Diagnosis and Debugging](https://arxiv.org/abs/2507.07846)
*Kavindie Katuwandeniya,Samith Rajapaksha Jayasekara Widhanapathirana*

Main category: cs.RO

TL;DR: ROS Help Desk通过直观的错误解释和调试支持，降低了ROS系统的维护时间，提升了人机协作效率。


<details>
  <summary>Details</summary>
Motivation: 随着机器人系统融入日常生活，ROS的复杂架构和技术消息系统为普通用户带来了理解和诊断错误的障碍，导致维护停机时间延长。

Method: ROS Help Desk提供用户中心的调试工具，实现主动错误检测，并整合多模态数据处理以全面理解系统状态。

Result: 测试表明，系统能主动且准确地诊断问题，显著减少维护时间。

Conclusion: ROS Help Desk有效解决了ROS系统的用户友好性问题，提升了人机协作的效率和可靠性。

Abstract: As the robotics systems increasingly integrate into daily life, from smart
home assistants to the new-wave of industrial automation systems (Industry
4.0), there's an increasing need to bridge the gap between complex robotic
systems and everyday users. The Robot Operating System (ROS) is a flexible
framework often utilised in writing robot software, providing tools and
libraries for building complex robotic systems. However, ROS's distributed
architecture and technical messaging system create barriers for understanding
robot status and diagnosing errors. This gap can lead to extended maintenance
downtimes, as users with limited ROS knowledge may struggle to quickly diagnose
and resolve system issues. Moreover, this deficit in expertise often delays
proactive maintenance and troubleshooting, further increasing the frequency and
duration of system interruptions. ROS Help Desk provides intuitive error
explanations and debugging support, dynamically customized to users of varying
expertise levels. It features user-centric debugging tools that simplify error
diagnosis, implements proactive error detection capabilities to reduce
downtime, and integrates multimodal data processing for comprehensive system
state understanding across multi-sensor data (e.g., lidar, RGB). Testing
qualitatively and quantitatively with artificially induced errors demonstrates
the system's ability to proactively and accurately diagnose problems,
ultimately reducing maintenance time and fostering more effective human-robot
collaboration.

</details>


### [22] [Improving AEBS Validation Through Objective Intervention Classification Leveraging the Prediction Divergence Principle](https://arxiv.org/abs/2507.07872)
*Daniel Betschinske,Steven Peters*

Main category: cs.RO

TL;DR: 提出了一种基于规则分类的方法（PDP）用于区分AEBS的误报和真报，结合人工标注提高验证过程的透明度和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决在开放循环重模拟中区分AEBS误报和真报的复杂性，避免人工标注的主观性和偏差。

Method: 采用基于预测分歧原则（PDP）的规则分类方法，应用于简化的AEBS系统。

Result: 揭示了该方法的优势、局限性和系统需求，表明其与人工标注结合可提升验证过程的透明度和一致性。

Conclusion: 该方法为AEBS验证提供了更可靠和可重复的框架，并提出了未来改进方向。

Abstract: The safety validation of automatic emergency braking system (AEBS) requires
accurately distinguishing between false positive (FP) and true positive (TP)
system activations. While simulations allow straightforward differentiation by
comparing scenarios with and without interventions, analyzing activations from
open-loop resimulations - such as those from field operational testing (FOT) -
is more complex. This complexity arises from scenario parameter uncertainty and
the influence of driver interventions in the recorded data. Human labeling is
frequently used to address these challenges, relying on subjective assessments
of intervention necessity or situational criticality, potentially introducing
biases and limitations. This work proposes a rule-based classification approach
leveraging the Prediction Divergence Principle (PDP) to address those issues.
Applied to a simplified AEBS, the proposed method reveals key strengths,
limitations, and system requirements for effective implementation. The findings
suggest that combining this approach with human labeling may enhance the
transparency and consistency of classification, thereby improving the overall
validation process. While the rule set for classification derived in this work
adopts a conservative approach, the paper outlines future directions for
refinement and broader applicability. Finally, this work highlights the
potential of such methods to complement existing practices, paving the way for
more reliable and reproducible AEBS validation frameworks.

</details>


### [23] [UniTac: Whole-Robot Touch Sensing Without Tactile Sensors](https://arxiv.org/abs/2507.07980)
*Wanjia Fu,Hongyu Li,Ivy X. He,Stefanie Tellex,Srinath Sridhar*

Main category: cs.RO

TL;DR: UniTac是一种数据驱动的全身触觉感知方法，仅使用本体感觉关节传感器，无需额外传感器即可实现接触定位。


<details>
  <summary>Details</summary>
Motivation: 商业机器人通常缺乏触觉皮肤，难以实现基本触觉功能，如接触定位。UniTac旨在普及触觉感知，为HRI研究人员提供现成工具。

Method: 利用数据驱动方法，仅依赖关节传感器实现接触定位，无需安装额外传感器。

Result: 在Franka机械臂上定位精度为8.0厘米，Spot四足机器人上为7.2厘米，频率达2000Hz。

Conclusion: UniTac为机器人提供了一种低成本、高效的触觉感知解决方案，适用于多种平台。

Abstract: Robots can better interact with humans and unstructured environments through
touch sensing. However, most commercial robots are not equipped with tactile
skins, making it challenging to achieve even basic touch-sensing functions,
such as contact localization. We present UniTac, a data-driven whole-body
touch-sensing approach that uses only proprioceptive joint sensors and does not
require the installation of additional sensors. Our approach enables a robot
equipped solely with joint sensors to localize contacts. Our goal is to
democratize touch sensing and provide an off-the-shelf tool for HRI researchers
to provide their robots with touch-sensing capabilities. We validate our
approach on two platforms: the Franka robot arm and the Spot quadruped. On
Franka, we can localize contact to within 8.0 centimeters, and on Spot, we can
localize to within 7.2 centimeters at around 2,000 Hz on an RTX 3090 GPU
without adding any additional sensors to the robot. Project website:
https://ivl.cs.brown.edu/research/unitac.

</details>
