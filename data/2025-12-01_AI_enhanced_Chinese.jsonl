{"id": "2511.21886", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21886", "abs": "https://arxiv.org/abs/2511.21886", "authors": ["Jingtian Yan", "Shuai Zhou", "Stephen F. Smith", "Jiaoyang Li"], "title": "Bridging Planning and Execution: Multi-Agent Path Finding Under Real-World Deadlines", "comment": null, "summary": "The Multi-Agent Path Finding (MAPF) problem aims to find collision-free paths for multiple agents while optimizing objectives such as the sum of costs or makespan. MAPF has wide applications in domains like automated warehouses, manufacturing systems, and airport logistics. However, most MAPF formulations assume a simplified robot model for planning, which overlooks execution-time factors such as kinodynamic constraints, communication latency, and controller variability. This gap between planning and execution is problematic for time-sensitive applications. To bridge this gap, we propose REMAP, an execution-informed MAPF planning framework that can be combined with leading search-based MAPF planners with minor changes. Our framework integrates the proposed ExecTimeNet to accurately estimate execution time based on planned paths. We demonstrate our method for solving MAPF with Real-world Deadlines (MAPF-RD) problem, where agents must reach their goals before a predefined wall-clock time. We integrate our framework with two popular MAPF methods, MAPF-LNS and CBS. Experiments show that REMAP achieves up to 20% improvement in solution quality over baseline methods (e.g., constant execution speed estimators) on benchmark maps with up to 300 agents.", "AI": {"tldr": "REMAP\u662f\u4e00\u4e2a\u6267\u884c\u611f\u77e5\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7ExecTimeNet\u51c6\u786e\u4f30\u8ba1\u6267\u884c\u65f6\u95f4\uff0c\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u6709\u622a\u6b62\u65f6\u95f4\u7684MAPF\u95ee\u9898\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534720%\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edfMAPF\u95ee\u9898\u5047\u8bbe\u7b80\u5316\u7684\u673a\u5668\u4eba\u6a21\u578b\uff0c\u5ffd\u7565\u4e86\u6267\u884c\u65f6\u7684\u52a8\u529b\u5b66\u7ea6\u675f\u3001\u901a\u4fe1\u5ef6\u8fdf\u548c\u63a7\u5236\u5668\u53d8\u5316\u7b49\u56e0\u7d20\uff0c\u5bfc\u81f4\u89c4\u5212\u4e0e\u6267\u884c\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u8fd9\u5bf9\u65f6\u95f4\u654f\u611f\u7684\u5e94\u7528\u6765\u8bf4\u662f\u6709\u95ee\u9898\u7684\u3002", "method": "\u63d0\u51faREMAP\u6267\u884c\u611f\u77e5MAPF\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408ExecTimeNet\u51c6\u786e\u4f30\u8ba1\u57fa\u4e8e\u89c4\u5212\u8def\u5f84\u7684\u6267\u884c\u65f6\u95f4\uff0c\u53ef\u96c6\u6210\u5230\u4e3b\u6d41\u641c\u7d22\u5f0fMAPF\u89c4\u5212\u5668\u4e2d\uff0c\u7528\u4e8e\u89e3\u51b3\u6709\u73b0\u5b9e\u4e16\u754c\u622a\u6b62\u65f6\u95f4\u7684MAPF\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cREMAP\u5728\u6700\u591a300\u4e2a\u667a\u80fd\u4f53\u7684\u57fa\u51c6\u5730\u56fe\u4e0a\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982\u6052\u5b9a\u6267\u884c\u901f\u5ea6\u4f30\u8ba1\u5668\uff09\u5728\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u4e0a\u63d0\u5347\u9ad8\u8fbe20%\u3002", "conclusion": "REMAP\u6846\u67b6\u6709\u6548\u5f25\u5408\u4e86MAPF\u89c4\u5212\u4e0e\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u51c6\u786e\u4f30\u8ba1\u6267\u884c\u65f6\u95f4\uff0c\u4e3a\u65f6\u95f4\u654f\u611f\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.21925", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.21925", "abs": "https://arxiv.org/abs/2511.21925", "authors": ["Alex Richardson", "Jonathan Sprinkle"], "title": "OpenTwinMap: An Open-Source Digital Twin Generator for Urban Autonomous Driving", "comment": null, "summary": "Digital twins of urban environments play a critical role in advancing autonomous vehicle (AV) research by enabling simulation, validation, and integration with emerging generative world models. While existing tools have demonstrated value, many publicly available solutions are tightly coupled to specific simulators, difficult to extend, or introduce significant technical overhead. For example, CARLA-the most widely used open-source AV simulator-provides a digital twin framework implemented entirely as an Unreal Engine C++ plugin, limiting flexibility and rapid prototyping. In this work, we propose OpenTwinMap, an open-source, Python-based framework for generating high-fidelity 3D urban digital twins. The completed framework will ingest LiDAR scans and OpenStreetMap (OSM) data to produce semantically segmented static environment assets, including road networks, terrain, and urban structures, which can be exported into Unreal Engine for AV simulation. OpenTwinMap emphasizes extensibility and parallelization, lowering the barrier for researchers to adapt and scale the pipeline to diverse urban contexts. We describe the current capabilities of the OpenTwinMap, which includes preprocessing of OSM and LiDAR data, basic road mesh and terrain generation, and preliminary support for CARLA integration.", "AI": {"tldr": "OpenTwinMap\u662f\u4e00\u4e2a\u5f00\u6e90\u7684Python\u6846\u67b6\uff0c\u7528\u4e8e\u4eceLiDAR\u626b\u63cf\u548cOpenStreetMap\u6570\u636e\u751f\u6210\u9ad8\u4fdd\u771f3D\u57ce\u5e02\u6570\u5b57\u5b6a\u751f\uff0c\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u4eff\u771f\u3002", "motivation": "\u73b0\u6709\u6570\u5b57\u5b6a\u751f\u5de5\u5177\u901a\u5e38\u4e0e\u7279\u5b9a\u4eff\u771f\u5668\u7d27\u5bc6\u8026\u5408\u3001\u96be\u4ee5\u6269\u5c55\u6216\u6280\u672f\u5f00\u9500\u5927\u3002\u4f8b\u5982CARLA\u4f5c\u4e3a\u6700\u5e7f\u6cdb\u4f7f\u7528\u7684\u5f00\u6e90\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u5668\uff0c\u5176\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u5b8c\u5168\u4f5c\u4e3aUnreal Engine C++\u63d2\u4ef6\u5b9e\u73b0\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u5feb\u901f\u539f\u578b\u5f00\u53d1\u3002", "method": "\u63d0\u51faOpenTwinMap\u6846\u67b6\uff0c\u57fa\u4e8ePython\u5f00\u53d1\uff0c\u5904\u7406LiDAR\u626b\u63cf\u548cOpenStreetMap\u6570\u636e\uff0c\u751f\u6210\u8bed\u4e49\u5206\u5272\u7684\u9759\u6001\u73af\u5883\u8d44\u4ea7\uff08\u5305\u62ec\u9053\u8def\u7f51\u7edc\u3001\u5730\u5f62\u548c\u57ce\u5e02\u7ed3\u6784\uff09\uff0c\u53ef\u5bfc\u51fa\u5230Unreal Engine\u8fdb\u884c\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u3002\u5f3a\u8c03\u53ef\u6269\u5c55\u6027\u548c\u5e76\u884c\u5316\u3002", "result": "\u5f53\u524d\u6846\u67b6\u5df2\u5177\u5907OSM\u548cLiDAR\u6570\u636e\u9884\u5904\u7406\u3001\u57fa\u672c\u9053\u8def\u7f51\u683c\u548c\u5730\u5f62\u751f\u6210\u529f\u80fd\uff0c\u5e76\u521d\u6b65\u652f\u6301CARLA\u96c6\u6210\u3002", "conclusion": "OpenTwinMap\u901a\u8fc7\u964d\u4f4e\u6280\u672f\u95e8\u69db\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u66f4\u7075\u6d3b\u5730\u9002\u5e94\u548c\u6269\u5c55\u6570\u5b57\u5b6a\u751f\u7ba1\u9053\u5230\u4e0d\u540c\u7684\u57ce\u5e02\u73af\u5883\uff0c\u4fc3\u8fdb\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u3002"}}
{"id": "2511.21957", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.21957", "abs": "https://arxiv.org/abs/2511.21957", "authors": ["Cahit Ikbal Er", "Amin Kashiri", "Yasin Yazicioglu"], "title": "RSPECT: Robust and Scalable Planner for Energy-Aware Coordination of UAV-UGV Teams in Aerial Monitoring", "comment": null, "summary": "We consider the robust planning of energy-constrained unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs), which act as mobile charging stations, to perform long-horizon aerial monitoring missions. More specifically, given a set of points to be visited by the UAVs and desired final positions of the UAV-UGV teams, the objective is to find a robust plan (the vehicle trajectories) that can be realized without a major revision in the face of uncertainty (e.g., unknown obstacles/terrain, wind) to complete this mission in minimum time. We provide a formal description of this problem as a mixed-integer program (MIP), which is NP-hard. Since exact solution methods are computationally intractable for such problems, we propose RSPECT, a scalable and efficient heuristic. We provide theoretical results on the complexity of our algorithm and the feasibility and robustness of resulting plans. We also demonstrate the performance of our method via simulations and experiments.", "AI": {"tldr": "\u63d0\u51faRSPECT\u7b97\u6cd5\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u4e0e\u5730\u9762\u5145\u7535\u8f66\u534f\u540c\u7684\u9c81\u68d2\u8def\u5f84\u89c4\u5212\uff0c\u4ee5\u6700\u5c0f\u5316\u65f6\u95f4\u5b8c\u6210\u957f\u671f\u7a7a\u4e2d\u76d1\u6d4b\u4efb\u52a1", "motivation": "\u65e0\u4eba\u673a\u5728\u6267\u884c\u957f\u671f\u7a7a\u4e2d\u76d1\u6d4b\u4efb\u52a1\u65f6\u9762\u4e34\u80fd\u91cf\u9650\u5236\uff0c\u9700\u8981\u5730\u9762\u5145\u7535\u8f66\u534f\u540c\uff0c\u4f46\u5728\u4e0d\u786e\u5b9a\u73af\u5883\uff08\u969c\u788d\u7269\u3001\u5730\u5f62\u3001\u98ce\u7b49\uff09\u4e0b\u9700\u8981\u9c81\u68d2\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6848", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u6df7\u5408\u6574\u6570\u89c4\u5212\uff08MIP\uff09\uff0c\u63d0\u51faRSPECT\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u63d0\u4f9b\u7406\u8bba\u590d\u6742\u5ea6\u5206\u6790\u548c\u9c81\u68d2\u6027\u4fdd\u8bc1", "result": "\u7b97\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u9ad8\u6548\u6027\uff0c\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6027\u80fd\uff0c\u7406\u8bba\u5206\u6790\u4e86\u7b97\u6cd5\u590d\u6742\u5ea6\u548c\u89c4\u5212\u65b9\u6848\u7684\u53ef\u884c\u6027\u3001\u9c81\u68d2\u6027", "conclusion": "RSPECT\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u65e0\u4eba\u673a-\u5730\u9762\u5145\u7535\u8f66\u534f\u540c\u7684\u9c81\u68d2\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u5b9e\u73b0\u6700\u5c0f\u65f6\u95f4\u5b8c\u6210\u76d1\u6d4b\u4efb\u52a1"}}
{"id": "2511.22042", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22042", "abs": "https://arxiv.org/abs/2511.22042", "authors": ["Lei Li", "Jiale Gong", "Ziyang Li", "Hong Wang"], "title": "Constant-Volume Deformation Manufacturing for Material-Efficient Shaping", "comment": "46 pages, 27 figures", "summary": "Additive and subtractive manufacturing enable complex geometries but rely on discrete stacking or local removal, limiting continuous and controllable deformation and causing volume loss and shape deviations. We present a volumepreserving digital-mold paradigm that integrates real-time volume-consistency modeling with geometry-informed deformation prediction and an error-compensation strategy to achieve highly predictable shaping of plastic materials. By analyzing deformation patterns and error trends from post-formed point clouds, our method corrects elastic rebound and accumulation errors, maintaining volume consistency and surface continuity. Experiments on five representative geometries demonstrate that the system reproduces target shapes with high fidelity while achieving over 98% material utilization. This approach establishes a digitally driven, reproducible pathway for sustainable, zero-waste shaping of user-defined designs, bridging digital modeling, real-time sensing, and adaptive forming, and advancing next-generation sustainable and customizable manufacturing.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f53\u79ef\u4fdd\u6301\u7684\u6570\u5b57\u6a21\u5177\u8303\u5f0f\uff0c\u901a\u8fc7\u5b9e\u65f6\u4f53\u79ef\u4e00\u81f4\u6027\u5efa\u6a21\u3001\u51e0\u4f55\u611f\u77e5\u53d8\u5f62\u9884\u6d4b\u548c\u8bef\u5dee\u8865\u507f\u7b56\u7565\uff0c\u5b9e\u73b0\u5851\u6599\u6750\u6599\u7684\u9ad8\u7cbe\u5ea6\u53ef\u9884\u6d4b\u6210\u5f62\uff0c\u8fbe\u523098%\u4ee5\u4e0a\u7684\u6750\u6599\u5229\u7528\u7387\u3002", "motivation": "\u4f20\u7edf\u589e\u6750\u548c\u51cf\u6750\u5236\u9020\u4f9d\u8d56\u79bb\u6563\u5806\u53e0\u6216\u5c40\u90e8\u53bb\u9664\uff0c\u5b58\u5728\u4f53\u79ef\u635f\u5931\u548c\u5f62\u72b6\u504f\u5dee\u95ee\u9898\uff0c\u9650\u5236\u4e86\u8fde\u7eed\u53ef\u63a7\u53d8\u5f62\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4fdd\u6301\u4f53\u79ef\u4e00\u81f4\u6027\u3001\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6210\u5f62\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u96c6\u6210\u5b9e\u65f6\u4f53\u79ef\u4e00\u81f4\u6027\u5efa\u6a21\u3001\u51e0\u4f55\u611f\u77e5\u53d8\u5f62\u9884\u6d4b\u548c\u8bef\u5dee\u8865\u507f\u7b56\u7565\u3002\u901a\u8fc7\u5206\u6790\u6210\u5f62\u540e\u70b9\u4e91\u7684\u53d8\u5f62\u6a21\u5f0f\u548c\u8bef\u5dee\u8d8b\u52bf\uff0c\u6821\u6b63\u5f39\u6027\u56de\u5f39\u548c\u7d2f\u79ef\u8bef\u5dee\uff0c\u4fdd\u6301\u4f53\u79ef\u4e00\u81f4\u6027\u548c\u8868\u9762\u8fde\u7eed\u6027\u3002", "result": "\u5728\u4e94\u79cd\u4ee3\u8868\u6027\u51e0\u4f55\u5f62\u72b6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7cfb\u7edf\u80fd\u591f\u9ad8\u4fdd\u771f\u5730\u518d\u73b0\u76ee\u6807\u5f62\u72b6\uff0c\u540c\u65f6\u5b9e\u73b0\u8d85\u8fc798%\u7684\u6750\u6599\u5229\u7528\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5efa\u7acb\u4e86\u6570\u5b57\u9a71\u52a8\u3001\u53ef\u91cd\u590d\u7684\u53ef\u6301\u7eed\u96f6\u5e9f\u6599\u6210\u5f62\u8def\u5f84\uff0c\u8fde\u63a5\u6570\u5b57\u5efa\u6a21\u3001\u5b9e\u65f6\u4f20\u611f\u548c\u81ea\u9002\u5e94\u6210\u5f62\uff0c\u63a8\u52a8\u4e0b\u4e00\u4ee3\u53ef\u6301\u7eed\u548c\u53ef\u5b9a\u5236\u5236\u9020\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.22043", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.22043", "abs": "https://arxiv.org/abs/2511.22043", "authors": ["Xuchen Liu", "Ruocheng Li", "Bin Xin", "Weijia Yao", "Qigeng Duan", "Jinqiang Cui", "Ben M. Chen", "Jie Chen"], "title": "SwordRiding: A Unified Navigation Framework for Quadrotors in Unknown Complex Environments via Online Guiding Vector Fields", "comment": "For an experimental demo, see https://www.youtube.com/watch?v=tKYCg266c4o. For the lemma proof, see https://github.com/SmartGroupSystems/GVF_close_loop_planning/blob/main/proofs.md", "summary": "Although quadrotor navigation has achieved high performance in trajectory planning and control, real-time adaptability in unknown complex environments remains a core challenge. This difficulty mainly arises because most existing planning frameworks operate in an open-loop manner, making it hard to cope with environmental uncertainties such as wind disturbances or external perturbations. This paper presents a unified real-time navigation framework for quadrotors in unknown complex environments, based on the online construction of guiding vector fields (GVFs) from discrete reference path points. In the framework, onboard perception modules build a Euclidean Signed Distance Field (ESDF) representation of the environment, which enables obstacle awareness and path distance evaluation. The system first generates discrete, collision-free path points using a global planner, and then parameterizes them via uniform B-splines to produce a smooth and physically feasible reference trajectory. An adaptive GVF is then synthesized from the ESDF and the optimized B-spline trajectory. Unlike conventional approaches, the method adopts a closed-loop navigation paradigm, which significantly enhances robustness under external disturbances. Compared with conventional GVF methods, the proposed approach directly accommodates discretized paths and maintains compatibility with standard planning algorithms. Extensive simulations and real-world experiments demonstrate improved robustness against external disturbances and superior real-time performance.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f15\u5bfc\u5411\u91cf\u573a\u7684\u56db\u65cb\u7ffc\u5b9e\u65f6\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u6784\u5efaGVF\u5b9e\u73b0\u672a\u77e5\u590d\u6742\u73af\u5883\u4e2d\u7684\u95ed\u73af\u5bfc\u822a\uff0c\u63d0\u9ad8\u5bf9\u5916\u90e8\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u56db\u65cb\u7ffc\u5bfc\u822a\u6846\u67b6\u591a\u4e3a\u5f00\u73af\u64cd\u4f5c\uff0c\u96be\u4ee5\u5e94\u5bf9\u73af\u5883\u4e0d\u786e\u5b9a\u6027\uff08\u5982\u98ce\u6270\u52a8\u3001\u5916\u90e8\u5e72\u6270\uff09\uff0c\u5728\u672a\u77e5\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u65f6\u9002\u5e94\u6027\u4e0d\u8db3\u3002", "method": "1) \u673a\u8f7d\u611f\u77e5\u6a21\u5757\u6784\u5efaESDF\u73af\u5883\u8868\u793a\uff1b2) \u5168\u5c40\u89c4\u5212\u5668\u751f\u6210\u79bb\u6563\u65e0\u78b0\u649e\u8def\u5f84\u70b9\uff1b3) \u5747\u5300B\u6837\u6761\u53c2\u6570\u5316\u751f\u6210\u5e73\u6ed1\u53c2\u8003\u8f68\u8ff9\uff1b4) \u4eceESDF\u548c\u4f18\u5316B\u6837\u6761\u8f68\u8ff9\u5408\u6210\u81ea\u9002\u5e94GVF\uff1b5) \u91c7\u7528\u95ed\u73af\u5bfc\u822a\u8303\u5f0f\u3002", "result": "\u5927\u91cf\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u5916\u90e8\u6270\u52a8\u9c81\u68d2\u6027\u548c\u4f18\u8d8a\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u5b9e\u65f6\u5bfc\u822a\u6846\u67b6\u901a\u8fc7\u5728\u7ebf\u6784\u5efaGVF\u5b9e\u73b0\u95ed\u73af\u5bfc\u822a\uff0c\u80fd\u76f4\u63a5\u5904\u7406\u79bb\u6563\u5316\u8def\u5f84\uff0c\u4e0e\u6807\u51c6\u89c4\u5212\u7b97\u6cd5\u517c\u5bb9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56db\u65cb\u7ffc\u5728\u672a\u77e5\u590d\u6742\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.22087", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22087", "abs": "https://arxiv.org/abs/2511.22087", "authors": ["Tai Inui", "Jee-Hwan Ryu"], "title": "SoftNash: Entropy-Regularized Nash Games for Non-Fighting Virtual Fixtures", "comment": null, "summary": "Virtual fixtures (VFs) improve precision in teleoperation but often ``fight'' the user, inflating mental workload and eroding the sense of agency. We propose Soft-Nash Virtual Fixtures, a game-theoretic shared-control policy that softens the classic two-player linear-quadratic (LQ) Nash solution by inflating the fixture's effort weight with a single, interpretable scalar parameter $\u03c4$. This yields a continuous dial on controller assertiveness: $\u03c4=0$ recovers a hard, performance-focused Nash / virtual fixture controller, while larger $\u03c4$ reduce gains and pushback, yet preserve the equilibrium structure and continuity of closed-loop stability. We derive Soft-Nash from both a KL-regularized trust-region and a maximum-entropy viewpoint, obtaining a closed-form robot best response that shrinks authority and aligns the fixture with the operator's input as $\u03c4$ grows. We implement Soft-Nash on a 6-DoF haptic device in 3D tracking task ($n=12$). Moderate softness ($\u03c4\\approx 1-3$, especially $\u03c4=2$) maintains tracking error statistically indistinguishable from a tuned classic VF while sharply reducing controller-user conflict, lowering NASA-TLX workload, and increasing Sense of Agency (SoAS). A composite BalancedScore that combines normalized accuracy and non-fighting behavior peaks near $\u03c4=2-3$. These results show that a one-parameter Soft-Nash policy can preserve accuracy while improving comfort and perceived agency, providing a practical and interpretable pathway to personalized shared control in haptics and teleoperation.", "AI": {"tldr": "\u63d0\u51faSoft-Nash\u865a\u62df\u5939\u5177\uff0c\u901a\u8fc7\u5355\u4e00\u53ef\u89e3\u91ca\u53c2\u6570\u03c4\u8f6f\u5316\u4f20\u7edf\u865a\u62df\u5939\u5177\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u51cf\u5c11\u7528\u6237\u51b2\u7a81\u548c\u5de5\u4f5c\u8d1f\u8377", "motivation": "\u4f20\u7edf\u865a\u62df\u5939\u5177\u867d\u7136\u80fd\u63d0\u9ad8\u8fdc\u7a0b\u64cd\u4f5c\u7cbe\u5ea6\uff0c\u4f46\u7ecf\u5e38\u4e0e\u7528\u6237\"\u5bf9\u6297\"\uff0c\u589e\u52a0\u5fc3\u7406\u8d1f\u8377\u5e76\u524a\u5f31\u4ee3\u7406\u611f", "method": "\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u5171\u4eab\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u53c2\u6570\u03c4\u81a8\u80c0\u5939\u5177\u7684\u52aa\u529b\u6743\u91cd\uff0c\u4eceKL\u6b63\u5219\u5316\u4fe1\u4efb\u533a\u57df\u548c\u6700\u5927\u71b5\u89c6\u89d2\u63a8\u5bfc\u51fa\u5c01\u95ed\u5f62\u5f0f\u7684\u673a\u5668\u4eba\u6700\u4f73\u54cd\u5e94", "result": "\u57286\u81ea\u7531\u5ea6\u89e6\u89c9\u8bbe\u5907\u4e0a\u76843D\u8ddf\u8e2a\u4efb\u52a1\u4e2d\uff0c\u4e2d\u7b49\u8f6f\u5316\u7a0b\u5ea6\uff08\u03c4\u22481-3\uff09\u4fdd\u6301\u8ddf\u8e2a\u8bef\u5dee\u4e0e\u4f20\u7edf\u865a\u62df\u5939\u5177\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u51b2\u7a81\u3001\u964d\u4f4e\u5de5\u4f5c\u8d1f\u8377\u3001\u63d0\u9ad8\u4ee3\u7406\u611f", "conclusion": "\u5355\u53c2\u6570Soft-Nash\u7b56\u7565\u80fd\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u9ad8\u8212\u9002\u5ea6\u548c\u611f\u77e5\u4ee3\u7406\u611f\uff0c\u4e3a\u89e6\u89c9\u548c\u8fdc\u7a0b\u64cd\u4f5c\u4e2d\u7684\u4e2a\u6027\u5316\u5171\u4eab\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u53ef\u89e3\u91ca\u7684\u9014\u5f84"}}
{"id": "2511.22100", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22100", "abs": "https://arxiv.org/abs/2511.22100", "authors": ["Zelong Zhou", "Wenrui Chen", "Zeyun Hu", "Qiang Diao", "Qixin Gao", "Yaonan Wang"], "title": "Design of an Adaptive Modular Anthropomorphic Dexterous Hand for Human-like Manipulation", "comment": "7 pages, 8 figures", "summary": "Biological synergies have emerged as a widely adopted paradigm for dexterous hand design, enabling human-like manipulation with a small number of actuators. Nonetheless, excessive coupling tends to diminish the dexterity of hands. This paper tackles the trade-off between actuation complexity and dexterity by proposing an anthropomorphic finger topology with 4 DoFs driven by 2 actuators, and by developing an adaptive, modular dexterous hand based on this finger topology. We explore the biological basis of hand synergies and human gesture analysis, translating joint-level coordination and structural attributes into a modular finger architecture. Leveraging these biomimetic mappings, we design a five-finger modular hand and establish its kinematic model to analyze adaptive grasping and in-hand manipulation. Finally, we construct a physical prototype and conduct preliminary experiments, which validate the effectiveness of the proposed design and analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4eff\u751f\u6a21\u5757\u5316\u7075\u5de7\u624b\u8bbe\u8ba1\uff0c\u901a\u8fc72\u4e2a\u9a71\u52a8\u5668\u63a7\u52364\u81ea\u7531\u5ea6\u7684\u4eff\u4eba\u624b\u6307\u62d3\u6251\u7ed3\u6784\uff0c\u5e73\u8861\u9a71\u52a8\u590d\u6742\u6027\u4e0e\u7075\u5de7\u6027", "motivation": "\u751f\u7269\u534f\u540c\u4f5c\u7528\u5df2\u6210\u4e3a\u7075\u5de7\u624b\u8bbe\u8ba1\u7684\u5e7f\u6cdb\u91c7\u7528\u8303\u5f0f\uff0c\u4f46\u8fc7\u5ea6\u8026\u5408\u4f1a\u964d\u4f4e\u624b\u7684\u7075\u5de7\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u9a71\u52a8\u590d\u6742\u6027\u4e0e\u7075\u5de7\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u75312\u4e2a\u9a71\u52a8\u5668\u9a71\u52a8\u76844\u81ea\u7531\u5ea6\u4eff\u4eba\u624b\u6307\u62d3\u6251\u7ed3\u6784\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u81ea\u9002\u5e94\u6a21\u5757\u5316\u7075\u5de7\u624b\u3002\u63a2\u7d22\u624b\u90e8\u534f\u540c\u4f5c\u7528\u7684\u751f\u7269\u5b66\u57fa\u7840\uff0c\u5c06\u5173\u8282\u7ea7\u534f\u8c03\u548c\u7ed3\u6784\u5c5e\u6027\u8f6c\u5316\u4e3a\u6a21\u5757\u5316\u624b\u6307\u67b6\u6784\uff0c\u8bbe\u8ba1\u4e94\u6307\u6a21\u5757\u5316\u624b\u5e76\u5efa\u7acb\u5176\u8fd0\u52a8\u5b66\u6a21\u578b\u3002", "result": "\u6784\u5efa\u7269\u7406\u539f\u578b\u5e76\u8fdb\u884c\u521d\u6b65\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u8bbe\u8ba1\u548c\u5206\u6790\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u4eff\u751f\u6620\u5c04\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u51cf\u5c11\u9a71\u52a8\u5668\u6570\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u7075\u5de7\u6027\u7684\u76ee\u6807\uff0c\u4e3a\u7075\u5de7\u624b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.22195", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22195", "abs": "https://arxiv.org/abs/2511.22195", "authors": ["Zhiyang Liu", "Ruiteng Zhao", "Lei Zhou", "Chengran Yuan", "Yuwei Wu", "Sheng Guo", "Zhengshen Zhang", "Chenchen Liu", "Marcelo H Ang"], "title": "3D Affordance Keypoint Detection for Robotic Manipulation", "comment": "Accepted to IROS 2024", "summary": "This paper presents a novel approach for affordance-informed robotic manipulation by introducing 3D keypoints to enhance the understanding of object parts' functionality. The proposed approach provides direct information about what the potential use of objects is, as well as guidance on where and how a manipulator should engage, whereas conventional methods treat affordance detection as a semantic segmentation task, focusing solely on answering the what question. To address this gap, we propose a Fusion-based Affordance Keypoint Network (FAKP-Net) by introducing 3D keypoint quadruplet that harnesses the synergistic potential of RGB and Depth image to provide information on execution position, direction, and extent. Benchmark testing demonstrates that FAKP-Net outperforms existing models by significant margins in affordance segmentation task and keypoint detection task. Real-world experiments also showcase the reliability of our method in accomplishing manipulation tasks with previously unseen objects.", "AI": {"tldr": "\u63d0\u51faFAKP-Net\uff0c\u901a\u8fc73D\u5173\u952e\u70b9\u56db\u5143\u7ec4\u589e\u5f3a\u673a\u5668\u4eba\u5bf9\u7269\u4f53\u529f\u80fd\u90e8\u5206\u7684\u7406\u89e3\uff0c\u540c\u65f6\u89e3\u51b3\"\u4ec0\u4e48\u3001\u54ea\u91cc\u3001\u5982\u4f55\"\u4e09\u4e2a\u95ee\u9898\uff0c\u8d85\u8d8a\u4f20\u7edf\u4ec5\u5173\u6ce8\u8bed\u4e49\u5206\u5272\u7684\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u53ef\u4f9b\u6027\u68c0\u6d4b\u89c6\u4e3a\u8bed\u4e49\u5206\u5272\u4efb\u52a1\uff0c\u53ea\u5173\u6ce8\"\u7269\u4f53\u80fd\u7528\u6765\u505a\u4ec0\u4e48\"\uff08what\uff09\uff0c\u800c\u5ffd\u7565\u4e86\"\u5728\u54ea\u91cc\u64cd\u4f5c\"\uff08where\uff09\u548c\"\u5982\u4f55\u64cd\u4f5c\"\uff08how\uff09\u7684\u5173\u952e\u4fe1\u606f\u3002\u8fd9\u9650\u5236\u4e86\u673a\u5668\u4eba\u5728\u5b9e\u9645\u64cd\u7eb5\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u878d\u5408\u7684\u53ef\u4f9b\u6027\u5173\u952e\u70b9\u7f51\u7edc\uff08FAKP-Net\uff09\uff0c\u5f15\u51653D\u5173\u952e\u70b9\u56db\u5143\u7ec4\uff0c\u5229\u7528RGB\u548c\u6df1\u5ea6\u56fe\u50cf\u7684\u534f\u540c\u6f5c\u529b\uff0c\u63d0\u4f9b\u6267\u884c\u4f4d\u7f6e\u3001\u65b9\u5411\u548c\u8303\u56f4\u4fe1\u606f\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793aFAKP-Net\u5728\u53ef\u4f9b\u6027\u5206\u5272\u4efb\u52a1\u548c\u5173\u952e\u70b9\u68c0\u6d4b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e5f\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u64cd\u7eb5\u672a\u89c1\u7269\u4f53\u65f6\u7684\u53ef\u9760\u6027\u3002", "conclusion": "FAKP-Net\u901a\u8fc73D\u5173\u952e\u70b9\u56db\u5143\u7ec4\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u53ef\u4f9b\u6027\u4fe1\u606f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u53ea\u5173\u6ce8\"what\"\u7684\u5c40\u9650\u6027\uff0c\u5728\u673a\u5668\u4eba\u64cd\u7eb5\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.22225", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.22225", "abs": "https://arxiv.org/abs/2511.22225", "authors": ["Gabriel Aguirre", "Simay Atasoy Bing\u00f6l", "Heiko Hamann", "Jonas Kuckling"], "title": "Bayesian Decentralized Decision-making for Multi-Robot Systems: Sample-efficient Estimation of Event Rates", "comment": "7 pages, 3 figures, submitted to IEEE MRS 2025", "summary": "Effective collective decision-making in swarm robotics often requires balancing exploration, communication and individual uncertainty estimation, especially in hazardous environments where direct measurements are limited or costly. We propose a decentralized Bayesian framework that enables a swarm of simple robots to identify the safer of two areas, each characterized by an unknown rate of hazardous events governed by a Poisson process. Robots employ a conjugate prior to gradually predict the times between events and derive confidence estimates to adapt their behavior. Our simulation results show that the robot swarm consistently chooses the correct area while reducing exposure to hazardous events by being sample-efficient. Compared to baseline heuristics, our proposed approach shows better performance in terms of safety and speed of convergence. The proposed scenario has potential to extend the current set of benchmarks in collective decision-making and our method has applications in adaptive risk-aware sampling and exploration in hazardous, dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u4f7f\u673a\u5668\u4eba\u7fa4\u4f53\u80fd\u591f\u5728\u4e24\u4e2a\u5371\u9669\u533a\u57df\u4e2d\u8bc6\u522b\u66f4\u5b89\u5168\u7684\u533a\u57df\uff0c\u901a\u8fc7\u6cca\u677e\u8fc7\u7a0b\u5efa\u6a21\u5371\u9669\u4e8b\u4ef6\uff0c\u5b9e\u73b0\u6837\u672c\u9ad8\u6548\u7684\u5b89\u5168\u51b3\u7b56\u3002", "motivation": "\u5728\u5371\u9669\u73af\u5883\u4e2d\uff0c\u7fa4\u4f53\u673a\u5668\u4eba\u9700\u8981\u5e73\u8861\u63a2\u7d22\u3001\u901a\u4fe1\u548c\u4e2a\u4f53\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u7279\u522b\u662f\u5728\u76f4\u63a5\u6d4b\u91cf\u53d7\u9650\u6216\u6210\u672c\u9ad8\u6602\u7684\u60c5\u51b5\u4e0b\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u66f4\u6709\u6548\u7684\u98ce\u9669\u611f\u77e5\u51b3\u7b56\u6846\u67b6\u3002", "method": "\u91c7\u7528\u53bb\u4e2d\u5fc3\u5316\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u4f7f\u7528\u5171\u8f6d\u5148\u9a8c\u9010\u6b65\u9884\u6d4b\u5371\u9669\u4e8b\u4ef6\u95f4\u7684\u65f6\u95f4\u95f4\u9694\uff0c\u5e76\u63a8\u5bfc\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u6765\u8c03\u6574\u673a\u5668\u4eba\u884c\u4e3a\u3002\u673a\u5668\u4eba\u901a\u8fc7\u6cca\u677e\u8fc7\u7a0b\u5efa\u6a21\u672a\u77e5\u5371\u9669\u4e8b\u4ef6\u7387\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u673a\u5668\u4eba\u7fa4\u4f53\u80fd\u591f\u4e00\u81f4\u9009\u62e9\u6b63\u786e\u533a\u57df\uff0c\u540c\u65f6\u901a\u8fc7\u6837\u672c\u9ad8\u6548\u6027\u51cf\u5c11\u66b4\u9732\u4e8e\u5371\u9669\u4e8b\u4ef6\u3002\u76f8\u6bd4\u57fa\u51c6\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5728\u5b89\u5168\u6027\u548c\u6536\u655b\u901f\u5ea6\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u8be5\u6846\u67b6\u6269\u5c55\u4e86\u7fa4\u4f53\u51b3\u7b56\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u9002\u7528\u4e8e\u5371\u9669\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u98ce\u9669\u611f\u77e5\u91c7\u6837\u548c\u63a2\u7d22\u5e94\u7528\u3002"}}
{"id": "2511.22238", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22238", "abs": "https://arxiv.org/abs/2511.22238", "authors": ["Ryosuke Ofuchi", "Yuichiro Toda", "Naoki Masuyama", "Takayuki Matsuno"], "title": "MLATC: Fast Hierarchical Topological Mapping from 3D LiDAR Point Clouds Based on Adaptive Resonance Theory", "comment": null, "summary": "This paper addresses the problem of building global topological maps from 3D LiDAR point clouds for autonomous mobile robots operating in large-scale, dynamic, and unknown environments. Adaptive Resonance Theory-based Topological Clustering with Different Topologies (ATC-DT) builds global topological maps represented as graphs while mitigating catastrophic forgetting during sequential processing. However, its winner selection mechanism relies on an exhaustive nearest-neighbor search over all existing nodes, leading to scalability limitations as the map grows. To address this challenge, we propose a hierarchical extension called Multi-Layer ATC (MLATC). MLATC organizes nodes into a hierarchy, enabling the nearest-neighbor search to proceed from coarse to fine resolutions, thereby drastically reducing the number of distance evaluations per query. The number of layers is not fixed in advance. MLATC employs an adaptive layer addition mechanism that automatically deepens the hierarchy when lower layers become saturated, keeping the number of user-defined hyperparameters low. Simulation experiments on synthetic large-scale environments show that MLATC accelerates topological map building compared to the original ATC-DT and exhibits a sublinear, approximately logarithmic scaling of search time with respect to the number of nodes. Experiments on campus-scale real-world LiDAR datasets confirm that MLATC maintains a millisecond-level per-frame runtime and enables real-time global topological map building in large-scale environments, significantly outperforming the original ATC-DT in terms of computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u591a\u5c42ATC\uff08MLATC\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u7ed3\u6784\u52a0\u901f\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u62d3\u6251\u5730\u56fe\u6784\u5efa\uff0c\u89e3\u51b3ATC-DT\u65b9\u6cd5\u4e2d\u6700\u8fd1\u90bb\u641c\u7d22\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "ATC-DT\u65b9\u6cd5\u5728\u6784\u5efa\u5168\u5c40\u62d3\u6251\u5730\u56fe\u65f6\uff0c\u5176\u8d62\u5bb6\u9009\u62e9\u673a\u5236\u9700\u8981\u5bf9\u6240\u6709\u73b0\u6709\u8282\u70b9\u8fdb\u884c\u7a77\u4e3e\u6700\u8fd1\u90bb\u641c\u7d22\uff0c\u968f\u7740\u5730\u56fe\u89c4\u6a21\u589e\u957f\u4f1a\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u9650\u5236\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u62d3\u6251\u5730\u56fe\u6784\u5efa\u3002", "method": "\u63d0\u51fa\u591a\u5c42ATC\uff08MLATC\uff09\u65b9\u6cd5\uff0c\u5c06\u8282\u70b9\u7ec4\u7ec7\u6210\u5c42\u6b21\u7ed3\u6784\uff0c\u4f7f\u6700\u8fd1\u90bb\u641c\u7d22\u4ece\u7c97\u5230\u7ec6\u5206\u8fa8\u7387\u8fdb\u884c\uff0c\u5927\u5e45\u51cf\u5c11\u6bcf\u6b21\u67e5\u8be2\u7684\u8ddd\u79bb\u8ba1\u7b97\u6b21\u6570\u3002\u91c7\u7528\u81ea\u9002\u5e94\u5c42\u6dfb\u52a0\u673a\u5236\uff0c\u5f53\u8f83\u4f4e\u5c42\u9971\u548c\u65f6\u81ea\u52a8\u52a0\u6df1\u5c42\u6b21\u7ed3\u6784\uff0c\u4fdd\u6301\u7528\u6237\u5b9a\u4e49\u8d85\u53c2\u6570\u8f83\u5c11\u3002", "result": "\u5728\u5408\u6210\u5927\u89c4\u6a21\u73af\u5883\u4eff\u771f\u5b9e\u9a8c\u4e2d\uff0cMLATC\u76f8\u6bd4\u539f\u59cbATC-DT\u52a0\u901f\u4e86\u62d3\u6251\u5730\u56fe\u6784\u5efa\uff0c\u641c\u7d22\u65f6\u95f4\u968f\u8282\u70b9\u6570\u5448\u4e9a\u7ebf\u6027\uff08\u8fd1\u4f3c\u5bf9\u6570\uff09\u7f29\u653e\u3002\u5728\u6821\u56ed\u89c4\u6a21\u771f\u5b9eLiDAR\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\uff0cMLATC\u4fdd\u6301\u6beb\u79d2\u7ea7\u6bcf\u5e27\u8fd0\u884c\u65f6\u95f4\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u5168\u5c40\u62d3\u6251\u5730\u56fe\u6784\u5efa\uff0c\u8ba1\u7b97\u6548\u7387\u663e\u8457\u4f18\u4e8e\u539f\u59cbATC-DT\u3002", "conclusion": "MLATC\u901a\u8fc7\u5206\u5c42\u7ed3\u6784\u548c\u81ea\u9002\u5e94\u5c42\u6dfb\u52a0\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86ATC-DT\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5b9e\u65f6\u5168\u5c40\u62d3\u6251\u5730\u56fe\u6784\u5efa\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u3001\u52a8\u6001\u3001\u672a\u77e5\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u3002"}}
{"id": "2511.22318", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22318", "abs": "https://arxiv.org/abs/2511.22318", "authors": ["Yuki Origane", "Koya Cho", "Hideyuki Tsukagoshi"], "title": "Soft Fluidic Sheet Transistor for Soft Robotic System Enabling Fluid Logic Operations", "comment": "7 pages, 16 figures", "summary": "Aiming to achieve both high functionality and flexibility in soft robot system, this paper presents a soft urethane sheet-like valve with an amplifier that can perform logical operations using only pneumatic signals. When the control chamber in the valve is pressurized, the main path is compressed along its central axis, buckling and being pressed,resulting in blockage. This allows control by a pressure signal smaller than that within the main channel. Furthermore, similar to transistors in electrical circuits, when combined, the proposed valve can perform a variety of logical operations. The basic type operates as a NOT logic element, which is named the fluidic sheet transistor (FST). By integrating multiple FSTs, logical operations such as positive logic, NAND, and NOR can be performed on a single sheet. This paper describes the operating principle, fabrication method, and characteristics of the FST,followed by a method for configuring logical operations.Moreover, we demonstrate the construction of a latch circuit(self-holding logic circuit) using FST, introducing a prototype of a fluid robot system that combines a tactile tube as a fluidic detector and fluid actuators. This demonstrates that it is possible to generate behavior that actively changes posture when hitting an obstacle using only air pressure from a single pipe, which verifies the effectiveness of the proposed methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8f6f\u8d28\u805a\u6c28\u916f\u7247\u72b6\u9600\u95e8\uff08FST\uff09\uff0c\u4ec5\u4f7f\u7528\u6c14\u52a8\u4fe1\u53f7\u5373\u53ef\u5b9e\u73b0\u903b\u8f91\u8fd0\u7b97\uff0c\u7c7b\u4f3c\u7535\u5b50\u7535\u8def\u4e2d\u7684\u6676\u4f53\u7ba1\uff0c\u53ef\u7528\u4e8e\u6784\u5efa\u6d41\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u9ad8\u529f\u80fd\u6027\u548c\u7075\u6d3b\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4ec5\u4f7f\u7528\u6c14\u52a8\u4fe1\u53f7\u6267\u884c\u903b\u8f91\u64cd\u4f5c\u7684\u5143\u4ef6\uff0c\u4ee5\u7b80\u5316\u7cfb\u7edf\u7ed3\u6784\u5e76\u63d0\u9ad8\u96c6\u6210\u5ea6\u3002", "method": "\u8bbe\u8ba1\u8f6f\u8d28\u805a\u6c28\u916f\u7247\u72b6\u9600\u95e8\uff0c\u5f53\u63a7\u5236\u8154\u52a0\u538b\u65f6\uff0c\u4e3b\u901a\u9053\u6cbf\u4e2d\u5fc3\u8f74\u538b\u7f29\u5e76\u5f2f\u66f2\u963b\u585e\uff0c\u5b9e\u73b0\u7528\u5c0f\u538b\u529b\u4fe1\u53f7\u63a7\u5236\u5927\u538b\u529b\u901a\u9053\u3002\u5c06\u591a\u4e2aFST\u96c6\u6210\u5728\u5355\u5f20\u7247\u4e0a\u5b9e\u73b0\u903b\u8f91\u8fd0\u7b97\u3002", "result": "\u6210\u529f\u5f00\u53d1\u51fa\u6d41\u4f53\u7247\u72b6\u6676\u4f53\u7ba1\uff08FST\uff09\uff0c\u53ef\u4f5c\u4e3aNOT\u903b\u8f91\u5143\u4ef6\uff0c\u96c6\u6210\u540e\u53ef\u5b9e\u73b0\u6b63\u903b\u8f91\u3001NAND\u3001NOR\u7b49\u8fd0\u7b97\u3002\u6784\u5efa\u4e86\u9501\u5b58\u7535\u8def\uff0c\u5e76\u6f14\u793a\u4e86\u7ed3\u5408\u89e6\u89c9\u7ba1\u548c\u6d41\u4f53\u6267\u884c\u5668\u7684\u6d41\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "conclusion": "FST\u80fd\u591f\u4ec5\u4f7f\u7528\u5355\u7ba1\u6c14\u538b\u5b9e\u73b0\u903b\u8f91\u8fd0\u7b97\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6784\u5efa\u529f\u80fd\u7075\u6d3b\u3001\u7ed3\u6784\u7b80\u5316\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.22338", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.22338", "abs": "https://arxiv.org/abs/2511.22338", "authors": ["Denghan Xiong", "Yanzhe Zhao", "Yutong Chen", "Zichun Wang"], "title": "Nonholonomic Narrow Dead-End Escape with Deep Reinforcement Learning", "comment": "14 pages, 5 figures, 1 table, submitted to arXiv", "summary": "Nonholonomic constraints restrict feasible velocities without reducing configuration-space dimension, which makes collision-free geometric paths generally non-executable for car-like robots. Ackermann steering further imposes curvature bounds and forbids in-place rotation, so escaping from narrow dead ends typically requires tightly sequenced forward and reverse maneuvers. Classical planners that decouple global search and local steering struggle in these settings because narrow passages occupy low-measure regions and nonholonomic reachability shrinks the set of valid connections, which degrades sampling efficiency and increases sensitivity to clearances. We study nonholonomic narrow dead-end escape for Ackermann vehicles and contribute three components. First, we construct a generator that samples multi-phase forward-reverse trajectories compatible with Ackermann kinematics and inflates their envelopes to synthesize families of narrow dead ends that are guaranteed to admit at least one feasible escape. Second, we construct a training environment that enforces kinematic constraints and train a policy using the soft actor-critic algorithm. Third, we evaluate against representative classical planners that combine global search with nonholonomic steering. Across parameterized dead-end families, the learned policy solves a larger fraction of instances, reduces maneuver count, and maintains comparable path length and planning time while under the same sensing and control limits. We provide our project as open source at https://github.com/gitagitty/cisDRL-RobotNav.git", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\u963f\u514b\u66fc\u8f6c\u5411\u8f66\u8f86\u5728\u72ed\u7a84\u6b7b\u80e1\u540c\u4e2d\u7684\u975e\u5b8c\u6574\u7ea6\u675f\u9003\u9038\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u76f8\u6bd4\u4f20\u7edf\u89c4\u5212\u5668\u5728\u89e3\u51b3\u7387\u3001\u673a\u52a8\u6b21\u6570\u7b49\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u975e\u5b8c\u6574\u7ea6\u675f\u9650\u5236\u4e86\u963f\u514b\u66fc\u8f6c\u5411\u8f66\u8f86\u7684\u53ef\u884c\u901f\u5ea6\uff0c\u4f7f\u5176\u65e0\u6cd5\u6267\u884c\u539f\u5730\u65cb\u8f6c\uff0c\u72ed\u7a84\u6b7b\u80e1\u540c\u9003\u9038\u9700\u8981\u590d\u6742\u7684\u8fdb\u9000\u5e8f\u5217\u673a\u52a8\u3002\u4f20\u7edf\u89c4\u5212\u5668\u5c06\u5168\u5c40\u641c\u7d22\u4e0e\u5c40\u90e8\u8f6c\u5411\u89e3\u8026\uff0c\u5728\u72ed\u7a84\u901a\u9053\u4e2d\u91c7\u6837\u6548\u7387\u4f4e\uff0c\u5bf9\u95f4\u9699\u654f\u611f\uff0c\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u8fd9\u7c7b\u95ee\u9898\u3002", "method": "1. \u6784\u5efa\u751f\u6210\u5668\u91c7\u6837\u4e0e\u963f\u514b\u66fc\u8fd0\u52a8\u5b66\u517c\u5bb9\u7684\u591a\u9636\u6bb5\u8fdb\u9000\u8f68\u8ff9\uff0c\u5e76\u81a8\u80c0\u5176\u5305\u7edc\u4ee5\u5408\u6210\u4fdd\u8bc1\u81f3\u5c11\u6709\u4e00\u4e2a\u53ef\u884c\u9003\u9038\u8def\u5f84\u7684\u72ed\u7a84\u6b7b\u80e1\u540c\u65cf\uff1b2. \u6784\u5efa\u5f3a\u5236\u6267\u884c\u8fd0\u52a8\u5b66\u7ea6\u675f\u7684\u8bad\u7ec3\u73af\u5883\uff0c\u4f7f\u7528\u8f6f\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\u8bad\u7ec3\u7b56\u7565\uff1b3. \u4e0e\u7ed3\u5408\u5168\u5c40\u641c\u7d22\u548c\u975e\u5b8c\u6574\u8f6c\u5411\u7684\u4f20\u7edf\u89c4\u5212\u5668\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u5728\u53c2\u6570\u5316\u7684\u6b7b\u80e1\u540c\u65cf\u4e2d\uff0c\u5b66\u4e60\u5230\u7684\u7b56\u7565\u89e3\u51b3\u4e86\u66f4\u5927\u6bd4\u4f8b\u7684\u5b9e\u4f8b\uff0c\u51cf\u5c11\u4e86\u673a\u52a8\u6b21\u6570\uff0c\u5728\u76f8\u540c\u611f\u77e5\u548c\u63a7\u5236\u9650\u5236\u4e0b\u4fdd\u6301\u4e86\u53ef\u6bd4\u8f83\u7684\u8def\u5f84\u957f\u5ea6\u548c\u89c4\u5212\u65f6\u95f4\u3002", "conclusion": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u89e3\u51b3\u963f\u514b\u66fc\u8f66\u8f86\u975e\u5b8c\u6574\u72ed\u7a84\u6b7b\u80e1\u540c\u9003\u9038\u95ee\u9898\u4e0a\u4f18\u4e8e\u4f20\u7edf\u89c4\u5212\u5668\uff0c\u9879\u76ee\u5df2\u5f00\u6e90\u4f9b\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2511.22354", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22354", "abs": "https://arxiv.org/abs/2511.22354", "authors": ["Suraj Borate", "Bhavish Rai B", "Vipul Pardeshi", "Madhu Vadali"], "title": "LLM-Based Generalizable Hierarchical Task Planning and Execution for Heterogeneous Robot Teams with Event-Driven Replanning", "comment": "submitted to ICRA 2026", "summary": "This paper introduces CoMuRoS (Collaborative Multi-Robot System), a generalizable hierarchical architecture for heterogeneous robot teams that unifies centralized deliberation with decentralized execution, and supports event-driven replanning. A Task Manager LLM interprets natural-language goals, classifies tasks, and allocates subtasks using static rules plus dynamic contexts (task, history, robot and task status, and events).Each robot runs a local LLM that composes executable Python code from primitive skills (ROS2 nodes, policies), while onboard perception (VLMs/image processing) continuously monitors events and classifies them into relevant or irrelevant to the task. Task failures or user intent changes trigger replanning, allowing robots to assist teammates, resume tasks, or request human help. Hardware studies demonstrate autonomous recovery from disruptive events, filtering of irrelevant distractions, and tightly coordinated transport with emergent human-robot cooperation (e.g., multirobot collaborative object recovery success rate: 9/10, coordinated transport: 8/8, human-assisted recovery: 5/5).Simulation studies show intention-aware replanning. A curated textual benchmark spanning 22 scenarios (3 tasks each, around 20 robots) evaluates task allocation, classification, IoU, executability, and correctness, with high average scores (e.g., correctness up to 0.91) across multiple LLMs, a separate replanning set (5 scenarios) achieves 1.0 correctness. Compared with prior LLM-based systems, CoMuRoS uniquely demonstrates runtime, event-driven replanning on physical robots, delivering robust, flexible multi-robot and human-robot collaboration.", "AI": {"tldr": "CoMuRoS\u662f\u4e00\u4e2a\u53ef\u6cdb\u5316\u7684\u5206\u5c42\u67b6\u6784\uff0c\u7528\u4e8e\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\uff0c\u7ed3\u5408\u4e86\u96c6\u4e2d\u5f0f\u89c4\u5212\u548c\u5206\u5e03\u5f0f\u6267\u884c\uff0c\u652f\u6301\u4e8b\u4ef6\u9a71\u52a8\u7684\u91cd\u89c4\u5212\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u591a\u673a\u5668\u4eba\u548c\u4eba\u673a\u534f\u4f5c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u7f3a\u4e4f\u8fd0\u884c\u65f6\u7684\u4e8b\u4ef6\u9a71\u52a8\u91cd\u89c4\u5212\u80fd\u529b\uff0c\u96be\u4ee5\u5904\u7406\u7269\u7406\u673a\u5668\u4eba\u73af\u5883\u4e2d\u7684\u52a8\u6001\u53d8\u5316\u3001\u4efb\u52a1\u5931\u8d25\u548c\u7528\u6237\u610f\u56fe\u53d8\u66f4\uff0c\u9650\u5236\u4e86\u591a\u673a\u5668\u4eba\u534f\u4f5c\u7684\u9c81\u68d2\u6027\u548c\u7075\u6d3b\u6027\u3002", "method": "\u91c7\u7528\u5206\u5c42\u67b6\u6784\uff1a\u4efb\u52a1\u7ba1\u7406\u5668LLM\u89e3\u91ca\u81ea\u7136\u8bed\u8a00\u76ee\u6807\u3001\u5206\u7c7b\u4efb\u52a1\u3001\u5206\u914d\u5b50\u4efb\u52a1\uff1b\u6bcf\u4e2a\u673a\u5668\u4eba\u8fd0\u884c\u672c\u5730LLM\u5c06\u539f\u59cb\u6280\u80fd\u7f16\u8bd1\u4e3a\u53ef\u6267\u884cPython\u4ee3\u7801\uff1b\u673a\u8f7d\u611f\u77e5\u6301\u7eed\u76d1\u63a7\u4e8b\u4ef6\u5e76\u5206\u7c7b\uff1b\u4efb\u52a1\u5931\u8d25\u6216\u7528\u6237\u610f\u56fe\u53d8\u5316\u89e6\u53d1\u91cd\u89c4\u5212\u3002", "result": "\u786c\u4ef6\u5b9e\u9a8c\uff1a\u81ea\u4e3b\u6062\u590d\u6210\u529f\u73879/10\uff0c\u534f\u8c03\u8fd0\u8f938/8\uff0c\u4eba\u673a\u534f\u52a9\u6062\u590d5/5\uff1b\u4eff\u771f\u663e\u793a\u610f\u56fe\u611f\u77e5\u91cd\u89c4\u5212\uff1b\u57fa\u51c6\u6d4b\u8bd5\uff0822\u4e2a\u573a\u666f\uff09\u6b63\u786e\u7387\u9ad8\u8fbe0.91\uff0c\u91cd\u89c4\u5212\u573a\u666f\u6b63\u786e\u73871.0\uff1b\u76f8\u6bd4\u73b0\u6709\u7cfb\u7edf\uff0c\u9996\u6b21\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u8fd0\u884c\u65f6\u4e8b\u4ef6\u9a71\u52a8\u91cd\u89c4\u5212\u3002", "conclusion": "CoMuRoS\u901a\u8fc7\u7edf\u4e00\u96c6\u4e2d\u5f0f\u89c4\u5212\u548c\u5206\u5e03\u5f0f\u6267\u884c\uff0c\u652f\u6301\u4e8b\u4ef6\u9a71\u52a8\u91cd\u89c4\u5212\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u3001\u7075\u6d3b\u7684\u591a\u673a\u5668\u4eba\u548c\u4eba\u673a\u534f\u4f5c\uff0c\u4e3a\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u63d0\u4f9b\u4e86\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.22364", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22364", "abs": "https://arxiv.org/abs/2511.22364", "authors": ["Seongwon Cho", "Daechul Ahn", "Donghyun Shin", "Hyeonbeom Choi", "San Kim", "Jonghyun Choi"], "title": "BINDER: Instantly Adaptive Mobile Manipulation with Open-Vocabulary Commands", "comment": "12 pages, 8 figures", "summary": "Open-vocabulary mobile manipulation (OVMM) requires robots to follow language instructions, navigate, and manipulate while updating their world representation under dynamic environmental changes. However, most prior approaches update their world representation only at discrete update points such as navigation targets, waypoints, or the end of an action step, leaving robots blind between updates and causing cascading failures: overlooked objects, late error detection, and delayed replanning. To address this limitation, we propose BINDER (Bridging INstant and DEliberative Reasoning), a dual process framework that decouples strategic planning from continuous environment monitoring. Specifically, BINDER integrates a Deliberative Response Module (DRM, a multimodal LLM for task planning) with an Instant Response Module (IRM, a VideoLLM for continuous monitoring). The two modules play complementary roles: the DRM performs strategic planning with structured 3D scene updates and guides what the IRM attends to, while the IRM analyzes video streams to update memory, correct ongoing actions, and trigger replanning when necessary. Through this bidirectional coordination, the modules address the trade off between maintaining awareness and avoiding costly updates, enabling robust adaptation under dynamic conditions. Evaluated in three real world environments with dynamic object placement, BINDER achieves substantially higher success and efficiency than SoTA baselines, demonstrating its effectiveness for real world deployment.", "AI": {"tldr": "BINDER\u662f\u4e00\u4e2a\u53cc\u8fc7\u7a0b\u6846\u67b6\uff0c\u5c06\u6218\u7565\u89c4\u5212\u4e0e\u8fde\u7eed\u73af\u5883\u76d1\u63a7\u89e3\u8026\uff0c\u901a\u8fc7\u5373\u65f6\u54cd\u5e94\u6a21\u5757\u6301\u7eed\u76d1\u6d4b\u89c6\u9891\u6d41\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u79bb\u6563\u66f4\u65b0\u70b9\u95f4\"\u5931\u660e\"\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u52a8\u6001\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c(OVMM)\u7cfb\u7edf\u4ec5\u5728\u79bb\u6563\u66f4\u65b0\u70b9\uff08\u5982\u5bfc\u822a\u76ee\u6807\u3001\u8def\u5f84\u70b9\u6216\u52a8\u4f5c\u6b65\u9aa4\u7ed3\u675f\u65f6\uff09\u66f4\u65b0\u4e16\u754c\u8868\u793a\uff0c\u5bfc\u81f4\u673a\u5668\u4eba\u5728\u66f4\u65b0\u4e4b\u95f4\"\u5931\u660e\"\uff0c\u9020\u6210\u7ea7\u8054\u5931\u8d25\uff1a\u9057\u6f0f\u7269\u4f53\u3001\u9519\u8bef\u68c0\u6d4b\u5ef6\u8fdf\u548c\u91cd\u89c4\u5212\u6ede\u540e\u3002", "method": "\u63d0\u51faBINDER\u53cc\u8fc7\u7a0b\u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u5ba1\u614e\u54cd\u5e94\u6a21\u5757(DRM)\uff1a\u591a\u6a21\u6001LLM\u8fdb\u884c\u6218\u7565\u89c4\u5212\u548c\u7ed3\u6784\u53163D\u573a\u666f\u66f4\u65b0\uff1b2) \u5373\u65f6\u54cd\u5e94\u6a21\u5757(IRM)\uff1aVideoLLM\u6301\u7eed\u76d1\u6d4b\u89c6\u9891\u6d41\uff0c\u66f4\u65b0\u8bb0\u5fc6\u3001\u7ea0\u6b63\u6b63\u5728\u8fdb\u884c\u7684\u52a8\u4f5c\u5e76\u5728\u5fc5\u8981\u65f6\u89e6\u53d1\u91cd\u89c4\u5212\u3002\u4e24\u4e2a\u6a21\u5757\u901a\u8fc7\u53cc\u5411\u534f\u8c03\u5de5\u4f5c\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u52a8\u6001\u7269\u4f53\u653e\u7f6e\u73af\u5883\u4e2d\u8bc4\u4f30\uff0cBINDER\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u6548\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "BINDER\u901a\u8fc7\u89e3\u8026\u6218\u7565\u89c4\u5212\u4e0e\u8fde\u7eed\u76d1\u63a7\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u4fdd\u6301\u611f\u77e5\u4e0e\u907f\u514d\u6602\u8d35\u66f4\u65b0\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2511.22445", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22445", "abs": "https://arxiv.org/abs/2511.22445", "authors": ["Yikai Tang", "Haoran Geng", "Sheng Zang", "Pieter Abbeel", "Jitendra Malik"], "title": "Visual-Geometry Diffusion Policy: Robust Generalization via Complementarity-Aware Multimodal Fusion", "comment": null, "summary": "Imitation learning has emerged as a crucial ap proach for acquiring visuomotor skills from demonstrations, where designing effective observation encoders is essential for policy generalization. However, existing methods often struggle to generalize under spatial and visual randomizations, instead tending to overfit. To address this challenge, we propose Visual Geometry Diffusion Policy (VGDP), a multimodal imitation learning framework built around a Complementarity-Aware Fusion Module where modality-wise dropout enforces balanced use of RGB and point-cloud cues, with cross-attention serving only as a lightweight interaction layer. Our experiments show that the expressiveness of the fused latent space is largely induced by the enforced complementarity from modality-wise dropout, with cross-attention serving primarily as a lightweight interaction mechanism rather than the main source of robustness. Across a benchmark of 18 simulated tasks and 4 real-world tasks, VGDP outperforms seven baseline policies with an average performance improvement of 39.1%. More importantly, VGDP demonstrates strong robustness under visual and spatial per turbations, surpassing baselines with an average improvement of 41.5% in different visual conditions and 15.2% in different spatial settings.", "AI": {"tldr": "VGDP\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e92\u8865\u611f\u77e5\u878d\u5408\u6a21\u5757\u548c\u6a21\u6001\u7ea7dropout\u5e73\u8861RGB\u548c\u70b9\u4e91\u7279\u5f81\uff0c\u5728\u89c6\u89c9\u548c\u7a7a\u95f4\u6270\u52a8\u4e0b\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u7a7a\u95f4\u548c\u89c6\u89c9\u968f\u673a\u5316\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u9700\u8981\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u89c2\u5bdf\u7f16\u7801\u5668\u6765\u63d0\u5347\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u51e0\u4f55\u6269\u6563\u7b56\u7565(VGDP)\uff0c\u5305\u542b\u4e92\u8865\u611f\u77e5\u878d\u5408\u6a21\u5757\uff0c\u901a\u8fc7\u6a21\u6001\u7ea7dropout\u5f3a\u5236\u5e73\u8861\u4f7f\u7528RGB\u548c\u70b9\u4e91\u7ebf\u7d22\uff0c\u4ea4\u53c9\u6ce8\u610f\u529b\u4ec5\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u4ea4\u4e92\u5c42\u3002", "result": "\u572818\u4e2a\u6a21\u62df\u4efb\u52a1\u548c4\u4e2a\u771f\u5b9e\u4efb\u52a1\u4e2d\uff0cVGDP\u5e73\u5747\u6027\u80fd\u63d0\u534739.1%\uff0c\u5728\u89c6\u89c9\u6270\u52a8\u4e0b\u5e73\u5747\u63d0\u534741.5%\uff0c\u5728\u7a7a\u95f4\u8bbe\u7f6e\u4e0b\u5e73\u5747\u63d0\u534715.2%\uff0c\u663e\u8457\u4f18\u4e8e7\u4e2a\u57fa\u7ebf\u7b56\u7565\u3002", "conclusion": "\u878d\u5408\u6f5c\u5728\u7a7a\u95f4\u7684\u8868\u8fbe\u80fd\u529b\u4e3b\u8981\u6765\u81ea\u6a21\u6001\u7ea7dropout\u5f3a\u5236\u5b9e\u73b0\u7684\u4e92\u8865\u6027\uff0c\u4ea4\u53c9\u6ce8\u610f\u529b\u4ec5\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u4ea4\u4e92\u673a\u5236\uff0cVGDP\u5728\u89c6\u89c9\u548c\u7a7a\u95f4\u6270\u52a8\u4e0b\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.22505", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22505", "abs": "https://arxiv.org/abs/2511.22505", "authors": ["Xiujian Liang", "Jiacheng Liu", "Mingyang Sun", "Qichen He", "Cewu Lu", "Jianhua Sun"], "title": "RealD$^2$iff: Bridging Real-World Gap in Robot Manipulation via Depth Diffusion", "comment": null, "summary": "Robot manipulation in the real world is fundamentally constrained by the visual sim2real gap, where depth observations collected in simulation fail to reflect the complex noise patterns inherent to real sensors. In this work, inspired by the denoising capability of diffusion models, we invert the conventional perspective and propose a clean-to-noisy paradigm that learns to synthesize noisy depth, thereby bridging the visual sim2real gap through purely simulation-driven robotic learning. Building on this idea, we introduce RealD$^2$iff, a hierarchical coarse-to-fine diffusion framework that decomposes depth noise into global structural distortions and fine-grained local perturbations. To enable progressive learning of these components, we further develop two complementary strategies: Frequency-Guided Supervision (FGS) for global structure modeling and Discrepancy-Guided Optimization (DGO) for localized refinement. To integrate RealD$^2$iff seamlessly into imitation learning, we construct a pipeline that spans six stages. We provide comprehensive empirical and experimental validation demonstrating the effectiveness of this paradigm. RealD$^2$iff enables two key applications: (1) generating real-world-like depth to construct clean-noisy paired datasets without manual sensor data collection. (2) Achieving zero-shot sim2real robot manipulation, substantially improving real-world performance without additional fine-tuning.", "AI": {"tldr": "\u63d0\u51faRealD\u00b2iff\u6846\u67b6\uff0c\u901a\u8fc7\u5e72\u51c0\u5230\u566a\u58f0\u7684\u6269\u6563\u6a21\u578b\u5b66\u4e60\u5408\u6210\u771f\u5b9e\u566a\u58f0\u6df1\u5ea6\uff0c\u89e3\u51b3\u89c6\u89c9sim2real\u5dee\u8ddd\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u673a\u5668\u4eba\u64cd\u4f5c\u8fc1\u79fb", "motivation": "\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u53d7\u89c6\u89c9sim2real\u5dee\u8ddd\u9650\u5236\uff0c\u6a21\u62df\u5668\u4e2d\u7684\u6df1\u5ea6\u89c2\u6d4b\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4f20\u611f\u5668\u7684\u590d\u6742\u566a\u58f0\u6a21\u5f0f\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u6839\u672c\u7ea6\u675f", "method": "\u63d0\u51faRealD\u00b2iff\u5206\u5c42\u6269\u6563\u6846\u67b6\uff0c\u5c06\u6df1\u5ea6\u566a\u58f0\u5206\u89e3\u4e3a\u5168\u5c40\u7ed3\u6784\u626d\u66f2\u548c\u5c40\u90e8\u6270\u52a8\uff1b\u4f7f\u7528\u9891\u7387\u5f15\u5bfc\u76d1\u7763(FGS)\u5efa\u6a21\u5168\u5c40\u7ed3\u6784\uff0c\u5dee\u5f02\u5f15\u5bfc\u4f18\u5316(DGO)\u8fdb\u884c\u5c40\u90e8\u7ec6\u5316\uff1b\u6784\u5efa\u516d\u9636\u6bb5\u6a21\u4eff\u5b66\u4e60\u6d41\u7a0b", "result": "\u80fd\u591f\u751f\u6210\u771f\u5b9e\u4e16\u754c\u6df1\u5ea6\u6570\u636e\u6784\u5efa\u5e72\u51c0-\u566a\u58f0\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u65e0\u9700\u624b\u52a8\u4f20\u611f\u5668\u6570\u636e\u6536\u96c6\uff1b\u5b9e\u73b0\u96f6\u6837\u672csim2real\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u800c\u65e0\u9700\u989d\u5916\u5fae\u8c03", "conclusion": "\u901a\u8fc7\u5e72\u51c0\u5230\u566a\u58f0\u7684\u8303\u5f0f\u6210\u529f\u6865\u63a5\u89c6\u89c9sim2real\u5dee\u8ddd\uff0cRealD\u00b2iff\u6846\u67b6\u4e3a\u7eaf\u6a21\u62df\u9a71\u52a8\u7684\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u96f6\u6837\u672c\u8fc1\u79fb\u65b9\u9762\u8868\u73b0\u51fa\u8272"}}
{"id": "2511.22541", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.22541", "abs": "https://arxiv.org/abs/2511.22541", "authors": ["Jinyang Li", "Marcello Farina", "Luca Mozzarelli", "Luca Cattaneo", "Panita Rattamasanaprapai", "Eleonora A. Tagarelli", "Matteo Corno", "Paolo Perego", "Giuseppe Andreoni", "Emanuele Lettieri"], "title": "BUDD-e: an autonomous robotic guide for visually impaired users", "comment": "14 pages", "summary": "This paper describes the design and the realization of a prototype of the novel guide robot BUDD-e for visually impaired users. The robot has been tested in a real scenario with the help of visually disabled volunteers at ASST Grande Ospedale Metropolitano Niguarda, in Milan. The results of the experimental campaign are throughly described in the paper, displaying its remarkable performance and user-acceptance.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u6b3e\u540d\u4e3aBUDD-e\u7684\u65b0\u578b\u5bfc\u76f2\u673a\u5668\u4eba\u539f\u578b\uff0c\u5e76\u5728\u533b\u9662\u73af\u5883\u4e2d\u4e0e\u89c6\u969c\u5fd7\u613f\u8005\u8fdb\u884c\u4e86\u5b9e\u9645\u6d4b\u8bd5\uff0c\u53d6\u5f97\u4e86\u826f\u597d\u6027\u80fd\u548c\u7528\u6237\u63a5\u53d7\u5ea6\u3002", "motivation": "\u4e3a\u89c6\u969c\u7528\u6237\u5f00\u53d1\u8f85\u52a9\u5bfc\u822a\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5e2e\u52a9\u4ed6\u4eec\u5728\u590d\u6742\u73af\u5883\u4e2d\u72ec\u7acb\u79fb\u52a8\uff0c\u63d0\u9ad8\u751f\u6d3b\u8d28\u91cf\u548c\u81ea\u4e3b\u6027\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86BUDD-e\u5bfc\u76f2\u673a\u5668\u4eba\u539f\u578b\uff0c\u5728\u7c73\u5170\u7684ASST Grande Ospedale Metropolitano Niguarda\u533b\u9662\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u4e0e\u89c6\u969c\u5fd7\u613f\u8005\u5408\u4f5c\u8fdb\u884c\u5b9e\u9a8c\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u673a\u5668\u4eba\u8868\u73b0\u51fa\u8272\uff0c\u7528\u6237\u63a5\u53d7\u5ea6\u9ad8\uff0c\u8bc1\u660e\u4e86\u8be5\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "BUDD-e\u5bfc\u76f2\u673a\u5668\u4eba\u539f\u578b\u6210\u529f\u5c55\u793a\u4e86\u4e3a\u89c6\u969c\u7528\u6237\u63d0\u4f9b\u5bfc\u822a\u8f85\u52a9\u7684\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u63a8\u5e7f\u3002"}}
{"id": "2511.22555", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22555", "abs": "https://arxiv.org/abs/2511.22555", "authors": ["Yanbo Mao", "Jianlong Fu", "Ruoxuan Zhang", "Hongxia Xie", "Meibao Yao"], "title": "Beyond Success: Refining Elegant Robot Manipulation from Mixed-Quality Data via Just-in-Time Intervention", "comment": null, "summary": "Vision-Language-Action (VLA) models have enabled notable progress in general-purpose robotic manipulation, yet their learned policies often exhibit variable execution quality. We attribute this variability to the mixed-quality nature of human demonstrations, where the implicit principles that govern how actions should be carried out are only partially satisfied. To address this challenge, we introduce the LIBERO-Elegant benchmark with explicit criteria for evaluating execution quality. Using these criteria, we develop a decoupled refinement framework that improves execution quality without modifying or retraining the base VLA policy. We formalize Elegant Execution as the satisfaction of Implicit Task Constraints (ITCs) and train an Elegance Critic via offline Calibrated Q-Learning to estimate the expected quality of candidate actions. At inference time, a Just-in-Time Intervention (JITI) mechanism monitors critic confidence and intervenes only at decision-critical moments, providing selective, on-demand refinement. Experiments on LIBERO-Elegant and real-world manipulation tasks show that the learned Elegance Critic substantially improves execution quality, even on unseen tasks. The proposed model enables robotic control that values not only whether tasks succeed, but also how they are performed.", "AI": {"tldr": "\u63d0\u51faLIBERO-Elegant\u57fa\u51c6\u548c\u53bb\u8026\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u96c5\u6267\u884c\u6279\u8bc4\u5bb6\u548c\u5373\u65f6\u5e72\u9884\u673a\u5236\u63d0\u5347VLA\u6a21\u578b\u6267\u884c\u8d28\u91cf\uff0c\u4e0d\u4fee\u6539\u57fa\u7840\u7b56\u7565", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u6267\u884c\u8d28\u91cf\u4e0d\u7a33\u5b9a\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u4eba\u7c7b\u6f14\u793a\u6570\u636e\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\uff0c\u9690\u542b\u7684\u4efb\u52a1\u7ea6\u675f\u672a\u88ab\u5145\u5206\u6ee1\u8db3", "method": "1) \u5efa\u7acbLIBERO-Elegant\u57fa\u51c6\uff0c\u660e\u786e\u5b9a\u4e49\u6267\u884c\u8d28\u91cf\u8bc4\u4f30\u6807\u51c6\uff1b2) \u5c06\u4f18\u96c5\u6267\u884c\u5f62\u5f0f\u5316\u4e3a\u6ee1\u8db3\u9690\u542b\u4efb\u52a1\u7ea6\u675f\uff1b3) \u901a\u8fc7\u79bb\u7ebf\u6821\u51c6Q\u5b66\u4e60\u8bad\u7ec3\u4f18\u96c5\u6267\u884c\u6279\u8bc4\u5bb6\uff1b4) \u63a8\u7406\u65f6\u4f7f\u7528\u5373\u65f6\u5e72\u9884\u673a\u5236\uff0c\u4ec5\u5728\u5173\u952e\u51b3\u7b56\u65f6\u523b\u8fdb\u884c\u9009\u62e9\u6027\u4f18\u5316", "result": "\u5728LIBERO-Elegant\u57fa\u51c6\u548c\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f18\u96c5\u6267\u884c\u6279\u8bc4\u5bb6\u663e\u8457\u63d0\u5347\u6267\u884c\u8d28\u91cf\uff0c\u5373\u4f7f\u9762\u5bf9\u672a\u89c1\u4efb\u52a1\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u4e0d\u4ec5\u5173\u6ce8\u4efb\u52a1\u662f\u5426\u6210\u529f\uff0c\u66f4\u5173\u6ce8\u4efb\u52a1\u6267\u884c\u65b9\u5f0f\u7684\u673a\u5668\u4eba\u63a7\u5236\uff0c\u4e3a\u63d0\u5347VLA\u6a21\u578b\u6267\u884c\u8d28\u91cf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.22685", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22685", "abs": "https://arxiv.org/abs/2511.22685", "authors": ["Haoyi Wang", "Licheng Luo", "Yiannis Kantaros", "Bruno Sinopoli", "Mingyu Cai"], "title": "Deadlock-Free Hybrid RL-MAPF Framework for Zero-Shot Multi-Robot Navigation", "comment": null, "summary": "Multi-robot navigation in cluttered environments presents fundamental challenges in balancing reactive collision avoidance with long-range goal achievement. When navigating through narrow passages\n  or confined spaces, deadlocks frequently emerge that prevent agents from reaching their destinations, particularly when Reinforcement Learning (RL) control policies encounter novel configurations out of learning distribution. Existing RL-based approaches suffer from limited generalization capability in unseen environments. We propose a hybrid framework that seamlessly integrates RL-based reactive navigation with on-demand Multi-Agent Path Finding (MAPF) to explicitly resolve topological deadlocks. Our approach integrates a safety layer that monitors agent progress to detect deadlocks and, when detected, triggers a coordination controller for affected agents. The framework constructs globally feasible trajectories via MAPF and regulates waypoint progression to reduce inter-agent conflicts during navigation.\n  Extensive evaluation on dense multi-agent benchmarks shows that our method boosts task completion from marginal to near-universal success, markedly reducing deadlocks and collisions. When integrated with hierarchical task planning, it enables coordinated navigation for heterogeneous robots, demonstrating that coupling reactive RL navigation with selective MAPF intervention yields a robust, zero-shot performance.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408RL\u53cd\u5e94\u5f0f\u5bfc\u822a\u4e0e\u6309\u9700MAPF\uff0c\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5bc6\u96c6\u73af\u5883\u5bfc\u822a\u4e2d\u7684\u6b7b\u9501\u95ee\u9898\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u5728\u5bc6\u96c6\u73af\u5883\u5bfc\u822a\u9762\u4e34\u5e73\u8861\u53cd\u5e94\u5f0f\u907f\u969c\u4e0e\u957f\u671f\u76ee\u6807\u8fbe\u6210\u7684\u6311\u6218\u3002\u72ed\u7a84\u901a\u9053\u6216\u53d7\u9650\u7a7a\u95f4\u4e2d\u5e38\u51fa\u73b0\u6b7b\u9501\uff0c\u7279\u522b\u662f\u5f53RL\u63a7\u5236\u7b56\u7565\u9047\u5230\u8d85\u51fa\u5b66\u4e60\u5206\u5e03\u7684\u65b0\u914d\u7f6e\u65f6\u3002\u73b0\u6709RL\u65b9\u6cd5\u5728\u672a\u89c1\u73af\u5883\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u6846\u67b6\uff1a1) RL\u53cd\u5e94\u5f0f\u5bfc\u822a\u5904\u7406\u5e38\u89c4\u907f\u969c\uff1b2) \u5b89\u5168\u5c42\u76d1\u63a7\u667a\u80fd\u4f53\u8fdb\u5ea6\u68c0\u6d4b\u6b7b\u9501\uff1b3) \u68c0\u6d4b\u5230\u6b7b\u9501\u65f6\u89e6\u53d1\u534f\u8c03\u63a7\u5236\u5668\uff1b4) \u901a\u8fc7MAPF\u751f\u6210\u5168\u5c40\u53ef\u884c\u8f68\u8ff9\uff1b5) \u8c03\u8282\u822a\u70b9\u8fdb\u5ea6\u51cf\u5c11\u667a\u80fd\u4f53\u95f4\u51b2\u7a81\u3002", "result": "\u5728\u5bc6\u96c6\u591a\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u4ece\u8fb9\u9645\u63d0\u5347\u5230\u63a5\u8fd1\u666e\u904d\u6210\u529f\uff0c\u663e\u8457\u51cf\u5c11\u6b7b\u9501\u548c\u78b0\u649e\u3002\u7ed3\u5408\u5206\u5c42\u4efb\u52a1\u89c4\u5212\uff0c\u5b9e\u73b0\u5f02\u6784\u673a\u5668\u4eba\u534f\u8c03\u5bfc\u822a\uff0c\u5c55\u793a\u51fa\u96f6\u6837\u672c\u6027\u80fd\u3002", "conclusion": "\u5c06\u53cd\u5e94\u5f0fRL\u5bfc\u822a\u4e0e\u9009\u62e9\u6027MAPF\u5e72\u9884\u76f8\u7ed3\u5408\uff0c\u80fd\u4ea7\u751f\u9c81\u68d2\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u6709\u6548\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u62d3\u6251\u6b7b\u9501\u95ee\u9898\u3002"}}
{"id": "2511.22697", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22697", "abs": "https://arxiv.org/abs/2511.22697", "authors": ["Chancharik Mitra", "Yusen Luo", "Raj Saravanan", "Dantong Niu", "Anirudh Pai", "Jesse Thomason", "Trevor Darrell", "Abrar Anwar", "Deva Ramanan", "Roei Herzig"], "title": "Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations", "comment": null, "summary": "Vision-Language Action (VLAs) models promise to extend the remarkable success of vision-language models (VLMs) to robotics. Yet, unlike VLMs in the vision-language domain, VLAs for robotics require finetuning to contend with varying physical factors like robot embodiment, environment characteristics, and spatial relationships of each task. Existing fine-tuning methods lack specificity, adapting the same set of parameters regardless of a task's visual, linguistic, and physical characteristics. Inspired by functional specificity in neuroscience, we hypothesize that it is more effective to finetune sparse model representations specific to a given task. In this work, we introduce Robotic Steering, a finetuning approach grounded in mechanistic interpretability that leverages few-shot demonstrations to identify and selectively finetune task-specific attention heads aligned with the physical, visual, and linguistic requirements of robotic tasks. Through comprehensive on-robot evaluations with a Franka Emika robot arm, we demonstrate that Robotic Steering outperforms LoRA while achieving superior robustness under task variation, reduced computational cost, and enhanced interpretability for adapting VLAs to diverse robotic tasks.", "AI": {"tldr": "Robotic Steering\uff1a\u57fa\u4e8e\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c11\u6837\u672c\u6f14\u793a\u8bc6\u522b\u5e76\u9009\u62e9\u6027\u5fae\u8c03\u4e0e\u673a\u5668\u4eba\u4efb\u52a1\u7269\u7406\u3001\u89c6\u89c9\u548c\u8bed\u8a00\u9700\u6c42\u5bf9\u9f50\u7684\u7279\u5b9a\u6ce8\u610f\u529b\u5934\uff0c\u4f18\u4e8eLoRA\u65b9\u6cd5", "motivation": "\u73b0\u6709VLA\u5fae\u8c03\u65b9\u6cd5\u7f3a\u4e4f\u7279\u5f02\u6027\uff0c\u65e0\u8bba\u4efb\u52a1\u7684\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u7269\u7406\u7279\u6027\u5982\u4f55\u90fd\u8c03\u6574\u76f8\u540c\u7684\u53c2\u6570\u96c6\u3002\u53d7\u795e\u7ecf\u79d1\u5b66\u529f\u80fd\u7279\u5f02\u6027\u542f\u53d1\uff0c\u5047\u8bbe\u5fae\u8c03\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u7a00\u758f\u6a21\u578b\u8868\u793a\u66f4\u6709\u6548", "method": "Robotic Steering\uff1a\u57fa\u4e8e\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u5229\u7528\u5c11\u6837\u672c\u6f14\u793a\u8bc6\u522b\u5e76\u9009\u62e9\u6027\u5fae\u8c03\u4e0e\u673a\u5668\u4eba\u4efb\u52a1\u7269\u7406\u3001\u89c6\u89c9\u548c\u8bed\u8a00\u9700\u6c42\u5bf9\u9f50\u7684\u4efb\u52a1\u7279\u5b9a\u6ce8\u610f\u529b\u5934", "result": "\u5728Franka Emika\u673a\u68b0\u81c2\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cRobotic Steering\u4f18\u4e8eLoRA\uff0c\u5728\u4efb\u52a1\u53d8\u5316\u4e0b\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\uff0c\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\uff0c\u9002\u5e94\u4e0d\u540c\u673a\u5668\u4eba\u4efb\u52a1\u7684\u53ef\u89e3\u91ca\u6027\u66f4\u5f3a", "conclusion": "Robotic Steering\u901a\u8fc7\u9009\u62e9\u6027\u5fae\u8c03\u4efb\u52a1\u7279\u5b9a\u6ce8\u610f\u529b\u5934\uff0c\u4e3aVLA\u6a21\u578b\u9002\u5e94\u4e0d\u540c\u673a\u5668\u4eba\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u3001\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5"}}
{"id": "2511.22705", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.22705", "abs": "https://arxiv.org/abs/2511.22705", "authors": ["Ian Lalonde", "Jeff Denis", "Mathieu Lamy", "Camille Martin", "Karina Lebel", "Alexandre Girard"], "title": "A Two Degrees-of-Freedom Floor-Based Robot for Transfer and Rehabilitation Applications", "comment": "13 pages, 16 figures", "summary": "The ability to accomplish a sit-to-stand (STS) motion is key to increase functional mobility and reduce rehospitalization risks. While raising aid (transfer) devices and partial bodyweight support (rehabilitation) devices exist, both are unable to adjust the STS training to different mobility levels. Therefore, We have developed an STS training device that allows various configurations of impedance and vertical/forward forces to adapt to many training needs while maintaining commercial raising aid transfer capabilities. Experiments with healthy adults (both men and women) of various heights and weights show that the device 1) has a low impact on the natural STS kinematics, 2) can provide precise weight unloading at the patient's center of mass and 3) can add a forward virtual spring to assist the transfer of the bodyweight to the feet for seat-off, at the start of the STS motion.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u578b\u5750\u7acb\u8bad\u7ec3\u8bbe\u5907\uff0c\u80fd\u591f\u901a\u8fc7\u8c03\u8282\u963b\u6297\u548c\u5782\u76f4/\u524d\u5411\u529b\u6765\u9002\u5e94\u4e0d\u540c\u6d3b\u52a8\u80fd\u529b\u6c34\u5e73\uff0c\u540c\u65f6\u4fdd\u6301\u5546\u4e1a\u5347\u964d\u8f85\u52a9\u8bbe\u5907\u7684\u8f6c\u79fb\u529f\u80fd\u3002", "motivation": "\u73b0\u6709\u5347\u964d\u8f85\u52a9\u8bbe\u5907\u548c\u90e8\u5206\u4f53\u91cd\u652f\u6491\u8bbe\u5907\u65e0\u6cd5\u6839\u636e\u4e0d\u540c\u7684\u6d3b\u52a8\u80fd\u529b\u6c34\u5e73\u8c03\u6574\u5750\u7acb\u8bad\u7ec3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u591a\u79cd\u8bad\u7ec3\u9700\u6c42\u540c\u65f6\u4fdd\u6301\u8f6c\u79fb\u529f\u80fd\u7684\u8bbe\u5907\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5750\u7acb\u8bad\u7ec3\u8bbe\u5907\uff0c\u5141\u8bb8\u914d\u7f6e\u4e0d\u540c\u7684\u963b\u6297\u548c\u5782\u76f4/\u524d\u5411\u529b\uff0c\u4ee5\u9002\u914d\u591a\u79cd\u8bad\u7ec3\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u5546\u4e1a\u5347\u964d\u8f85\u52a9\u8bbe\u5907\u7684\u8f6c\u79fb\u80fd\u529b\u3002", "result": "\u5065\u5eb7\u6210\u5e74\u4eba\u5b9e\u9a8c\u8868\u660e\uff1a1\uff09\u8bbe\u5907\u5bf9\u81ea\u7136\u5750\u7acb\u8fd0\u52a8\u5b66\u5f71\u54cd\u5c0f\uff1b2\uff09\u80fd\u5728\u60a3\u8005\u8d28\u5fc3\u63d0\u4f9b\u7cbe\u786e\u7684\u51cf\u91cd\u652f\u6301\uff1b3\uff09\u80fd\u5728\u5750\u7acb\u8fd0\u52a8\u5f00\u59cb\u65f6\u589e\u52a0\u865a\u62df\u524d\u5411\u5f39\u7c27\uff0c\u8f85\u52a9\u4f53\u91cd\u8f6c\u79fb\u5230\u811a\u90e8\u5b8c\u6210\u79bb\u5ea7\u52a8\u4f5c\u3002", "conclusion": "\u8be5\u8bbe\u5907\u80fd\u591f\u6709\u6548\u9002\u5e94\u4e0d\u540c\u8bad\u7ec3\u9700\u6c42\uff0c\u5728\u4fdd\u6301\u8f6c\u79fb\u529f\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u7cbe\u786e\u7684\u51cf\u91cd\u652f\u6301\u548c\u8fd0\u52a8\u8f85\u52a9\uff0c\u6709\u671b\u63d0\u9ad8\u529f\u80fd\u6027\u6d3b\u52a8\u80fd\u529b\u5e76\u964d\u4f4e\u518d\u4f4f\u9662\u98ce\u9669\u3002"}}
{"id": "2511.22744", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22744", "abs": "https://arxiv.org/abs/2511.22744", "authors": ["R\u00e9my Rahem", "Wael Suleiman"], "title": "Beyond Egocentric Limits: Multi-View Depth-Based Learning for Robust Quadrupedal Locomotion", "comment": "12 pages, 6 figures, code available at https://anonymous.4open.science/r/multiview-parkour-6FB8", "summary": "Recent progress in legged locomotion has allowed highly dynamic and parkour-like behaviors for robots, similar to their biological counterparts. Yet, these methods mostly rely on egocentric (first-person) perception, limiting their performance, especially when the viewpoint of the robot is occluded. A promising solution would be to enhance the robot's environmental awareness by using complementary viewpoints, such as multiple actors exchanging perceptual information. Inspired by this idea, this work proposes a multi-view depth-based locomotion framework that combines egocentric and exocentric observations to provide richer environmental context during agile locomotion. Using a teacher-student distillation approach, the student policy learns to fuse proprioception with dual depth streams while remaining robust to real-world sensing imperfections. To further improve robustness, we introduce extensive domain randomization, including stochastic remote-camera dropouts and 3D positional perturbations that emulate aerial-ground cooperative sensing. Simulation results show that multi-viewpoints policies outperform single-viewpoint baseline in gap crossing, step descent, and other dynamic maneuvers, while maintaining stability when the exocentric camera is partially or completely unavailable. Additional experiments show that moderate viewpoint misalignment is well tolerated when incorporated during training. This study demonstrates that heterogeneous visual feedback improves robustness and agility in quadrupedal locomotion. Furthermore, to support reproducibility, the implementation accompanying this work is publicly available at https://anonymous.4open.science/r/multiview-parkour-6FB8", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u89c6\u89d2\u6df1\u5ea6\u611f\u77e5\u7684\u817f\u90e8\u8fd0\u52a8\u6846\u67b6\uff0c\u7ed3\u5408\u7b2c\u4e00\u4eba\u79f0\u548c\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\uff0c\u901a\u8fc7\u5e08\u751f\u84b8\u998f\u65b9\u6cd5\u5b66\u4e60\u878d\u5408\u672c\u4f53\u611f\u77e5\u548c\u53cc\u6df1\u5ea6\u6d41\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u654f\u6377\u6027\u3002", "motivation": "\u73b0\u6709\u817f\u90e8\u8fd0\u52a8\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7b2c\u4e00\u4eba\u79f0\u611f\u77e5\uff0c\u5f53\u673a\u5668\u4eba\u89c6\u91ce\u88ab\u906e\u6321\u65f6\u6027\u80fd\u53d7\u9650\u3002\u9700\u8981\u589e\u5f3a\u73af\u5883\u611f\u77e5\u80fd\u529b\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u4e92\u8865\u4fe1\u606f\u6765\u6539\u5584\u52a8\u6001\u8fd0\u52a8\u8868\u73b0\u3002", "method": "\u91c7\u7528\u5e08\u751f\u84b8\u998f\u65b9\u6cd5\uff0c\u5b66\u751f\u7b56\u7565\u5b66\u4e60\u878d\u5408\u672c\u4f53\u611f\u77e5\u548c\u53cc\u6df1\u5ea6\u6d41\uff08\u7b2c\u4e00\u4eba\u79f0\u548c\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\uff09\u3002\u5f15\u5165\u5e7f\u6cdb\u7684\u9886\u57df\u968f\u673a\u5316\uff0c\u5305\u62ec\u968f\u673a\u8fdc\u7a0b\u76f8\u673a\u4e22\u5931\u548c3D\u4f4d\u7f6e\u6270\u52a8\uff0c\u6a21\u62df\u7a7a\u5730\u534f\u540c\u611f\u77e5\u3002", "result": "\u591a\u89c6\u89d2\u7b56\u7565\u5728\u8de8\u8d8a\u95f4\u9699\u3001\u53f0\u9636\u4e0b\u964d\u7b49\u52a8\u6001\u64cd\u4f5c\u4e2d\u4f18\u4e8e\u5355\u89c6\u89d2\u57fa\u7ebf\uff0c\u5728\u7b2c\u4e09\u4eba\u79f0\u76f8\u673a\u90e8\u5206\u6216\u5b8c\u5168\u4e0d\u53ef\u7528\u65f6\u4ecd\u80fd\u4fdd\u6301\u7a33\u5b9a\u6027\u3002\u9002\u5ea6\u7684\u89c6\u89d2\u504f\u5dee\u5728\u8bad\u7ec3\u4e2d\u88ab\u826f\u597d\u5bb9\u5fcd\u3002", "conclusion": "\u5f02\u6784\u89c6\u89c9\u53cd\u9988\u663e\u8457\u63d0\u9ad8\u4e86\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u9c81\u68d2\u6027\u548c\u654f\u6377\u6027\u3002\u591a\u89c6\u89d2\u611f\u77e5\u6846\u67b6\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u817f\u90e8\u8fd0\u52a8\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.22773", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22773", "abs": "https://arxiv.org/abs/2511.22773", "authors": ["Rui Heng Yang", "Xuan Zhao", "Leo Maxime Brunswic", "Montgomery Alban", "Mateo Clemente", "Tongtong Cao", "Jun Jin", "Amir Rasouli"], "title": "CAPE: Context-Aware Diffusion Policy Via Proximal Mode Expansion for Collision Avoidance", "comment": "4 tables, 9 figures", "summary": "In robotics, diffusion models can capture multi-modal trajectories from demonstrations, making them a transformative approach in imitation learning. However, achieving optimal performance following this regiment requires a large-scale dataset, which is costly to obtain, especially for challenging tasks, such as collision avoidance. In those tasks, generalization at test time demands coverage of many obstacles types and their spatial configurations, which are impractical to acquire purely via data. To remedy this problem, we propose Context-Aware diffusion policy via Proximal mode Expansion (CAPE), a framework that expands trajectory distribution modes with context-aware prior and guidance at inference via a novel prior-seeded iterative guided refinement procedure. The framework generates an initial trajectory plan and executes a short prefix trajectory, and then the remaining trajectory segment is perturbed to an intermediate noise level, forming a trajectory prior. Such a prior is context-aware and preserves task intent. Repeating the process with context-aware guided denoising iteratively expands mode support to allow finding smoother, less collision-prone trajectories. For collision avoidance, CAPE expands trajectory distribution modes with collision-aware context, enabling the sampling of collision-free trajectories in previously unseen environments while maintaining goal consistency. We evaluate CAPE on diverse manipulation tasks in cluttered unseen simulated and real-world settings and show up to 26% and 80% higher success rates respectively compared to SOTA methods, demonstrating better generalization to unseen environments.", "AI": {"tldr": "CAPE\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6269\u6563\u6a21\u578b\u6269\u5c55\u8f68\u8ff9\u5206\u5e03\u6a21\u5f0f\uff0c\u5728\u672a\u89c1\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u907f\u969c\u548c\u6cdb\u5316\u80fd\u529b", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\u80fd\u6355\u6349\u591a\u6a21\u6001\u8f68\u8ff9\uff0c\u4f46\u9700\u8981\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u8fd9\u5728\u907f\u969c\u7b49\u590d\u6742\u4efb\u52a1\u4e2d\u6210\u672c\u9ad8\u6602\u3002\u6cdb\u5316\u9700\u8981\u8986\u76d6\u591a\u79cd\u969c\u788d\u7269\u7c7b\u578b\u548c\u7a7a\u95f4\u914d\u7f6e\uff0c\u7eaf\u6570\u636e\u83b7\u53d6\u4e0d\u73b0\u5b9e\u3002", "method": "\u63d0\u51faCAPE\u6846\u67b6\uff1a\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u5148\u9a8c\u548c\u5f15\u5bfc\u6269\u5c55\u8f68\u8ff9\u5206\u5e03\u6a21\u5f0f\uff0c\u91c7\u7528\u5148\u9a8c\u79cd\u5b50\u8fed\u4ee3\u5f15\u5bfc\u7ec6\u5316\u8fc7\u7a0b\u3002\u5148\u751f\u6210\u521d\u59cb\u8f68\u8ff9\u5e76\u6267\u884c\u524d\u7f00\uff0c\u7136\u540e\u5c06\u5269\u4f59\u8f68\u8ff9\u6270\u52a8\u5230\u4e2d\u95f4\u566a\u58f0\u6c34\u5e73\u5f62\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u5148\u9a8c\uff0c\u518d\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u5f15\u5bfc\u53bb\u566a\u8fed\u4ee3\u6269\u5c55\u6a21\u5f0f\u652f\u6301\u3002", "result": "\u5728\u6742\u4e71\u672a\u89c1\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u591a\u6837\u5316\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4SOTA\u65b9\u6cd5\u5206\u522b\u5b9e\u73b026%\u548c80%\u66f4\u9ad8\u7684\u6210\u529f\u7387\uff0c\u5728\u672a\u89c1\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CAPE\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6269\u6563\u6a21\u578b\u6709\u6548\u6269\u5c55\u8f68\u8ff9\u5206\u5e03\u6a21\u5f0f\uff0c\u80fd\u591f\u5728\u672a\u89c1\u73af\u5883\u4e2d\u91c7\u6837\u65e0\u78b0\u649e\u8f68\u8ff9\u5e76\u4fdd\u6301\u76ee\u6807\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u9700\u8981\u5927\u89c4\u6a21\u6570\u636e\u7684\u95ee\u9898\u3002"}}
{"id": "2511.22777", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22777", "abs": "https://arxiv.org/abs/2511.22777", "authors": ["Sajjad Pakdamansavoji", "Mozhgan Pourkeshavarz", "Adam Sigal", "Zhiyuan Li", "Rui Heng Yang", "Amir Rasouli"], "title": "Improving Robotic Manipulation Robustness via NICE Scene Surgery", "comment": "11 figures, 3 tables", "summary": "Learning robust visuomotor policies for robotic manipulation remains a challenge in real-world settings, where visual distractors can significantly degrade performance and safety. In this work, we propose an effective and scalable framework, Naturalistic Inpainting for Context Enhancement (NICE). Our method minimizes out-of-distribution (OOD) gap in imitation learning by increasing visual diversity through construction of new experiences using existing demonstrations. By utilizing image generative frameworks and large language models, NICE performs three editing operations, object replacement, restyling, and removal of distracting (non-target) objects. These changes preserve spatial relationships without obstructing target objects and maintain action-label consistency. Unlike previous approaches, NICE requires no additional robot data collection, simulator access, or custom model training, making it readily applicable to existing robotic datasets.\n  Using real-world scenes, we showcase the capability of our framework in producing photo-realistic scene enhancement. For downstream tasks, we use NICE data to finetune a vision-language model (VLM) for spatial affordance prediction and a vision-language-action (VLA) policy for object manipulation. Our evaluations show that NICE successfully minimizes OOD gaps, resulting in over 20% improvement in accuracy for affordance prediction in highly cluttered scenes. For manipulation tasks, success rate increases on average by 11% when testing in environments populated with distractors in different quantities. Furthermore, we show that our method improves visual robustness, lowering target confusion by 6%, and enhances safety by reducing collision rate by 7%.", "AI": {"tldr": "NICE\u6846\u67b6\u901a\u8fc7\u56fe\u50cf\u751f\u6210\u548c\u8bed\u8a00\u6a21\u578b\u5bf9\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\u8fdb\u884c\u81ea\u7136\u7f16\u8f91\uff0c\u589e\u52a0\u89c6\u89c9\u591a\u6837\u6027\u4ee5\u51cf\u5c11\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u5916\u5dee\u8ddd\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6536\u96c6\u6216\u8bad\u7ec3", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u89c6\u89c9\u5e72\u6270\u7269\u4f1a\u663e\u8457\u964d\u4f4e\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\uff0c\u9700\u8981\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u5916\u6cdb\u5316\u95ee\u9898", "method": "\u4f7f\u7528\u56fe\u50cf\u751f\u6210\u6846\u67b6\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u73b0\u6709\u6f14\u793a\u6570\u636e\u8fdb\u884c\u4e09\u79cd\u7f16\u8f91\u64cd\u4f5c\uff1a\u7269\u4f53\u66ff\u6362\u3001\u91cd\u65b0\u98ce\u683c\u5316\u3001\u79fb\u9664\u5e72\u6270\u7269\uff0c\u4fdd\u6301\u7a7a\u95f4\u5173\u7cfb\u548c\u52a8\u4f5c\u6807\u7b7e\u4e00\u81f4\u6027", "result": "\u5728\u9ad8\u5ea6\u6742\u4e71\u573a\u666f\u4e2d\uff0c\u7a7a\u95f4\u53ef\u4f9b\u6027\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc720%\uff1b\u5728\u542b\u5e72\u6270\u7269\u7684\u73af\u5883\u4e2d\uff0c\u64cd\u4f5c\u4efb\u52a1\u6210\u529f\u7387\u5e73\u5747\u63d0\u534711%\uff1b\u76ee\u6807\u6df7\u6dc6\u7387\u964d\u4f4e6%\uff0c\u78b0\u649e\u7387\u964d\u4f4e7%", "conclusion": "NICE\u6846\u67b6\u80fd\u6709\u6548\u589e\u5f3a\u673a\u5668\u4eba\u89c6\u89c9\u9c81\u68d2\u6027\uff0c\u51cf\u5c11\u5206\u5e03\u5916\u5dee\u8ddd\uff0c\u63d0\u9ad8\u64cd\u4f5c\u6027\u80fd\u548c\u5b89\u5168\uff0c\u4e14\u65e0\u9700\u989d\u5916\u6570\u636e\u6536\u96c6\u6216\u5b9a\u5236\u6a21\u578b\u8bad\u7ec3"}}
{"id": "2511.22780", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22780", "abs": "https://arxiv.org/abs/2511.22780", "authors": ["Amir Rasouli", "Montgomery Alban", "Sajjad Pakdamansavoji", "Zhiyuan Li", "Zhanguang Zhang", "Aaron Wu", "Xuan Zhao"], "title": "Distracted Robot: How Visual Clutter Undermine Robotic Manipulation", "comment": "12 figures, 2 tables", "summary": "In this work, we propose an evaluation protocol for examining the performance of robotic manipulation policies in cluttered scenes. Contrary to prior works, we approach evaluation from a psychophysical perspective, therefore we use a unified measure of clutter that accounts for environmental factors as well as the distractors quantity, characteristics, and arrangement. Using this measure, we systematically construct evaluation scenarios in both hyper-realistic simulation and real-world and conduct extensive experimentation on manipulation policies, in particular vision-language-action (VLA) models. Our experiments highlight the significant impact of scene clutter, lowering the performance of the policies, by as much as 34% and show that despite achieving similar average performance across the tasks, different VLA policies have unique vulnerabilities and a relatively low agreement on success scenarios. We further show that our clutter measure is an effective indicator of performance degradation and analyze the impact of distractors in terms of their quantity and occluding influence. At the end, we show that finetuning on enhanced data, although effective, does not equally remedy all negative impacts of clutter on performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5fc3\u7406\u7269\u7406\u5b66\u89d2\u5ea6\u8bc4\u4f30\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u5728\u6742\u4e71\u573a\u666f\u4e2d\u6027\u80fd\u7684\u534f\u8bae\uff0c\u4f7f\u7528\u7edf\u4e00\u7684\u6742\u4e71\u5ea6\u91cf\uff0c\u5e76\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4e2d\u7cfb\u7edf\u6784\u5efa\u8bc4\u4f30\u573a\u666f\uff0c\u6d4b\u8bd5\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u53d1\u73b0\u6742\u4e71\u573a\u666f\u663e\u8457\u964d\u4f4e\u7b56\u7565\u6027\u80fd\u8fbe34%\uff0c\u4e0d\u540cVLA\u7b56\u7565\u6709\u72ec\u7279\u5f31\u70b9\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u6742\u4e71\u573a\u666f\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u5f71\u54cd\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u6742\u4e71\u5ea6\u91cf\u6807\u51c6\u3002\u9700\u8981\u4ece\u5fc3\u7406\u7269\u7406\u5b66\u89d2\u5ea6\u51fa\u53d1\uff0c\u7efc\u5408\u8003\u8651\u73af\u5883\u56e0\u7d20\u3001\u5e72\u6270\u7269\u6570\u91cf\u3001\u7279\u5f81\u548c\u6392\u5217\uff0c\u7cfb\u7edf\u8bc4\u4f30\u7b56\u7565\u5728\u6742\u4e71\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u6742\u4e71\u5ea6\u91cf\u65b9\u6cd5\uff0c\u8003\u8651\u73af\u5883\u56e0\u7d20\u548c\u5e72\u6270\u7269\u7279\u6027\uff1b\u5728\u8d85\u771f\u5b9e\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4e2d\u7cfb\u7edf\u6784\u5efa\u8bc4\u4f30\u573a\u666f\uff1b\u5bf9\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff1b\u5206\u6790\u6742\u4e71\u5ea6\u91cf\u4e0e\u6027\u80fd\u4e0b\u964d\u7684\u5173\u7cfb\uff1b\u7814\u7a76\u5fae\u8c03\u5bf9\u6742\u4e71\u573a\u666f\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u573a\u666f\u6742\u4e71\u663e\u8457\u964d\u4f4e\u7b56\u7565\u6027\u80fd\uff0c\u6700\u591a\u8fbe34%\uff1b\u4e0d\u540cVLA\u7b56\u7565\u867d\u7136\u5e73\u5747\u6027\u80fd\u76f8\u4f3c\uff0c\u4f46\u5404\u6709\u72ec\u7279\u5f31\u70b9\uff0c\u6210\u529f\u573a\u666f\u7684\u4e00\u81f4\u6027\u8f83\u4f4e\uff1b\u63d0\u51fa\u7684\u6742\u4e71\u5ea6\u91cf\u80fd\u6709\u6548\u6307\u793a\u6027\u80fd\u4e0b\u964d\uff1b\u5e72\u6270\u7269\u6570\u91cf\u548c\u906e\u6321\u5f71\u54cd\u663e\u8457\uff1b\u5fae\u8c03\u867d\u6709\u6548\uff0c\u4f46\u4e0d\u80fd\u540c\u7b49\u7f13\u89e3\u6742\u4e71\u5e26\u6765\u7684\u6240\u6709\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u6742\u4e71\u573a\u666f\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u6709\u663e\u8457\u8d1f\u9762\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002\u63d0\u51fa\u7684\u5fc3\u7406\u7269\u7406\u5b66\u8bc4\u4f30\u534f\u8bae\u548c\u7edf\u4e00\u6742\u4e71\u5ea6\u91cf\u80fd\u6709\u6548\u8bc4\u4f30\u7b56\u7565\u9c81\u68d2\u6027\u3002\u4e0d\u540cVLA\u7b56\u7565\u5728\u6742\u4e71\u573a\u666f\u4e2d\u8868\u73b0\u5dee\u5f02\u660e\u663e\uff0c\u9700\u8981\u9488\u5bf9\u6027\u6539\u8fdb\u3002\u5fae\u8c03\u4e0d\u80fd\u5b8c\u5168\u89e3\u51b3\u6742\u4e71\u5e26\u6765\u7684\u6311\u6218\u3002"}}
{"id": "2511.22829", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22829", "abs": "https://arxiv.org/abs/2511.22829", "authors": ["Zhen Tian", "Zhihao Lin"], "title": "Safe Autonomous Lane Changing: Planning with Dynamic Risk Fields and Time-Varying Convex Space Generation", "comment": null, "summary": "This paper presents a novel trajectory planning pipeline for complex driving scenarios like autonomous lane changing, by integrating risk-aware planning with guaranteed collision avoidance into a unified optimization framework. We first construct a dynamic risk fields (DRF) that captures both the static and dynamic collision risks from surrounding vehicles. Then, we develop a rigorous strategy for generating time-varying convex feasible spaces that ensure kinematic feasibility and safety requirements. The trajectory planning problem is formulated as a finite-horizon optimal control problem and solved using a constrained iterative Linear Quadratic Regulator (iLQR) algorithm that jointly optimizes trajectory smoothness, control effort, and risk exposure while maintaining strict feasibility. Extensive simulations demonstrate that our method outperforms traditional approaches in terms of safety and efficiency, achieving collision-free trajectories with shorter lane-changing distances (28.59 m) and times (2.84 s) while maintaining smooth and comfortable acceleration patterns. In dense roundabout environments the planner further demonstrates robust adaptability, producing larger safety margins, lower jerk, and superior curvature smoothness compared with APF, MPC, and RRT based baselines. These results confirm that the integrated DRF with convex feasible space and constrained iLQR solver provides a balanced solution for safe, efficient, and comfortable trajectory generation in dynamic and interactive traffic scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u98ce\u9669\u611f\u77e5\u89c4\u5212\u4e0e\u4fdd\u8bc1\u78b0\u649e\u907f\u514d\u7684\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u53d8\u9053\u7b49\u590d\u6742\u573a\u666f\uff0c\u901a\u8fc7\u52a8\u6001\u98ce\u9669\u573a\u3001\u65f6\u53d8\u51f8\u53ef\u884c\u7a7a\u95f4\u548c\u7ea6\u675fiLQR\u6c42\u89e3\u5668\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684\u8f68\u8ff9\u751f\u6210\u3002", "motivation": "\u4f20\u7edf\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u5728\u590d\u6742\u52a8\u6001\u4ea4\u901a\u573a\u666f\u4e2d\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u8212\u9002\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6574\u5408\u98ce\u9669\u611f\u77e5\u3001\u4e25\u683c\u78b0\u649e\u907f\u514d\u548c\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u7684\u7edf\u4e00\u4f18\u5316\u6846\u67b6\u3002", "method": "1. \u6784\u5efa\u52a8\u6001\u98ce\u9669\u573a(DRF)\u6355\u6349\u9759\u6001\u548c\u52a8\u6001\u78b0\u649e\u98ce\u9669\uff1b2. \u751f\u6210\u65f6\u53d8\u51f8\u53ef\u884c\u7a7a\u95f4\u4fdd\u8bc1\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u548c\u5b89\u5168\u8981\u6c42\uff1b3. \u5c06\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\u5efa\u6a21\u4e3a\u6709\u9650\u65f6\u57df\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u4f7f\u7528\u7ea6\u675f\u8fed\u4ee3\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668(iLQR)\u7b97\u6cd5\u8054\u5408\u4f18\u5316\u8f68\u8ff9\u5e73\u6ed1\u5ea6\u3001\u63a7\u5236\u52aa\u529b\u548c\u98ce\u9669\u66b4\u9732\u3002", "result": "\u5728\u53d8\u9053\u573a\u666f\u4e2d\u5b9e\u73b0\u66f4\u77ed\u7684\u53d8\u9053\u8ddd\u79bb(28.59 m)\u548c\u65f6\u95f4(2.84 s)\uff0c\u4fdd\u6301\u5e73\u6ed1\u8212\u9002\u7684\u52a0\u901f\u5ea6\u6a21\u5f0f\uff1b\u5728\u5bc6\u96c6\u73af\u5c9b\u73af\u5883\u4e2d\u76f8\u6bd4APF\u3001MPC\u548cRRT\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u5927\u7684\u5b89\u5168\u88d5\u5ea6\u3001\u66f4\u4f4e\u7684\u6025\u52a8\u5ea6\u548c\u66f4\u4f18\u7684\u66f2\u7387\u5e73\u6ed1\u6027\u3002", "conclusion": "\u96c6\u6210\u7684DRF\u3001\u51f8\u53ef\u884c\u7a7a\u95f4\u548c\u7ea6\u675fiLQR\u6c42\u89e3\u5668\u4e3a\u52a8\u6001\u4ea4\u4e92\u4ea4\u901a\u573a\u666f\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u9ad8\u6548\u4e14\u8212\u9002\u7684\u8f68\u8ff9\u751f\u6210\u5e73\u8861\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u590d\u6742\u9a7e\u9a76\u573a\u666f\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2511.22847", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22847", "abs": "https://arxiv.org/abs/2511.22847", "authors": ["Yuying Zhang", "Na Fan", "Haowen Zheng", "Junning Liang", "Zongliang Pan", "Qifeng Chen", "Ximin Lyu"], "title": "Threat-Aware UAV Dodging of Human-Thrown Projectiles with an RGB-D Camera", "comment": null, "summary": "Uncrewed aerial vehicles (UAVs) performing tasks such as transportation and aerial photography are vulnerable to intentional projectile attacks from humans. Dodging such a sudden and fast projectile poses a significant challenge for UAVs, requiring ultra-low latency responses and agile maneuvers. Drawing inspiration from baseball, in which pitchers' body movements are analyzed to predict the ball's trajectory, we propose a novel real-time dodging system that leverages an RGB-D camera. Our approach integrates human pose estimation with depth information to predict the attacker's motion trajectory and the subsequent projectile trajectory. Additionally, we introduce an uncertainty-aware dodging strategy to enable the UAV to dodge incoming projectiles efficiently. Our perception system achieves high prediction accuracy and outperforms the baseline in effective distance and latency. The dodging strategy addresses temporal and spatial uncertainties to ensure UAV safety. Extensive real-world experiments demonstrate the framework's reliable dodging capabilities against sudden attacks and its outstanding robustness across diverse scenarios.", "AI": {"tldr": "\u57fa\u4e8eRGB-D\u76f8\u673a\u548c\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u65e0\u4eba\u673a\u5b9e\u65f6\u8eb2\u907f\u7cfb\u7edf\uff0c\u901a\u8fc7\u9884\u6d4b\u653b\u51fb\u8005\u52a8\u4f5c\u548c\u629b\u5c04\u7269\u8f68\u8ff9\u6765\u5e94\u5bf9\u7a81\u53d1\u653b\u51fb", "motivation": "\u65e0\u4eba\u673a\u5728\u6267\u884c\u8fd0\u8f93\u3001\u822a\u62cd\u7b49\u4efb\u52a1\u65f6\u5bb9\u6613\u53d7\u5230\u4eba\u4e3a\u629b\u5c04\u7269\u653b\u51fb\uff0c\u9700\u8981\u8d85\u4f4e\u5ef6\u8fdf\u54cd\u5e94\u548c\u654f\u6377\u673a\u52a8\u80fd\u529b\u6765\u8eb2\u907f\u5feb\u901f\u7a81\u53d1\u7684\u653b\u51fb", "method": "\u7ed3\u5408RGB-D\u76f8\u673a\u3001\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u6df1\u5ea6\u4fe1\u606f\u9884\u6d4b\u653b\u51fb\u8005\u52a8\u4f5c\u8f68\u8ff9\u548c\u629b\u5c04\u7269\u8f68\u8ff9\uff0c\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u8eb2\u907f\u7b56\u7565", "result": "\u611f\u77e5\u7cfb\u7edf\u8fbe\u5230\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5728\u6709\u6548\u8ddd\u79bb\u548c\u5ef6\u8fdf\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\uff0c\u8eb2\u907f\u7b56\u7565\u80fd\u5904\u7406\u65f6\u7a7a\u4e0d\u786e\u5b9a\u6027\uff0c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u53ef\u9760\u8eb2\u907f\u80fd\u529b\u548c\u9c81\u68d2\u6027", "conclusion": "\u63d0\u51fa\u7684\u5b9e\u65f6\u8eb2\u907f\u7cfb\u7edf\u80fd\u6709\u6548\u5e94\u5bf9\u7a81\u53d1\u629b\u5c04\u7269\u653b\u51fb\uff0c\u5728\u5404\u79cd\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u65e0\u4eba\u673a\u5b89\u5168\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848"}}
