<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 47]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Using Natural Language for Human-Robot Collaboration in the Real World](https://arxiv.org/abs/2508.11759)
*Peter Lindes,Kaoutar Skiker*

Main category: cs.RO

TL;DR: 本文探讨了如何通过大语言模型提升机器人的自然语言理解能力，以实现人机协作执行复杂任务的远景。


<details>
  <summary>Details</summary>
Motivation: 传统交互式任务学习系统的语言理解能力有限，而大语言模型为提升机器人语言能力提供了机会，但需要解决与物理世界操作的集成挑战。

Method: 提出了一种以认知代理为核心的AI系统方案，让机器人与人类和LLM交互，通过经验累积情境知识。使用ChatGPT进行了三个理解自然语言挑战的概念验证实验。

Result: 提供了基于LLM的语言理解在机器人系统中的可行性证据，为建立集成化语言协作机器助手系统奠定了基础。

Conclusion: 通过大语言模型的集成可以实现更自然的人机协作，但需要将概念验证转化为可操作的集成系统，这是实现自主机器人协作助手远景的关键步骤。

Abstract: We have a vision of a day when autonomous robots can collaborate with humans
as assistants in performing complex tasks in the physical world. This vision
includes that the robots will have the ability to communicate with their human
collaborators using language that is natural to the humans. Traditional
Interactive Task Learning (ITL) systems have some of this ability, but the
language they can understand is very limited. The advent of large language
models (LLMs) provides an opportunity to greatly improve the language
understanding of robots, yet integrating the language abilities of LLMs with
robots that operate in the real physical world is a challenging problem.
  In this chapter we first review briefly a few commercial robot products that
work closely with humans, and discuss how they could be much better
collaborators with robust language abilities. We then explore how an AI system
with a cognitive agent that controls a physical robot at its core, interacts
with both a human and an LLM, and accumulates situational knowledge through its
experiences, can be a possible approach to reach that vision. We focus on three
specific challenges of having the robot understand natural language, and
present a simple proof-of-concept experiment using ChatGPT for each. Finally,
we discuss what it will take to turn these simple experiments into an
operational system where LLM-assisted language understanding is a part of an
integrated robotic assistant that uses language to collaborate with humans.

</details>


### [2] [Anticipatory and Adaptive Footstep Streaming for Teleoperated Bipedal Robots](https://arxiv.org/abs/2508.11802)
*Luigi Penco,Beomyeong Park,Stefan Fasano,Nehar Poddar,Stephen McCrory,Nicholas Kitchel,Tomasz Bialek,Dexton Anderson,Duncan Calvert,Robert Griffin*

Main category: cs.RO

TL;DR: 通过预测用户脚步并重定向到机器人脚步位置，实现了高速任务中用户与机器人运动的实时同步


<details>
  <summary>Details</summary>
Motivation: 解决高速任务中用户与机器人运动同步的挑战，特别是在平坦地面与不平地形环境差异的情况下

Method: 预测用户脚步位置，将其重定向到机器人脚步位置，让机器人利用自身动力学进行移动，并根据环境自主调整步伐

Result: 在Nadia人型机器人上的实验结果证明了系统的有效性

Conclusion: 该方法能够有效减少延迟，确保机器人平衡稳定，并适应不同地形环境

Abstract: Achieving seamless synchronization between user and robot motion in
teleoperation, particularly during high-speed tasks, remains a significant
challenge. In this work, we propose a novel approach for transferring stepping
motions from the user to the robot in real-time. Instead of directly
replicating user foot poses, we retarget user steps to robot footstep
locations, allowing the robot to utilize its own dynamics for locomotion,
ensuring better balance and stability. Our method anticipates user footsteps to
minimize delays between when the user initiates and completes a step and when
the robot does it. The step estimates are continuously adapted to converge with
the measured user references. Additionally, the system autonomously adjusts the
robot's steps to account for its surrounding terrain, overcoming challenges
posed by environmental mismatches between the user's flat-ground setup and the
robot's uneven terrain. Experimental results on the humanoid robot Nadia
demonstrate the effectiveness of the proposed system.

</details>


### [3] [LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba](https://arxiv.org/abs/2508.11849)
*Allen Wang,Gavin Tao*

Main category: cs.RO

TL;DR: LocoMamba是一个基于Mamba选择性状态空间模型的视觉驱动跨模态DRL框架，实现了近线性时间序列建模，能够高效捕获长距离依赖关系，在具有挑战性的模拟环境中表现出优越的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统方法在序列建模中的计算复杂度和长距离依赖捕获问题，同时提高训练效率和泛化能力，特别是在具有静态/动态障碍物和不平地形的复杂环境中。

Method: 1) 使用MLP嵌入本体感知状态，CNN处理深度图像生成紧凑token；2) 堆叠Mamba层通过选择性扫描融合token，降低延迟和内存占用；3) 使用PPO进行端到端策略训练，结合地形外观随机化和障碍密度课程学习。

Result: 相比最先进基线方法，LocoMamba获得更高回报和成功率，碰撞更少，对未见地形和障碍密度有更强泛化能力，在相同计算预算下用更少更新次数收敛。

Conclusion: LocoMamba框架通过选择性状态空间模型和高效的token处理机制，在复杂环境中实现了卓越的导航性能、泛化能力和训练效率，为视觉驱动DRL提供了有效的解决方案。

Abstract: We introduce LocoMamba, a vision-driven cross-modal DRL framework built on
selective state-space models, specifically leveraging Mamba, that achieves
near-linear-time sequence modeling, effectively captures long-range
dependencies, and enables efficient training with longer sequences. First, we
embed proprioceptive states with a multilayer perceptron and patchify depth
images with a lightweight convolutional neural network, producing compact
tokens that improve state representation. Second, stacked Mamba layers fuse
these tokens via near-linear-time selective scanning, reducing latency and
memory footprint, remaining robust to token length and image resolution, and
providing an inductive bias that mitigates overfitting. Third, we train the
policy end-to-end with Proximal Policy Optimization under terrain and
appearance randomization and an obstacle-density curriculum, using a compact
state-centric reward that balances progress, smoothness, and safety. We
evaluate our method in challenging simulated environments with static and
moving obstacles as well as uneven terrain. Compared with state-of-the-art
baselines, our method achieves higher returns and success rates with fewer
collisions, exhibits stronger generalization to unseen terrains and obstacle
densities, and improves training efficiency by converging in fewer updates
under the same compute budget.

</details>


### [4] [Data Shift of Object Detection in Autonomous Driving](https://arxiv.org/abs/2508.11868)
*Lida Xu*

Main category: cs.RO

TL;DR: 这篇论文研究了自主驾驶中的数据偏移问题，通过数据偏移检测和CycleGAN数据增帽技术优化YOLOv5目标检测模型，在BDD100K数据集上得到了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 自主驾驶系统中的机器学习模型容易受到数据分布变化的影响，季节、天气等因素导致的数据偏移问题会很大程度下降模型性能。

Method: 系统分析数据偏移问题的复杂性和表现形式，综述数据偏移检测方法，进行数据集分类和平衡处理，构建目标检测模型，并结合CycleGAN数据增帽技术优化YOLOv5框架。

Result: 在BDD100K数据集上的实验结果显示，该方法在目标检测任务中表现出优于基线模型的性能。

Conclusion: 通过系统化的数据偏移检测和数据增帽技术，可以有效提升自主驾驶系统在实际应用中的适应性和稳健性。

Abstract: With the widespread adoption of machine learning technologies in autonomous
driving systems, their role in addressing complex environmental perception
challenges has become increasingly crucial. However, existing machine learning
models exhibit significant vulnerability, as their performance critically
depends on the fundamental assumption that training and testing data satisfy
the independent and identically distributed condition, which is difficult to
guarantee in real-world applications. Dynamic variations in data distribution
caused by seasonal changes, weather fluctuations lead to data shift problems in
autonomous driving systems. This study investigates the data shift problem in
autonomous driving object detection tasks, systematically analyzing its
complexity and diverse manifestations. We conduct a comprehensive review of
data shift detection methods and employ shift detection analysis techniques to
perform dataset categorization and balancing. Building upon this foundation, we
construct an object detection model. To validate our approach, we optimize the
model by integrating CycleGAN-based data augmentation techniques with the
YOLOv5 framework. Experimental results demonstrate that our method achieves
superior performance compared to baseline models on the BDD100K dataset.

</details>


### [5] [Bioinspired underwater soft robots: from biology to robotics and back](https://arxiv.org/abs/2508.11883)
*Lei Li,Boyang Qin,Wenzhuo Gao,Yanyu Li,Yiyuan Zhang,Bo Wang,Shihan Kong,Jian Wang,Dekui He,Junzhi Yu*

Main category: cs.RO

TL;DR: 本文提出了一个双向的生物启发软机器人框架，不仅从生物学中获取灵感，还能通过机器人实验反馈验证生物学假设，并引入跨物种通用设计的新范式。


<details>
  <summary>Details</summary>
Motivation: 现有水下软机器人研究主要是单向的（生物学指导机器人学），缺乏从机器人学到生物学的反馈机制，限制了科学发现和工程应用的协同发展。

Method: 提出整体性双向框架，整合生物学原理、机器人实现和生物学验证，利用软机器人作为实验工具来探索生物功能和测试进化假设。

Result: 软机器人凭借其固有柔顺性在非结构化环境中优于刚性系统，支持海洋探索、操作和医疗应用；同时能够作为验证生物学假设的实验平台。

Conclusion: 通过统一生物学和工程学，软机器人可以推动海洋探索并深化科学发现，但材料耐用性、驱动效率、自主性和智能性等挑战仍需解决。

Abstract: The ocean vast unexplored regions and diverse soft-bodied marine organisms
have spurred interest in bio-inspired underwater soft robotics. Recent advances
have enabled new capabilities in underwater movement, sensing, and interaction.
However, these efforts are largely unidirectional, with biology guiding
robotics while insights from robotics rarely feed back into biology. Here we
propose a holistic, bidirectional framework that integrates biological
principles, robotic implementation, and biological validation. We show that
soft robots can serve as experimental tools to probe biological functions and
even test evolutionary hypotheses. Their inherent compliance also allows them
to outperform rigid systems in unstructured environments, supporting
applications in marine exploration, manipulation, and medicine. Looking
forward, we introduce bio-universal-inspired robotics, a paradigm that
transcends species-specific mimicry by identifying convergent principles across
species to inspire more adaptable designs. Despite rapid progress, challenges
persist in material robustness, actuation efficiency, autonomy, and
intelligence. By uniting biology and engineering, soft robots can advance ocean
exploration and deepen scientific discovery.

</details>


### [6] [From Screen to Stage: Kid Cosmo, A Life-Like, Torque-Controlled Humanoid for Entertainment Robotics](https://arxiv.org/abs/2508.11884)
*Havel Liu,Mingzhang Zhu,Arturo Moises Flores Alvarez,Yuan Hung Lo,Conrad Ku,Federico Parres,Justin Quan,Colin Togashi,Aditya Navghare,Quanyou Wang,Dennis W. Hong*

Main category: cs.RO

TL;DR: 小彩人形机器人Kid Cosmo为娱乐行业而设计，结合视觉表现与技术功能，能够实现稳定的行走和生动的运动生成。


<details>
  <summary>Details</summary>
Motivation: 当今人形机器人多重视功能性而忽视形式美学，娱乐行业需要旗舰性的视觉表现与流畅运动能力的结合。

Method: 开发了儿童尺寸的Kid Cosmo机器人(1.45米高，25千克)，具28个自由度，使用本体感知驱动器实现转矩控制行走和生动运动生成。

Result: 通过全球展示验证了系统的可靠性，能够在同时进行上下半身运动时保持稳定性，展示了旗舰性娱乐机器人的可行性。

Conclusion: Kid Cosmo成功展示了旗舰性娱乐机器人既具备视觉表现力又有技术功能的可能性，为娱乐人形机器人设计开创了新方向。

Abstract: Humanoid robots represent the cutting edge of robotics research, yet their
potential in entertainment remains largely unexplored. Entertainment as a field
prioritizes visuals and form, a principle that contrasts with the purely
functional designs of most contemporary humanoid robots. Designing
entertainment humanoid robots capable of fluid movement presents a number of
unique challenges. In this paper, we present Kid Cosmo, a research platform
designed for robust locomotion and life-like motion generation while imitating
the look and mannerisms of its namesake character from Netflix's movie The
Electric State. Kid Cosmo is a child-sized humanoid robot, standing 1.45 m tall
and weighing 25 kg. It contains 28 degrees of freedom and primarily uses
proprioceptive actuators, enabling torque-control walking and lifelike motion
generation. Following worldwide showcases as part of the movie's press tour, we
present the system architecture, challenges of a functional entertainment robot
and unique solutions, and our initial findings on stability during simultaneous
upper and lower body movement. We demonstrate the viability of
performance-oriented humanoid robots that prioritize both character embodiment
and technical functionality.

</details>


### [7] [Contact-Rich and Deformable Foot Modeling for Locomotion Control of the Human Musculoskeletal System](https://arxiv.org/abs/2508.11885)
*Haixin Gong,Chen Zhang,Yanan Sui*

Main category: cs.RO

TL;DR: 一种新的可变形脚部骨骼机模型，通过两阶段策略训练自然步态，提升了步态模拟的准确性和稳定性


<details>
  <summary>Details</summary>
Motivation: 现有骨骼机模型对脚地接触机制过于简化，限制了模拟人类步态动力学的准确性

Method: 开发了接触丰富的可变形脚部模型，集成到完整的骨骼机系统中，采用两阶段策略训练方法来学习自然步态

Result: 与传统粗糕模型相比，在运动学、动力学和步态稳定性指标上都显著改善，通过人体实验数据验证

Conclusion: 推进了人体骨骼机系统的接触丰富界面模型技术，为需要精确脚地交互控制的人形机器人应用建立了稳健框架

Abstract: The human foot serves as the critical interface between the body and
environment during locomotion. Existing musculoskeletal models typically
oversimplify foot-ground contact mechanics, limiting their ability to
accurately simulate human gait dynamics. We developed a novel contact-rich and
deformable model of the human foot integrated within a complete musculoskeletal
system that captures the complex biomechanical interactions during walking. To
overcome the control challenges inherent in modeling multi-point contacts and
deformable material, we developed a two-stage policy training strategy to learn
natural walking patterns for this interface-enhanced model. Comparative
analysis between our approach and conventional rigid musculoskeletal models
demonstrated improvements in kinematic, kinetic, and gait stability metrics.
Validation against human subject data confirmed that our simulation closely
reproduced real-world biomechanical measurements. This work advances
contact-rich interface modeling for human musculoskeletal systems and
establishes a robust framework that can be extended to humanoid robotics
applications requiring precise foot-ground interaction control.

</details>


### [8] [Saliency-Based Attention Shifting: A Framework for Improving Driver Situational Awareness of Out-of-Label Hazards](https://arxiv.org/abs/2508.11887)
*Yousra Shleibik,Jordan Sinclair,Kerstin Haring*

Main category: cs.RO

TL;DR: 本文提出了一种通过视觉和听觉注意力重定向技术来增强半自动驾驶场景中驾驶员情境意识的框架，以减少接管过程中的风险。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶技术向更高水平发展，需要集成系统来支持人类在决策中的参与。某些场景（如车辆无法识别物体时）需要人类介入，因此情境意识对于降低接管风险至关重要。

Method: 提出了一个结合实时注视跟踪、上下文感知显著性分析和同步视听警报的概念框架，通过视线操纵技术帮助驾驶员保持对潜在危险的关注。

Result: 该框架旨在增强情境意识，主动应对潜在危险，并促进人类与自动驾驶系统之间的有效协作。

Conclusion: 注意力重定向技术可以有效改善半自动驾驶场景中的安全性和人机协作效果，为自动驾驶系统的安全接管提供重要支持。

Abstract: The advent of autonomous driving systems promises to transform transportation
by enhancing safety, efficiency, and comfort. As these technologies evolve
toward higher levels of autonomy, the need for integrated systems that
seamlessly support human involvement in decision-making becomes increasingly
critical. Certain scenarios necessitate human involvement, including those
where the vehicle is unable to identify an object or element in the scene, and
as such cannot take independent action. Therefore, situational awareness is
essential to mitigate potential risks during a takeover, where a driver must
assume control and autonomy from the vehicle. The need for driver attention is
important to avoid collisions with external agents and ensure a smooth
transition during takeover operations. This paper explores the integration of
attention redirection techniques, such as gaze manipulation through targeted
visual and auditory cues, to help drivers maintain focus on emerging hazards
and reduce target fixation in semi-autonomous driving scenarios. We propose a
conceptual framework that combines real-time gaze tracking, context-aware
saliency analysis, and synchronized visual and auditory alerts to enhance
situational awareness, proactively address potential hazards, and foster
effective collaboration between humans and autonomous systems.

</details>


### [9] [Integrating Symbolic RL Planning into a BDI-based Autonomous UAV Framework: System Integration and SIL Validation](https://arxiv.org/abs/2508.11890)
*Sangwoo Jeon,Juchul Shin,YeonJe Cho,Gyeong-Tae Kim,Seongwoo Kim*

Main category: cs.RO

TL;DR: 基于符号强化学习的AMAD-SRL框架，通过结合BDI和符号RL规划，在软件在环模拟中实现了无人机动态任务规划，任务效率提升75%。


<details>
  <summary>Details</summary>
Motivation: 现代无人机任务需要结合符号规划和适应性强化学习的框架，以处理动态复杂环境中的自主决策需求。

Method: 提出AMAD-SRL框架，扩展了AMAD认知多代理架构，集成符号强化学习技术，使用PDDL语言表达领域知识和约束。在SIL环境中验证框架。

Result: 实验结果显示模块集成稳定、规划阶段转换成功，在目标获取场景中通过减少行程距离实现了4倍效率提升。

Conclusion: 该研究为处理复杂无人机任务打下了坚实基础，并提出了进一步提升和验证的方向。

Abstract: Modern autonomous drone missions increasingly require software frameworks
capable of seamlessly integrating structured symbolic planning with adaptive
reinforcement learning (RL). Although traditional rule-based architectures
offer robust structured reasoning for drone autonomy, their capabilities fall
short in dynamically complex operational environments that require adaptive
symbolic planning. Symbolic RL (SRL), using the Planning Domain Definition
Language (PDDL), explicitly integrates domain-specific knowledge and
operational constraints, significantly improving the reliability and safety of
unmanned aerial vehicle (UAV) decision making. In this study, we propose the
AMAD-SRL framework, an extended and refined version of the Autonomous Mission
Agents for Drones (AMAD) cognitive multi-agent architecture, enhanced with
symbolic reinforcement learning for dynamic mission planning and execution. We
validated our framework in a Software-in-the-Loop (SIL) environment structured
identically to an intended Hardware-In-the-Loop Simulation (HILS) platform,
ensuring seamless transition to real hardware. Experimental results demonstrate
stable integration and interoperability of modules, successful transitions
between BDI-driven and symbolic RL-driven planning phases, and consistent
mission performance. Specifically, we evaluate a target acquisition scenario in
which the UAV plans a surveillance path followed by a dynamic reentry path to
secure the target while avoiding threat zones. In this SIL evaluation, mission
efficiency improved by approximately 75% over a coverage-based baseline,
measured by travel distance reduction. This study establishes a robust
foundation for handling complex UAV missions and discusses directions for
further enhancement and validation.

</details>


### [10] [OmniD: Generalizable Robot Manipulation Policy via Image-Based BEV Representation](https://arxiv.org/abs/2508.11898)
*Jilei Mao,Jiarui Guan,Yingjuan Tang,Qirui Hu,Zhihang Li,Junjie Yu,Yongjie Mao,Yunzhe Sun,Shuang Liu,Xiaozhu Ju*

Main category: cs.RO

TL;DR: OmniD是一个多视角融合框架，通过可变形注意力机制将图像观察合成为统一的鸟瞰图表示，解决了视觉运动策略的过拟合问题和多视角信息融合难题。


<details>
  <summary>Details</summary>
Motivation: 视觉运动策略容易在训练数据上过拟合（如固定相机位置和背景），导致在分布外泛化性能下降，且现有方法难以有效融合多视角信息生成3D表示。

Method: 提出Omni-Vision Diffusion Policy (OmniD)框架，使用基于可变形注意力的Omni-Feature Generator (OFG)选择性提取任务相关特征，抑制视角特定噪声和背景干扰，合成统一的鸟瞰图表示。

Result: 在分布内、分布外和少样本实验中，分别比最佳基线模型平均提升11%、17%和84%。

Conclusion: OmniD通过有效的多视角融合和特征选择机制，显著提升了视觉运动策略的泛化性能和3D表示能力。

Abstract: The visuomotor policy can easily overfit to its training datasets, such as
fixed camera positions and backgrounds. This overfitting makes the policy
perform well in the in-distribution scenarios but underperform in the
out-of-distribution generalization. Additionally, the existing methods also
have difficulty fusing multi-view information to generate an effective 3D
representation. To tackle these issues, we propose Omni-Vision Diffusion Policy
(OmniD), a multi-view fusion framework that synthesizes image observations into
a unified bird's-eye view (BEV) representation. We introduce a deformable
attention-based Omni-Feature Generator (OFG) to selectively abstract
task-relevant features while suppressing view-specific noise and background
distractions. OmniD achieves 11\%, 17\%, and 84\% average improvement over the
best baseline model for in-distribution, out-of-distribution, and few-shot
experiments, respectively. Training code and simulation benchmark are
available: https://github.com/1mather/omnid.git

</details>


### [11] [Control of Legged Robots using Model Predictive Optimized Path Integral](https://arxiv.org/abs/2508.11917)
*Hossein Keshavarz,Alejandro Ramirez-Serrano,Majid Khadiv*

Main category: cs.RO

TL;DR: 基于采样的模型预测控制器MPOPI，结合MPPI、交叉熵和协方差矩阵适应方法，提高四足机器人的运动能力和样本效率。


<details>
  <summary>Details</summary>
Motivation: 四足机器人在复杂非结构化环境中具有潜力，但现有控制方法还未达到自然系统的水平。需要更高效的实时全身运动生成方法。

Method: 提出MPOPI算法，结合模型预测路径积分(MPPI)、交叉熵(CE)和协方差矩阵适应(CMA)方法，通过采样基础预测控制生成四足机器人的实时全身运动。

Result: 模拟实验结果显示MPOPI具有更高的样本效率，能够以更少的采样数获得更优的运动性能。该算法可作为随时控制策略，在每次迭代中不断提升运动能力。

Conclusion: MPOPI算法通过结合多种采样基础预测控制方法的优势，显著提高了四足机器人在多种场景下的运动效率和性能，为复杂环境中的机器人控制提供了有效解决方案。

Abstract: Legged robots possess a unique ability to traverse rough terrains and
navigate cluttered environments, making them well-suited for complex,
real-world unstructured scenarios. However, such robots have not yet achieved
the same level as seen in natural systems. Recently, sampling-based predictive
controllers have demonstrated particularly promising results. This paper
investigates a sampling-based model predictive strategy combining model
predictive path integral (MPPI) with cross-entropy (CE) and covariance matrix
adaptation (CMA) methods to generate real-time whole-body motions for legged
robots across multiple scenarios. The results show that combining the benefits
of MPPI, CE and CMA, namely using model predictive optimized path integral
(MPOPI), demonstrates greater sample efficiency, enabling robots to attain
superior locomotion results using fewer samples when compared to typical MPPI
algorithms. Extensive simulation experiments in multiple scenarios on a
quadruped robot show that MPOPI can be used as an anytime control strategy,
increasing locomotion capabilities at each iteration.

</details>


### [12] [ExploreVLM: Closed-Loop Robot Exploration Task Planning with Vision-Language Models](https://arxiv.org/abs/2508.11918)
*Zhichen Lou,Kechun Xu,Zhongxiang Zhou,Rong Xiong*

Main category: cs.RO

TL;DR: ExploreVLM是一个基于视觉语言模型的闭环任务规划框架，通过逐步反馈机制实现实时计划调整和交互式探索，在探索型任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着具身智能的发展，机器人需要理解高级指令、规划任务并在动态环境中感知和适应。现有VLM方法在交互探索、精确感知和实时计划适应方面存在困难。

Method: 提出ExploreVLM框架，包含：1）具有自我反思的双阶段任务规划器；2）以对象为中心的空间关系图提供结构化场景表示；3）执行验证器支持闭环验证和重新规划。

Result: 大量真实世界实验表明ExploreVLM显著优于最先进的基线方法，特别是在探索型任务中。消融研究验证了反思规划器和结构化感知的关键作用。

Conclusion: ExploreVLM通过闭环反馈机制和结构化感知表示，实现了鲁棒高效的任务执行，为解决VLM在机器人任务规划中的挑战提供了有效解决方案。

Abstract: The advancement of embodied intelligence is accelerating the integration of
robots into daily life as human assistants. This evolution requires robots to
not only interpret high-level instructions and plan tasks but also perceive and
adapt within dynamic environments. Vision-Language Models (VLMs) present a
promising solution by combining visual understanding and language reasoning.
However, existing VLM-based methods struggle with interactive exploration,
accurate perception, and real-time plan adaptation. To address these
challenges, we propose ExploreVLM, a novel closed-loop task planning framework
powered by Vision-Language Models (VLMs). The framework is built around a
step-wise feedback mechanism that enables real-time plan adjustment and
supports interactive exploration. At its core is a dual-stage task planner with
self-reflection, enhanced by an object-centric spatial relation graph that
provides structured, language-grounded scene representations to guide
perception and planning. An execution validator supports the closed loop by
verifying each action and triggering re-planning. Extensive real-world
experiments demonstrate that ExploreVLM significantly outperforms
state-of-the-art baselines, particularly in exploration-centric tasks. Ablation
studies further validate the critical role of the reflective planner and
structured perception in achieving robust and efficient task execution.

</details>


### [13] [No More Blind Spots: Learning Vision-Based Omnidirectional Bipedal Locomotion for Challenging Terrain](https://arxiv.org/abs/2508.11929)
*Mohitvishnu S. Gadde,Pranay Dugar,Ashish Malik,Alan Fern*

Main category: cs.RO

TL;DR: 通过结合盲目控制器和教师-学生学习框架，实现了基于视觉的全向双足行走，避免了模拟中浮点渲染的高计算成本


<details>
  <summary>Details</summary>
Motivation: 在动态环境中实现灵活的全向双足行走需要全向地形感知和能处理这种输入的控制器，但模拟中渲染全向深度图像的计算成本过高

Method: 结合稳健的盲目控制器和教师策略，让教师监督训练基于视觉的学生策略，使用噪声增强的地形数据避免RL过程中的渲染成本，并提出了一种超级学生训练数据增布技术

Result: 在模拟和实际环境中验证成功，实现了高效的全向双足行走，对深度图像渲染的依赖最小化，训练速度比传统方法提高10倍

Conclusion: 这是首次展示基于视觉的全向双足行走，显示了其在多样地形上的适应能力，为动态环境中的灵活移动提供了新方案

Abstract: Effective bipedal locomotion in dynamic environments, such as cluttered
indoor spaces or uneven terrain, requires agile and adaptive movement in all
directions. This necessitates omnidirectional terrain sensing and a controller
capable of processing such input. We present a learning framework for
vision-based omnidirectional bipedal locomotion, enabling seamless movement
using depth images. A key challenge is the high computational cost of rendering
omnidirectional depth images in simulation, making traditional sim-to-real
reinforcement learning (RL) impractical. Our method combines a robust blind
controller with a teacher policy that supervises a vision-based student policy,
trained on noise-augmented terrain data to avoid rendering costs during RL and
ensure robustness. We also introduce a data augmentation technique for
supervised student training, accelerating training by up to 10 times compared
to conventional methods. Our framework is validated through simulation and
real-world tests, demonstrating effective omnidirectional locomotion with
minimal reliance on expensive rendering. This is, to the best of our knowledge,
the first demonstration of vision-based omnidirectional bipedal locomotion,
showcasing its adaptability to diverse terrains.

</details>


### [14] [Toward General Physical Intelligence for Resilient Agile Manufacturing Automation](https://arxiv.org/abs/2508.11960)
*Sandeep Kanta,Mehrdad Tavassoli,Varun Teja Chirkuri,Venkata Akhil Kumar,Santhi Bharath Punati,Praveen Damacharla,Sunny Katyara*

Main category: cs.RO

TL;DR: 这篇实践性综述系统分析了视觉语言动作(VLA)模型在通用物理智能(GPI)中的最新进展，通过结构化研究评估了其在工业部署中的应用潜力和挑战。


<details>
  <summary>Details</summary>
Motivation: 超1.0时代的灵活人本中心制造需要具备上下文理解能力和安全交互能力的弹性机器人解决方案。虽然GPI概念已经提出，但其在当代灵活制造中的实际应用和发展角色仍未得到充分探索。

Method: 系统性调研GPI背景下VLA模型的最新进展，对领先实现进行全面比较分析，通过结构化的剪除研究评估其工业部署准备度。将最新技术组织为五大主题架构。

Result: 分析将领先技术组织成五大主题架构：多感知表征学习、模拟到实际转移、规划与控制、不确定性与安全措施、以及基准测试。通过结构化研究评估了各方案的工业部署潜力。

Conclusion: 文章提出了开放性研究挑战，并指明了下一步研究方向，以更好地将GPI集成到符合工业4.0标准的下一代工业生态系统中。

Abstract: Agile and human-centric manufacturing stipulates resilient robotic solutions
capable of contextual reasoning and safe interaction in unstructured
environments. Foundation models particularly the Vision Language Action (VLA)
models have emerged to fuse multimodal perception, reasoning and physically
grounded action across varied embodiments into unified representation, termed
as General Physical Intelligence (GPI). While GPI has already been described in
the literature but its practical application and evolving role in contemporary
agile manufacturing processes have yet to be duly explored. To bridge this gap,
this practical review systematically surveys recent advancements in VLA models
within GPI context, performs comprehensive comparative analysis of leading
implementations and evaluates their readiness for industrial deployment through
structured ablation study. Our analysis has organized state-of-the-art into
five thematic pillars including multisensory representation learning, sim2real
transfer, planning and control, uncertainty and safety measures and
benchmarking. Finally, we articulate open research challenges and propose
directions to better integrate GPI into next-generation industrial ecosystems
in line with Industry 5.0.

</details>


### [15] [Fully Spiking Actor-Critic Neural Network for Robotic Manipulation](https://arxiv.org/abs/2508.12038)
*Liwen Zhang,Heng Deng,Guanghui Sun*

Main category: cs.RO

TL;DR: 提出一种基于全空间神经网络(SNN)的混合课程强化学习框架，用于9自由度机器手的目标到达和抓取任务，具有低网络复杂度、低能耗和高效能特点。


<details>
  <summary>Details</summary>
Motivation: 解决传统人工神经网络(ANN)在机器手操纵任务中的高能耗和计算复杂性问题，利用SNN的高速推理、低能耗和生物可行性优势。

Method: 简化SNN网络结构仅包含输入和输出层，集成时间进度分区课程策略与PPO算法，引入能耗模型框架进行定量比较，采用动态两阶段奖励调整机制和优化观测空间。

Result: 在Isaac Gym模拟平台上进行实验，证明方法在现实物理约束下获得优异性能，与传统PPO和ANN基线的对比评估验证了方法在动态机器操纵任务中的可扩展性和能效。

Conclusion: 该混合课程强化学习框架通过SNN技术和课程学习策略的结合，有效提高了机器手操纵的学习效率和政策准确性，为资源受限环境下的高效能深度学习提供了新的解决方案。

Abstract: This study proposes a hybrid curriculum reinforcement learning (CRL)
framework based on a fully spiking neural network (SNN) for 9-degree-of-freedom
robotic arms performing target reaching and grasping tasks. To reduce network
complexity and inference latency, the SNN architecture is simplified to include
only an input and an output layer, which shows strong potential for
resource-constrained environments. Building on the advantages of SNNs-high
inference speed, low energy consumption, and spike-based biological
plausibility, a temporal progress-partitioned curriculum strategy is integrated
with the Proximal Policy Optimization (PPO) algorithm. Meanwhile, an energy
consumption modeling framework is introduced to quantitatively compare the
theoretical energy consumption between SNNs and conventional Artificial Neural
Networks (ANNs). A dynamic two-stage reward adjustment mechanism and optimized
observation space further improve learning efficiency and policy accuracy.
Experiments on the Isaac Gym simulation platform demonstrate that the proposed
method achieves superior performance under realistic physical constraints.
Comparative evaluations with conventional PPO and ANN baselines validate the
scalability and energy efficiency of the proposed approach in dynamic robotic
manipulation tasks.

</details>


### [16] [Talk Less, Fly Lighter: Autonomous Semantic Compression for UAV Swarm Communication via LLMs](https://arxiv.org/abs/2508.12043)
*Fei Lin,Tengchao Zhang,Qinghua Ni,Jun Huang,Siji Ma,Yonglin Tian,Yisheng Lv,Naiqi Wu*

Main category: cs.RO

TL;DR: LLM驱动的无人机群在带宽受限条件下通过语义压缩实现高效协作通信


<details>
  <summary>Details</summary>
Motivation: 无人机群采用大语言模型后语义理解能力提升，但带宽限制和高频交互需求对语义信息传输构成严重挑战

Method: 构建4种不同环境复杂度的2D仿真场景，设计系统提示与任务指令提示结合的通信-执行流程，评估9种主流LLM的语义压缩性能

Result: 实验结果表明基于LLM的无人机群在带宽受限和多跳链路条件下有潜力实现高效协作通信

Conclusion: LLM驱动的无人机群能够通过语义压缩有效减少通信负载，同时保持关键任务语义信息

Abstract: The rapid adoption of Large Language Models (LLMs) in unmanned systems has
significantly enhanced the semantic understanding and autonomous task execution
capabilities of Unmanned Aerial Vehicle (UAV) swarms. However, limited
communication bandwidth and the need for high-frequency interactions pose
severe challenges to semantic information transmission within the swarm. This
paper explores the feasibility of LLM-driven UAV swarms for autonomous semantic
compression communication, aiming to reduce communication load while preserving
critical task semantics. To this end, we construct four types of 2D simulation
scenarios with different levels of environmental complexity and design a
communication-execution pipeline that integrates system prompts with task
instruction prompts. On this basis, we systematically evaluate the semantic
compression performance of nine mainstream LLMs in different scenarios and
analyze their adaptability and stability through ablation studies on
environmental complexity and swarm size. Experimental results demonstrate that
LLM-based UAV swarms have the potential to achieve efficient collaborative
communication under bandwidth-constrained and multi-hop link conditions.

</details>


### [17] [OASIS: Real-Time Opti-Acoustic Sensing for Intervention Systems in Unstructured Environments](https://arxiv.org/abs/2508.12071)
*Amy Phung,Richard Camilli*

Main category: cs.RO

TL;DR: OASIS是一种实时水下3D重建方法，通过融合光学图像和声纳数据，在机器人操作臂上实现实时空间感知


<details>
  <summary>Details</summary>
Motivation: 现有水下3D重建方法多为离线处理，而水下车辆操作需要实时空间感知能力

Method: 采用"手眼"配置，结合光学相机和声纳传感器，利用体素雕刻技术进行实时数据融合

Result: 通过水箱实验验证，在定性定量结果上都显示出对水下操作任务的有效性

Conclusion: OASIS方法实现了实时水下3D重建，为自主和遥控水下车辆操作提供了重要的空间感知能力

Abstract: High resolution underwater 3D scene reconstruction is crucial for various
applications, including construction, infrastructure maintenance, monitoring,
exploration, and scientific investigation. Prior work has leveraged the
complementary sensing modalities of imaging sonars and optical cameras for
opti-acoustic 3D scene reconstruction, demonstrating improved results over
methods which rely solely on either sensor. However, while most existing
approaches focus on offline reconstruction, real-time spatial awareness is
essential for both autonomous and piloted underwater vehicle operations. This
paper presents OASIS, an opti-acoustic fusion method that integrates data from
optical images with voxel carving techniques to achieve real-time 3D
reconstruction unstructured underwater workspaces. Our approach utilizes an
"eye-in-hand" configuration, which leverages the dexterity of robotic
manipulator arms to capture multiple workspace views across a short baseline.
We validate OASIS through tank-based experiments and present qualitative and
quantitative results that highlight its utility for underwater manipulation
tasks.

</details>


### [18] [Into the Wild: When Robots Are Not Welcome](https://arxiv.org/abs/2508.12075)
*Shaul Ashkenazi,Gabriel Skantze,Jane Stuart-Smith,Mary Ellen Foster*

Main category: cs.RO

TL;DR: 社交机器人在公共空间部署遇到技术和用户反对挑战，通过建立信任关系最终成功完成部署


<details>
  <summary>Details</summary>
Motivation: 研究社交机器人在公共空间部署时面临的技术困难和相关方反对挑战

Method: 在两个公共场景中部署社交机器人：学生服务中心和难民穿插服务，通过建立信任关系解决反对问题

Result: 虽然初期遇到困难，但最终成功获得员工信任，完成机器人部署并进行研究

Conclusion: 在公共空间部署社交机器人需要重视相关方信任建立，建立良好关系是成功部署的关键因素

Abstract: Social robots are increasingly being deployed in public spaces, where they
face not only technological difficulties and unexpected user utterances, but
also objections from stakeholders who may not be comfortable with introducing a
robot into those spaces. We describe our difficulties with deploying a social
robot in two different public settings: 1) Student services center; 2) Refugees
and asylum seekers drop-in service. Although this is a failure report, in each
use case we eventually managed to earn the trust of the staff and form a
relationship with them, allowing us to deploy our robot and conduct our
studies.

</details>


### [19] [Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing](https://arxiv.org/abs/2508.12166)
*Gokul Puthumanaillam,Aditya Penumarti,Manav Vora,Paulo Padrao,Jose Fuentes,Leonardo Bobadilla,Jane Shin,Melkior Ornik*

Main category: cs.RO

TL;DR: B-COD是一种基于扩散模型的规划器，通过单次前向传播在10毫秒内生成轨迹、方差和定位误差代理，结合SAC在线选择最小传感器子集，在保证任务完成的同时显著降低能耗


<details>
  <summary>Details</summary>
Motivation: 解决机器人如何在保持足够状态不确定性的前提下，选择最小传感器子集来完成任务的问题，避免持续开启所有传感器造成的能源浪费

Method: 提出Belief-Conditioned One-Step Diffusion (B-COD)规划器，将位姿信念栅格和传感器掩码作为条件输入扩散模型，通过去噪轨迹的扩散程度获得校准的定位误差代理，结合软演员-评论家算法在线选择传感器

Result: 在无人水面车辆的真实海洋试验中，B-COD在匹配全传感器基线性能的同时显著降低了传感能耗

Conclusion: B-COD首次实现了在单次前向传播中同时生成轨迹和定位误差估计，为在线传感器选择提供了高效解决方案，在保证任务性能的同时优化能源使用

Abstract: Robots equipped with rich sensor suites can localize reliably in
partially-observable environments, but powering every sensor continuously is
wasteful and often infeasible. Belief-space planners address this by
propagating pose-belief covariance through analytic models and switching
sensors heuristically--a brittle, runtime-expensive approach. Data-driven
approaches--including diffusion models--learn multi-modal trajectories from
demonstrations, but presuppose an accurate, always-on state estimate. We
address the largely open problem: for a given task in a mapped environment,
which \textit{minimal sensor subset} must be active at each location to
maintain state uncertainty \textit{just low enough} to complete the task? Our
key insight is that when a diffusion planner is explicitly conditioned on a
pose-belief raster and a sensor mask, the spread of its denoising trajectories
yields a calibrated, differentiable proxy for the expected localisation error.
Building on this insight, we present Belief-Conditioned One-Step Diffusion
(B-COD), the first planner that, in a 10 ms forward pass, returns a
short-horizon trajectory, per-waypoint aleatoric variances, and a proxy for
localisation error--eliminating external covariance rollouts. We show that this
single proxy suffices for a soft-actor-critic to choose sensors online,
optimising energy while bounding pose-covariance growth. We deploy B-COD in
real-time marine trials on an unmanned surface vehicle and show that it reduces
sensing energy consumption while matching the goal-reach performance of an
always-on baseline.

</details>


### [20] [Energy Efficiency in Robotics Software: A Systematic Literature Review (2020-2024)](https://arxiv.org/abs/2508.12170)
*Aryan Gupta*

Main category: cs.RO

TL;DR: 这是一份2020-2024年软件层面机器人能源效率研究的系统性文献综述，分析了79份研究，发现运动/轨迹优化是主要技术，动力系统是主要能源消耗者，并提出了最小报告检查单和研究机遇。


<details>
  <summary>Details</summary>
Motivation: 更新和扩展2020年之前的证据，系统评估软件层面的机器人能源效率技术状况，以支持更有效的能源管理和发展。

Method: 采用自动化但审计的流水线，结合Google Scholar种子、向后/向前雪球投放法，以及大语言模型辅助过滤和数据提取，每个自动步骤有10%人工审计。

Result: 分析了79份同行审查研究，发现工业设置占主导(31.6%)，动力/执行器是主要能源消耗者(68.4%)，运动和轨迹优化是主要技术(69.6%)，但报告标准异质性限制了跨文章可比性。

Conclusion: 研究提出了最小报告检查单，并强调了跨层设计和量化非性能交易(精度、稳定性)的机遇，降低了能源管理/空闲控制和通信/数据效率方面的研究潜力。

Abstract: This study presents a systematic literature review of software-level
approaches to energy efficiency in robotics published from 2020 through 2024,
updating and extending pre-2020 evidence. An automated-but-audited pipeline
combined Google Scholar seeding, backward/forward snowballing, and
large-language-model (LLM) assistance for screening and data extraction, with
~10% human audits at each automated step and consensus-with-tie-breaks for
full-text decisions. The final corpus comprises 79 peer-reviewed studies
analyzed across application domain, metrics, evaluation type, energy models,
major energy consumers, software technique families, and energy-quality
trade-offs. Industrial settings dominate (31.6%) followed by exploration
(25.3%). Motors/actuators are identified as the primary consumer in 68.4% of
studies, with computing/controllers a distant second (13.9%). Simulation-only
evaluations remain most common (51.9%), though hybrid evaluations are frequent
(25.3%). Representational (physics-grounded) energy models predominate (87.3%).
Motion and trajectory optimization is the leading technique family (69.6%),
often paired with learning/prediction (40.5%) and computation
allocation/scheduling (26.6%); power management/idle control (11.4%) and
communication/data efficiency (3.8%) are comparatively underexplored. Reporting
is heterogeneous: composite objectives that include energy are most common,
while task-normalized and performance-per-energy metrics appear less often,
limiting cross-paper comparability. The review offers a minimal reporting
checklist (e.g., total energy and average power plus a task-normalized metric
and clear baselines) and highlights opportunities in cross-layer designs and in
quantifying non-performance trade-offs (accuracy, stability). A replication
package with code, prompts, and frozen datasets accompanies the review.

</details>


### [21] [Humanoid Motion Scripting with Postural Synergies](https://arxiv.org/abs/2508.12184)
*Rhea Malhotra,William Chong,Catie Cuan,Oussama Khatib*

Main category: cs.RO

TL;DR: SynSculptor是一个基于姿态协同的人形机器人运动分析与编辑框架，通过PCA提取主要姿态协同，构建风格条件协同库，实现免训练的人类运动生成。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人人类运动生成中的挑战，包括参考运动收集分析、新运动合成以及运动映射到机器人上的问题。

Method: 收集3+小时20人的动作捕捉数据，使用PCA提取速度轨迹的主要姿态协同，构建风格条件协同库，并开发基于运动-语言变换器的运动生成方法。

Result: 开发了评估生成运动的指标（脚滑动比、动量平滑度、动能偏差），并与参考运动进行比较验证。

Conclusion: SynSculptor框架能够有效分析和编辑人形机器人运动，利用姿态协同实现人类运动生成，并通过运动-语言变换器实现任务执行时的姿态自适应。

Abstract: Generating sequences of human-like motions for humanoid robots presents
challenges in collecting and analyzing reference human motions, synthesizing
new motions based on these reference motions, and mapping the generated motion
onto humanoid robots. To address these issues, we introduce SynSculptor, a
humanoid motion analysis and editing framework that leverages postural
synergies for training-free human-like motion scripting. To analyze human
motion, we collect 3+ hours of motion capture data across 20 individuals where
a real-time operational space controller mimics human motion on a simulated
humanoid robot. The major postural synergies are extracted using principal
component analysis (PCA) for velocity trajectories segmented by changes in
robot momentum, constructing a style-conditioned synergy library for free-space
motion generation. To evaluate generated motions using the synergy library, the
foot-sliding ratio and proposed metrics for motion smoothness involving total
momentum and kinetic energy deviations are computed for each generated motion,
and compared with reference motions. Finally, we leverage the synergies with a
motion-language transformer, where the humanoid, during execution of motion
tasks with its end-effectors, adapts its posture based on the chosen synergy.
Supplementary material, code, and videos are available at
https://rhea-mal.github.io/humanoidsynergies.io.

</details>


### [22] [Self-Guided Action Diffusion](https://arxiv.org/abs/2508.12189)
*Rhea Malhotra,Yuejiang Liu,Chelsea Finn*

Main category: cs.RO

TL;DR: 自导向行为激散方法，通过在每个激散步骤基于之前决策指导建议分布，以更高效的方式实现双向解码，在保持性能的同时大幅降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 当前的推理时问量搜索方法在行为样本多样性增加时计算成本过高，需要更高效的方法来提高模型的一致性和反应性。

Method: 提出自导向行为激散方法，在每个激散步骤中基于之前的决策来指导建议分布，从而更高效地实现双向解码。

Result: 在模拟任务中实现了近优性能，推理成本可以忽略不计。在严格的取样预算下，在具有挑战性的动态任务上比现有方法成功率提高了70%。

Conclusion: 自导向行为激散方法作为双向解码的高效变体，能够在保持性能的同时显著降低推理成本，在动态环境中表现出艰强的性能。

Abstract: Recent works have shown the promise of inference-time search over action
samples for improving generative robot policies. In particular, optimizing
cross-chunk coherence via bidirectional decoding has proven effective in
boosting the consistency and reactivity of diffusion policies. However, this
approach remains computationally expensive as the diversity of sampled actions
grows. In this paper, we introduce self-guided action diffusion, a more
efficient variant of bidirectional decoding tailored for diffusion-based
policies. At the core of our method is to guide the proposal distribution at
each diffusion step based on the prior decision. Experiments in simulation
tasks show that the proposed self-guidance enables near-optimal performance at
negligible inference cost. Notably, under a tight sampling budget, our method
achieves up to 70% higher success rates than existing counterparts on
challenging dynamic tasks. See project website at
https://rhea-mal.github.io/selfgad.github.io.

</details>


### [23] [Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search](https://arxiv.org/abs/2508.12211)
*Cyrus Neary,Omar G. Younis,Artur Kuramshin,Ozgur Aslan,Glen Berseth*

Main category: cs.RO

TL;DR: VLAPS是一个将基于模型的搜索嵌入预训练VLA模型推理过程的新框架，通过结合蒙特卡洛树搜索和环境模型来提升机器人任务的性能表现。


<details>
  <summary>Details</summary>
Motivation: 预训练的视觉-语言-动作(VLA)模型在零样本部署到分布外场景时经常产生脆弱行为或不安全故障，需要改进其鲁棒性和性能。

Method: 提出VLAPS框架，使用修改的蒙特卡洛树搜索(MCTS)算法，结合环境模型和VLA策略定义的动作先验，在语言条件化机器人任务中进行高效搜索。

Result: 在所有实验中，VLAPS显著优于仅使用VLA的基线方法，在语言指定任务上的成功率最高提升了67个百分点。

Conclusion: VLAPS提供了一个原则性框架来控制VLA模型的测试时计算、利用机器人环境的先验知识，并将规划与强化学习技术集成到VLA推理过程中。

Abstract: Pre-trained vision-language-action (VLA) models offer a promising foundation
for generalist robot policies, but often produce brittle behaviours or unsafe
failures when deployed zero-shot in out-of-distribution scenarios. We present
Vision-Language-Action Planning & Search (VLAPS) -- a novel framework and
accompanying algorithms that embed model-based search into the inference
procedure of pre-trained VLA policies to improve their performance on robotic
tasks. Specifically, our method biases a modified Monte Carlo Tree Search
(MCTS) algorithm -- run using a model of the target environment -- using action
priors defined by the VLA policy. By using VLA-derived abstractions and priors
in model-based search, VLAPS efficiently explores language-conditioned robotics
tasks whose search spaces would otherwise be intractably large. Conversely, by
integrating model-based search with the VLA policy's inference procedure, VLAPS
yields behaviours that are more performant than those obtained by directly
following the VLA policy's action predictions. VLAPS offers a principled
framework to: i) control test-time compute in VLA models, ii) leverage a priori
knowledge of the robotic environment, and iii) integrate established planning
and reinforcement learning techniques into the VLA inference process. Across
all experiments, VLAPS significantly outperforms VLA-only baselines on
language-specified tasks that would otherwise be intractable for uninformed
search algorithms, increasing success rates by as much as 67 percentage points.

</details>


### [24] [Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids](https://arxiv.org/abs/2508.12252)
*Kaizhe Hu,Haochen Shi,Yao He,Weizhuo Wang,C. Karen Liu,Shuran Song*

Main category: cs.RO

TL;DR: 提出了Robot-Trains-Robot (RTR)框架，使用机械臂教师主动指导人形机器人学生，实现高效的长时期真实世界人形机器人训练，并通过新的RL管道促进sim-to-real迁移。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界人形机器人强化学习面临的安全、奖励设计和学习效率等挑战，克服sim-to-real差距，充分发挥人形机器人的潜力。

Method: RTR框架：机械臂教师提供保护、学习计划、奖励、扰动、故障检测和自动重置；提出新的RL管道，通过在真实世界中优化单个动力学编码潜在变量来稳定sim-to-real迁移。

Result: 在两个具有挑战性的真实世界人形任务中验证：精确速度跟踪的行走策略微调，以及从零开始学习人形摆动任务，展示了RTR系统实现真实世界人形学习的有前景能力。

Conclusion: RTR框架能够以最少的人工干预实现高效的长时期真实世界人形训练，为真实世界人形机器人学习提供了有前景的解决方案。

Abstract: Simulation-based reinforcement learning (RL) has significantly advanced
humanoid locomotion tasks, yet direct real-world RL from scratch or adapting
from pretrained policies remains rare, limiting the full potential of humanoid
robots. Real-world learning, despite being crucial for overcoming the
sim-to-real gap, faces substantial challenges related to safety, reward design,
and learning efficiency. To address these limitations, we propose
Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher
actively supports and guides a humanoid robot student. The RTR system provides
protection, learning schedule, reward, perturbation, failure detection, and
automatic resets. It enables efficient long-term real-world humanoid training
with minimal human intervention. Furthermore, we propose a novel RL pipeline
that facilitates and stabilizes sim-to-real transfer by optimizing a single
dynamics-encoded latent variable in the real world. We validate our method
through two challenging real-world humanoid tasks: fine-tuning a walking policy
for precise speed tracking and learning a humanoid swing-up task from scratch,
illustrating the promising capabilities of real-world humanoid learning
realized by RTR-style systems. See https://robot-trains-robot.github.io/ for
more info.

</details>


### [25] [Bimanual Robot-Assisted Dressing: A Spherical Coordinate-Based Strategy for Tight-Fitting Garments](https://arxiv.org/abs/2508.12274)
*Jian Zhao,Yunlong Lian,Andy M Tyrrell,Michael Gienger,Jihong Zhu*

Main category: cs.RO

TL;DR: 本文提出了一种适用于穿紧身服装的双手机器人穿衣策略，通过理想球坐标系和模建方法实现了适应不同人体手臂姿势的穿衣轨迹学习。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人辅助穿衣研究主要集中在松身服装，对紧身服装关注少，单手机器人穿紧身服装时容易失败。

Method: 建立球坐标系统作为穿衣任务的坐标系，使用方位角作为双手操作的任务相关特征，采用高斯混合模型和高斯混合回归进行仿真学习。

Result: 通过多种实验验证了所提方法的有效性，能够生成适应不同人手臂姿势的穿衣策略。

Conclusion: 该双手穿衣策略有效解决了紧身服装穿着的挑战，为机器人辅助穿衣领域提供了新的解决方案。

Abstract: Robot-assisted dressing is a popular but challenging topic in the field of
robotic manipulation, offering significant potential to improve the quality of
life for individuals with mobility limitations. Currently, the majority of
research on robot-assisted dressing focuses on how to put on loose-fitting
clothing, with little attention paid to tight garments. For the former, since
the armscye is larger, a single robotic arm can usually complete the dressing
task successfully. However, for the latter, dressing with a single robotic arm
often fails due to the narrower armscye and the property of diminishing
rigidity in the armscye, which eventually causes the armscye to get stuck. This
paper proposes a bimanual dressing strategy suitable for dressing tight-fitting
clothing. To facilitate the encoding of dressing trajectories that adapt to
different human arm postures, a spherical coordinate system for dressing is
established. We uses the azimuthal angle of the spherical coordinate system as
a task-relevant feature for bimanual manipulation. Based on this new
coordinate, we employ Gaussian Mixture Model (GMM) and Gaussian Mixture
Regression (GMR) for imitation learning of bimanual dressing trajectories,
generating dressing strategies that adapt to different human arm postures. The
effectiveness of the proposed method is validated through various experiments.

</details>


### [26] [A robust and compliant robotic assembly control strategy for batch precision assembly task with uncertain fit types and fit amounts](https://arxiv.org/abs/2508.12296)
*Bin Wang,Jiwen Zhang,Song Wang,Dan Wu*

Main category: cs.RO

TL;DR: 提出了一种力视觉融合控制器驱动的多任务强化学习方法（FVFC-MTRL），通过多教师策略萌荐技术构建统一的高精度装配策略，解决不确定配合类型和配合量的扰动问题。


<details>
  <summary>Details</summary>
Motivation: 高精度工业装配份使用过渡配合时，机加工误差导致细间隔或干涉配合的不确定性，需要稳健的符合控制策略来应对扰动。

Method: 将扰动装配任务分解为多个确定子任务，采用力视觉融合控制器驱动的多任务强化学习方法（FVFC-MTRL）联合学习多个符合控制策略，然后通过多教师策略萌荐技术集成到统一的学生网络中。

Result: 真实实验证明该方法成功构建了可靠的控制策略，MTRL框架显著提高了训练效率，最终控制策略在力符合性和成功率方面超过了现有方法。

Conclusion: 该研究提出的FVFC-MTRL方法能够有效处理高精度装配份中的不确定配合问题，通过多任务学习和策略萌荐技术实现了高效训练和稳健控制，在实际应用中表现优异。

Abstract: In some high-precision industrial applications, robots are deployed to
perform precision assembly tasks on mass batches of manufactured pegs and
holes. If the peg and hole are designed with transition fit, machining errors
may lead to either a clearance or an interference fit for a specific pair of
components, with uncertain fit amounts. This paper focuses on the robotic batch
precision assembly task involving components with uncertain fit types and fit
amounts, and proposes an efficient methodology to construct the robust and
compliant assembly control strategy. Specifically, the batch precision assembly
task is decomposed into multiple deterministic subtasks, and a force-vision
fusion controller-driven reinforcement learning method and a multi-task
reinforcement learning training method (FVFC-MTRL) are proposed to jointly
learn multiple compliance control strategies for these subtasks. Subsequently,
the multi-teacher policy distillation approach is designed to integrate
multiple trained strategies into a unified student network, thereby
establishing a robust control strategy. Real-world experiments demonstrate that
the proposed method successfully constructs the robust control strategy for
high-precision assembly task with different fit types and fit amounts.
Moreover, the MTRL framework significantly improves training efficiency, and
the final developed control strategy achieves superior force compliance and
higher success rate compared with many existing methods.

</details>


### [27] [Implementation and evaluation of a prediction algorithm for an autonomous vehicle](https://arxiv.org/abs/2508.12312)
*Marco Leon Rapp*

Main category: cs.RO

TL;DR: 一种基于动态自行车模型的车辆轨迹预测算法，每5毫秒预测一次，位置偏差仅1.25cm/m，比动学模型精确82.6%


<details>
  <summary>Details</summary>
Motivation: 为自主驾驶汽车提供高精度的轨迹预测能力，特别是在高速情况下提升预测准确性

Method: 使用动态自行车模型，通过光学位置跟踪新方法测量轮胎偶急度，并将模型集成到扩展卡尔曼滤波器中，在ROS环境用C++实现

Result: 在整个测试过程中实现了每米路程仅1.25厘米的位置偏差，在高速情况下比动学模型精确度提升82.6%

Conclusion: 动态自行车模型在高速轨迹预测中显示出明显优势，通过实验方法确定车辆参数和新的轮胎偶急度测量技术有效提升了模型准确性

Abstract: This paper presents a prediction algorithm that estimates the vehicle
trajectory every five milliseconds for an autonomous vehicle. A kinematic and a
dynamic bicycle model are compared, with the dynamic model exhibiting superior
accuracy at higher speeds. Vehicle parameters such as mass, center of gravity,
moment of inertia, and cornering stiffness are determined experimentally. For
cornering stiffness, a novel measurement procedure using optical position
tracking is introduced. The model is incorporated into an extended Kalman
filter and implemented in a ROS node in C++. The algorithm achieves a
positional deviation of only 1.25 cm per meter over the entire test drive and
is up to 82.6% more precise than the kinematic model.

</details>


### [28] [Semi-Infinite Programming for Collision-Avoidance in Optimal and Model Predictive Control](https://arxiv.org/abs/2508.12335)
*Yunfan Gao,Florian Messerer,Niels van Duijkeren,Rashmi Dabir,Moritz Diehl*

Main category: cs.RO

TL;DR: 这篇论文提出了一种新的碰撞避免方法，通过将环境表示为大量点并将机器人模型化为带填充的多边形联合体，利用半无穷规划和主动集方法高效解决碰撞避免问题，并在实际机器人上实现了20Hz的快速无碰撞导航。


<details>
  <summary>Details</summary>
Motivation: 传统碰撞避免方法在处理大量环境点时会产生无穷多的约束条件，需要开发高效的数值方法来解决这些半无穷规划问题，同时还需要考虑状态不确定性下的稳健碰撞避免。

Method: 采用半无穷规划(SIP)检查方法，通过局部约化和外部主动集算法迭代识别最近的障碍点，确定机器人形状参数的距离最小化器，并求解有限约束子问题。对于状态不确定性，使用局部约化处理平移不确定性，重新形式化处理旋转不确定性。

Result: 在实际机器人上实现了20Hz的控制器，能够在窄窄空间中实现快速的无碰撞导航。同时在仿真中也展示了3D碰撞避免的应用效果。

Conclusion: 该方法能够高效处理来自环境点的无穷多约束，并在存在状态不确定性的情况下实现稳健的碰撞避免，为机器人在复杂环境中的安全导航提供了有效的解决方案。

Abstract: This paper presents a novel approach for collision avoidance in optimal and
model predictive control, in which the environment is represented by a large
number of points and the robot as a union of padded polygons. The conditions
that none of the points shall collide with the robot can be written in terms of
an infinite number of constraints per obstacle point. We show that the
resulting semi-infinite programming (SIP) optimal control problem (OCP) can be
efficiently tackled through a combination of two methods: local reduction and
an external active-set method. Specifically, this involves iteratively
identifying the closest point obstacles, determining the lower-level distance
minimizer among all feasible robot shape parameters, and solving the
upper-level finitely-constrained subproblems.
  In addition, this paper addresses robust collision avoidance in the presence
of ellipsoidal state uncertainties. Enforcing constraint satisfaction over all
possible uncertainty realizations extends the dimension of constraint
infiniteness. The infinitely many constraints arising from translational
uncertainty are handled by local reduction together with the robot shape
parameterization, while rotational uncertainty is addressed via a backoff
reformulation.
  A controller implemented based on the proposed method is demonstrated on a
real-world robot running at 20Hz, enabling fast and collision-free navigation
in tight spaces. An application to 3D collision avoidance is also demonstrated
in simulation.

</details>


### [29] [SIGN: Safety-Aware Image-Goal Navigation for Autonomous Drones via Reinforcement Learning](https://arxiv.org/abs/2508.12394)
*Zichen Yan,Rui Huang,Lei He,Shao Guo,Lin Zhao*

Main category: cs.RO

TL;DR: 通过视觉强化学习和辅助任务训练，为自主飞行器实现不依赖全局定位的图像目标导航，包括自主探索、障碍避免和目标寻找功能


<details>
  <summary>Details</summary>
Motivation: 解决自主飞行器在未知环境中实现图像目标导航的挑战，特别是需要高频率反馈控制和全局定位以保持稳定飞行

Method: 使用视觉强化学习，通过图像扰动和未来过渡预测等辅助任务增强视觉表征能力，实现端到端的速度控制，并集成深度基于的安全模块进行实时障碍避免

Result: 开发了一个不需要外部定位的完整导航框架，支持自主探索、障碍避免和图像目标寻找，无需显式全局地图构建

Conclusion: 该方法为自主飞行器提供了一种综合性的导航解决方案，能够在复杂环境中安全实现图像目标导航，为无人机自主导航领域带来重要进展

Abstract: Image-goal navigation (ImageNav) tasks a robot with autonomously exploring an
unknown environment and reaching a location that visually matches a given
target image. While prior works primarily study ImageNav for ground robots,
enabling this capability for autonomous drones is substantially more
challenging due to their need for high-frequency feedback control and global
localization for stable flight. In this paper, we propose a novel sim-to-real
framework that leverages visual reinforcement learning (RL) to achieve ImageNav
for drones. To enhance visual representation ability, our approach trains the
vision backbone with auxiliary tasks, including image perturbations and future
transition prediction, which results in more effective policy training. The
proposed algorithm enables end-to-end ImageNav with direct velocity control,
eliminating the need for external localization. Furthermore, we integrate a
depth-based safety module for real-time obstacle avoidance, allowing the drone
to safely navigate in cluttered environments. Unlike most existing drone
navigation methods that focus solely on reference tracking or obstacle
avoidance, our framework supports comprehensive navigation
behaviors--autonomous exploration, obstacle avoidance, and image-goal
seeking--without requiring explicit global mapping. Code and model checkpoints
will be released upon acceptance.

</details>


### [30] [PUB: A Plasma-Propelled Ultra-Quiet Blimp with Two-DOF Vector Thrusting](https://arxiv.org/abs/2508.12395)
*Zihan Wang*

Main category: cs.RO

TL;DR: 一种采用等离子风推进的超静音气球飞行器，通过横向推进器配置和二自由度头部控制实现稳定机动飞行，适用于噪声敏感环境


<details>
  <summary>Details</summary>
Motivation: 开发一种无机械弹系、超低噪音的新型航空机器人，满足噪声敏感、封闭式和近空间应用的需求

Method: 采用氧气升力平台提供持续能力，四层环形非对称电容器生成离子风推力，模块化推进单元支持灵活配置，二自由度头部实现推力向量控制，闭环滑移控制策略保证稳定性

Result: 飞行实验证明完整飞行包线能力，包括起飞、攀升、悬停、降落和平滑着陆，验证了等离子向量推进的可行性、自由度向量控制的有效性和控制系统的稳定性

Conclusion: 该等离子推进飞行器具有超低噪音、结构简单和高机动性优势，在噪声敏感、封闭环境和近空间应用中展现出良好的应用前景

Abstract: This study presents the design and control of a Plasma-propelled
Ultra-silence Blimp (PUB), a novel aerial robot employing plasma vector
propulsion for ultra-quiet flight without mechanical propellers. The system
utilizes a helium-lift platform for extended endurance and a four-layer ring
asymmetric capacitor to generate ionic wind thrust. The modular propulsion
units allow flexible configuration to meet mission-specific requirements, while
a two-degree-of-freedom (DOF) head enables thrust vector control. A closed-loop
slip control scheme is implemented for stable maneuvering. Flight experiments
demonstrate full-envelope capability, including take-off, climb, hover,
descent, and smooth landing, confirming the feasibility of plasma vector
propulsion, the effectiveness of DOF vector control, and the stability of the
control system. Owing to its low acoustic signature, structural simplicity, and
high maneuverability, PUB is well suited for noise-sensitive, enclosed, and
near-space applications.

</details>


### [31] [Tactile Gesture Recognition with Built-in Joint Sensors for Industrial Robots](https://arxiv.org/abs/2508.12435)
*Deqing Song,Weimin Yang,Maryam Rezayati,Hans Wernher van de Venn*

Main category: cs.RO

TL;DR: 使用机器人关节传感器数据通过深度学习实现手势识别，无需外部传感器，详细测试了不同CNN模型和数据表示方法的效果


<details>
  <summary>Details</summary>
Motivation: 探索仅依靠机器人内置关节传感器来实现手势识别的可行性，避免使用外部传感器的成本和复杂性

Method: 收集两个数据集，评估多种卷积神经网络架构，重点研究了谱图基于数据表示方法的效果

Result: STFT2DCNN和STT3DCNN两种方法在Franka Emika研究机器人上实现了超过95%的接触检测和手势分类准确率，谱图表示方法显著提高了准确性

Conclusion: 证明了不依赖外部传感器的触觉识别的可行性，为人机协作领域提供了成本效益更高、可扩展性更强的解决方案

Abstract: While gesture recognition using vision or robot skins is an active research
area in Human-Robot Collaboration (HRC), this paper explores deep learning
methods relying solely on a robot's built-in joint sensors, eliminating the
need for external sensors. We evaluated various convolutional neural network
(CNN) architectures and collected two datasets to study the impact of data
representation and model architecture on the recognition accuracy. Our results
show that spectrogram-based representations significantly improve accuracy,
while model architecture plays a smaller role. We also tested generalization to
new robot poses, where spectrogram-based models performed better. Implemented
on a Franka Emika Research robot, two of our methods, STFT2DCNN and STT3DCNN,
achieved over 95% accuracy in contact detection and gesture classification.
These findings demonstrate the feasibility of external-sensor-free tactile
recognition and promote further research toward cost-effective, scalable
solutions for HRC.

</details>


### [32] [Geodesic Tracing-Based Kinematic Integration of Rolling and Sliding Contact on Manifold Meshes for Dexterous In-Hand Manipulation](https://arxiv.org/abs/2508.12439)
*Sunyu Wang,Arjun S. Lakshmipathy,Jean Oh,Nancy S. Pollard*

Main category: cs.RO

TL;DR: 通过基于港线迹踪的积分方案，将滚动-滑动接触模型扩展到流形网格，支持高保真度的手势操控规划


<details>
  <summary>Details</summary>
Motivation: 现有滚动-滑动接触研究主要集中在连续形状，需要扩展到离散网格表示以支持更精细的操控任务

Method: 基于港线迹踪的一阶时间积分方案，直接在网格上实现滚动-滑动接触模型，使用最小二乘优化器规划手势动作

Result: 在五个对象的多指机械手操控任务中，方法在精度和准确性方面都超过了碰撞检测基线和基本形状基线，甚至对粗糕网格也有良好表现

Conclusion: 方法能够在网格上实现高保真度的滚动-滑动接触模型，未来将研究多重接触和接触力的集成以实现更准确稳健的表面接触模型

Abstract: Reasoning about rolling and sliding contact, or roll-slide contact for short,
is critical for dexterous manipulation tasks that involve intricate geometries.
But existing works on roll-slide contact mostly focus on continuous shapes with
differentiable parametrizations. This work extends roll-slide contact modeling
to manifold meshes. Specifically, we present an integration scheme based on
geodesic tracing to first-order time-integrate roll-slide contact directly on
meshes, enabling dexterous manipulation to reason over high-fidelity discrete
representations of an object's true geometry. Using our method, we planned
dexterous motions of a multi-finger robotic hand manipulating five objects
in-hand in simulation. The planning was achieved with a least-squares optimizer
that strives to maintain the most stable instantaneous grasp by minimizing
contact sliding and spinning. Then, we evaluated our method against a baseline
using collision detection and a baseline using primitive shapes. The results
show that our method performed the best in accuracy and precision, even for
coarse meshes. We conclude with a future work discussion on incorporating
multiple contacts and contact forces to achieve accurate and robust mesh-based
surface contact modeling.

</details>


### [33] [Autonomous Oil Spill Response Through Liquid Neural Trajectory Modeling and Coordinated Marine Robotics](https://arxiv.org/abs/2508.12456)
*Hadas C. Kuzmenko,David Ehevich,Oren Gal*

Main category: cs.RO

TL;DR: 基于湿流体动力学和群机器人的油漏预测与响应框架，通过LTCN网络实现高精度预测，空间准确度达96%，超过LSTM方法23%


<details>
  <summary>Details</summary>
Motivation: 海洋油漏造成严重环境和经济损失，需要准确的实时预测和动态响应来减少影响

Method: 结合MOOS-IvP平台的多机器人群系统与液体时间常数神经网络(LTCN)，利用群智能实现分布式决策

Result: LTC-RK4模型在Deepwater Horizon漏油事件中达到0.96空间准确度，超过LSTM方法23%，显著提升预测精度和操作可扩展性

Conclusion: 该研究通过先进神经模型与自主机器人协同系统的集成，提升了油漏管理的预测准确性、灵活性和可持续性，为环境保护提供了有力技术支撑

Abstract: Marine oil spills pose grave environmental and economic risks, threatening
marine ecosystems, coastlines, and dependent industries. Predicting and
managing oil spill trajectories is highly complex, due to the interplay of
physical, chemical, and environmental factors such as wind, currents, and
temperature, which makes timely and effective response challenging. Accurate
real-time trajectory forecasting and coordinated mitigation are vital for
minimizing the impact of these disasters. This study introduces an integrated
framework combining a multi-agent swarm robotics system built on the MOOS-IvP
platform with Liquid Time-Constant Neural Networks (LTCNs). The proposed system
fuses adaptive machine learning with autonomous marine robotics, enabling
real-time prediction, dynamic tracking, and rapid response to evolving oil
spills. By leveraging LTCNs--well-suited for modeling complex, time-dependent
processes--the framework achieves real-time, high-accuracy forecasts of spill
movement. Swarm intelligence enables decentralized, scalable, and resilient
decision-making among robot agents, enhancing collective monitoring and
containment efforts. Our approach was validated using data from the Deepwater
Horizon spill, where the LTC-RK4 model achieved 0.96 spatial accuracy,
surpassing LSTM approaches by 23%. The integration of advanced neural modeling
with autonomous, coordinated robotics demonstrates substantial improvements in
prediction precision, flexibility, and operational scalability. Ultimately,
this research advances the state-of-the-art for sustainable, autonomous oil
spill management and environmental protection by enhancing both trajectory
prediction and response coordination.

</details>


### [34] [Mechanical Automation with Vision: A Design for Rubik's Cube Solver](https://arxiv.org/abs/2508.12469)
*Abhinav Chalise,Nimesh Gopal Pradhan,Nishan Khanal,Prashant Raj Bista,Dinesh Baniya Kshatri*

Main category: cs.RO

TL;DR: 基于YOLOv8实时检测和Kociemba算法的高精度魔方自动解法系统，通过三个步进电机实现物理操控，平均解法时间约2.2分钟


<details>
  <summary>Details</summary>
Motivation: 开发一种能够自动解决魔方的机械系统，结合计算机视觉和控制算法来实现高效准确的魔方恢复

Method: 使用YOLOv8模型进行实时魔方状态检测（精度0.98443，召回0.98419），通过Unity开发GUI界面，采用Kociemba算法求解，使用3个步进电机进行物理操控

Result: 实现了高精度的魔方状态检测（盒损失0.42051，类别损失0.2611），系统平均解法时间约2.2分钟

Conclusion: 该系统成功结合了计算机视觉、控制算法和机械执行器，实现了高效准确的魔方自动解法，为自动化操作系统提供了有效的解决方案

Abstract: The core mechanical system is built around three stepper motors for physical
manipulation, a microcontroller for hardware control, a camera and YOLO
detection model for real-time cube state detection. A significant software
component is the development of a user-friendly graphical user interface (GUI)
designed in Unity. The initial state after detection from real-time YOLOv8
model (Precision 0.98443, Recall 0.98419, Box Loss 0.42051, Class Loss 0.2611)
is virtualized on GUI. To get the solution, the system employs the Kociemba's
algorithm while physical manipulation with a single degree of freedom is done
by combination of stepper motors' interaction with the cube achieving the
average solving time of ~2.2 minutes.

</details>


### [35] [PROD: Palpative Reconstruction of Deformable Objects through Elastostatic Signed Distance Functions](https://arxiv.org/abs/2508.12554)
*Hamza El-Kebir*

Main category: cs.RO

TL;DR: PROD是一种通过触觉交互重建可变形物体形状和力学特性的新方法，使用弹性静力学SDF来估计软材料的静态和动态响应。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖纯几何或视觉数据，无法有效估计可变形物体的力学特性。PROD通过整合触觉交互（力控表面探测）来解决这一问题。

Method: 将物体变形建模为弹性静力学过程，推导出控制泊松方程，从稀疏的姿态和力测量中估计SDF。结合稳态弹性动力学假设，从变形观测中恢复未变形SDF。

Result: PROD能够处理姿态误差、非垂直力施加和曲率误差，在模拟软体交互中表现出鲁棒性，并能通过分析位移响应来估计材料刚度。

Conclusion: PROD是重建可变形物体的强大工具，适用于机器人操作、医学成像和触觉反馈系统等多种应用场景。

Abstract: We introduce PROD (Palpative Reconstruction of Deformables), a novel method
for reconstructing the shape and mechanical properties of deformable objects
using elastostatic signed distance functions (SDFs). Unlike traditional
approaches that rely on purely geometric or visual data, PROD integrates
palpative interaction -- measured through force-controlled surface probing --
to estimate both the static and dynamic response of soft materials. We model
the deformation of an object as an elastostatic process and derive a governing
Poisson equation for estimating its SDF from a sparse set of pose and force
measurements. By incorporating steady-state elastodynamic assumptions, we show
that the undeformed SDF can be recovered from deformed observations with
provable convergence. Our approach also enables the estimation of material
stiffness by analyzing displacement responses to varying force inputs. We
demonstrate the robustness of PROD in handling pose errors, non-normal force
application, and curvature errors in simulated soft body interactions. These
capabilities make PROD a powerful tool for reconstructing deformable objects in
applications ranging from robotic manipulation to medical imaging and haptic
feedback systems.

</details>


### [36] [Temporal and Rotational Calibration for Event-Centric Multi-Sensor Systems](https://arxiv.org/abs/2508.12564)
*Jiayao Mai,Xiuyuan Lu,Kuan Dai,Shaojie Shen,Yi Zhou*

Main category: cs.RO

TL;DR: 事件相机多传感器系统的无标定物时间-旋转检定框架，通过角速度估计和连续时间优化实现高精度外参检定


<details>
  <summary>Details</summary>
Motivation: 事件相机作为一种新型传感器，具有微秒级延迟优势，但多传感器融合中的外参检定问题尚未得到充分研究，需要一种无需专门标定物的检定方法

Method: 提出基于运动的两步检定方法：首先利用动力学相关性通过CCA初始化时间偏移和旋转外参，然后采用SO(3)中的连续时间参数化进行时间-旋转聚合非线性优化，通过法流观测估计角速度

Result: 在公开和自收数据集上的广泛评估显示，该方法达到了与基于标定物方法相当的检定精度，同时具有比纯CCA方法更优的稳定性，展现了高精度、骏性和灵活性

Conclusion: 该研究为事件相机多传感器系统提供了一种无需标定物的高精度外参检定方案，解决了该领域的关键技术挑战，并将开源代码以促进未来研究

Abstract: Event cameras generate asynchronous signals in response to pixel-level
brightness changes, offering a sensing paradigm with theoretically
microsecond-scale latency that can significantly enhance the performance of
multi-sensor systems. Extrinsic calibration is a critical prerequisite for
effective sensor fusion; however, the configuration that involves event cameras
remains an understudied topic. In this paper, we propose a motion-based
temporal and rotational calibration framework tailored for event-centric
multi-sensor systems, eliminating the need for dedicated calibration targets.
Our method uses as input the rotational motion estimates obtained from event
cameras and other heterogeneous sensors, respectively. Different from
conventional approaches that rely on event-to-frame conversion, our method
efficiently estimates angular velocity from normal flow observations, which are
derived from the spatio-temporal profile of event data. The overall calibration
pipeline adopts a two-step approach: it first initializes the temporal offset
and rotational extrinsics by exploiting kinematic correlations in the spirit of
Canonical Correlation Analysis (CCA), and then refines both temporal and
rotational parameters through a joint non-linear optimization using a
continuous-time parametrization in SO(3). Extensive evaluations on both
publicly available and self-collected datasets validate that the proposed
method achieves calibration accuracy comparable to target-based methods, while
exhibiting superior stability over purely CCA-based methods, and highlighting
its precision, robustness and flexibility. To facilitate future research, our
implementation will be made open-source. Code:
https://github.com/NAIL-HNU/EvMultiCalib.

</details>


### [37] [Adaptive Model-Predictive Control of a Soft Continuum Robot Using a Physics-Informed Neural Network Based on Cosserat Rod Theory](https://arxiv.org/abs/2508.12681)
*Johann Licher,Max Bartholdt,Henrik Krauss,Tim-Lukas Habich,Thomas Seel,Moritz Schappler*

Main category: cs.RO

TL;DR: 基于域解耦物理信息神经网络的非线性模型预测控制框架，实现了软连续体机器人的高速实时动态控制


<details>
  <summary>Details</summary>
Motivation: 软连续体机器人的动态控制应用潜力巨大，但准确动态模型计算复杂度高，而现有的数据驱动方法缺乏适应性且无法捕捉全部机器人形状

Method: 使用域解耦物理信息神经网络(DD-PINN)作为动态Cosserat柱模型的代理模型，速度提升40000倍，并结合无味变卡尔滤波估计模型状态和弯曲度，在GPU上实现70Hz的非线性进化模型预测控制

Result: 模拟中准确跟踪动态轨迹，端执器位置误差低于3mm(2.3%长度)；实际实验中达到类似精度，加速度达3.55m/s²

Conclusion: 该框架成功实现了软连续体机器人的高速实时动态控制，为扩大其应用范围提供了可行方案

Abstract: Dynamic control of soft continuum robots (SCRs) holds great potential for
expanding their applications, but remains a challenging problem due to the high
computational demands of accurate dynamic models. While data-driven approaches
like Koopman-operator-based methods have been proposed, they typically lack
adaptability and cannot capture the full robot shape, limiting their
applicability. This work introduces a real-time-capable nonlinear
model-predictive control (MPC) framework for SCRs based on a domain-decoupled
physics-informed neural network (DD-PINN) with adaptable bending stiffness. The
DD-PINN serves as a surrogate for the dynamic Cosserat rod model with a
speed-up factor of 44000. It is also used within an unscented Kalman filter for
estimating the model states and bending compliance from end-effector position
measurements. We implement a nonlinear evolutionary MPC running at 70 Hz on the
GPU. In simulation, it demonstrates accurate tracking of dynamic trajectories
and setpoint control with end-effector position errors below 3 mm (2.3% of the
actuator's length). In real-world experiments, the controller achieves similar
accuracy and accelerations up to 3.55 m/s2.

</details>


### [38] [MCTR: Midpoint Corrected Triangulation for Autonomous Racing via Digital Twin Simulation in CARLA](https://arxiv.org/abs/2508.12729)
*Junhao Ye,Cheng Hu,Yiqin Wang,Weizhan Huang,Nicolas Baumann,Jie He,Meixun Qu,Lei Xie,Hongye Su*

Main category: cs.RO

TL;DR: 提出了MCTR算法，通过曲率校正移动平均提高轨迹平滑度，并在CARLA模拟器中实现数字孪生系统来验证3D LiDAR感知下的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有DTR算法使用外接圆生成轨迹导致路径不够平滑，且F1TENTH模拟器缺乏3D LiDAR感知支持，限制了真实测试效果

Method: 使用曲率校正移动平均(CCMA)改进轨迹平滑度，在CARLA模拟器中构建数字孪生系统进行3D LiDAR感知验证

Result: 算法在仿真和真实车辆实验中得到了充分验证

Conclusion: MCTR算法有效解决了轨迹平滑度和3D感知验证问题，提升了自动驾驶赛车性能

Abstract: In autonomous racing, reactive controllers eliminate the computational burden
of the full See-Think-Act autonomy stack by directly mapping sensor inputs to
control actions. This bypasses the need for explicit localization and
trajectory planning. A widely adopted baseline in this category is the
Follow-The-Gap method, which performs trajectory planning using LiDAR data.
Building on FTG, the Delaunay Triangulation-based Racing algorithm introduces
further enhancements. However, DTR's use of circumcircles for trajectory
generation often results in insufficiently smooth paths, ultimately degrading
performance. Additionally, the commonly used F1TENTH-simulator for autonomous
racing competitions lacks support for 3D LiDAR perception, limiting its
effectiveness in realistic testing. To address these challenges, this work
proposes the MCTR algorithm. MCTR improves trajectory smoothness through the
use of Curvature Corrected Moving Average and implements a digital twin system
within the CARLA simulator to validate the algorithm's robustness under 3D
LiDAR perception. The proposed algorithm has been thoroughly validated through
both simulation and real-world vehicle experiments.

</details>


### [39] [RoboRetriever: Single-Camera Robot Object Retrieval via Active and Interactive Perception with Dynamic Scene Graph](https://arxiv.org/abs/2508.12916)
*Hecheng Wang,Jiankun Ren,Jia Yu,Lizhe Qi,Yunquan Sun*

Main category: cs.RO

TL;DR: RoboRetriever是一个仅使用单个腕部RGB-D相机和自然语言指令的机器人对象检索框架，通过动态层次场景图、主动感知和交互感知实现真实世界中的物体检索。


<details>
  <summary>Details</summary>
Motivation: 人类能够轻松在杂乱、部分可观测环境中检索物体，而现有机器人系统依赖多摄像头设置，硬件成本高且适应性有限。

Method: 构建动态层次场景图编码对象语义、几何和关系；使用视觉提示方案确定6-DoF相机位姿；结合主动感知、交互感知和操作。

Result: 在多样化真实世界对象检索任务中表现出强大的适应性和鲁棒性，包括有人干预的场景。

Conclusion: 该方法证明了仅用单个RGB-D相机就能在杂乱场景中实现有效的对象检索，为低成本适应性机器人系统提供了新思路。

Abstract: Humans effortlessly retrieve objects in cluttered, partially observable
environments by combining visual reasoning, active viewpoint adjustment, and
physical interaction-with only a single pair of eyes. In contrast, most
existing robotic systems rely on carefully positioned fixed or multi-camera
setups with complete scene visibility, which limits adaptability and incurs
high hardware costs. We present \textbf{RoboRetriever}, a novel framework for
real-world object retrieval that operates using only a \textbf{single}
wrist-mounted RGB-D camera and free-form natural language instructions.
RoboRetriever grounds visual observations to build and update a \textbf{dynamic
hierarchical scene graph} that encodes object semantics, geometry, and
inter-object relations over time. The supervisor module reasons over this
memory and task instruction to infer the target object and coordinate an
integrated action module combining \textbf{active perception},
\textbf{interactive perception}, and \textbf{manipulation}. To enable
task-aware scene-grounded active perception, we introduce a novel visual
prompting scheme that leverages large reasoning vision-language models to
determine 6-DoF camera poses aligned with the semantic task goal and geometry
scene context. We evaluate RoboRetriever on diverse real-world object retrieval
tasks, including scenarios with human intervention, demonstrating strong
adaptability and robustness in cluttered scenes with only one RGB-D camera.

</details>


### [40] [Deformation of the panoramic sphere into an ellipsoid to induce self-motion in telepresence users](https://arxiv.org/abs/2508.12925)
*Eetu Laukka,Evan G. Center,Timo Ojala,Steven M. LaValle,Matti Pouke*

Main category: cs.RO

TL;DR: 这篇论文探索了在高延迟移动遥控机器人系统中使用光流效应来创造自我运动幻觉的方法，但实验结果显示该方法在500毫秒延迟下对控制性能没有显著改善，反而可能增加VR舔晕感


<details>
  <summary>Details</summary>
Motivation: 解决使用360度摄像头的移动遥控机器人系统在网络延迟期间的实时控制困难，需要某种形式的用户协助

Method: 提出了一种新题的方法，利用光流效应在用户发送运动命令到看到实际运动的延迟期间内为用户创造自我运动的幻觉

Result: 在500毫秒延迟下，使用自我运动幻觉对任务完成时间和碰撞物体的控制精度没有显著改善，反而可能会增加虚拟现实舔晕感

Conclusion: 该方法需要进一步调整才能成为可行的解决方案

Abstract: Mobile telepresence robots allow users to feel present and explore remote
environments using technology. Traditionally, these systems are implemented
using a camera onboard a mobile robot that can be controlled. Although
high-immersion technologies, such as 360-degree cameras, can increase
situational awareness and presence, they also introduce significant challenges.
Additional processing and bandwidth requirements often result in latencies of
up to seconds. The current delay with a 360-degree camera streaming over the
internet makes real-time control of these systems difficult. Working with
high-latency systems requires some form of assistance to the users.
  This study presents a novel way to utilize optical flow to create an illusion
of self-motion to the user during the latency period between user sending
motion commands to the robot and seeing the actual motion through the
360-camera stream. We find no significant benefit of using the self-motion
illusion to performance or accuracy of controlling a telepresence robot with a
latency of 500 ms, as measured by the task completion time and collisions into
objects. Some evidence is shown that the method might increase virtual reality
(VR) sickness, as measured by the simulator sickness questionnaire (SSQ). We
conclude that further adjustments are necessary in order to render the method
viable.

</details>


### [41] [Simultaneous Contact Sequence and Patch Planning for Dynamic Locomotion](https://arxiv.org/abs/2508.12928)
*Victor Dhédin,Haizhou Zhao,Majid Khadiv*

Main category: cs.RO

TL;DR: 基于MCTS和全身轨迹优化的四足机器人多接触运动规划框架，能够同时解决接触序列和接触点选择问题


<details>
  <summary>Details</summary>
Motivation: 四足机器人在高约束环境中进行灵活机动时，需要解决混合连续和离散变量的高难度优化问题

Method: 采用蒙特卡洛树搜索(MCTS)算法结合全身轨迹优化(TO)，实现同时接触序列和接触点选择

Result: 框架能够快速找到多样化的动态一致规划，并成功转移到真实四足机器人上，同时支持复杂的非周期性人形机动

Conclusion: 这是首次展示了使用全身动力学模型进行同时接触序列和接触点选择的非周期性多接触行走方法

Abstract: Legged robots have the potential to traverse highly constrained environments
with agile maneuvers. However, planning such motions requires solving a highly
challenging optimization problem with a mixture of continuous and discrete
decision variables. In this paper, we present a full pipeline based on
Monte-Carlo tree search (MCTS) and whole-body trajectory optimization (TO) to
perform simultaneous contact sequence and patch selection on highly challenging
environments. Through extensive simulation experiments, we show that our
framework can quickly find a diverse set of dynamically consistent plans. We
experimentally show that these plans are transferable to a real quadruped
robot. We further show that the same framework can find highly complex acyclic
humanoid maneuvers. To the best of our knowledge, this is the first
demonstration of simultaneous contact sequence and patch selection for acyclic
multi-contact locomotion using the whole-body dynamics of a quadruped.

</details>


### [42] [Insights from Interviews with Teachers and Students on the Use of a Social Robot in Computer Science Class in Sixth Grade](https://arxiv.org/abs/2508.12946)
*Ann-Sophie Schenk,Stefan Schiffer,Heqiu Song*

Main category: cs.RO

TL;DR: 这篇论文通过访谈六年级计算机科学课的教师和学生，探索了社交机器人在教室中的应用需求和潜力。研究发现两者都对机器人教学持开放态度，但需求存在异质性，带来了复杂的设计挑战。


<details>
  <summary>Details</summary>
Motivation: 了解教师和学生对社交机器人在计算机科学课堂中应用的需求和观点，获取双方视角下的实际需求

Method: 采用访谈法，访谈六年级计算机科学课的教师和学生

Result: 教师和学生都对机器人进入教室持很开放的态度，但两者的需求存在明显异质性

Conclusion: 社交机器人在教育领域具有潜力，但需要考虑不同群体的异质性需求，这为设计带来了复杂挑战

Abstract: In this paper we report on first insights from interviews with teachers and
students on using social robots in computer science class in sixth grade. Our
focus is on learning about requirements and potential applications. We are
particularly interested in getting both perspectives, the teachers' and the
learners' view on how robots could be used and what features they should or
should not have. Results show that teachers as well as students are very open
to robots in the classroom. However, requirements are partially quite
heterogeneous among the groups. This leads to complex design challenges which
we discuss at the end of this paper.

</details>


### [43] [Scaling Whole-body Multi-contact Manipulation with Contact Optimization](https://arxiv.org/abs/2508.12980)
*Victor Levé,João Moura,Sachiya Fujita,Tamon Miyake,Steve Tonneau,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: 提出一种新的人形机器人全身操纵规划框架，通过表面表征和梯度优化解决连续接触问题，计划时间提升75%


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人在手部不可用时的全身操纵任务需求，充分利用机器人和物体表面的连续性质

Method: 使用闭式计算近似点的表面表征方法，结合梯度基优化规划和有效的成本设计

Result: 解决了现有方法无法处理的问题，计划时间比最先进方法提升77%，并在真实人形机器上验证了箱子操纵能力

Conclusion: 该框架为全身操纵规划提供了高效的解决方案，充分利用了连续表面表征的优势，在实际硬件上表现出良好的应用前景

Abstract: Daily tasks require us to use our whole body to manipulate objects, for
instance when our hands are unavailable. We consider the issue of providing
humanoid robots with the ability to autonomously perform similar whole-body
manipulation tasks. In this context, the infinite possibilities for where and
how contact can occur on the robot and object surfaces hinder the scalability
of existing planning methods, which predominantly rely on discrete sampling.
Given the continuous nature of contact surfaces, gradient-based optimization
offers a more suitable approach for finding solutions. However, a key remaining
challenge is the lack of an efficient representation of robot surfaces. In this
work, we propose (i) a representation of robot and object surfaces that enables
closed-form computation of proximity points, and (ii) a cost design that
effectively guides whole-body manipulation planning. Our experiments
demonstrate that the proposed framework can solve problems unaddressed by
existing methods, and achieves a 77% improvement in planning time over the
state of the art. We also validate the suitability of our approach on real
hardware through the whole-body manipulation of boxes by a humanoid robot.

</details>


### [44] [BOW: Bayesian Optimization over Windows for Motion Planning in Complex Environments](https://arxiv.org/abs/2508.13052)
*Sourav Raxit,Abdullah Al Redwan Newaz,Paulo Padrao,Jose Fuentes,Leonardo Bobadilla*

Main category: cs.RO

TL;DR: BOW Planner是一个基于约束贝叶斯优化的运动规划算法，通过关注可达速度窗口和高效采样控制输入，在复杂环境中实现快速安全的轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 传统运动规划方法在处理动力学约束（如速度、加速度限制）时存在困难，需要一种能够高效处理高维目标函数和严格安全约束的新方法。

Method: 采用约束贝叶斯优化（CBO）方法，集中在可达速度的规划窗口内采样控制输入，以最小采样量管理高维目标函数和安全约束。

Result: 理论分析证明算法渐近收敛到接近最优解，在复杂约束环境中相比现有技术显著提高了计算时间、轨迹长度和求解时间。

Conclusion: BOW Planner在实际机器人系统中表现出卓越的采样效率、安全感知优化和快速规划能力，已作为开源包发布，是推进机器人应用的有价值工具。

Abstract: This paper introduces the BOW Planner, a scalable motion planning algorithm
designed to navigate robots through complex environments using constrained
Bayesian optimization (CBO). Unlike traditional methods, which often struggle
with kinodynamic constraints such as velocity and acceleration limits, the BOW
Planner excels by concentrating on a planning window of reachable velocities
and employing CBO to sample control inputs efficiently. This approach enables
the planner to manage high-dimensional objective functions and stringent safety
constraints with minimal sampling, ensuring rapid and secure trajectory
generation. Theoretical analysis confirms the algorithm's asymptotic
convergence to near-optimal solutions, while extensive evaluations in cluttered
and constrained settings reveal substantial improvements in computation times,
trajectory lengths, and solution times compared to existing techniques.
Successfully deployed across various real-world robotic systems, the BOW
Planner demonstrates its practical significance through exceptional sample
efficiency, safety-aware optimization, and rapid planning capabilities, making
it a valuable tool for advancing robotic applications. The BOW Planner is
released as an open-source package and videos of real-world and simulated
experiments are available at https://bow-web.github.io.

</details>


### [45] [Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey](https://arxiv.org/abs/2508.13073)
*Rui Shao,Wei Li,Lingsen Zhang,Renshan Zhang,Zhiyang Liu,Ran Chen,Liqiang Nie*

Main category: cs.RO

TL;DR: 这是一份关于大视觉语言模型基础的视觉-语言-动作模型在机器人操纵领域的系统性调研报告，提供了架构分类、技术集成和未来发展方向的全面分析。


<details>
  <summary>Details</summary>
Motivation: 传统规则基础的机器人操纵方法在非结构化环境中缺乏扩展性和通用性，而基于大视觉语言模型的VLA模型成为了转型性的解决方案。本调研旨在系统整理该领域的研究进展，解决现有分类不一致和研究碎片化问题。

Method: 调研采用系统性的分类学方法，首先定义大VLM基础的VLA模型，并划分为两种主要架构范式：单体模型（包括单系统和双系统设计）和层次模型（通过可解释的中间表示显式解耦规划与执行）。继而深入分析了与各个高级领域的集成、特征综合以及未来发展方向。

Result: 调研结果系统整理了大VLM基础VLA模型的架构特征、运作优势以及支持其开发的数据集和测试标准。识别出了许多有前景的研究方向，包括记忆机制、4D感知、高效适应、多代理体合作以及其他新兴能力。

Conclusion: 这份调研报告为大视觉语言模型与机器人操纵交叉领域的研究提供了系统性的整合，有助于消除现有分类不一致和研究碎片化问题。通过明确的架构分类和深入分析，为该领域的未来发展指明了方向，包括内存机制、4D感知等关键技术路径。

Abstract: Robotic manipulation, a key frontier in robotics and embodied AI, requires
precise motor control and multimodal understanding, yet traditional rule-based
methods fail to scale or generalize in unstructured, novel environments. In
recent years, Vision-Language-Action (VLA) models, built upon Large
Vision-Language Models (VLMs) pretrained on vast image-text datasets, have
emerged as a transformative paradigm. This survey provides the first
systematic, taxonomy-oriented review of large VLM-based VLA models for robotic
manipulation. We begin by clearly defining large VLM-based VLA models and
delineating two principal architectural paradigms: (1) monolithic models,
encompassing single-system and dual-system designs with differing levels of
integration; and (2) hierarchical models, which explicitly decouple planning
from execution via interpretable intermediate representations. Building on this
foundation, we present an in-depth examination of large VLM-based VLA models:
(1) integration with advanced domains, including reinforcement learning,
training-free optimization, learning from human videos, and world model
integration; (2) synthesis of distinctive characteristics, consolidating
architectural traits, operational strengths, and the datasets and benchmarks
that support their development; (3) identification of promising directions,
including memory mechanisms, 4D perception, efficient adaptation, multi-agent
cooperation, and other emerging capabilities. This survey consolidates recent
advances to resolve inconsistencies in existing taxonomies, mitigate research
fragmentation, and fill a critical gap through the systematic integration of
studies at the intersection of large VLMs and robotic manipulation. We provide
a regularly updated project page to document ongoing progress:
https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation.

</details>


### [46] [Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy](https://arxiv.org/abs/2508.13103)
*Tianyi Zhang,Haonan Duan,Haoran Hao,Yu Qiao,Jifeng Dai,Zhi Hou*

Main category: cs.RO

TL;DR: OC-VLA框架通过将动作预测直接建立在相机观测空间中，解决了VLA模型在观察和动作空间不一致的问题，提高了模型对相机视角变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: VLA模型在处理真实世界环境时面临观察空间和动作空间不一致的挑战，模型在不同相机视角下预测末端执行器位姿时存在空间不一致问题。

Method: 提出OC-VLA框架，利用相机外参标定矩阵将末端执行器位姿从机器人基坐标系转换到相机坐标系，统一不同视角的预测目标。

Result: 在仿真和真实机器人操作任务上的综合评估表明，OC-VLA加速了收敛速度，提高了任务成功率，并改善了跨视角泛化能力。

Conclusion: OC-VLA是一个轻量级即插即用策略，无需对现有VLA架构进行重大修改，就能显著提升感知和动作之间的对齐鲁棒性。

Abstract: Vision-Language-Action (VLA) models frequently encounter challenges in
generalizing to real-world environments due to inherent discrepancies between
observation and action spaces. Although training data are collected from
diverse camera perspectives, the models typically predict end-effector poses
within the robot base coordinate frame, resulting in spatial inconsistencies.
To mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA)
framework, which grounds action predictions directly in the camera observation
space. Leveraging the camera's extrinsic calibration matrix, OC-VLA transforms
end-effector poses from the robot base coordinate system into the camera
coordinate system, thereby unifying prediction targets across heterogeneous
viewpoints. This lightweight, plug-and-play strategy ensures robust alignment
between perception and action, substantially improving model resilience to
camera viewpoint variations. The proposed approach is readily compatible with
existing VLA architectures, requiring no substantial modifications.
Comprehensive evaluations on both simulated and real-world robotic manipulation
tasks demonstrate that OC-VLA accelerates convergence, enhances task success
rates, and improves cross-view generalization. The code will be publicly
available.

</details>


### [47] [Manipulate-to-Navigate: Reinforcement Learning with Visual Affordances and Manipulability Priors](https://arxiv.org/abs/2508.13151)
*Yuying Zhang,Joni Pajarinen*

Main category: cs.RO

TL;DR: 通过强化学习结合操作性先验知识和供应性地图，学习在动态环境中清除障碍物以便导航的操作策略


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中移动性障碍物阻塞导航路径的问题，传统方法将导航和操作分离处理效果不佳

Method: 采用强化学习方法，结合操作性先验知识确定高操作性位置，使用供应性地图选择高质量操作动作，减少不必要的探索

Result: 在Reach和Door任务中，方法能够有效交互并穿越动态环境，学到的策略成功转移到真实Boston Dynamics Spot机器人并完成Reach任务

Conclusion: 该方法能够有效解决动态环境中的操作导航问题，通过结合先验知识提高学习效率和性能

Abstract: Mobile manipulation in dynamic environments is challenging due to movable
obstacles blocking the robot's path. Traditional methods, which treat
navigation and manipulation as separate tasks, often fail in such
'manipulate-to-navigate' scenarios, as obstacles must be removed before
navigation. In these cases, active interaction with the environment is required
to clear obstacles while ensuring sufficient space for movement. To address the
manipulate-to-navigate problem, we propose a reinforcement learning-based
approach for learning manipulation actions that facilitate subsequent
navigation. Our method combines manipulability priors to focus the robot on
high manipulability body positions with affordance maps for selecting
high-quality manipulation actions. By focusing on feasible and meaningful
actions, our approach reduces unnecessary exploration and allows the robot to
learn manipulation strategies more effectively. We present two new
manipulate-to-navigate simulation tasks called Reach and Door with the Boston
Dynamics Spot robot. The first task tests whether the robot can select a good
hand position in the target area such that the robot base can move effectively
forward while keeping the end effector position fixed. The second task requires
the robot to move a door aside in order to clear the navigation path. Both of
these tasks need first manipulation and then navigating the base forward.
Results show that our method allows a robot to effectively interact with and
traverse dynamic environments. Finally, we transfer the learned policy to a
real Boston Dynamics Spot robot, which successfully performs the Reach task.

</details>
