<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 26]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Vi-TacMan: Articulated Object Manipulation via Vision and Touch](https://arxiv.org/abs/2510.06339)
*Leiyao Cui,Zihang Zhao,Sirui Xie,Wenhuan Zhang,Zhi Han,Yixin Zhu*

Main category: cs.RO

TL;DR: Vi-TacMan结合视觉和触觉进行关节物体操作：视觉提供抓取和方向建议，触觉控制器进行精确执行，无需显式运动学模型。


<details>
  <summary>Details</summary>
Motivation: 视觉方法对陌生物体估计不精确，触觉方法需要准确初始化。需要结合视觉的全局引导和触觉的局部精度来实现通用关节操作。

Method: 使用视觉提出抓取和粗略方向，作为触觉控制器的种子。引入表面法线作为几何先验，通过von Mises-Fisher分布建模方向。触觉控制器通过实时接触调节来细化视觉估计。

Result: 在超过50,000个模拟和真实物体上测试，相比基线方法有显著提升（所有p<0.0001），实现跨类别的鲁棒泛化。

Conclusion: 粗视觉线索结合触觉反馈足以实现可靠操作，为无结构环境中的自主系统提供了可扩展范式。

Abstract: Autonomous manipulation of articulated objects remains a fundamental
challenge for robots in human environments. Vision-based methods can infer
hidden kinematics but can yield imprecise estimates on unfamiliar objects.
Tactile approaches achieve robust control through contact feedback but require
accurate initialization. This suggests a natural synergy: vision for global
guidance, touch for local precision. Yet no framework systematically exploits
this complementarity for generalized articulated manipulation. Here we present
Vi-TacMan, which uses vision to propose grasps and coarse directions that seed
a tactile controller for precise execution. By incorporating surface normals as
geometric priors and modeling directions via von Mises-Fisher distributions,
our approach achieves significant gains over baselines (all p<0.0001).
Critically, manipulation succeeds without explicit kinematic models -- the
tactile controller refines coarse visual estimates through real-time contact
regulation. Tests on more than 50,000 simulated and diverse real-world objects
confirm robust cross-category generalization. This work establishes that coarse
visual cues suffice for reliable manipulation when coupled with tactile
feedback, offering a scalable paradigm for autonomous systems in unstructured
environments.

</details>


### [2] [A Formal gatekeeper Framework for Safe Dual Control with Active Exploration](https://arxiv.org/abs/2510.06351)
*Kaleb Ben Naveed,Devansh R. Agrawal,Dimitra Panagou*

Main category: cs.RO

TL;DR: 提出一个集成鲁棒规划与主动探索的框架，仅在探索能提供可验证改进且不损害安全时进行探索，通过门控器架构确保安全并生成信息丰富的轨迹。


<details>
  <summary>Details</summary>
Motivation: 解决模型不确定性下的安全轨迹规划问题，传统鲁棒规划过于保守且忽略不确定性减少，而现有双控制方法缺乏对探索时机和安全性的正式考虑。

Method: 扩展门控器架构，生成既能减少不确定性又能降低任务成本的安全信息轨迹，或在用户定义预算内保持成本。

Result: 通过四旋翼无人机参数不确定性的在线双控制仿真案例验证了方法的有效性。

Conclusion: 该框架在确保安全的前提下，智能地平衡探索与利用，仅在有益时进行探索，避免了不必要的保守行为。

Abstract: Planning safe trajectories under model uncertainty is a fundamental
challenge. Robust planning ensures safety by considering worst-case
realizations, yet ignores uncertainty reduction and leads to overly
conservative behavior. Actively reducing uncertainty on-the-fly during a
nominal mission defines the dual control problem. Most approaches address this
by adding a weighted exploration term to the cost, tuned to trade off the
nominal objective and uncertainty reduction, but without formal consideration
of when exploration is beneficial. Moreover, safety is enforced in some methods
but not in others. We propose a framework that integrates robust planning with
active exploration under formal guarantees as follows: The key innovation and
contribution is that exploration is pursued only when it provides a verifiable
improvement without compromising safety. To achieve this, we utilize our
earlier work on gatekeeper as an architecture for safety verification, and
extend it so that it generates both safe and informative trajectories that
reduce uncertainty and the cost of the mission, or keep it within a
user-defined budget. The methodology is evaluated via simulation case studies
on the online dual control of a quadrotor under parametric uncertainty.

</details>


### [3] [Constrained Natural Language Action Planning for Resilient Embodied Systems](https://arxiv.org/abs/2510.06357)
*Grayson Byrd,Corban Rivera,Bethany Kemp,Meghan Booker,Aurora Schmidt,Celso M de Melo,Lalithkumar Seenivasan,Mathias Unberath*

Main category: cs.RO

TL;DR: 提出了一种结合大语言模型和符号规划的新机器人规划方法，通过符号规划监督提高LLM规划器的可靠性和可重复性，在ALFWorld基准测试中达到99%成功率，在真实四足机器人上实现100%任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在机器人任务规划中的幻觉问题，以及符号规划方法难以扩展至复杂现实世界任务的问题，旨在提高LLM规划器的可靠性、可重复性和透明度。

Method: 将大语言模型规划器与符号规划监督相结合，通过符号规划提供硬约束定义，保留LLM的推理能力，同时提高系统的可靠性和透明度。

Result: 在ALFWorld规划基准测试中达到99%成功率，优于现有最先进方法；在真实四足机器人上实现100%任务成功率，而纯LLM和符号规划器分别只有50%和30%成功率。

Conclusion: 该方法有效提升了基于LLM的机器人规划器的可靠性、可重复性和透明度，同时保持了其在复杂现实环境中的灵活性和泛化能力，为构建弹性具身智能系统提供了有效策略。

Abstract: Replicating human-level intelligence in the execution of embodied tasks
remains challenging due to the unconstrained nature of real-world environments.
Novel use of large language models (LLMs) for task planning seeks to address
the previously intractable state/action space of complex planning tasks, but
hallucinations limit their reliability, and thus, viability beyond a research
context. Additionally, the prompt engineering required to achieve adequate
system performance lacks transparency, and thus, repeatability. In contrast to
LLM planning, symbolic planning methods offer strong reliability and
repeatability guarantees, but struggle to scale to the complexity and ambiguity
of real-world tasks. We introduce a new robotic planning method that augments
LLM planners with symbolic planning oversight to improve reliability and
repeatability, and provide a transparent approach to defining hard constraints
with considerably stronger clarity than traditional prompt engineering.
Importantly, these augmentations preserve the reasoning capabilities of LLMs
and retain impressive generalization in open-world environments. We demonstrate
our approach in simulated and real-world environments. On the ALFWorld planning
benchmark, our approach outperforms current state-of-the-art methods, achieving
a near-perfect 99% success rate. Deployment of our method to a real-world
quadruped robot resulted in 100% task success compared to 50% and 30% for pure
LLM and symbolic planners across embodied pick and place tasks. Our approach
presents an effective strategy to enhance the reliability, repeatability and
transparency of LLM-based robot planners while retaining their key strengths:
flexibility and generalizability to complex real-world environments. We hope
that this work will contribute to the broad goal of building resilient embodied
intelligent systems.

</details>


### [4] [Active Next-Best-View Optimization for Risk-Averse Path Planning](https://arxiv.org/abs/2510.06481)
*Amirhossein Mollaei Khass,Guangyi Liu,Vivek Pandey,Wen Jiang,Boshu Lei,Kostas Daniilidis,Nader Motee*

Main category: cs.RO

TL;DR: 提出了一个统一框架，通过在线更新的3D高斯泼溅辐射场构建尾部敏感风险地图来细化粗参考路径，同时将最佳视点选择建模为SE(3)姿态流形上的优化问题，实现风险规避路径规划与主动感知的耦合。


<details>
  <summary>Details</summary>
Motivation: 在不确定环境中实现安全导航需要将风险规避与主动感知相结合的规划方法，现有方法在这两方面的耦合不够紧密。

Method: 使用平均风险价值统计构建尾部敏感风险地图来细化参考路径；将最佳视点选择建模为SE(3)流形上的优化问题，通过黎曼梯度下降最大化预期信息增益。

Result: 通过广泛的计算研究证明了所提框架的有效性，能够生成局部安全可行的轨迹并有效降低关键不确定性。

Conclusion: 该框架通过耦合风险规避路径细化与最佳视点规划，并引入可扩展的梯度分解支持复杂环境中的高效在线更新，推进了该领域的技术水平。

Abstract: Safe navigation in uncertain environments requires planning methods that
integrate risk aversion with active perception. In this work, we present a
unified framework that refines a coarse reference path by constructing
tail-sensitive risk maps from Average Value-at-Risk statistics on an
online-updated 3D Gaussian-splat Radiance Field. These maps enable the
generation of locally safe and feasible trajectories. In parallel, we formulate
Next-Best-View (NBV) selection as an optimization problem on the SE(3) pose
manifold, where Riemannian gradient descent maximizes an expected information
gain objective to reduce uncertainty most critical for imminent motion. Our
approach advances the state-of-the-art by coupling risk-averse path refinement
with NBV planning, while introducing scalable gradient decompositions that
support efficient online updates in complex environments. We demonstrate the
effectiveness of the proposed framework through extensive computational
studies.

</details>


### [5] [What You Don't Know Can Hurt You: How Well do Latent Safety Filters Understand Partially Observable Safety Constraints?](https://arxiv.org/abs/2510.06492)
*Matthew Kim,Kensuke Nakamura,Andrea Bajcsy*

Main category: cs.RO

TL;DR: 该论文研究了基于潜在空间的安全控制方法，发现仅使用RGB观测可能导致短视的安全行为，提出了基于互信息的度量方法来识别观测是否捕获了安全相关特征，并提出了多模态监督训练策略。


<details>
  <summary>Details</summary>
Motivation: 现有潜在空间安全控制方法假设安全关键特征可在学习的潜在状态中观察到，但实际中RGB观测可能无法充分捕获安全相关信息，导致短视的安全行为。

Method: 引入基于互信息的度量方法识别观测缺陷，提出多模态监督训练策略，在训练时使用额外感官输入塑造潜在状态，但部署时无需额外模态。

Result: 在仿真和Franka Research 3机械臂硬件实验中验证了方法有效性，成功防止蜡锅过热。

Conclusion: 多模态监督训练策略能够改善潜在状态表示，使控制器能够从原始观测中有效执行安全控制，而无需部署时的额外传感器。

Abstract: Safe control techniques, such as Hamilton-Jacobi reachability, provide
principled methods for synthesizing safety-preserving robot policies but
typically assume hand-designed state spaces and full observability. Recent work
has relaxed these assumptions via latent-space safe control, where state
representations and dynamics are learned jointly through world models that
reconstruct future high-dimensional observations (e.g., RGB images) from
current observations and actions. This enables safety constraints that are
difficult to specify analytically (e.g., spilling) to be framed as
classification problems in latent space, allowing controllers to operate
directly from raw observations. However, these methods assume that
safety-critical features are observable in the learned latent state. We ask:
when are latent state spaces sufficient for safe control? To study this, we
examine temperature-based failures, comparable to overheating in cooking or
manufacturing tasks, and find that RGB-only observations can produce myopic
safety behaviors, e.g., avoiding seeing failure states rather than preventing
failure itself. To predict such behaviors, we introduce a mutual
information-based measure that identifies when observations fail to capture
safety-relevant features. Finally, we propose a multimodal-supervised training
strategy that shapes the latent state with additional sensory inputs during
training, but requires no extra modalities at deployment, and validate our
approach in simulation and on hardware with a Franka Research 3 manipulator
preventing a pot of wax from overheating.

</details>


### [6] [Real-Time Glass Detection and Reprojection using Sensor Fusion Onboard Aerial Robots](https://arxiv.org/abs/2510.06518)
*Malakhi Hopkins,Varun Murali,Vijay Kumar,Camillo J Taylor*

Main category: cs.RO

TL;DR: 提出了一种用于小型四旋翼无人机的实时透明障碍物检测与建图框架，融合ToF相机和超声波传感器数据，通过轻量级2D卷积模型检测镜面反射并填充深度图空白区域。


<details>
  <summary>Details</summary>
Motivation: 透明障碍物对自主飞行机器人构成重大挑战，传统感知系统难以检测，现有方法通常依赖大型昂贵传感器或高计算量算法，不适合低SWaP机器人。

Method: 融合ToF相机和超声波传感器数据，使用定制轻量级2D卷积模型检测镜面反射，并将深度信息传播到深度图的对应空白区域。

Result: 系统在嵌入式处理器上仅占用少量CPU核心即可实时运行，在受控和真实环境实验中成功映射包含玻璃的室内环境。

Conclusion: 这是首个在低SWaP四旋翼无人机上仅使用CPU实现实时机载透明障碍物建图的系统。

Abstract: Autonomous aerial robots are increasingly being deployed in real-world
scenarios, where transparent obstacles present significant challenges to
reliable navigation and mapping. These materials pose a unique problem for
traditional perception systems because they lack discernible features and can
cause conventional depth sensors to fail, leading to inaccurate maps and
potential collisions. To ensure safe navigation, robots must be able to
accurately detect and map these transparent obstacles. Existing methods often
rely on large, expensive sensors or algorithms that impose high computational
burdens, making them unsuitable for low Size, Weight, and Power (SWaP) robots.
In this work, we propose a novel and computationally efficient framework for
detecting and mapping transparent obstacles onboard a sub-300g quadrotor. Our
method fuses data from a Time-of-Flight (ToF) camera and an ultrasonic sensor
with a custom, lightweight 2D convolution model. This specialized approach
accurately detects specular reflections and propagates their depth into
corresponding empty regions of the depth map, effectively rendering transparent
obstacles visible. The entire pipeline operates in real-time, utilizing only a
small fraction of a CPU core on an embedded processor. We validate our system
through a series of experiments in both controlled and real-world environments,
demonstrating the utility of our method through experiments where the robot
maps indoor environments containing glass. Our work is, to our knowledge, the
first of its kind to demonstrate a real-time, onboard transparent obstacle
mapping system on a low-SWaP quadrotor using only the CPU.

</details>


### [7] [RAISE: A self-driving laboratory for interfacial property formulation discovery](https://arxiv.org/abs/2510.06546)
*Mohammad Nazeri,Sheldon Mei,Jeffrey Watchorn,Alex Zhang,Erin Ng,Tao Wen,Abhijoy Mandal,Kevin Golovin,Alan Aspuru-Guzik,Frank Gu*

Main category: cs.RO

TL;DR: RAISE是一个自主机器人实验室系统，通过贝叶斯优化自动优化液体配方以实现目标接触角测量，实现每分钟1次的高通量测量。


<details>
  <summary>Details</summary>
Motivation: 表面润湿性是生物医学设备、涂层和纺织品的关键设计参数，但液体配方对接触角测量影响很大，需要自动化系统来优化配方与润湿性评估的关联。

Method: RAISE系统包含实验协调器，能够混合液体成分、转移液滴到高通量平台，使用拾取相机自动捕获液滴图像，并通过自动图像处理管道测量接触角，集成贝叶斯优化进行迭代探索。

Result: 系统能以每分钟约1次的速度测量接触角，成功探索了表面活性剂润湿性，展示了多目标贝叶斯优化如何根据应用特定目标找到精确最优配方。

Conclusion: RAISE能够自主地将液体配方与接触角测量在闭环系统中关联起来，使用多目标贝叶斯优化高效识别符合研究人员定义标准的最优配方。

Abstract: Surface wettability is a critical design parameter for biomedical devices,
coatings, and textiles. Contact angle measurements quantify liquid-surface
interactions, which depend strongly on liquid formulation. Herein, we present
the Robotic Autonomous Imaging Surface Evaluator (RAISE), a closed-loop,
self-driving laboratory that is capable of linking liquid formulation
optimization with surface wettability assessment. RAISE comprises a full
experimental orchestrator with the ability of mixing liquid ingredients to
create varying formulation cocktails, transferring droplets of prepared
formulations to a high-throughput stage, and using a pick-and-place camera tool
for automated droplet image capture. The system also includes an automated
image processing pipeline to measure contact angles. This closed loop
experiment orchestrator is integrated with a Bayesian Optimization (BO) client,
which enables iterative exploration of new formulations based on previous
contact angle measurements to meet user-defined objectives. The system operates
in a high-throughput manner and can achieve a measurement rate of approximately
1 contact angle measurement per minute. Here we demonstrate RAISE can be used
to explore surfactant wettability and how surfactant combinations create
tunable formulations that compensate for purity-related variations.
Furthermore, multi-objective BO demonstrates how precise and optimal
formulations can be reached based on application-specific goals. The
optimization is guided by a desirability score, which prioritizes formulations
that are within target contact angle ranges, minimize surfactant usage and
reduce cost. This work demonstrates the capabilities of RAISE to autonomously
link liquid formulations to contact angle measurements in a closed-loop system,
using multi-objective BO to efficiently identify optimal formulations aligned
with researcher-defined criteria.

</details>


### [8] [Safe Obstacle-Free Guidance of Space Manipulators in Debris Removal Missions via Deep Reinforcement Learning](https://arxiv.org/abs/2510.06566)
*Vincent Lam,Robin Chhabra*

Main category: cs.RO

TL;DR: 开发基于TD3强化学习的无模型空间机械臂轨迹规划器，用于安全可靠的太空碎片捕获任务


<details>
  <summary>Details</summary>
Motivation: 解决空间机械臂在捕获非合作目标时需同时满足精确跟踪、避免自碰撞和防止意外接触的复杂挑战

Method: 采用基于课程学习的多评价器网络（一个关注精确跟踪，一个关注碰撞避免），结合优先经验回放缓冲区，以及包含奇点避免和可操作性增强的局部控制策略

Result: 在Matlab/Simulink中模拟的七自由度KUKA LBR iiwa机械臂上验证了框架的有效性，能够生成安全自适应的轨迹

Conclusion: 所提出的框架能够为碎片清除任务生成安全可靠的工作空间轨迹，具有良好的适应性和鲁棒性

Abstract: The objective of this study is to develop a model-free workspace trajectory
planner for space manipulators using a Twin Delayed Deep Deterministic Policy
Gradient (TD3) agent to enable safe and reliable debris capture. A local
control strategy with singularity avoidance and manipulability enhancement is
employed to ensure stable execution. The manipulator must simultaneously track
a capture point on a non-cooperative target, avoid self-collisions, and prevent
unintended contact with the target. To address these challenges, we propose a
curriculum-based multi-critic network where one critic emphasizes accurate
tracking and the other enforces collision avoidance. A prioritized experience
replay buffer is also used to accelerate convergence and improve policy
robustness. The framework is evaluated on a simulated seven-degree-of-freedom
KUKA LBR iiwa mounted on a free-floating base in Matlab/Simulink, demonstrating
safe and adaptive trajectory generation for debris removal missions.

</details>


### [9] [Assist-As-Needed: Adaptive Multimodal Robotic Assistance for Medication Management in Dementia Care](https://arxiv.org/abs/2510.06633)
*Kruthika Gangaraju,Tanmayi Inaparthy,Jiaqi Yang,Yihao Zheng,Fengpei Yuan*

Main category: cs.RO

TL;DR: 提出了一个基于Pepper机器人的自适应多模态框架，为痴呆症患者提供动态调整的用药管理辅助，从简单提醒到全面指导的层次化干预模型。


<details>
  <summary>Details</summary>
Motivation: 现有辅助技术无法适应痴呆症患者能力衰退的渐进性变化，一刀切的方法损害了患者的自主性，增加了照护负担。需要根据个体能力匹配辅助水平的适应性系统。

Method: 使用Pepper机器人实现层次化干预模型：从简单口头提醒，到口头+手势提示，再到结合物理导航和逐步指导的完整多模态引导。系统通过LLM驱动的交互策略和多模态感知实时评估任务状态。

Result: 在实验室环境中对健康成人和痴呆症照护利益相关者进行了初步研究，评估了系统的可用性、可理解性和适应性反馈机制的适当性。

Conclusion: 这项工作贡献了：(1)基于职业治疗原则的自适应辅助框架，(2)通过渐进式支持维护痴呆症患者尊严的多模态机器人实现，(3)关于利益相关者对适应性机器人照护看法的实证见解。

Abstract: People living with dementia (PLWDs) face progressively declining abilities in
medication management-from simple forgetfulness to complete task breakdown-yet
most assistive technologies fail to adapt to these changing needs. This
one-size-fits-all approach undermines autonomy, accelerates dependence, and
increases caregiver burden. Occupational therapy principles emphasize matching
assistance levels to individual capabilities: minimal reminders for those who
merely forget, spatial guidance for those who misplace items, and comprehensive
multimodal support for those requiring step-by-step instruction. However,
existing robotic systems lack this adaptive, graduated response framework
essential for maintaining PLWD independence. We present an adaptive multimodal
robotic framework using the Pepper robot that dynamically adjusts assistance
based on real-time assessment of user needs. Our system implements a
hierarchical intervention model progressing from (1) simple verbal reminders,
to (2) verbal + gestural cues, to (3) full multimodal guidance combining
physical navigation to medication locations with step-by-step verbal and
gestural instructions. Powered by LLM-driven interaction strategies and
multimodal sensing, the system continuously evaluates task states to provide
just-enough assistance-preserving autonomy while ensuring medication adherence.
We conducted a preliminary study with healthy adults and dementia care
stakeholders in a controlled lab setting, evaluating the system's usability,
comprehensibility, and appropriateness of adaptive feedback mechanisms. This
work contributes: (1) a theoretically grounded adaptive assistance framework
translating occupational therapy principles into HRI design, (2) a multimodal
robotic implementation that preserves PLWD dignity through graduated support,
and (3) empirical insights into stakeholder perceptions of adaptive robotic
care.

</details>


### [10] [RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training](https://arxiv.org/abs/2510.06710)
*Hongzhi Zang,Mingjie Wei,Si Xu,Yongji Wu,Zhen Guo,Yuanqing Wang,Hao Lin,Liangzhi Shi,Yuqing Xie,Zhexuan Xu,Zhihao Liu,Kang Chen,Wenhao Tang,Quanlu Zhang,Weinan Zhang,Chao Yu,Yu Wang*

Main category: cs.RO

TL;DR: 提出了RLinf-VLA框架，用于视觉语言动作模型的可扩展强化学习训练，解决了监督微调泛化能力不足的问题，在仿真和真实机器人上均表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型主要使用监督微调训练，在分布偏移下泛化能力不足。强化学习能直接优化任务性能，但缺乏统一的训练平台进行公平比较。

Method: 开发了RLinf-VLA统一框架，采用灵活资源分配设计，支持多种VLA架构、RL算法和仿真器，实现了混合细粒度流水线分配模式。

Result: 在130个LIBERO任务上达到98.11%成功率，25个ManiSkill任务上达到97.66%成功率。真实机器人部署显示RL训练策略比SFT具有更强泛化能力。

Conclusion: RLinf-VLA为具身智能研究提供了加速和标准化基础，并总结了一套RL应用于VLA训练的最佳实践。

Abstract: Recent progress in vision and language foundation models has significantly
advanced multimodal understanding, reasoning, and generation, inspiring a surge
of interest in extending such capabilities to embodied settings through
vision-language-action (VLA) models. Yet, most VLA models are still trained
with supervised fine-tuning (SFT), which struggles to generalize under
distribution shifts due to error accumulation. Reinforcement learning (RL)
offers a promising alternative by directly optimizing task performance through
interaction, but existing attempts remain fragmented and lack a unified
platform for fair and systematic comparison across model architectures and
algorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and
efficient framework for scalable RL training of VLA models. The system adopts a
highly flexible resource allocation design that addresses the challenge of
integrating rendering, training, and inference in RL+VLA training. In
particular, for GPU-parallelized simulators, RLinf-VLA implements a novel
hybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup
in training. Through a unified interface, RLinf-VLA seamlessly supports diverse
VLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g.,
PPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a
unified model achieves 98.11\% across 130 LIBERO tasks and 97.66\% across 25
ManiSkill tasks. Beyond empirical performance, our study distills a set of best
practices for applying RL to VLA training and sheds light on emerging patterns
in this integration. Furthermore, we present preliminary deployment on a
real-world Franka robot, where RL-trained policies exhibit stronger
generalization than those trained with SFT. We envision RLinf-VLA as a
foundation to accelerate and standardize research on embodied intelligence.

</details>


### [11] [SanDRA: Safe Large-Language-Model-Based Decision Making for Automated Vehicles Using Reachability Analysis](https://arxiv.org/abs/2510.06717)
*Yuanfei Lin,Sebastian Illing,Matthias Althoff*

Main category: cs.RO

TL;DR: 提出了SanDRA框架，这是首个使用可达性分析的安全大语言模型自动驾驶决策系统，通过将LLM生成的驾驶动作转化为时序逻辑公式并集成可达性分析来确保安全性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动驾驶决策存在幻觉问题和缺乏车辆动力学集成，无法确保决策安全性。

Method: 首先全面描述驾驶场景，提示LLM生成并排序可行驾驶动作；然后将这些动作转化为包含形式化交通规则的时序逻辑公式；最后集成可达性分析来消除不安全动作。

Result: 在开环和闭环驾驶环境中验证，使用现成和微调的LLM，证明该框架能提供可证明安全的驾驶动作，在高密度交通条件下也能保持合法合规。

Conclusion: SanDRA框架成功解决了LLM决策的安全性问题，为自动驾驶提供了可证明安全的决策支持，所有代码和实验设置已开源。

Abstract: Large language models have been widely applied to knowledge-driven
decision-making for automated vehicles due to their strong generalization and
reasoning capabilities. However, the safety of the resulting decisions cannot
be ensured due to possible hallucinations and the lack of integrated vehicle
dynamics. To address this issue, we propose SanDRA, the first safe
large-language-model-based decision making framework for automated vehicles
using reachability analysis. Our approach starts with a comprehensive
description of the driving scenario to prompt large language models to generate
and rank feasible driving actions. These actions are translated into temporal
logic formulas that incorporate formalized traffic rules, and are subsequently
integrated into reachability analysis to eliminate unsafe actions. We validate
our approach in both open-loop and closed-loop driving environments using
off-the-shelf and finetuned large language models, showing that it can provide
provably safe and, where possible, legally compliant driving actions, even
under high-density traffic conditions. To ensure transparency and facilitate
future research, all code and experimental setups are publicly available at
github.com/CommonRoad/SanDRA.

</details>


### [12] [UniFField: A Generalizable Unified Neural Feature Field for Visual, Semantic, and Spatial Uncertainties in Any Scene](https://arxiv.org/abs/2510.06754)
*Christian Maurer,Snehal Jauhri,Sophie Lueth,Georgia Chalvatzaki*

Main category: cs.RO

TL;DR: UniFField是一个统一的不确定性感知神经特征场，结合视觉、语义和几何特征，同时预测每个模态的不确定性，可零样本应用于新环境。


<details>
  <summary>Details</summary>
Motivation: 现有3D神经特征场方法存在两个关键局限：通常是场景特定的，且缺乏对预测不确定性的建模能力，而机器人需要评估感知信息的可靠性以做出稳健决策。

Method: 提出基于体素的统一特征表示，增量集成RGB-D图像，同时更新不确定性估计，可零样本应用于新环境。

Result: 不确定性估计能准确描述场景重建和语义特征预测中的模型预测误差，成功应用于移动机械臂的主动物体搜索任务。

Conclusion: UniFField能够实现稳健决策，展示了不确定性感知特征场在机器人任务中的有效性。

Abstract: Comprehensive visual, geometric, and semantic understanding of a 3D scene is
crucial for successful execution of robotic tasks, especially in unstructured
and complex environments. Additionally, to make robust decisions, it is
necessary for the robot to evaluate the reliability of perceived information.
While recent advances in 3D neural feature fields have enabled robots to
leverage features from pretrained foundation models for tasks such as
language-guided manipulation and navigation, existing methods suffer from two
critical limitations: (i) they are typically scene-specific, and (ii) they lack
the ability to model uncertainty in their predictions. We present UniFField, a
unified uncertainty-aware neural feature field that combines visual, semantic,
and geometric features in a single generalizable representation while also
predicting uncertainty in each modality. Our approach, which can be applied
zero shot to any new environment, incrementally integrates RGB-D images into
our voxel-based feature representation as the robot explores the scene,
simultaneously updating uncertainty estimation. We evaluate our uncertainty
estimations to accurately describe the model prediction errors in scene
reconstruction and semantic feature prediction. Furthermore, we successfully
leverage our feature predictions and their respective uncertainty for an active
object search task using a mobile manipulator robot, demonstrating the
capability for robust decision-making.

</details>


### [13] [Distributed 3D Source Seeking via SO(3) Geometric Control of Robot Swarms](https://arxiv.org/abs/2510.06836)
*Jesús Bautista,Héctor García de Marina*

Main category: cs.RO

TL;DR: 提出了在SO(3)李群上的几何控制框架，用于具有一阶姿态动力学和恒定平移速度的机器人进行3D源搜索。


<details>
  <summary>Details</summary>
Motivation: 通过直接在SO(3)上工作，避免欧拉角奇异性和四元数歧义性，提供独特的内在方向表示。

Method: 设计了比例前馈控制器，确保每个智能体与估计的3D标量场源上升方向呈指数对齐。控制器适应有界未知变化并保持良好形成的群集编队。

Result: 数值模拟证明了该方法的有效性，所有代码开源提供以确保可重现性。

Conclusion: 该框架为3D源搜索提供了鲁棒且无奇异的几何控制解决方案。

Abstract: This paper presents a geometric control framework on the Lie group SO(3) for
3D source-seeking by robots with first-order attitude dynamics and constant
translational speed. By working directly on SO(3), the approach avoids
Euler-angle singularities and quaternion ambiguities, providing a unique,
intrinsic representation of orientation. We design a proportional feed-forward
controller that ensures exponential alignment of each agent to an estimated
ascending direction toward a 3D scalar field source. The controller adapts to
bounded unknown variations and preserves well-posed swarm formations. Numerical
simulations demonstrate the effectiveness of the method, with all code provided
open source for reproducibility.

</details>


### [14] [Tailoring materials into kirigami robots](https://arxiv.org/abs/2510.07027)
*Saravana Prashanth Murali Babu,Aida Parvaresh,Ahmad Rafsanjani*

Main category: cs.RO

TL;DR: 这篇论文探讨了剪纸艺术在机器人技术中的应用潜力，包括执行器、传感器、电池、控制器和结构体的多功能集成，展示了其在抓取、移动和可穿戴设备等领域的适应性。


<details>
  <summary>Details</summary>
Motivation: 传统剪纸工艺具有轻量化、多功能和适应性强的特点，可以为机器人技术提供创新的解决方案，特别是在需要抗拉伸力和形状变换的应用中。

Method: 通过优化剪纸图案来定制机器人组件，包括基于剪纸原理的执行器、传感器、集成电池和控制机制，这些组件能够通过不同能源实现复杂运动并模拟机械计算。

Result: 剪纸机器人组件展现出良好的性能：执行器可实现程序化复杂运动，传感器连接导电性与柔顺性，集成电池增强结构灵活性和紧凑性，控制机制实现形状变换和记忆功能。

Conclusion: 剪纸技术在机器人领域具有巨大潜力，能够创造多功能、轻量化和适应性强的解决方案，但在剪纸图案设计和制造技术优化方面仍面临挑战。

Abstract: Kirigami, the traditional paper-cutting craft, holds immense potential for
revolutionizing robotics by providing multifunctional, lightweight, and
adaptable solutions. Kirigami structures, characterized by their
bending-dominated deformation, offer resilience to tensile forces and
facilitate shape morphing under small actuation forces. Kirigami components
such as actuators, sensors, batteries, controllers, and body structures can be
tailored to specific robotic applications by optimizing cut patterns. Actuators
based on kirigami principles exhibit complex motions programmable through
various energy sources, while kirigami sensors bridge the gap between
electrical conductivity and compliance. Kirigami-integrated batteries enable
energy storage directly within robot structures, enhancing flexibility and
compactness. Kirigami-controlled mechanisms mimic mechanical computations,
enabling advanced functionalities such as shape morphing and memory functions.
Applications of kirigami-enabled robots include grasping, locomotion, and
wearables, showcasing their adaptability to diverse environments and tasks.
Despite promising opportunities, challenges remain in the design of cut
patterns for a given function and streamlining fabrication techniques.

</details>


### [15] [Temporal-Prior-Guided View Planning for Periodic 3D Plant Reconstruction](https://arxiv.org/abs/2510.07028)
*Sicong Pan,Xuying Huang,Maren Bennewitz*

Main category: cs.RO

TL;DR: 提出了一种基于时间先验的视图规划方法，用于周期性植物3D重建，利用先前重建模型指导新观测的视图采集，减少资源浪费。


<details>
  <summary>Details</summary>
Motivation: 周期性3D重建对作物监测至关重要，但每次从头开始重建成本高昂，浪费资源且忽略了先前捕获的信息。

Method: 将先前重建模型非刚性对齐到新的部分观测，形成当前几何的近似；为适应植物生长，膨胀该近似并求解集合覆盖优化问题计算最小视图集；集成完整流程，在配准前获取额外最佳视图以提高鲁棒性，规划全局最短路径连接视图序列。

Result: 在玉米和番茄上的半球和球体视图空间实验中，系统在保持或提高表面覆盖率的同时，需要更少的视图和可比较的运动成本。

Conclusion: 该方法通过利用时间先验信息，有效减少了周期性植物重建所需的视图数量，同时保持了重建质量。

Abstract: Periodic 3D reconstruction is essential for crop monitoring, but costly when
each cycle restarts from scratch, wasting resources and ignoring information
from previous captures. We propose temporal-prior-guided view planning for
periodic plant reconstruction, in which a previously reconstructed model of the
same plant is non-rigidly aligned to a new partial observation to form an
approximation of the current geometry. To accommodate plant growth, we inflate
this approximation and solve a set covering optimization problem to compute a
minimal set of views. We integrated this method into a complete pipeline that
acquires one additional next-best view before registration for robustness and
then plans a globally shortest path to connect the planned set of views and
outputs the best view sequence. Experiments on maize and tomato under
hemisphere and sphere view spaces show that our system maintains or improves
surface coverage while requiring fewer views and comparable movement cost
compared to state-of-the-art baselines.

</details>


### [16] [Diffusing Trajectory Optimization Problems for Recovery During Multi-Finger Manipulation](https://arxiv.org/abs/2510.07030)
*Abhinav Kumar,Fan Yang,Sergio Aguilera Marinovic,Soshi Iba,Rana Soltani Zarrin,Dmitry Berenson*

Main category: cs.RO

TL;DR: 提出基于扩散模型的恢复框架，用于多指手在操作任务中检测异常状态并规划接触丰富的恢复轨迹，提高任务成功率96%


<details>
  <summary>Details</summary>
Motivation: 多指手在执行精细操作任务时，环境扰动或执行错误会阻碍任务性能，需要恢复行为来恢复正常任务执行

Method: 使用扩散模型检测任务执行不适宜的状态（作为分布外检测问题），通过扩散采样将状态投影回分布内，并用轨迹优化规划接触丰富的恢复轨迹。还提出新的扩散方法来高效扩散恢复轨迹优化问题的完整参数化

Result: 在硬件螺丝刀转动任务中，使用该方法恢复可将任务性能提高96%，且是唯一能在不导致灾难性任务失败的情况下尝试恢复的方法

Conclusion: 基于扩散模型的恢复框架能有效检测异常状态并规划恢复轨迹，显著提高多指手在接触丰富操作任务中的鲁棒性和成功率

Abstract: Multi-fingered hands are emerging as powerful platforms for performing fine
manipulation tasks, including tool use. However, environmental perturbations or
execution errors can impede task performance, motivating the use of recovery
behaviors that enable normal task execution to resume. In this work, we take
advantage of recent advances in diffusion models to construct a framework that
autonomously identifies when recovery is necessary and optimizes contact-rich
trajectories to recover. We use a diffusion model trained on the task to
estimate when states are not conducive to task execution, framed as an
out-of-distribution detection problem. We then use diffusion sampling to
project these states in-distribution and use trajectory optimization to plan
contact-rich recovery trajectories. We also propose a novel diffusion-based
approach that distills this process to efficiently diffuse the full
parameterization, including constraints, goal state, and initialization, of the
recovery trajectory optimization problem, saving time during online execution.
We compare our method to a reinforcement learning baseline and other methods
that do not explicitly plan contact interactions, including on a hardware
screwdriver-turning task where we show that recovering using our method
improves task performance by 96% and that ours is the only method evaluated
that can attempt recovery without causing catastrophic task failure. Videos can
be found at https://dtourrecovery.github.io/.

</details>


### [17] [Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied AI Commands on VLA Models](https://arxiv.org/abs/2510.07067)
*Daria Pugacheva,Andrey Moskalenko,Denis Shepelev,Andrey Kuznetsov,Vlad Shakhuro,Elena Tutubalina*

Main category: cs.RO

TL;DR: 该论文系统研究了VLA模型对语言扰动的鲁棒性，发现模型在语义和词汇相似的长上下文干扰下性能下降约50%，并提出基于LLM的过滤框架来恢复性能。


<details>
  <summary>Details</summary>
Motivation: VLA模型在具身AI中广泛应用，但其在真实场景中对自然语言变化的鲁棒性尚未得到充分研究。

Method: 评估VLA模型在两种指令噪声下的性能：(1)人工生成的释义；(2)添加无关上下文，并根据长度和语义/词汇相似度对无关上下文进行分类。

Result: 随着上下文扩展，模型性能持续下降；对随机上下文相对鲁棒（性能下降<10%），但对语义和词汇相似的上下文性能下降约50%；人工释义导致性能下降近20%。

Conclusion: 提出的基于LLM的过滤框架能够从噪声输入中提取核心指令，使模型在噪声条件下恢复高达98.5%的原始性能。

Abstract: Vision Language Action (VLA) models are widely used in Embodied AI, enabling
robots to interpret and execute language instructions. However, their
robustness to natural language variability in real-world scenarios has not been
thoroughly investigated. In this work, we present a novel systematic study of
the robustness of state-of-the-art VLA models under linguistic perturbations.
Specifically, we evaluate model performance under two types of instruction
noise: (1) human-generated paraphrasing and (2) the addition of irrelevant
context. We further categorize irrelevant contexts into two groups according to
their length and their semantic and lexical proximity to robot commands. In
this study, we observe consistent performance degradation as context size
expands. We also demonstrate that the model can exhibit relative robustness to
random context, with a performance drop within 10%, while semantically and
lexically similar context of the same length can trigger a quality decline of
around 50%. Human paraphrases of instructions lead to a drop of nearly 20%. To
mitigate this, we propose an LLM-based filtering framework that extracts core
commands from noisy inputs. Incorporating our filtering step allows models to
recover up to 98.5% of their original performance under noisy conditions.

</details>


### [18] [Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications](https://arxiv.org/abs/2510.07077)
*Kento Kawaharazuka,Jihoon Oh,Jun Yamada,Ingmar Posner,Yuke Zhu*

Main category: cs.RO

TL;DR: 这篇论文对视觉-语言-动作(VLA)模型进行了全面综述，涵盖软件和硬件组件，旨在为机器人社区在实际系统中应用VLA提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和视觉语言模型在机器人领域的应用日益增多，VLA模型通过统一视觉、语言和动作数据，旨在学习能够泛化到多样化任务、对象、体现和环境的策略。

Method: 提供了系统性的VLA综述，涵盖策略和架构转变、架构和构建模块、模态特定处理技术、学习范式，以及机器人平台、数据收集策略、数据集、数据增强方法和评估基准。

Result: 构建了一个全面的VLA系统调查框架，整合了软件和硬件组件，并提供了按训练方法、评估方法、模态和数据集分类的参考资料。

Conclusion: 这项工作为机器人社区在实际机器人系统中应用VLA模型提供了实用指导，促进了更灵活和可扩展的现实世界部署。

Abstract: Amid growing efforts to leverage advances in large language models (LLMs) and
vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models
have recently gained significant attention. By unifying vision, language, and
action data at scale, which have traditionally been studied separately, VLA
models aim to learn policies that generalise across diverse tasks, objects,
embodiments, and environments. This generalisation capability is expected to
enable robots to solve novel downstream tasks with minimal or no additional
task-specific data, facilitating more flexible and scalable real-world
deployment. Unlike previous surveys that focus narrowly on action
representations or high-level model architectures, this work offers a
comprehensive, full-stack review, integrating both software and hardware
components of VLA systems. In particular, this paper provides a systematic
review of VLAs, covering their strategy and architectural transition,
architectures and building blocks, modality-specific processing techniques, and
learning paradigms. In addition, to support the deployment of VLAs in
real-world robotic applications, we also review commonly used robot platforms,
data collection strategies, publicly available datasets, data augmentation
methods, and evaluation benchmarks. Throughout this comprehensive survey, this
paper aims to offer practical guidance for the robotics community in applying
VLAs to real-world robotic systems. All references categorized by training
approach, evaluation method, modality, and dataset are available in the table
on our project website: https://vla-survey.github.io .

</details>


### [19] [Sampling Strategies for Robust Universal Quadrupedal Locomotion Policies](https://arxiv.org/abs/2510.07094)
*David Rytz,Kim Tien Ly,Ioannis Havoutis*

Main category: cs.RO

TL;DR: 该研究比较了三种关节增益采样策略，通过在训练中随机化物理机器人参数和关节PD增益，生成能够泛化到多种参数配置的通用四足机器人运动策略，显著缩小了仿真到现实的差距。


<details>
  <summary>Details</summary>
Motivation: 研究目标是开发能够适应多种物理参数配置的鲁棒通用四足机器人运动策略，解决仿真到现实部署中的泛化问题。

Method: 比较了三种关节增益采样策略：线性/多项式函数映射的质量-增益关系、基于性能的自适应滤波、均匀随机采样，并通过偏置配置和使用参考模型来提高策略鲁棒性。

Result: 结果表明，显著的关节控制器增益随机化对于鲁棒地缩小仿真到现实差距至关重要，所有训练在RaiSim中进行，并在ANYmal四足机器人上实现了零样本硬件部署。

Conclusion: 关节控制器增益的显著随机化是实现鲁棒仿真到现实迁移的关键因素，所提出的采样策略能够有效生成通用四足机器人运动策略。

Abstract: This work focuses on sampling strategies of configuration variations for
generating robust universal locomotion policies for quadrupedal robots. We
investigate the effects of sampling physical robot parameters and joint
proportional-derivative gains to enable training a single reinforcement
learning policy that generalizes to multiple parameter configurations. Three
fundamental joint gain sampling strategies are compared: parameter sampling
with (1) linear and polynomial function mappings of mass-to-gains, (2)
performance-based adaptive filtering, and (3) uniform random sampling. We
improve the robustness of the policy by biasing the configurations using
nominal priors and reference models. All training was conducted on RaiSim,
tested in simulation on a range of diverse quadrupeds, and zero-shot deployed
onto hardware using the ANYmal quadruped robot. Compared to multiple baseline
implementations, our results demonstrate the need for significant joint
controller gains randomization for robust closing of the sim-to-real gap.

</details>


### [20] [A Digital Twin Framework for Metamorphic Testing of Autonomous Driving Systems Using Generative Model](https://arxiv.org/abs/2510.07133)
*Tony Zhang,Burak Kantarci,Umair Siddique*

Main category: cs.RO

TL;DR: 提出了一种基于数字孪生的蜕变测试框架，通过结合数字孪生技术和AI图像生成模型，为自动驾驶系统生成多样化且真实的驾驶场景，显著提升测试覆盖率和效果。


<details>
  <summary>Details</summary>
Motivation: 传统测试方法面临oracle问题和场景覆盖不足的挑战，难以确保自动驾驶汽车在复杂不可预测的真实驾驶环境中的安全性。

Method: 创建自动驾驶系统及其运行环境的虚拟副本，结合数字孪生和Stable Diffusion等AI图像生成模型，系统生成保持原始场景核心语义的多样化驾驶场景，并定义了三个基于真实交通规则和车辆行为的蜕变关系。

Result: 在Udacity自动驾驶模拟器中验证，相比基线方法获得了最高的真阳性率(0.719)、F1分数(0.689)和精确率(0.662)。

Conclusion: 将数字孪生与AI驱动的场景生成相结合，为自动驾驶车辆安全提供了可扩展、自动化且高保真的测试解决方案。

Abstract: Ensuring the safety of self-driving cars remains a major challenge due to the
complexity and unpredictability of real-world driving environments. Traditional
testing methods face significant limitations, such as the oracle problem, which
makes it difficult to determine whether a system's behavior is correct, and the
inability to cover the full range of scenarios an autonomous vehicle may
encounter. In this paper, we introduce a digital twin-driven metamorphic
testing framework that addresses these challenges by creating a virtual replica
of the self-driving system and its operating environment. By combining digital
twin technology with AI-based image generative models such as Stable Diffusion,
our approach enables the systematic generation of realistic and diverse driving
scenes. This includes variations in weather, road topology, and environmental
features, all while maintaining the core semantics of the original scenario.
The digital twin provides a synchronized simulation environment where changes
can be tested in a controlled and repeatable manner. Within this environment,
we define three metamorphic relations inspired by real-world traffic rules and
vehicle behavior. We validate our framework in the Udacity self-driving
simulator and demonstrate that it significantly enhances test coverage and
effectiveness. Our method achieves the highest true positive rate (0.719), F1
score (0.689), and precision (0.662) compared to baseline approaches. This
paper highlights the value of integrating digital twins with AI-powered
scenario generation to create a scalable, automated, and high-fidelity testing
solution for autonomous vehicle safety.

</details>


### [21] [TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking](https://arxiv.org/abs/2510.07134)
*Jiahang Liu,Yunpeng Qi,Jiazhao Zhang,Minghan Li,Shaoan Wang,Kui Wu,Hanjing Ye,Hong Zhang,Zhibo Chen,Fangwei Zhong,Zhizheng Zhang,He Wang*

Main category: cs.RO

TL;DR: TrackVLA++是一个视觉-语言-动作模型，通过空间推理机制和目标识别记忆模块，解决了现有方法在严重遮挡和相似干扰物情况下跟踪失败的问题，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的语言引导跟踪方法缺乏明确的空间推理和有效的时间记忆，导致在严重遮挡或存在相似干扰物时跟踪失败。

Method: 提出TrackVLA++模型，包含两个关键模块：1）空间推理机制（Polar-CoT），通过思维链范式推断目标相对位置并编码为极坐标标记；2）目标识别记忆（TIM），采用门控更新策略保持长期目标记忆。

Result: 在公开基准测试中达到最先进性能，在EVT-Bench DT分割上分别超过之前领先方法5.1和12个百分点，并展现出强大的零样本泛化能力。

Conclusion: TrackVLA++通过空间推理和长期记忆机制，显著提升了在动态和遮挡场景下的鲁棒跟踪能力，为实际应用提供了可靠解决方案。

Abstract: Embodied Visual Tracking (EVT) is a fundamental ability that underpins
practical applications, such as companion robots, guidance robots and service
assistants, where continuously following moving targets is essential. Recent
advances have enabled language-guided tracking in complex and unstructured
scenes. However, existing approaches lack explicit spatial reasoning and
effective temporal memory, causing failures under severe occlusions or in the
presence of similar-looking distractors. To address these challenges, we
present TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances
embodied visual tracking with two key modules, a spatial reasoning mechanism
and a Target Identification Memory (TIM). The reasoning module introduces a
Chain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative
position and encodes it as a compact polar-coordinate token for action
prediction. Guided by these spatial priors, the TIM employs a gated update
strategy to preserve long-horizon target memory, ensuring spatiotemporal
consistency and mitigating target loss during extended occlusions. Extensive
experiments show that TrackVLA++ achieves state-of-the-art performance on
public benchmarks across both egocentric and multi-camera settings. On the
challenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading
approach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong
zero-shot generalization, enabling robust real-world tracking in dynamic and
occluded scenarios.

</details>


### [22] [DPL: Depth-only Perceptive Humanoid Locomotion via Realistic Depth Synthesis and Cross-Attention Terrain Reconstruction](https://arxiv.org/abs/2510.07152)
*Jingkai Sun,Gang Han,Pihai Sun,Wen Zhao,Jiahang Cao,Jiaxu Wang,Yijie Guo,Qiang Zhang*

Main category: cs.RO

TL;DR: 提出了一种新型人形机器人地形感知运动框架，通过结合盲主干地形感知运动策略、多模态交叉注意力变换器和真实深度图像合成方法，解决了现有方法的训练效率低、仿真到现实差距大、延迟高等问题。


<details>
  <summary>Details</summary>
Motivation: 现有地形感知人形机器人运动方法存在两大问题：基于深度图像的端到端学习方法训练效率低且仿真到现实差距大；基于高程图的方法依赖多个视觉传感器和定位系统，导致延迟和鲁棒性降低。

Method: 1) 盲主干地形感知运动策略，利用预训练的高程图感知指导强化学习；2) 多模态交叉注意力变换器，从噪声深度图像重建结构化地形表示；3) 真实深度图像合成方法，采用自遮挡感知光线投射和噪声感知建模。

Result: 实现了超过30%的地形重建误差减少，在完整尺寸人形机器人上验证了框架有效性，展示了在多样化挑战性地形上的敏捷自适应运动能力。

Conclusion: 该框架能够在有限数据和硬件资源下实现高效策略训练，同时保留对泛化至关重要的关键地形特征，为人形机器人的地形感知运动提供了有效解决方案。

Abstract: Recent advancements in legged robot perceptive locomotion have shown
promising progress. However, terrain-aware humanoid locomotion remains largely
constrained to two paradigms: depth image-based end-to-end learning and
elevation map-based methods. The former suffers from limited training
efficiency and a significant sim-to-real gap in depth perception, while the
latter depends heavily on multiple vision sensors and localization systems,
resulting in latency and reduced robustness. To overcome these challenges, we
propose a novel framework that tightly integrates three key components: (1)
Terrain-Aware Locomotion Policy with a Blind Backbone, which leverages
pre-trained elevation map-based perception to guide reinforcement learning with
minimal visual input; (2) Multi-Modality Cross-Attention Transformer, which
reconstructs structured terrain representations from noisy depth images; (3)
Realistic Depth Images Synthetic Method, which employs self-occlusion-aware ray
casting and noise-aware modeling to synthesize realistic depth observations,
achieving over 30\% reduction in terrain reconstruction error. This combination
enables efficient policy training with limited data and hardware resources,
while preserving critical terrain features essential for generalization. We
validate our framework on a full-sized humanoid robot, demonstrating agile and
adaptive locomotion across diverse and challenging terrains.

</details>


### [23] [A Narwhal-Inspired Sensing-to-Control Framework for Small Fixed-Wing Aircraft](https://arxiv.org/abs/2510.07160)
*Fengze Xie,Xiaozhou Fan,Jacob Schuster,Yisong Yue,Morteza Gharib*

Main category: cs.RO

TL;DR: 提出了一种结合仿生硬件、物理信息动力学学习和凸控制分配的端到端感知控制管道，用于提升固定翼无人机的低速敏捷性。


<details>
  <summary>Details</summary>
Motivation: 固定翼无人机具有续航和效率优势，但由于高度耦合的动力学特性，缺乏低速敏捷性。测量小型机架上的气流很困难，因为近体空气动力学、螺旋桨滑流、控制面作动和环境阵风会扭曲压力信号。

Method: 1. 仿生硬件设计：基于独角鲸长牙灵感，在机身前部安装自制多孔探头，辅以稀疏分布的机翼压力传感器；2. 数据驱动校准：将探头压力映射到空速和流动角度；3. 控制仿射动力学模型学习：使用估计的空速/角度和稀疏传感器；4. 软左右对称正则化器提高可识别性；5. 正则化最小二乘分配器实现平滑、配平的作动。

Result: 风洞研究表明：添加机翼压力可将力估计误差降低25-30%；所提模型在分布偏移下性能下降较少（约12% vs 无结构基线的44%）；力跟踪性能随输入平滑度提升而改善，与普通仿射模型相比，法向力RMSE降低27%，与无结构基线相比降低34%。

Conclusion: 该端到端方法通过仿生传感、物理信息建模和凸控制分配，有效提升了固定翼无人机的低速敏捷性和控制性能，特别是在气流扰动环境下表现出更好的鲁棒性。

Abstract: Fixed-wing unmanned aerial vehicles (UAVs) offer endurance and efficiency but
lack low-speed agility due to highly coupled dynamics. We present an end-to-end
sensing-to-control pipeline that combines bio-inspired hardware,
physics-informed dynamics learning, and convex control allocation. Measuring
airflow on a small airframe is difficult because near-body aerodynamics,
propeller slipstream, control-surface actuation, and ambient gusts distort
pressure signals. Inspired by the narwhal's protruding tusk, we mount in-house
multi-hole probes far upstream and complement them with sparse, carefully
placed wing pressure sensors for local flow measurement. A data-driven
calibration maps probe pressures to airspeed and flow angles. We then learn a
control-affine dynamics model using the estimated airspeed/angles and sparse
sensors. A soft left/right symmetry regularizer improves identifiability under
partial observability and limits confounding between wing pressures and
flaperon inputs. Desired wrenches (forces and moments) are realized by a
regularized least-squares allocator that yields smooth, trimmed actuation.
Wind-tunnel studies across a wide operating range show that adding wing
pressures reduces force-estimation error by 25-30%, the proposed model degrades
less under distribution shift (about 12% versus 44% for an unstructured
baseline), and force tracking improves with smoother inputs, including a 27%
reduction in normal-force RMSE versus a plain affine model and 34% versus an
unstructured baseline.

</details>


### [24] [TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics](https://arxiv.org/abs/2510.07181)
*Yi Han,Cheng Chi,Enshen Zhou,Shanyu Rong,Jingkun An,Pengwei Wang,Zhongyuan Wang,Lu Sheng,Shanghang Zhang*

Main category: cs.RO

TL;DR: TIGeR框架将视觉语言模型从感知估计器转变为几何计算机，通过外部工具生成和执行精确几何计算，实现机器人操作所需的厘米级精度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在空间推理中仅限于定性精度，缺乏机器人实际应用所需的计算精度，无法利用深度传感器和相机标定的度量线索。

Method: 采用两阶段训练流程：监督微调(SFT)和强化微调(RFT)，结合分层奖励设计，让模型识别几何推理需求、合成计算代码并调用专业库进行精确计算。

Result: 在几何推理基准测试中达到SOTA性能，在真实世界机器人操作任务中展示厘米级精度。

Conclusion: TIGeR框架成功将VLMs从模式识别器转变为几何计算机，通过工具集成实现了机器人操作所需的精确几何推理能力。

Abstract: Vision-Language Models (VLMs) have shown remarkable capabilities in spatial
reasoning, yet they remain fundamentally limited to qualitative precision and
lack the computational precision required for real-world robotics. Current
approaches fail to leverage metric cues from depth sensors and camera
calibration, instead reducing geometric problems to pattern recognition tasks
that cannot deliver the centimeter-level accuracy essential for robotic
manipulation. We present TIGeR (Tool-Integrated Geometric Reasoning), a novel
framework that transforms VLMs from perceptual estimators to geometric
computers by enabling them to generate and execute precise geometric
computations through external tools. Rather than attempting to internalize
complex geometric operations within neural networks, TIGeR empowers models to
recognize geometric reasoning requirements, synthesize appropriate
computational code, and invoke specialized libraries for exact calculations. To
support this paradigm, we introduce TIGeR-300K, a comprehensive
tool-invocation-oriented dataset covering point transformations, pose
estimation, trajectory generation, and spatial compatibility verification,
complete with tool invocation sequences and intermediate computations. Through
a two-stage training pipeline combining supervised fine-tuning (SFT) and
reinforcement fine-tuning (RFT) with our proposed hierarchical reward design,
TIGeR achieves SOTA performance on geometric reasoning benchmarks while
demonstrating centimeter-level precision in real-world robotic manipulation
tasks.

</details>


### [25] [COMPAct: Computational Optimization and Automated Modular design of Planetary Actuators](https://arxiv.org/abs/2510.07197)
*Aman Singh,Deepak Kapa,Suryank Joshi,Shishir Kolathaya*

Main category: cs.RO

TL;DR: COMPAct框架系统优化四种行星齿轮箱参数并自动生成CAD模型，实现直接3D打印，实验验证了SSPG和CPG执行器的性能。


<details>
  <summary>Details</summary>
Motivation: 机器人执行器优化设计研究不足，特别是齿轮箱参数优化和CAD自动化方面存在空白。

Method: 开发COMPAct框架，系统优化四种行星齿轮箱参数（SSPG、CPG、WPG、DSPG），最小化质量和宽度，最大化效率，并自动化CAD生成。

Result: SSPG执行器效率60-80%，空载背隙0.59度，刚度242.7 Nm/rad；CPG执行器效率60%，背隙2.6度，刚度201.6 Nm/rad。

Conclusion: 该框架能有效优化齿轮箱设计并自动生成CAD，为不同传动比范围提供合适的齿轮箱类型选择。

Abstract: The optimal design of robotic actuators is a critical area of research, yet
limited attention has been given to optimizing gearbox parameters and
automating actuator CAD. This paper introduces COMPAct: Computational
Optimization and Automated Modular Design of Planetary Actuators, a framework
that systematically identifies optimal gearbox parameters for a given motor
across four gearbox types, single-stage planetary gearbox (SSPG), compound
planetary gearbox (CPG), Wolfrom planetary gearbox (WPG), and double-stage
planetary gearbox (DSPG). The framework minimizes mass and actuator width while
maximizing efficiency, and further automates actuator CAD generation to enable
direct 3D printing without manual redesign. Using this framework, optimal
gearbox designs are explored over a wide range of gear ratios, providing
insights into the suitability of different gearbox types across various gear
ratio ranges. In addition, the framework is used to generate CAD models of all
four gearbox types with varying gear ratios and motors. Two actuator types are
fabricated and experimentally evaluated through power efficiency, no-load
backlash, and transmission stiffness tests. Experimental results indicate that
the SSPG actuator achieves a mechanical efficiency of 60-80 %, a no-load
backlash of 0.59 deg, and a transmission stiffness of 242.7 Nm/rad, while the
CPG actuator demonstrates 60 % efficiency, 2.6 deg backlash, and a stiffness of
201.6 Nm/rad. Code available at:
https://anonymous.4open.science/r/COMPAct-SubNum-3408 Video:
https://youtu.be/99zOKgxsDho

</details>


### [26] [HyPlan: Hybrid Learning-Assisted Planning Under Uncertainty for Safe Autonomous Driving](https://arxiv.org/abs/2510.07210)
*Donald Pfaffmann,Matthias Klusch,Marcel Steinmetz*

Main category: cs.RO

TL;DR: 提出HyPlan混合学习方法，用于部分可观测交通环境中自动驾驶汽车的碰撞规避导航，结合多智能体行为预测、深度强化学习和在线POMDP规划


<details>
  <summary>Details</summary>
Motivation: 解决部分可观测交通环境中自动驾驶汽车的碰撞规避导航问题，需要在保证安全的同时提高规划效率

Method: 结合多智能体行为预测、近端策略优化的深度强化学习、基于启发式置信度垂直剪枝的近似在线POMDP规划

Result: 在CARLA-CTS2关键交通场景基准测试中，HyPlan比相关基线方法更安全，比替代在线POMDP规划器显著更快

Conclusion: HyPlan方法在不影响驾驶安全的前提下有效减少了执行时间，在部分可观测环境中实现了安全高效的自动驾驶导航

Abstract: We present a novel hybrid learning-assisted planning method, named HyPlan,
for solving the collision-free navigation problem for self-driving cars in
partially observable traffic environments. HyPlan combines methods for
multi-agent behavior prediction, deep reinforcement learning with proximal
policy optimization and approximated online POMDP planning with heuristic
confidence-based vertical pruning to reduce its execution time without
compromising safety of driving. Our experimental performance analysis on the
CARLA-CTS2 benchmark of critical traffic scenarios with pedestrians revealed
that HyPlan may navigate safer than selected relevant baselines and perform
significantly faster than considered alternative online POMDP planners.

</details>
