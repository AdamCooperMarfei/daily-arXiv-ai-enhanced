<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 29]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Mining the Long Tail: A Comparative Study of Data-Centric Criticality Metrics for Robust Offline Reinforcement Learning in Autonomous Motion Planning](https://arxiv.org/abs/2508.18397)
*Antonio Guillen-Perez*

Main category: cs.RO

TL;DR: 本文通过系统性比较研究六种数据筛选策略，解决离线强化学习中数据不平衡问题，使自动驾驶计划策略更安全可靠。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在大规模实际驾驶日志中遇到极端数据不平衡问题，平凡场景过多而稀有长尾事件过少，导致策略脆弱不安全。

Method: 研究了六种临界权重方案，分为三大类：含义基础、不确定性基础和行为基础。在两个时间尺度（单个时间步和完整场景）进行评估，训练七个目标条件CQL继注器。

Result: 所有数据筛选方法都显著超过基准线，其中使用模型不确定性的方法安全性改善最为显著，碰撞率从16.0%降至5.5%，减少了近三倍。时间步级权重在反应式安全性上表现优异，而场景级权重在长期计划上更好。

Conclusion: 智能的非均匀采样是构建安全可靠自主代理的关键组件，本研究为离线强化学习数据筛选提供了全面框架。

Abstract: Offline Reinforcement Learning (RL) presents a promising paradigm for
training autonomous vehicle (AV) planning policies from large-scale, real-world
driving logs. However, the extreme data imbalance in these logs, where mundane
scenarios vastly outnumber rare "long-tail" events, leads to brittle and unsafe
policies when using standard uniform data sampling. In this work, we address
this challenge through a systematic, large-scale comparative study of data
curation strategies designed to focus the learning process on information-rich
samples. We investigate six distinct criticality weighting schemes which are
categorized into three families: heuristic-based, uncertainty-based, and
behavior-based. These are evaluated at two temporal scales, the individual
timestep and the complete scenario. We train seven goal-conditioned
Conservative Q-Learning (CQL) agents with a state-of-the-art, attention-based
architecture and evaluate them in the high-fidelity Waymax simulator. Our
results demonstrate that all data curation methods significantly outperform the
baseline. Notably, data-driven curation using model uncertainty as a signal
achieves the most significant safety improvements, reducing the collision rate
by nearly three-fold (from 16.0% to 5.5%). Furthermore, we identify a clear
trade-off where timestep-level weighting excels at reactive safety while
scenario-level weighting improves long-horizon planning. Our work provides a
comprehensive framework for data curation in Offline RL and underscores that
intelligent, non-uniform sampling is a critical component for building safe and
reliable autonomous agents.

</details>


### [2] [Maintenance automation: methods for robotics manipulation planning and execution](https://arxiv.org/abs/2508.18399)
*Christian Friedrich,Ralf Gulde,Armin Lechler,Alexander Verl*

Main category: cs.RO

TL;DR: 该论文提出了一个完整的机器人维护自动化系统，能够在环境不确定性下自动化执行拆卸和组装操作。


<details>
  <summary>Details</summary>
Motivation: 自动化复杂机器人任务需要规划、控制和执行技能，特别是在存在环境不确定性（如先验计划信息偏差）的情况下。

Method: 基于CAD和RGBD数据的规划方法，包括符号化计划解释和转换为可执行机器人指令的方法。

Result: 通过真实世界应用进行了实验评估，证明了系统的有效性。

Conclusion: 这是将理论成果转化为实用机器人解决方案的第一步，展示了完整的机器人维护自动化系统的可行性。

Abstract: Automating complex tasks using robotic systems requires skills for planning,
control and execution. This paper proposes a complete robotic system for
maintenance automation, which can automate disassembly and assembly operations
under environmental uncertainties (e.g. deviations between prior plan
information). The cognition of the robotic system is based on a planning
approach (using CAD and RGBD data) and includes a method to interpret a
symbolic plan and transform it to a set of executable robot instructions. The
complete system is experimentally evaluated using real-world applications. This
work shows the first step to transfer these theoretical results into a
practical robotic solution.

</details>


### [3] [Efficient task and path planning for maintenance automation using a robot system](https://arxiv.org/abs/2508.18400)
*Christian Friedrich,Akos Csiszar,Armin Lechler,Alexander Verl*

Main category: cs.RO

TL;DR: 本文提出了一种用于工厂维护自动化的智能机器人系统，结合离线CAD数据和在线RGBD视觉数据，通过概率滤波补偿不确定性，使用基于采样的符号化方法进行任务规划，并采用自适应步长的路径规划算法来减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 工厂维护自动化是未来工厂的关键突破点，需要开发低计算复杂度且能处理环境不确定性的自主机器人系统来完成维护任务。

Method: 结合离线CAD数据和在线RGBD视觉数据通过概率滤波补偿不确定性；使用基于符号化描述的采样方法计算拆卸空间进行任务规划；采用自适应探索步长的全局路径规划算法减少规划时间。

Result: 所有方法都经过实验验证和讨论，证明了系统的可行性和有效性。

Conclusion: 该研究为工厂维护自动化提供了一种有效的解决方案，通过融合多源数据和优化规划算法，实现了在不确定环境中的自主任务执行能力。

Abstract: The research and development of intelligent automation solutions is a
ground-breaking point for the factory of the future. A promising and
challenging mission is the use of autonomous robot systems to automate tasks in
the field of maintenance. For this purpose, the robot system must be able to
plan autonomously the different manipulation tasks and the corresponding paths.
Basic requirements are the development of algorithms with a low computational
complexity and the possibility to deal with environmental uncertainties. In
this work, an approach is presented, which is especially suited to solve the
problem of maintenance automation. For this purpose, offline data from CAD is
combined with online data from an RGBD vision system via a probabilistic
filter, to compensate uncertainties from offline data. For planning the
different tasks, a method is explained, which use a symbolic description,
founded on a novel sampling-based method to compute the disassembly space. For
path planning we use global state-of-the art algorithms with a method that
allows the adaption of the exploration stepsize in order to reduce the planning
time. Every method is experimentally validated and discussed.

</details>


### [4] [PneuGelSight: Soft Robotic Vision-Based Proprioception and Tactile Sensing](https://arxiv.org/abs/2508.18443)
*Ruohan Zhang,Uksang Yoo,Yichen Li,Arpit Argawal,Wenzhen Yuan*

Main category: cs.RO

TL;DR: 提出了一种基于视觉的软体机器人传感方法PneuGelSight，通过嵌入式摄像头实现高分辨率本体感知和触觉感知，并开发了从仿真到真实环境的零样本知识迁移管道。


<details>
  <summary>Details</summary>
Motivation: 软体气动机器人在工业和人际交互应用中具有顺应性和灵活性优势，但在实际部署中需要先进的传感技术来提供触觉反馈和本体感知。

Method: 开发了PneuGelSight气动操作器，采用嵌入式摄像头实现传感功能，并建立了完整的光学和动力学特性仿真管道，支持从仿真到真实环境的零样本迁移。

Result: 实现了高分辨率的本体感知和触觉感知能力，验证了仿真到真实环境的有效知识迁移。

Conclusion: PneuGelSight和仿真到真实管道为软体机器人提供了一种新颖、易于实现且鲁棒的传感方法，为开发具有增强感知能力的先进软体机器人铺平了道路。

Abstract: Soft pneumatic robot manipulators are popular in industrial and
human-interactive applications due to their compliance and flexibility.
However, deploying them in real-world scenarios requires advanced sensing for
tactile feedback and proprioception. Our work presents a novel vision-based
approach for sensorizing soft robots. We demonstrate our approach on
PneuGelSight, a pioneering pneumatic manipulator featuring high-resolution
proprioception and tactile sensing via an embedded camera. To optimize the
sensor's performance, we introduce a comprehensive pipeline that accurately
simulates its optical and dynamic properties, facilitating a zero-shot
knowledge transition from simulation to real-world applications. PneuGelSight
and our sim-to-real pipeline provide a novel, easily implementable, and robust
sensing methodology for soft robots, paving the way for the development of more
advanced soft robots with enhanced sensory capabilities.

</details>


### [5] [Mimicking associative learning of rats via a neuromorphic robot in open field maze using spatial cell models](https://arxiv.org/abs/2508.18460)
*Tianze Liu,Md Abu Bakr Siddique,Hongyu An*

Main category: cs.RO

TL;DR: 本文提出通过模拟动物联想学习机制来增强智能机器人的自主能力，使用神经形态机器人在开放迷宫环境中实现实时空间任务的在线联想学习


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动的AI方法依赖大数据集和神经网络，存在高功耗和适应性有限的问题，特别是在SWaP受限的应用中如行星探索。需要寻找更高效的自主学习方式

Method: 模拟啮齿类动物的联想学习机制，利用空间细胞（位置细胞和网格细胞）的生物学见解，在神经形态机器人中实现关联记忆和实时环境适应

Result: 通过神经形态机器人成功在开放迷宫环境中实现了联想学习能力，能够自主导航动态环境并通过交互学习优化性能

Conclusion: 该方法成功将生物空间认知与机器人技术相结合，为自主系统的发展提供了新途径，特别是在资源受限的环境中具有重要应用价值

Abstract: Data-driven Artificial Intelligence (AI) approaches have exhibited remarkable
prowess across various cognitive tasks using extensive training data. However,
the reliance on large datasets and neural networks presents challenges such as
highpower consumption and limited adaptability, particularly in
SWaP-constrained applications like planetary exploration. To address these
issues, we propose enhancing the autonomous capabilities of intelligent robots
by emulating the associative learning observed in animals. Associative learning
enables animals to adapt to their environment by memorizing concurrent events.
By replicating this mechanism, neuromorphic robots can navigate dynamic
environments autonomously, learning from interactions to optimize performance.
This paper explores the emulation of associative learning in rodents using
neuromorphic robots within open-field maze environments, leveraging insights
from spatial cells such as place and grid cells. By integrating these models,
we aim to enable online associative learning for spatial tasks in real-time
scenarios, bridging the gap between biological spatial cognition and robotics
for advancements in autonomous systems.

</details>


### [6] [SignLoc: Robust Localization using Navigation Signs and Public Maps](https://arxiv.org/abs/2508.18606)
*Nicky Zimmerman,Joel Loo,Ayush Agrawal,David Hsu*

Main category: cs.RO

TL;DR: SignLoc是一种利用导航标志进行全局定位的方法，无需先验传感器建图，仅需观察1-2个标志即可在公开地图上实现鲁棒的拓扑语义定位


<details>
  <summary>Details</summary>
Motivation: 导航标志和地图（如平面图和街道地图）在人类环境中广泛存在，但很少被机器人系统使用。本文旨在利用这些现成的导航标志来实现机器人的全局定位

Method: 首先从输入地图中提取导航图，然后使用概率观测模型将检测到的标志中的方向和位置线索与图进行匹配，在蒙特卡洛框架内实现鲁棒的拓扑语义定位

Result: 在大型环境（大学校园、购物中心、医院综合体）中的实验结果表明，SignLoc在仅观察1-2个标志后就能可靠地定位机器人

Conclusion: SignLoc成功证明了利用现成导航标志和公开地图进行机器人全局定位的可行性，为无需传感器建图的定位提供了新途径

Abstract: Navigation signs and maps, such as floor plans and street maps, are widely
available and serve as ubiquitous aids for way-finding in human environments.
Yet, they are rarely used by robot systems. This paper presents SignLoc, a
global localization method that leverages navigation signs to localize the
robot on publicly available maps -- specifically floor plans and OpenStreetMap
(OSM) graphs -- without prior sensor-based mapping. SignLoc first extracts a
navigation graph from the input map. It then employs a probabilistic
observation model to match directional and locational cues from the detected
signs to the graph, enabling robust topo-semantic localization within a Monte
Carlo framework. We evaluated SignLoc in diverse large-scale environments: part
of a university campus, a shopping mall, and a hospital complex. Experimental
results show that SignLoc reliably localizes the robot after observing only one
to two signs.

</details>


### [7] [Integration of Robot and Scene Kinematics for Sequential Mobile Manipulation Planning](https://arxiv.org/abs/2508.18627)
*Ziyuan Jiao,Yida Niu,Zeyu Zhang,Yangyang Wu,Yao Su,Yixin Zhu,Hangxin Liu,Song-Chun Zhu*

Main category: cs.RO

TL;DR: 通过构建增广配置空间(A-Space)统一导航和操作约束，提出三层次模型来解决长时域移动操作任务，在模拟中任务成功率提升84.6%，并在真实机器人系统上验证了具有可扩展性的复杂操作能力。


<details>
  <summary>Details</summary>
Motivation: 解决长时域多步骤移动操作任务中导航和操作约束分离的问题，需要一种统一的规划方法来处理机器人基座、手臂和操作对象的协调达到性问题。

Method: 构建增广配置空间(A-Space)，将环境结构抽象为运动学模型并与机器人运动学集成，采用任务规划器、优化基于运动规划器和中间规划精炼阶段的三层框架进行规划。

Result: 模拟实验显示任务成功率比基准方法提高84.6%，真实机器人系统在17个不同场景中成功处理7种类型的刚体和关节对象，能够完成最长14个步骤的长时域任务。

Conclusion: 将场景运动学模型集成到规划实体中，而非编码任务特定约束，提供了一种可扩展和通用的复杂机器人操作方法。

Abstract: We present a Sequential Mobile Manipulation Planning (SMMP) framework that
can solve long-horizon multi-step mobile manipulation tasks with coordinated
whole-body motion, even when interacting with articulated objects. By
abstracting environmental structures as kinematic models and integrating them
with the robot's kinematics, we construct an Augmented Configuration Apace
(A-Space) that unifies the previously separate task constraints for navigation
and manipulation, while accounting for the joint reachability of the robot
base, arm, and manipulated objects. This integration facilitates efficient
planning within a tri-level framework: a task planner generates symbolic action
sequences to model the evolution of A-Space, an optimization-based motion
planner computes continuous trajectories within A-Space to achieve desired
configurations for both the robot and scene elements, and an intermediate plan
refinement stage selects action goals that ensure long-horizon feasibility. Our
simulation studies first confirm that planning in A-Space achieves an 84.6\%
higher task success rate compared to baseline methods. Validation on real
robotic systems demonstrates fluid mobile manipulation involving (i) seven
types of rigid and articulated objects across 17 distinct contexts, and (ii)
long-horizon tasks of up to 14 sequential steps. Our results highlight the
significance of modeling scene kinematics into planning entities, rather than
encoding task-specific constraints, offering a scalable and generalizable
approach to complex robotic manipulation.

</details>


### [8] [Engineering Automotive Digital Twins on Standardized Architectures: A Case Study](https://arxiv.org/abs/2508.18662)
*Stefan Ramdhan,Winnie Trandinh,Istvan David,Vera Pantelic,Mark Lawford*

Main category: cs.RO

TL;DR: 这篇论文研究ISO 23247标准在汽车数字双胞架构中的适用性，通过适应性巡航控制案例分析其优势和局限性


<details>
  <summary>Details</summary>
Motivation: 汽车行业对数字双胞技术的需求增长，但缺乏专门的架构指南，ISO 23247是少数可用的开始点

Method: 通过为1/10缩比自动驾驶车轨道车开发适应性巡航控制数字双胞的案例研究

Result: 识别了ISO 23247参考架构在汽车领域的一些优势和局限性

Conclusion: 为研究人员、实践者和标准制定者提供了未来发展方向的启示

Abstract: Digital twin (DT) technology has become of interest in the automotive
industry. There is a growing need for smarter services that utilize the unique
capabilities of DTs, ranging from computer-aided remote control to cloud-based
fleet coordination. Developing such services starts with the software
architecture. However, the scarcity of DT architectural guidelines poses a
challenge for engineering automotive DTs. Currently, the only DT architectural
standard is the one defined in ISO 23247. Though not developed for automotive
systems, it is one of the few feasible starting points for automotive DTs. In
this work, we investigate the suitability of the ISO 23247 reference
architecture for developing automotive DTs. Through the case study of
developing an Adaptive Cruise Control DT for a 1/10\textsuperscript{th}-scale
autonomous vehicle, we identify some strengths and limitations of the reference
architecture and begin distilling future directions for researchers,
practitioners, and standard developers.

</details>


### [9] [Deep Sensorimotor Control by Imitating Predictive Models of Human Motion](https://arxiv.org/abs/2508.18691)
*Himanshu Gaurav Singh,Pieter Abbeel,Jitendra Malik,Antonio Loquercio*

Main category: cs.RO

TL;DR: 提出了一种通过模仿人类运动预测模型来训练机器人传感器运动策略的新方法，无需梯度运动重定向或对抗损失，可直接利用人类交互数据


<details>
  <summary>Details</summary>
Motivation: 随着机器人与人类之间的体现差距缩小，可以利用人类与环境交互的大规模数据集来进行机器人学习，但现有方法受限于运动重定向和对抗损失的限制

Method: 使用在人类数据上训练的预测模型零样本应用于机器人数据，训练传感器运动策略来跟踪模型预测，同时优化稀疏任务奖励

Result: 该方法在不同机器人和任务上表现优异，大幅超越现有基线方法，且能替代精心设计的密集奖励和课程学习

Conclusion: 通过跟踪人类运动预测模型可以有效地利用人类场景交互数据集，为机器人学习提供新的有效途径

Abstract: As the embodiment gap between a robot and a human narrows, new opportunities
arise to leverage datasets of humans interacting with their surroundings for
robot learning. We propose a novel technique for training sensorimotor policies
with reinforcement learning by imitating predictive models of human motions.
Our key insight is that the motion of keypoints on human-inspired robot
end-effectors closely mirrors the motion of corresponding human body keypoints.
This enables us to use a model trained to predict future motion on human data
\emph{zero-shot} on robot data. We train sensorimotor policies to track the
predictions of such a model, conditioned on a history of past robot states,
while optimizing a relatively sparse task reward. This approach entirely
bypasses gradient-based kinematic retargeting and adversarial losses, which
limit existing methods from fully leveraging the scale and diversity of modern
human-scene interaction datasets. Empirically, we find that our approach can
work across robots and tasks, outperforming existing baselines by a large
margin. In addition, we find that tracking a human motion model can substitute
for carefully designed dense rewards and curricula in manipulation tasks. Code,
data and qualitative results available at
https://jirl-upenn.github.io/track_reward/.

</details>


### [10] [AgriChrono: A Multi-modal Dataset Capturing Crop Growth and Lighting Variability with a Field Robot](https://arxiv.org/abs/2508.18694)
*Jaehwan Jeong,Tuan-Anh Vu,Mohammad Jony,Shahab Ahmad,Md. Mukhlesur Rahman,Sangpil Kim,M. Khalid Jawed*

Main category: cs.RO

TL;DR: AgriChrono是一个新颖的机器人数据收集平台和多模态数据集，专门设计用于捕捉真实农业环境的动态条件，解决了现有数据集在静态或受控环境中收集的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有精准农业数据集主要在静态或受控环境中收集，传感器多样性有限，时间跨度受限，无法反映真实农田的动态特性（如光照变化、作物生长变化和自然干扰），导致训练模型在真实场景中缺乏鲁棒性和泛化能力。

Method: 开发了一个集成了多种传感器的机器人数据收集平台，支持远程、时间同步采集RGB、深度、LiDAR和IMU数据，能够在不同光照和作物生长阶段进行高效、可重复的长期数据收集。

Result: 在AgriChrono数据集上对多种最先进的3D重建模型进行了基准测试，揭示了在真实农田环境中进行重建的困难，并证明了该数据集作为研究资产在提升动态条件下模型泛化能力方面的价值。

Conclusion: AgriChrono平台和数据集为精准农业研究提供了重要的数据资源，能够有效促进模型在真实动态农业环境中的性能提升，代码和数据集已公开提供。

Abstract: Existing datasets for precision agriculture have primarily been collected in
static or controlled environments such as indoor labs or greenhouses, often
with limited sensor diversity and restricted temporal span. These conditions
fail to reflect the dynamic nature of real farmland, including illumination
changes, crop growth variation, and natural disturbances. As a result, models
trained on such data often lack robustness and generalization when applied to
real-world field scenarios. In this paper, we present AgriChrono, a novel
robotic data collection platform and multi-modal dataset designed to capture
the dynamic conditions of real-world agricultural environments. Our platform
integrates multiple sensors and enables remote, time-synchronized acquisition
of RGB, Depth, LiDAR, and IMU data, supporting efficient and repeatable
long-term data collection across varying illumination and crop growth stages.
We benchmark a range of state-of-the-art 3D reconstruction models on the
AgriChrono dataset, highlighting the difficulty of reconstruction in real-world
field environments and demonstrating its value as a research asset for
advancing model generalization under dynamic conditions. The code and dataset
are publicly available at: https://github.com/StructuresComp/agri-chrono

</details>


### [11] [Enhancing Video-Based Robot Failure Detection Using Task Knowledge](https://arxiv.org/abs/2508.18705)
*Santosh Thoduka,Sebastian Houben,Juergen Gall,Paul G. Plöger*

Main category: cs.RO

TL;DR: 提出基于视频的机器人执行失败检测方法，利用动作信息和任务相关物体的时空知识，通过数据增强技术提升检测性能


<details>
  <summary>Details</summary>
Motivation: 现有失败检测方法在真实场景中性能有限，需要可靠检测执行失败来触发安全操作模式、恢复策略或任务重规划

Method: 使用机器人执行动作和视野中任务相关物体的时空知识，提出可变帧率的数据增强方法，对视频不同部分应用不同帧率

Result: 在ARMBench数据集上F1分数从77.9提升到80.0，使用测试时增强后进一步提升到81.4，无需额外计算开销

Conclusion: 时空信息对失败检测至关重要，建议未来研究进一步探索合适的启发式方法

Abstract: Robust robotic task execution hinges on the reliable detection of execution
failures in order to trigger safe operation modes, recovery strategies, or task
replanning. However, many failure detection methods struggle to provide
meaningful performance when applied to a variety of real-world scenarios. In
this paper, we propose a video-based failure detection approach that uses
spatio-temporal knowledge in the form of the actions the robot performs and
task-relevant objects within the field of view. Both pieces of information are
available in most robotic scenarios and can thus be readily obtained. We
demonstrate the effectiveness of our approach on three datasets that we amend,
in part, with additional annotations of the aforementioned task-relevant
knowledge. In light of the results, we also propose a data augmentation method
that improves performance by applying variable frame rates to different parts
of the video. We observe an improvement from 77.9 to 80.0 in F1 score on the
ARMBench dataset without additional computational expense and an additional
increase to 81.4 with test-time augmentation. The results emphasize the
importance of spatio-temporal information during failure detection and suggest
further investigation of suitable heuristics in future implementations. Code
and annotations are available.

</details>


### [12] [HyperTASR: Hypernetwork-Driven Task-Aware Scene Representations for Robust Manipulation](https://arxiv.org/abs/2508.18802)
*Li Sun,Jiefeng Wu,Feng Chen,Ruizhe Liu,Yanchao Yang*

Main category: cs.RO

TL;DR: HyperTASR是一个基于超网络的框架，通过任务目标和执行阶段动态调制场景表征，提升机器人操作中的策略学习效果。


<details>
  <summary>Details</summary>
Motivation: 当前机器人操作中的表征提取方法通常是任务无关的，无法模拟人类认知中的动态感知适应能力，需要开发能够根据任务上下文动态调整的表征学习方法。

Method: 使用超网络架构，根据任务规范和进展状态动态生成表征转换参数，使表征能够在任务执行过程中上下文演化，同时保持与现有策略学习框架的架构兼容性。

Result: 在仿真和真实环境中的综合评估显示，该方法在不同表征范式下都取得了显著的性能提升，通过消融研究和注意力可视化证实了其选择性关注任务相关信息的能力。

Conclusion: HyperTASR通过建立任务上下文和状态依赖处理路径的计算分离，提高了学习效率和表征质量，更好地模拟了人类在操作任务中的自适应感知能力。

Abstract: Effective policy learning for robotic manipulation requires scene
representations that selectively capture task-relevant environmental features.
Current approaches typically employ task-agnostic representation extraction,
failing to emulate the dynamic perceptual adaptation observed in human
cognition. We present HyperTASR, a hypernetwork-driven framework that modulates
scene representations based on both task objectives and the execution phase.
Our architecture dynamically generates representation transformation parameters
conditioned on task specifications and progression state, enabling
representations to evolve contextually throughout task execution. This approach
maintains architectural compatibility with existing policy learning frameworks
while fundamentally reconfiguring how visual features are processed. Unlike
methods that simply concatenate or fuse task embeddings with task-agnostic
representations, HyperTASR establishes computational separation between
task-contextual and state-dependent processing paths, enhancing learning
efficiency and representational quality. Comprehensive evaluations in both
simulation and real-world environments demonstrate substantial performance
improvements across different representation paradigms. Through ablation
studies and attention visualization, we confirm that our approach selectively
prioritizes task-relevant scene information, closely mirroring human adaptive
perception during manipulation tasks. The project website is at
\href{https://lisunphil.github.io/HyperTASR_projectpage/}{lisunphil.github.io/HyperTASR\_projectpage}.

</details>


### [13] [Learning Real-World Acrobatic Flight from Human Preferences](https://arxiv.org/abs/2508.18817)
*Colin Merk,Ismail Geles,Jiaxu Xing,Angel Romero,Giorgia Ramponi,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 本文提出了一种基于偏好的强化学习方法（REC），用于无人机特技飞行动作控制，通过改进奖励建模和学习稳定性，在模拟和真实环境中都取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 特技飞行具有复杂的动力学特性，传统方法难以设计合适的奖励函数。基于偏好的强化学习能够从人类主观偏好中学习，更适合捕捉动作的风格和质量要求。

Method: 基于Preference PPO提出Reward Ensemble under Confidence (REC)方法，改进奖励学习目标，提升偏好建模和学习稳定性。在模拟环境中训练策略，然后迁移到真实无人机上。

Result: REC方法达到88.4%的成型奖励性能，显著优于标准Preference PPO的55.2%。成功实现多种特技飞行动作，人工设计的奖励函数与人类偏好的一致性仅为60.7%。

Conclusion: 基于偏好的强化学习能有效捕捉复杂的人类中心目标，REC方法在物理和模拟领域都表现出色，证明了其在难以形式化任务中的优势。

Abstract: Preference-based reinforcement learning (PbRL) enables agents to learn
control policies without requiring manually designed reward functions, making
it well-suited for tasks where objectives are difficult to formalize or
inherently subjective. Acrobatic flight poses a particularly challenging
problem due to its complex dynamics, rapid movements, and the importance of
precise execution. In this work, we explore the use of PbRL for agile drone
control, focusing on the execution of dynamic maneuvers such as powerloops.
Building on Preference-based Proximal Policy Optimization (Preference PPO), we
propose Reward Ensemble under Confidence (REC), an extension to the reward
learning objective that improves preference modeling and learning stability.
Our method achieves 88.4% of the shaped reward performance, compared to 55.2%
with standard Preference PPO. We train policies in simulation and successfully
transfer them to real-world drones, demonstrating multiple acrobatic maneuvers
where human preferences emphasize stylistic qualities of motion. Furthermore,
we demonstrate the applicability of our probabilistic reward model in a
representative MuJoCo environment for continuous control. Finally, we highlight
the limitations of manually designed rewards, observing only 60.7% agreement
with human preferences. These results underscore the effectiveness of PbRL in
capturing complex, human-centered objectives across both physical and simulated
domains.

</details>


### [14] [AS2FM: Enabling Statistical Model Checking of ROS 2 Systems for Robust Autonomy](https://arxiv.org/abs/2508.18820)
*Christian Henkel,Marco Lampacrescia,Michaela Klauck,Matteo Morelli*

Main category: cs.RO

TL;DR: 提出AS2FM工具，将自主机器人系统模型转换为JANI格式，使用统计模型检验(SMC)在设计时验证系统属性，相比现有方法支持更全面的系统特性且验证时间线性增长。


<details>
  <summary>Details</summary>
Motivation: 自主机器人在未知环境中运行时面临挑战，需要设计时验证系统属性以确保安全性和可靠性。

Method: 扩展SCXML格式建模ROS 2和行为树组件，开发AS2FM工具将系统模型转换为标准JANI格式，利用现成SMC工具进行验证。

Result: 成功识别ROS 2机械臂用例中的问题，验证时间少于1秒，验证运行时间随模型大小线性增长而非指数增长。

Conclusion: AS2FM工具实用性强，能够有效验证真实自主机器人控制系统，验证效率高且可扩展性好。

Abstract: Designing robotic systems to act autonomously in unforeseen environments is a
challenging task. This work presents a novel approach to use formal
verification, specifically Statistical Model Checking (SMC), to verify system
properties of autonomous robots at design-time. We introduce an extension of
the SCXML format, designed to model system components including both Robot
Operating System 2 (ROS 2) and Behavior Tree (BT) features. Further, we
contribute Autonomous Systems to Formal Models (AS2FM), a tool to translate the
full system model into JANI. The use of JANI, a standard format for
quantitative model checking, enables verification of system properties with
off-the-shelf SMC tools. We demonstrate the practical usability of AS2FM both
in terms of applicability to real-world autonomous robotic control systems, and
in terms of verification runtime scaling. We provide a case study, where we
successfully identify problems in a ROS 2-based robotic manipulation use case
that is verifiable in less than one second using consumer hardware.
Additionally, we compare to the state of the art and demonstrate that our
method is more comprehensive in system feature support, and that the
verification runtime scales linearly with the size of the model, instead of
exponentially.

</details>


### [15] [VisionSafeEnhanced VPC: Cautious Predictive Control with Visibility Constraints under Uncertainty for Autonomous Robotic Surgery](https://arxiv.org/abs/2508.18937)
*Wang Jiayin,Wei Yanran,Jiang Lei,Guo Xiaoyu,Zheng Ayong,Zhao Weidong,Li Zhongkui*

Main category: cs.RO

TL;DR: 这篇论文提出了VisionSafeEnhanced VPC框架，通过核算子回归进行不确定性量化，并结合控制障碍函数来实现自主腿镜控制的稳健性和视野安全保障。


<details>
  <summary>Details</summary>
Motivation: 解决目标可见性不连续、参数化错误、测量噪声等复杂干扰对手术视觉体验和安兠的影响。

Method: 利用高斯过程回归(GPR)进行混合不确定性量化，提出了一种具有概率保障的安全感知轨迹优化框架，包括不确定性适应性安全控制障碍函数条件和机会约束。

Result: 在商用手术机器人平台上进行的模拟和实验验证显示，该框架能维持近于完美的目标可见性(>99.9%)，并减少跟踪错误。

Conclusion: 该方法能够在不确定性条件下保障手术视野安全，实现自适应控制劳动分配，在保持稳健性的同时最小化摄像头运动。

Abstract: Autonomous control of the laparoscope in robot-assisted Minimally Invasive
Surgery (MIS) has received considerable research interest due to its potential
to improve surgical safety. Despite progress in pixel-level Image-Based Visual
Servoing (IBVS) control, the requirement of continuous visibility and the
existence of complex disturbances, such as parameterization error, measurement
noise, and uncertainties of payloads, could degrade the surgeon's visual
experience and compromise procedural safety. To address these limitations, this
paper proposes VisionSafeEnhanced Visual Predictive Control (VPC), a robust and
uncertainty-adaptive framework for autonomous laparoscope control that
guarantees Field of View (FoV) safety under uncertainty. Firstly, Gaussian
Process Regression (GPR) is utilized to perform hybrid (deterministic +
stochastic) quantification of operational uncertainties including residual
model uncertainties, stochastic uncertainties, and external disturbances. Based
on uncertainty quantification, a novel safety aware trajectory optimization
framework with probabilistic guarantees is proposed, where a
uncertainty-adaptive safety Control Barrier Function (CBF) condition is given
based on uncertainty propagation, and chance constraints are simultaneously
formulated based on probabilistic approximation. This uncertainty aware
formulation enables adaptive control effort allocation, minimizing unnecessary
camera motion while maintaining robustness. The proposed method is validated
through comparative simulations and experiments on a commercial surgical robot
platform (MicroPort MedBot Toumai) performing a sequential multi-target lymph
node dissection. Compared with baseline methods, the framework maintains
near-perfect target visibility (>99.9%), reduces tracking e

</details>


### [16] [Enhanced UAV Path Planning Using the Tangent Intersection Guidance (TIG) Algorithm](https://arxiv.org/abs/2508.18967)
*Hichem Cheriet,Khellat Kihel Badra,Chouraqui Samira*

Main category: cs.RO

TL;DR: 本文提出了一种基于椭圆切线交点的无人机路径规划算法TIG，能够在静态和动态环境中快速生成平滑的最短路径，相比现有算法具有更快的计算速度和更好的性能表现。


<details>
  <summary>Details</summary>
Motivation: 无人机在作战支持、包裹投递和搜救行动等应用中需要高效安全的导航，现有路径规划算法在实时性和路径质量方面存在不足，需要开发更高效的算法。

Method: 采用椭圆切线交点方法生成可行路径，为每个威胁生成两条子路径，基于启发式规则选择最优路线，并通过迭代优化直至到达目标。结合无人机运动学约束，使用基于二次贝塞尔曲线的改进平滑技术生成平滑路径。

Result: 实验结果表明，TIG算法在静态环境中相比A*、PRM、RRT*等算法能以更短时间（从0.01秒开始）生成最短路径，且转弯角度更少。在未知和部分已知环境中，TIG表现出高效的实时避障能力，优于APF和Dynamic APPATT算法。

Conclusion: TIG算法是一种高效的无人机路径规划方法，在静态和动态环境中都能快速生成平滑的最优路径，具有很好的实时应用前景。

Abstract: Efficient and safe navigation of Unmanned Aerial Vehicles (UAVs) is critical
for various applications, including combat support, package delivery and Search
and Rescue Operations. This paper introduces the Tangent Intersection Guidance
(TIG) algorithm, an advanced approach for UAV path planning in both static and
dynamic environments. The algorithm uses the elliptic tangent intersection
method to generate feasible paths. It generates two sub-paths for each threat,
selects the optimal route based on a heuristic rule, and iteratively refines
the path until the target is reached. Considering the UAV kinematic and dynamic
constraints, a modified smoothing technique based on quadratic B\'ezier curves
is adopted to generate a smooth and efficient route. Experimental results show
that the TIG algorithm can generate the shortest path in less time, starting
from 0.01 seconds, with fewer turning angles compared to A*, PRM, RRT*, Tangent
Graph, and Static APPATT algorithms in static environments. Furthermore, in
completely unknown and partially known environments, TIG demonstrates efficient
real-time path planning capabilities for collision avoidance, outperforming APF
and Dynamic APPATT algorithms.

</details>


### [17] [HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots](https://arxiv.org/abs/2508.19002)
*Shipeng Lyu,Fangyuan Wang,Weiwei Lin,Luhao Zhu,David Navarro-Alarcon,Guodong Guo*

Main category: cs.RO

TL;DR: HuBE是一个双层闭环框架，通过整合机器人状态、目标姿态和情境上下文来生成类人行为，解决了人形机器人运动生成中行为相似性和适当性的挑战，并实现了跨异构机器人的毫米级兼容性。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人运动生成中同时实现行为相似性和适当性的开放挑战，以及缺乏跨具身适应性的问题。

Method: 提出HuBE双层闭环框架，整合机器人状态、目标姿态和情境上下文；构建HPose情境丰富数据集；引入基于骨骼缩放的数据增强策略确保跨异构机器人的毫米级兼容性。

Result: 在多个商业平台上的综合评估表明，HuBE在运动相似性、行为适当性和计算效率方面显著优于现有最先进基线方法。

Conclusion: HuBE为跨多样化人形机器人的可迁移和类人行为执行奠定了坚实基础。

Abstract: Achieving both behavioral similarity and appropriateness in human-like motion
generation for humanoid robot remains an open challenge, further compounded by
the lack of cross-embodiment adaptability. To address this problem, we propose
HuBE, a bi-level closed-loop framework that integrates robot state, goal poses,
and contextual situations to generate human-like behaviors, ensuring both
behavioral similarity and appropriateness, and eliminating structural
mismatches between motion generation and execution. To support this framework,
we construct HPose, a context-enriched dataset featuring fine-grained
situational annotations. Furthermore, we introduce a bone scaling-based data
augmentation strategy that ensures millimeter-level compatibility across
heterogeneous humanoid robots. Comprehensive evaluations on multiple commercial
platforms demonstrate that HuBE significantly improves motion similarity,
behavioral appropriateness, and computational efficiency over state-of-the-art
baselines, establishing a solid foundation for transferable and human-like
behavior execution across diverse humanoid robots.

</details>


### [18] [An LLM-powered Natural-to-Robotic Language Translation Framework with Correctness Guarantees](https://arxiv.org/abs/2508.19074)
*ZhenDong Chen,ZhanShang Nie,ShiXing Wan,JunYi Li,YongTian Cheng,Shuai Zhao*

Main category: cs.RO

TL;DR: 这篇论文提出了一种新的自然-机器人语言翻译框架NRTrans，通过Robot Skill Language (RSL)和编译器验证机制，解决了LLM生成机器人控制程序时的错误问题，提高了程序生成的正确性和轻量级LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用LLM直接从自然语言生成可执行程序，但由于LLM的不稳定性和任务的复杂性，导致生成的代码出现大量错误，尤其是在使用轻量级LLM时效果更差。

Method: 提出Robot Skill Language (RSL)来抽象控制程序的复杂细节，构建RSL编译器和调试器验证LLM生成的RSL程序，并通过错误反馈细调LLM输出，直到程序通过编译器验证。

Result: 实验结果显示NRTrans在多种LLM和任务下都超过现有方法，并为轻量级LLM实现了高成功率。

Conclusion: 该框架为LLM生成的机器人控制程序提供了正确性保证，显著提高了LLM驱动机器人应用的效果。

Abstract: The Large Language Models (LLM) are increasingly being deployed in robotics
to generate robot control programs for specific user tasks, enabling embodied
intelligence. Existing methods primarily focus on LLM training and prompt
design that utilize LLMs to generate executable programs directly from user
tasks in natural language. However, due to the inconsistency of the LLMs and
the high complexity of the tasks, such best-effort approaches often lead to
tremendous programming errors in the generated code, which significantly
undermines the effectiveness especially when the light-weight LLMs are applied.
This paper introduces a natural-robotic language translation framework that (i)
provides correctness verification for generated control programs and (ii)
enhances the performance of LLMs in program generation via feedback-based
fine-tuning for the programs. To achieve this, a Robot Skill Language (RSL) is
proposed to abstract away from the intricate details of the control programs,
bridging the natural language tasks with the underlying robot skills. Then, the
RSL compiler and debugger are constructed to verify RSL programs generated by
the LLM and provide error feedback to the LLM for refining the outputs until
being verified by the compiler. This provides correctness guarantees for the
LLM-generated programs before being offloaded to the robots for execution,
significantly enhancing the effectiveness of LLM-powered robotic applications.
Experiments demonstrate NRTrans outperforms the existing method under a range
of LLMs and tasks, and achieves a high success rate for light-weight LLMs.

</details>


### [19] [DELIVER: A System for LLM-Guided Coordinated Multi-Robot Pickup and Delivery using Voronoi-Based Relay Planning](https://arxiv.org/abs/2508.19114)
*Alkesh K. Srivastava,Jared Michael Levin,Alexander Derrico,Philip Dames*

Main category: cs.RO

TL;DR: DELIVER是一个基于自然语言指令的多机器人协作拾取配送框架，通过语言理解、空间分解、中继规划和运动执行实现可扩展的无碰撞协调。


<details>
  <summary>Details</summary>
Motivation: 为了解决多机器人系统中自然语言指令驱动的协作拾取配送问题，需要开发一个集成框架来实现可扩展、无碰撞的实时协调。

Method: 使用LLaMA3进行自然语言理解提取位置信息，Voronoi空间分解定义机器人操作区域，计算最优中继点协调交接，有限状态机控制机器人行为。

Result: 在MultiTRAIL平台和真实TurtleBot3机器人上验证，相比单机器人系统降低55%的单体工作量，团队规模增大时中继代理数量保持低位，展现良好可扩展性。

Conclusion: DELIVER的模块化可扩展架构推动了语言引导的多机器人协调技术发展，为网络物理系统集成提供了先进解决方案。

Abstract: We present DELIVER (Directed Execution of Language-instructed Item Via
Engineered Relay), a fully integrated framework for cooperative multi-robot
pickup and delivery driven by natural language commands. DELIVER unifies
natural language understanding, spatial decomposition, relay planning, and
motion execution to enable scalable, collision-free coordination in real-world
settings. Given a spoken or written instruction, a lightweight instance of
LLaMA3 interprets the command to extract pickup and delivery locations. The
environment is partitioned using a Voronoi tessellation to define
robot-specific operating regions. Robots then compute optimal relay points
along shared boundaries and coordinate handoffs. A finite-state machine governs
each robot's behavior, enabling robust execution. We implement DELIVER on the
MultiTRAIL simulation platform and validate it in both ROS2-based Gazebo
simulations and real-world hardware using TurtleBot3 robots. Empirical results
show that DELIVER maintains consistent mission cost across varying team sizes
while reducing per-agent workload by up to 55% compared to a single-agent
system. Moreover, the number of active relay agents remains low even as team
size increases, demonstrating the system's scalability and efficient agent
utilization. These findings underscore DELIVER's modular and extensible
architecture for language-guided multi-robot coordination, advancing the
frontiers of cyber-physical system integration.

</details>


### [20] [ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown Environments](https://arxiv.org/abs/2508.19131)
*Shreya Gummadi,Mateus V. Gasparino,Gianluca Capezzuto,Marcelo Becker,Girish Chowdhary*

Main category: cs.RO

TL;DR: ZeST利用大型语言模型的视觉推理能力，实现零样本地形可通行性预测，无需让机器人进入危险环境即可生成实时可通行性地图。


<details>
  <summary>Details</summary>
Motivation: 传统的地形可通行性预测方法需要将机器人置于危险环境中收集数据，存在设备损坏和安全风险。需要一种更安全、成本效益更高的解决方案。

Method: 利用大型语言模型（LLMs）的视觉推理能力，通过零样本学习方式实时生成地形可通行性地图，避免实际环境中的风险数据收集。

Result: 在受控室内和非结构化室外环境中进行导航测试，相比其他最先进方法提供更安全的导航，能够持续到达最终目标。

Conclusion: ZeST方法为机器人导航系统开发提供了安全、成本效益高且可扩展的解决方案，加速了先进导航系统的发展。

Abstract: The advancement of robotics and autonomous navigation systems hinges on the
ability to accurately predict terrain traversability. Traditional methods for
generating datasets to train these prediction models often involve putting
robots into potentially hazardous environments, posing risks to equipment and
safety. To solve this problem, we present ZeST, a novel approach leveraging
visual reasoning capabilities of Large Language Models (LLMs) to create a
traversability map in real-time without exposing robots to danger. Our approach
not only performs zero-shot traversability and mitigates the risks associated
with real-world data collection but also accelerates the development of
advanced navigation systems, offering a cost-effective and scalable solution.
To support our findings, we present navigation results, in both controlled
indoor and unstructured outdoor environments. As shown in the experiments, our
method provides safer navigation when compared to other state-of-the-art
methods, constantly reaching the final goal.

</details>


### [21] [Uncertainty-Resilient Active Intention Recognition for Robotic Assistants](https://arxiv.org/abs/2508.19150)
*Juan Carlos Saborío,Marc Vinci,Oscar Lima,Sebastian Stock,Lennart Niecksch,Martin Günther,Alexander Sung,Joachim Hertzberg,Martin Atzmüller*

Main category: cs.RO

TL;DR: 基于POMDP的意图识别框架，通过结合多种规划器和实时传感器数据，解决人机协作中的不确定性和感知错误问题，实现了更高自主性的机器人助手行为。


<details>
  <summary>Details</summary>
Motivation: 现有机器人助手系统多仅限于识别显式提示，或作出过于简化的假设（如近完美信息），无法有效处理人类意图识别中常见的不确定性和感知错误。

Method: 提出一种基于意图识别POMDP的框架，整合实时传感器数据和多种规划器，以应对不确定性和传感器噪声。该方法重点解决不确定条件下的协作规划和行动问题。

Result: 该集成框架已在物理机器人上成功进行了测试，并取得了有前景的结果，证明了其在实际应用中的可行性和效果。

Conclusion: 该研究填补了人机协作中处理不确定性和感知错误的关键空白，通过POMDP框架实现了更自主、更健壮的机器人助手行为，为现实世界中的机器人协作提供了有效解决方案。

Abstract: Purposeful behavior in robotic assistants requires the integration of
multiple components and technological advances. Often, the problem is reduced
to recognizing explicit prompts, which limits autonomy, or is oversimplified
through assumptions such as near-perfect information. We argue that a critical
gap remains unaddressed -- specifically, the challenge of reasoning about the
uncertain outcomes and perception errors inherent to human intention
recognition. In response, we present a framework designed to be resilient to
uncertainty and sensor noise, integrating real-time sensor data with a
combination of planners. Centered around an intention-recognition POMDP, our
approach addresses cooperative planning and acting under uncertainty. Our
integrated framework has been successfully tested on a physical robot with
promising results.

</details>


### [22] [QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning](https://arxiv.org/abs/2508.19153)
*Allen Wang,Gavin Tao*

Main category: cs.RO

TL;DR: 提出QuadKAN框架，使用样条参数化的KAN网络结合本体感觉和视觉输入，实现四足机器人鲁棒运动控制，在多种地形上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决视觉引导四足运动控制问题，强调结合本体感觉和视觉对于鲁棒控制的必要性。

Method: 提出QuadKAN框架，使用样条编码器处理本体感觉，样条融合头处理多模态输入，采用MMDR和PPO进行端到端训练。

Result: 在多种地形测试中，QuadKAN获得更高回报、更远距离和更少碰撞，优于现有最佳基线方法。

Conclusion: 样条参数化策略为鲁棒视觉引导运动提供了简单、有效且可解释的替代方案。

Abstract: We address vision-guided quadruped motion control with reinforcement learning
(RL) and highlight the necessity of combining proprioception with vision for
robust control. We propose QuadKAN, a spline-parameterized cross-modal policy
instantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates
a spline encoder for proprioception and a spline fusion head for
proprioception-vision inputs. This structured function class aligns the
state-to-action mapping with the piecewise-smooth nature of gait, improving
sample efficiency, reducing action jitter and energy consumption, and providing
interpretable posture-action sensitivities. We adopt Multi-Modal Delay
Randomization (MMDR) and perform end-to-end training with Proximal Policy
Optimization (PPO). Evaluations across diverse terrains, including both even
and uneven surfaces and scenarios with static or dynamic obstacles, demonstrate
that QuadKAN achieves consistently higher returns, greater distances, and fewer
collisions than state-of-the-art (SOTA) baselines. These results show that
spline-parameterized policies offer a simple, effective, and interpretable
alternative for robust vision-guided locomotion. A repository will be made
available upon acceptance.

</details>


### [23] [Real-time Testing of Satellite Attitude Control With a Reaction Wheel Hardware-In-the-Loop Platform](https://arxiv.org/abs/2508.19164)
*Morokot Sakal,George Nehma,Camilo Riano-Rios,Madhur Tiwari*

Main category: cs.RO

TL;DR: 硬件在环测试通过实际电机和CAN总线验证卫星姿态控制系统的反应轮健康估计能力


<details>
  <summary>Details</summary>
Motivation: 之前的仿真和软件在环测试推动了需要在循环中包含实际动量交换设备来验证控制器有效性的进一步实验

Method: 构建HIL测试平台，包含无刷直流电机、CAN总线通信、嵌入式计算机执行控制算法，以及卫星仿真器生成传感器数据和响应外部执行器动作

Result: 提出了人工引发反应轮故障的方法，并展示了相关问题和经验教训

Conclusion: 这项工作是往完善太空载具姿态控制算法验证框架进行的一步

Abstract: We propose the Hardware-in-the-Loop (HIL) test of an adaptive satellite
attitude control system with reaction wheel health estimation capabilities.
Previous simulations and Software-in-the-Loop testing have prompted further
experiments to explore the validity of the controller with real momentum
exchange devices in the loop. This work is a step toward a comprehensive
testing framework for validation of spacecraft attitude control algorithms. The
proposed HIL testbed includes brushless DC motors and drivers that communicate
using a CAN bus, an embedded computer that executes control and adaptation
laws, and a satellite simulator that produces simulated sensor data, estimated
attitude states, and responds to actions of the external actuators. We propose
methods to artificially induce failures on the reaction wheels, and present
related issues and lessons learned.

</details>


### [24] [Direction Informed Trees (DIT*): Optimal Path Planning via Direction Filter and Direction Cost Heuristic](https://arxiv.org/abs/2508.19168)
*Liding Zhang,Kejia Chen,Kuanqi Cai,Yu Zhang,Yixuan Dang,Yansong Wu,Zhenshan Bing,Fan Wu,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: DIT*是一种基于采样的路径规划算法，通过优化搜索方向和建立方向滤波器，比现有算法在R^4到R^16空间中收敛更快


<details>
  <summary>Details</summary>
Motivation: 现有路径规划算法（如EIT*）使用effort启发式来指导搜索，但准确性和计算效率往往难以兼顾，需要更好的启发式方法

Method: 将边定义为广义向量，集成相似性指数建立方向滤波器来选择最近邻和估计方向成本，在边评估中使用估计的方向成本启发式

Result: DIT*在R^4到R^16的测试问题上比现有的单查询基于采样规划器收敛更快，并在各种规划任务的真实环境中得到验证

Conclusion: 通过优化搜索方向和共享方向信息，DIT*实现了更高效的路径规划，为复杂环境中的运动规划提供了有效解决方案

Abstract: Optimal path planning requires finding a series of feasible states from the
starting point to the goal to optimize objectives. Popular path planning
algorithms, such as Effort Informed Trees (EIT*), employ effort heuristics to
guide the search. Effective heuristics are accurate and computationally
efficient, but achieving both can be challenging due to their conflicting
nature. This paper proposes Direction Informed Trees (DIT*), a sampling-based
planner that focuses on optimizing the search direction for each edge,
resulting in goal bias during exploration. We define edges as generalized
vectors and integrate similarity indexes to establish a directional filter that
selects the nearest neighbors and estimates direction costs. The estimated
direction cost heuristics are utilized in edge evaluation. This strategy allows
the exploration to share directional information efficiently. DIT* convergence
faster than existing single-query, sampling-based planners on tested problems
in R^4 to R^16 and has been demonstrated in real-world environments with
various planning tasks. A video showcasing our experimental results is
available at: https://youtu.be/2SX6QT2NOek

</details>


### [25] [From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity](https://arxiv.org/abs/2508.19172)
*Luca Grillotti,Lisa Coiffard,Oscar Pang,Maxence Faldor,Antoine Cully*

Main category: cs.RO

TL;DR: URSA是一种无监督真实世界技能获取方法，扩展了QDAC算法，使机器人能够在真实环境中自主发现和掌握多样化高性能技能，无需人工定义技能空间或精心调整启发式方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法如QDAC需要手动定义技能空间和精心调整启发式方法，限制了在真实世界的应用。直接在物理硬件上学习行为面临安全和数据效率的挑战。

Method: URSA是QDAC的扩展，支持启发式驱动的技能发现和完全无监督设置，能够在真实世界中自主发现多样化运动技能。

Result: 在Unitree A1四足机器人上成功发现了多样化运动技能，在仿真和真实世界中都有效。在损伤适应任务中，在5/9仿真和3/5真实世界损伤场景中优于所有基线方法。

Conclusion: URSA为真实世界机器人学习建立了新框架，实现了有限人工干预下的持续技能发现，是迈向更自主和适应性机器人系统的重要一步。

Abstract: Autonomous skill discovery aims to enable robots to acquire diverse behaviors
without explicit supervision. Learning such behaviors directly on physical
hardware remains challenging due to safety and data efficiency constraints.
Existing methods, including Quality-Diversity Actor-Critic (QDAC), require
manually defined skill spaces and carefully tuned heuristics, limiting
real-world applicability. We propose Unsupervised Real-world Skill Acquisition
(URSA), an extension of QDAC that enables robots to autonomously discover and
master diverse, high-performing skills directly in the real world. We
demonstrate that URSA successfully discovers diverse locomotion skills on a
Unitree A1 quadruped in both simulation and the real world. Our approach
supports both heuristic-driven skill discovery and fully unsupervised settings.
We also show that the learned skill repertoire can be reused for downstream
tasks such as real-world damage adaptation, where URSA outperforms all
baselines in 5 out of 9 simulated and 3 out of 5 real-world damage scenarios.
Our results establish a new framework for real-world robot learning that
enables continuous skill discovery with limited human intervention,
representing a significant step toward more autonomous and adaptable robotic
systems. Demonstration videos are available at
http://adaptive-intelligent-robotics.github.io/URSA .

</details>


### [26] [Real-Time Model Checking for Closed-Loop Robot Reactive Planning](https://arxiv.org/abs/2508.19186)
*Christopher Chandler,Bernd Porr,Giulia Lafratta,Alice Miller*

Main category: cs.RO

TL;DR: 基于模型检查的实时多步规划技术，在低功耗设备上实现自主机器人的障碍避免和多步规划


<details>
  <summary>Details</summary>
Motivation: 仿照生物代理的"u6838心知识"和注意力机制，开发能够在实时环境中进行多步规划的方法，提升反应式机器人的性能

Method: 使用专门设计的小型模型检查算法，通过链式临时控制系统来对抗环境干扰，采用对局部环境有界变化敏感的2D LiDAR数据离散化方法，进行前向深度优先搜索

Result: 在cul-de-sac和游戏场场景中实现了多步规划，证明模型检查能够为局部障碍避免创建高效多步计划，性能超过仅能规划一步的反应式代理

Conclusion: 该方法为自主车辆领域开发安全、可靠和可解释的规划系统提供了教学案例研究，证明了模型检查在实时多步规划中的应用价值

Abstract: We present a new application of model checking which achieves real-time
multi-step planning and obstacle avoidance on a real autonomous robot. We have
developed a small, purpose-built model checking algorithm which generates plans
in situ based on "core" knowledge and attention as found in biological agents.
This is achieved in real-time using no pre-computed data on a low-powered
device. Our approach is based on chaining temporary control systems which are
spawned to counteract disturbances in the local environment that disrupt an
autonomous agent from its preferred action (or resting state). A novel
discretization of 2D LiDAR data sensitive to bounded variations in the local
environment is used. Multi-step planning using model checking by forward
depth-first search is applied to cul-de-sac and playground scenarios. Both
empirical results and informal proofs of two fundamental properties of our
approach demonstrate that model checking can be used to create efficient
multi-step plans for local obstacle avoidance, improving on the performance of
a reactive agent which can only plan one step. Our approach is an instructional
case study for the development of safe, reliable and explainable planning in
the context of autonomous vehicles.

</details>


### [27] [AutoRing: Imitation Learning--based Autonomous Intraocular Foreign Body Removal Manipulation with Eye Surgical Robot](https://arxiv.org/abs/2508.19191)
*Yue Wang,Wenjie Deng,Haotian Xue,Di Cui,Yiqi Chen,Mingchuan Zhou,Haochao Ying,Jian Wu*

Main category: cs.RO

TL;DR: AutoRing是一个基于模仿学习的自主眼内异物环操作框架，通过动态RCM校准和RCM-ACT架构解决运动缩放和RCM点变化带来的运动学不确定性，仅使用立体视觉数据和专家演示即可完成精确操作。


<details>
  <summary>Details</summary>
Motivation: 眼内异物移除需要在有限空间内实现毫米级精度，现有机器人系统主要依赖手动遥操作，学习曲线陡峭，需要解决自主操作中的运动学不确定性挑战。

Method: 提出AutoRing模仿学习框架，整合动态RCM校准解决坐标系不一致问题，引入结合动作分块变换器和实时运动学重新对齐的RCM-ACT架构，仅使用立体视觉数据和专家演示的仪器运动学数据进行训练。

Result: 在未校准显微镜条件下实现端到端自主操作，成功完成环抓取和定位任务，无需显式深度感知。

Conclusion: 为开发能够执行复杂眼内手术的智能眼科手术系统提供了可行框架。

Abstract: Intraocular foreign body removal demands millimeter-level precision in
confined intraocular spaces, yet existing robotic systems predominantly rely on
manual teleoperation with steep learning curves. To address the challenges of
autonomous manipulation (particularly kinematic uncertainties from variable
motion scaling and variation of the Remote Center of Motion (RCM) point), we
propose AutoRing, an imitation learning framework for autonomous intraocular
foreign body ring manipulation. Our approach integrates dynamic RCM calibration
to resolve coordinate-system inconsistencies caused by intraocular instrument
variation and introduces the RCM-ACT architecture, which combines
action-chunking transformers with real-time kinematic realignment. Trained
solely on stereo visual data and instrument kinematics from expert
demonstrations in a biomimetic eye model, AutoRing successfully completes ring
grasping and positioning tasks without explicit depth sensing. Experimental
validation demonstrates end-to-end autonomy under uncalibrated microscopy
conditions. The results provide a viable framework for developing intelligent
eye-surgical systems capable of complex intraocular procedures.

</details>


### [28] [Planning-Query-Guided Model Generation for Model-Based Deformable Object Manipulation](https://arxiv.org/abs/2508.19199)
*Alex LaGrassa,Zixuan Huang,Dmitry Berenson,Oliver Kroemer*

Main category: cs.RO

TL;DR: 这篇论文提出了一种基于渗透模型的方法，能够根据任务需求自动生成空间适配的动力学模型，通过学习识别对任务性能关键的区域来提高规划效率。


<details>
  <summary>Details</summary>
Motivation: 在高维度空间（如可变形物体）中进行高效规划需要计算可处理但充分表达的动力学模型。传统方法通常使用固定分辨率的模型，导致计算费用高或性能不足。

Method: 提出基于渗透模型的模型生成器，根据起始和目标点云预测各区域的模型分辨率。采用两阶段数据收集过程：先使用预测动力学作为先验优化分辨率，然后直接优化闭环性能。

Result: 在树木操纵任务中，该方法将规划速度提高了一倍，而任务性能仅有轻微下降，较使用全分辨率模型更高效。

Conclusion: 这种方法为利用历史规划和控制数据生成计算高效且充分表达的动力学模型提供了新的途径，适用于新任务的高效规划。

Abstract: Efficient planning in high-dimensional spaces, such as those involving
deformable objects, requires computationally tractable yet sufficiently
expressive dynamics models. This paper introduces a method that automatically
generates task-specific, spatially adaptive dynamics models by learning which
regions of the object require high-resolution modeling to achieve good task
performance for a given planning query. Task performance depends on the complex
interplay between the dynamics model, world dynamics, control, and task
requirements. Our proposed diffusion-based model generator predicts per-region
model resolutions based on start and goal pointclouds that define the planning
query. To efficiently collect the data for learning this mapping, a two-stage
process optimizes resolution using predictive dynamics as a prior before
directly optimizing using closed-loop performance. On a tree-manipulation task,
our method doubles planning speed with only a small decrease in task
performance over using a full-resolution model. This approach informs a path
towards using previous planning and control data to generate computationally
efficient yet sufficiently expressive dynamics models for new tasks.

</details>


### [29] [MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation](https://arxiv.org/abs/2508.19236)
*Hao Shi,Bin Xie,Yingfei Liu,Lin Sun,Fengrong Liu,Tiancai Wang,Erjin Zhou,Haoqiang Fan,Xiangyu Zhang,Gao Huang*

Main category: cs.RO

TL;DR: MemoryVLA是一个受人类记忆机制启发的机器人操作框架，通过工作记忆和长期记忆系统处理时序上下文，在长时程任务中显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 主流VLA模型忽略时序上下文，难以处理长时程、时间依赖的任务。受认知科学启发，人类依赖工作记忆和长期记忆系统来处理时序信息

Method: 提出Cognition-Memory-Action框架：预训练VLM编码观测为感知和认知token形成工作记忆，感知-认知记忆库存储低层细节和高层语义，工作记忆从库中检索相关条目并融合，记忆条件扩散动作专家生成时序感知动作序列

Result: 在150+仿真和真实任务中测试：SimplerEnv-Bridge 71.9%、Fractal 72.7%、LIBERO-5 96.5%成功率，均优于CogACT和pi-0，Bridge任务提升14.6%；真实世界12个任务84.0%成功率，长时程任务提升26%

Conclusion: MemoryVLA通过模拟人类记忆机制有效处理机器人操作中的时序上下文依赖，在长时程任务中表现出显著优势，为时序感知的机器人操作提供了新思路

Abstract: Temporal context is essential for robotic manipulation because such tasks are
inherently non-Markovian, yet mainstream VLA models typically overlook it and
struggle with long-horizon, temporally dependent tasks. Cognitive science
suggests that humans rely on working memory to buffer short-lived
representations for immediate control, while the hippocampal system preserves
verbatim episodic details and semantic gist of past experience for long-term
memory. Inspired by these mechanisms, we propose MemoryVLA, a
Cognition-Memory-Action framework for long-horizon robotic manipulation. A
pretrained VLM encodes the observation into perceptual and cognitive tokens
that form working memory, while a Perceptual-Cognitive Memory Bank stores
low-level details and high-level semantics consolidated from it. Working memory
retrieves decision-relevant entries from the bank, adaptively fuses them with
current tokens, and updates the bank by merging redundancies. Using these
tokens, a memory-conditioned diffusion action expert yields temporally aware
action sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasks
across three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, it
achieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperforming
state-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain on
Bridge. On 12 real-world tasks spanning general skills and long-horizon
temporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizon
tasks showing a +26 improvement over state-of-the-art baseline. Project Page:
https://shihao1895.github.io/MemoryVLA

</details>
