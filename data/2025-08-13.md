<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 25]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Humanoid Robot Acrobatics Utilizing Complete Articulated Rigid Body Dynamics](https://arxiv.org/abs/2508.08258)
*Gerald Brantner*

Main category: cs.RO

TL;DR: 该论文提出了一种结合轨迹优化和全身控制的架构，通过模型抽象实现人形机器人高动态动作的执行。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人执行高动态动作（如杂技）的挑战，克服高自由度带来的复杂性。

Method: 采用轨迹优化和全身控制，结合模型抽象，基于完整刚体运动方程。

Result: 在仿真中验证了系统的有效性。

Conclusion: 该架构能够成功执行复杂动作，克服了传统方法的局限性。

Abstract: Endowing humanoid robots with the ability to perform highly dynamic motions
akin to human-level acrobatics has been a long-standing challenge. Successfully
performing these maneuvers requires close consideration of the underlying
physics in both trajectory optimization for planning and control during
execution. This is particularly challenging due to humanoids' high
degree-of-freedom count and associated exponentially scaling complexities,
which makes planning on the explicit equations of motion intractable. Typical
workarounds include linearization methods and model approximations. However,
neither are sufficient because they produce degraded performance on the true
robotic system. This paper presents a control architecture comprising
trajectory optimization and whole-body control, intermediated by a matching
model abstraction, that enables the execution of acrobatic maneuvers, including
constraint and posture behaviors, conditioned on the unabbreviated equations of
motion of the articulated rigid body model. A review of underlying modeling and
control methods is given, followed by implementation details including model
abstraction, trajectory optimization and whole-body controller. The system's
effectiveness is analyzed in simulation.

</details>


### [2] [Koopman Operator Based Linear Model Predictive Control for Quadruped Trotting](https://arxiv.org/abs/2508.08259)
*Chun-Ming Yang,Pranav A. Bhounsule*

Main category: cs.RO

TL;DR: 使用Koopman算子为四足机器人构建高维线性模型，结合LMPC实现高精度跟踪和干扰抑制。


<details>
  <summary>Details</summary>
Motivation: 在线优化控制使四足机器人能实时适应变化输入和条件，但传统LMPC的线性化模型可能导致不准确。

Method: 利用Koopman算子在高维空间中构建保留非线性特性的线性模型，再结合LMPC。

Result: 实现了四足机器人的高精度跟踪和干扰抑制。

Conclusion: 这是首次将Koopman算子理论应用于四足机器人LMPC的研究。

Abstract: Online optimal control of quadruped robots would enable them to adapt to
varying inputs and changing conditions in real time. A common way of achieving
this is linear model predictive control (LMPC), where a quadratic programming
(QP) problem is formulated over a finite horizon with a quadratic cost and
linear constraints obtained by linearizing the equations of motion and solved
on the fly. However, the model linearization may lead to model inaccuracies. In
this paper, we use the Koopman operator to create a linear model of the
quadrupedal system in high dimensional space which preserves the nonlinearity
of the equations of motion. Then using LMPC, we demonstrate high fidelity
tracking and disturbance rejection on a quadrupedal robot. This is the first
work that uses the Koopman operator theory for LMPC of quadrupedal locomotion.

</details>


### [3] [Forecast-Driven MPC for Decentralized Multi-Robot Collision Avoidance](https://arxiv.org/abs/2508.08264)
*Hadush Hailu,Bruk Gebregziabher,Prudhvi Raj*

Main category: cs.RO

TL;DR: eIFP-MPC是IFP的优化版本，通过改进威胁优先级、路径生成和动态可行性，提升了多机器人路径规划的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决IFP在对称配置中因镜像交互导致的碰撞和死锁问题。

Method: 结合时间碰撞启发式优化威胁优先级，通过成本选择的路径点稳定路径生成，并集成模型预测控制（MPC）确保动态可行性。

Result: 在对称和高密度场景中显著减少振荡，确保无碰撞运动并提高轨迹效率。

Conclusion: 几何规划器通过优化可在复杂多智能体环境中实现稳健的大规模性能。

Abstract: The Iterative Forecast Planner (IFP) is a geometric planning approach that
offers lightweight computations, scalable, and reactive solutions for
multi-robot path planning in decentralized, communication-free settings.
However, it struggles in symmetric configurations, where mirrored interactions
often lead to collisions and deadlocks. We introduce eIFP-MPC, an optimized and
extended version of IFP that improves robustness and path consistency in dense,
dynamic environments. The method refines threat prioritization using a
time-to-collision heuristic, stabilizes path generation through cost-based
via-point selection, and ensures dynamic feasibility by incorporating model
predictive control (MPC) into the planning process. These enhancements are
tightly integrated into the IFP to preserve its efficiency while improving its
adaptability and stability. Extensive simulations across symmetric and
high-density scenarios show that eIFP-MPC significantly reduces oscillations,
ensures collision-free motion, and improves trajectory efficiency. The results
demonstrate that geometric planners can be strengthened through optimization,
enabling robust performance at scale in complex multi-agent environments.

</details>


### [4] [emg2tendon: From sEMG Signals to Tendon Control in Musculoskeletal Hands](https://arxiv.org/abs/2508.08269)
*Sagar Verma*

Main category: cs.RO

TL;DR: 论文提出首个大规模EMG-to-Tendon Control数据集，用于肌腱驱动机械手控制，并提出了基于扩散的回归模型。


<details>
  <summary>Details</summary>
Motivation: 肌腱驱动机械手控制复杂且缺乏直接映射，现有视觉跟踪方法易受遮挡和不准确影响，sEMG信号虽廉价但映射困难。

Method: 扩展emg2pose数据集，引入肌腱控制信号，提供三种基线回归模型，并提出基于扩散的回归模型。

Result: 创建了包含193名受试者、370小时数据的大规模数据集，解决了无效姿势问题，模型表现良好。

Conclusion: 该数据集和模型为肌腱驱动机械手的精确控制奠定了基础，推动了可扩展控制的发展。

Abstract: Tendon-driven robotic hands offer unparalleled dexterity for manipulation
tasks, but learning control policies for such systems presents unique
challenges. Unlike joint-actuated robotic hands, tendon-driven systems lack a
direct one-to-one mapping between motion capture (mocap) data and tendon
controls, making the learning process complex and expensive. Additionally,
visual tracking methods for real-world applications are prone to occlusions and
inaccuracies, further complicating joint tracking. Wrist-wearable surface
electromyography (sEMG) sensors present an inexpensive, robust alternative to
capture hand motion. However, mapping sEMG signals to tendon control remains a
significant challenge despite the availability of EMG-to-pose data sets and
regression-based models in the existing literature.
  We introduce the first large-scale EMG-to-Tendon Control dataset for robotic
hands, extending the emg2pose dataset, which includes recordings from 193
subjects, spanning 370 hours and 29 stages with diverse gestures. This dataset
incorporates tendon control signals derived using the MyoSuite MyoHand model,
addressing limitations such as invalid poses in prior methods. We provide three
baseline regression models to demonstrate emg2tendon utility and propose a
novel diffusion-based regression model for predicting tendon control from sEMG
recordings. This dataset and modeling framework marks a significant step
forward for tendon-driven dexterous robotic manipulation, laying the groundwork
for scalable and accurate tendon control in robotic hands.
https://emg2tendon.github.io/

</details>


### [5] [Evaluation of an Autonomous Surface Robot Equipped with a Transformable Mobility Mechanism for Efficient Mobility Control](https://arxiv.org/abs/2508.08303)
*Yasuyuki Fujii,Dinh Tuan Tran,Joo-Ho Lee*

Main category: cs.RO

TL;DR: 研究开发了一种可变形的移动机制，用于水面机器人，通过两种控制模式（定点保持和移动模式）提升能源效率和机动性。实验表明，移动模式比定点模式节能10%，并减少5%的移动时间。


<details>
  <summary>Details</summary>
Motivation: 提高水面机器人在长期环境监测中的移动效率和能源效率。

Method: 开发并评估了一种可变形的移动机制，包含两种控制模式：定点保持和移动模式。

Result: 移动模式比定点模式节能10%，并减少5%的移动时间。

Conclusion: 可变形的移动机制显著提升了水面机器人的操作效率。

Abstract: Efficient mobility and power consumption are critical for autonomous water
surface robots in long-term water environmental monitoring. This study develops
and evaluates a transformable mobility mechanism for a water surface robot with
two control modes: station-keeping and traveling to improve energy efficiency
and maneuverability. Field experiments show that, in a round-trip task between
two points, the traveling mode reduces power consumption by 10\% and decreases
the total time required for travel by 5\% compared to the station-keeping mode.
These results confirm the effectiveness of the transformable mobility mechanism
for enhancing operational efficiency in patrolling on water surface.

</details>


### [6] [Whole-Body Coordination for Dynamic Object Grasping with Legged Manipulators](https://arxiv.org/abs/2508.08328)
*Qiwei Liang,Boyang Cai,Rongyi He,Hui Li,Tao Teng,Haihan Duan,Changxin Huang,Runhao Zeng*

Main category: cs.RO

TL;DR: DQ-Bench是一个新的基准测试，用于评估动态抓取任务，DQ-Net是一个紧凑的师生框架，用于从有限感知线索中推断抓取配置，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注静态物体抓取，忽视了动态目标的挑战，限制了在动态场景（如物流分拣和人机协作）中的应用。

Method: 提出DQ-Bench基准测试，评估动态抓取任务；设计DQ-Net师生框架，教师网络利用特权信息建模目标静态和动态特性，学生网络仅使用目标掩码、深度图和本体状态进行双视角时间建模。

Result: DQ-Net在多个任务设置中实现了鲁棒的动态物体抓取，成功率和响应速度显著优于基线方法。

Conclusion: DQ-Bench和DQ-Net为解决动态抓取任务提供了有效工具，扩展了四足机器人操纵器的应用场景。

Abstract: Quadrupedal robots with manipulators offer strong mobility and adaptability
for grasping in unstructured, dynamic environments through coordinated
whole-body control. However, existing research has predominantly focused on
static-object grasping, neglecting the challenges posed by dynamic targets and
thus limiting applicability in dynamic scenarios such as logistics sorting and
human-robot collaboration. To address this, we introduce DQ-Bench, a new
benchmark that systematically evaluates dynamic grasping across varying object
motions, velocities, heights, object types, and terrain complexities, along
with comprehensive evaluation metrics. Building upon this benchmark, we propose
DQ-Net, a compact teacher-student framework designed to infer grasp
configurations from limited perceptual cues. During training, the teacher
network leverages privileged information to holistically model both the static
geometric properties and dynamic motion characteristics of the target, and
integrates a grasp fusion module to deliver robust guidance for motion
planning. Concurrently, we design a lightweight student network that performs
dual-viewpoint temporal modeling using only the target mask, depth map, and
proprioceptive state, enabling closed-loop action outputs without reliance on
privileged data. Extensive experiments on DQ-Bench demonstrate that DQ-Net
achieves robust dynamic objects grasping across multiple task settings,
substantially outperforming baseline methods in both success rate and
responsiveness.

</details>


### [7] [A Minimal Model for Emergent Collective Behaviors in Autonomous Robotic Multi-Agent Systems](https://arxiv.org/abs/2508.08473)
*Hossein B. Jond*

Main category: cs.RO

TL;DR: 提出了一种新的群体行为模型，通过相对位置、速度和局部密度调控智能体动态，实现灵活、无碰撞的群体行为，并扩展至认知自主系统。


<details>
  <summary>Details</summary>
Motivation: 现有模型（如Vicsek、Cucker-Smale）缺乏碰撞避免，Olfati-Saber模型则限制群体形态，限制了其在群体机器人中的适用性。

Method: 使用相对位置、速度和局部密度调控智能体动态，引入空间偏移和动力学偏移两个可调参数。

Result: 实现了空间灵活、无碰撞的群体行为，并扩展至认知自主系统，支持能量感知的群体与集群行为切换。

Conclusion: 该模型为多机器人系统（如自主空中群体）提供了稳健的基础。

Abstract: Collective behaviors such as swarming and flocking emerge from simple,
decentralized interactions in biological systems. Existing models, such as
Vicsek and Cucker-Smale, lack collision avoidance, whereas the Olfati-Saber
model imposes rigid formations, limiting their applicability in swarm robotics.
To address these limitations, this paper proposes a minimal yet expressive
model that governs agent dynamics using relative positions, velocities, and
local density, modulated by two tunable parameters: the spatial offset and
kinetic offset. The model achieves spatially flexible, collision-free behaviors
that reflect naturalistic group dynamics. Furthermore, we extend the framework
to cognitive autonomous systems, enabling energy-aware phase transitions
between swarming and flocking through adaptive control parameter tuning. This
cognitively inspired approach offers a robust foundation for real-world
applications in multi-robot systems, particularly autonomous aerial swarms.

</details>


### [8] [AZRA: Extending the Affective Capabilities of Zoomorphic Robots using Augmented Reality](https://arxiv.org/abs/2508.08507)
*Shaun Macdonald,Salma ElSayed,Mark McGill*

Main category: cs.RO

TL;DR: AZRA是一个增强现实框架，通过扩展情感交互能力提升仿生机器人与用户的互动，无需物理修改。


<details>
  <summary>Details</summary>
Motivation: 仿生机器人的情感交互通常简单且短暂，限制了其家庭应用潜力。

Method: 通过AR技术为仿生机器人Petit Qoobo添加情感显示和交互方式，并结合情感计算模型。

Result: AZRA展示了动态情感交互和快速原型设计的能力。

Conclusion: AZRA为未来仿生机器人开发提供了新的方向和潜力。

Abstract: Zoomorphic robots could serve as accessible and practical alternatives for
users unable or unwilling to keep pets. However, their affective interactions
are often simplistic and short-lived, limiting their potential for domestic
adoption. In order to facilitate more dynamic and nuanced affective
interactions and relationships between users and zoomorphic robots we present
AZRA, a novel augmented reality (AR) framework that extends the affective
capabilities of these robots without physical modifications. To demonstrate
AZRA, we augment a zoomorphic robot, Petit Qoobo, with novel emotional displays
(face, light, sound, thought bubbles) and interaction modalities (voice, touch,
proximity, gaze). Additionally, AZRA features a computational model of emotion
to calculate the robot's emotional responses, daily moods, evolving personality
and needs. We highlight how AZRA can be used for rapid participatory
prototyping and enhancing existing robots, then discuss implications on future
zoomorphic robot development.

</details>


### [9] [DeepFleet: Multi-Agent Foundation Models for Mobile Robots](https://arxiv.org/abs/2508.08574)
*Ameya Agaskar,Sriram Siva,William Pickering,Kyle O'Brien,Charles Kekeh,Ang Li,Brianna Gallo Sarker,Alicia Chua,Mayur Nemade,Charun Thattai,Jiaming Di,Isaac Iyengar,Ramya Dharoor,Dino Kirouani,Jimmy Erskine,Tamir Hegazy,Scott Niekum,Usman A. Khan,Federico Pecora,Joseph W. Durham*

Main category: cs.RO

TL;DR: DeepFleet是一套用于大规模移动机器人车队协调与规划的基础模型，包含四种架构，其中机器人中心（RC）和图-地板（GF）模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决大规模机器人车队协调与规划的挑战，利用亚马逊仓库的机器人数据训练模型。

Method: 设计了四种模型架构（RC、RF、IF、GF），分别采用不同的归纳偏置，评估其在预测任务中的表现。

Result: RC和GF模型表现最佳，能有效利用更大规模的数据集。

Conclusion: RC和GF模型因其异步状态更新和局部交互结构，在大规模机器人车队规划中具有潜力。

Abstract: We introduce DeepFleet, a suite of foundation models designed to support
coordination and planning for large-scale mobile robot fleets. These models are
trained on fleet movement data, including robot positions, goals, and
interactions, from hundreds of thousands of robots in Amazon warehouses
worldwide. DeepFleet consists of four architectures that each embody a distinct
inductive bias and collectively explore key points in the design space for
multi-agent foundation models: the robot-centric (RC) model is an
autoregressive decision transformer operating on neighborhoods of individual
robots; the robot-floor (RF) model uses a transformer with cross-attention
between robots and the warehouse floor; the image-floor (IF) model applies
convolutional encoding to a multi-channel image representation of the full
fleet; and the graph-floor (GF) model combines temporal attention with graph
neural networks for spatial relationships. In this paper, we describe these
models and present our evaluation of the impact of these design choices on
prediction task performance. We find that the robot-centric and graph-floor
models, which both use asynchronous robot state updates and incorporate the
localized structure of robot interactions, show the most promise. We also
present experiments that show that these two models can make effective use of
larger warehouses operation datasets as the models are scaled up.

</details>


### [10] [Developing a Calibrated Physics-Based Digital Twin for Construction Vehicles](https://arxiv.org/abs/2508.08576)
*Deniz Karanfil,Daniel Lindmark,Martin Servin,David Torick,Bahram Ravani*

Main category: cs.RO

TL;DR: 本文开发了一个校准的数字孪生轮式装载机，通过高保真数字模型实现自动化诊断、操作优化和预规划模拟。


<details>
  <summary>Details</summary>
Motivation: 提升建筑操作的自动化能力，通过数字孪生技术实现高保真模拟和优化。

Method: 使用AGX Dynamics软件中的多体动力学模型，结合传感器校准数字模型。

Result: 校准后的数字孪生能高精度估计铲斗受力，提供高保真模拟。

Conclusion: 校准数字孪生技术可有效提升建筑操作的自动化规划和优化能力。

Abstract: This paper presents the development of a calibrated digital twin of a wheel
loader. A calibrated digital twin integrates a construction vehicle with a
high-fidelity digital model allowing for automated diagnostics and optimization
of operations as well as pre-planning simulations enhancing automation
capabilities. The high-fidelity digital model is a virtual twin of the physical
wheel loader. It uses a physics-based multibody dynamic model of the wheel
loader in the software AGX Dynamics. Interactions of the wheel loader's bucket
while in use in construction can be simulated in the virtual model. Calibration
makes this simulation of high-fidelity which can enhance realistic planning for
automation of construction operations. In this work, a wheel loader was
instrumented with several sensors used to calibrate the digital model. The
calibrated digital twin was able to estimate the magnitude of the forces on the
bucket base with high accuracy, providing a high-fidelity simulation.

</details>


### [11] [Autonomous Mobile Plant Watering Robot : A Kinematic Approach](https://arxiv.org/abs/2508.08607)
*Justin London*

Main category: cs.RO

TL;DR: 论文介绍了一种新型自主移动植物浇水机器人，配备6自由度机械臂和4轮驱动底盘，结合计算机视觉和传感器技术，实现智能浇水。


<details>
  <summary>Details</summary>
Motivation: 现有农业机器人成本高且功能有限，无法满足植物浇水的灵活需求。

Method: 机器人使用Jetson Nano和Arduino微控制器，结合YOLOv5和Pl@ntNet-300K数据集进行植物识别，利用LIDAR避障，并通过机械臂和土壤湿度传感器实现精准浇水。

Result: 提供了DH表、运动学模型及仿真与实验结果，验证了机器人的可行性和有效性。

Conclusion: 该机器人具有低成本、高灵活性和智能化的特点，适用于植物养护。

Abstract: Plants need regular and the appropriate amount of watering to thrive and
survive. While agricultural robots exist that can spray water on plants and
crops such as the , they are expensive and have limited mobility and/or
functionality. We introduce a novel autonomous mobile plant watering robot that
uses a 6 degree of freedom (DOF) manipulator, connected to a 4 wheel drive
alloy chassis, to be able to hold a garden hose, recognize and detect plants,
and to water them with the appropriate amount of water by being able to insert
a soil humidity/moisture sensor into the soil. The robot uses Jetson Nano and
Arduino microcontroller and real sense camera to perform computer vision to
detect plants using real-time YOLOv5 with the Pl@ntNet-300K dataset. The robot
uses LIDAR for object and collision avoideance and does not need to move on a
pre-defined path and can keep track of which plants it has watered. We provide
the Denavit-Hartenberg (DH) Table, forward kinematics, differential driving
kinematics, and inverse kinematics along with simulation and experiment results

</details>


### [12] [Communication Efficient Robotic Mixed Reality with Gaussian Splatting Cross-Layer Optimization](https://arxiv.org/abs/2508.08624)
*Chenxuan Liu,He Li,Zongze Li,Shuai Wang,Wei Xu,Kejiang Ye,Derrick Wing Kwan Ng,Chengzhong Xu*

Main category: cs.RO

TL;DR: 论文提出GSMR和GSCLO框架，通过高斯泼溅技术和跨层优化降低机器人混合现实系统的通信成本。


<details>
  <summary>Details</summary>
Motivation: 解决机器人混合现实系统中高分辨率图像上传导致的通信成本问题。

Method: 使用高斯泼溅（GS）技术渲染逼真视图，结合GSCLO框架优化内容切换和功率分配。

Result: 实验表明，GSMR和GSCLO在多种场景和机器人上显著优于现有基准，通信成本极低。

Conclusion: 首次证明机器人混合现实可实现超低通信成本，动态场景中混合数据有助于提升GS性能。

Abstract: Realizing low-cost communication in robotic mixed reality (RoboMR) systems
presents a challenge, due to the necessity of uploading high-resolution images
through wireless channels. This paper proposes Gaussian splatting (GS) RoboMR
(GSMR), which enables the simulator to opportunistically render a
photo-realistic view from the robot's pose by calling ``memory'' from a GS
model, thus reducing the need for excessive image uploads. However, the GS
model may involve discrepancies compared to the actual environments. To this
end, a GS cross-layer optimization (GSCLO) framework is further proposed, which
jointly optimizes content switching (i.e., deciding whether to upload image or
not) and power allocation (i.e., adjusting to content profiles) across
different frames by minimizing a newly derived GSMR loss function. The GSCLO
problem is addressed by an accelerated penalty optimization (APO) algorithm
that reduces computational complexity by over $10$x compared to traditional
branch-and-bound and search algorithms. Moreover, variants of GSCLO are
presented to achieve robust, low-power, and multi-robot GSMR. Extensive
experiments demonstrate that the proposed GSMR paradigm and GSCLO method
achieve significant improvements over existing benchmarks on both wheeled and
legged robots in terms of diverse metrics in various scenarios. For the first
time, it is found that RoboMR can be achieved with ultra-low communication
costs, and mixture of data is useful for enhancing GS performance in dynamic
scenarios.

</details>


### [13] [ZS-Puffin: Design, Modeling and Implementation of an Unmanned Aerial-Aquatic Vehicle with Amphibious Wings](https://arxiv.org/abs/2508.08690)
*Zhenjiang Wang,Yunhua Jiang,Zikun Zhen,Yifan Jiang,Yubin Tan,Wubin Wang*

Main category: cs.RO

TL;DR: 提出一种基于海雀翅膀启发的两栖翼无人空中-水下飞行器（UAAV），通过单自由度俯仰设计实现空中和水下推进，减少对海洋生物的干扰。


<details>
  <summary>Details</summary>
Motivation: 解决介质差异对飞行器推进系统的挑战，同时减少对海洋环境的影响。

Method: 重新设计固定翼结构为两栖翼，引入人工中央模式生成器（CPG）优化扑翼运动。

Result: 成功开发出原型机，验证了两栖翼在空中和水下的功能。

Conclusion: 两栖翼设计为UAAV提供了高效且环保的解决方案。

Abstract: Unmanned aerial-aquatic vehicles (UAAVs) can operate both in the air and
underwater, giving them broad application prospects. Inspired by the
dual-function wings of puffins, we propose a UAAV with amphibious wings to
address the challenge posed by medium differences on the vehicle's propulsion
system. The amphibious wing, redesigned based on a fixed-wing structure,
features a single degree of freedom in pitch and requires no additional
components. It can generate lift in the air and function as a flapping wing for
propulsion underwater, reducing disturbance to marine life and making it
environmentally friendly. Additionally, an artificial central pattern generator
(CPG) is introduced to enhance the smoothness of the flapping motion. This
paper presents the prototype, design details, and practical implementation of
this concept.

</details>


### [14] [OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing](https://arxiv.org/abs/2508.08706)
*Zhengxue Cheng,Yiqian Zhang,Wenkang Zhang,Haoyu Li,Keyu Wang,Li Song,Hengdi Zhang*

Main category: cs.RO

TL;DR: OmniVTLA是一种结合触觉感知的视觉-语言-动作（VLA）模型，通过双路径触觉编码器和新数据集ObjTac，显著提升了接触密集型任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型因触觉传感器异构性和数据获取困难，忽视了触觉感知的重要性，导致在接触密集型任务中表现不佳。

Method: 提出双路径触觉编码器框架（ViT和SA-ViT），并引入包含13.5万样本的触觉数据集ObjTac，训练语义对齐的触觉编码器。

Result: 在真实实验中，OmniVTLA在抓取任务中成功率提升21.9%（夹爪）和6.2%（灵巧手），任务完成时间缩短，轨迹更平滑。

Conclusion: OmniVTLA通过触觉感知显著提升了VLA模型的性能，尤其在接触密集型任务中表现突出。

Abstract: Recent vision-language-action (VLA) models build upon vision-language
foundations, and have achieved promising results and exhibit the possibility of
task generalization in robot manipulation. However, due to the heterogeneity of
tactile sensors and the difficulty of acquiring tactile data, current VLA
models significantly overlook the importance of tactile perception and fail in
contact-rich tasks. To address this issue, this paper proposes OmniVTLA, a
novel architecture involving tactile sensing. Specifically, our contributions
are threefold. First, our OmniVTLA features a dual-path tactile encoder
framework. This framework enhances tactile perception across diverse
vision-based and force-based tactile sensors by using a pretrained vision
transformer (ViT) and a semantically-aligned tactile ViT (SA-ViT). Second, we
introduce ObjTac, a comprehensive force-based tactile dataset capturing
textual, visual, and tactile information for 56 objects across 10 categories.
With 135K tri-modal samples, ObjTac supplements existing visuo-tactile
datasets. Third, leveraging this dataset, we train a semantically-aligned
tactile encoder to learn a unified tactile representation, serving as a better
initialization for OmniVTLA. Real-world experiments demonstrate substantial
improvements over state-of-the-art VLA baselines, achieving 96.9% success rates
with grippers, (21.9% higher over baseline) and 100% success rates with
dexterous hands (6.2% higher over baseline) in pick-and-place tasks. Besides,
OmniVTLA significantly reduces task completion time and generates smoother
trajectories through tactile sensing compared to existing VLA.

</details>


### [15] [Towards Safe Imitation Learning via Potential Field-Guided Flow Matching](https://arxiv.org/abs/2508.08707)
*Haoran Ding,Anqing Duan,Zezhou Sun,Leonel Rozo,Noémie Jaquier,Dezhen Song,Yoshihiko Nakamura*

Main category: cs.RO

TL;DR: PF2MP是一种结合势场引导和流匹配的新方法，用于生成安全运动策略，显著减少碰撞。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型在模仿学习中生成的运动策略在复杂环境中安全性不足，PF2MP旨在填补这一空白。

Method: PF2MP通过同一组成功演示同时学习任务策略和障碍物势场，推理时用势场调整流匹配向量场。

Result: 实验表明PF2MP在仿真和实际环境中均能提高安全性，显著减少碰撞。

Conclusion: PF2MP为复杂环境中的安全运动生成提供了有效解决方案。

Abstract: Deep generative models, particularly diffusion and flow matching models, have
recently shown remarkable potential in learning complex policies through
imitation learning. However, the safety of generated motions remains
overlooked, particularly in complex environments with inherent obstacles. In
this work, we address this critical gap by proposing Potential Field-Guided
Flow Matching Policy (PF2MP), a novel approach that simultaneously learns task
policies and extracts obstacle-related information, represented as a potential
field, from the same set of successful demonstrations. During inference, PF2MP
modulates the flow matching vector field via the learned potential field,
enabling safe motion generation. By leveraging these complementary fields, our
approach achieves improved safety without compromising task success across
diverse environments, such as navigation tasks and robotic manipulation
scenarios. We evaluate PF2MP in both simulation and real-world settings,
demonstrating its effectiveness in task space and joint space control.
Experimental results demonstrate that PF2MP enhances safety, achieving a
significant reduction of collisions compared to baseline policies. This work
paves the way for safer motion generation in unstructured and obstaclerich
environments.

</details>


### [16] [CRADLE: Conversational RTL Design Space Exploration with LLM-based Multi-Agent Systems](https://arxiv.org/abs/2508.08709)
*Lukas Krupp,Maximilian Schöffel,Elias Biehl,Norbert Wehn*

Main category: cs.RO

TL;DR: CRADLE是一个基于LLM的多智能体对话框架，用于RTL设计空间探索，支持用户引导、自验证和优化，显著减少FPGA资源使用。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于僵化，CRADLE旨在提供更灵活、用户友好的设计探索流程。

Method: 采用生成器-批评家智能体系统，结合先进LLM技术，专注于FPGA资源最小化。

Result: 在RTLLM基准测试中，LUT和FF资源使用平均减少48%和40%。

Conclusion: CRADLE通过多智能体协作和LLM技术，有效优化了RTL设计资源使用。

Abstract: This paper presents CRADLE, a conversational framework for design space
exploration of RTL designs using LLM-based multi-agent systems. Unlike existing
rigid approaches, CRADLE enables user-guided flows with internal
self-verification, correction, and optimization. We demonstrate the framework
with a generator-critic agent system targeting FPGA resource minimization using
state-of-the-art LLMs. Experimental results on the RTLLM benchmark show that
CRADLE achieves significant reductions in resource usage with averages of 48%
and 40% in LUTs and FFs across all benchmark designs.

</details>


### [17] [Boosting Action-Information via a Variational Bottleneck on Unlabelled Robot Videos](https://arxiv.org/abs/2508.08743)
*Haoyu Zhang,Long Cheng*

Main category: cs.RO

TL;DR: 论文提出了一种新框架，通过最大化潜在动作与真实动作的互信息，直接从无标签视频演示中学习，提升了策略性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于演示的学习（LfD）依赖大量带标签的专家轨迹，限制了训练数据的规模。直接从无标签视频学习是替代方案，但现有方法提取的潜在动作与真实动作互信息低，导致控制性能不佳。

Method: 引入了一种新框架，利用变分信息瓶颈技术，显式最大化潜在动作与真实动作的互信息，即使没有动作标签。

Result: 实验验证表明，该方法在仿真和真实机器人环境中显著提升了互信息和策略性能。

Conclusion: 该方法有效解决了无标签视频学习中潜在动作与真实动作互信息低的问题，为LfD提供了更高效的数据利用方式。

Abstract: Learning from demonstrations (LfD) typically relies on large amounts of
action-labeled expert trajectories, which fundamentally constrains the scale of
available training data. A promising alternative is to learn directly from
unlabeled video demonstrations. However, we find that existing methods tend to
encode latent actions that share little mutual information with the true robot
actions, leading to suboptimal control performance. To address this limitation,
we introduce a novel framework that explicitly maximizes the mutual information
between latent actions and true actions, even in the absence of action labels.
Our method leverage the variational information-bottleneck to extract
action-relevant representations while discarding task-irrelevant information.
We provide a theoretical analysis showing that our objective indeed maximizes
the mutual information between latent and true actions. Finally, we validate
our approach through extensive experiments: first in simulated robotic
environments and then on real-world robotic platforms, the experimental results
demonstrate that our method significantly enhances mutual information and
consistently improves policy performance.

</details>


### [18] [Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT](https://arxiv.org/abs/2508.08748)
*Muhammad A. Muttaqien,Tomohiro Motoda,Ryo Hanai,Yukiyasu Domae*

Main category: cs.RO

TL;DR: 论文提出了一种基于标注引导视觉提示和动作分块模仿学习的机器人抓取-放置系统，提升了零售环境中的操作准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 便利店中的机器人抓取-放置任务因物体密集、遮挡和属性多样而复杂，传统方法难以应对。

Method: 采用标注引导视觉提示提供空间结构信息，并利用动作分块模仿学习（ACT）算法预测连续动作序列。

Result: 实验表明系统在抓取成功率和适应性上有显著提升。

Conclusion: 该方法为复杂零售环境中的机器人操作提供了高效解决方案。

Abstract: Robotic pick-and-place tasks in convenience stores pose challenges due to
dense object arrangements, occlusions, and variations in object properties such
as color, shape, size, and texture. These factors complicate trajectory
planning and grasping. This paper introduces a perception-action pipeline
leveraging annotation-guided visual prompting, where bounding box annotations
identify both pickable objects and placement locations, providing structured
spatial guidance. Instead of traditional step-by-step planning, we employ
Action Chunking with Transformers (ACT) as an imitation learning algorithm,
enabling the robotic arm to predict chunked action sequences from human
demonstrations. This facilitates smooth, adaptive, and data-driven
pick-and-place operations. We evaluate our system based on success rate and
visual analysis of grasping behavior, demonstrating improved grasp accuracy and
adaptability in retail environments.

</details>


### [19] [Robot can reduce superior's dominance in group discussions with human social hierarchy](https://arxiv.org/abs/2508.08767)
*Kazuki Komura,Kumi Ozaki,Seiji Yamada*

Main category: cs.RO

TL;DR: 研究探讨了机器人代理是否能通过社交层级关系减少上级的支配性并均衡讨论参与度。实验结果显示机器人行为可能影响发言时间，但未显著差异。机器人行为可能在不降低上级满意度的情况下均衡参与。


<details>
  <summary>Details</summary>
Motivation: 在层级结构的讨论中，上级可能主导发言，导致参与不均衡。研究旨在探索机器人是否能通过社交层级干预减少这种不平等。

Method: 招募30名医生和学生进行实验，机器人根据层级关系鼓励发言，并与无干预和均等干预策略对比。

Result: 机器人行为可能影响发言时间，但未显著差异。机器人行为可能在不降低上级满意度的情况下均衡参与。

Conclusion: 在层级讨论中，机器人通过控制反馈行为可能抑制上级支配性并均衡参与。

Abstract: This study investigated whether robotic agents that deal with social
hierarchical relationships can reduce the dominance of superiors and equalize
participation among participants in discussions with hierarchical structures.
Thirty doctors and students having hierarchical relationship were gathered as
participants, and an intervention experiment was conducted using a robot that
can encourage participants to speak depending on social hierarchy. These were
compared with strategies that intervened equally for all participants without
considering hierarchy and with a no-action. The robots performed follow
actions, showing backchanneling to speech, and encourage actions, prompting
speech from members with less speaking time, on the basis of the hierarchical
relationships among group members to equalize participation. The experimental
results revealed that the robot's actions could potentially influence the
speaking time among members, but it could not be conclusively stated that there
were significant differences between the robot's action conditions. However,
the results suggested that it might be possible to influence speaking time
without decreasing the satisfaction of superiors. This indicates that in
discussion scenarios where experienced superiors are likely to dominate,
controlling the robot's backchanneling behavior could potentially suppress
dominance and equalize participation among group members.

</details>


### [20] [Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors](https://arxiv.org/abs/2508.08896)
*Haoyu Zhao,Linghao Zhuang,Xingyue Zhao,Cheng Zeng,Haoran Xu,Yuming Jiang,Jun Cen,Kexiang Wang,Jiayan Guo,Siteng Huang,Xin Li,Deli Zhao,Hua Zou*

Main category: cs.RO

TL;DR: AffordDex是一个两阶段训练框架，通过模仿人类手部运动和物体功能感知，实现通用灵巧抓取。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注抓取稳定性，忽略了功能感知定位和类人姿势，这对下游操作至关重要。

Method: AffordDex分两阶段训练：第一阶段模仿人类手部运动，第二阶段通过残差模块和功能感知模块（NAA）适应具体物体。

Result: AffordDex在通用抓取、类人姿势和功能接触方面显著优于现有方法，适用于已知和未知物体。

Conclusion: AffordDex通过结合运动先验和功能感知，实现了更通用且类人的灵巧抓取。

Abstract: A dexterous hand capable of generalizable grasping objects is fundamental for
the development of general-purpose embodied AI. However, previous methods focus
narrowly on low-level grasp stability metrics, neglecting affordance-aware
positioning and human-like poses which are crucial for downstream manipulation.
To address these limitations, we propose AffordDex, a novel framework with
two-stage training that learns a universal grasping policy with an inherent
understanding of both motion priors and object affordances. In the first stage,
a trajectory imitator is pre-trained on a large corpus of human hand motions to
instill a strong prior for natural movement. In the second stage, a residual
module is trained to adapt these general human-like motions to specific object
instances. This refinement is critically guided by two components: our Negative
Affordance-aware Segmentation (NAA) module, which identifies functionally
inappropriate contact regions, and a privileged teacher-student distillation
process that ensures the final vision-based policy is highly successful.
Extensive experiments demonstrate that AffordDex not only achieves universal
dexterous grasping but also remains remarkably human-like in posture and
functionally appropriate in contact location. As a result, AffordDex
significantly outperforms state-of-the-art baselines across seen objects,
unseen instances, and even entirely novel categories.

</details>


### [21] [Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion](https://arxiv.org/abs/2508.08982)
*Seungeun Rho,Kartik Garg,Morgan Byrd,Sehoon Ha*

Main category: cs.RO

TL;DR: SDAX框架通过无监督技能发现减少人工工程需求，使四足机器人自主掌握多种敏捷行为。


<details>
  <summary>Details</summary>
Motivation: 探索对于四足机器人学习克服多样化障碍的敏捷行为至关重要，但现有方法依赖人工工程或专家演示，限制了泛化能力。

Method: 提出SDAX框架，利用无监督技能发现和双层优化动态调节探索程度。

Result: SDAX使机器人掌握爬行、攀爬、跳跃等敏捷行为，并成功迁移到真实硬件。

Conclusion: SDAX显著减少人工干预，提升机器人自主学习和泛化能力。

Abstract: Exploration is crucial for enabling legged robots to learn agile locomotion
behaviors that can overcome diverse obstacles. However, such exploration is
inherently challenging, and we often rely on extensive reward engineering,
expert demonstrations, or curriculum learning - all of which limit
generalizability. In this work, we propose Skill Discovery as Exploration
(SDAX), a novel learning framework that significantly reduces human engineering
effort. SDAX leverages unsupervised skill discovery to autonomously acquire a
diverse repertoire of skills for overcoming obstacles. To dynamically regulate
the level of exploration during training, SDAX employs a bi-level optimization
process that autonomously adjusts the degree of exploration. We demonstrate
that SDAX enables quadrupedal robots to acquire highly agile behaviors
including crawling, climbing, leaping, and executing complex maneuvers such as
jumping off vertical walls. Finally, we deploy the learned policy on real
hardware, validating its successful transfer to the real world.

</details>


### [22] [Rational Inverse Reasoning](https://arxiv.org/abs/2508.08983)
*Ben Zandonati,Tomás Lozano-Pérez,Leslie Pack Kaelbling*

Main category: cs.RO

TL;DR: 论文提出Rational Inverse Reasoning (RIR)框架，通过分层生成模型推断潜在程序，实现机器人从少量演示中泛化到新任务。


<details>
  <summary>Details</summary>
Motivation: 人类能从单个不完美的演示中泛化到不同任务，而机器人需要大量数据且泛化能力差。研究认为这是由于机器人无法恢复智能行为的潜在解释。

Method: RIR框架通过分层生成模型推断潜在程序，结合视觉语言模型和规划器评分，生成可执行程序。

Result: RIR在连续操作任务中，仅需一次演示即可推断任务结构并泛化到新场景，优于现有视觉语言模型基线。

Conclusion: RIR通过程序归纳实现了机器人从少量演示中的高效泛化，为智能行为解释提供了新思路。

Abstract: Humans can observe a single, imperfect demonstration and immediately
generalize to very different problem settings. Robots, in contrast, often
require hundreds of examples and still struggle to generalize beyond the
training conditions. We argue that this limitation arises from the inability to
recover the latent explanations that underpin intelligent behavior, and that
these explanations can take the form of structured programs consisting of
high-level goals, sub-task decomposition, and execution constraints. In this
work, we introduce Rational Inverse Reasoning (RIR), a framework for inferring
these latent programs through a hierarchical generative model of behavior. RIR
frames few-shot imitation as Bayesian program induction: a vision-language
model iteratively proposes structured symbolic task hypotheses, while a
planner-in-the-loop inference scheme scores each by the likelihood of the
observed demonstration under that hypothesis. This loop yields a posterior over
concise, executable programs. We evaluate RIR on a suite of continuous
manipulation tasks designed to test one-shot and few-shot generalization across
variations in object pose, count, geometry, and layout. With as little as one
demonstration, RIR infers the intended task structure and generalizes to novel
settings, outperforming state-of-the-art vision-language model baselines.

</details>


### [23] [Generation of Real-time Robotic Emotional Expressions Learning from Human Demonstration in Mixed Reality](https://arxiv.org/abs/2508.08999)
*Chao Wang,Michael Gienger,Fan Zhang*

Main category: cs.RO

TL;DR: 提出一种基于混合现实（MR）的框架，通过专家演示生成机器人情感表达，利用流匹配生成实时多样化行为。


<details>
  <summary>Details</summary>
Motivation: 机器人情感表达对与人类互动至关重要，但现有方法缺乏多样性和真实性。

Method: 专家通过第一人称视角远程操作虚拟机器人，捕捉其面部表情、头部动作和上半身手势，并映射到机器人组件；使用流匹配生成模型实时生成多样化行为。

Result: 初步测试验证了该方法生成自主情感表达的有效性。

Conclusion: 该框架能有效生成真实且多样化的机器人情感表达，提升人机互动体验。

Abstract: Expressive behaviors in robots are critical for effectively conveying their
emotional states during interactions with humans. In this work, we present a
framework that autonomously generates realistic and diverse robotic emotional
expressions based on expert human demonstrations captured in Mixed Reality
(MR). Our system enables experts to teleoperate a virtual robot from a
first-person perspective, capturing their facial expressions, head movements,
and upper-body gestures, and mapping these behaviors onto corresponding robotic
components including eyes, ears, neck, and arms. Leveraging a
flow-matching-based generative process, our model learns to produce coherent
and varied behaviors in real-time in response to moving objects, conditioned
explicitly on given emotional states. A preliminary test validated the
effectiveness of our approach for generating autonomous expressions.

</details>


### [24] [Large Scale Robotic Material Handling: Learning, Planning, and Control](https://arxiv.org/abs/2508.09003)
*Filippo A. Spinelli,Yifan Zhai,Fang Nan,Pascal Egli,Julian Nubert,Thilo Bleumer,Lukas Miller,Ferdinand Hofmann,Marco Hutter*

Main category: cs.RO

TL;DR: 提出了一种用于大规模物料搬运任务的自主执行框架，结合了感知、路径规划和运动控制模块，并通过强化学习优化抓取点和轨迹跟踪。


<details>
  <summary>Details</summary>
Motivation: 物料搬运是许多行业中的核心操作，但传统方法依赖人工操作，效率低且存在安全隐患。

Method: 系统整合了环境感知、抓取点选择、路径规划和运动控制模块，采用强化学习优化抓取点和轨迹跟踪。

Result: 在40吨物料搬运设备上的实验表明，系统在精度、重复性和安全性上优于人工操作。

Conclusion: 首次实现了全规模物料搬运任务的完整自动化，验证了框架的有效性。

Abstract: Bulk material handling involves the efficient and precise moving of large
quantities of materials, a core operation in many industries, including cargo
ship unloading, waste sorting, construction, and demolition. These repetitive,
labor-intensive, and safety-critical operations are typically performed using
large hydraulic material handlers equipped with underactuated grippers. In this
work, we present a comprehensive framework for the autonomous execution of
large-scale material handling tasks. The system integrates specialized modules
for environment perception, pile attack point selection, path planning, and
motion control. The main contributions of this work are two reinforcement
learning-based modules: an attack point planner that selects optimal grasping
locations on the material pile to maximize removal efficiency and minimize the
number of scoops, and a robust trajectory following controller that addresses
the precision and safety challenges associated with underactuated grippers in
movement, while utilizing their free-swinging nature to release material
through dynamic throwing. We validate our framework through real-world
experiments on a 40 t material handler in a representative worksite, focusing
on two key tasks: high-throughput bulk pile management and high-precision truck
loading. Comparative evaluations against human operators demonstrate the
system's effectiveness in terms of precision, repeatability, and operational
safety. To the best of our knowledge, this is the first complete automation of
material handling tasks on a full scale.

</details>


### [25] [GeoVLA: Empowering 3D Representations in Vision-Language-Action Models](https://arxiv.org/abs/2508.09071)
*Lin Sun,Bin Xie,Yingfei Liu,Hao Shi,Tiancai Wang,Jiale Cao*

Main category: cs.RO

TL;DR: GeoVLA是一种新型的Vision-Language-Action框架，通过整合3D几何信息提升机器人操作能力，优于现有2D视觉输入的VLA模型。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型主要依赖2D视觉输入，忽略了3D物理世界的几何信息，限制了空间感知和适应性。

Method: 结合视觉语言模型处理图像和语言指令，同时通过点云编码器提取3D几何信息，最终通过空间感知动作专家生成精确动作序列。

Result: 在LIBERO和ManiSkill2仿真基准测试中表现优异，并在需要高度适应性、尺度感知和视角不变性的实际任务中展现出强大鲁棒性。

Conclusion: GeoVLA通过整合3D信息显著提升了机器人操作的性能和适应性，为VLA模型的发展提供了新方向。

Abstract: Vision-Language-Action (VLA) models have emerged as a promising approach for
enabling robots to follow language instructions and predict corresponding
actions.However, current VLA models mainly rely on 2D visual inputs, neglecting
the rich geometric information in the 3D physical world, which limits their
spatial awareness and adaptability. In this paper, we present GeoVLA, a novel
VLA framework that effectively integrates 3D information to advance robotic
manipulation. It uses a vision-language model (VLM) to process images and
language instructions,extracting fused vision-language embeddings. In parallel,
it converts depth maps into point clouds and employs a customized point
encoder, called Point Embedding Network, to generate 3D geometric embeddings
independently. These produced embeddings are then concatenated and processed by
our proposed spatial-aware action expert, called 3D-enhanced Action Expert,
which combines information from different sensor modalities to produce precise
action sequences. Through extensive experiments in both simulation and
real-world environments, GeoVLA demonstrates superior performance and
robustness. It achieves state-of-the-art results in the LIBERO and ManiSkill2
simulation benchmarks and shows remarkable robustness in real-world tasks
requiring height adaptability, scale awareness and viewpoint invariance.

</details>
