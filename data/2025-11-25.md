<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 65]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [AUTOSAR AP and ROS 2 Collaboration Framework](https://arxiv.org/abs/2511.17540)
*Ryudai Iwakami,Bo Peng,Hiroyuki Hanyu,Tasuku Ishigooka,Takuya Azumi*

Main category: cs.RO

TL;DR: 提出了一个连接AUTOSAR AP和ROS 2的协作框架，通过DDS协议桥接两种平台的通信差异，实现无缝交互并验证了转换效率和集成便利性。


<details>
  <summary>Details</summary>
Motivation: 解决AUTOSAR AP在研究中因许可限制难以使用，而ROS 2在研究中广泛使用但无法直接用于商业化部署的问题，弥合研究与开发平台之间的差距。

Method: 使用数据分发服务(DDS)构建桥接转换器，实现AUTOSAR AP的SOME/IP协议与ROS 2的DDS协议之间的通信转换，并自动生成配置文件。

Result: 通过实证分析验证了桥接转换器的功能和性能，展示了其在转换时间和与ROS 2工具集成方面的效率。

Conclusion: 提出的协作框架成功连接了AUTOSAR AP和ROS 2平台，提高了框架的可用性，有助于加速自动驾驶技术的商业化进程。

Abstract: The field of autonomous vehicle research is advancing rapidly, necessitating platforms that meet real-time performance, safety, and security requirements for practical deployment. AUTOSAR Adaptive Platform (AUTOSAR AP) is widely adopted in development to meet these criteria; however, licensing constraints and tool implementation challenges limit its use in research. Conversely, Robot Operating System 2 (ROS 2) is predominantly used in research within the autonomous driving domain, leading to a disparity between research and development platforms that hinders swift commercialization. This paper proposes a collaboration framework that enables AUTOSAR AP and ROS 2 to communicate with each other using a Data Distribution Service for Real-Time Systems (DDS). In contrast, AUTOSAR AP uses Scalable service-Oriented Middleware over IP (SOME/IP) for communication. The proposed framework bridges these protocol differences, ensuring seamless interaction between the two platforms. We validate the functionality and performance of our bridge converter through empirical analysis, demonstrating its efficiency in conversion time and ease of integration with ROS 2 tools. Furthermore, the availability of the proposed collaboration framework is improved by automatically generating a configuration file for the proposed bridge converter.

</details>


### [2] [Implicit Neural Field-Based Process Planning for Multi-Axis Manufacturing: Direct Control over Collision Avoidance and Toolpath Geometry](https://arxiv.org/abs/2511.17578)
*Neelotpal Dutta,Tianyu Zhang,Tao Liu,Yongxue Chen,Charlie C. L. Wang*

Main category: cs.RO

TL;DR: 提出基于隐式神经场的多轴制造工艺规划框架，将层生成和刀具路径设计集成在单一可微分流程中，实现直接碰撞避免和联合优化。


<details>
  <summary>Details</summary>
Motivation: 现有基于弯曲层的多轴制造方法仅间接处理碰撞问题，并在后处理步骤中生成刀具路径，导致在优化过程中无法控制刀具路径几何形状。

Method: 使用正弦激活神经网络将层和刀具路径表示为隐式场，实现任意空间点的场值和导数直接评估，支持显式碰撞避免和制造层与刀具路径的联合优化。

Result: 该方法在增材和减材制造示例中得到验证，展示了其通用性和有效性，同时研究了网络超参数和目标定义对奇异行为和拓扑转换的影响。

Conclusion: 所提出的框架克服了现有方法的局限性，提供了正则化和稳定性控制的内置机制，为多轴制造工艺规划提供了更优的解决方案。

Abstract: Existing curved-layer-based process planning methods for multi-axis manufacturing address collisions only indirectly and generate toolpaths in a post-processing step, leaving toolpath geometry uncontrolled during optimization. We present an implicit neural field-based framework for multi-axis process planning that overcomes these limitations by embedding both layer generation and toolpath design within a single differentiable pipeline. Using sinusoidally activated neural networks to represent layers and toolpaths as implicit fields, our method enables direct evaluation of field values and derivatives at any spatial point, thereby allowing explicit collision avoidance and joint optimization of manufacturing layers and toolpaths. We further investigate how network hyperparameters and objective definitions influence singularity behavior and topology transitions, offering built-in mechanisms for regularization and stability control. The proposed approach is demonstrated on examples in both additive and subtractive manufacturing, validating its generality and effectiveness.

</details>


### [3] [Translating Cultural Choreography from Humanoid Forms to Robotic Arm](https://arxiv.org/abs/2511.17603)
*Chelsea-Xi Chen,Zhe Zhang,Aven-Le Zhou*

Main category: cs.RO

TL;DR: 本文提出了ROPERA系统，通过符号姿态转移和关节空间兼容符号来保持机器人手臂舞蹈的文化语义保真度，并在昆曲《牡丹亭》场景中验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人手臂编舞往往只重现轨迹而忽略了文化语义，需要一种能够保持文化语义保真度且能在不同形态机器人间移植的方法。

Method: 开发了ROPERA三阶段流程：编码文化编码的姿态、组合符号序列、解码为伺服命令。使用昆曲《牡丹亭》场景进行评估，包括基于语料库的姿态选择、符号记谱、直接关节角度执行，以及带有光绘和服装色彩的可视化层。

Result: 结果显示能够按预期时间重现执行，专家和观众报告了文化可读性。实现了非人类中心的文化保存和可移植的创作工作流程。

Conclusion: 该方法为文化保存提供了非人类中心的解决方案，未来工作将设计舞蹈感知的过渡配置文件，将符号扩展到包含触觉、音乐和空间线索的运动，并测试跨平台可移植性。

Abstract: Robotic arm choreography often reproduces trajectories while missing cultural semantics. This study examines whether symbolic posture transfer with joint space compatible notation can preserve semantic fidelity on a six-degree-of-freedom arm and remain portable across morphologies. We implement ROPERA, a three-stage pipeline for encoding culturally codified postures, composing symbolic sequences, and decoding to servo commands. A scene from Kunqu opera, \textit{The Peony Pavilion}, serves as the material for evaluation. The procedure includes corpus-based posture selection, symbolic scoring, direct joint angle execution, and a visual layer with light painting and costume-informed colors. Results indicate reproducible execution with intended timing and cultural legibility reported by experts and audiences. The study points to non-anthropocentric cultural preservation and portable authoring workflows. Future work will design dance-informed transition profiles, extend the notation to locomotion with haptic, musical, and spatial cues, and test portability across platforms.

</details>


### [4] [Robot joint characterisation and control using a magneto-optical rotary encoder](https://arxiv.org/abs/2511.17608)
*Yunlong Guo,John Canning,Zenon Chaczko,Gang-Ding Peng*

Main category: cs.RO

TL;DR: 提出了一种用于机器人旋转关节表征的紧凑型磁光旋转编码器，通过磁场诱导光学衰减实现360°连续旋转跟踪，具有低成本和高可靠性。


<details>
  <summary>Details</summary>
Motivation: 为机器人旋转关节提供一种低成本、可靠的替代传统旋转编码器的解决方案，同时保持竞争力性能。

Method: 采用双通配置，利用旋转非均匀磁体在光学环行器反射模式下产生磁场诱导的光学衰减效应。

Result: 编码器能够跟踪360°连续旋转，旋转扫描速率从135°/s到370°/s，角分辨率为0.3°。

Conclusion: 该系统为传统机器人旋转编码器提供了一个低成本且可靠的替代方案，同时保持了竞争性能。

Abstract: A robust and compact magneto-optical rotary encoder for the characterisation of robotic rotary joints is demonstrated. The system employs magnetic field-induced optical attenuation in a double-pass configuration using rotating nonuniform magnets around an optical circulator operating in reflection. The encoder tracks continuous 360° rotation with rotation sweep rates from ν = 135 °/s to ν = 370 °/s, and an angular resolution of Δθ = 0.3°. This offers a low-cost and reliable alternative to conventional robot rotation encoders while maintaining competitive performance.

</details>


### [5] [Vision-Guided Optic Flow Navigation for Small Lunar Missions](https://arxiv.org/abs/2511.17720)
*Sean Cowan,Pietro Fanti,Leon B. S. Williams,Chit Hong Yam,Kaneyasu Asakuma,Yuichiro Nada,Dario Izzo*

Main category: cs.RO

TL;DR: 提出了一种基于光流和测距仪深度估计的运动场反演框架，用于月球着陆过程中的自主导航，该方案在CPU上运行，适合小型月球着陆器的资源限制。


<details>
  <summary>Details</summary>
Motivation: 解决私人月球任务在质量、功耗和计算资源严格限制下的鲁棒自主导航挑战，特别是月球下降阶段的运动估计问题。

Method: 扩展经典光流公式，结合针对月球/行星接近、下降和着陆几何形状的深度建模策略（平面和球形地形近似），使用激光测距仪参数化，通过最小二乘框架进行运动场反演，采用金字塔Lucas-Kanade算法提取稀疏光流特征。

Result: 在月球南极复杂地形上使用合成图像验证，速度估计准确，复杂地形误差低于10%，典型地形误差约1%，性能适合实时应用。

Conclusion: 该框架有望为小型月球任务提供鲁棒、轻量级的机载导航解决方案。

Abstract: Private lunar missions are faced with the challenge of robust autonomous navigation while operating under stringent constraints on mass, power, and computational resources. This work proposes a motion-field inversion framework that uses optical flow and rangefinder-based depth estimation as a lightweight CPU-based solution for egomotion estimation during lunar descent. We extend classical optical flow formulations by integrating them with depth modeling strategies tailored to the geometry for lunar/planetary approach, descent, and landing, specifically, planar and spherical terrain approximations parameterized by a laser rangefinder. Motion field inversion is performed through a least-squares framework, using sparse optical flow features extracted via the pyramidal Lucas-Kanade algorithm. We verify our approach using synthetically generated lunar images over the challenging terrain of the lunar south pole, using CPU budgets compatible with small lunar landers. The results demonstrate accurate velocity estimation from approach to landing, with sub-10% error for complex terrain and on the order of 1% for more typical terrain, as well as performances suitable for real-time applications. This framework shows promise for enabling robust, lightweight on-board navigation for small lunar missions.

</details>


### [6] [LEARN: Learning End-to-End Aerial Resource-Constrained Multi-Robot Navigation](https://arxiv.org/abs/2511.17765)
*Darren Chiu,Zhehui Huang,Ruohai Ge,Gaurav S. Sukhatme*

Main category: cs.RO

TL;DR: LEARN是一个轻量级的两阶段安全引导强化学习框架，用于多无人机在复杂环境中的导航，结合低分辨率ToF传感器和紧凑的注意力机制RL策略，在资源受限的纳米无人机上实现高效导航。


<details>
  <summary>Details</summary>
Motivation: 纳米无人机团队具有高度灵活性，但受限于机载传感、通信和计算能力，现有基于高分辨率视觉或计算密集型规划器的方法无法适用。

Method: 使用两阶段安全引导强化学习框架，结合低分辨率ToF传感器、简单运动规划器和紧凑的注意力机制RL策略。

Result: 在仿真中性能优于两种最先进规划器10%，资源使用显著减少；在6架Crazyflie四旋翼上实现完全机载飞行，在室内外环境中速度达2.0m/s，可穿越0.2m间隙。

Conclusion: LEARN框架在资源受限的纳米无人机上实现了高效的多机导航，证明了轻量级RL方法在复杂环境导航中的可行性。

Abstract: Nano-UAV teams offer great agility yet face severe navigation challenges due to constrained onboard sensing, communication, and computation. Existing approaches rely on high-resolution vision or compute-intensive planners, rendering them infeasible for these platforms. We introduce LEARN, a lightweight, two-stage safety-guided reinforcement learning (RL) framework for multi-UAV navigation in cluttered spaces. Our system combines low-resolution Time-of-Flight (ToF) sensors and a simple motion planner with a compact, attention-based RL policy. In simulation, LEARN outperforms two state-of-the-art planners by $10\%$ while using substantially fewer resources. We demonstrate LEARN's viability on six Crazyflie quadrotors, achieving fully onboard flight in diverse indoor and outdoor environments at speeds up to $2.0 m/s$ and traversing $0.2 m$ gaps.

</details>


### [7] [Learning Diffusion Policies for Robotic Manipulation of Timber Joinery under Fabrication Uncertainty](https://arxiv.org/abs/2511.17774)
*Salma Mozaffari,Daniel Ruan,William van den Bogert,Nima Fazeli,Sigrid Adriaenssens,Arash Adel*

Main category: cs.RO

TL;DR: 该研究评估了扩散策略学习在建筑尺度接触敏感机器人装配中的性能和鲁棒性，以木工榫卯接头为案例，在存在制造不确定性的情况下实现了75%的平均成功率。


<details>
  <summary>Details</summary>
Motivation: 建筑不确定性（如制造误差和材料缺陷）对接触密集的机器人操作构成重大挑战，阻碍精确和稳健的装配。

Method: 采用两阶段研究：首先评估策略性能和适用性；其次评估处理制造不确定性的鲁棒性，通过随机扰动榫眼位置来模拟不确定性。

Result: 最佳策略在高达10毫米的扰动下实现了75%的总平均成功率，其中无扰动情况下达到100%成功率。

Conclusion: 结果表明感觉运动扩散策略有潜力推广到建筑和制造中各种复杂的接触密集装配任务，推进不确定性下的机器人建造，促进更安全、更高效的建筑实践。

Abstract: Construction uncertainties such as fabrication inaccuracies and material imperfections pose a significant challenge to contact-rich robotic manipulation by hindering precise and robust assembly. In this paper, we explore the performance and robustness of diffusion policy learning as a promising solution for contact-sensitive robotic assembly at construction scale, using timber mortise and tenon joints as a case study. A two-phase study is conducted: first, to evaluate policy performance and applicability; second, to assess robustness in handling fabrication uncertainties simulated as randomized perturbations to the mortise position. The best-performing policy achieved a total average success rate of 75% with perturbations up to 10 mm, including 100% success in unperturbed cases. The results demonstrate the potential of sensory-motor diffusion policies to generalize to a wide range of complex, contact-rich assembly tasks across construction and manufacturing, advancing robotic construction under uncertainty and contributing to safer, more efficient building practices.

</details>


### [8] [See, Plan, Cut: MPC-Based Autonomous Volumetric Robotic Laser Surgery with OCT Guidance](https://arxiv.org/abs/2511.17777)
*Ravi Prakash,Vincent Y. Wang,Arpit Mishra,Devi Yuliarti,Pei Zhong,Ryan P. McNabb,Patrick J. Codd,Leila J. Bridgeman*

Main category: cs.RO

TL;DR: RATS是一个智能光学机械机器人平台，集成OCT引导和手术激光，用于自主体积软组织切除，通过多级校准、激光-组织相互作用建模和模型预测控制实现高精度手术。


<details>
  <summary>Details</summary>
Motivation: 现有机器人激光系统缺乏体积规划和术中反馈，限制了在手术应用中的精确组织切除能力。

Method: 集成RGB-D成像、OCT和光纤耦合手术激光，采用多级校准管道、超高斯激光-组织相互作用模型和基于采样的模型预测控制框架。

Result: OCT到激光校准精度0.161±0.031mm，激光-组织相互作用模型平均RMSE 0.231±0.121mm，切除轨迹RMSE 0.842mm，相比前馈执行提升IoU一致性64.8%。

Conclusion: RATS平台能够检测亚表面结构并修改规划目标以保护这些结构，展示了临床可行性。

Abstract: Robotic laser systems offer the potential for sub-millimeter, non-contact, high-precision tissue resection, yet existing platforms lack volumetric planning and intraoperative feedback. We present RATS (Robot-Assisted Tissue Surgery), an intelligent opto-mechanical, optical coherence tomography (OCT)-guided robotic platform designed for autonomous volumetric soft tissue resection in surgical applications. RATS integrates macro-scale RGB-D imaging, micro-scale OCT, and a fiber-coupled surgical laser, calibrated through a novel multistage alignment pipeline that achieves OCT-to-laser calibration accuracy of 0.161+-0.031mm on tissue phantoms and ex vivo porcine tissue. A super-Gaussian laser-tissue interaction (LTI) model characterizes ablation crater morphology with an average RMSE of 0.231+-0.121mm, outperforming Gaussian baselines. A sampling-based model predictive control (MPC) framework operates directly on OCT voxel data to generate constraint-aware resection trajectories with closed-loop feedback, achieving 0.842mm RMSE and improving intersection-over-union agreement by 64.8% compared to feedforward execution. With OCT, RATS detects subsurface structures and modifies the planner's objective to preserve them, demonstrating clinical feasibility.

</details>


### [9] [SAFE-SMART: Safety Analysis and Formal Evaluation using STL Metrics for Autonomous RoboTs](https://arxiv.org/abs/2511.17781)
*Kristy Sakano,Jianyu An,Dinesh Manocha,Huan Xu*

Main category: cs.RO

TL;DR: 提出了一种基于监管的后验安全评估方法，用于学习型黑盒自主移动机器人，通过信号时序逻辑规范验证安全合规性，并指导针对性再训练。


<details>
  <summary>Details</summary>
Motivation: 需要确保学习型黑盒自主移动机器人持续符合不断演进的人类定义安全规则，实现监管驱动的安全评估和改进。

Method: 将人类安全需求转换为信号时序逻辑规范，对黑盒模型的轨迹进行外部合规验证，计算总鲁棒性值和最大鲁棒性值作为安全指标，指导针对性再训练。

Result: 在虚拟驾驶场景中：遵守限速轨迹增加177%，最小化越野驾驶轨迹增加1138%，按时到达目标轨迹增加16%。在自主导航场景中：避免急转弯轨迹增加300%，按时到达目标轨迹增加200%，最小化障碍物附近时间轨迹增加49%。在真实TurtleBot3机器人上验证了改进的障碍物导航能力。

Conclusion: 该方法能有效评估和改进学习型黑盒机器人的安全性，在虚拟和真实环境中均实现了统计显著的性能提升，证明了监管驱动安全评估框架的可行性。

Abstract: We present a novel, regulator-driven approach for post hoc safety evaluation of learning-based, black-box autonomous mobile robots, ensuring ongoing compliance with evolving, human-defined safety rules. In our iterative workflow, human safety requirements are translated by regulators into Signal Temporal Logic (STL) specifications. Rollout traces from the black-box model are externally verified for compliance, yielding quantitative safety metrics, Total Robustness Value (TRV) and Largest Robustness Value (LRV), which measure average and worst-case specification adherence. These metrics inform targeted retraining and iterative improvement by model designers. We apply our method across two different applications: a virtual driving scenario and an autonomous mobile robot navigating a complex environment, and observe statistically significant improvements across both scenarios. In the virtual driving scenario, we see a 177% increase in traces adhering to the simulation speed limit, a 1138% increase in traces minimizing off-road driving, and a 16% increase in traces successfully reaching the goal within the time limit. In the autonomous navigation scenario, there is a 300% increase in traces avoiding sharp turns, a 200% increase in traces reaching the goal within the time limit, and a 49% increase in traces minimizing time spent near obstacles. Finally, we validate our approach on a TurtleBot3 robot in the real world, and demonstrate improved obstacle navigation with safety buffers.

</details>


### [10] [SM$^2$ITH: Safe Mobile Manipulation with Interactive Human Prediction via Task-Hierarchical Bilevel Model Predictive Control](https://arxiv.org/abs/2511.17798)
*Francesco D'Orazio,Sepehr Samavi,Xintong Du,Siqi Zhou,Giuseppe Oriolo,Angela P. Schoellig*

Main category: cs.RO

TL;DR: SM²ITH框架将分层任务模型预测控制与交互式人体运动预测相结合，通过双层优化实现移动机器人在动态人机环境中的安全高效协调。


<details>
  <summary>Details</summary>
Motivation: 现有优化方法主要应用于静态或结构化场景，需要扩展至动态人机环境，这需要预测模型来捕捉人类对机器人动作的反应。

Method: 结合分层任务模型预测控制与交互式人体运动预测，通过双层优化共同考虑机器人和人类动力学。

Result: 在两个移动机械臂上验证了框架的有效性，在递送任务、顺序拾放任务和对抗性人类行为交互中表现出色。

Conclusion: 交互式预测能够实现安全高效的协调，优于依赖加权目标或开环人类模型的基线方法。

Abstract: Mobile manipulators are designed to perform complex sequences of navigation and manipulation tasks in human-centered environments. While recent optimization-based methods such as Hierarchical Task Model Predictive Control (HTMPC) enable efficient multitask execution with strict task priorities, they have so far been applied mainly to static or structured scenarios. Extending these approaches to dynamic human-centered environments requires predictive models that capture how humans react to the actions of the robot. This work introduces Safe Mobile Manipulation with Interactive Human Prediction via Task-Hierarchical Bilevel Model Predictive Control (SM$^2$ITH), a unified framework that combines HTMPC with interactive human motion prediction through bilevel optimization that jointly accounts for robot and human dynamics. The framework is validated on two different mobile manipulators, the Stretch 3 and the Ridgeback-UR10, across three experimental settings: (i) delivery tasks with different navigation and manipulation priorities, (ii) sequential pick-and-place tasks with different human motion prediction models, and (iii) interactions involving adversarial human behavior. Our results highlight how interactive prediction enables safe and efficient coordination, outperforming baselines that rely on weighted objectives or open-loop human models.

</details>


### [11] [MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots](https://arxiv.org/abs/2511.17889)
*Ting Huang,Dongjian Li,Rui Yang,Zeyu Zhang,Zida Yang,Hao Tang*

Main category: cs.RO

TL;DR: MobileVLA-R1是一个统一的视觉-语言-动作框架，通过构建大规模多粒度思维链数据集和两阶段训练范式，实现了四足机器人的显式推理和连续控制，在VLN和VLA任务上性能提升约5%。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以将高级语义推理与低级驱动连接起来，导致在真实世界中存在不稳定的grounding和弱泛化问题。

Method: 构建MobileVLA-CoT大规模多粒度思维链数据集，采用两阶段训练范式：监督CoT对齐和GRPO强化学习，以增强推理一致性、控制稳定性和长时程执行能力。

Result: 在VLN和VLA任务上表现出优于强基线的性能，提升约5%。在四足机器人上的真实世界部署验证了在复杂环境中的鲁棒性能。

Conclusion: MobileVLA-R1通过显式推理和连续控制的统一框架，有效解决了四足机器人自然语言指令grounding的挑战，在仿真和真实环境中均表现出色。

Abstract: Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.

</details>


### [12] [L1 Sample Flow for Efficient Visuomotor Learning](https://arxiv.org/abs/2511.17898)
*Weixi Song,Zhetao Chen,Tao Xu,Xianchao Zeng,Xinyu Zhou,Lixin Yang,Donglin Wang,Cewu Lu,Yong-Lu Li*

Main category: cs.RO

TL;DR: L1 Flow是一种结合去噪模型多模态分布拟合能力和L1回归效率的两步采样方法，将原始v预测流匹配转化为样本预测，仅需两次神经网络评估即可生成精确动作序列


<details>
  <summary>Details</summary>
Motivation: 结合去噪模型（如扩散和流匹配）的多模态分布拟合能力与L1回归的快速收敛和推理效率，避免模式崩溃同时提升训练和推理速度

Method: 将v预测流匹配重新表述为样本预测的L1训练目标，通过两步采样：单次积分步生成次优动作序列，单次预测重构精确动作序列

Result: 在MimicGen的8个任务、RoboMimic和PushT Bench的5个任务以及真实世界场景中，相比基线方法在训练效率、推理速度和整体性能方面均表现出优势

Conclusion: L1 Flow方法在保留流匹配优势的同时，将迭代神经网络评估减少到仅两次，缓解了直接样本回归可能带来的性能下降问题

Abstract: Denoising-based models, such as diffusion and flow matching, have been a critical component of robotic manipulation for their strong distribution-fitting and scaling capacity. Concurrently, several works have demonstrated that simple learning objectives, such as L1 regression, can achieve performance comparable to denoising-based methods on certain tasks, while offering faster convergence and inference. In this paper, we focus on how to combine the advantages of these two paradigms: retaining the ability of denoising models to capture multi-modal distributions and avoid mode collapse while achieving the efficiency of the L1 regression objective. To achieve this vision, we reformulate the original v-prediction flow matching and transform it into sample-prediction with the L1 training objective. We empirically show that the multi-modality can be expressed via a single ODE step. Thus, we propose \textbf{L1 Flow}, a two-step sampling schedule that generates a suboptimal action sequence via a single integration step and then reconstructs the precise action sequence through a single prediction. The proposed method largely retains the advantages of flow matching while reducing the iterative neural function evaluations to merely two and mitigating the potential performance degradation associated with direct sample regression. We evaluate our method with varying baselines and benchmarks, including 8 tasks in MimicGen, 5 tasks in RoboMimic \& PushT Bench, and one task in the real-world scenario. The results show the advantages of the proposed method with regard to training efficiency, inference speed, and overall performance. \href{https://song-wx.github.io/l1flow.github.io/}{Project Website.}

</details>


### [13] [Switch-JustDance: Benchmarking Whole Body Motion Tracking Policies Using a Commercial Console Game](https://arxiv.org/abs/2511.17925)
*Jeonghwan Kim,Wontaek Kim,Yidan Lu,Jin Cheng,Fatemeh Zargarbashi,Zicheng Zeng,Zekun Qi,Zhiyang Dou,Nitish Sontakke,Donghoon Baek,Sehoon Ha,Tianyu Li*

Main category: cs.RO

TL;DR: Switch-JustDance是一个利用任天堂Switch的Just Dance游戏进行机器人全身控制评估的低成本、可复现基准测试框架


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖预收集的人类运动数据集或基于仿真的实验，限制了可复现性、忽视了硬件因素，并阻碍了公平的人机比较

Method: 通过流媒体、运动重建和运动重定向模块将游戏中的编舞转换为机器人可执行动作，并利用游戏内置评分系统评估控制器性能

Result: 验证了Just Dance平台的评估特性，显示其提供一致且可解释的性能度量，适合作为具身AI的基准测试工具

Conclusion: 基于该框架对三种最先进的人形机器人全身控制器进行了硬件基准测试，揭示了它们的相对优势和局限性

Abstract: Recent advances in whole-body robot control have enabled humanoid and legged robots to perform increasingly agile and coordinated motions. However, standardized benchmarks for evaluating these capabilities in real-world settings, and in direct comparison to humans, remain scarce. Existing evaluations often rely on pre-collected human motion datasets or simulation-based experiments, which limit reproducibility, overlook hardware factors, and hinder fair human-robot comparisons. We present Switch-JustDance, a low-cost and reproducible benchmarking pipeline that leverages motion-sensing console games, Just Dance on the Nintendo Switch, to evaluate robot whole-body control. Using Just Dance on the Nintendo Switch as a representative platform, Switch-JustDance converts in-game choreography into robot-executable motions through streaming, motion reconstruction, and motion retargeting modules and enables users to evaluate controller performance through the game's built-in scoring system. We first validate the evaluation properties of Just Dance, analyzing its reliability, validity, sensitivity, and potential sources of bias. Our results show that the platform provides consistent and interpretable performance measures, making it a suitable tool for benchmarking embodied AI. Building on this foundation, we benchmark three state-of-the-art humanoid whole-body controllers on hardware and provide insights into their relative strengths and limitations.

</details>


### [14] [RoboArmGS: High-Quality Robotic Arm Splatting via Bézier Curve Refinement](https://arxiv.org/abs/2511.17961)
*Hao Wang,Xiaobao Wei,Ying Li,Qingpo Wuwu,Dongli Wu,Jiajun Cao,Ming Lu,Wenzhao Zheng,Shanghang Zhang*

Main category: cs.RO

TL;DR: RoboArmGS提出了一种混合表示方法，通过可学习的贝塞尔曲线修正URDF刚体运动，解决了真实机械臂运动与理想化URDF运动不匹配导致的渲染伪影问题。


<details>
  <summary>Details</summary>
Motivation: 构建高质量的机械臂数字资产对Real2Sim2Real流程至关重要，但现有方法将静态3D高斯绑定到URDF链接上，无法准确建模真实世界中的噪声运动，导致严重渲染伪影。

Method: 提出RoboArmGS混合表示，使用可学习的贝塞尔曲线运动修正器来修正每个关节的残差，更准确地建模真实世界运动，同时实现跨机械臂部件的3D高斯连贯绑定。

Result: 在RoboArm4D数据集上的评估显示，RoboArmGS在真实世界运动建模和渲染质量方面达到了最先进的性能。

Conclusion: RoboArmGS能够学习更准确的真实世界运动，同时贡献了RoboArm4D数据集以支持未来研究。

Abstract: Building high-quality digital assets of robotic arms is crucial yet challenging for the Real2Sim2Real pipeline. Current approaches naively bind static 3D Gaussians according to URDF links, forcing them to follow an URDF-rigged motion passively. However, real-world arm motion is noisy, and the idealized URDF-rigged motion cannot accurately model it, leading to severe rendering artifacts in 3D Gaussians. To address these challenges, we propose RoboArmGS, a novel hybrid representation that refines the URDF-rigged motion with learnable Bézier curves, enabling more accurate real-world motion modeling. To be more specific, we present a learnable Bézier Curve motion refiner that corrects per-joint residuals to address mismatches between real-world motion and URDF-rigged motion. RoboArmGS enables the learning of more accurate real-world motion while achieving a coherent binding of 3D Gaussians across arm parts. To support future research, we contribute a carefully collected dataset named RoboArm4D, which comprises several widely used robotic arms for evaluating the quality of building high-quality digital assets. We evaluate our approach on RoboArm4D, and RoboArmGS achieves state-of-the-art performance in real-world motion modeling and rendering quality. The code and dataset will be released.

</details>


### [15] [Unobservable Subspace Evolution and Alignment for Consistent Visual-Inertial Navigation](https://arxiv.org/abs/2511.17992)
*Chungeng Tian,Fenghua He,Ning Hao*

Main category: cs.RO

TL;DR: 提出了USE分析框架和USA解决方案，通过跟踪不可观测子空间的演化来系统分析VINS不一致性问题，并设计两种轻量级方法消除不一致性


<details>
  <summary>Details</summary>
Motivation: 解决VINS中长期存在的不一致性问题，现有研究主要归因于可观测性不匹配，但分析基于简化理论，未能涵盖实际VINS估计器中的非标准估计步骤

Method: 提出不可观测子空间演化(USE)分析框架，通过显式跟踪评估点变化来系统描述不可观测子空间在整个估计流程中的演化；基于此提出不可观测子空间对齐(USA)解决方案，包括变换基和重评估基两种方法

Result: 分析发现某些步骤引起的可观测性错位是可观测性不匹配的前因；提出的USA方法在广泛仿真和真实世界实验中验证了有效性

Conclusion: USE框架为理解VINS不一致性提供了新视角，USA解决方案通过选择性干预诱导错位的估计步骤，提供了准确且计算轻量的不一致性消除方法

Abstract: The inconsistency issue in the Visual-Inertial Navigation System (VINS) is a long-standing and fundamental challenge. While existing studies primarily attribute the inconsistency to observability mismatch, these analyses are often based on simplified theoretical formulations that consider only prediction and SLAM correction. Such formulations fail to cover the non-standard estimation steps, such as MSCKF correction and delayed initialization, which are critical for practical VINS estimators. Furthermore, the lack of a comprehensive understanding of how inconsistency dynamically emerges across estimation steps has hindered the development of precise and efficient solutions. As a result, current approaches often face a trade-off between estimator accuracy, consistency, and implementation complexity. To address these limitations, this paper proposes a novel analysis framework termed Unobservable Subspace Evolution (USE), which systematically characterizes how the unobservable subspace evolves throughout the entire estimation pipeline by explicitly tracking changes in its evaluation points. This perspective sheds new light on how individual estimation steps contribute to inconsistency. Our analysis reveals that observability misalignment induced by certain steps is the antecedent of observability mismatch. Guided by this insight, we propose a simple yet effective solution paradigm, Unobservable Subspace Alignment (USA), which eliminates inconsistency by selectively intervening only in those estimation steps that induce misalignment. We design two USA methods: transformation-based and re-evaluation-based, both offering accurate and computationally lightweight solutions. Extensive simulations and real-world experiments validate the effectiveness of the proposed methods.

</details>


### [16] [Continually Evolving Skill Knowledge in Vision Language Action Model](https://arxiv.org/abs/2511.18085)
*Yuxuan Wu,Guangming Wang,Zhiheng Yang,Maoqing Yao,Brian Sheil,Hesheng Wang*

Main category: cs.RO

TL;DR: Stellar VLA是一个知识驱动的持续学习框架，通过建模任务中心知识空间和分层任务-技能结构，实现VLA模型的持续技能学习，在LIBERO基准和真实任务中相比基线有50%以上的成功率提升。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型严重依赖任务特定的微调，缺乏持续学习能力，而现有持续学习方法难以扩展到VLA模型，需要资源密集型的训练。

Method: 提出Stellar VLA框架，包含T-Stellar（建模任务中心知识空间）和TS-Stellar（捕获分层任务-技能结构），通过联合学习任务潜在表示和知识空间实现自监督知识演化，使用知识引导的专家路由提供任务专业化。

Result: 在LIBERO基准和真实世界任务中，相比基线平均最终成功率提升超过50%，TS-Stellar在复杂动作推理方面表现更佳，验证了有效的知识保留和发现。

Conclusion: Stellar VLA框架成功解决了VLA模型的持续学习问题，通过知识驱动的方法减少了标注需求，降低了训练开销，实现了有效的技能学习和知识演化。

Abstract: Developing general robot intelligence in open environments requires continual skill learning. Recent Vision-Language-Action (VLA) models leverage massive pretraining data to support diverse manipulation tasks, but they still depend heavily on task-specific fine-tuning, revealing a lack of continual learning capability. Existing continual learning methods are also resource-intensive to scale to VLA models. We propose Stellar VLA, a knowledge-driven continual learning framework with two variants: T-Stellar, modeling task-centric knowledge space, and TS-Stellar, capturing hierarchical task-skill structure. Stellar VLA enables self-supervised knowledge evolution through joint learning of task latent representation and the knowledge space, reducing annotation needs. Knowledge-guided expert routing provide task specialization without extra network parameters, lowering training overhead.Experiments on the LIBERO benchmark and real-world tasks show over 50 percentage average improvement in final success rates relative to baselines. TS-Stellar further excels in complex action inference, and in-depth analyses verify effective knowledge retention and discovery. Our code will be released soon.

</details>


### [17] [Anti-Jamming based on Null-Steering Antennas and Intelligent UAV Swarm Behavior](https://arxiv.org/abs/2511.18086)
*Miguel Lourenço,António Grilo*

Main category: cs.RO

TL;DR: 提出了一种结合遗传算法、监督学习和强化学习的统一优化框架，使无人机群能够在干扰环境下保持通信和任务效率，通过零转向天线和智能算法有效缓解干扰。


<details>
  <summary>Details</summary>
Motivation: 无人机群依赖无线通信，容易受到干扰攻击，这会破坏协调和任务成功。本研究旨在探索无人机群是否能在干扰环境下有效保持通信和任务效率。

Method: 提出了结合遗传算法、监督学习和强化学习的统一优化框架，采用分时段的任务模型进行动态路径规划、天线定向和群组编队，使用零转向天线将天线零点指向干扰源增强抗干扰能力。

Result: 遗传算法实现了稳定无碰撞轨迹但计算成本高；监督学习模型能复制遗传算法配置但在动态或受限环境下泛化能力不足；强化学习（PPO）表现出适应性和实时决策能力，通信稳定且计算需求低。自适应运动模型通过旋转机制将无人机运动推广到任意方向。

Conclusion: 配备零转向天线并由智能优化算法引导的无人机群能有效缓解干扰，同时保持通信稳定性、编队凝聚力和碰撞安全性。该框架为弹性群组通信系统的未来研究建立了统一、灵活和可复现的基础。

Abstract: Unmanned Aerial Vehicle (UAV) swarms represent a key advancement in autonomous systems, enabling coordinated missions through inter-UAV communication. However, their reliance on wireless links makes them vulnerable to jamming, which can disrupt coordination and mission success. This work investigates whether a UAV swarm can effectively overcome jamming while maintaining communication and mission efficiency.
  To address this, a unified optimization framework combining Genetic Algorithms (GA), Supervised Learning (SL), and Reinforcement Learning (RL) is proposed. The mission model, structured into epochs and timeslots, allows dynamic path planning, antenna orientation, and swarm formation while progressively enforcing collision rules. Null-steering antennas enhance resilience by directing antenna nulls toward interference sources.
  Results show that the GA achieved stable, collision-free trajectories but with high computational cost. SL models replicated GA-based configurations but struggled to generalize under dynamic or constrained settings. RL, trained via Proximal Policy Optimization (PPO), demonstrated adaptability and real-time decision-making with consistent communication and lower computational demand. Additionally, the Adaptive Movement Model generalized UAV motion to arbitrary directions through a rotation-based mechanism, validating the scalability of the proposed system.
  Overall, UAV swarms equipped with null-steering antennas and guided by intelligent optimization algorithms effectively mitigate jamming while maintaining communication stability, formation cohesion, and collision safety. The proposed framework establishes a unified, flexible, and reproducible basis for future research on resilient swarm communication systems.

</details>


### [18] [A Unified Multi-Dynamics Framework for Perception-Oriented Modeling in Tendon-Driven Continuum Robots](https://arxiv.org/abs/2511.18088)
*Ibrahim Alsarraj,Yuhao Wang,Abdalla Swikir,Cesare Stefanini,Dezhen Song,Zhanchi Wang,Ke Wu*

Main category: cs.RO

TL;DR: 提出了一个统一的多动力学建模框架，用于肌腱驱动连续体机器人系统，通过整合电机电气动力学、电机-卷轴动力学和连续体机器人动力学，实现基于内在动力学的感知能力。


<details>
  <summary>Details</summary>
Motivation: 肌腱驱动连续体机器人具有运动冗余和结构柔顺性，能提供本质安全和丰富的接触交互，但其感知通常依赖外部传感器，增加了硬件复杂性并限制了可扩展性。

Method: 开发了统一的多动力学建模框架，整合电机电气动力学、电机-卷轴动力学和连续体机器人动力学，通过电机信号（电流和角位移）揭示外部交互的机电特征。

Result: 模型成功捕获并验证了真实系统的关键物理行为，包括驱动迟滞和运动极限的自接触。在环境交互应用中，实现了被动接触检测、主动接触感知和物体尺寸估计。

Conclusion: 该框架为肌腱驱动连续体机器人提供了一种基于物理的方法，能够从内在电机信号中解释交互特征，减少对外部传感器的依赖。

Abstract: Tendon-driven continuum robots offer intrinsically safe and contact-rich interactions owing to their kinematic redundancy and structural compliance. However, their perception often depends on external sensors, which increase hardware complexity and limit scalability. This work introduces a unified multi-dynamics modeling framework for tendon-driven continuum robotic systems, exemplified by a spiral-inspired robot named Spirob. The framework integrates motor electrical dynamics, motor-winch dynamics, and continuum robot dynamics into a coherent system model. Within this framework, motor signals such as current and angular displacement are modeled to expose the electromechanical signatures of external interactions, enabling perception grounded in intrinsic dynamics. The model captures and validates key physical behaviors of the real system, including actuation hysteresis and self-contact at motion limits. Building on this foundation, the framework is applied to environmental interaction: first for passive contact detection, verified experimentally against simulation data; then for active contact sensing, where control and perception strategies from simulation are successfully applied to the real robot; and finally for object size estimation, where a policy learned in simulation is directly deployed on hardware. The results demonstrate that the proposed framework provides a physically grounded way to interpret interaction signatures from intrinsic motor signals in tendon-driven continuum robots.

</details>


### [19] [EchoVLA: Robotic Vision-Language-Action Model with Synergistic Declarative Memory for Mobile Manipulation](https://arxiv.org/abs/2511.18112)
*Min Lin,Xiwen Liang,Bingqian Lin,Liu Jingzhi,Zijian Jiao,Kehan Li,Yuhan Ma,Yuecheng Liu,Shen Zhao,Yuzheng Zhuang,Xiaodan Liang*

Main category: cs.RO

TL;DR: EchoVLA是一个具有记忆能力的视觉-语言-动作模型，专门用于长视野移动操作任务，通过场景记忆和情景记忆的协同工作来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型主要局限于短视野的桌面操作，缺乏处理长视野移动操作所需的记忆和推理能力，特别是在导航和操作协调方面。

Method: EchoVLA引入了受人类大脑启发的声明性记忆系统，包括场景记忆（维护空间-语义地图）和情景记忆（存储任务级经验）。通过粗粒度和细粒度注意力融合记忆表示来指导移动臂扩散策略。

Result: 在模拟和真实环境实验中，EchoVLA在操作/导航任务上达到0.52成功率，在移动操作任务上达到0.31成功率，相比基线模型分别提升了0.08和0.11。

Conclusion: EchoVLA通过记忆增强机制有效提升了长视野移动操作的性能，证明了记忆系统在复杂机器人任务中的重要性。

Abstract: Recent progress in Vision-Language-Action (VLA) models has enabled embodied agents to interpret multimodal instructions and perform complex tasks. However, existing VLAs are mostly confined to short-horizon, table-top manipulation, lacking the memory and reasoning capability required for long-horizon mobile manipulation, where agents must coordinate navigation and manipulation under changing spatial contexts. In this work, we present EchoVLA, a memory-aware VLA model for long-horizon mobile manipulation. EchoVLA incorporates a synergistic declarative memory inspired by the human brain, consisting of a scene memory that maintains a collection of spatial-semantic maps and an episodic memory that stores task-level experiences with multimodal contextual features. During both training and inference, the two memories are individually stored, updated, and retrieved based on current observations, task history, and instructions, and their retrieved representations are fused via coarse- and fine-grained attention to guide mobile-arm diffusion policies. To support large-scale training and evaluation, we further introduce MoMani, an automated benchmark that generates expert-level long-horizon trajectories through multimodal large language model (MLLM)-guided planning and feedback-driven refinement, supplemented with real-robot demonstrations. Experiments in simulated and real-world settings show that EchoVLA improves long-horizon performance, reaching 0.52 SR on manipulation/navigation and 0.31 on mobile manipulation, exceeding $π_{0.5}$ by +0.08 and +0.11.

</details>


### [20] [Observer Actor: Active Vision Imitation Learning with Sparse View Gaussian Splatting](https://arxiv.org/abs/2511.18140)
*Yilong Wang,Cheng Qian,Ruomeng Fan,Edward Johns*

Main category: cs.RO

TL;DR: ObAct是一个主动视觉模仿学习框架，通过动态分配观察者和执行者角色，让观察者移动到最佳视觉观察位置，从而提升策略执行效果。


<details>
  <summary>Details</summary>
Motivation: 解决静态摄像头在机器人操作任务中因遮挡和视角问题导致的观察质量下降，影响策略性能的问题。

Method: 使用双臂机器人系统，观察者臂构建3D高斯溅射表示，虚拟探索找到最优相机位姿后移动到该位置，执行者臂使用观察者的观察执行策略。

Result: 相比静态摄像头设置，轨迹转移性能提升145%（无遮挡）和233%（有遮挡），行为克隆提升75%和143%。

Conclusion: ObAct框架通过主动视觉观察显著提升了模仿学习的鲁棒性和性能，使策略在更接近无遮挡训练分布的条件下执行。

Abstract: We propose Observer Actor (ObAct), a novel framework for active vision imitation learning in which the observer moves to optimal visual observations for the actor. We study ObAct on a dual-arm robotic system equipped with wrist-mounted cameras. At test time, ObAct dynamically assigns observer and actor roles: the observer arm constructs a 3D Gaussian Splatting (3DGS) representation from three images, virtually explores this to find an optimal camera pose, then moves to this pose; the actor arm then executes a policy using the observer's observations. This formulation enhances the clarity and visibility of both the object and the gripper in the policy's observations. As a result, we enable the training of ambidextrous policies on observations that remain closer to the occlusion-free training distribution, leading to more robust policies. We study this formulation with two existing imitation learning methods -- trajectory transfer and behavior cloning -- and experiments show that ObAct significantly outperforms static-camera setups: trajectory transfer improves by 145% without occlusion and 233% with occlusion, while behavior cloning improves by 75% and 143%, respectively. Videos are available at https://obact.github.io.

</details>


### [21] [A Coordinated Dual-Arm Framework for Delicate Snap-Fit Assemblies](https://arxiv.org/abs/2511.18153)
*Shreyas Kumar,Barat S,Debojit Das,Yug Desai,Siddhi Jain,Rajesh Kumar,Harish J. Palanthandalam-Madapusi*

Main category: cs.RO

TL;DR: 提出SnapNet轻量神经网络从关节速度瞬变中实时检测卡扣装配啮合，结合基于动力学的双臂协调框架实现精确对齐和柔性插入，在异构双手平台上验证了超过96%的召回率和30%的峰值冲击力降低。


<details>
  <summary>Details</summary>
Motivation: 精密卡扣装配（如将镜片插入眼镜框或电子元件组装）需要及时检测啮合并快速衰减力，以防止过冲导致的组件损坏或装配失败。

Method: 1) SnapNet轻量神经网络从关节速度瞬变实时检测卡扣啮合；2) 基于动力学的双臂协调框架，将SnapNet检测与事件触发的阻抗调制相结合，实现精确对齐和柔性插入。

Result: 在异构双手平台上的多样化几何形状实验中，显示出高检测精度（召回率超过96%），与标准阻抗控制相比峰值冲击力降低达30%。

Conclusion: 该方法仅使用本体感觉信号即可实现可靠的卡扣装配啮合检测，无需外部传感器，为精密装配任务提供了有效的解决方案。

Abstract: Delicate snap-fit assemblies, such as inserting a lens into an eye-wear frame or during electronics assembly, demand timely engagement detection and rapid force attenuation to prevent overshoot-induced component damage or assembly failure. We address these challenges with two key contributions. First, we introduce SnapNet, a lightweight neural network that detects snap-fit engagement from joint-velocity transients in real-time, showing that reliable detection can be achieved using proprioceptive signals without external sensors. Second, we present a dynamical-systems-based dual-arm coordination framework that integrates SnapNet driven detection with an event-triggered impedance modulation, enabling accurate alignment and compliant insertion during delicate snap-fit assemblies. Experiments across diverse geometries on a heterogeneous bimanual platform demonstrate high detection accuracy (over 96% recall) and up to a 30% reduction in peak impact forces compared to standard impedance control.

</details>


### [22] [Time-aware Motion Planning in Dynamic Environments with Conformal Prediction](https://arxiv.org/abs/2511.18170)
*Kaier Liang,Licheng Luo,Yixuan Wang,Mingyu Cai,Cristian Ioan Vasile*

Main category: cs.RO

TL;DR: 提出了两个利用保形预测的运动规划框架：全局规划器集成安全间隔路径规划进行不确定性感知轨迹生成，局部规划器执行在线反应式规划。通过自适应保形预测和自适应分位数机制增强轨迹可行性。


<details>
  <summary>Details</summary>
Motivation: 动态环境中安全导航面临障碍物行为不确定性和缺乏形式化预测保证的挑战。

Method: 全局规划器：SIPP+CP提供分布无关安全保证；局部规划器：自适应CP减少预测误差；引入自适应分位数机制自动调整置信水平以保持轨迹可行性。

Result: 在动态拥挤环境中进行数值实验验证，框架能够适应更高不确定性区域并收紧安全边界。

Conclusion: 提出的框架通过保形预测和自适应机制实现了动态环境中鲁棒且响应迅速的运动规划。

Abstract: Safe navigation in dynamic environments remains challenging due to uncertain obstacle behaviors and the lack of formal prediction guarantees. We propose two motion planning frameworks that leverage conformal prediction (CP): a global planner that integrates Safe Interval Path Planning (SIPP) for uncertainty-aware trajectory generation, and a local planner that performs online reactive planning. The global planner offers distribution-free safety guarantees for long-horizon navigation, while the local planner mitigates inaccuracies in obstacle trajectory predictions through adaptive CP, enabling robust and responsive motion in dynamic environments. To further enhance trajectory feasibility, we introduce an adaptive quantile mechanism in the CP-based uncertainty quantification. Instead of using a fixed confidence level, the quantile is automatically tuned to the optimal value that preserves trajectory feasibility, allowing the planner to adaptively tighten safety margins in regions with higher uncertainty. We validate the proposed framework through numerical experiments conducted in dynamic and cluttered environments. The project page is available at https://time-aware-planning.github.io

</details>


### [23] [Off-Road Navigation via Implicit Neural Representation of Terrain Traversability](https://arxiv.org/abs/2511.18183)
*Yixuan Jia,Qingyuan Li,Jonathan P. How*

Main category: cs.RO

TL;DR: TRAIL是一个越野导航框架，使用隐式神经表示来参数化地形属性，并通过梯度优化方法同时调整路径几何和速度分布。


<details>
  <summary>Details</summary>
Motivation: 传统基于采样的规划器只能优化短期控制动作，无法考虑完整路径几何，且不能根据地形颠簸度调整速度，限制了在挑战性越野环境中的导航能力。

Method: 利用隐式神经表示连续参数化地形属性，生成空间梯度，与新颖的基于梯度的轨迹优化方法集成，基于地形可通行性调整路径几何和速度分布。

Result: 该方法能够同时优化路径几何和速度分布，适应地形可通行性。

Conclusion: TRAIL框架通过隐式神经表示和梯度优化，解决了传统方法在路径几何推理和速度适应方面的局限性。

Abstract: Autonomous off-road navigation requires robots to estimate terrain traversability from onboard sensors and plan accordingly. Conventional approaches typically rely on sampling-based planners such as MPPI to generate short-term control actions that aim to minimize traversal time and risk measures derived from the traversability estimates. These planners can react quickly but optimize only over a short look-ahead window, limiting their ability to reason about the full path geometry, which is important for navigating in challenging off-road environments. Moreover, they lack the ability to adjust speed based on the terrain bumpiness, which is important for smooth navigation on challenging terrains. In this paper, we introduce TRAIL (Traversability with an Implicit Learned Representation), an off-road navigation framework that leverages an implicit neural representation to continuously parameterize terrain properties. This representation yields spatial gradients that enable integration with a novel gradient-based trajectory optimization method that adapts the path geometry and speed profile based on terrain traversability.

</details>


### [24] [SkillWrapper: Generative Predicate Invention for Skill Abstraction](https://arxiv.org/abs/2511.18203)
*Ziyi Yang,Benned Hedegaard,Ahmed Jaafar,Yichen Wei,Skye Thompson,Shreyas S. Raman,Haotian Fu,Stefanie Tellex,George Konidaris,David Paulius,Naman Shah*

Main category: cs.RO

TL;DR: 提出了SkillWrapper方法，利用基础模型主动收集机器人数据，学习黑盒技能的可规划抽象表示，从而解决未见过的长时程任务。


<details>
  <summary>Details</summary>
Motivation: 从个体技能执行泛化到解决长时程任务是构建自主智能体的核心挑战。需要学习高层符号抽象来表示低层技能，以便独立于低层状态空间进行推理和规划。

Method: 提出SkillWrapper方法，利用基础模型主动收集机器人数据，仅使用RGB图像观察来学习人类可解释、可规划的黑盒技能表示。

Result: 在仿真和真实机器人上的广泛实验表明，SkillWrapper学习的抽象表示能够使用黑盒技能解决真实世界中未见过的长时程任务。

Conclusion: 提出了生成谓词发明的形式化理论，确保学习到的符号操作符可用于可证明正确和完备的规划，为技能抽象提供了理论基础。

Abstract: Generalizing from individual skill executions to solving long-horizon tasks remains a core challenge in building autonomous agents. A promising direction is learning high-level, symbolic abstractions of the low-level skills of the agents, enabling reasoning and planning independent of the low-level state space. Among possible high-level representations, object-centric skill abstraction with symbolic predicates has been proven to be efficient because of its compatibility with domain-independent planners. Recent advances in foundation models have made it possible to generate symbolic predicates that operate on raw sensory inputs, a process we call generative predicate invention, to facilitate downstream abstraction learning. However, it remains unclear which formal properties the learned representations must satisfy, and how they can be learned to guarantee these properties. In this paper, we address both questions by presenting a formal theory of generative predicate invention for skill abstraction, resulting in symbolic operators that can be used for provably sound and complete planning. Within this framework, we propose SkillWrapper, a method that leverages foundation models to actively collect robot data and learn human-interpretable, plannable representations of black-box skills, using only RGB image observations. Our extensive empirical evaluation in simulation and on real robots shows that SkillWrapper learns abstract representations that enable solving unseen, long-horizon tasks in the real world with black-box skills.

</details>


### [25] [AFT: Appearance-Based Feature Tracking for Markerless and Training-Free Shape Reconstruction of Soft Robots](https://arxiv.org/abs/2511.18215)
*Shangyuan Yuan,Preston Fairchild,Yu Mei,Xinyu Zhou,Xiaobo Tan*

Main category: cs.RO

TL;DR: 提出了一种基于视觉、无标记、无需训练的方法，利用软机器人自然表面特征进行实时形状重建，在动态环境中实现可靠部署


<details>
  <summary>Details</summary>
Motivation: 现有视觉方法依赖复杂相机设置、特定背景或大规模训练数据，限制了在实际场景中的实用性

Method: 利用机器人自然表面特征作为隐式视觉标记，采用分层匹配策略，将局部分区对齐与全局运动学优化解耦

Result: 在连续软机器人上验证，实时操作时平均尖端误差为2.6%，在闭环控制任务中表现稳定

Conclusion: 该方法在动态现实环境中具有可靠、低成本部署的潜力

Abstract: Accurate shape reconstruction is essential for precise control and reliable operation of soft robots. Compared to sensor-based approaches, vision-based methods offer advantages in cost, simplicity, and ease of deployment. However, existing vision-based methods often rely on complex camera setups, specific backgrounds, or large-scale training datasets, limiting their practicality in real-world scenarios. In this work, we propose a vision-based, markerless, and training-free framework for soft robot shape reconstruction that directly leverages the robot's natural surface appearance. These surface features act as implicit visual markers, enabling a hierarchical matching strategy that decouples local partition alignment from global kinematic optimization. Requiring only an initial 3D reconstruction and kinematic alignment, our method achieves real-time shape tracking across diverse environments while maintaining robustness to occlusions and variations in camera viewpoints. Experimental validation on a continuum soft robot demonstrates an average tip error of 2.6% during real-time operation, as well as stable performance in practical closed-loop control tasks. These results highlight the potential of the proposed approach for reliable, low-cost deployment in dynamic real-world settings.

</details>


### [26] [APULSE: A Scalable Hybrid Algorithm for the RCSPP on Large-Scale Dense Graphs](https://arxiv.org/abs/2511.18236)
*Nuno Soares,António Grilo*

Main category: cs.RO

TL;DR: APULSE是一种混合标签设置算法，用于高效解决资源受限最短路径问题(RCSPP)，在大规模密集图上表现出色，比现有方法快几个数量级


<details>
  <summary>Details</summary>
Motivation: 现有RCSPP求解器在复杂现实场景的大规模密集图上存在可扩展性限制，无法满足时间关键型规划需求，特别是在无人地面车辆任务规划等领域

Method: APULSE结合了A*启发式引导的最佳优先搜索、脉冲式剪枝机制和时间分桶策略，实现有效的状态空间缩减

Result: 在大规模UGV规划场景的基准测试中，APULSE始终找到接近最优解，速度比竞争方法快几个数量级，在大型问题实例上更稳健

Conclusion: APULSE在复杂大规模环境中为RCSPP提供了有效解决方案，支持交互式决策支持和动态重规划能力

Abstract: The resource-constrained shortest path problem (RCSPP) is a fundamental NP-hard optimization challenge with broad applications, from network routing to autonomous navigation. This problem involves finding a path that minimizes a primary cost subject to a budget on a secondary resource. While various RCSPP solvers exist, they often face critical scalability limitations when applied to the large, dense graphs characteristic of complex, real-world scenarios, making them impractical for time-critical planning. This challenge is particularly acute in domains like mission planning for unmanned ground vehicles (UGVs), which demand solutions on large-scale terrain graphs. This paper introduces APULSE, a hybrid label-setting algorithm designed to efficiently solve the RCSPP on such challenging graphs. APULSE integrates a best-first search guided by an A* heuristic with aggressive, Pulse-style pruning mechanisms and a time-bucketing strategy for effective state-space reduction. A computational study, using a large-scale UGV planning scenario, benchmarks APULSE against state-of-the-art algorithms. The results demonstrate that APULSE consistently finds near-optimal solutions while being orders of magnitude faster and more robust, particularly on large problem instances where competing methods fail. This superior scalability establishes APULSE as an effective solution for RCSPP in complex, large-scale environments, enabling capabilities such as interactive decision support and dynamic replanning.

</details>


### [27] [Dreaming Falcon: Physics-Informed Model-Based Reinforcement Learning for Quadcopters](https://arxiv.org/abs/2511.18243)
*Eashan Vytla,Bhavanishankar Kalavakolanu,Andrew Perrault,Matthew McCrink*

Main category: cs.RO

TL;DR: 论文探讨了将物理知识融入世界模型学习的方法，以改进无人机控制策略性能，但发现标准RNN模型和物理启发模型在泛化到新轨迹时都会出现状态发散问题。


<details>
  <summary>Details</summary>
Motivation: 当前无人机控制算法在动态环境和恶劣条件下缺乏鲁棒性，基于模型的强化学习虽然样本效率高，但Dreamer等方法在无人机系统上应用困难，主要由于样本效率低和动力学模型泛化能力差。

Method: 提出物理启发方法构建世界模型，将四旋翼视为自由体系统，预测作用在其上的净力和力矩，然后通过6自由度龙格-库塔积分器预测未来状态展开。将此方法与标准RNN世界模型进行对比。

Result: 两种模型在训练数据上都表现良好，但都无法泛化到新轨迹，导致状态展开迅速发散，阻碍了策略收敛。

Conclusion: 虽然物理启发方法在理论上具有优势，但在实际应用中与标准RNN模型一样面临泛化挑战，需要进一步改进模型泛化能力。

Abstract: Current control algorithms for aerial robots struggle with robustness in dynamic environments and adverse conditions. Model-based reinforcement learning (RL) has shown strong potential in handling these challenges while remaining sample-efficient. Additionally, Dreamer has demonstrated that online model-based RL can be achieved using a recurrent world model trained on replay buffer data. However, applying Dreamer to aerial systems has been quite challenging due to its sample inefficiency and poor generalization of dynamics models. Our work explores a physics-informed approach to world model learning and improves policy performance. The world model treats the quadcopter as a free-body system and predicts the net forces and moments acting on it, which are then passed through a 6-DOF Runge-Kutta integrator (RK4) to predict future state rollouts. In this paper, we compare this physics-informed method to a standard RNN-based world model. Although both models perform well on the training data, we observed that they fail to generalize to new trajectories, leading to rapid divergence in state rollouts, preventing policy convergence.

</details>


### [28] [Skypilot: Fine-Tuning LLM with Physical Grounding for AAV Coverage Search](https://arxiv.org/abs/2511.18270)
*Zhongkai Chen,Yihao Sun,Chao Yan,Han Zhou,Xiaojia Xiang,Jie Jiang*

Main category: cs.RO

TL;DR: Skypilot是一个基于大语言模型的无人机智能决策框架，通过集成蒙特卡洛树搜索实现物理现实接地，解决LLM在空间推理中的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在无人机自主决策中缺乏物理接地导致的幻觉和可复现性问题，提升无人机在覆盖操作和搜索任务中的智能决策能力

Method: 提出两阶段框架：第一阶段构建多样化动作空间和物理信息奖励函数；第二阶段在MCTS生成的23000个样本上微调Qwen3-4B模型，实现推理加速

Result: 通过数值模拟和真实飞行实验验证了方法的效率和优越性，在保持解质量的同时实现了显著的推理加速

Conclusion: Skypilot框架成功地将语言模型与物理现实接地，为无人机智能决策提供了可靠高效的解决方案

Abstract: Autonomous aerial vehicles (AAVs) have played a pivotal role in coverage operations and search missions. Recent advances in large language models (LLMs) offer promising opportunities to augment AAV intelligence. These advances help address complex challenges like area coverage optimization, dynamic path planning, and adaptive decision-making. However, the absence of physical grounding in LLMs leads to hallucination and reproducibility problems in spatial reasoning and decision-making. To tackle these issues, we present Skypilot, an LLM-enhanced two-stage framework that grounds language models in physical reality by integrating monte carlo tree search (MCTS). In the first stage, we introduce a diversified action space that encompasses generate, regenerate, fine-tune, and evaluate operations, coupled with physics-informed reward functions to ensure trajectory feasibility. In the second stage, we fine-tune Qwen3-4B on 23,000 MCTS-generated samples, achieving substantial inference acceleration while maintaining solution quality. Extensive numerical simulations and real-world flight experiments validate the efficiency and superiority of our proposed approach. Detailed information and experimental results are accessible at https://sky-pilot.top.

</details>


### [29] [AIA-UltraNeRF:Acoustic-Impedance-Aware Neural Radiance Field with Hash Encodings for Robotic Ultrasound Reconstruction and Localization](https://arxiv.org/abs/2511.18293)
*Shuai Zhang,Jingsong Mu,Cancan Zhao,Leiqi Tian,Zhijun Xing,Bo Ouyang,Xiang Li*

Main category: cs.RO

TL;DR: 提出了一种基于声阻抗感知的超声NeRF（AIA-UltraNeRF）系统，通过哈希编码空间坐标来建模3D超声图，实现扫描与诊断过程的解耦，并加速重建和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF方法忽略了声阻抗在超声成像中的关键作用，且定位方法因初始姿态选择而面临局部极小值问题。

Method: 设计机器人超声系统（RUSS），采用AIA-UltraNeRF建模声阻抗的连续函数，利用双监督网络通过师生模型哈希编码渲染图像，无需重新渲染即可检索相似哈希值提供初始定位位置。

Result: 在体模和人体实验上验证了声阻抗在隐式表征超声图像颜色方面的有效性，AIA-UltraNeRF的重建和定位推理速度比原始NeRF快9.9倍。

Conclusion: AIA-UltraNeRF系统成功实现了超声图像的重建和定位，显著提升了处理效率，为超声诊断工作流程提供了有效解决方案。

Abstract: Neural radiance field (NeRF) is a promising approach for reconstruction and new view synthesis. However, previous NeRF-based reconstruction methods overlook the critical role of acoustic impedance in ultrasound imaging. Localization methods face challenges related to local minima due to the selection of initial poses. In this study, we design a robotic ultrasound system (RUSS) with an acoustic-impedance-aware ultrasound NeRF (AIA-UltraNeRF) to decouple the scanning and diagnostic processes. Specifically, AIA-UltraNeRF models a continuous function of hash-encoded spatial coordinates for the 3D ultrasound map, allowing for the storage of acoustic impedance without dense sampling. This approach accelerates both reconstruction and inference speeds. We then propose a dual-supervised network that leverages teacher and student models to hash-encode the rendered ultrasound images from the reconstructed map. AIA-UltraNeRF retrieves the most similar hash values without the need to render images again, providing an offline initial image position for localization. Moreover, we develop a RUSS with a spherical remote center of motion mechanism to hold the probe, implementing operator-independent scanning modes that separate image acquisition from diagnostic workflows. Experimental results on a phantom and human subjects demonstrate the effectiveness of acoustic impedance in implicitly characterizing the color of ultrasound images. AIAUltraNeRF achieves both reconstruction and localization with inference speeds that are 9.9 faster than those of vanilla NeRF.

</details>


### [30] [MicCheck: Repurposing Off-the-Shelf Pin Microphones for Easy and Low-Cost Contact Sensing](https://arxiv.org/abs/2511.18299)
*Steven Oh,Tai Inui,Magdeline Kuan,Jia-Yeu Lin*

Main category: cs.RO

TL;DR: MicCheck是一种低成本、即插即用的声学传感方法，利用现成的蓝牙针式麦克风作为接触传感器，通过3D打印的夹爪插入件实现，无需定制电子设备或驱动程序。


<details>
  <summary>Details</summary>
Motivation: 机器人操作任务需要丰富的接触信息，但大多数模仿学习方法主要依赖视觉，难以捕捉刚度、粗糙度、滑动等精细交互线索。现有触觉传感器通常昂贵、易损或集成复杂。

Method: 将现成的蓝牙针式麦克风作为接触传感器，通过3D打印的夹爪插入件固定，使用标准USB接收器传输音频信号。在材料分类和操作任务中验证其感知和控制能力。

Result: 在10类材料的4种交互类型分类中达到92.9%准确率；在拾取和倾倒任务中，将成功率从0.40提升到0.80；能够可靠执行拔插和基于声音的排序等接触密集型技能。

Conclusion: 针式麦克风在高分辨率触觉传感器的空间细节与成本和集成便利性之间取得平衡，为低成本机器人部署声学接触传感提供了实用途径。

Abstract: Robotic manipulation tasks are contact-rich, yet most imitation learning (IL) approaches rely primarily on vision, which struggles to capture stiffness, roughness, slip, and other fine interaction cues. Tactile signals can address this gap, but existing sensors often require expensive, delicate, or integration-heavy hardware. In this work, we introduce MicCheck, a plug-and-play acoustic sensing approach that repurposes an off-the-shelf Bluetooth pin microphone as a low-cost contact sensor. The microphone clips into a 3D-printed gripper insert and streams audio via a standard USB receiver, requiring no custom electronics or drivers. Despite its simplicity, the microphone provides signals informative enough for both perception and control. In material classification, it achieves 92.9% accuracy on a 10-class benchmark across four interaction types (tap, knock, slow press, drag). For manipulation, integrating pin microphone into an IL pipeline with open source hardware improves the success rate on picking and pouring task from 0.40 to 0.80 and enables reliable execution of contact-rich skills such as unplugging and sound-based sorting. Compared with high-resolution tactile sensors, pin microphones trade spatial detail for cost and ease of integration, offering a practical pathway for deploying acoustic contact sensing in low-cost robot setups.

</details>


### [31] [Learning Visually Interpretable Oscillator Networks for Soft Continuum Robots from Video](https://arxiv.org/abs/2511.18322)
*Henrik Krauss,Johann Licher,Naoya Takeishi,Annika Raatz,Takehisa Yairi*

Main category: cs.RO

TL;DR: 提出了Attention Broadcast Decoder (ABCD)模块，将高维观测数据转化为可解释的物理模型，显著提高了软体连续机器人的多步预测精度。


<details>
  <summary>Details</summary>
Motivation: 数据驱动方法缺乏物理可解释性，而基于模型的方法需要先验知识且计算昂贵，需要在两者之间建立桥梁。

Method: 使用ABCD模块生成像素级注意力图定位每个潜在维度的贡献，并将注意力图与2D振荡器网络耦合，实现学习动力学的直接可视化。

Result: 在单段和双段软体连续机器人上验证，ABCD模型显著提高多步预测精度：Koopman算子误差减少5.7倍，振荡器网络误差减少3.5倍。

Conclusion: 该方法实现了完全数据驱动的紧凑、物理可解释模型，适合控制应用，并能平滑地外推到训练数据之外。

Abstract: Data-driven learning of soft continuum robot (SCR) dynamics from high-dimensional observations offers flexibility but often lacks physical interpretability, while model-based approaches require prior knowledge and can be computationally expensive. We bridge this gap by introducing (1) the Attention Broadcast Decoder (ABCD), a plug-and-play module for autoencoder-based latent dynamics learning that generates pixel-accurate attention maps localizing each latent dimension's contribution while filtering static backgrounds. (2) By coupling these attention maps to 2D oscillator networks, we enable direct on-image visualization of learned dynamics (masses, stiffness, and forces) without prior knowledge. We validate our approach on single- and double-segment SCRs, demonstrating that ABCD-based models significantly improve multi-step prediction accuracy: 5.7x error reduction for Koopman operators and 3.5x for oscillator networks on the two-segment robot. The learned oscillator network autonomously discovers a chain structure of oscillators. Unlike standard methods, ABCD models enable smooth latent space extrapolation beyond training data. This fully data-driven approach yields compact, physically interpretable models suitable for control applications.

</details>


### [32] [Enhancing UAV Search under Occlusion using Next Best View Planning](https://arxiv.org/abs/2511.18353)
*Sigrid Helene Strand,Thomas Wiedemann,Bram Burczek,Dmitriy Shutin*

Main category: cs.RO

TL;DR: 本文提出了一种针对遮挡环境中无人机搜索救援任务的最优规划策略和高效算法，通过几何启发式和可见性启发式两种优化方法选择最佳相机视角，其中可见性启发式在模拟和真实环境中都表现出更优性能。


<details>
  <summary>Details</summary>
Motivation: 在密集森林等遮挡环境中进行搜索救援任务时，无人机需要有效的搜索策略来优化相机定位和视角，以捕捉清晰的地面视图，从而提高搜索效果。

Method: 提出了两种新颖的优化启发式方法：几何启发式和可见性启发式，用于选择最优相机视点，解决遮挡环境中的下一个最佳视角问题。

Result: 可见性启发式在模拟森林中识别出超过90%的隐藏物体，比几何启发式的检测率高10%。真实环境实验显示可见性启发式在树冠下提供更好的覆盖范围。

Conclusion: 可见性启发式在遮挡环境中具有更好的搜索性能，有潜力显著改善搜索救援任务的效果。

Abstract: Search and rescue missions are often critical following sudden natural disasters or in high-risk environmental situations. The most challenging search and rescue missions involve difficult-to-access terrains, such as dense forests with high occlusion. Deploying unmanned aerial vehicles for exploration can significantly enhance search effectiveness, facilitate access to challenging environments, and reduce search time. However, in dense forests, the effectiveness of unmanned aerial vehicles depends on their ability to capture clear views of the ground, necessitating a robust search strategy to optimize camera positioning and perspective. This work presents an optimized planning strategy and an efficient algorithm for the next best view problem in occluded environments. Two novel optimization heuristics, a geometry heuristic, and a visibility heuristic, are proposed to enhance search performance by selecting optimal camera viewpoints. Comparative evaluations in both simulated and real-world settings reveal that the visibility heuristic achieves greater performance, identifying over 90% of hidden objects in simulated forests and offering 10% better detection rates than the geometry heuristic. Additionally, real-world experiments demonstrate that the visibility heuristic provides better coverage under the canopy, highlighting its potential for improving search and rescue missions in occluded environments.

</details>


### [33] [Explicit Bounds on the Hausdorff Distance for Truncated mRPI Sets via Norm-Dependent Contraction Rates](https://arxiv.org/abs/2511.18374)
*Jiaxun Sun*

Main category: cs.RO

TL;DR: 本文首次建立了截断最小鲁棒正不变集与其无限时域极限之间Hausdorff距离的显式闭式上界，提供了可计算的截断误差量化表达式。


<details>
  <summary>Details</summary>
Motivation: 现有mRPI近似方法通过几何或范数论证保证渐近收敛，但无法为给定时域提供可计算的截断误差量化表达式。

Method: 推导出误差满足d_H(ℰ_N,ℰ_∞) ≤ r_W·γ^(N+1)/(1-γ)，其中γ<1是诱导范数收缩因子，r_W仅依赖于扰动集。该边界完全解析，无需迭代集计算。

Result: 证明了向量范数选择可作为设计参数加速收敛，显著收紧鲁棒不变集计算和基于管的MPC的时域选择。数值实验验证了所提边界的锐度、可扩展性和实际相关性。

Conclusion: 本文提供了首个可计算的mRPI截断误差上界，为鲁棒控制中的不变集计算提供了实用的理论工具。

Abstract: This paper establishes the first explicit and closed-form upper bound on the Hausdorff distance between the truncated minimal robust positively invariant (mRPI) set and its infinite-horizon limit. While existing mRPI approximations guarantee asymptotic convergence through geometric or norm-based arguments, none provides a computable expression that quantifies the truncation error for a given horizon. We show that the error satisfies \( d_H(\mathcal{E}_N,\mathcal{E}_\infty) \le r_W\,γ^{N+1}/(1-γ), \) where $γ<1$ is the induced-norm contraction factor and $r_W$ depends only on the disturbance set. The bound is fully analytic, requires no iterative set computations, and directly characterizes the decay rate of the truncated Minkowski series. We further demonstrate that the choice of vector norm serves as a design parameter that accelerates convergence, enabling substantially tighter horizon selection for robust invariant-set computations and tube-based MPC. Numerical experiments validate the sharpness, scalability, and practical relevance of the proposed bound.

</details>


### [34] [Expanding the Workspace of Electromagnetic Navigation Systems Using Dynamic Feedback for Single- and Multi-agent Control](https://arxiv.org/abs/2511.18486)
*Jasan Zughaibi,Denis von Arx,Maurus Derungs,Florian Heemeyer,Luca A. Antonelli,Quentin Boehler,Michael Muehlebach,Bradley J. Nelson*

Main category: cs.RO

TL;DR: 通过系统级控制设计显著扩展电磁导航系统的工作空间，采用运动中心的转矩/力目标、能量最优电流分配等五种方法，将稳定3D倒立摆所需电流从8-14A降至0.1-0.2A，并在临床导向的Navion系统上实现50cm距离的稳定平衡。


<details>
  <summary>Details</summary>
Motivation: 电磁导航系统的有效工作空间常受功率和热限制严重约束，需要寻找方法扩展工作空间以提高磁引导手术工具的操控能力。

Method: 采用五种系统方法：运动中心的转矩/力目标、能量最优电流分配、实时位姿估计、动态反馈和高带宽eMNS组件，通过反馈控制策略替代传统的场中心场对齐方法。

Result: 在OctoMag系统上稳定3D倒立摆的电流需求从8-14A大幅降低至0.1-0.2A，成功实现双倒立摆在共享工作空间内的同时稳定，并在Navion系统上达到50cm距离的稳定平衡。

Conclusion: 反馈控制是实现可扩展、高效且临床相关的磁操作的实际路径，系统级控制设计能显著扩展电磁导航系统的工作空间。

Abstract: Electromagnetic navigation systems (eMNS) enable a number of magnetically guided surgical procedures. A challenge in magnetically manipulating surgical tools is that the effective workspace of an eMNS is often severely constrained by power and thermal limits. We show that system-level control design significantly expands this workspace by reducing the currents needed to achieve a desired motion. We identified five key system approaches that enable this expansion: (i) motion-centric torque/force objectives, (ii) energy-optimal current allocation, (iii) real-time pose estimation, (iv) dynamic feedback, and (v) high-bandwidth eMNS components. As a result, we stabilize a 3D inverted pendulum on an eight-coil OctoMag eMNS with significantly lower currents (0.1-0.2 A vs. 8-14 A), by replacing a field-centric field-alignment strategy with a motion-centric torque/force-based approach. We generalize to multi-agent control by simultaneously stabilizing two inverted pendulums within a shared workspace, exploiting magnetic-field nonlinearity and coil redundancy for independent actuation. A structured analysis compares the electromagnetic workspaces of both paradigms and examines current-allocation strategies that map motion objectives to coil currents. Cross-platform evaluation of the clinically oriented Navion eMNS further demonstrates substantial workspace expansion by maintaining stable balancing at distances up to 50 cm from the coils. The results demonstrate that feedback is a practical path to scalable, efficient, and clinically relevant magnetic manipulation.

</details>


### [35] [SafeFall: Learning Protective Control for Humanoid Robots](https://arxiv.org/abs/2511.18509)
*Ziyu Meng,Tengyu Liu,Le Ma,Yingying Wu,Ran Song,Wei Zhang,Siyuan Huang*

Main category: cs.RO

TL;DR: SafeFall是一个保护人形机器人免受跌倒损坏的框架，通过预测不可避免的跌倒并执行保护性动作来最小化硬件损伤。


<details>
  <summary>Details</summary>
Motivation: 双足行走使人形机器人容易跌倒，导致昂贵的传感器、执行器和结构组件损坏，这是实际部署的关键障碍。

Method: 结合轻量级GRU跌倒预测器和强化学习保护策略，在预测到不可避免跌倒时激活保护动作，使用损伤感知奖励函数训练。

Result: 在Unitree G1人形机器人上验证，峰值接触力减少68.3%，峰值关节扭矩减少78.4%，99.3%的易损组件碰撞被消除。

Conclusion: SafeFall为人形机器人提供了关键的安全网，支持更激进的实验并加速在复杂现实环境中的部署。

Abstract: Bipedal locomotion makes humanoid robots inherently prone to falls, causing catastrophic damage to the expensive sensors, actuators, and structural components of full-scale robots. To address this critical barrier to real-world deployment, we present \method, a framework that learns to predict imminent, unavoidable falls and execute protective maneuvers to minimize hardware damage. SafeFall is designed to operate seamlessly alongside existing nominal controller, ensuring no interference during normal operation. It combines two synergistic components: a lightweight, GRU-based fall predictor that continuously monitors the robot's state, and a reinforcement learning policy for damage mitigation. The protective policy remains dormant until the predictor identifies a fall as unavoidable, at which point it activates to take control and execute a damage-minimizing response. This policy is trained with a novel, damage-aware reward function that incorporates the robot's specific structural vulnerabilities, learning to shield critical components like the head and hands while absorbing energy with more robust parts of its body. Validated on a full-scale Unitree G1 humanoid, SafeFall demonstrated significant performance improvements over unprotected falls. It reduced peak contact forces by 68.3\%, peak joint torques by 78.4\%, and eliminated 99.3\% of collisions with vulnerable components. By enabling humanoids to fail safely, SafeFall provides a crucial safety net that allows for more aggressive experiments and accelerates the deployment of these robots in complex, real-world environments.

</details>


### [36] [Splatblox: Traversability-Aware Gaussian Splatting for Outdoor Robot Navigation](https://arxiv.org/abs/2511.18525)
*Samarth Chopra,Jing Liang,Gershom Seneviratne,Yonghan Lee,Jaehoon Choi,Jianyu An,Stephen Cheng,Dinesh Manocha*

Main category: cs.RO

TL;DR: Splatblox是一个用于室外密集植被环境的实时自主导航系统，通过融合RGB图像和LiDAR点云构建可通行性感知的ESDF地图，在植被丰富场景中比现有方法成功率提高50%以上。


<details>
  <summary>Details</summary>
Motivation: 解决室外环境中密集植被、不规则障碍物和复杂地形带来的导航挑战，需要同时处理几何和语义信息来区分可穿越植被和刚性障碍物。

Method: 使用高斯泼溅技术融合分割的RGB图像和LiDAR点云，构建在线更新的可通行性感知欧几里得符号距离场(ESDF)，结合语义推理和360度几何覆盖。

Result: 在植被丰富场景的现场测试中，相比最先进方法：成功率提高50%以上，冻结事件减少40%，路径缩短5%，目标到达时间加快13%，支持长达100米的远程任务。

Conclusion: Splatblox系统在复杂室外环境中实现了高效的自主导航，通过几何和语义的联合编码显著提升了导航性能，并展示了跨平台的可迁移性。

Abstract: We present Splatblox, a real-time system for autonomous navigation in outdoor environments with dense vegetation, irregular obstacles, and complex terrain. Our method fuses segmented RGB images and LiDAR point clouds using Gaussian Splatting to construct a traversability-aware Euclidean Signed Distance Field (ESDF) that jointly encodes geometry and semantics. Updated online, this field enables semantic reasoning to distinguish traversable vegetation (e.g., tall grass) from rigid obstacles (e.g., trees), while LiDAR ensures 360-degree geometric coverage for extended planning horizons. We validate Splatblox on a quadruped robot and demonstrate transfer to a wheeled platform. In field trials across vegetation-rich scenarios, it outperforms state-of-the-art methods with over 50% higher success rate, 40% fewer freezing incidents, 5% shorter paths, and up to 13% faster time to goal, while supporting long-range missions up to 100 meters. Experiment videos and more details can be found on our project page: https://splatblox.github.io

</details>


### [37] [Object-centric Task Representation and Transfer using Diffused Orientation Fields](https://arxiv.org/abs/2511.18563)
*Cem Bilaloglu,Tobias Löw,Sylvain Calinon*

Main category: cs.RO

TL;DR: 提出使用扩散方向场（DOF）来表示局部参考系，通过建立稀疏关键点对应关系，实现在曲面物体间的技能迁移。


<details>
  <summary>Details</summary>
Motivation: 曲面物体缺乏全局参考系，使得任务相关方向（如"朝向"或"沿着"表面）随位置和几何形状变化，难以在不同形状的物体间迁移技能。

Method: 使用扩散方向场（DOF）作为局部参考系的平滑表示，通过基于偏微分方程的扩散过程从原始点云数据在线计算DOF，并以关键点为条件。

Result: 在几何、拓扑和定位扰动下评估DOF，成功实现了需要连续物理交互的任务（如检查、切割和剥皮）在不同物体间的迁移。

Conclusion: DOF方法通过局部参考系表示和稀疏关键点对应，有效解决了曲面物体间的技能迁移问题。

Abstract: Curved objects pose a fundamental challenge for skill transfer in robotics: unlike planar surfaces, they do not admit a global reference frame. As a result, task-relevant directions such as "toward" or "along" the surface vary with position and geometry, making object-centric tasks difficult to transfer across shapes. To address this, we introduce an approach using Diffused Orientation Fields (DOF), a smooth representation of local reference frames, for transfer learning of tasks across curved objects. By expressing manipulation tasks in these smoothly varying local frames, we reduce the problem of transferring tasks across curved objects to establishing sparse keypoint correspondences. DOF is computed online from raw point cloud data using diffusion processes governed by partial differential equations, conditioned on keypoints. We evaluate DOF under geometric, topological, and localization perturbations, and demonstrate successful transfer of tasks requiring continuous physical interaction such as inspection, slicing, and peeling across varied objects. We provide our open-source codes at our website https://github.com/idiap/diffused_fields_robotics

</details>


### [38] [An Analysis of Constraint-Based Multi-Agent Pathfinding Algorithms](https://arxiv.org/abs/2511.18604)
*Hannah Lee,James D. Motes,Marco Morales,Nancy M. Amato*

Main category: cs.RO

TL;DR: 该研究通过约束分类指导多智能体路径规划算法设计，比较保守约束和激进约束在CBS算法中的表现，发现激进约束在解决更多实例方面表现更好，而保守约束在解质量上更优。


<details>
  <summary>Details</summary>
Motivation: 为多智能体路径规划和多机器人运动规划算法设计提供基于约束分类的指导，帮助用户根据具体需求选择合适的约束类型。

Method: 使用混合网格-路线图表示法，在不同分辨率下测试CBS和CBSw/P算法，将约束分为保守约束和激进约束两类进行分析。

Result: 激进约束（优先级约束）在智能体数量或分辨率增加时能解决更多实例，而保守约束（运动约束）在两者都成功时提供更好的解质量。

Conclusion: 研究提供了约束选择的决策流程图，建议在多机器人运动规划中考虑拓扑特征，并强调约束选择应根据问题、解和表示特征进行权衡。

Abstract: This study informs the design of future multi-agent pathfinding (MAPF) and multi-robot motion planning (MRMP) algorithms by guiding choices based on constraint classification for constraint-based search algorithms. We categorize constraints as conservative or aggressive and provide insights into their search behavior, focusing specifically on vanilla Conflict-Based Search (CBS) and Conflict-Based Search with Priorities (CBSw/P). Under a hybrid grid-roadmap representation with varying resolution, we observe that aggressive (priority constraint) formulations tend to solve more instances as agent count or resolution increases, whereas conservative (motion constraint) formulations yield stronger solution quality when both succeed. Findings are synthesized in a decision flowchart, aiding users in selecting suitable constraints. Recommendations extend to Multi-Robot Motion Planning (MRMP), emphasizing the importance of considering topological features alongside problem, solution, and representation features. A comprehensive exploration of the study, including raw data and map performance, is available in our public GitHub Repository: https://GitHub.com/hannahjmlee/constraint-mapf-analysis

</details>


### [39] [How to Train Your Latent Control Barrier Function: Smooth Safety Filtering Under Hard-to-Model Constraints](https://arxiv.org/abs/2511.18606)
*Kensuke Nakamura,Arun L. Bishop,Steven Man,Aaron M. Johnson,Zachary Manchester,Andrea Bajcsy*

Main category: cs.RO

TL;DR: LatentCBF提出了一种新的潜在空间安全过滤方法，通过梯度惩罚实现平滑的边界函数，并混合名义策略和安全策略数据进行值函数训练，解决了现有方法中值函数不兼容的问题，实现了平滑的安全过滤并显著提高了任务完成率。


<details>
  <summary>Details</summary>
Motivation: 现有潜在安全过滤器采用"最小限制"过滤，在名义策略和安全策略之间离散切换，这会损害现代视觉运动策略的任务性能。虽然可达性值函数理论上可以调整为控制屏障函数进行平滑优化过滤，但当前的潜在空间学习方法产生的值函数本质上不兼容。

Method: LatentCBF通过梯度惩罚实现平滑的边界函数而无需额外标注，并采用混合名义策略和安全策略分布数据的值训练程序。这解决了值函数不兼容的两个来源：饱和值函数的跳跃问题和强化学习近似中名义策略动作值估计不准确的问题。

Result: 在模拟基准测试和基于视觉的机械臂操作硬件实验中，LatentCBF实现了平滑的安全过滤，相比之前的切换方法将任务完成率提高了一倍。

Conclusion: LatentCBF成功解决了潜在安全过滤中的值函数不兼容问题，通过平滑的边界函数和混合数据训练，实现了既保证安全又不损害任务性能的视觉运动控制。

Abstract: Latent safety filters extend Hamilton-Jacobi (HJ) reachability to operate on latent state representations and dynamics learned directly from high-dimensional observations, enabling safe visuomotor control under hard-to-model constraints. However, existing methods implement "least-restrictive" filtering that discretely switch between nominal and safety policies, potentially undermining the task performance that makes modern visuomotor policies valuable. While reachability value functions can, in principle, be adapted to be control barrier functions (CBFs) for smooth optimization-based filtering, we theoretically and empirically show that current latent-space learning methods produce fundamentally incompatible value functions. We identify two sources of incompatibility: First, in HJ reachability, failures are encoded via a "margin function" in latent space, whose sign indicates whether or not a latent is in the constraint set. However, representing the margin function as a classifier yields saturated value functions that exhibit discontinuous jumps. We prove that the value function's Lipschitz constant scales linearly with the margin function's Lipschitz constant, revealing that smooth CBFs require smooth margins. Second, reinforcement learning (RL) approximations trained solely on safety policy data yield inaccurate value estimates for nominal policy actions, precisely where CBF filtering needs them. We propose the LatentCBF, which addresses both challenges through gradient penalties that lead to smooth margin functions without additional labeling, and a value-training procedure that mixes data from both nominal and safety policy distributions. Experiments on simulated benchmarks and hardware with a vision-based manipulation policy demonstrate that LatentCBF enables smooth safety filtering while doubling the task-completion rate over prior switching methods.

</details>


### [40] [AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations](https://arxiv.org/abs/2511.18617)
*Litian Gong,Fatemeh Bahrani,Yutai Zhou,Amin Banayeeanzade,Jiachen Li,Erdem Biyik*

Main category: cs.RO

TL;DR: AutoFocus-IL是一种通过视觉语言模型自动识别关键对象并生成时间显著性图来改进视觉模仿学习的方法，无需昂贵的人工监督，在CARLA模拟器和真实机器人任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于显著性正则化的方法需要昂贵的人工监督（如人类注视数据或手动标注），限制了其可扩展性。AutoFocus-IL旨在通过自动方式实现类似效果。

Method: 利用视觉语言模型自动识别和跟踪演示中的关键对象，生成时间显著性图来突出因果视觉信号并抑制干扰因素，然后将这些图用于正则化行为克隆策略。

Result: 在CARLA模拟器和真实机器人操作任务中，AutoFocus-IL不仅优于标准行为克隆，还超过了需要人类监督的最先进基线方法。

Conclusion: AutoFocus-IL提供了一种无需昂贵人工监督的有效方法，能显著提高视觉模仿学习的数据效率和泛化能力。

Abstract: AutoFocus-IL is a simple yet effective method to improve data efficiency and generalization in visual imitation learning by guiding policies to attend to task-relevant features rather than distractors and spurious correlations. Although saliency regularization has emerged as a promising way to achieve this, existing approaches typically require costly supervision such as human gaze data or manual saliency annotations. In contrast, AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors. These maps are then used to regularize behavior cloning policies, yielding stronger alignment between visual attention and task-relevant cues. Experiments in both the CARLA simulator and real-robot manipulation tasks demonstrate that AutoFocus-IL not only outperforms standard behavior cloning but also surpasses state-of-the-art baselines that assume privileged access to human supervision, such as gaze data. Code, datasets, and trained policy videos are available at https://AutoFocus-IL.github.io/.

</details>


### [41] [Online Learning-Enhanced Lie Algebraic MPC for Robust Trajectory Tracking of Autonomous Surface Vehicles](https://arxiv.org/abs/2511.18683)
*Yinan Dong,Ziyu Xu,Tsimafei Lazouski,Sangli Teng,Maani Ghaffari*

Main category: cs.RO

TL;DR: 提出了一种结合李群上凸误差状态MPC和在线学习模块的高效控制器，用于在未知扰动下实现海洋车辆轨迹跟踪


<details>
  <summary>Details</summary>
Motivation: 自主水面车辆(ASV)容易受到风浪等环境扰动影响，在动态海洋条件下实现精确轨迹跟踪是一个持续挑战

Method: 将李群上的凸误差状态模型预测控制(MPC)与在线学习模块相结合，实时补偿未知扰动

Result: 在数值模拟、VRX模拟器和真实世界现场实验中的广泛评估表明，该方法在各种扰动场景下相比现有方法实现了更优越的跟踪精度

Conclusion: 该设计能够在保持计算效率的同时实现自适应和鲁棒控制

Abstract: Autonomous surface vehicles (ASVs) are easily influenced by environmental disturbances such as wind and waves, making accurate trajectory tracking a persistent challenge in dynamic marine conditions. In this paper, we propose an efficient controller for trajectory tracking of marine vehicles under unknown disturbances by combining a convex error-state MPC on the Lie group with an online learning module to compensate for these disturbances in real time. This design enables adaptive and robust control while maintaining computational efficiency. Extensive evaluations in numerical simulations, the Virtual RobotX (VRX) simulator, and real-world field experiments demonstrate that our method achieves superior tracking accuracy under various disturbance scenarios compared with existing approaches.

</details>


### [42] [Stable Multi-Drone GNSS Tracking System for Marine Robots](https://arxiv.org/abs/2511.18694)
*Shuo Wen,Edwin Meriaux,Mariana Sosa Guzmán,Zhizun Wang,Junming Shi,Gregory Dudek*

Main category: cs.RO

TL;DR: 提出了一种基于多无人机GNSS的海洋机器人跟踪系统，通过视觉检测、多目标跟踪、三角定位和置信度加权EKF实现实时稳定的GNSS估计。


<details>
  <summary>Details</summary>
Motivation: 水下GNSS信号不可靠，传统定位方法存在误差累积、计算量大或依赖基础设施的问题，需要一种可扩展的海洋机器人跟踪方案。

Method: 结合高效视觉检测、轻量级多目标跟踪、GNSS三角定位和置信度加权EKF，并引入跨无人机跟踪ID对齐算法确保全局一致性。

Result: 在多样化复杂环境中验证了系统的可扩展性和鲁棒性。

Conclusion: 该系统为表面和近表面海洋机器人提供了实时稳定的GNSS估计解决方案，解决了水下定位的挑战。

Abstract: Accurate localization is essential for marine robotics, yet Global Navigation Satellite System (GNSS) signals are unreliable or unavailable even at a very short distance below the water surface. Traditional alternatives, such as inertial navigation, Doppler Velocity Loggers (DVL), SLAM, and acoustic methods, suffer from error accumulation, high computational demands, or infrastructure dependence. In this work, we present a scalable multi-drone GNSS-based tracking system for surface and near-surface marine robots. Our approach combines efficient visual detection, lightweight multi-object tracking, GNSS-based triangulation, and a confidence-weighted Extended Kalman Filter (EKF) to provide stable GNSS estimation in real time. We further introduce a cross-drone tracking ID alignment algorithm that enforces global consistency across views, enabling robust multi-robot tracking with redundant aerial coverage. We validate our system in diversified complex settings to show the scalability and robustness of the proposed algorithm.

</details>


### [43] [CNN-Based Camera Pose Estimation and Localisation of Scan Images for Aircraft Visual Inspection](https://arxiv.org/abs/2511.18702)
*Xueyan Oh,Leonard Loh,Shaohui Foong,Zhong Bao Andy Koh,Kow Leong Ng,Poh Kang Tan,Pei Lin Pearlin Toh,U-Xuan Tan*

Main category: cs.RO

TL;DR: 提出了一种无需基础设施的飞机视觉检测方法，使用PTZ相机通过深度卷积神经网络预测自身位姿，实现了在机场停机坪上的快速自动检测。


<details>
  <summary>Details</summary>
Motivation: 传统飞机外观检测依赖人工，自动化需求日益增长。现有定位方法需要基础设施，在室外环境和有限周转时间内难以部署，且航空公司禁止接触飞机表面或使用无人机。

Method: 使用PTZ相机，通过基于合成图像微调的深度卷积神经网络预测相机位姿，应用领域随机化生成训练数据集，并利用飞机几何改进损失函数。提出初始化、扫描路径规划和图像精确定位的工作流程。

Result: 在真实飞机上的实验表明，所有真实场景的相机位姿估计均方根误差小于0.24米和2度。

Conclusion: 该方法实现了基础设施自由、易于部署的飞机视觉检测系统，在真实环境中表现出高精度的位姿估计能力。

Abstract: General Visual Inspection is a manual inspection process regularly used to detect and localise obvious damage on the exterior of commercial aircraft. There has been increasing demand to perform this process at the boarding gate to minimise the downtime of the aircraft and automating this process is desired to reduce the reliance on human labour. Automating this typically requires estimating a camera's pose with respect to the aircraft for initialisation but most existing localisation methods require infrastructure, which is very challenging in uncontrolled outdoor environments and within the limited turnover time (approximately 2 hours) on an airport tarmac. Additionally, many airlines and airports do not allow contact with the aircraft's surface or using UAVs for inspection between flights, and restrict access to commercial aircraft. Hence, this paper proposes an on-site method that is infrastructure-free and easy to deploy for estimating a pan-tilt-zoom camera's pose and localising scan images. This method initialises using the same pan-tilt-zoom camera used for the inspection task by utilising a Deep Convolutional Neural Network fine-tuned on only synthetic images to predict its own pose. We apply domain randomisation to generate the dataset for fine-tuning the network and modify its loss function by leveraging aircraft geometry to improve accuracy. We also propose a workflow for initialisation, scan path planning, and precise localisation of images captured from a pan-tilt-zoom camera. We evaluate and demonstrate our approach through experiments with real aircraft, achieving root-mean-square camera pose estimation errors of less than 0.24 m and 2 degrees for all real scenes.

</details>


### [44] [Asynchronous Distributed Multi-Robot Motion Planning Under Imperfect Communication](https://arxiv.org/abs/2511.18703)
*Ardalan Tajbakhsh,Augustinos Saravanos,James Zhu,Evangelos A. Theodorou,Lorenz T. Biegler,Aaron M. Johnson*

Main category: cs.RO

TL;DR: 提出了一种延迟感知ADMM（DA-ADMM）算法，通过基于实时延迟统计调整惩罚参数，提高多机器人系统在通信延迟下的协调性能。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人系统在现实通信延迟下的协调挑战，现有共识ADMM算法对惩罚参数调整敏感，且未明确考虑延迟影响。

Method: 引入DA-ADMM变体，基于实时延迟统计调整惩罚参数，使智能体能够降低陈旧信息的权重，在共识和双重更新中优先处理最新更新。

Result: 在2D和3D环境中通过双积分器、Dubins-car和无人机动力学进行广泛仿真，DA-ADMM相比固定参数、残差平衡和固定约束基线显著提高了鲁棒性、成功率和解决方案质量。

Conclusion: 性能下降不仅由延迟长度或频率决定，还取决于优化器对延迟信息的上下文推理能力。DA-ADMM在各种延迟条件下实现了持续更好的协调性能。

Abstract: This paper addresses the challenge of coordinating multi-robot systems under realistic communication delays using distributed optimization. We focus on consensus ADMM as a scalable framework for generating collision-free, dynamically feasible motion plans in both trajectory optimization and receding-horizon control settings. In practice, however, these algorithms are sensitive to penalty tuning or adaptation schemes (e.g. residual balancing and adaptive parameter heuristics) that do not explicitly consider delays. To address this, we introduce a Delay-Aware ADMM (DA-ADMM) variant that adapts penalty parameters based on real-time delay statistics, allowing agents to down-weight stale information and prioritize recent updates during consensus and dual updates. Through extensive simulations in 2D and 3D environments with double-integrator, Dubins-car, and drone dynamics, we show that DA-ADMM significantly improves robustness, success rate, and solution quality compared to fixed-parameter, residual-balancing, and fixed-constraint baselines. Our results highlight that performance degradation is not solely determined by delay length or frequency, but by the optimizer's ability to contextually reason over delayed information. The proposed DA-ADMM achieves consistently better coordination performance across a wide range of delay conditions, offering a principled and efficient mechanism for resilient multi-robot motion planning under imperfect communication.

</details>


### [45] [GVD-TG: Topological Graph based on Fast Hierarchical GVD Sampling for Robot Exploration](https://arxiv.org/abs/2511.18708)
*Yanbin Li,Canran Xiao,Shenghai Yuan,Peilai Yu,Ziruo Li,Zhiguo Zhang,Wenzheng Chi,Wei Zhang*

Main category: cs.RO

TL;DR: 提出了一种基于广义Voronoi图(GVD)的拓扑地图更新方法，通过多粒度分层GVD生成、节点聚类与连接机制、基于形态学膨胀的前沿提取等技术，解决了拓扑地图实时更新和路径回溯问题，提高了机器人探索效率。


<details>
  <summary>Details</summary>
Motivation: 拓扑地图比度量地图更适合机器人探索任务，但实时更新准确且细节丰富的环境拓扑地图仍然是一个挑战。

Method: 1. 基于GVD的拓扑地图更新方法：包括新观测区域去噪、多粒度分层GVD生成、节点聚类与连接机制、基于形态学膨胀的前沿提取；2. 使用轻量级成本函数实时评估和切换下一个视点。

Result: 通过对比测试验证了系统在探索任务中的性能优于SOTA方法，提高了探索效率、减少了路径回溯概率、增强了细节特征捕捉能力。

Conclusion: 该方法能够有效解决拓扑地图实时更新问题，提高机器人探索的灵活性和效率，避免路径回溯困境。

Abstract: Topological maps are more suitable than metric maps for robotic exploration tasks. However, real-time updating of accurate and detail-rich environmental topological maps remains a challenge. This paper presents a topological map updating method based on the Generalized Voronoi Diagram (GVD). First, the newly observed areas are denoised to avoid low-efficiency GVD nodes misleading the topological structure. Subsequently, a multi-granularity hierarchical GVD generation method is designed to control the sampling granularity at both global and local levels. This not only ensures the accuracy of the topological structure but also enhances the ability to capture detail features, reduces the probability of path backtracking, and ensures no overlap between GVDs through the maintenance of a coverage map, thereby improving GVD utilization efficiency. Second, a node clustering method with connectivity constraints and a connectivity method based on a switching mechanism are designed to avoid the generation of unreachable nodes and erroneous nodes caused by obstacle attraction. A special cache structure is used to store all connectivity information, thereby improving exploration efficiency. Finally, to address the issue of frontiers misjudgment caused by obstacles within the scope of GVD units, a frontiers extraction method based on morphological dilation is designed to effectively ensure the reachability of frontiers. On this basis, a lightweight cost function is used to assess and switch to the next viewpoint in real time. This allows the robot to quickly adjust its strategy when signs of path backtracking appear, thereby escaping the predicament and increasing exploration flexibility. And the performance of system for exploration task is verified through comparative tests with SOTA methods.

</details>


### [46] [Autonomous Surface Selection For Manipulator-Based UV Disinfection In Hospitals Using Foundation Models](https://arxiv.org/abs/2511.18709)
*Xueyan Oh,Jonathan Her,Zhixiang Ong,Brandon Koh,Yun Hann Tan,U-Xuan Tan*

Main category: cs.RO

TL;DR: 提出了一种利用基础模型简化基于机械臂的UV消毒表面选择的方法，减少人工干预且无需模型训练，并通过VLM辅助分割细化来检测和排除细小非目标物体。


<details>
  <summary>Details</summary>
Motivation: 传统UV消毒方法需要大量人工干预来定义消毒区域，而基于深度学习的方法通常需要大量微调和数据集，且往往不处理部分表面消毒的场景理解问题。

Method: 利用基础模型简化表面选择，无需模型训练；通过VLM辅助分割细化来检测和排除细小非目标物体，减少误分割错误。

Result: 目标和非目标表面的正确分割成功率超过92%，真实世界机械臂和模拟UV光实验证明了其实际应用潜力。

Conclusion: 该方法在UV表面消毒中表现出良好的性能，减少了人工干预需求，具有实际应用价值。

Abstract: Ultraviolet (UV) germicidal radiation is an established non-contact method for surface disinfection in medical environments. Traditional approaches require substantial human intervention to define disinfection areas, complicating automation, while deep learning-based methods often need extensive fine-tuning and large datasets, which can be impractical for large-scale deployment. Additionally, these methods often do not address scene understanding for partial surface disinfection, which is crucial for avoiding unintended UV exposure. We propose a solution that leverages foundation models to simplify surface selection for manipulator-based UV disinfection, reducing human involvement and removing the need for model training. Additionally, we propose a VLM-assisted segmentation refinement to detect and exclude thin and small non-target objects, showing that this reduces mis-segmentation errors. Our approach achieves over 92\% success rate in correctly segmenting target and non-target surfaces, and real-world experiments with a manipulator and simulated UV light demonstrate its practical potential for real-world applications.

</details>


### [47] [Head Stabilization for Wheeled Bipedal Robots via Force-Estimation-Based Admittance Control](https://arxiv.org/abs/2511.18712)
*Tianyu Wang,Chunxiang Yan,Xuanhong Liao,Tao Zhang,Ping Wang,Cong Wen,Dingchuan Liu,Haowen Yu,Ximin Lyu*

Main category: cs.RO

TL;DR: 开发了一种基于模型的地面力估计方法和导纳控制算法，用于提高轮式双足机器人在不平坦地形上的头部稳定性。


<details>
  <summary>Details</summary>
Motivation: 轮式双足机器人在不平坦地形上运行时，头部不稳定性会降低传感器精度或损坏有效载荷，现有研究主要关注平台稳定而忽略了头部在世界坐标系中的主动稳定。

Method: 使用基于模型的地面力估计方法，结合导纳控制算法来增强地形适应性。

Result: 仿真实验验证了力估计器的实时性能和机器人在不平坦地形上的鲁棒性。

Conclusion: 该方法能有效解决轮式双足机器人在不平坦地形上的头部稳定性问题。

Abstract: Wheeled bipedal robots are emerging as flexible platforms for field exploration. However, head instability induced by uneven terrain can degrade the accuracy of onboard sensors or damage fragile payloads. Existing research primarily focuses on stabilizing the mobile platform but overlooks active stabilization of the head in the world frame, resulting in vertical oscillations that undermine overall stability. To address this challenge, we developed a model-based ground force estimation method for our 6-degree-of-freedom wheeled bipedal robot. Leveraging these force estimates, we implemented an admittance control algorithm to enhance terrain adaptability. Simulation experiments validated the real-time performance of the force estimator and the robot's robustness when traversing uneven terrain.

</details>


### [48] [AIRHILT: A Human-in-the-Loop Testbed for Multimodal Conflict Detection in Aviation](https://arxiv.org/abs/2511.18718)
*Omar Garib,Jayaprakash D. Kambhampaty,Olivia J. Pinon Fischer,Dimitri N. Mavris*

Main category: cs.RO

TL;DR: AIRHILT是一个基于Godot引擎的轻量级模拟环境，用于评估航空冲突检测中的多模态飞行员和空管辅助系统，支持人机交互和标准化接口。


<details>
  <summary>Details</summary>
Motivation: 需要开发一个统一的平台来评估航空冲突检测中的多模态辅助系统，整合无线电通信、视觉场景理解和ADS-B监视数据。

Method: 基于开源Godot引擎构建，同步飞行员和空管无线电通信、摄像头视觉场景理解和ADS-B监视数据，提供标准化JSON接口集成ASR、视觉检测、决策和TTS模型。

Result: 在跑道重叠场景中，辅助系统平均首次警告时间为7.7秒，ASR和视觉延迟分别为5.9秒和0.4秒。

Conclusion: AIRHILT提供了一个可扩展、可复现的研究平台，支持航空多模态态势感知和冲突检测研究，代码和场景已开源。

Abstract: We introduce AIRHILT (Aviation Integrated Reasoning, Human-in-the-Loop Testbed), a modular and lightweight simulation environment designed to evaluate multimodal pilot and air traffic control (ATC) assistance systems for aviation conflict detection. Built on the open-source Godot engine, AIRHILT synchronizes pilot and ATC radio communications, visual scene understanding from camera streams, and ADS-B surveillance data within a unified, scalable platform. The environment supports pilot- and controller-in-the-loop interactions, providing a comprehensive scenario suite covering both terminal area and en route operational conflicts, including communication errors and procedural mistakes. AIRHILT offers standardized JSON-based interfaces that enable researchers to easily integrate, swap, and evaluate automatic speech recognition (ASR), visual detection, decision-making, and text-to-speech (TTS) models. We demonstrate AIRHILT through a reference pipeline incorporating fine-tuned Whisper ASR, YOLO-based visual detection, ADS-B-based conflict logic, and GPT-OSS-20B structured reasoning, and present preliminary results from representative runway-overlap scenarios, where the assistant achieves an average time-to-first-warning of approximately 7.7 s, with average ASR and vision latencies of approximately 5.9 s and 0.4 s, respectively. The AIRHILT environment and scenario suite are openly available, supporting reproducible research on multimodal situational awareness and conflict detection in aviation; code and scenarios are available at https://github.com/ogarib3/airhilt.

</details>


### [49] [SP-VINS: A Hybrid Stereo Visual Inertial Navigation System based on Implicit Environmental Map](https://arxiv.org/abs/2511.18756)
*Xueyu Du,Lilian Zhang,Fuan Duan,Xincan Luo,Maosong Wang,Wenqi Wu,JunMao*

Main category: cs.RO

TL;DR: 提出了一种基于滤波器的立体视觉惯性导航系统SP-VINS，通过隐式环境地图实现高效闭环约束，结合混合残差滤波框架和在线外参标定，在保持高计算效率的同时实现长期高精度定位。


<details>
  <summary>Details</summary>
Motivation: 传统基于滤波器的VINS系统在精度和效率之间取得了良好平衡，但其有限的地图质量阻碍了长期高精度状态估计。

Method: 1. 提出基于关键帧和2D关键点隐式环境地图的立体VINS系统；2. 设计结合地标重投影和射线约束的混合残差滤波框架；3. 在退化环境中将相机-IMU外参纳入视觉描述实现在线标定。

Result: 基准测试表明，SP-VINS在保持高计算效率的同时实现了长期高精度定位性能，优于现有最先进方法。

Conclusion: 所提出的SP-VINS系统通过创新的隐式地图表示和混合滤波框架，有效解决了传统滤波VINS在长期定位中的精度限制问题。

Abstract: Filter-based visual inertial navigation system (VINS) has attracted mobile-robot researchers for the good balance between accuracy and efficiency, but its limited mapping quality hampers long-term high-accuracy state estimation. To this end, we first propose a novel filter-based stereo VINS, differing from traditional simultaneous localization and mapping (SLAM) systems based on 3D map, which performs efficient loop closure constraints with implicit environmental map composed of keyframes and 2D keypoints. Secondly, we proposed a hybrid residual filter framework that combines landmark reprojection and ray constraints to construct a unified Jacobian matrix for measurement updates. Finally, considering the degraded environment, we incorporated the camera-IMU extrinsic parameters into visual description to achieve online calibration. Benchmark experiments demonstrate that the proposed SP-VINS achieves high computational efficiency while maintaining long-term high-accuracy localization performance, and is superior to existing state-of-the-art (SOTA) methods.

</details>


### [50] [MergeVLA: Cross-Skill Model Merging Toward a Generalist Vision-Language-Action Agent](https://arxiv.org/abs/2511.18810)
*Yuxia Fu,Zhizhen Zhang,Yuqi Zhang,Zijian Wang,Zi Huang,Yadan Luo*

Main category: cs.RO

TL;DR: MergeVLA是一个面向合并的视觉-语言-动作架构，通过稀疏激活的LoRA适配器和仅交叉注意力块来解决VLA模型在多技能设置中的合并问题，实现了跨任务、具身和环境的鲁棒泛化。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在单一任务上表现良好，但扩展到多技能设置时面临挑战：直接合并不同任务训练的VLA专家会导致成功率接近零。这引发了一个基本问题：什么阻止了VLA在一个模型中掌握多种技能？

Method: 通过经验性分解VLA微调中的可学习参数，识别了两个不可合并性的关键来源：1）微调驱动VLM骨干中的LoRA适配器向不同的任务特定方向发散；2）动作专家通过自注意力反馈发展出块间依赖关系。提出MergeVLA架构，使用任务掩码的稀疏激活LoRA适配器保持参数一致性，并用仅交叉注意力块替换自注意力以保持专业化的局部性和可组合性。

Result: 在LIBERO、LIBERO-Plus、RoboTwin和真实SO101机械臂上的多任务实验中，MergeVLA实现了与单独微调专家相当甚至更好的性能，展示了跨任务、具身和环境的鲁棒泛化能力。

Conclusion: MergeVLA通过设计保持了可合并性，解决了VLA模型在多技能设置中的关键挑战，为构建通用多技能VLA模型提供了可行方案。

Abstract: Recent Vision-Language-Action (VLA) models reformulate vision-language models by tuning them with millions of robotic demonstrations. While they perform well when fine-tuned for a single embodiment or task family, extending them to multi-skill settings remains challenging: directly merging VLA experts trained on different tasks results in near-zero success rates. This raises a fundamental question: what prevents VLAs from mastering multiple skills within one model? With an empirical decomposition of learnable parameters during VLA fine-tuning, we identify two key sources of non-mergeability: (1) Finetuning drives LoRA adapters in the VLM backbone toward divergent, task-specific directions beyond the capacity of existing merging methods to unify. (2) Action experts develop inter-block dependencies through self-attention feedback, causing task information to spread across layers and preventing modular recombination. To address these challenges, we present MergeVLA, a merging-oriented VLA architecture that preserves mergeability by design. MergeVLA introduces sparsely activated LoRA adapters via task masks to retain consistent parameters and reduce irreconcilable conflicts in the VLM. Its action expert replaces self-attention with cross-attention-only blocks to keep specialization localized and composable. When the task is unknown, it uses a test-time task router to adaptively select the appropriate task mask and expert head from the initial observation, enabling unsupervised task inference. Across LIBERO, LIBERO-Plus, RoboTwin, and multi-task experiments on the real SO101 robotic arm, MergeVLA achieves performance comparable to or even exceeding individually finetuned experts, demonstrating robust generalization across tasks, embodiments, and environments.

</details>


### [51] [AutoOdom: Learning Auto-regressive Proprioceptive Odometry for Legged Locomotion](https://arxiv.org/abs/2511.18857)
*Changsheng Luo,Yushi Wang,Wenhan Cai,Mingguo Zhao*

Main category: cs.RO

TL;DR: AutoOdom是一种新型自回归本体感知里程计系统，通过两阶段训练范式解决足式机器人在GPS缺失和视觉退化环境中的定位问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决足式机器人在GPS缺失和视觉退化环境中的精确定位问题，克服传统方法在建模不确定性、累积漂移、仿真到现实迁移困难等方面的局限性。

Method: 采用两阶段训练范式：第一阶段使用大规模仿真数据学习足式运动的复杂非线性动力学和快速变化的接触状态；第二阶段使用有限真实世界数据引入自回归增强机制，弥合仿真到现实的差距。

Result: 在Booster T1人形机器人上的实验验证显示，相比Legolas基线，AutoOdom在绝对轨迹误差上提升57.2%，在Umeyama对齐误差上提升59.2%，在相对位姿误差上提升36.2%。

Conclusion: AutoOdom通过创新的自回归训练方法有效解决了足式机器人本体感知里程计的关键挑战，在动态环境中展现出卓越的鲁棒性和准确性。

Abstract: Accurate proprioceptive odometry is fundamental for legged robot navigation in GPS-denied and visually degraded environments where conventional visual odometry systems fail. Current approaches face critical limitations: analytical filtering methods suffer from modeling uncertainties and cumulative drift, hybrid learning-filtering approaches remain constrained by their analytical components, while pure learning-based methods struggle with simulation-to-reality transfer and demand extensive real-world data collection. This paper introduces AutoOdom, a novel autoregressive proprioceptive odometry system that overcomes these challenges through an innovative two-stage training paradigm. Stage 1 employs large-scale simulation data to learn complex nonlinear dynamics and rapidly changing contact states inherent in legged locomotion, while Stage 2 introduces an autoregressive enhancement mechanism using limited real-world data to effectively bridge the sim-to-real gap. The key innovation lies in our autoregressive training approach, where the model learns from its own predictions to develop resilience against sensor noise and improve robustness in highly dynamic environments. Comprehensive experimental validation on the Booster T1 humanoid robot demonstrates that AutoOdom significantly outperforms state-of-the-art methods across all evaluation metrics, achieving 57.2% improvement in absolute trajectory error, 59.2% improvement in Umeyama-aligned error, and 36.2% improvement in relative pose error compared to the Legolas baseline. Extensive ablation studies provide critical insights into sensor modality selection and temporal modeling, revealing counterintuitive findings about IMU acceleration data and validating our systematic design choices for robust proprioceptive odometry in challenging locomotion scenarios.

</details>


### [52] [Accelerating Reinforcement Learning via Error-Related Human Brain Signals](https://arxiv.org/abs/2511.18878)
*Suzie Kim,Hye-Bin Shin,Hyo-Jeong Jang*

Main category: cs.RO

TL;DR: EEG神经反馈可以加速复杂机器人操作任务中的强化学习，通过将脑电信号解码的错误相关电位整合到奖励塑造中，在7自由度机械臂的障碍物环境中显著提升学习效率。


<details>
  <summary>Details</summary>
Motivation: 探索在复杂机器人操作任务中，基于脑电图的隐式神经反馈是否能加速强化学习，特别是在涉及障碍物和精确末端执行器控制的高维操作任务中。

Method: 将离线训练的EEG分类器解码的错误相关电位整合到奖励塑造中，系统评估人类反馈权重的影响，并在7自由度机械臂的障碍物环境中进行实验。

Result: 神经反馈加速了强化学习，根据反馈权重的不同，任务成功率有时超过稀疏奖励基线；最佳权重在所有受试者中都能一致加速学习；留一受试者验证显示框架对个体EEG解码差异具有鲁棒性。

Conclusion: EEG引导的强化学习可以扩展到运动任务之外，为人类对齐的操作技能获取提供了可行途径。

Abstract: In this work, we investigate how implicit neural feed back can accelerate reinforcement learning in complex robotic manipulation settings. While prior electroencephalogram (EEG) guided reinforcement learning studies have primarily focused on navigation or low-dimensional locomotion tasks, we aim to understand whether such neural evaluative signals can improve policy learning in high-dimensional manipulation tasks involving obstacles and precise end-effector control. We integrate error related potentials decoded from offline-trained EEG classifiers into reward shaping and systematically evaluate the impact of human-feedback weighting. Experiments on a 7-DoF manipulator in an obstacle-rich reaching environment show that neural feedback accelerates reinforcement learning and, depending on the human-feedback weighting, can yield task success rates that at times exceed those of sparse-reward baselines. Moreover, when applying the best-performing feedback weighting across all sub jects, we observe consistent acceleration of reinforcement learning relative to the sparse-reward setting. Furthermore, leave-one subject-out evaluations confirm that the proposed framework remains robust despite the intrinsic inter-individual variability in EEG decodability. Our findings demonstrate that EEG-based reinforcement learning can scale beyond locomotion tasks and provide a viable pathway for human-aligned manipulation skill acquisition.

</details>


### [53] [An Efficient Closed-Form Solution to Full Visual-Inertial State Initialization](https://arxiv.org/abs/2511.18910)
*Samuel Cerezo,Seong Hun Lee,Javier Civera*

Main category: cs.RO

TL;DR: 提出了一种无需非线性优化的闭式初始化方法，用于恢复完整的视觉-惯性状态，相比基于优化的方法具有更低的初始化误差、更短的初始化窗口和更低的计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖迭代求解器，而本文旨在开发解析、易实现且数值稳定的解决方案，以实现可靠的系统启动。

Method: 基于小旋转和恒定速度近似，保持公式紧凑同时保留运动与惯性测量之间的基本耦合；提出基于可观测性的两阶段初始化方案，平衡精度与初始化延迟。

Result: 在EuRoC数据集上的广泛实验验证了假设：相比基于优化的方法，初始化误差降低10-20%，初始化窗口缩短4倍，计算成本降低5倍。

Conclusion: 该方法提供了一种高效可靠的视觉-惯性系统初始化解决方案，在精度和计算效率方面均优于现有方法。

Abstract: In this letter, we present a closed-form initialization method that recovers the full visual-inertial state without nonlinear optimization. Unlike previous approaches that rely on iterative solvers, our formulation yields analytical, easy-to-implement, and numerically stable solutions for reliable start-up. Our method builds on small-rotation and constant-velocity approximations, which keep the formulation compact while preserving the essential coupling between motion and inertial measurements. We further propose an observability-driven, two-stage initialization scheme that balances accuracy with initialization latency. Extensive experiments on the EuRoC dataset validate our assumptions: our method achieves 10-20% lower initialization error than optimization-based approaches, while using 4x shorter initialization windows and reducing computational cost by 5x.

</details>


### [54] [Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation](https://arxiv.org/abs/2511.18950)
*Juntao Gao,Feiyang Ye,Jing Zhang,Wenjing Qian*

Main category: cs.RO

TL;DR: 提出Compressor-VLA框架，通过语义任务压缩器和空间细化压缩器实现指令调节的视觉令牌压缩，在保持任务性能的同时显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作模型中的冗余视觉令牌处理带来巨大计算开销，阻碍实时机器人部署。现有任务无关的令牌修剪方法难以保留任务关键视觉信息。

Method: 提出混合指令调节令牌压缩框架，包含语义任务压缩器（提取整体任务相关上下文）和空间细化压缩器（保留细粒度空间细节），通过自然语言指令动态调节压缩过程。

Result: 在LIBERO基准测试中达到竞争性成功率，FLOPs减少59%，视觉令牌数量减少超过3倍。真实机器人部署验证了模型的仿真到现实迁移性和实用性。

Conclusion: Compressor-VLA框架有效解决了VLA模型的计算效率问题，通过指令引导动态聚焦任务相关对象，实现了高效的任务导向视觉信息压缩。

Abstract: Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.

</details>


### [55] [End-to-end Autonomous Vehicle Following System using Monocular Fisheye Camera](https://arxiv.org/abs/2511.19011)
*Jiale Zhang,Yeqiang Qian,Tong Qin,Mingyang Jiang,Siyuan Chen,Ming Yang*

Main category: cs.RO

TL;DR: 提出了一种仅使用摄像头的车辆跟随框架，通过端到端方法将车辆编队能力从受限场景扩展到通用场景，解决了传统系统对车道线和昂贵传感器的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 车辆保有量增加导致交通拥堵、事故和碳排放问题，车辆编队是解决这些问题的有前景方案，但现有系统依赖车道线和昂贵传感器，限制了通用适用性。

Method: 提出端到端方法，包含语义掩码解决多帧数据融合中的因果混淆问题，引入动态采样机制精确跟踪前车轨迹，仅使用摄像头实现车辆跟随。

Result: 在真实世界车辆实验中进行了广泛闭环验证，系统能够在各种场景下跟随车辆，性能优于传统多阶段算法。

Conclusion: 该方法为成本效益高的自动驾驶车辆编队提供了有前景的解决方案，仅需摄像头即可实现通用场景的车辆跟随。

Abstract: The increase in vehicle ownership has led to increased traffic congestion, more accidents, and higher carbon emissions. Vehicle platooning is a promising solution to address these issues by improving road capacity and reducing fuel consumption. However, existing platooning systems face challenges such as reliance on lane markings and expensive high-precision sensors, which limits their general applicability. To address these issues, we propose a vehicle following framework that expands its capability from restricted scenarios to general scenario applications using only a camera. This is achieved through our newly proposed end-to-end method, which improves overall driving performance. The method incorporates a semantic mask to address causal confusion in multi-frame data fusion. Additionally, we introduce a dynamic sampling mechanism to precisely track the trajectories of preceding vehicles. Extensive closed-loop validation in real-world vehicle experiments demonstrates the system's ability to follow vehicles in various scenarios, outperforming traditional multi-stage algorithms. This makes it a promising solution for cost-effective autonomous vehicle platooning. A complete real-world vehicle experiment is available at https://youtu.be/zL1bcVb9kqQ.

</details>


### [56] [Multi-Agent Monocular Dense SLAM With 3D Reconstruction Priors](https://arxiv.org/abs/2511.19031)
*Haihang Wu,Yuchen Zhou*

Main category: cs.RO

TL;DR: 本文提出了首个多智能体单目稠密SLAM系统，通过3D重建先验和基于闭环的地图融合机制，在保持精度的同时提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有单目SLAM系统计算成本高，且MASt3R-SLAM仅限于单智能体操作，需要扩展到多智能体场景。

Method: 每个智能体使用3D重建先验进行局部SLAM，通过基于闭环的地图融合机制将个体地图融合为全局一致地图。

Result: 在真实世界数据集上的评估显示，相比最先进方法，该方法提高了计算效率，同时保持了相似的建图精度。

Conclusion: 成功实现了首个多智能体单目稠密SLAM系统，在效率和精度之间取得了良好平衡。

Abstract: Monocular Simultaneous Localization and Mapping (SLAM) aims to estimate a robot's pose while simultaneously reconstructing an unknown 3D scene using a single camera. While existing monocular SLAM systems generate detailed 3D geometry through dense scene representations, they are computationally expensive due to the need for iterative optimization. To address this challenge, MASt3R-SLAM utilizes learned 3D reconstruction priors, enabling more efficient and accurate estimation of both 3D structures and camera poses. However, MASt3R-SLAM is limited to single-agent operation. In this paper, we extend MASt3R-SLAM to introduce the first multi-agent monocular dense SLAM system. Each agent performs local SLAM using a 3D reconstruction prior, and their individual maps are fused into a globally consistent map through a loop-closure-based map fusion mechanism. Our approach improves computational efficiency compared to state-of-the-art methods, while maintaining similar mapping accuracy when evaluated on real-world datasets.

</details>


### [57] [Analysis of Deep-Learning Methods in an ISO/TS 15066-Compliant Human-Robot Safety Framework](https://arxiv.org/abs/2511.19094)
*David Bricher,Andreas Mueller*

Main category: cs.RO

TL;DR: 提出基于深度学习的人机安全框架，通过动态调整机器人速度来提升协作效率，相比传统安全技术可减少15%周期时间


<details>
  <summary>Details</summary>
Motivation: 当前符合ISO/TS-15066标准的人机协作实现因保守的速度限制而影响任务效率，需要更智能的安全框架

Method: 开发深度学习人机安全框架，使用人体识别、分割、姿态估计和身体部位分割四种方法进行人体提取，根据人机距离动态调整速度

Result: 实验显示相比传统安全技术，周期时间最多可减少15%

Conclusion: 该框架能够区分人体各部位与其他物体，实现优化的机器人流程执行，在保证安全的同时显著提升协作效率

Abstract: Over the last years collaborative robots have gained great success in manufacturing applications where human and robot work together in close proximity. However, current ISO/TS-15066-compliant implementations often limit the efficiency of collaborative tasks due to conservative speed restrictions. For this reason, this paper introduces a deep-learning-based human-robot-safety framework (HRSF) that aims at a dynamical adaptation of robot velocities depending on the separation distance between human and robot while respecting maximum biomechanical force and pressure limits. The applicability of the framework was investigated for four different deep learning approaches that can be used for human body extraction: human body recognition, human body segmentation, human pose estimation, and human body part segmentation. Unlike conventional industrial safety systems, the proposed HRSF differentiates individual human body parts from other objects, enabling optimized robot process execution. Experiments demonstrated a quantitative reduction in cycle time of up to 15% compared to conventional safety technology.

</details>


### [58] [Autonomous Docking of Multi-Rotor UAVs on Blimps under the Influence of Wind Gusts](https://arxiv.org/abs/2511.19135)
*Pascal Goldschmid,Aamir Ahmad*

Main category: cs.RO

TL;DR: 提出了一种多旋翼无人机在飞艇上自主对接的方法，通过时间卷积网络预测飞艇对风扰动的响应，并结合模型预测控制器计算无碰撞对接轨迹。


<details>
  <summary>Details</summary>
Motivation: 多旋翼无人机受限于电池续航时间，通过在飞艇上进行电池充电和数据传输可以延长任务时间，但飞艇易受风扰导致轨迹偏差，需要精确的对接策略。

Method: 使用时间卷积网络预测飞艇对风扰动的响应，结合模型预测控制器计算无碰撞对接轨迹，并采用新颖的近距离避障方法。

Result: 仿真结果显示该方法显著优于基于恒定速度模型的基线方法，并在真实实验中首次实现了多旋翼无人机在飞艇上的自主对接控制。

Conclusion: 该方法成功解决了风扰动下多旋翼无人机在飞艇上的自主对接问题，为延长无人机任务时间提供了可行方案。

Abstract: Multi-rotor UAVs face limited flight time due to battery constraints. Autonomous docking on blimps with onboard battery recharging and data offloading offers a promising solution for extended UAV missions. However, the vulnerability of blimps to wind gusts causes trajectory deviations, requiring precise, obstacle-aware docking strategies. To this end, this work introduces two key novelties: (i) a temporal convolutional network that predicts blimp responses to wind gusts, enabling rapid gust detection and estimation of points where the wind gust effect has subsided; (ii) a model predictive controller (MPC) that leverages these predictions to compute collision-free trajectories for docking, enabled by a novel obstacle avoidance method for close-range manoeuvres near the blimp. Simulation results show our method outperforms a baseline constant-velocity model of the blimp significantly across different scenarios. We further validate the approach in real-world experiments, demonstrating the first autonomous multi-rotor docking control strategy on blimps shown outside simulation. Source code is available here https://github.com/robot-perception-group/multi_rotor_airship_docking.

</details>


### [59] [Efficient Optimization of a Permanent Magnet Array for a Stable 2D Trap](https://arxiv.org/abs/2511.19201)
*Ann-Sophia Müller,Moonkwang Jeong,Jiyuan Tian,Meng Zhang,Tian Qiu*

Main category: cs.RO

TL;DR: 提出了一种使用永磁体阵列实现稳定2D磁力陷阱的方法，用于控制医疗微型机器人，通过GPU加速优化算法高效计算磁体最佳角度。


<details>
  <summary>Details</summary>
Motivation: 解决在医疗应用中远距离对小型机器人施加高驱动力的挑战，同时克服永磁体反馈控制困难的问题。

Method: 开发了基于均方误差和Adam优化器的GPU加速优化算法，计算永磁体阵列的最佳角度配置，实现稳定的2D磁力陷阱。

Result: 成功在20-120mm距离范围内实现稳定磁力陷阱，能够控制微型机器人完成复杂轨迹跟踪，算法可扩展到100个磁体的优化。

Conclusion: 该算法为永磁体阵列控制医疗微型机器人提供了高效可扩展的解决方案，能够适应人体解剖尺度需求。

Abstract: Untethered magnetic manipulation of biomedical millirobots has a high potential for minimally invasive surgical applications. However, it is still challenging to exert high actuation forces on the small robots over a large distance. Permanent magnets offer stronger magnetic torques and forces than electromagnetic coils, however, feedback control is more difficult. As proven by Earnshaw's theorem, it is not possible to achieve a stable magnetic trap in 3D by static permanent magnets. Here, we report a stable 2D magnetic force trap by an array of permanent magnets to control a millirobot. The trap is located in an open space with a tunable distance to the magnet array in the range of 20 - 120mm, which is relevant to human anatomical scales. The design is achieved by a novel GPU-accelerated optimization algorithm that uses mean squared error (MSE) and Adam optimizer to efficiently compute the optimal angles for any number of magnets in the array. The algorithm is verified using numerical simulation and physical experiments with an array of two magnets. A millirobot is successfully trapped and controlled to follow a complex trajectory. The algorithm demonstrates high scalability by optimizing the angles for 100 magnets in under three seconds. Moreover, the optimization workflow can be adapted to optimize a permanent magnet array to achieve the desired force vector fields.

</details>


### [60] [Reference-Free Sampling-Based Model Predictive Control](https://arxiv.org/abs/2511.19204)
*Fabian Schramm,Pierre Fabre,Nicolas Perrin-Gilbert,Justin Carpentier*

Main category: cs.RO

TL;DR: 提出了一种基于采样的模型预测控制框架，无需依赖手工步态模式或预定义接触序列，通过优化高层目标自动发现多样化运动模式，包括小跑、疾跑、站立、跳跃和倒立平衡等。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要手工设计步态模式和接触序列，限制了机器人的运动多样性和适应性。本文旨在通过纯优化方法实现自然涌现的多样化运动能力。

Method: 基于模型预测路径积分(MPPI)，提出双空间样条参数化方法，在位置和速度控制点上操作，实现自适应接触策略，仅需有限采样轨迹即可实现实时控制。

Result: 在Go2四足机器人上验证了多种涌现步态和基本跳跃能力，在仿真中展示了更复杂行为如后空翻、动态倒立平衡和人形机器人运动，无需参考跟踪或离线预训练。

Conclusion: 该方法实现了实时控制，无需GPU加速，能够自动适应任务需求，为机器人运动控制提供了灵活高效的解决方案。

Abstract: We present a sampling-based model predictive control (MPC) framework that enables emergent locomotion without relying on handcrafted gait patterns or predefined contact sequences. Our method discovers diverse motion patterns, ranging from trotting to galloping, robust standing policies, jumping, and handstand balancing, purely through the optimization of high-level objectives. Building on model predictive path integral (MPPI), we propose a dual-space spline parameterization that operates on position and velocity control points. Our approach enables contact-making and contact-breaking strategies that adapt automatically to task requirements, requiring only a limited number of sampled trajectories. This sample efficiency allows us to achieve real-time control on standard CPU hardware, eliminating the need for GPU acceleration typically required by other state-of-the-art MPPI methods. We validate our approach on the Go2 quadrupedal robot, demonstrating various emergent gaits and basic jumping capabilities. In simulation, we further showcase more complex behaviors, such as backflips, dynamic handstand balancing and locomotion on a Humanoid, all without requiring reference tracking or offline pre-training.

</details>


### [61] [Soft pneumatic grippers: Topology optimization, 3D-printing and experimental validation](https://arxiv.org/abs/2511.19211)
*Prabhat Kumar,Chandra Prakash,Josh Pinskier,David Howard,Matthijs Langelaar*

Main category: cs.RO

TL;DR: 提出了一个系统性的拓扑优化框架来设计软体气动夹爪，明确考虑了驱动载荷的设计依赖性，使用Darcy定律建模，通过2D软臂单元优化和3D组装实现高性能夹持。


<details>
  <summary>Details</summary>
Motivation: 传统软体气动夹爪设计缺乏系统性优化方法，需要开发考虑设计依赖载荷的拓扑优化框架来提升性能。

Method: 采用稳健公式将2D软臂单元建模为柔顺机构设计问题，使用Darcy定律建模气动载荷，通过min-max优化和MMA算法求解，最后将优化单元挤出为3D模块并组装成夹爪。

Result: 优化的2D单元在气动载荷下性能优于传统矩形设计，组装成的软体夹爪能够有效夹持不同重量、尺寸、刚度和形状的物体。

Conclusion: 提出的拓扑优化框架成功实现了高性能软体气动夹爪的设计，验证了考虑设计依赖载荷的优化方法的有效性。

Abstract: This paper presents a systematic topology optimization framework for designing a soft pneumatic gripper (SPG), explicitly considering the design-dependent nature of the actuating load. The load is modeled using Darcy's law with an added drainage term. A 2D soft arm unit is optimized by formulating it as a compliant mechanism design problem using the robust formulation. The problem is posed as a min-max optimization, where the output deformations of blueprint and eroded designs are considered. A volume constraint is imposed on the blueprint part, while a strain-energy constraint is enforced on the eroded part. The MMA is employed to solve the optimization problem and obtain the optimized soft unit. Finite element analysis with the Ogden material model confirms that the optimized 2D unit outperforms a conventional rectangular design under pneumatic loading. The optimized 2D unit is extruded to obtain a 3D module, and ten such units are assembled to create a soft arm. Deformation profiles of the optimized arm are analysed under different pressure loads. Four arms are 3D-printed and integrated with a supporting structure to realize the proposed SPG. The gripping performance of the SPG is demonstrated on objects with different weights, sizes, stiffness, and shapes.

</details>


### [62] [SENTINEL: A Fully End-to-End Language-Action Model for Humanoid Whole Body Control](https://arxiv.org/abs/2511.19236)
*Yuxuan Wang,Haobin Jiang,Shiqing Yao,Ziluo Ding,Zongqing Lu*

Main category: cs.RO

TL;DR: SENTINEL是一个端到端的语言-动作模型，用于人形机器人全身控制，直接将语言命令和本体感觉输入映射到低级动作，无需中间表示。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人控制系统要么依赖遥操作（完全由人类驱动），要么采用模块化生成管道（将语言理解与物理执行分离），缺乏语言命令与物理行为之间的紧密对齐。

Method: 通过在仿真中使用预训练的全身控制器跟踪人类运动并添加文本注释构建大规模数据集。模型使用流匹配生成动作块，并通过残差动作头进行细化以适应真实世界部署。

Result: 该方法在仿真和真实世界部署中表现出强大的语义理解和稳定执行能力，并支持通过将输入转换为文本来实现多模态扩展。

Conclusion: SENTINEL提供了一种完全端到端的语言-动作模型，能够实现语言命令与物理行为之间的紧密对齐，在人形机器人控制方面表现出色。

Abstract: Existing humanoid control systems often rely on teleoperation or modular generation pipelines that separate language understanding from physical execution. However, the former is entirely human-driven, and the latter lacks tight alignment between language commands and physical behaviors. In this paper, we present SENTINEL, a fully end-to-end language-action model for humanoid whole-body control. We construct a large-scale dataset by tracking human motions in simulation using a pretrained whole body controller, combined with their text annotations. The model directly maps language commands and proprioceptive inputs to low-level actions without any intermediate representation. The model generates action chunks using flow matching, which can be subsequently refined by a residual action head for real-world deployment. Our method exhibits strong semantic understanding and stable execution on humanoid robots in both simulation and real-world deployment, and also supports multi-modal extensions by converting inputs into texts.

</details>


### [63] [Rethinking Intermediate Representation for VLM-based Robot Manipulation](https://arxiv.org/abs/2511.19315)
*Weiliang Tang,Jialin Gao,Jia-Hui Pan,Gang Wang,Li Erran Li,Yunhui Liu,Mingyu Ding,Pheng-Ann Heng,Chi-Wing Fu*

Main category: cs.RO

TL;DR: 提出了SEAM表示方法，将中间表示分解为词汇表和语法，实现VLM可理解性和泛化性的平衡，并设计了开放词汇分割范式来定位细粒度物体部件。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在机器人操作中翻译人类指令时，VLM可理解性和任务泛化性之间的权衡问题。

Method: 受上下文无关语法启发，设计SEAM语义装配表示，将中间表示分解为语义丰富的操作词汇表和VLM友好的语法；提出检索增强的少样本学习开放词汇分割范式。

Result: 在动作泛化性和VLM可理解性两方面都优于主流表示方法，在真实世界实验中展现出最先进的性能，推理时间最短。

Conclusion: SEAM表示方法通过词汇表和语法的分解，有效平衡了VLM可理解性和任务泛化性，为机器人操作提供了高效的中间表示解决方案。

Abstract: Vision-Language Model (VLM) is an important component to enable robust robot manipulation. Yet, using it to translate human instructions into an action-resolvable intermediate representation often needs a tradeoff between VLM-comprehensibility and generalizability. Inspired by context-free grammar, we design the Semantic Assembly representation named SEAM, by decomposing the intermediate representation into vocabulary and grammar. Doing so leads us to a concise vocabulary of semantically-rich operations and a VLM-friendly grammar for handling diverse unseen tasks. In addition, we design a new open-vocabulary segmentation paradigm with a retrieval-augmented few-shot learning strategy to localize fine-grained object parts for manipulation, effectively with the shortest inference time over all state-of-the-art parallel works. Also, we formulate new metrics for action-generalizability and VLM-comprehensibility, demonstrating the compelling performance of SEAM over mainstream representations on both aspects. Extensive real-world experiments further manifest its SOTA performance under varying settings and tasks.

</details>


### [64] [Deployment Dynamics and Optimization of Novel Space Antenna Deployable Mechanism](https://arxiv.org/abs/2511.19377)
*Mamoon Aamir,Mariyam Sattar,Naveed Ur Rehman Junejo,Aqsa Zafar Abbasi*

Main category: cs.RO

TL;DR: 提出了一种新型三重剪刀可展开桁架机构（TSDTM），用于空间天线任务，可在发射时收起并在轨道上高效展开，实现最大孔径尺寸和最小发射体积。


<details>
  <summary>Details</summary>
Motivation: 随着空间任务对大孔径天线需求的增加，需要设计可展开天线系统来解决大型结构难以装入小型运载火箭的问题。

Method: 采用几何建模、螺旋理论和牛顿方法进行运动学分析，特征值和仿真方法进行动力学分析，并通过SolidWorks验证。基于支持向量机进行材料选择优化，使用机器学习方法进行几何设置优化。

Result: TSDTM具有增强的结构动力学性能，仿真与解析预测结果良好吻合。优化结构精度高，机器学习预测与仿真自然频率偏差仅为1.94%。

Conclusion: 该研究展示了将人工智能方法融入空间结构设计的潜力，为大型可展开空间天线提供了有效的解决方案。

Abstract: Given the increasing need for large aperture antennas in space missions, the difficulty of fitting such structures into small launch vehicles has prompted the design of deployable antenna systems. The thesis introduces a new Triple Scissors Deployable Truss Mechanism (TSDTM) for space antenna missions. The new mechanism is to be stowed during launch and efficiently deploy in orbit, offering maximum aperture size while taking up minimal launch volume. The thesis covers the entire design process from geometric modeling, kinematic analysis with screw theory and Newtonian approaches, dynamic analysis by eigenvalue and simulation methods, and verification with SolidWorks. In addition, optimization routines were coded based on Support Vector Machines for material choice in LEO environments and machine learning method for geometric setup. The TSDTM presented has enhanced structural dynamics with good comparison between simulation and analytical predictions. The structure optimized proved highly accurate, with a deviation of just 1.94% between machine learning-predicted and simulated natural frequencies, demonstrating the potential of incorporating AI-based methods in space structural design.

</details>


### [65] [Mixture of Horizons in Action Chunking](https://arxiv.org/abs/2511.19433)
*Dong Jing,Gang Wang,Jiaqi Liu,Weiliang Tang,Zelong Sun,Yunchao Yao,Zhenyu Wei,Yunhui Liu,Zhiwu Lu,Mingyu Ding*

Main category: cs.RO

TL;DR: 提出混合视野（MoH）策略解决VLA模型中行动块长度选择的权衡问题，通过并行处理不同视野长度的行动段并融合输出，同时获得长期预见性和短期精确性。


<details>
  <summary>Details</summary>
Motivation: 传统VLA模型在行动块长度选择上存在固有权衡：较长视野提供全局预见性但降低细粒度精度，较短视野提升局部控制但难以处理长期任务，单一固定视野选择是次优的。

Method: 将行动块重新排列为不同视野长度的多个段，使用共享行动变换器并行处理，通过轻量线性门融合输出，实现动态推理和自适应视野选择。

Result: 在多种策略（π₀、π₀.₅、π_reg）上均获得一致显著提升，在混合任务设置下π₀.₅+MoH在LIBERO上达到99%平均成功率的新SOTA，推理吞吐量比基线高2.5倍。

Conclusion: MoH策略有效缓解了行动块长度选择的权衡问题，在保持高性能的同时显著提升推理效率，为VLA模型提供了更优的解决方案。

Abstract: Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\textbf{action chunk length}$ used during training, termed $\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $π_0$, $π_{0.5}$, and one-step regression policy $π_{\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $π_{0.5}$ with MoH reaches a new state-of-the-art with 99$\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons

</details>
