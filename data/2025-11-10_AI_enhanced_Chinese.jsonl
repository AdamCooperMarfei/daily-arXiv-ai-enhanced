{"id": "2511.04758", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.04758", "abs": "https://arxiv.org/abs/2511.04758", "authors": ["Caelan Garrett", "Fabio Ramos"], "title": "ScheduleStream: Temporal Planning with Samplers for GPU-Accelerated Multi-Arm Task and Motion Planning & Scheduling", "comment": "Project website: https://schedulestream.github.io", "summary": "Bimanual and humanoid robots are appealing because of their human-like\nability to leverage multiple arms to efficiently complete tasks. However,\ncontrolling multiple arms at once is computationally challenging due to the\ngrowth in the hybrid discrete-continuous action space. Task and Motion Planning\n(TAMP) algorithms can efficiently plan in hybrid spaces but generally produce\nplans, where only one arm is moving at a time, rather than schedules that allow\nfor parallel arm motion. In order to extend TAMP to produce schedules, we\npresent ScheduleStream, the first general-purpose framework for planning &\nscheduling with sampling operations. ScheduleStream models temporal dynamics\nusing hybrid durative actions, which can be started asynchronously and persist\nfor a duration that's a function of their parameters. We propose\ndomain-independent algorithms that solve ScheduleStream problems without any\napplication-specific mechanisms. We apply ScheduleStream to Task and Motion\nPlanning & Scheduling (TAMPAS), where we use GPU acceleration within samplers\nto expedite planning. We compare ScheduleStream algorithms to several ablations\nin simulation and find that they produce more efficient solutions. We\ndemonstrate ScheduleStream on several real-world bimanual robot tasks at\nhttps://schedulestream.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e86ScheduleStream\u6846\u67b6\uff0c\u9996\u4e2a\u7528\u4e8e\u89c4\u5212\u4e0e\u8c03\u5ea6\u7684\u901a\u7528\u6846\u67b6\uff0c\u652f\u6301\u91c7\u6837\u64cd\u4f5c\uff0c\u80fd\u591f\u4e3a\u53cc\u673a\u68b0\u81c2\u548c\u4eba\u5f62\u673a\u5668\u4eba\u751f\u6210\u5e76\u884c\u8fd0\u52a8\u8c03\u5ea6\uff0c\u800c\u975e\u4f20\u7edf\u7684\u987a\u5e8f\u89c4\u5212\u3002", "motivation": "\u53cc\u673a\u68b0\u81c2\u548c\u4eba\u5f62\u673a\u5668\u4eba\u9700\u8981\u591a\u81c2\u534f\u540c\u5b8c\u6210\u4efb\u52a1\uff0c\u4f46\u4f20\u7edf\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212(TAMP)\u7b97\u6cd5\u53ea\u80fd\u751f\u6210\u987a\u5e8f\u6267\u884c\u8ba1\u5212\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5e76\u884c\u8fd0\u52a8\uff0c\u9650\u5236\u4e86\u6548\u7387\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u6301\u7eed\u52a8\u4f5c\u5efa\u6a21\u65f6\u95f4\u52a8\u6001\uff0c\u652f\u6301\u5f02\u6b65\u542f\u52a8\u548c\u53c2\u6570\u5316\u6301\u7eed\u65f6\u95f4\uff0c\u63d0\u51fa\u9886\u57df\u65e0\u5173\u7b97\u6cd5\u89e3\u51b3ScheduleStream\u95ee\u9898\uff0c\u5728TAMPAS\u4e2d\u5229\u7528GPU\u52a0\u901f\u91c7\u6837\u5668\u52a0\u5feb\u89c4\u5212\u901f\u5ea6\u3002", "result": "\u4e0e\u591a\u4e2a\u6d88\u878d\u5b9e\u9a8c\u76f8\u6bd4\uff0cScheduleStream\u7b97\u6cd5\u80fd\u751f\u6210\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u53cc\u673a\u68b0\u81c2\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u6f14\u793a\u9a8c\u8bc1\u3002", "conclusion": "ScheduleStream\u6210\u529f\u6269\u5c55\u4e86TAMP\u6846\u67b6\uff0c\u80fd\u591f\u4e3a\u591a\u81c2\u673a\u5668\u4eba\u7cfb\u7edf\u751f\u6210\u5e76\u884c\u8fd0\u52a8\u8c03\u5ea6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6267\u884c\u6548\u7387\u3002"}}
{"id": "2511.04769", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04769", "abs": "https://arxiv.org/abs/2511.04769", "authors": ["Phat Nguyen", "Tsun-Hsuan Wang", "Zhang-Wei Hong", "Erfan Aasi", "Andrew Silva", "Guy Rosman", "Sertac Karaman", "Daniela Rus"], "title": "ReGen: Generative Robot Simulation via Inverse Design", "comment": null, "summary": "Simulation plays a key role in scaling robot learning and validating\npolicies, but constructing simulations remains a labor-intensive process. This\npaper introduces ReGen, a generative simulation framework that automates\nsimulation design via inverse design. Given a robot's behavior -- such as a\nmotion trajectory or an objective function -- and its textual description,\nReGen infers plausible scenarios and environments that could have caused the\nbehavior. ReGen leverages large language models to synthesize scenarios by\nexpanding a directed graph that encodes cause-and-effect relationships,\nrelevant entities, and their properties. This structured graph is then\ntranslated into a symbolic program, which configures and executes a robot\nsimulation environment. Our framework supports (i) augmenting simulations based\non ego-agent behaviors, (ii) controllable, counterfactual scenario generation,\n(iii) reasoning about agent cognition and mental states, and (iv) reasoning\nwith distinct sensing modalities, such as braking due to faulty GPS signals. We\ndemonstrate ReGen in autonomous driving and robot manipulation tasks,\ngenerating more diverse, complex simulated environments compared to existing\nsimulations with high success rates, and enabling controllable generation for\ncorner cases. This approach enhances the validation of robot policies and\nsupports data or simulation augmentation, advancing scalable robot learning for\nimproved generalization and robustness. We provide code and example videos at:\nhttps://regen-sim.github.io/", "AI": {"tldr": "ReGen\u662f\u4e00\u4e2a\u751f\u6210\u5f0f\u4eff\u771f\u6846\u67b6\uff0c\u901a\u8fc7\u9006\u5411\u8bbe\u8ba1\u81ea\u52a8\u5316\u4eff\u771f\u6784\u5efa\u8fc7\u7a0b\u3002\u7ed9\u5b9a\u673a\u5668\u4eba\u7684\u884c\u4e3a\u8f68\u8ff9\u6216\u76ee\u6807\u51fd\u6570\u53ca\u5176\u6587\u672c\u63cf\u8ff0\uff0cReGen\u80fd\u591f\u63a8\u65ad\u51fa\u53ef\u80fd\u5bfc\u81f4\u8be5\u884c\u4e3a\u7684\u5408\u7406\u573a\u666f\u548c\u73af\u5883\u3002", "motivation": "\u4f20\u7edf\u4eff\u771f\u6784\u5efa\u8fc7\u7a0b\u52b3\u52a8\u5bc6\u96c6\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u7684\u6269\u5c55\u6027\u548c\u7b56\u7565\u9a8c\u8bc1\u3002\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u751f\u6210\u591a\u6837\u5316\u7684\u4eff\u771f\u73af\u5883\uff0c\u4ee5\u589e\u5f3a\u673a\u5668\u4eba\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u6269\u5c55\u7f16\u7801\u56e0\u679c\u5173\u7cfb\u7684\u5b9a\u5411\u56fe\u6765\u5408\u6210\u573a\u666f\uff0c\u7136\u540e\u5c06\u7ed3\u6784\u5316\u56fe\u8f6c\u6362\u4e3a\u7b26\u53f7\u7a0b\u5e8f\uff0c\u914d\u7f6e\u548c\u6267\u884c\u673a\u5668\u4eba\u4eff\u771f\u73af\u5883\u3002\u652f\u6301\u57fa\u4e8e\u81ea\u6211\u884c\u4e3a\u589e\u5f3a\u4eff\u771f\u3001\u53ef\u63a7\u53cd\u4e8b\u5b9e\u573a\u666f\u751f\u6210\u3001\u63a8\u7406\u4ee3\u7406\u8ba4\u77e5\u548c\u4e0d\u540c\u611f\u77e5\u6a21\u6001\u3002", "result": "\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cReGen\u76f8\u6bd4\u73b0\u6709\u4eff\u771f\u65b9\u6cd5\u751f\u6210\u4e86\u66f4\u591a\u6837\u5316\u3001\u590d\u6742\u7684\u4eff\u771f\u73af\u5883\uff0c\u5177\u6709\u9ad8\u6210\u529f\u7387\uff0c\u5e76\u80fd\u53ef\u63a7\u5730\u751f\u6210\u6781\u7aef\u60c5\u51b5\u3002", "conclusion": "ReGen\u901a\u8fc7\u81ea\u52a8\u5316\u4eff\u771f\u8bbe\u8ba1\u589e\u5f3a\u4e86\u673a\u5668\u4eba\u7b56\u7565\u9a8c\u8bc1\uff0c\u652f\u6301\u6570\u636e\u548c\u4eff\u771f\u589e\u5f3a\uff0c\u63a8\u52a8\u4e86\u53ef\u6269\u5c55\u7684\u673a\u5668\u4eba\u5b66\u4e60\uff0c\u4ee5\u6539\u5584\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.04812", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04812", "abs": "https://arxiv.org/abs/2511.04812", "authors": ["Zixuan Huang", "Huaidian Hou", "Dmitry Berenson"], "title": "Unified Multimodal Diffusion Forcing for Forceful Manipulation", "comment": "Project website: https://unified-df.github.io", "summary": "Given a dataset of expert trajectories, standard imitation learning\napproaches typically learn a direct mapping from observations (e.g., RGB\nimages) to actions. However, such methods often overlook the rich interplay\nbetween different modalities, i.e., sensory inputs, actions, and rewards, which\nis crucial for modeling robot behavior and understanding task outcomes. In this\nwork, we propose Multimodal Diffusion Forcing, a unified framework for learning\nfrom multimodal robot trajectories that extends beyond action generation.\nRather than modeling a fixed distribution, MDF applies random partial masking\nand trains a diffusion model to reconstruct the trajectory. This training\nobjective encourages the model to learn temporal and cross-modal dependencies,\nsuch as predicting the effects of actions on force signals or inferring states\nfrom partial observations. We evaluate MDF on contact-rich, forceful\nmanipulation tasks in simulated and real-world environments. Our results show\nthat MDF not only delivers versatile functionalities, but also achieves strong\nperformance, and robustness under noisy observations. More visualizations can\nbe found on our website https://unified-df.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u6269\u6563\u5f3a\u5236(MDF)\u6846\u67b6\uff0c\u901a\u8fc7\u968f\u673a\u90e8\u5206\u63a9\u7801\u548c\u6269\u6563\u6a21\u578b\u91cd\u5efa\u8f68\u8ff9\u6765\u5b66\u4e60\u591a\u6a21\u6001\u673a\u5668\u4eba\u8f68\u8ff9\u4e2d\u7684\u65f6\u7a7a\u548c\u8de8\u6a21\u6001\u4f9d\u8d56\u5173\u7cfb\u3002", "motivation": "\u6807\u51c6\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u53ea\u5b66\u4e60\u4ece\u89c2\u5bdf\u5230\u52a8\u4f5c\u7684\u76f4\u63a5\u6620\u5c04\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u6a21\u6001\uff08\u611f\u5b98\u8f93\u5165\u3001\u52a8\u4f5c\u3001\u5956\u52b1\uff09\u4e4b\u95f4\u7684\u4e30\u5bcc\u4ea4\u4e92\uff0c\u800c\u8fd9\u4e9b\u4ea4\u4e92\u5bf9\u4e8e\u5efa\u6a21\u673a\u5668\u4eba\u884c\u4e3a\u548c\u7406\u89e3\u4efb\u52a1\u7ed3\u679c\u81f3\u5173\u91cd\u8981\u3002", "method": "MDF\u91c7\u7528\u968f\u673a\u90e8\u5206\u63a9\u7801\u7b56\u7565\uff0c\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u6765\u91cd\u5efa\u8f68\u8ff9\uff0c\u800c\u4e0d\u662f\u5efa\u6a21\u56fa\u5b9a\u5206\u5e03\u3002\u8fd9\u79cd\u8bad\u7ec3\u76ee\u6807\u9f13\u52b1\u6a21\u578b\u5b66\u4e60\u65f6\u7a7a\u548c\u8de8\u6a21\u6001\u4f9d\u8d56\u5173\u7cfb\uff0c\u5982\u9884\u6d4b\u52a8\u4f5c\u5bf9\u529b\u4fe1\u53f7\u7684\u5f71\u54cd\u6216\u4ece\u90e8\u5206\u89c2\u5bdf\u63a8\u65ad\u72b6\u6001\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u7684\u63a5\u89e6\u4e30\u5bcc\u3001\u5f3a\u529b\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cMDF\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u591a\u529f\u80fd\u6027\uff0c\u8fd8\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u548c\u566a\u58f0\u89c2\u5bdf\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "MDF\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u673a\u5668\u4eba\u8f68\u8ff9\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5b66\u4e60\u8de8\u6a21\u6001\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.04827", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.04827", "abs": "https://arxiv.org/abs/2511.04827", "authors": ["Tobias Fischer", "Wolf Vollprecht", "Bas Zalmstra", "Ruben Arts", "Tim de Jager", "Alejandro Fontan", "Adam D Hines", "Michael Milford", "Silvio Traversaro", "Daniel Claes", "Scarlett Raine"], "title": "Pixi: Unified Software Development and Distribution for Robotics and AI", "comment": "20 pages, 3 figures, 11 code snippets", "summary": "The reproducibility crisis in scientific computing constrains robotics\nresearch. Existing studies reveal that up to 70% of robotics algorithms cannot\nbe reproduced by independent teams, while many others fail to reach deployment\nbecause creating shareable software environments remains prohibitively complex.\nThese challenges stem from fragmented, multi-language, and hardware-software\ntoolchains that lead to dependency hell. We present Pixi, a unified\npackage-management framework that addresses these issues by capturing exact\ndependency states in project-level lockfiles, ensuring bit-for-bit\nreproducibility across platforms. Its high-performance SAT solver achieves up\nto 10x faster dependency resolution than comparable tools, while integration of\nthe conda-forge and PyPI ecosystems removes the need for multiple managers.\nAdopted in over 5,300 projects since 2023, Pixi reduces setup times from hours\nto minutes and lowers technical barriers for researchers worldwide. By enabling\nscalable, reproducible, collaborative research infrastructure, Pixi accelerates\nprogress in robotics and AI.", "AI": {"tldr": "Pixi\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5305\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u9879\u76ee\u7ea7\u9501\u6587\u4ef6\u786e\u4fdd\u8de8\u5e73\u53f0\u7684\u6bd4\u7279\u7ea7\u53ef\u590d\u73b0\u6027\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u5b66\u7814\u7a76\u4e2d\u7684\u53ef\u590d\u73b0\u6027\u5371\u673a\u3002", "motivation": "\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684\u53ef\u590d\u73b0\u6027\u5371\u673a\u9650\u5236\u4e86\u673a\u5668\u4eba\u5b66\u7814\u7a76\uff0c\u9ad8\u8fbe70%\u7684\u673a\u5668\u4eba\u7b97\u6cd5\u65e0\u6cd5\u88ab\u72ec\u7acb\u56e2\u961f\u590d\u73b0\uff0c\u4e14\u521b\u5efa\u53ef\u5171\u4eab\u7684\u8f6f\u4ef6\u73af\u5883\u8fc7\u4e8e\u590d\u6742\u3002", "method": "Pixi\u4f7f\u7528\u9ad8\u6027\u80fdSAT\u6c42\u89e3\u5668\u8fdb\u884c\u4f9d\u8d56\u89e3\u6790\uff08\u6bd4\u540c\u7c7b\u5de5\u5177\u5feb10\u500d\uff09\uff0c\u6574\u5408conda-forge\u548cPyPI\u751f\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u9879\u76ee\u7ea7\u9501\u6587\u4ef6\u6355\u83b7\u7cbe\u786e\u4f9d\u8d56\u72b6\u6001\u3002", "result": "\u81ea2023\u5e74\u4ee5\u6765\u5df2\u57285,300\u591a\u4e2a\u9879\u76ee\u4e2d\u91c7\u7528\uff0c\u5c06\u8bbe\u7f6e\u65f6\u95f4\u4ece\u6570\u5c0f\u65f6\u7f29\u77ed\u81f3\u6570\u5206\u949f\uff0c\u964d\u4f4e\u4e86\u5168\u7403\u7814\u7a76\u8005\u7684\u6280\u672f\u95e8\u69db\u3002", "conclusion": "Pixi\u901a\u8fc7\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u53ef\u590d\u73b0\u7684\u534f\u4f5c\u7814\u7a76\u57fa\u7840\u8bbe\u65bd\uff0c\u52a0\u901f\u4e86\u673a\u5668\u4eba\u548cAI\u9886\u57df\u7684\u8fdb\u5c55\u3002"}}
{"id": "2511.04831", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04831", "abs": "https://arxiv.org/abs/2511.04831", "authors": ["NVIDIA", ":", "Mayank Mittal", "Pascal Roth", "James Tigue", "Antoine Richard", "Octi Zhang", "Peter Du", "Antonio Serrano-Mu\u00f1oz", "Xinjie Yao", "Ren\u00e9 Zurbr\u00fcgg", "Nikita Rudin", "Lukasz Wawrzyniak", "Milad Rakhsha", "Alain Denzler", "Eric Heiden", "Ales Borovicka", "Ossama Ahmed", "Iretiayo Akinola", "Abrar Anwar", "Mark T. Carlson", "Ji Yuan Feng", "Animesh Garg", "Renato Gasoto", "Lionel Gulich", "Yijie Guo", "M. Gussert", "Alex Hansen", "Mihir Kulkarni", "Chenran Li", "Wei Liu", "Viktor Makoviychuk", "Grzegorz Malczyk", "Hammad Mazhar", "Masoud Moghani", "Adithyavairavan Murali", "Michael Noseworthy", "Alexander Poddubny", "Nathan Ratliff", "Welf Rehberg", "Clemens Schwarke", "Ritvik Singh", "James Latham Smith", "Bingjie Tang", "Ruchik Thaker", "Matthew Trepte", "Karl Van Wyk", "Fangzhou Yu", "Alex Millane", "Vikram Ramasamy", "Remo Steiner", "Sangeeta Subramanian", "Clemens Volk", "CY Chen", "Neel Jawale", "Ashwin Varghese Kuruttukulam", "Michael A. Lin", "Ajay Mandlekar", "Karsten Patzwaldt", "John Welsh", "Huihua Zhao", "Fatima Anes", "Jean-Francois Lafleche", "Nicolas Mo\u00ebnne-Loccoz", "Soowan Park", "Rob Stepinski", "Dirk Van Gelder", "Chris Amevor", "Jan Carius", "Jumyung Chang", "Anka He Chen", "Pablo de Heras Ciechomski", "Gilles Daviet", "Mohammad Mohajerani", "Julia von Muralt", "Viktor Reutskyy", "Michael Sauter", "Simon Schirm", "Eric L. Shi", "Pierre Terdiman", "Kenny Vilella", "Tobias Widmer", "Gordon Yeoman", "Tiffany Chen", "Sergey Grizan", "Cathy Li", "Lotus Li", "Connor Smith", "Rafael Wiltz", "Kostas Alexis", "Yan Chang", "David Chu", "Linxi \"Jim\" Fan", "Farbod Farshidian", "Ankur Handa", "Spencer Huang", "Marco Hutter", "Yashraj Narang", "Soha Pouya", "Shiwei Sheng", "Yuke Zhu", "Miles Macklin", "Adam Moravanszky", "Philipp Reist", "Yunrong Guo", "David Hoeller", "Gavriel State"], "title": "Isaac Lab: A GPU-Accelerated Simulation Framework for Multi-Modal Robot Learning", "comment": "Code and documentation are available here:\n  https://github.com/isaac-sim/IsaacLab", "summary": "We present Isaac Lab, the natural successor to Isaac Gym, which extends the\nparadigm of GPU-native robotics simulation into the era of large-scale\nmulti-modal learning. Isaac Lab combines high-fidelity GPU parallel physics,\nphotorealistic rendering, and a modular, composable architecture for designing\nenvironments and training robot policies. Beyond physics and rendering, the\nframework integrates actuator models, multi-frequency sensor simulation, data\ncollection pipelines, and domain randomization tools, unifying best practices\nfor reinforcement and imitation learning at scale within a single extensible\nplatform. We highlight its application to a diverse set of challenges,\nincluding whole-body control, cross-embodiment mobility, contact-rich and\ndexterous manipulation, and the integration of human demonstrations for skill\nacquisition. Finally, we discuss upcoming integration with the differentiable,\nGPU-accelerated Newton physics engine, which promises new opportunities for\nscalable, data-efficient, and gradient-based approaches to robot learning. We\nbelieve Isaac Lab's combination of advanced simulation capabilities, rich\nsensing, and data-center scale execution will help unlock the next generation\nof breakthroughs in robotics research.", "AI": {"tldr": "Isaac Lab\u662fIsaac Gym\u7684\u7ee7\u4efb\u8005\uff0c\u63d0\u4f9bGPU\u539f\u751f\u673a\u5668\u4eba\u4eff\u771f\u5e73\u53f0\uff0c\u652f\u6301\u5927\u89c4\u6a21\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u96c6\u6210\u4e86\u9ad8\u4fdd\u771f\u7269\u7406\u5f15\u64ce\u3001\u6e32\u67d3\u3001\u4f20\u611f\u5668\u6a21\u62df\u548c\u6570\u636e\u6536\u96c6\u7b49\u529f\u80fd\u3002", "motivation": "\u5c06GPU\u539f\u751f\u673a\u5668\u4eba\u4eff\u771f\u6269\u5c55\u5230\u5927\u89c4\u6a21\u591a\u6a21\u6001\u5b66\u4e60\u65f6\u4ee3\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u7edf\u4e00\u7684\u3001\u53ef\u6269\u5c55\u7684\u5e73\u53f0\u3002", "method": "\u7ed3\u5408GPU\u5e76\u884c\u7269\u7406\u5f15\u64ce\u3001\u7167\u7247\u7ea7\u6e32\u67d3\u3001\u6a21\u5757\u5316\u67b6\u6784\uff0c\u96c6\u6210\u6267\u884c\u5668\u6a21\u578b\u3001\u591a\u9891\u4f20\u611f\u5668\u4eff\u771f\u3001\u6570\u636e\u6536\u96c6\u7ba1\u9053\u548c\u9886\u57df\u968f\u673a\u5316\u5de5\u5177\u3002", "result": "\u6210\u529f\u5e94\u7528\u4e8e\u5168\u8eab\u63a7\u5236\u3001\u8de8\u4f53\u73b0\u79fb\u52a8\u3001\u63a5\u89e6\u4e30\u5bcc\u548c\u7075\u5de7\u64cd\u4f5c\u7b49\u591a\u6837\u5316\u6311\u6218\uff0c\u5e76\u6574\u5408\u4eba\u7c7b\u6f14\u793a\u8fdb\u884c\u6280\u80fd\u83b7\u53d6\u3002", "conclusion": "Isaac Lab\u7684\u5148\u8fdb\u4eff\u771f\u80fd\u529b\u3001\u4e30\u5bcc\u611f\u77e5\u548c\u6570\u636e\u4e2d\u5fc3\u7ea7\u6267\u884c\u5c06\u63a8\u52a8\u673a\u5668\u4eba\u7814\u7a76\u7684\u4e0b\u4e00\u4ee3\u7a81\u7834\uff0c\u672a\u6765\u5c06\u96c6\u6210\u53ef\u5fae\u5206\u7269\u7406\u5f15\u64ce\u4ee5\u652f\u6301\u68af\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2511.04835", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04835", "abs": "https://arxiv.org/abs/2511.04835", "authors": ["Shubham Natraj", "Bruno Sinopoli", "Yiannis Kantaros"], "title": "Conformalized Non-uniform Sampling Strategies for Accelerated Sampling-based Motion Planning", "comment": null, "summary": "Sampling-based motion planners (SBMPs) are widely used to compute dynamically\nfeasible robot paths. However, their reliance on uniform sampling often leads\nto poor efficiency and slow planning in complex environments. We introduce a\nnovel non-uniform sampling strategy that integrates into existing SBMPs by\nbiasing sampling toward `certified' regions. These regions are constructed by\n(i) generating an initial, possibly infeasible, path using any heuristic path\npredictor (e.g., A* or vision-language models) and (ii) applying conformal\nprediction to quantify the predictor's uncertainty. This process yields\nprediction sets around the initial-guess path that are guaranteed, with\nuser-specified probability, to contain the optimal solution. To our knowledge,\nthis is the first non-uniform sampling approach for SBMPs that provides such\nprobabilistically correct guarantees on the sampling regions. Extensive\nevaluations demonstrate that our method consistently finds feasible paths\nfaster and generalizes better to unseen environments than existing baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8ba4\u8bc1\u533a\u57df\u7684\u975e\u5747\u5300\u91c7\u6837\u7b56\u7565\uff0c\u901a\u8fc7\u7ed3\u5408\u542f\u53d1\u5f0f\u8def\u5f84\u9884\u6d4b\u5668\u548c\u4fdd\u5f62\u9884\u6d4b\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3aSBMPs\u63d0\u4f9b\u6982\u7387\u6b63\u786e\u7684\u91c7\u6837\u533a\u57df\u4fdd\u8bc1\uff0c\u663e\u8457\u63d0\u9ad8\u89c4\u5212\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u91c7\u6837\u7684\u8fd0\u52a8\u89c4\u5212\u5668\u4f9d\u8d56\u5747\u5300\u91c7\u6837\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u6548\u7387\u4f4e\u4e0b\u4e14\u89c4\u5212\u7f13\u6162\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u4f9b\u6982\u7387\u4fdd\u8bc1\u7684\u9ad8\u6548\u91c7\u6837\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u542f\u53d1\u5f0f\u8def\u5f84\u9884\u6d4b\u5668\u751f\u6210\u521d\u59cb\u8def\u5f84\uff0c\u5e94\u7528\u4fdd\u5f62\u9884\u6d4b\u91cf\u5316\u9884\u6d4b\u5668\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u6784\u5efa\u56f4\u7ed5\u521d\u59cb\u8def\u5f84\u7684\u8ba4\u8bc1\u533a\u57df\uff0c\u5e76\u504f\u5411\u8fd9\u4e9b\u533a\u57df\u8fdb\u884c\u975e\u5747\u5300\u91c7\u6837\u3002", "result": "\u5728\u5e7f\u6cdb\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u66f4\u5feb\u627e\u5230\u53ef\u884c\u8def\u5f84\uff0c\u5e76\u4e14\u5728\u672a\u89c1\u73af\u5883\u4e2d\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u4e3aSBMPs\u63d0\u4f9b\u6982\u7387\u6b63\u786e\u91c7\u6837\u533a\u57df\u4fdd\u8bc1\u7684\u975e\u5747\u5300\u91c7\u6837\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u89c4\u5212\u7684\u6548\u7387\u3002"}}
{"id": "2511.04837", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04837", "abs": "https://arxiv.org/abs/2511.04837", "authors": ["Cameron Robinson", "Ganghee Jang"], "title": "Design Exploration for Protection and Cleaning of Solar Panels with Case Studies for Space Missions", "comment": "4 pages, 3 figures (5 assets)", "summary": "Solar energy is used for many mission-critical applications including space\nexploration, sensor systems to monitor wildfires, etc. Their operation can be\nlimited or even terminated if solar panels are covered with dust or hit by\nspace debris. To address this issue, we designed panel cleaning mechanisms and\ntested protective materials. For cleaning mechanisms, we designed and compared\na wiper system and a rail system. For protective materials, we found through\ncollision tests that polycarbonate was very promising, though the most\nimportant factor was layering a soft material between the panel's surface and a\nhard material. In the cleaning system comparisons, the wiper-based system was\nmore efficient than the rail-based system in terms of cost, cleaning speed, and\ntotal power consumption.", "AI": {"tldr": "\u8bbe\u8ba1\u5e76\u6d4b\u8bd5\u4e86\u592a\u9633\u80fd\u9762\u677f\u6e05\u6d01\u673a\u5236\u548c\u9632\u62a4\u6750\u6599\uff0c\u4ee5\u89e3\u51b3\u7070\u5c18\u548c\u592a\u7a7a\u788e\u7247\u5bf9\u592a\u9633\u80fd\u7cfb\u7edf\u7684\u5f71\u54cd\u3002", "motivation": "\u592a\u9633\u80fd\u7cfb\u7edf\u5728\u5173\u952e\u4efb\u52a1\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u9762\u677f\u88ab\u7070\u5c18\u8986\u76d6\u6216\u88ab\u592a\u7a7a\u788e\u7247\u51fb\u4e2d\u4f1a\u9650\u5236\u751a\u81f3\u7ec8\u6b62\u5176\u8fd0\u884c\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u79cd\u6e05\u6d01\u673a\u5236\uff08\u96e8\u522e\u7cfb\u7edf\u548c\u8f68\u9053\u7cfb\u7edf\uff09\u5e76\u8fdb\u884c\u6bd4\u8f83\uff1b\u901a\u8fc7\u78b0\u649e\u6d4b\u8bd5\u8bc4\u4f30\u9632\u62a4\u6750\u6599\uff0c\u91cd\u70b9\u5173\u6ce8\u8f6f\u786c\u6750\u6599\u5206\u5c42\u7ed3\u6784\u3002", "result": "\u96e8\u522e\u7cfb\u7edf\u5728\u6210\u672c\u3001\u6e05\u6d01\u901f\u5ea6\u548c\u603b\u529f\u8017\u65b9\u9762\u6bd4\u8f68\u9053\u7cfb\u7edf\u66f4\u9ad8\u6548\uff1b\u805a\u78b3\u9178\u916f\u4f5c\u4e3a\u9632\u62a4\u6750\u6599\u8868\u73b0\u826f\u597d\uff0c\u8f6f\u786c\u6750\u6599\u5206\u5c42\u662f\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u96e8\u522e\u7cfb\u7edf\u662f\u66f4\u4f18\u7684\u6e05\u6d01\u89e3\u51b3\u65b9\u6848\uff0c\u8f6f\u786c\u6750\u6599\u5206\u5c42\u7ed3\u6784\u80fd\u6709\u6548\u4fdd\u62a4\u592a\u9633\u80fd\u9762\u677f\u514d\u53d7\u635f\u574f\u3002"}}
{"id": "2511.04976", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04976", "abs": "https://arxiv.org/abs/2511.04976", "authors": ["Xin Nie", "Zhiyuan Cheng", "Yuan Zhang", "Chao Ji", "Jiajia Wu", "Yuhan Zhang", "Jia Pan"], "title": "iFlyBot-VLM Technical Report", "comment": null, "summary": "We introduce iFlyBot-VLM, a general-purpose Vision-Language Model (VLM) used\nto improve the domain of Embodied Intelligence. The central objective of\niFlyBot-VLM is to bridge the cross-modal semantic gap between high-dimensional\nenvironmental perception and low-level robotic motion control. To this end, the\nmodel abstracts complex visual and spatial information into a body-agnostic and\ntransferable Operational Language, thereby enabling seamless perception-action\nclosed-loop coordination across diverse robotic platforms. The architecture of\niFlyBot-VLM is systematically designed to realize four key functional\ncapabilities essential for embodied intelligence: 1) Spatial Understanding and\nMetric Reasoning; 2) Interactive Target Grounding; 3) Action Abstraction and\nControl Parameter Generation; 4) Task Planning and Skill Sequencing. We\nenvision iFlyBot-VLM as a scalable and generalizable foundation model for\nembodied AI, facilitating the progression from specialized task-oriented\nsystems toward generalist, cognitively capable agents. We conducted evaluations\non 10 current mainstream embodied intelligence-related VLM benchmark datasets,\nsuch as Blink and Where2Place, and achieved optimal performance while\npreserving the model's general capabilities. We will publicly release both the\ntraining data and model weights to foster further research and development in\nthe field of Embodied Intelligence.", "AI": {"tldr": "iFlyBot-VLM\u662f\u4e00\u4e2a\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u901a\u8fc7\u5c06\u590d\u6742\u89c6\u89c9\u7a7a\u95f4\u4fe1\u606f\u62bd\u8c61\u4e3a\u4e0e\u8eab\u4f53\u65e0\u5173\u7684\u53ef\u8f6c\u79fb\u64cd\u4f5c\u8bed\u8a00\uff0c\u6765\u5f25\u5408\u73af\u5883\u611f\u77e5\u4e0e\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\uff0c\u5b9e\u73b0\u8de8\u673a\u5668\u4eba\u5e73\u53f0\u7684\u65e0\u7f1d\u611f\u77e5-\u52a8\u4f5c\u95ed\u73af\u534f\u8c03\u3002", "motivation": "\u5f25\u5408\u9ad8\u7ef4\u73af\u5883\u611f\u77e5\u4e0e\u4f4e\u7ea7\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u4e4b\u95f4\u7684\u8de8\u6a21\u6001\u8bed\u4e49\u9e3f\u6c9f\uff0c\u63a8\u52a8\u4ece\u4e13\u7528\u4efb\u52a1\u5bfc\u5411\u7cfb\u7edf\u5411\u901a\u7528\u8ba4\u77e5\u667a\u80fd\u4f53\u7684\u53d1\u5c55\u3002", "method": "\u7cfb\u7edf\u8bbe\u8ba1\u67b6\u6784\u5b9e\u73b0\u56db\u4e2a\u5173\u952e\u529f\u80fd\uff1a\u7a7a\u95f4\u7406\u89e3\u4e0e\u5ea6\u91cf\u63a8\u7406\u3001\u4ea4\u4e92\u5f0f\u76ee\u6807\u5b9a\u4f4d\u3001\u52a8\u4f5c\u62bd\u8c61\u4e0e\u63a7\u5236\u53c2\u6570\u751f\u6210\u3001\u4efb\u52a1\u89c4\u5212\u4e0e\u6280\u80fd\u5e8f\u5217\u5316\u3002", "result": "\u572810\u4e2a\u4e3b\u6d41\u5177\u8eab\u667a\u80fd\u76f8\u5173VLM\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5982Blink\u548cWhere2Place\uff09\u4e0a\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002", "conclusion": "iFlyBot-VLM\u4f5c\u4e3a\u5177\u8eabAI\u7684\u53ef\u6269\u5c55\u901a\u7528\u57fa\u7840\u6a21\u578b\uff0c\u5c06\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u7814\u7a76\u53d1\u5c55\uff0c\u4f5c\u8005\u5c06\u516c\u5f00\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u578b\u6743\u91cd\u3002"}}
{"id": "2511.04992", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04992", "abs": "https://arxiv.org/abs/2511.04992", "authors": ["Bibekananda Patra", "Sandipan Bandyopadhyay"], "title": "A semi-analytical approach for computing the largest singularity-free spheres of a class of 6-6 Stewart-Gough platforms for specified orientation workspaces", "comment": null, "summary": "This article presents a method for computing the largest singularity-free\nsphere (SFS) of a 6-6 Stewart-Gough platform manipulator (SGPM) over a\nspecified orientation workspace. For a fixed orientation of the moving\nplatform, the SFS is computed analytically. This process is repeated over a set\nof samples generated within the orientation workspace, and the smallest among\nthem is designated as the desired SFS for the given orientation workspace.\nNumerical experiments are performed on four distinct architectures of the SGPM\nto understand their relative performances w.r.t. SFS volumes over the same\norientation workspace. This study demonstrates the potential utility of the\nproposed computational method both in analysis and design of SGPMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b976-6 Stewart-Gough\u5e73\u53f0\u673a\u68b0\u81c2\u5728\u6307\u5b9a\u65b9\u5411\u5de5\u4f5c\u7a7a\u95f4\u5185\u6700\u5927\u65e0\u5947\u5f02\u7403\u4f53\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u8ba1\u7b97\u56fa\u5b9a\u65b9\u5411\u4e0b\u7684SFS\uff0c\u5e76\u5728\u91c7\u6837\u70b9\u4e2d\u9009\u53d6\u6700\u5c0f\u503c\u4f5c\u4e3a\u6574\u4e2a\u5de5\u4f5c\u7a7a\u95f4\u7684SFS\u3002", "motivation": "\u7814\u7a766-6 Stewart-Gough\u5e73\u53f0\u673a\u68b0\u81c2\u5728\u65b9\u5411\u5de5\u4f5c\u7a7a\u95f4\u5185\u7684\u65e0\u5947\u5f02\u6027\u80fd\uff0c\u4e3a\u673a\u68b0\u81c2\u7684\u5206\u6790\u548c\u8bbe\u8ba1\u63d0\u4f9b\u6709\u6548\u7684\u8ba1\u7b97\u5de5\u5177\u3002", "method": "\u5bf9\u4e8e\u56fa\u5b9a\u7684\u79fb\u52a8\u5e73\u53f0\u65b9\u5411\uff0c\u89e3\u6790\u8ba1\u7b97SFS\uff1b\u5728\u65b9\u5411\u5de5\u4f5c\u7a7a\u95f4\u5185\u751f\u6210\u91c7\u6837\u70b9\uff0c\u91cd\u590d\u6b64\u8fc7\u7a0b\uff0c\u9009\u53d6\u6700\u5c0f\u7684SFS\u4f5c\u4e3a\u6574\u4e2a\u5de5\u4f5c\u7a7a\u95f4\u7684SFS\u3002", "result": "\u5728\u56db\u79cd\u4e0d\u540c\u7684SGPM\u67b6\u6784\u4e0a\u8fdb\u884c\u4e86\u6570\u503c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u5b83\u4eec\u5728\u76f8\u540c\u65b9\u5411\u5de5\u4f5c\u7a7a\u95f4\u5185SFS\u4f53\u79ef\u7684\u76f8\u5bf9\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8ba1\u7b97\u65b9\u6cd5\u5728SGPM\u7684\u5206\u6790\u548c\u8bbe\u8ba1\u4e2d\u5177\u6709\u6f5c\u5728\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.04994", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04994", "abs": "https://arxiv.org/abs/2511.04994", "authors": ["Xingyuan Zhou", "Peter Paik", "S. Farokh Atashzar"], "title": "Encoding Biomechanical Energy Margin into Passivity-based Synchronization for Networked Telerobotic Systems", "comment": null, "summary": "Maintaining system stability and accurate position tracking is imperative in\nnetworked robotic systems, particularly for haptics-enabled human-robot\ninteraction. Recent literature has integrated human biomechanics into the\nstabilizers implemented for teleoperation, enhancing force preservation while\nguaranteeing convergence and safety. However, position desynchronization due to\nimperfect communication and non-passive behaviors remains a challenge. This\npaper proposes a two-port biomechanics-aware passivity-based synchronizer and\nstabilizer, referred to as TBPS2. This stabilizer optimizes position\nsynchronization by leveraging human biomechanics while reducing the\nstabilizer's conservatism in its activation. We provide the mathematical design\nsynthesis of the stabilizer and the proof of stability. We also conducted a\nseries of grid simulations and systematic experiments, comparing their\nperformance with that of state-of-the-art solutions under varying time delays\nand environmental conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTBPS2\u7684\u53cc\u7aef\u53e3\u751f\u7269\u529b\u5b66\u611f\u77e5\u7684\u57fa\u4e8e\u65e0\u6e90\u6027\u7684\u540c\u6b65\u5668\u548c\u7a33\u5b9a\u5668\uff0c\u7528\u4e8e\u7f51\u7edc\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u4f4d\u7f6e\u540c\u6b65\u548c\u7a33\u5b9a\u6027\u63a7\u5236\u3002", "motivation": "\u5728\u7f51\u7edc\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\uff0c\u7279\u522b\u662f\u5728\u89e6\u89c9\u4eba\u673a\u4ea4\u4e92\u4e2d\uff0c\u4fdd\u6301\u7cfb\u7edf\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u7684\u4f4d\u7f6e\u8ddf\u8e2a\u81f3\u5173\u91cd\u8981\u3002\u7531\u4e8e\u4e0d\u5b8c\u7f8e\u7684\u901a\u4fe1\u548c\u975e\u88ab\u52a8\u884c\u4e3a\u5bfc\u81f4\u7684\u4f4d\u7f6e\u4e0d\u540c\u6b65\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53cc\u7aef\u53e3\u751f\u7269\u529b\u5b66\u611f\u77e5\u7684\u57fa\u4e8e\u65e0\u6e90\u6027\u7684\u540c\u6b65\u5668\u548c\u7a33\u5b9a\u5668\uff08TBPS2\uff09\uff0c\u901a\u8fc7\u5229\u7528\u4eba\u4f53\u751f\u7269\u529b\u5b66\u6765\u4f18\u5316\u4f4d\u7f6e\u540c\u6b65\uff0c\u540c\u65f6\u51cf\u5c11\u7a33\u5b9a\u5668\u6fc0\u6d3b\u65f6\u7684\u4fdd\u5b88\u6027\u3002\u63d0\u4f9b\u4e86\u6570\u5b66\u8bbe\u8ba1\u7efc\u5408\u548c\u7a33\u5b9a\u6027\u8bc1\u660e\u3002", "result": "\u8fdb\u884c\u4e86\u4e00\u7cfb\u5217\u7f51\u683c\u6a21\u62df\u548c\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u5728\u4e0d\u540c\u65f6\u95f4\u5ef6\u8fdf\u548c\u73af\u5883\u6761\u4ef6\u4e0b\u4e0e\u6700\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u4e86\u6027\u80fd\u6bd4\u8f83\u3002", "conclusion": "TBPS2\u7a33\u5b9a\u5668\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f4d\u7f6e\u4e0d\u540c\u6b65\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7cfb\u7edf\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4f4d\u7f6e\u540c\u6b65\u6027\u80fd\u3002"}}
{"id": "2511.05007", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05007", "abs": "https://arxiv.org/abs/2511.05007", "authors": ["Baiye Cheng", "Tianhai Liang", "Suning Huang", "Maanping Shao", "Feihong Zhang", "Botian Xu", "Zhengrong Xue", "Huazhe Xu"], "title": "MoE-DP: An MoE-Enhanced Diffusion Policy for Robust Long-Horizon Robotic Manipulation with Skill Decomposition and Failure Recovery", "comment": null, "summary": "Diffusion policies have emerged as a powerful framework for robotic\nvisuomotor control, yet they often lack the robustness to recover from subtask\nfailures in long-horizon, multi-stage tasks and their learned representations\nof observations are often difficult to interpret. In this work, we propose the\nMixture of Experts-Enhanced Diffusion Policy (MoE-DP), where the core idea is\nto insert a Mixture of Experts (MoE) layer between the visual encoder and the\ndiffusion model. This layer decomposes the policy's knowledge into a set of\nspecialized experts, which are dynamically activated to handle different phases\nof a task. We demonstrate through extensive experiments that MoE-DP exhibits a\nstrong capability to recover from disturbances, significantly outperforming\nstandard baselines in robustness. On a suite of 6 long-horizon simulation\ntasks, this leads to a 36% average relative improvement in success rate under\ndisturbed conditions. This enhanced robustness is further validated in the real\nworld, where MoE-DP also shows significant performance gains. We further show\nthat MoE-DP learns an interpretable skill decomposition, where distinct experts\ncorrespond to semantic task primitives (e.g., approaching, grasping). This\nlearned structure can be leveraged for inference-time control, allowing for the\nrearrangement of subtasks without any re-training.Our video and code are\navailable at the https://moe-dp-website.github.io/MoE-DP-Website/.", "AI": {"tldr": "\u63d0\u51faMoE-DP\u65b9\u6cd5\uff0c\u5728\u89c6\u89c9\u7f16\u7801\u5668\u548c\u6269\u6563\u6a21\u578b\u4e4b\u95f4\u63d2\u5165\u6df7\u5408\u4e13\u5bb6\u5c42\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6269\u6563\u7b56\u7565\u5728\u673a\u5668\u4eba\u89c6\u89c9\u63a7\u5236\u4e2d\u7f3a\u4e4f\u4ece\u5b50\u4efb\u52a1\u5931\u8d25\u4e2d\u6062\u590d\u7684\u9c81\u68d2\u6027\uff0c\u4e14\u5b66\u4e60\u5230\u7684\u89c2\u5bdf\u8868\u793a\u96be\u4ee5\u89e3\u91ca\u3002", "method": "\u5728\u89c6\u89c9\u7f16\u7801\u5668\u548c\u6269\u6563\u6a21\u578b\u4e4b\u95f4\u63d2\u5165\u6df7\u5408\u4e13\u5bb6\u5c42\uff0c\u5c06\u7b56\u7565\u77e5\u8bc6\u5206\u89e3\u4e3a\u4e13\u95e8\u5904\u7406\u4efb\u52a1\u4e0d\u540c\u9636\u6bb5\u7684\u4e13\u5bb6\u3002", "result": "\u57286\u4e2a\u957f\u65f6\u57df\u4eff\u771f\u4efb\u52a1\u4e2d\uff0c\u6270\u52a8\u6761\u4ef6\u4e0b\u6210\u529f\u7387\u5e73\u5747\u76f8\u5bf9\u63d0\u534736%\uff0c\u771f\u5b9e\u4e16\u754c\u4e5f\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "MoE-DP\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u8fd8\u5b66\u4e60\u4e86\u53ef\u89e3\u91ca\u7684\u6280\u80fd\u5206\u89e3\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u91cd\u65b0\u6392\u5217\u5b50\u4efb\u52a1\u3002"}}
{"id": "2511.05026", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05026", "abs": "https://arxiv.org/abs/2511.05026", "authors": ["Xingyuan Zhou", "Peter Paik", "S. Farokh Atashzar"], "title": "Tunable Passivity Control for Centralized Multiport Networked Systems", "comment": null, "summary": "Centralized Multiport Networked Dynamic (CMND) systems have emerged as a key\narchitecture with applications in several complex network systems, such as\nmultilateral telerobotics and multi-agent control. These systems consist of a\nhub node/subsystem connecting with multiple remote nodes/subsystems via a\nnetworked architecture. One challenge for this system is stability, which can\nbe affected by non-ideal network artifacts. Conventional passivity-based\napproaches can stabilize the system under specialized applications like\nsmall-scale networked systems. However, those conventional passive stabilizers\nhave several restrictions, such as distributing compensation across subsystems\nin a decentralized manner, limiting flexibility, and, at the same time, relying\non the restrictive assumptions of node passivity. This paper synthesizes a\ncentralized optimal passivity-based stabilization framework for CMND systems.\nIt consists of a centralized passivity observer monitoring overall energy flow\nand an optimal passivity controller that distributes the just-needed\ndissipation among various nodes, guaranteeing strict passivity and, thus, L2\nstability. The proposed data-driven model-free approach, i.e., Tunable\nCentralized Optimal Passivity Control (TCoPC), optimizes total performance\nbased on the prescribed dissipation distribution strategy while ensuring\nstability. The controller can put high dissipation loads on some sub-networks\nwhile relaxing the dissipation on other nodes. Simulation results demonstrate\nthe proposed frameworks performance in a complex task under different\ntime-varying delay scenarios while relaxing the remote nodes minimum phase and\npassivity assumption, enhancing the scalability and generalizability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u4e2d\u5f0f\u6700\u4f18\u88ab\u52a8\u6027\u7a33\u5b9a\u6846\u67b6\uff08TCoPC\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u96c6\u4e2d\u5f0f\u591a\u7aef\u53e3\u7f51\u7edc\u52a8\u6001\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u96c6\u4e2d\u5f0f\u88ab\u52a8\u6027\u89c2\u6d4b\u5668\u548c\u6700\u4f18\u88ab\u52a8\u6027\u63a7\u5236\u5668\u5b9e\u73b0L2\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u88ab\u52a8\u6027\u7a33\u5b9a\u65b9\u6cd5\u5728\u5206\u5e03\u5f0f\u8865\u507f\u548c\u8282\u70b9\u88ab\u52a8\u6027\u5047\u8bbe\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u65e0\u6cd5\u6ee1\u8db3\u590d\u6742\u7f51\u7edc\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u7684\u65e0\u6a21\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u4e2d\u5f0f\u88ab\u52a8\u6027\u89c2\u6d4b\u5668\u76d1\u63a7\u6574\u4f53\u80fd\u91cf\u6d41\uff0c\u6700\u4f18\u88ab\u52a8\u6027\u63a7\u5236\u5668\u6309\u9700\u5206\u914d\u8017\u6563\uff0c\u4fdd\u8bc1\u4e25\u683c\u88ab\u52a8\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u65f6\u53d8\u5ef6\u8fdf\u573a\u666f\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u653e\u5bbd\u4e86\u8fdc\u7a0b\u8282\u70b9\u7684\u6700\u5c0f\u76f8\u4f4d\u548c\u88ab\u52a8\u6027\u5047\u8bbe\uff0c\u589e\u5f3a\u4e86\u53ef\u6269\u5c55\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "TCoPC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86CMND\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u6027\u80fd\u4f18\u5316\u548c\u7a33\u5b9a\u6027\u4fdd\u8bc1\u3002"}}
{"id": "2511.05033", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05033", "abs": "https://arxiv.org/abs/2511.05033", "authors": ["Jennifer K. Leestma", "Siddharth R. Nathella", "Christoph P. O. Nuesslein", "Snehil Mathur", "Gregory S. Sawicki", "Aaron J. Young"], "title": "Epically Powerful: An open-source software and mechatronics infrastructure for wearable robotic systems", "comment": "11 pages, 5 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Epically Powerful is an open-source robotics infrastructure that streamlines\nthe underlying framework of wearable robotic systems - managing communication\nprotocols, clocking, actuator commands, visualization, sensor data acquisition,\ndata logging, and more - while also providing comprehensive guides for hardware\nselection, system assembly, and controller implementation. Epically Powerful\ncontains a code base enabling simplified user implementation via Python that\nseamlessly interfaces with various commercial state-of-the-art quasi-direct\ndrive (QDD) actuators, single-board computers, and common sensors, provides\nexample controllers, and enables real-time visualization. To further support\ndevice development, the package also includes a recommended parts list and\ncompatibility guide and detailed documentation on hardware and software\nimplementation. The goal of Epically Powerful is to lower the barrier to\ndeveloping and deploying custom wearable robotic systems without a\npre-specified form factor, enabling researchers to go from raw hardware to\nmodular, robust devices quickly and effectively. Though originally designed\nwith wearable robotics in mind, Epically Powerful is broadly applicable to\nother robotic domains that utilize QDD actuators, single-board computers, and\nsensors for closed-loop control.", "AI": {"tldr": "Epically Powerful\u662f\u4e00\u4e2a\u5f00\u6e90\u673a\u5668\u4eba\u57fa\u7840\u8bbe\u65bd\uff0c\u7b80\u5316\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u7cfb\u7edf\u5f00\u53d1\uff0c\u63d0\u4f9b\u901a\u4fe1\u534f\u8bae\u3001\u6570\u636e\u91c7\u96c6\u3001\u53ef\u89c6\u5316\u7b49\u6838\u5fc3\u529f\u80fd\uff0c\u652f\u6301Python\u5feb\u901f\u5f00\u53d1\u3002", "motivation": "\u964d\u4f4e\u5f00\u53d1\u5b9a\u5236\u5316\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u95e8\u69db\uff0c\u8ba9\u7814\u7a76\u4eba\u5458\u80fd\u591f\u5feb\u901f\u4ece\u786c\u4ef6\u5230\u6a21\u5757\u5316\u8bbe\u5907\u3002", "method": "\u63d0\u4f9b\u4ee3\u7801\u5e93\u652f\u6301Python\u7b80\u5316\u5b9e\u73b0\uff0c\u96c6\u6210QDD\u6267\u884c\u5668\u3001\u5355\u677f\u8ba1\u7b97\u673a\u548c\u4f20\u611f\u5668\uff0c\u5305\u542b\u793a\u4f8b\u63a7\u5236\u5668\u548c\u5b9e\u65f6\u53ef\u89c6\u5316\u5de5\u5177\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u5f00\u53d1\u6846\u67b6\uff0c\u5305\u542b\u786c\u4ef6\u9009\u578b\u6307\u5357\u3001\u7cfb\u7edf\u7ec4\u88c5\u8bf4\u660e\u548c\u63a7\u5236\u5668\u5b9e\u73b0\u6587\u6863\u3002", "conclusion": "Epically Powerful\u4e0d\u4ec5\u9002\u7528\u4e8e\u53ef\u7a7f\u6234\u673a\u5668\u4eba\uff0c\u4e5f\u5e7f\u6cdb\u9002\u7528\u4e8e\u5176\u4ed6\u4f7f\u7528QDD\u6267\u884c\u5668\u548c\u5355\u677f\u8ba1\u7b97\u673a\u7684\u673a\u5668\u4eba\u9886\u57df\u3002"}}
{"id": "2511.05052", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.05052", "abs": "https://arxiv.org/abs/2511.05052", "authors": ["Zihao Li", "Yiming Zhu", "Zhe Zhong", "Qinyuan Ren", "Yijiang Huang"], "title": "TAPOM: Task-Space Topology-Guided Motion Planning for Manipulating Elongated Object in Cluttered Environments", "comment": null, "summary": "Robotic manipulation in complex, constrained spaces is vital for widespread\napplications but challenging, particularly when navigating narrow passages with\nelongated objects. Existing planning methods often fail in these low-clearance\nscenarios due to the sampling difficulties or the local minima. This work\nproposes Topology-Aware Planning for Object Manipulation (TAPOM), which\nexplicitly incorporates task-space topological analysis to enable efficient\nplanning. TAPOM uses a high-level analysis to identify critical pathways and\ngenerate guiding keyframes, which are utilized in a low-level planner to find\nfeasible configuration space trajectories. Experimental validation demonstrates\nsignificantly high success rates and improved efficiency over state-of-the-art\nmethods on low-clearance manipulation tasks. This approach offers broad\nimplications for enhancing manipulation capabilities of robots in complex\nreal-world environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86TAPOM\u65b9\u6cd5\uff0c\u901a\u8fc7\u4efb\u52a1\u7a7a\u95f4\u62d3\u6251\u5206\u6790\u6765\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u72ed\u7a84\u901a\u9053\u7684\u89c4\u5212\u96be\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f4e\u95f4\u9699\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u548c\u6548\u7387", "motivation": "\u73b0\u6709\u89c4\u5212\u65b9\u6cd5\u5728\u72ed\u7a84\u901a\u9053\u64cd\u4f5c\u7ec6\u957f\u7269\u4f53\u65f6\u7ecf\u5e38\u5931\u8d25\uff0c\u4e3b\u8981\u7531\u4e8e\u91c7\u6837\u56f0\u96be\u6216\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u5904\u7406\u4f4e\u95f4\u9699\u573a\u666f\u7684\u6311\u6218", "method": "TAPOM\u91c7\u7528\u5206\u5c42\u89c4\u5212\uff1a\u9ad8\u5c42\u8fdb\u884c\u4efb\u52a1\u7a7a\u95f4\u62d3\u6251\u5206\u6790\u8bc6\u522b\u5173\u952e\u8def\u5f84\u5e76\u751f\u6210\u5f15\u5bfc\u5173\u952e\u5e27\uff0c\u4f4e\u5c42\u89c4\u5212\u5668\u5229\u7528\u8fd9\u4e9b\u5173\u952e\u5e27\u5bfb\u627e\u53ef\u884c\u7684\u914d\u7f6e\u7a7a\u95f4\u8f68\u8ff9", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u4f4e\u95f4\u9699\u64cd\u4f5c\u4efb\u52a1\u4e0a\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5177\u6709\u663e\u8457\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u6539\u8fdb\u7684\u6548\u7387", "conclusion": "TAPOM\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u7ed3\u5408\u62d3\u6251\u5206\u6790\uff0c\u4e3a\u673a\u5668\u4eba\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u80fd\u529b\u63d0\u5347\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f"}}
{"id": "2511.05129", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05129", "abs": "https://arxiv.org/abs/2511.05129", "authors": ["Bin Fan", "Jianjian Jiang", "Zhuohao Li", "Yixiang He", "Xiaoming Wu", "Yihan Yang", "Shengbang Liu", "Weishi Zheng"], "title": "Decomposed Object Manipulation via Dual-Actor Policy", "comment": "9 pages, 7 figures, 5 tables", "summary": "Object manipulation, which focuses on learning to perform tasks on similar\nparts across different types of objects, can be divided into an approaching\nstage and a manipulation stage. However, previous works often ignore this\ncharacteristic of the task and rely on a single policy to directly learn the\nwhole process of object manipulation. To address this problem, we propose a\nnovel Dual-Actor Policy, termed DAP, which explicitly considers different\nstages and leverages heterogeneous visual priors to enhance each stage.\nSpecifically, we introduce an affordance-based actor to locate the functional\npart in the manipulation task, thereby improving the approaching process.\nFollowing this, we propose a motion flow-based actor to capture the movement of\nthe component, facilitating the manipulation process. Finally, we introduce a\ndecision maker to determine the current stage of DAP and select the\ncorresponding actor. Moreover, existing object manipulation datasets contain\nfew objects and lack the visual priors needed to support training. To address\nthis, we construct a simulated dataset, the Dual-Prior Object Manipulation\nDataset, which combines the two visual priors and includes seven tasks,\nincluding two challenging long-term, multi-stage tasks. Experimental results on\nour dataset, the RoboTwin benchmark and real-world scenarios illustrate that\nour method consistently outperforms the SOTA method by 5.55%, 14.7% and 10.4%\non average respectively.", "AI": {"tldr": "\u63d0\u51faDAP\u53cc\u6267\u884c\u5668\u7b56\u7565\uff0c\u5c06\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\u5206\u4e3a\u63a5\u8fd1\u9636\u6bb5\u548c\u64cd\u4f5c\u9636\u6bb5\uff0c\u5206\u522b\u4f7f\u7528\u529f\u80fd\u53ef\u4f9b\u6027\u548c\u8fd0\u52a8\u6d41\u4e24\u79cd\u89c6\u89c9\u5148\u9a8c\u6765\u589e\u5f3a\u5404\u9636\u6bb5\u6027\u80fd\uff0c\u5e76\u5728\u65b0\u6784\u5efa\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u5355\u4e00\u7b56\u7565\u5b66\u4e60\u6574\u4e2a\u7269\u4f53\u64cd\u4f5c\u8fc7\u7a0b\uff0c\u5ffd\u7565\u4e86\u4efb\u52a1\u672c\u8eab\u5206\u4e3a\u63a5\u8fd1\u9636\u6bb5\u548c\u64cd\u4f5c\u9636\u6bb5\u7684\u7279\u6027\uff0c\u4e14\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u8db3\u591f\u7684\u89c6\u89c9\u5148\u9a8c\u652f\u6301\u8bad\u7ec3\u3002", "method": "\u63d0\u51faDAP\u53cc\u6267\u884c\u5668\u7b56\u7565\uff1a1) \u529f\u80fd\u53ef\u4f9b\u6027\u6267\u884c\u5668\u5b9a\u4f4d\u529f\u80fd\u90e8\u4ef6\u6539\u5584\u63a5\u8fd1\u8fc7\u7a0b\uff1b2) \u8fd0\u52a8\u6d41\u6267\u884c\u5668\u6355\u6349\u90e8\u4ef6\u8fd0\u52a8\u4fc3\u8fdb\u64cd\u4f5c\u8fc7\u7a0b\uff1b3) \u51b3\u7b56\u5668\u786e\u5b9a\u5f53\u524d\u9636\u6bb5\u5e76\u9009\u62e9\u5bf9\u5e94\u6267\u884c\u5668\u3002\u540c\u65f6\u6784\u5efa\u4e86\u5305\u542b\u4e24\u79cd\u89c6\u89c9\u5148\u9a8c\u7684Dual-Prior\u6570\u636e\u96c6\u3002", "result": "\u5728\u81ea\u5efa\u6570\u636e\u96c6\u3001RoboTwin\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u65b9\u6cd5\u5e73\u5747\u5206\u522b\u8d85\u8d8aSOTA\u65b9\u6cd55.55%\u300114.7%\u548c10.4%\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u8003\u8651\u7269\u4f53\u64cd\u4f5c\u7684\u4e0d\u540c\u9636\u6bb5\u5e76\u5229\u7528\u5f02\u6784\u89c6\u89c9\u5148\u9a8c\uff0cDAP\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u957f\u671f\u591a\u9636\u6bb5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2511.05158", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05158", "abs": "https://arxiv.org/abs/2511.05158", "authors": ["Sahar Salimpour", "Iacopo Catalano", "Tomi Westerlund", "Mohsen Falahi", "Jorge Pe\u00f1a Queralta"], "title": "Follow-Me in Micro-Mobility with End-to-End Imitation Learning", "comment": null, "summary": "Autonomous micro-mobility platforms face challenges from the perspective of\nthe typical deployment environment: large indoor spaces or urban areas that are\npotentially crowded and highly dynamic. While social navigation algorithms have\nprogressed significantly, optimizing user comfort and overall user experience\nover other typical metrics in robotics (e.g., time or distance traveled) is\nunderstudied. Specifically, these metrics are critical in commercial\napplications. In this paper, we show how imitation learning delivers smoother\nand overall better controllers, versus previously used manually-tuned\ncontrollers. We demonstrate how DAAV's autonomous wheelchair achieves\nstate-of-the-art comfort in follow-me mode, in which it follows a human\noperator assisting persons with reduced mobility (PRM). This paper analyzes\ndifferent neural network architectures for end-to-end control and demonstrates\ntheir usability in real-world production-level deployments.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u81ea\u4e3b\u5fae\u79fb\u52a8\u5e73\u53f0\u5728\u62e5\u6324\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u95ee\u9898\uff0c\u91cd\u70b9\u4f18\u5316\u7528\u6237\u8212\u9002\u5ea6\u548c\u4f53\u9a8c\u800c\u975e\u4f20\u7edf\u673a\u5668\u4eba\u6307\u6807\u3002\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u5f00\u53d1\u4e86\u66f4\u5e73\u6ed1\u7684\u63a7\u5236\u5668\uff0c\u5728\u8ddf\u968f\u6a21\u5f0f\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8212\u9002\u5ea6\u3002", "motivation": "\u81ea\u4e3b\u5fae\u79fb\u52a8\u5e73\u53f0\u5728\u62e5\u6324\u52a8\u6001\u73af\u5883\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u65f6\u95f4\u6216\u8ddd\u79bb\u7b49\u4f20\u7edf\u673a\u5668\u4eba\u6307\u6807\uff0c\u800c\u7528\u6237\u8212\u9002\u5ea6\u548c\u6574\u4f53\u4f53\u9a8c\u5728\u5546\u4e1a\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u4f46\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5f00\u53d1\u63a7\u5236\u5668\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u7684\u7aef\u5230\u7aef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u751f\u4ea7\u7ea7\u90e8\u7f72\u4e2d\u9a8c\u8bc1\u5176\u53ef\u7528\u6027\u3002", "result": "\u6a21\u4eff\u5b66\u4e60\u76f8\u6bd4\u4e4b\u524d\u624b\u52a8\u8c03\u4f18\u7684\u63a7\u5236\u5668\u80fd\u63d0\u4f9b\u66f4\u5e73\u6ed1\u548c\u66f4\u597d\u7684\u63a7\u5236\u6548\u679c\uff0cDAAV\u81ea\u4e3b\u8f6e\u6905\u5728\u8ddf\u968f\u6a21\u5f0f\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8212\u9002\u5ea6\u3002", "conclusion": "\u6a21\u4eff\u5b66\u4e60\u662f\u5f00\u53d1\u81ea\u4e3b\u5fae\u79fb\u52a8\u5e73\u53f0\u63a7\u5236\u5668\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u7528\u6237\u8212\u9002\u5ea6\u548c\u4f53\u9a8c\uff0c\u9002\u5408\u5728\u771f\u5b9e\u4e16\u754c\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72\u3002"}}
{"id": "2511.05185", "categories": ["cs.RO", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.05185", "abs": "https://arxiv.org/abs/2511.05185", "authors": ["Adri\u00e1n Campazas-Vega", "Claudia \u00c1lvarez-Aparicio", "David Sobr\u00edn-Hidalgo", "Laura Inyesto-Alonso", "Francisco Javier Rodr\u00edguez-Lera", "Vicente Matell\u00e1n-Olivera", "\u00c1ngel Manuel Guerrero-Higueras"], "title": "Procedimiento de auditor\u00eda de ciberseguridad para sistemas aut\u00f3nomos: metodolog\u00eda, amenazas y mitigaciones", "comment": "32 pages, in Spanish language, 7 tables, 12 Figures. White paper\n  under the TESCAC project", "summary": "The deployment of autonomous systems has experienced remarkable growth in\nrecent years, driven by their integration into sectors such as industry,\nmedicine, logistics, and domestic environments. This expansion is accompanied\nby a series of security issues that entail significant risks due to the\ncritical nature of autonomous systems, especially those operating in\nhuman-interaction environments. Furthermore, technological advancement and the\nhigh operational and architectural complexity of autonomous systems have\nresulted in an increased attack surface. This article presents a specific\nsecurity auditing procedure for autonomous systems, based on a layer-structured\nmethodology, a threat taxonomy adapted to the robotic context, and a set of\nconcrete mitigation measures. The validity of the proposed approach is\ndemonstrated through four practical case studies applied to representative\nrobotic platforms: the Vision 60 military quadruped from Ghost Robotics, the A1\nrobot from Unitree Robotics, the UR3 collaborative arm from Universal Robots,\nand the Pepper social robot from Aldebaran Robotics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u81ea\u4e3b\u7cfb\u7edf\u7684\u5206\u5c42\u5b89\u5168\u5ba1\u8ba1\u7a0b\u5e8f\uff0c\u5305\u62ec\u5206\u5c42\u7ed3\u6784\u65b9\u6cd5\u3001\u673a\u5668\u4eba\u7279\u5b9a\u5a01\u80c1\u5206\u7c7b\u548c\u5177\u4f53\u7f13\u89e3\u63aa\u65bd\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u4ee3\u8868\u6027\u673a\u5668\u4eba\u5e73\u53f0\u7684\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u81ea\u4e3b\u7cfb\u7edf\u5728\u5de5\u4e1a\u3001\u533b\u7597\u3001\u7269\u6d41\u7b49\u5173\u952e\u9886\u57df\u7684\u90e8\u7f72\u5feb\u901f\u589e\u957f\uff0c\u4f46\u4f34\u968f\u7684\u5b89\u5168\u95ee\u9898\u5e26\u6765\u4e86\u91cd\u5927\u98ce\u9669\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u5728\u4eba\u7c7b\u4ea4\u4e92\u73af\u5883\u4e2d\u8fd0\u884c\u7684\u81ea\u4e3b\u7cfb\u7edf\u3002\u6280\u672f\u8fdb\u6b65\u548c\u7cfb\u7edf\u590d\u6742\u6027\u589e\u52a0\u4e86\u653b\u51fb\u9762\u3002", "method": "\u57fa\u4e8e\u5206\u5c42\u7ed3\u6784\u65b9\u6cd5\u3001\u9002\u5e94\u673a\u5668\u4eba\u73af\u5883\u7684\u5a01\u80c1\u5206\u7c7b\u6cd5\u4ee5\u53ca\u4e00\u5957\u5177\u4f53\u7f13\u89e3\u63aa\u65bd\uff0c\u5f00\u53d1\u4e86\u4e13\u95e8\u7684\u5b89\u5168\u5ba1\u8ba1\u7a0b\u5e8f\u3002", "result": "\u901a\u8fc7\u56db\u4e2a\u5b9e\u9645\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff1aGhost Robotics\u7684Vision 60\u519b\u7528\u56db\u8db3\u673a\u5668\u4eba\u3001Unitree Robotics\u7684A1\u673a\u5668\u4eba\u3001Universal Robots\u7684UR3\u534f\u4f5c\u81c2\u548cAldebaran Robotics\u7684Pepper\u793e\u4ea4\u673a\u5668\u4eba\u3002", "conclusion": "\u63d0\u51fa\u7684\u5b89\u5168\u5ba1\u8ba1\u7a0b\u5e8f\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b89\u5168\u4fdd\u969c\u65b9\u6cd5\uff0c\u80fd\u591f\u5e94\u5bf9\u65e5\u76ca\u589e\u957f\u7684\u5b89\u5168\u6311\u6218\u548c\u590d\u6742\u7684\u653b\u51fb\u9762\u3002"}}
{"id": "2511.05199", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05199", "abs": "https://arxiv.org/abs/2511.05199", "authors": ["Yichen Zhu", "Feifei Feng"], "title": "Let Me Show You: Learning by Retrieving from Egocentric Video for Robotic Manipulation", "comment": "Accepted by IROS 2025", "summary": "Robots operating in complex and uncertain environments face considerable\nchallenges. Advanced robotic systems often rely on extensive datasets to learn\nmanipulation tasks. In contrast, when humans are faced with unfamiliar tasks,\nsuch as assembling a chair, a common approach is to learn by watching video\ndemonstrations. In this paper, we propose a novel method for learning robot\npolicies by Retrieving-from-Video (RfV), using analogies from human\ndemonstrations to address manipulation tasks. Our system constructs a video\nbank comprising recordings of humans performing diverse daily tasks. To enrich\nthe knowledge from these videos, we extract mid-level information, such as\nobject affordance masks and hand motion trajectories, which serve as additional\ninputs to enhance the robot model's learning and generalization capabilities.\nWe further feature a dual-component system: a video retriever that taps into an\nexternal video bank to fetch task-relevant video based on task specification,\nand a policy generator that integrates this retrieved knowledge into the\nlearning cycle. This approach enables robots to craft adaptive responses to\nvarious scenarios and generalize to tasks beyond those in the training data.\nThrough rigorous testing in multiple simulated and real-world settings, our\nsystem demonstrates a marked improvement in performance over conventional\nrobotic systems, showcasing a significant breakthrough in the field of\nrobotics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4ece\u89c6\u9891\u4e2d\u68c0\u7d22\u7c7b\u6bd4\u6765\u5b66\u4e60\u673a\u5668\u4eba\u7b56\u7565\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u6784\u5efa\u77e5\u8bc6\u5e93\uff0c\u901a\u8fc7\u63d0\u53d6\u7269\u4f53\u529f\u80fd\u63a9\u7801\u548c\u624b\u90e8\u8fd0\u52a8\u8f68\u8ff9\u7b49\u4e2d\u5c42\u4fe1\u606f\u6765\u589e\u5f3a\u673a\u5668\u4eba\u7684\u5b66\u4e60\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u673a\u5668\u4eba\u9762\u5bf9\u590d\u6742\u4e0d\u786e\u5b9a\u73af\u5883\u65f6\u9762\u4e34\u6311\u6218\uff0c\u800c\u4eba\u7c7b\u901a\u5e38\u901a\u8fc7\u89c2\u770b\u89c6\u9891\u6f14\u793a\u6765\u5b66\u4e60\u65b0\u4efb\u52a1\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u5e0c\u671b\u8ba9\u673a\u5668\u4eba\u4e5f\u80fd\u901a\u8fc7\u89c2\u770b\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u6765\u5b66\u4e60\u64cd\u4f5c\u4efb\u52a1\u3002", "method": "\u6784\u5efa\u4eba\u7c7b\u65e5\u5e38\u4efb\u52a1\u89c6\u9891\u5e93\uff0c\u63d0\u53d6\u7269\u4f53\u529f\u80fd\u63a9\u7801\u548c\u624b\u90e8\u8fd0\u52a8\u8f68\u8ff9\u7b49\u4e2d\u5c42\u4fe1\u606f\u3002\u7cfb\u7edf\u5305\u542b\u89c6\u9891\u68c0\u7d22\u5668\u548c\u7b56\u7565\u751f\u6210\u5668\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u524d\u8005\u4ece\u5916\u90e8\u89c6\u9891\u5e93\u4e2d\u68c0\u7d22\u4efb\u52a1\u76f8\u5173\u89c6\u9891\uff0c\u540e\u8005\u5c06\u68c0\u7d22\u5230\u7684\u77e5\u8bc6\u6574\u5408\u5230\u5b66\u4e60\u5faa\u73af\u4e2d\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u4e25\u683c\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u76f8\u6bd4\u4f20\u7edf\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u6027\u80fd\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5bf9\u5404\u79cd\u573a\u666f\u505a\u51fa\u81ea\u9002\u5e94\u54cd\u5e94\uff0c\u5e76\u6cdb\u5316\u5230\u8bad\u7ec3\u6570\u636e\u4e4b\u5916\u7684\u4efb\u52a1\uff0c\u5728\u673a\u5668\u4eba\u9886\u57df\u53d6\u5f97\u4e86\u91cd\u8981\u7a81\u7834\u3002"}}
{"id": "2511.05203", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05203", "abs": "https://arxiv.org/abs/2511.05203", "authors": ["Linus Nwankwo", "Bj\u00f6rn Ellensohn", "Christian Rauch", "Elmar Rueckert"], "title": "Beyond Master and Apprentice: Grounding Foundation Models for Symbiotic Interactive Learning in a Shared Latent Space", "comment": null, "summary": "Today's autonomous agents can understand free-form natural language\ninstructions and execute long-horizon tasks in a manner akin to human-level\nreasoning. These capabilities are mostly driven by large-scale pre-trained\nfoundation models (FMs). However, the approaches with which these models are\ngrounded for human-robot interaction (HRI) perpetuate a master-apprentice\nmodel, where the apprentice (embodied agent) passively receives and executes\nthe master's (human's) commands without reciprocal learning. This reactive\ninteraction approach does not capture the co-adaptive dynamics inherent in\neveryday multi-turn human-human interactions. To address this, we propose a\nSymbiotic Interactive Learning (SIL) approach that enables both the master and\nthe apprentice to co-adapt through mutual, bidirectional interactions. We\nformalised SIL as a co-adaptation process within a shared latent task space,\nwhere the agent and human maintain joint belief states that evolve based on\ninteraction history. This enables the agent to move beyond reactive execution\nto proactive clarification, adaptive suggestions, and shared plan refinement.\nTo realise these novel behaviours, we leveraged pre-trained FMs for spatial\nperception and reasoning, alongside a lightweight latent encoder that grounds\nthe models' outputs into task-specific representations. Furthermore, to ensure\nstability as the tasks evolve, we augment SIL with a memory architecture that\nprevents the forgetting of learned task-space representations. We validate SIL\non both simulated and real-world embodied tasks, including instruction\nfollowing, information retrieval, query-oriented reasoning, and interactive\ndialogues. Demos and resources are public\nat:~\\href{https://linusnep.github.io/SIL/}{https://linusnep.github.io/SIL/}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5171\u751f\u4ea4\u4e92\u5b66\u4e60(SIL)\u65b9\u6cd5\uff0c\u4f7f\u4eba\u7c7b\u548c\u5177\u8eab\u667a\u80fd\u4f53\u80fd\u591f\u901a\u8fc7\u53cc\u5411\u4ea4\u4e92\u5171\u540c\u9002\u5e94\uff0c\u8d85\u8d8a\u4f20\u7edf\u7684\u4e3b\u4ece\u6a21\u5f0f\uff0c\u5b9e\u73b0\u4e3b\u52a8\u6f84\u6e05\u3001\u9002\u5e94\u6027\u5efa\u8bae\u548c\u5171\u4eab\u8ba1\u5212\u4f18\u5316\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u867d\u7136\u80fd\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548c\u6267\u884c\u957f\u65f6\u7a0b\u4efb\u52a1\uff0c\u4f46\u91c7\u7528\u7684\u662f\u4e3b\u4ece\u4ea4\u4e92\u6a21\u5f0f\uff0c\u667a\u80fd\u4f53\u88ab\u52a8\u6267\u884c\u4eba\u7c7b\u547d\u4ee4\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u65e5\u5e38\u4ea4\u4e92\u4e2d\u7684\u5171\u540c\u9002\u5e94\u52a8\u6001\u3002", "method": "\u5c06SIL\u5f62\u5f0f\u5316\u4e3a\u5171\u4eab\u6f5c\u5728\u4efb\u52a1\u7a7a\u95f4\u4e2d\u7684\u5171\u540c\u9002\u5e94\u8fc7\u7a0b\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u7a7a\u95f4\u611f\u77e5\u548c\u63a8\u7406\uff0c\u914d\u5408\u8f7b\u91cf\u7ea7\u6f5c\u5728\u7f16\u7801\u5668\u5c06\u6a21\u578b\u8f93\u51fa\u63a5\u5730\u5230\u4efb\u52a1\u7279\u5b9a\u8868\u793a\uff0c\u5e76\u6dfb\u52a0\u8bb0\u5fc6\u67b6\u6784\u9632\u6b62\u4efb\u52a1\u7a7a\u95f4\u8868\u793a\u9057\u5fd8\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u5177\u8eab\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86SIL\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u6307\u4ee4\u8ddf\u968f\u3001\u4fe1\u606f\u68c0\u7d22\u3001\u67e5\u8be2\u5bfc\u5411\u63a8\u7406\u548c\u4ea4\u4e92\u5bf9\u8bdd\u3002", "conclusion": "SIL\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u4eba\u7c7b\u4e0e\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u53cc\u5411\u5171\u540c\u9002\u5e94\uff0c\u63a8\u52a8\u4ea4\u4e92\u5f0f\u4eba\u5de5\u667a\u80fd\u5411\u66f4\u81ea\u7136\u3001\u534f\u4f5c\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2511.05234", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05234", "abs": "https://arxiv.org/abs/2511.05234", "authors": ["Philipp Dahlinger", "Niklas Freymuth", "Tai Hoang", "Tobias W\u00fcrth", "Michael Volpp", "Luise K\u00e4rger", "Gerhard Neumann"], "title": "Context-aware Learned Mesh-based Simulation via Trajectory-Level Meta-Learning", "comment": "35 pages. Submitted to Transactions on Machine Learning Research\n  (TMLR)", "summary": "Simulating object deformations is a critical challenge across many scientific\ndomains, including robotics, manufacturing, and structural mechanics. Learned\nGraph Network Simulators (GNSs) offer a promising alternative to traditional\nmesh-based physics simulators. Their speed and inherent differentiability make\nthem particularly well suited for applications that require fast and accurate\nsimulations, such as robotic manipulation or manufacturing optimization.\nHowever, existing learned simulators typically rely on single-step\nobservations, which limits their ability to exploit temporal context. Without\nthis information, these models fail to infer, e.g., material properties.\nFurther, they rely on auto-regressive rollouts, which quickly accumulate error\nfor long trajectories. We instead frame mesh-based simulation as a\ntrajectory-level meta-learning problem. Using Conditional Neural Processes, our\nmethod enables rapid adaptation to new simulation scenarios from limited\ninitial data while capturing their latent simulation properties. We utilize\nmovement primitives to directly predict fast, stable and accurate simulations\nfrom a single model call. The resulting approach, Movement-primitive\nMeta-MeshGraphNet (M3GN), provides higher simulation accuracy at a fraction of\nthe runtime cost compared to state-of-the-art GNSs across several tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86M3GN\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f68\u8ff9\u7ea7\u5143\u5b66\u4e60\u548c\u8fd0\u52a8\u57fa\u5143\u76f4\u63a5\u9884\u6d4b\u7a33\u5b9a\u51c6\u786e\u7684\u6a21\u62df\u7ed3\u679c\uff0c\u76f8\u6bd4\u73b0\u6709\u56fe\u7f51\u7edc\u6a21\u62df\u5668\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6a21\u62df\u7cbe\u5ea6\u548c\u66f4\u4f4e\u7684\u8fd0\u884c\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u5b66\u4e60\u6a21\u62df\u5668\u4f9d\u8d56\u5355\u6b65\u89c2\u6d4b\uff0c\u65e0\u6cd5\u5229\u7528\u65f6\u95f4\u4e0a\u4e0b\u6587\u63a8\u65ad\u6750\u6599\u5c5e\u6027\uff0c\u4e14\u81ea\u56de\u5f52\u63a8\u6f14\u4f1a\u5feb\u901f\u7d2f\u79ef\u8bef\u5dee\uff0c\u9650\u5236\u4e86\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u5236\u9020\u4f18\u5316\u7b49\u5e94\u7528\u4e2d\u7684\u6548\u679c\u3002", "method": "\u5c06\u7f51\u683c\u6a21\u62df\u6784\u5efa\u4e3a\u8f68\u8ff9\u7ea7\u5143\u5b66\u4e60\u95ee\u9898\uff0c\u4f7f\u7528\u6761\u4ef6\u795e\u7ecf\u8fc7\u7a0b\u4ece\u6709\u9650\u521d\u59cb\u6570\u636e\u5feb\u901f\u9002\u5e94\u65b0\u573a\u666f\uff0c\u5229\u7528\u8fd0\u52a8\u57fa\u5143\u76f4\u63a5\u901a\u8fc7\u5355\u6b21\u6a21\u578b\u8c03\u7528\u9884\u6d4b\u5feb\u901f\u7a33\u5b9a\u7684\u6a21\u62df\u7ed3\u679c\u3002", "result": "M3GN\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u56fe\u7f51\u7edc\u6a21\u62df\u5668\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u6a21\u62df\u7cbe\u5ea6\uff0c\u540c\u65f6\u8fd0\u884c\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8f68\u8ff9\u7ea7\u5143\u5b66\u4e60\u548c\u8fd0\u52a8\u57fa\u5143\u7684\u7ed3\u5408\u4e3a\u5b66\u4e60\u6a21\u62df\u5668\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u5feb\u901f\u51c6\u786e\u6a21\u62df\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2511.05275", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05275", "abs": "https://arxiv.org/abs/2511.05275", "authors": ["Hokyun Im", "Euijin Jeong", "Jianlong Fu", "Andrey Kolobov", "Youngwoon Lee"], "title": "TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models", "comment": "Project webpage : https://jellyho.github.io/TwinVLA/", "summary": "Vision-language-action models (VLAs) trained on large-scale robotic datasets\nhave demonstrated strong performance on manipulation tasks, including bimanual\ntasks. However, because most public datasets focus on single-arm\ndemonstrations, adapting VLAs for bimanual tasks typically requires substantial\nadditional bimanual data and fine-tuning. To address this challenge, we\nintroduce TwinVLA, a modular framework that composes two copies of a pretrained\nsingle-arm VLA into a coordinated bimanual VLA. Unlike monolithic\ncross-embodiment models trained on mixtures of single-arm and bimanual data,\nTwinVLA improves both data efficiency and performance by composing pretrained\nsingle-arm policies. Across diverse bimanual tasks in real-world and simulation\nsettings, TwinVLA outperforms a comparably-sized monolithic RDT-1B model\nwithout requiring any bimanual pretraining. Furthermore, it narrows the gap to\nstate-of-the-art model, $\\pi_0$ which rely on extensive proprietary bimanual\ndata and compute cost. These results establish our modular composition approach\nas a data-efficient and scalable path toward high-performance bimanual\nmanipulation, leveraging public single-arm data.", "AI": {"tldr": "TwinVLA\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5c06\u4e24\u4e2a\u9884\u8bad\u7ec3\u7684\u5355\u81c2\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7ec4\u5408\u6210\u534f\u8c03\u7684\u53cc\u81c2VLA\uff0c\u65e0\u9700\u53cc\u81c2\u9884\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u7684\u53cc\u81c2\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u516c\u5171\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u5355\u81c2\u6f14\u793a\uff0c\u5c06VLA\u9002\u5e94\u53cc\u81c2\u4efb\u52a1\u901a\u5e38\u9700\u8981\u5927\u91cf\u989d\u5916\u7684\u53cc\u81c2\u6570\u636e\u548c\u5fae\u8c03\uff0c\u8fd9\u9650\u5236\u4e86\u53cc\u81c2\u64cd\u4f5c\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u6a21\u5757\u5316\u7ec4\u5408\u4e24\u4e2a\u9884\u8bad\u7ec3\u7684\u5355\u81c2VLA\u7b56\u7565\uff0c\u5f62\u6210\u4e00\u4e2a\u534f\u8c03\u7684\u53cc\u81c2VLA\u6846\u67b6\uff0c\u800c\u4e0d\u662f\u8bad\u7ec3\u5355\u4e00\u8de8\u5177\u8eab\u6a21\u578b\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u4eff\u771f\u73af\u5883\u4e2d\uff0cTwinVLA\u5728\u591a\u79cd\u53cc\u81c2\u4efb\u52a1\u4e0a\u4f18\u4e8e\u540c\u7b49\u89c4\u6a21\u7684\u5355\u4f53RDT-1B\u6a21\u578b\uff0c\u4e14\u65e0\u9700\u4efb\u4f55\u53cc\u81c2\u9884\u8bad\u7ec3\uff0c\u7f29\u5c0f\u4e86\u4e0e\u4f9d\u8d56\u5927\u91cf\u4e13\u6709\u53cc\u81c2\u6570\u636e\u7684\u5148\u8fdb\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u6a21\u5757\u5316\u7ec4\u5408\u65b9\u6cd5\u4e3a\u9ad8\u6027\u80fd\u53cc\u81c2\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u6761\u6570\u636e\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u80fd\u591f\u5145\u5206\u5229\u7528\u516c\u5171\u5355\u81c2\u6570\u636e\u3002"}}
{"id": "2511.05307", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.05307", "abs": "https://arxiv.org/abs/2511.05307", "authors": ["Akua K. Dickson", "Juan C. Pacheco Garcia", "Andrew P. Sabelhaus"], "title": "Force-Safe Environment Maps and Real-Time Detection for Soft Robot Manipulators", "comment": null, "summary": "Soft robot manipulators have the potential for deployment in delicate\nenvironments to perform complex manipulation tasks. However, existing obstacle\ndetection and avoidance methods do not consider limits on the forces that\nmanipulators may exert upon contact with delicate obstacles. This work\nintroduces a framework that maps force safety criteria from task space (i.e.\npositions along the robot's body) to configuration space (i.e. the robot's\njoint angles) and enables real-time force safety detection. We incorporate\nlimits on allowable environmental contact forces for given task-space\nobstacles, and map them into configuration space (C-space) through the\nmanipulator's forward kinematics. This formulation ensures that configurations\nclassified as safe are provably below the maximum force thresholds, thereby\nallowing us to determine force-safe configurations of the soft robot\nmanipulator in real-time. We validate our approach in simulation and hardware\nexperiments on a two-segment pneumatic soft robot manipulator. Results\ndemonstrate that the proposed method accurately detects force safety during\ninteractions with deformable obstacles, thereby laying the foundation for\nreal-time safe planning of soft manipulators in delicate, cluttered\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u529b\u5b89\u5168\u6807\u51c6\u4ece\u4efb\u52a1\u7a7a\u95f4\u6620\u5c04\u5230\u914d\u7f6e\u7a7a\u95f4\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8f6f\u4f53\u673a\u5668\u4eba\u673a\u68b0\u81c2\u5728\u6613\u635f\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u529b\u5b89\u5168\u68c0\u6d4b", "motivation": "\u73b0\u6709\u969c\u788d\u7269\u68c0\u6d4b\u548c\u907f\u8ba9\u65b9\u6cd5\u672a\u8003\u8651\u8f6f\u4f53\u673a\u68b0\u81c2\u4e0e\u6613\u635f\u969c\u788d\u7269\u63a5\u89e6\u65f6\u7684\u529b\u9650\u5236\uff0c\u9700\u8981\u5728\u6613\u635f\u73af\u5883\u4e2d\u5b9e\u73b0\u5b89\u5168\u64cd\u4f5c", "method": "\u901a\u8fc7\u6b63\u5411\u8fd0\u52a8\u5b66\u5c06\u4efb\u52a1\u7a7a\u95f4\u4e2d\u7684\u5141\u8bb8\u63a5\u89e6\u529b\u9650\u5236\u6620\u5c04\u5230\u914d\u7f6e\u7a7a\u95f4\uff0c\u786e\u4fdd\u88ab\u5206\u7c7b\u4e3a\u5b89\u5168\u7684\u914d\u7f6e\u5728\u6700\u5927\u529b\u9608\u503c\u4ee5\u4e0b", "result": "\u5728\u53cc\u6bb5\u6c14\u52a8\u8f6f\u4f53\u673a\u5668\u4eba\u673a\u68b0\u81c2\u7684\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u68c0\u6d4b\u4e0e\u53ef\u53d8\u5f62\u969c\u788d\u7269\u4ea4\u4e92\u65f6\u7684\u529b\u5b89\u5168\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8f6f\u4f53\u673a\u68b0\u81c2\u5728\u6613\u635f\u3001\u6742\u4e71\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u5b89\u5168\u89c4\u5212\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2511.05379", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.05379", "abs": "https://arxiv.org/abs/2511.05379", "authors": ["Eric Godden", "Jacquie Groenewegen", "Matthew K. X. J. Pan"], "title": "ETHOS: A Robotic Encountered-Type Haptic Display for Social Interaction in Virtual Reality", "comment": "8 pages", "summary": "We present ETHOS (Encountered-Type Haptics for On-demand Social Interaction),\na dynamic encountered-type haptic display (ETHD) that enables natural physical\ncontact in virtual reality (VR) during social interactions such as handovers,\nfist bumps, and high-fives. The system integrates a torque-controlled robotic\nmanipulator with interchangeable passive props (silicone hand replicas and a\nbaton), marker-based physical-virtual registration via a ChArUco board, and a\nsafety monitor that gates motion based on the user's head and hand pose. We\nintroduce two control strategies: (i) a static mode that presents a stationary\nprop aligned with its virtual counterpart, consistent with prior ETHD\nbaselines, and (ii) a dynamic mode that continuously updates prop position by\nexponentially blending an initial mid-point trajectory with real-time hand\ntracking, generating a unique contact point for each interaction. Bench tests\nshow static colocation accuracy of 5.09 +/- 0.94 mm, while user interactions\nachieved temporal alignment with an average contact latency of 28.53 +/- 31.21\nms across all interaction and control conditions. These results demonstrate the\nfeasibility of recreating socially meaningful haptics in VR. By incorporating\nessential safety and control mechanisms, ETHOS establishes a practical\nfoundation for high-fidelity, dynamic interpersonal interactions in virtual\nenvironments.", "AI": {"tldr": "ETHOS\u662f\u4e00\u4e2a\u52a8\u6001\u89e6\u89c9\u663e\u793a\u7cfb\u7edf\uff0c\u901a\u8fc7\u626d\u77e9\u63a7\u5236\u673a\u68b0\u81c2\u548c\u53ef\u4e92\u6362\u9053\u5177\uff0c\u5728VR\u4e2d\u5b9e\u73b0\u81ea\u7136\u793e\u4ea4\u4e92\u52a8\u89e6\u89c9\u53cd\u9988\uff0c\u5305\u62ec\u63e1\u624b\u3001\u51fb\u638c\u7b49\u63a5\u89e6\u5f0f\u4e92\u52a8\u3002", "motivation": "\u5728\u865a\u62df\u73b0\u5b9e\u4e2d\u91cd\u73b0\u5177\u6709\u793e\u4ea4\u610f\u4e49\u7684\u89e6\u89c9\u4e92\u52a8\uff0c\u89e3\u51b3\u4f20\u7edfVR\u793e\u4ea4\u4e92\u52a8\u4e2d\u7f3a\u4e4f\u7269\u7406\u63a5\u89e6\u7684\u95ee\u9898\u3002", "method": "\u96c6\u6210\u626d\u77e9\u63a7\u5236\u673a\u68b0\u81c2\u4e0e\u53ef\u4e92\u6362\u88ab\u52a8\u9053\u5177\uff0c\u91c7\u7528\u57fa\u4e8e\u6807\u8bb0\u7684\u7269\u7406-\u865a\u62df\u914d\u51c6\uff0c\u5f15\u5165\u9759\u6001\u548c\u52a8\u6001\u4e24\u79cd\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u914d\u5907\u57fa\u4e8e\u7528\u6237\u59ff\u6001\u7684\u5b89\u5168\u76d1\u63a7\u7cfb\u7edf\u3002", "result": "\u9759\u6001\u914d\u51c6\u7cbe\u5ea6\u8fbe5.09\u00b10.94mm\uff0c\u7528\u6237\u4e92\u52a8\u5e73\u5747\u63a5\u89e6\u5ef6\u8fdf\u4e3a28.53\u00b131.21ms\uff0c\u8bc1\u660e\u4e86\u5728VR\u4e2d\u91cd\u73b0\u793e\u4ea4\u89e6\u89c9\u7684\u53ef\u884c\u6027\u3002", "conclusion": "ETHOS\u901a\u8fc7\u6574\u5408\u5173\u952e\u5b89\u5168\u548c\u63a7\u5236\u673a\u5236\uff0c\u4e3a\u865a\u62df\u73af\u5883\u4e2d\u9ad8\u4fdd\u771f\u3001\u52a8\u6001\u7684\u4eba\u9645\u4e92\u52a8\u5efa\u7acb\u4e86\u5b9e\u7528\u57fa\u7840\u3002"}}
{"id": "2511.05397", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05397", "abs": "https://arxiv.org/abs/2511.05397", "authors": ["Samarth Chopra", "Alex McMoil", "Ben Carnovale", "Evan Sokolson", "Rajkumar Kubendran", "Samuel Dickerson"], "title": "EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation", "comment": "Submitted to ICRA 2026", "summary": "While Vision-Language-Action (VLA) models map visual inputs and language\ninstructions directly to robot actions, they often rely on costly hardware and\nstruggle in novel or cluttered scenes. We introduce EverydayVLA, a 6-DOF\nmanipulator that can be assembled for under $300, capable of modest payloads\nand workspace. A single unified model jointly outputs discrete and continuous\nactions, and our adaptive-horizon ensemble monitors motion uncertainty to\ntrigger on-the-fly re-planning for safe, reliable operation. On LIBERO,\nEverydayVLA matches state-of-the-art success rates, and in real-world tests it\noutperforms prior methods by 49% in-distribution and 34.9% out-of-distribution.\nBy combining a state-of-the-art VLA with cost-effective hardware, EverydayVLA\ndemocratizes access to a robotic foundation model and paves the way for\neconomical use in homes and research labs alike. Experiment videos and details:\nhttps://everydayvla.github.io/", "AI": {"tldr": "EverydayVLA\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\uff08300\u7f8e\u5143\u4ee5\u4e0b\uff09\u76846\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\uff0c\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u89c4\u5212\u96c6\u6210\u5b9e\u73b0\u53ef\u9760\u64cd\u4f5c\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4f9d\u8d56\u6602\u8d35\u786c\u4ef6\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u9700\u8981\u5f00\u53d1\u4f4e\u6210\u672c\u4e14\u53ef\u9760\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u4f7f\u5176\u80fd\u5728\u5bb6\u5ead\u548c\u7814\u7a76\u5b9e\u9a8c\u5ba4\u4e2d\u666e\u53ca\u4f7f\u7528\u3002", "method": "\u4f7f\u7528\u5355\u4e00\u7edf\u4e00\u6a21\u578b\u8f93\u51fa\u79bb\u6563\u548c\u8fde\u7eed\u52a8\u4f5c\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u89c4\u5212\u96c6\u6210\u76d1\u63a7\u8fd0\u52a8\u4e0d\u786e\u5b9a\u6027\u5e76\u89e6\u53d1\u5b9e\u65f6\u91cd\u65b0\u89c4\u5212\uff0c\u786e\u4fdd\u5b89\u5168\u53ef\u9760\u7684\u64cd\u4f5c\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6210\u529f\u7387\uff0c\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\u5206\u522b\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u573a\u666f\u4e2d\u63d0\u534749%\u548c34.9%\u7684\u6027\u80fd\u3002", "conclusion": "EverydayVLA\u901a\u8fc7\u7ed3\u5408\u6700\u5148\u8fdb\u7684VLA\u6a21\u578b\u4e0e\u4f4e\u6210\u672c\u786c\u4ef6\uff0c\u4e3a\u673a\u5668\u4eba\u5728\u5bb6\u5ead\u548c\u7814\u7a76\u5b9e\u9a8c\u5ba4\u7684\u7ecf\u6d4e\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u7684\u6c11\u4e3b\u5316\u8bbf\u95ee\u3002"}}
{"id": "2511.05402", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05402", "abs": "https://arxiv.org/abs/2511.05402", "authors": ["Muhammad Saud Ul Hassan", "Derek Vasquez", "Hamza Asif", "Christian Hubicki"], "title": "Stable and Robust SLIP Model Control via Energy Conservation-Based Feedback Cancellation for Quadrupedal Applications", "comment": null, "summary": "In this paper, we present an energy-conservation based control architecture\nfor stable dynamic motion in quadruped robots. We model the robot as a\nSpring-loaded Inverted Pendulum (SLIP), a model well-suited to represent the\nbouncing motion characteristic of running gaits observed in various biological\nquadrupeds and bio-inspired robotic systems. The model permits leg-orientation\ncontrol during flight and leg-length control during stance, a design choice\ninspired by natural quadruped behaviors and prevalent in robotic quadruped\nsystems. Our control algorithm uses the reduced-order SLIP dynamics of the\nquadruped to track a stable parabolic spline during stance, which is calculated\nusing the principle of energy conservation. Through simulations based on the\ndesign specifications of an actual quadruped robot, Ghost Robotics Minitaur, we\ndemonstrate that our control algorithm generates stable bouncing gaits.\nAdditionally, we illustrate the robustness of our controller by showcasing its\nability to maintain stable bouncing even when faced with up to a 10% error in\nsensor measurements.", "AI": {"tldr": "\u57fa\u4e8e\u80fd\u91cf\u5b88\u6052\u7684\u56db\u8db3\u673a\u5668\u4eba\u63a7\u5236\u67b6\u6784\uff0c\u4f7f\u7528\u5f39\u7c27\u5012\u7acb\u6446\u6a21\u578b\u5b9e\u73b0\u7a33\u5b9a\u7684\u52a8\u6001\u8fd0\u52a8\uff0c\u5728\u6a21\u62df\u4e2d\u5c55\u793a\u4e86\u7a33\u5b9a\u7684\u5f39\u8df3\u6b65\u6001\u548c\u5bf9\u4f20\u611f\u5668\u8bef\u5dee\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5b9e\u73b0\u56db\u8db3\u673a\u5668\u4eba\u7a33\u5b9a\u52a8\u6001\u8fd0\u52a8\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5954\u8dd1\u6b65\u6001\uff0c\u501f\u9274\u751f\u7269\u56db\u8db3\u52a8\u7269\u7684\u8fd0\u52a8\u7279\u6027\u548c\u73b0\u6709\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8bbe\u8ba1\u3002", "method": "\u5c06\u673a\u5668\u4eba\u5efa\u6a21\u4e3a\u5f39\u7c27\u5012\u7acb\u6446\u6a21\u578b\uff0c\u5728\u98de\u884c\u9636\u6bb5\u63a7\u5236\u817f\u90e8\u65b9\u5411\uff0c\u5728\u652f\u6491\u9636\u6bb5\u63a7\u5236\u817f\u90e8\u957f\u5ea6\uff0c\u57fa\u4e8e\u80fd\u91cf\u5b88\u6052\u539f\u7406\u8ddf\u8e2a\u7a33\u5b9a\u7684\u629b\u7269\u7ebf\u6837\u6761\u3002", "result": "\u5728\u57fa\u4e8eGhost Robotics Minitaur\u673a\u5668\u4eba\u89c4\u683c\u7684\u6a21\u62df\u4e2d\uff0c\u63a7\u5236\u7b97\u6cd5\u6210\u529f\u751f\u6210\u4e86\u7a33\u5b9a\u7684\u5f39\u8df3\u6b65\u6001\uff0c\u5e76\u4e14\u5728\u9762\u5bf9\u9ad8\u8fbe10%\u7684\u4f20\u611f\u5668\u6d4b\u91cf\u8bef\u5dee\u65f6\u4ecd\u80fd\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "\u57fa\u4e8e\u80fd\u91cf\u5b88\u6052\u7684SLIP\u6a21\u578b\u63a7\u5236\u67b6\u6784\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u56db\u8db3\u673a\u5668\u4eba\u7684\u7a33\u5b9a\u52a8\u6001\u8fd0\u52a8\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.05426", "categories": ["cs.RO", "J.2"], "pdf": "https://arxiv.org/pdf/2511.05426", "abs": "https://arxiv.org/abs/2511.05426", "authors": ["Luca Girardi", "Gabriel Maquignaz", "Stefano Mintchev"], "title": "Bioinspired Soft Quadrotors Jointly Unlock Agility, Squeezability, and Collision Resilience", "comment": "26 pages, 12 figures, 2 tables, 9 videos (not yet disclosed, awaiting\n  peer review)", "summary": "Natural flyers use soft wings to seamlessly enable a wide range of flight\nbehaviours, including agile manoeuvres, squeezing through narrow passageways,\nand withstanding collisions. In contrast, conventional quadrotor designs rely\non rigid frames that support agile flight but inherently limit collision\nresilience and squeezability, thereby constraining flight capabilities in\ncluttered environments. Inspired by the anisotropic stiffness and distributed\nmass-energy structures observed in biological organisms, we introduce\nFlexiQuad, a soft-frame quadrotor design approach that limits this trade-off.\nWe demonstrate a 405-gram FlexiQuad prototype, three orders of magnitude more\ncompliant than conventional quadrotors, yet capable of acrobatic manoeuvres\nwith peak speeds above 80 km/h and linear and angular accelerations exceeding 3\ng and 300 rad/s$^2$, respectively. Analysis demonstrates it can replicate\naccelerations of rigid counterparts up to a thrust-to-weight ratio of 8.\nSimultaneously, FlexiQuad exhibits fourfold higher collision resilience,\nsurviving frontal impacts at 5 m/s without damage and reducing destabilising\nforces in glancing collisions by a factor of 39. Its frame can fully compress,\nenabling flight through gaps as narrow as 70% of its nominal width. Our\nanalysis identifies an optimal structural softness range, from 0.006 to 0.77\nN/mm, comparable to that of natural flyers' wings, whereby agility,\nsqueezability, and collision resilience are jointly achieved for FlexiQuad\nmodels from 20 to 3000 grams. FlexiQuad expands hovering drone capabilities in\ncomplex environments, enabling robust physical interactions without\ncompromising flight performance.", "AI": {"tldr": "FlexiQuad\u662f\u4e00\u79cd\u8f6f\u6846\u67b6\u56db\u65cb\u7ffc\u8bbe\u8ba1\uff0c\u901a\u8fc7\u6a21\u4eff\u751f\u7269\u98de\u884c\u5668\u7684\u5404\u5411\u5f02\u6027\u521a\u5ea6\u548c\u5206\u5e03\u5f0f\u8d28\u91cf\u80fd\u91cf\u7ed3\u6784\uff0c\u5728\u4fdd\u6301\u654f\u6377\u98de\u884c\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u78b0\u649e\u6062\u590d\u80fd\u529b\u548c\u6324\u538b\u901a\u8fc7\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u56db\u65cb\u7ffc\u7684\u521a\u6027\u6846\u67b6\u9650\u5236\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u98de\u884c\u80fd\u529b\uff0c\u7279\u522b\u662f\u78b0\u649e\u6062\u590d\u548c\u901a\u8fc7\u72ed\u7a84\u901a\u9053\u7684\u80fd\u529b\u3002\u53d7\u751f\u7269\u98de\u884c\u5668\u8f6f\u7ffc\u7ed3\u6784\u7684\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u540c\u65f6\u4fdd\u6301\u654f\u6377\u98de\u884c\u548c\u7269\u7406\u4ea4\u4e92\u80fd\u529b\u7684\u8f6f\u6846\u67b6\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u8f6f\u6846\u67b6\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4f7f\u56db\u65cb\u7ffc\u6846\u67b6\u5177\u6709\u5404\u5411\u5f02\u6027\u521a\u5ea6\u548c\u5206\u5e03\u5f0f\u8d28\u91cf\u80fd\u91cf\u7ed3\u6784\uff0c\u6846\u67b6\u67d4\u5ea6\u6bd4\u4f20\u7edf\u56db\u65cb\u7ffc\u9ad8\u4e09\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u673a\u52a8\u6027\u98de\u884c\u3002", "result": "405\u514b\u539f\u578b\u673a\u5b9e\u73b0\u4e86\u5cf0\u503c\u901f\u5ea680+ km/h\u3001\u7ebf\u6027\u52a0\u901f\u5ea63g\u3001\u89d2\u52a0\u901f\u5ea6300 rad/s\u00b2\u7684\u654f\u6377\u673a\u52a8\uff0c\u80fd\u627f\u53d75 m/s\u6b63\u9762\u78b0\u649e\u65e0\u635f\u4f24\uff0c\u53ef\u901a\u8fc770%\u540d\u4e49\u5bbd\u5ea6\u7684\u95f4\u9699\uff0c\u78b0\u649e\u6062\u590d\u80fd\u529b\u63d0\u9ad84\u500d\uff0c\u6ed1\u79fb\u78b0\u649e\u529b\u51cf\u5c0f39\u500d\u3002", "conclusion": "FlexiQuad\u57280.006-0.77 N/mm\u7684\u6700\u4f73\u7ed3\u6784\u67d4\u5ea6\u8303\u56f4\u5185\uff0c\u53ef\u540c\u65f6\u5b9e\u73b0\u654f\u6377\u6027\u3001\u6324\u538b\u6027\u548c\u78b0\u649e\u6062\u590d\u6027\uff0c\u6269\u5c55\u4e86\u60ac\u505c\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u98de\u884c\u6027\u80fd\u4e0e\u7269\u7406\u4ea4\u4e92\u80fd\u529b\u7684\u5e73\u8861\u3002"}}
