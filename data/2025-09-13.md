<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 19]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Multi Robot Coordination in Highly Dynamic Environments: Tackling Asymmetric Obstacles and Limited Communication](https://arxiv.org/abs/2509.08859)
*Vincenzo Suriani,Daniele Affinita,Domenico D. Bloisi,Daniele Nardi*

Main category: cs.RO

TL;DR: 提出一种分布式多智能体协调方法，用于通信受限、环境部分可观测且存在非对称障碍物的场景，显著减少任务重叠52%


<details>
  <summary>Details</summary>
Motivation: 解决通信能力有限、环境部分可观测且存在非对称障碍物的多智能体系统协调挑战

Method: 受市场机制启发的分布式协调算法，专门处理非对称障碍物和频繁任务重分配

Result: 在仿真和真实NAO机器人实验中验证，在有限通信环境下任务重叠减少52%

Conclusion: 该方法能有效处理通信受限、部分可观测环境中的非对称障碍物协调问题

Abstract: Coordinating a fully distributed multi-agent system (MAS) can be challenging
when the communication channel has very limited capabilities in terms of
sending rate and packet payload. When the MAS has to deal with active obstacles
in a highly partially observable environment, the communication channel
acquires considerable relevance. In this paper, we present an approach to deal
with task assignments in extremely active scenarios, where tasks need to be
frequently reallocated among the agents participating in the coordination
process. Inspired by market-based task assignments, we introduce a novel
distributed coordination method to orchestrate autonomous agents' actions
efficiently in low communication scenarios. In particular, our algorithm takes
into account asymmetric obstacles. While in the real world, the majority of
obstacles are asymmetric, they are usually treated as symmetric ones, thus
limiting the applicability of existing methods. To summarize, the presented
architecture is designed to tackle scenarios where the obstacles are active and
asymmetric, the communication channel is poor and the environment is partially
observable. Our approach has been validated in simulation and in the real
world, using a team of NAO robots during official RoboCup competitions.
Experimental results show a notable reduction in task overlaps in limited
communication settings, with a decrease of 52% in the most frequent reallocated
task.

</details>


### [2] [Rapid Manufacturing of Lightweight Drone Frames Using Single-Tow Architected Composites](https://arxiv.org/abs/2509.09024)
*Md Habib Ullah Khan,Kaiyue Deng,Ismail Mujtaba Khan,Kelvin Fu*

Main category: cs.RO

TL;DR: 通过3D纤维缆结构技术（3DFiT）制造的连续纤维增强面心立方格子旗屏机架，轻量化效果显著（仅260g），比商用DJI F450轻10%，特异强度是金属和热塑料的4-8倍，并延长飞行时间3分钟


<details>
  <summary>Details</summary>
Motivation: 传统复合材料制造方法无法实现复杂三维结构轻量化，且组装接头存在弱点，连续纤维增强遇到挑战

Method: 采用3D纤维缆结构技术（3DFiT），使用连续单纽纤维构建面心立方格子结构，确保精确的纤维对齐，消除传统组装弱点

Result: 制造的旗屏机架特异强度达到金属和热塑料的4-8倍，重量仅260g，比DJI F450轻10%，飞行时间延长3分钟，飞行测试证明其稳定性和耐用性

Conclusion: 单纽格子架构旗屏机具有很大潜力，3DFiT技术作为一种可扩展的高效制造方法，为航空航天和机器人领域的轻量化高强度复合材料结构提供了新的解决方案

Abstract: The demand for lightweight and high-strength composite structures is rapidly
growing in aerospace and robotics, particularly for optimized drone frames.
However, conventional composite manufacturing methods struggle to achieve
complex 3D architectures for weight savings and rely on assembling separate
components, which introduce weak points at the joints. Additionally,
maintaining continuous fiber reinforcement remains challenging, limiting
structural efficiency. In this study, we demonstrate the lightweight Face
Centered Cubic (FFC) lattice structured conceptualization of drone frames for
weight reduction and complex topology fabrication through 3D Fiber Tethering
(3DFiT) using continuous single tow fiber ensuring precise fiber alignment,
eliminating weak points associated with traditional composite assembly.
Mechanical testing demonstrates that the fabricated drone frame exhibits a high
specific strength of around four to eight times the metal and thermoplastic,
outperforming other conventional 3D printing methods. The drone frame weighs
only 260 g, making it 10% lighter than the commercial DJI F450 frame, enhancing
structural integrity and contributing to an extended flight time of three
minutes, while flight testing confirms its stability and durability under
operational conditions. The findings demonstrate the potential of single tow
lattice truss-based drone frames, with 3DFiT serving as a scalable and
efficient manufacturing method.

</details>


### [3] [KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning](https://arxiv.org/abs/2509.09074)
*Alice Kate Li,Thales C Silva,Victoria Edwards,Vijay Kumar,M. Ani Hsieh*

Main category: cs.RO

TL;DR: KoopMotion是一种基于流场的运动规划方法，利用Koopman算子理论构建动态系统，使机器人从任意初始状态收敛到期望轨迹的终点，具有高样本效率和优异的时空动态建模性能。


<details>
  <summary>Details</summary>
Motivation: 虽然Koopman算子理论在动态系统建模中表现出色，但它本身不能确保系统收敛到期望轨迹或指定目标点，这在从演示中学习(LfD)场景中是必需的要求。

Method: 提出KoopMotion方法，将运动流场表示为动态系统，通过Koopman算子参数化来模仿期望轨迹，并利用学习到的流场的发散特性获得平滑的运动场，使机器人能够收敛到参考轨迹并跟踪至终点。

Result: 在LASA手写数据集和3D机械臂末端轨迹数据集上进行了评估，包括频谱分析。在物理机器人上的实验验证了方法在非静态流体环境中的有效性，仅需LASA数据集的3%即可生成密集运动规划，在时空动态建模指标上显著优于基线方法。

Conclusion: KoopMotion提供了一种样本高效的流场运动规划方法，能够有效解决机器人从任意初始状态收敛到期望轨迹的问题，在复杂环境中表现出良好的性能。

Abstract: In this work, we propose a novel flow field-based motion planning method that
drives a robot from any initial state to a desired reference trajectory such
that it converges to the trajectory's end point. Despite demonstrated efficacy
in using Koopman operator theory for modeling dynamical systems, Koopman does
not inherently enforce convergence to desired trajectories nor to specified
goals -- a requirement when learning from demonstrations (LfD). We present
KoopMotion which represents motion flow fields as dynamical systems,
parameterized by Koopman Operators to mimic desired trajectories, and leverages
the divergence properties of the learnt flow fields to obtain smooth motion
fields that converge to a desired reference trajectory when a robot is placed
away from the desired trajectory, and tracks the trajectory until the end
point. To demonstrate the effectiveness of our approach, we show evaluations of
KoopMotion on the LASA human handwriting dataset and a 3D manipulator
end-effector trajectory dataset, including spectral analysis. We also perform
experiments on a physical robot, verifying KoopMotion on a miniature autonomous
surface vehicle operating in a non-static fluid flow environment. Our approach
is highly sample efficient in both space and time, requiring only 3\% of the
LASA dataset to generate dense motion plans. Additionally, KoopMotion provides
a significant improvement over baselines when comparing metrics that measure
spatial and temporal dynamics modeling efficacy.

</details>


### [4] [Kinetostatics and Particle-Swarm Optimization of Vehicle-Mounted Underactuated Metamorphic Loading Manipulators](https://arxiv.org/abs/2509.09093)
*Nan Mao,Guanglu Jia,Junpeng Chen,Emmanouil Spyrakos-Papastavridis,Jian S. Dai*

Main category: cs.RO

TL;DR: 提出了一种欠驱动变形加载机械臂(UMLM)，结合变形臂和被动自适应夹爪，通过几何约束实现拓扑重构和灵活运动轨迹，无需额外驱动器。


<details>
  <summary>Details</summary>
Motivation: 解决传统固定自由度加载机构存在执行器过多、控制复杂、对动态任务适应性有限的问题，需要开发更高效、自适应的加载解决方案。

Method: 集成变形臂和被动自适应夹爪，利用几何约束实现拓扑重构；建立结构模型并进行运动静力学分析；使用粒子群优化(PSO)优化夹爪尺寸参数。

Result: 仿真验证了UMLM控制策略易于实现、操作灵活，在动态环境中能有效抓取各种物体，表现出强大的适应性。

Conclusion: 欠驱动变形机构在需要高效自适应加载的应用中具有实际潜力，所提出的通用建模和优化框架可扩展到更广泛的机械臂类别，为开发高效、灵活、鲁棒的机器人系统提供了可扩展方法。

Abstract: Fixed degree-of-freedom (DoF) loading mechanisms often suffer from excessive
actuators, complex control, and limited adaptability to dynamic tasks. This
study proposes an innovative mechanism of underactuated metamorphic loading
manipulators (UMLM), integrating a metamorphic arm with a passively adaptive
gripper. The metamorphic arm exploits geometric constraints, enabling the
topology reconfiguration and flexible motion trajectories without additional
actuators. The adaptive gripper, driven entirely by the arm, conforms to
diverse objects through passive compliance. A structural model is developed,
and a kinetostatics analysis is conducted to investigate isomorphic grasping
configurations. To optimize performance, Particle-Swarm Optimization (PSO) is
utilized to refine the gripper's dimensional parameters, ensuring robust
adaptability across various applications. Simulation results validate the
UMLM's easily implemented control strategy, operational versatility, and
effectiveness in grasping diverse objects in dynamic environments. This work
underscores the practical potential of underactuated metamorphic mechanisms in
applications requiring efficient and adaptable loading solutions. Beyond the
specific design, this generalized modeling and optimization framework extends
to a broader class of manipulators, offering a scalable approach to the
development of robotic systems that require efficiency, flexibility, and robust
performance.

</details>


### [5] [LIPM-Guided Reinforcement Learning for Stable and Perceptive Locomotion in Bipedal Robots](https://arxiv.org/abs/2509.09106)
*Haokai Su,Haoxiang Luo,Shunpeng Yang,Kaiwen Jiang,Wei Zhang,Hua Chen*

Main category: cs.RO

TL;DR: 提出基于LIPM模型的新型奖励设计，通过双评论家架构和奖励融合模块实现双足机器人在非结构化户外环境中的感知性稳定运动


<details>
  <summary>Details</summary>
Motivation: 解决双足机器人在复杂非结构化户外环境中因地形几何复杂和外部干扰导致的感知运动不稳定问题

Method: 基于LIPM模型设计奖励函数，调节质心高度和躯干姿态；采用奖励融合模块(RFM)自适应权衡速度跟踪与稳定性；使用双评论家架构分别评估稳定性和运动目标

Result: 在仿真和真实户外环境中验证，表现出优异的地形适应性、抗干扰能力，以及在各种速度和感知条件下的一致性能

Conclusion: 提出的LIPM启发奖励设计和双评论家架构有效提升了双足机器人在野外环境中的感知性稳定运动能力

Abstract: Achieving stable and robust perceptive locomotion for bipedal robots in
unstructured outdoor environments remains a critical challenge due to complex
terrain geometry and susceptibility to external disturbances. In this work, we
propose a novel reward design inspired by the Linear Inverted Pendulum Model
(LIPM) to enable perceptive and stable locomotion in the wild. The LIPM
provides theoretical guidance for dynamic balance by regulating the center of
mass (CoM) height and the torso orientation. These are key factors for
terrain-aware locomotion, as they help ensure a stable viewpoint for the
robot's camera. Building on this insight, we design a reward function that
promotes balance and dynamic stability while encouraging accurate CoM
trajectory tracking. To adaptively trade off between velocity tracking and
stability, we leverage the Reward Fusion Module (RFM) approach that prioritizes
stability when needed. A double-critic architecture is adopted to separately
evaluate stability and locomotion objectives, improving training efficiency and
robustness. We validate our approach through extensive experiments on a bipedal
robot in both simulation and real-world outdoor environments. The results
demonstrate superior terrain adaptability, disturbance rejection, and
consistent performance across a wide range of speeds and perceptual conditions.

</details>


### [6] [AEOS: Active Environment-aware Optimal Scanning Control for UAV LiDAR-Inertial Odometry in Complex Scenes](https://arxiv.org/abs/2509.09141)
*Jianping Li,Xinhang Xu,Zhongyuan Liu,Shenghai Yuan,Muqing Cao,Lihua Xie*

Main category: cs.RO

TL;DR: AEOS是一个受猫头鹰主动感知启发的自适应LiDAR控制框架，结合MPC和RL来提高无人机LiDAR惯性里程计在复杂环境中的性能


<details>
  <summary>Details</summary>
Motivation: 解决无人机LiDAR感知的局限性：紧凑LiDAR的窄视场角限制、无法使用多传感器配置、传统电机扫描系统缺乏场景感知和任务适应性

Method: 混合架构结合模型预测控制(MPC)和强化学习(RL)：分析不确定性模型预测未来位姿可观测性，轻量神经网络从全景深度表示学习隐式代价图来指导探索

Result: 在仿真和真实环境中的大量实验表明，AEOS相比固定速率、仅优化和完全学习基线显著提高了里程计精度，同时保持实时性能

Conclusion: AEOS框架通过生物启发的方法有效解决了无人机LiDAR感知的局限性，在复杂遮挡环境中实现了更好的定位和建图性能

Abstract: LiDAR-based 3D perception and localization on unmanned aerial vehicles (UAVs)
are fundamentally limited by the narrow field of view (FoV) of compact LiDAR
sensors and the payload constraints that preclude multi-sensor configurations.
Traditional motorized scanning systems with fixed-speed rotations lack scene
awareness and task-level adaptability, leading to degraded odometry and mapping
performance in complex, occluded environments. Inspired by the active sensing
behavior of owls, we propose AEOS (Active Environment-aware Optimal Scanning),
a biologically inspired and computationally efficient framework for adaptive
LiDAR control in UAV-based LiDAR-Inertial Odometry (LIO). AEOS combines model
predictive control (MPC) and reinforcement learning (RL) in a hybrid
architecture: an analytical uncertainty model predicts future pose
observability for exploitation, while a lightweight neural network learns an
implicit cost map from panoramic depth representations to guide exploration. To
support scalable training and generalization, we develop a point cloud-based
simulation environment with real-world LiDAR maps across diverse scenes,
enabling sim-to-real transfer. Extensive experiments in both simulation and
real-world environments demonstrate that AEOS significantly improves odometry
accuracy compared to fixed-rate, optimization-only, and fully learned
baselines, while maintaining real-time performance under onboard computational
constraints. The project page can be found at
https://kafeiyin00.github.io/AEOS/.

</details>


### [7] [Occupancy-aware Trajectory Planning for Autonomous Valet Parking in Uncertain Dynamic Environments](https://arxiv.org/abs/2509.09206)
*Farhad Nawaz,Faizan M. Tariq,Sangjae Bae,David Isele,Avinash Singh,Nadia Figueroa,Nikolai Matni,Jovin D'sa*

Main category: cs.RO

TL;DR: 基于动态环境中预测停车位占用情况的自主代客停车方案，通过区分初始空闲/占用位置、动态代理运动预测和信息收益平衡，提升了停车效率和安全性


<details>
  <summary>Details</summary>
Motivation: 解决在动态不确定环境中自主代客停车的挑战，现有方法仅依靠瞬时观测或静态假设，无法准确预测未来停车位可用性

Method: 提出概率性停车位占用估计器，在限制视野模型中结合部分和噪声观测，考虑未观测区域的不确定性；设计策略规划器，基于信息收益适应性平衡目标导向停车机动和探索导航，在有前景位置智能采用等待-行动行为

Result: 通过模拟大型停车场的随机模拟实验，证明该框架在停车效率、安全距离和轨迹平滑性方面显著优于现有方法

Conclusion: 该方法通过系统性地预测停车位可用性和智能规划策略，为自主代客停车提供了更加安全高效的解决方案

Abstract: Accurately reasoning about future parking spot availability and integrated
planning is critical for enabling safe and efficient autonomous valet parking
in dynamic, uncertain environments. Unlike existing methods that rely solely on
instantaneous observations or static assumptions, we present an approach that
predicts future parking spot occupancy by explicitly distinguishing between
initially vacant and occupied spots, and by leveraging the predicted motion of
dynamic agents. We introduce a probabilistic spot occupancy estimator that
incorporates partial and noisy observations within a limited Field-of-View
(FoV) model and accounts for the evolving uncertainty of unobserved regions.
Coupled with this, we design a strategy planner that adaptively balances
goal-directed parking maneuvers with exploratory navigation based on
information gain, and intelligently incorporates wait-and-go behaviors at
promising spots. Through randomized simulations emulating large parking lots,
we demonstrate that our framework significantly improves parking efficiency,
safety margins, and trajectory smoothness compared to existing approaches.

</details>


### [8] [RENet: Fault-Tolerant Motion Control for Quadruped Robots via Redundant Estimator Networks under Visual Collapse](https://arxiv.org/abs/2509.09283)
*Yueqi Zhang,Quancheng Qian,Taixian Hou,Peng Zhai,Xiaoyi Wei,Kangmai Hu,Jiafu Yi,Lihua Zhang*

Main category: cs.RO

TL;DR: 提出了Redundant Estimator Network (RENet)框架，通过双估计器架构解决四足机器人在户外环境中视觉定位的挑战，特别是在深度传感器噪声和视觉感知失效情况下的稳定部署问题。


<details>
  <summary>Details</summary>
Motivation: 解决四足机器人在户外环境中基于视觉的 locomotion 面临的挑战，包括环境预测准确性不足和深度传感器噪声处理困难，这些限制了算法在户外环境中的实际应用。

Method: 采用冗余估计器网络(RENet)框架，使用双估计器架构，通过在线估计器自适应实现估计模块之间的无缝切换，以处理视觉感知不确定性。

Result: 在真实机器人上的实验验证表明，该框架在复杂户外环境中有效，特别是在视觉感知退化场景中表现出优势。

Conclusion: 该框架展示了在具有挑战性的野外条件下实现可靠机器人部署的潜力，是一个实用的解决方案。

Abstract: Vision-based locomotion in outdoor environments presents significant
challenges for quadruped robots. Accurate environmental prediction and
effective handling of depth sensor noise during real-world deployment remain
difficult, severely restricting the outdoor applications of such algorithms. To
address these deployment challenges in vision-based motion control, this letter
proposes the Redundant Estimator Network (RENet) framework. The framework
employs a dual-estimator architecture that ensures robust motion performance
while maintaining deployment stability during onboard vision failures. Through
an online estimator adaptation, our method enables seamless transitions between
estimation modules when handling visual perception uncertainties. Experimental
validation on a real-world robot demonstrates the framework's effectiveness in
complex outdoor environments, showing particular advantages in scenarios with
degraded visual perception. This framework demonstrates its potential as a
practical solution for reliable robotic deployment in challenging field
conditions. Project website: https://RENet-Loco.github.io/

</details>


### [9] [OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning](https://arxiv.org/abs/2509.09332)
*Yuecheng Liu,Dafeng Chi,Shiguang Wu,Zhanguang Zhang,Yuzheng Zhuang,Bowen Yang,He Zhu,Lingfeng Zhang,Pengwei Xie,David Gamaliel Arcos Bravo,Yingxue Zhang,Jianye Hao,Xingyue Quan*

Main category: cs.RO

TL;DR: OmniEVA是一个多模态大语言模型驱动的具身智能规划器，通过任务自适应的3D接地机制和具身感知推理框架，解决了现有系统在几何适应性和物理约束方面的局限性，在多种具身任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前基于MLLM的具身系统面临两个关键限制：几何适应性差距（2D输入或硬编码3D几何注入导致空间信息不足或泛化能力受限）和具身约束差距（忽视真实机器人的物理约束导致计划不可行）。

Method: 提出两个关键创新：(1)任务自适应的3D接地机制，使用门控路由器根据上下文需求进行选择性3D融合；(2)具身感知推理框架，将任务目标和物理约束共同纳入推理循环。

Result: 实验结果表明OmniEVA在通用具身推理性能上达到最先进水平，在广泛的下游场景中表现出强大能力，在原始和复合任务基准测试中确认了其鲁棒和通用的规划能力。

Conclusion: OmniEVA通过创新的3D接地和具身感知推理方法，有效解决了现有MLLM具身系统的局限性，为具身智能提供了更实用和适应性强的规划解决方案。

Abstract: Recent advances in multimodal large language models (MLLMs) have opened new
opportunities for embodied intelligence, enabling multimodal understanding,
reasoning, and interaction, as well as continuous spatial decision-making.
Nevertheless, current MLLM-based embodied systems face two critical
limitations. First, Geometric Adaptability Gap: models trained solely on 2D
inputs or with hard-coded 3D geometry injection suffer from either insufficient
spatial information or restricted 2D generalization, leading to poor
adaptability across tasks with diverse spatial demands. Second, Embodiment
Constraint Gap: prior work often neglects the physical constraints and
capacities of real robots, resulting in task plans that are theoretically valid
but practically infeasible.To address these gaps, we introduce OmniEVA -- an
embodied versatile planner that enables advanced embodied reasoning and task
planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding
mechanism, which introduces a gated router to perform explicit selective
regulation of 3D fusion based on contextual requirements, enabling
context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware
Reasoning framework that jointly incorporates task goals and embodiment
constraints into the reasoning loop, resulting in planning decisions that are
both goal-directed and executable. Extensive experimental results demonstrate
that OmniEVA not only achieves state-of-the-art general embodied reasoning
performance, but also exhibits a strong ability across a wide range of
downstream scenarios. Evaluations of a suite of proposed embodied benchmarks,
including both primitive and composite tasks, confirm its robust and versatile
planning capabilities. Project page: https://omnieva.github.io

</details>


### [10] [AGILOped: Agile Open-Source Humanoid Robot for Research](https://arxiv.org/abs/2509.09364)
*Grzegorz Ficht,Luis Denninger,Sven Behnke*

Main category: cs.RO

TL;DR: AGILOped是一个开源人形机器人，填补了高性能与可访问性之间的差距，使用现成的驱动器和标准电子元件，单人即可操作，实验验证了其行走、跳跃、冲击缓解和起身等能力。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人平台多为闭源或成本高昂，限制了研究和应用的可及性。本文旨在开发一个开源、高性能且易于获取的人形机器人平台。

Method: 使用现成的可反向驱动驱动器和高功率密度驱动器，采用标准电子元件，设计了一个高110cm、重14.5kg的轻量化机器人结构，无需龙门架即可单人操作。

Result: 实验展示了AGILOped在行走、跳跃、冲击缓解和起身等多种运动任务中的可行性和性能表现。

Conclusion: AGILOped成功实现了高性能与可访问性的平衡，为研究社区提供了一个实用的开源人形机器人平台。

Abstract: With academic and commercial interest for humanoid robots peaking, multiple
platforms are being developed. Through a high level of customization, they
showcase impressive performance. Most of these systems remain closed-source or
have high acquisition and maintenance costs, however. In this work, we present
AGILOped - an open-source humanoid robot that closes the gap between high
performance and accessibility. Our robot is driven by off-the-shelf
backdrivable actuators with high power density and uses standard electronic
components. With a height of 110 cm and weighing only 14.5 kg, AGILOped can be
operated without a gantry by a single person. Experiments in walking, jumping,
impact mitigation and getting-up demonstrate its viability for use in research.

</details>


### [11] [VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model](https://arxiv.org/abs/2509.09372)
*Yihao Wang,Pengxiang Ding,Lingxiao Li,Can Cui,Zirui Ge,Xinyang Tong,Wenxuan Song,Han Zhao,Wei Zhao,Pengxu Hou,Siteng Huang,Yifan Tang,Wenhui Wang,Ru Zhang,Jianyi Liu,Donglin Wang*

Main category: cs.RO

TL;DR: VLA-Adapter是一种轻量级方法，通过桥接注意力机制将视觉语言表示有效连接到动作空间，无需大规模VLM预训练和机器人数据，仅需0.5B参数主干网络即可实现高性能。


<details>
  <summary>Details</summary>
Motivation: 传统VLA模型依赖大规模视觉语言模型预训练和机器人数据，训练成本高昂。本文旨在降低这种依赖，探索如何有效桥接视觉语言表示与动作空间。

Method: 首先系统分析各种视觉语言条件的有效性，提出关键发现；然后设计轻量级策略模块，采用桥接注意力机制自动将最优条件注入动作空间。

Result: 在仿真和真实机器人基准测试中达到最先进性能，推理速度最快，仅需单消费级GPU8小时即可训练强大VLA模型。

Conclusion: VLA-Adapter显著降低了VLA模型的部署门槛，提供高效且成本低廉的解决方案，无需大规模预训练即可实现高性能。

Abstract: Vision-Language-Action (VLA) models typically bridge the gap between
perceptual and action spaces by pre-training a large-scale Vision-Language
Model (VLM) on robotic data. While this approach greatly enhances performance,
it also incurs significant training costs. In this paper, we investigate how to
effectively bridge vision-language (VL) representations to action (A). We
introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA
models on large-scale VLMs and extensive pre-training. To this end, we first
systematically analyze the effectiveness of various VL conditions and present
key findings on which conditions are essential for bridging perception and
action spaces. Based on these insights, we propose a lightweight Policy module
with Bridge Attention, which autonomously injects the optimal condition into
the action space. In this way, our method achieves high performance using only
a 0.5B-parameter backbone, without any robotic data pre-training. Extensive
experiments on both simulated and real-world robotic benchmarks demonstrate
that VLA-Adapter not only achieves state-of-the-art level performance, but also
offers the fast inference speed reported to date. Furthermore, thanks to the
proposed advanced bridging paradigm, VLA-Adapter enables the training of a
powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly
lowering the barrier to deploying the VLA model. Project page:
https://vla-adapter.github.io/.

</details>


### [12] [A Hybrid Hinge-Beam Continuum Robot with Passive Safety Capping for Real-Time Fatigue Awareness](https://arxiv.org/abs/2509.09404)
*Tongshun Chen,Zezhou Sun,Yanhan Sun,Yuhao Wang,Dezhen Song,Ke Wu*

Main category: cs.RO

TL;DR: 基于混合轴翼-梁结构的疲动感知连续体机器人设计，通过解耦扩展和弯曲、被动限位机构和电机扭矩监测、基于刚度估计的实时疲动评估方法，实现了疲动积累减少49%，提高了长期操作的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 线驱连续体机器人在长期使用中容易发生机械疲动，包括塑性变形和材料降级，影响性能并带来结构失效风险。目前连续体机器人的疲动估计研究较少，限制了其长期操作能力。

Method: 提出了三项核心创新：(1)混合轴翼-梁结构，通过TwistBeam和BendBeam解耦扩展和弯曲，减少应力集中；(2)被动限位器，通过机械约束安全限制运动并使用电机扭矩传感检测极限扭矩；(3)实时疲动感知方法，基于极限位置的电机扭矩估计刚度，实现无需额外传感器的在线疲动估计。

Result: 实验结果显示，提出的设计与传统设计相比疲动积累减少约49%，被动机械限制结合电机侦测能够准确估计结构疲动和损坏。

Conclusion: 证实了提出的架构在安全可靠的长期操作方面的有效性，为连续体机器人的疲动监测和长期稳定性提供了新的解决方案。

Abstract: Cable-driven continuum robots offer high flexibility and lightweight design,
making them well-suited for tasks in constrained and unstructured environments.
However, prolonged use can induce mechanical fatigue from plastic deformation
and material degradation, compromising performance and risking structural
failure. In the state of the art, fatigue estimation of continuum robots
remains underexplored, limiting long-term operation. To address this, we
propose a fatigue-aware continuum robot with three key innovations: (1) a
Hybrid Hinge-Beam structure where TwistBeam and BendBeam decouple torsion and
bending: passive revolute joints in the BendBeam mitigate stress concentration,
while TwistBeam's limited torsional deformation reduces BendBeam stress
magnitude, enhancing durability; (2) a Passive Stopper that safely constrains
motion via mechanical constraints and employs motor torque sensing to detect
corresponding limit torque, ensuring safety and enabling data collection; and
(3) a real-time fatigue-awareness method that estimates stiffness from motor
torque at the limit pose, enabling online fatigue estimation without additional
sensors. Experiments show that the proposed design reduces fatigue accumulation
by about 49% compared with a conventional design, while passive mechanical
limiting combined with motor-side sensing allows accurate estimation of
structural fatigue and damage. These results confirm the effectiveness of the
proposed architecture for safe and reliable long-term operation.

</details>


### [13] [BagIt! An Adaptive Dual-Arm Manipulation of Fabric Bags for Object Bagging](https://arxiv.org/abs/2509.09484)
*Peng Zhou,Jiaming Qi,Hongmin Wu,Chen Wang,Yizhou Chen,Zeqing Zhang*

Main category: cs.RO

TL;DR: 一种基于双机器臂和视觉反馈的自动装袋系统，通过适应性SOI操控策略处理可变形袋子的复杂操作。


<details>
  <summary>Details</summary>
Motivation: 工业场景中的装袋任务面临可变形袋子的复杂性和不可预测性挑战，需要一种不依赖于袋子先验知识的自动化解决方案。

Method: 采用高斯混合模型(GMM)估计SOI状态，优化技术生成SOI，通过约束双向快速随机树(CBiRRT)进行运动规划，以及使用模型预测控制(MPC)实现双机器臂协同。

Result: 经过大量实验验证，系统能够在各种物体上进行精确和稳健的装袋操作，显示了其良好的适应性。

Conclusion: 该研究为机器人可变形对象操作(DOM)特别是自动装袋任务提供了一种新的解决方案，具有重要的应用价值。

Abstract: Bagging tasks, commonly found in industrial scenarios, are challenging
considering deformable bags' complicated and unpredictable nature. This paper
presents an automated bagging system from the proposed adaptive
Structure-of-Interest (SOI) manipulation strategy for dual robot arms. The
system dynamically adjusts its actions based on real-time visual feedback,
removing the need for pre-existing knowledge of bag properties. Our framework
incorporates Gaussian Mixture Models (GMM) for estimating SOI states,
optimization techniques for SOI generation, motion planning via Constrained
Bidirectional Rapidly-exploring Random Tree (CBiRRT), and dual-arm coordination
using Model Predictive Control (MPC). Extensive experiments validate the
capability of our system to perform precise and robust bagging across various
objects, showcasing its adaptability. This work offers a new solution for
robotic deformable object manipulation (DOM), particularly in automated bagging
tasks. Video of this work is available at https://youtu.be/6JWjCOeTGiQ.

</details>


### [14] [SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking](https://arxiv.org/abs/2509.09509)
*Pedro Miguel Bastos Soares,Ali Tourani,Miguel Fernandez-Cortizas,Asier Bikandi Noya,Jose Luis Sanchez-Lopez,Holger Voos*

Main category: cs.RO

TL;DR: SMapper是一个开源的硬件平台和数据集，专为SLAM研究设计，提供多传感器同步采集和精确标定，解决了现有数据集在传感器模态、环境多样性和硬件可复现性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前SLAM和自主导航研究缺乏可靠、可复现的多模态数据集，现有数据集在传感模态、环境多样性和硬件设置可复现性方面存在局限，阻碍了研究进展。

Method: 开发了SMapper开源硬件平台，集成了同步的LiDAR、多摄像头和惯性传感，配备强大的标定和同步流水线；发布了SMapper-light数据集，包含室内外场景的同步多模态数据和亚厘米精度的地面真实轨迹。

Result: 构建了完整的硬件平台和数据集，并在最先进的LiDAR和视觉SLAM框架上进行了基准测试，验证了平台和数据的实用性。

Conclusion: SMapper通过开源硬件设计、可复现数据采集和全面基准测试，为SLAM算法开发、评估和可复现性研究奠定了坚实基础。

Abstract: Advancing research in fields like Simultaneous Localization and Mapping
(SLAM) and autonomous navigation critically depends on reliable and
reproducible multimodal datasets. While several influential datasets have
driven progress in these domains, they often suffer from limitations in sensing
modalities, environmental diversity, and the reproducibility of the underlying
hardware setups. To address these challenges, this paper introduces SMapper, a
novel open-hardware, multi-sensor platform designed explicitly for, though not
limited to, SLAM research. The device integrates synchronized LiDAR,
multi-camera, and inertial sensing, supported by a robust calibration and
synchronization pipeline that ensures precise spatio-temporal alignment across
modalities. Its open and replicable design allows researchers to extend its
capabilities and reproduce experiments across both handheld and robot-mounted
scenarios. To demonstrate its practicality, we additionally release
SMapper-light, a publicly available SLAM dataset containing representative
indoor and outdoor sequences. The dataset includes tightly synchronized
multimodal data and ground-truth trajectories derived from offline LiDAR-based
SLAM with sub-centimeter accuracy, alongside dense 3D reconstructions.
Furthermore, the paper contains benchmarking results on state-of-the-art LiDAR
and visual SLAM frameworks using the SMapper-light dataset. By combining
open-hardware design, reproducible data collection, and comprehensive
benchmarking, SMapper establishes a robust foundation for advancing SLAM
algorithm development, evaluation, and reproducibility.

</details>


### [15] [A Neuromorphic Incipient Slip Detection System using Papillae Morphology](https://arxiv.org/abs/2509.09546)
*Yanhui Lu,Zeyu Deng,Stephen J. Redmond,Efi Psomopoulou,Benjamin Ward-Cherrier*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Detecting incipient slip enables early intervention to prevent object
slippage and enhance robotic manipulation safety. However, deploying such
systems on edge platforms remains challenging, particularly due to energy
constraints. This work presents a neuromorphic tactile sensing system based on
the NeuroTac sensor with an extruding papillae-based skin and a spiking
convolutional neural network (SCNN) for slip-state classification. The SCNN
model achieves 94.33% classification accuracy across three classes (no slip,
incipient slip, and gross slip) in slip conditions induced by sensor motion.
Under the dynamic gravity-induced slip validation conditions, after temporal
smoothing of the SCNN's final-layer spike counts, the system detects incipient
slip at least 360 ms prior to gross slip across all trials, consistently
identifying incipient slip before gross slip occurs. These results demonstrate
that this neuromorphic system has stable and responsive incipient slip
detection capability.

</details>


### [16] [ObjectReact: Learning Object-Relative Control for Visual Navigation](https://arxiv.org/abs/2509.09594)
*Sourav Garg,Dustin Craggs,Vineeth Bhat,Lachlan Mares,Stefan Podgorski,Madhava Krishna,Feras Dayoub,Ian Reid*

Main category: cs.RO

TL;DR: 这篇论文提出了一种基于对象的相对控制方法(Object-relative control)，通过构建相对3D场景图来实现更好的视觉导航能力，充分利用对象表征的体验和路径不变性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于图像的相对控制方法存在限制，图像与代理的姿态和体验绑定很紧，而对象作为地图属性能够提供更好的体验和路径不变性表征。

Method: 提出了一种顶度地形地图表征形式：相对3D场景图，用于获取更信息丰富的对象级全局路径规划成本。训练了一个本地控制器ObjectReact，直接条件化在高层次的"WayObject Costmap"表征上，消除了显式RGB输入的需求。

Result: 在传感器高度变化和多种导航任务中，对象相对控制方法在空间理解能力方面显示出明显优势，包括反向导航地图轨迹。仅使用模拟器训练的策略能够较好地沿用到实际室内环境。

Conclusion: 对象相对控制方法为视觉导航提供了一种更加健壮和可沿用的方案，能够在不同体验和设备间实现高度不变性，为单相机导航系统开启了新的可能性。

Abstract: Visual navigation using only a single camera and a topological map has
recently become an appealing alternative to methods that require additional
sensors and 3D maps. This is typically achieved through an "image-relative"
approach to estimating control from a given pair of current observation and
subgoal image. However, image-level representations of the world have
limitations because images are strictly tied to the agent's pose and
embodiment. In contrast, objects, being a property of the map, offer an
embodiment- and trajectory-invariant world representation. In this work, we
present a new paradigm of learning "object-relative" control that exhibits
several desirable characteristics: a) new routes can be traversed without
strictly requiring to imitate prior experience, b) the control prediction
problem can be decoupled from solving the image matching problem, and c) high
invariance can be achieved in cross-embodiment deployment for variations across
both training-testing and mapping-execution settings. We propose a topometric
map representation in the form of a "relative" 3D scene graph, which is used to
obtain more informative object-level global path planning costs. We train a
local controller, dubbed "ObjectReact", conditioned directly on a high-level
"WayObject Costmap" representation that eliminates the need for an explicit RGB
input. We demonstrate the advantages of learning object-relative control over
its image-relative counterpart across sensor height variations and multiple
navigation tasks that challenge the underlying spatial understanding
capability, e.g., navigating a map trajectory in the reverse direction. We
further show that our sim-only policy is able to generalize well to real-world
indoor environments. Code and supplementary material are accessible via project
page: https://object-react.github.io/

</details>


### [17] [MOFU: Development of a MOrphing Fluffy Unit with Expansion and Contraction Capabilities and Evaluation of the Animacy of Its Movements](https://arxiv.org/abs/2509.09613)
*Taisei Mogi,Mari Saito,Yoshihiro Nakata*

Main category: cs.RO

TL;DR: 开发了能够全身膨胀收缩的毛绒机器人MOFU，研究发现膨胀收缩运动能显著增强机器人被感知的生命感，是机器人设计中的重要元素


<details>
  <summary>Details</summary>
Motivation: 现有机器人主要模仿外观和关节运动，但忽视了生物体中观察到的全身膨胀收缩这种体积变化运动对生命感感知的影响

Method: 开发了采用Jitterbug结构的MOFU机器人，使用单个电机实现全身膨胀收缩，并通过在线调查视频评估膨胀收缩运动的效果，使用Godspeed问卷评估感知印象

Result: 膨胀收缩运动显著提高了感知的生命感；两个机器人相比单个机器人没有显著差异；膨胀收缩与运动结合时生命感评分高于单独运动

Conclusion: 体积变化运动如膨胀收缩能增强机器人被感知的生命感，应在未来旨在塑造人类印象的机器人开发中作为重要设计元素考虑

Abstract: Robots for therapy and social interaction are often intended to evoke
"animacy" in humans. While many robots imitate appearance and joint movements,
little attention has been given to whole-body expansion-contraction,
volume-changing movements observed in living organisms, and their effect on
animacy perception. We developed a mobile robot called "MOFU (Morphing Fluffy
Unit)," capable of whole-body expansion-contraction with a single motor and
covered with a fluffy exterior. MOFU employs a "Jitterbug" structure, a
geometric transformation mechanism that enables smooth volume change in
diameter from 210 to 280 mm using one actuator. It is also equipped with a
differential two-wheel drive mechanism for locomotion. To evaluate the effect
of expansion-contraction movements, we conducted an online survey using videos
of MOFU's behavior. Participants rated impressions with the Godspeed
Questionnaire Series. First, we compared videos of MOFU in a stationary state
with and without expansion-contraction and turning, finding that
expansion-contraction significantly increased perceived animacy. Second, we
hypothesized that presenting two MOFUs would increase animacy compared with a
single robot; however, this was not supported, as no significant difference
emerged. Exploratory analyses further compared four dual-robot motion
conditions. Third, when expansion-contraction was combined with locomotion,
animacy ratings were higher than locomotion alone. These results suggest that
volume-changing movements such as expansion and contraction enhance perceived
animacy in robots and should be considered an important design element in
future robot development aimed at shaping human impressions.

</details>


### [18] [Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference-Scoped Exploration](https://arxiv.org/abs/2509.09671)
*Sirui Xu,Yu-Wei Chao,Liuyu Bian,Arsalan Mousavian,Yu-Xiong Wang,Liang-Yan Gui,Wei Yang*

Main category: cs.RO

TL;DR: Dexplore是一个统一单循环优化框架，直接从大规模手-物体运动捕捉数据学习机器人控制策略，避免了传统三阶段方法的问题，并通过强化学习保持策略在自适应空间范围内完成任务。


<details>
  <summary>Details</summary>
Motivation: 现有手-物体运动捕捉数据存在演示不准确和人体-机器人手部形态差异问题，传统三阶段方法（重定向、跟踪、残差校正）导致演示数据利用不足和误差累积。

Method: 提出统一单循环优化，联合执行重定向和跟踪，将演示作为软指导而非绝对真值，从原始轨迹推导自适应空间范围，使用强化学习训练策略在范围内完成任务并最小化控制努力。

Result: 统一框架保留了演示意图，使机器人特定策略涌现，提高对噪声的鲁棒性，并能扩展到大规模演示语料库。最终将跟踪策略蒸馏为基于视觉的技能条件生成控制器。

Conclusion: Dexplore提供了一个原则性桥梁，将不完美的演示转化为灵巧操作的有效训练信号，支持跨物体泛化和真实世界部署。

Abstract: Hand-object motion-capture (MoCap) repositories offer large-scale,
contact-rich demonstrations and hold promise for scaling dexterous robotic
manipulation. Yet demonstration inaccuracies and embodiment gaps between human
and robot hands limit the straightforward use of these data. Existing methods
adopt a three-stage workflow, including retargeting, tracking, and residual
correction, which often leaves demonstrations underused and compound errors
across stages. We introduce Dexplore, a unified single-loop optimization that
jointly performs retargeting and tracking to learn robot control policies
directly from MoCap at scale. Rather than treating demonstrations as ground
truth, we use them as soft guidance. From raw trajectories, we derive adaptive
spatial scopes, and train with reinforcement learning to keep the policy
in-scope while minimizing control effort and accomplishing the task. This
unified formulation preserves demonstration intent, enables robot-specific
strategies to emerge, improves robustness to noise, and scales to large
demonstration corpora. We distill the scaled tracking policy into a
vision-based, skill-conditioned generative controller that encodes diverse
manipulation skills in a rich latent representation, supporting generalization
across objects and real-world deployment. Taken together, these contributions
position Dexplore as a principled bridge that transforms imperfect
demonstrations into effective training signals for dexterous manipulation.

</details>


### [19] [SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning](https://arxiv.org/abs/2509.09674)
*Haozhan Li,Yuxin Zuo,Jiale Yu,Yuhao Zhang,Zhaohui Yang,Kaiyan Zhang,Xuekai Zhu,Yuchen Zhang,Tianxing Chen,Ganqu Cui,Dehui Wang,Dingxiang Luo,Yuchen Fan,Youbang Sun,Jia Zeng,Jiangmiao Pang,Shanghang Zhang,Yu Wang,Yao Mu,Bowen Zhou,Ning Ding*

Main category: cs.RO

TL;DR: SimpleVLA-RL是一个针对视觉-语言-动作模型的强化学习框架，通过改进采样、并行化和损失计算，减少了对大规模人类操作数据的需求，在多个基准测试中实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型面临的两个核心挑战：大规模人类操作轨迹数据稀缺且成本高昂，以及在分布偏移任务上的泛化能力有限。受大型推理模型中RL成功提升推理能力的启发，探索RL是否能类似地提升VLA模型的长期动作规划能力。

Method: 基于veRL框架，引入了VLA特定的轨迹采样、可扩展并行化、多环境渲染和优化的损失计算。应用于OpenVLA-OFT模型，并引入了探索增强策略。

Result: 在LIBERO基准测试中达到最先进性能，在RoboTwin 1.0和2.0上甚至超越了π_0。不仅减少了对大规模数据的依赖，实现了鲁棒泛化，而且在真实世界任务中显著超越了监督微调方法。

Conclusion: SimpleVLA-RL框架有效解决了VLA模型的数据依赖和泛化问题，通过RL训练发现了新的策略模式（"pushcut"现象），为VLA模型的进一步发展提供了有力工具。

Abstract: Vision-Language-Action (VLA) models have recently emerged as a powerful
paradigm for robotic manipulation. Despite substantial progress enabled by
large-scale pretraining and supervised fine-tuning (SFT), these models face two
fundamental challenges: (i) the scarcity and high cost of large-scale
human-operated robotic trajectories required for SFT scaling, and (ii) limited
generalization to tasks involving distribution shift. Recent breakthroughs in
Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can
dramatically enhance step-by-step reasoning capabilities, raising a natural
question: Can RL similarly improve the long-horizon step-by-step action
planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL
framework tailored for VLA models. Building upon veRL, we introduce
VLA-specific trajectory sampling, scalable parallelization, multi-environment
rendering, and optimized loss computation. When applied to OpenVLA-OFT,
SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\pi_0$
on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce.
SimpleVLA-RL not only reduces dependence on large-scale data and enables robust
generalization, but also remarkably surpasses SFT in real-world tasks.
Moreover, we identify a novel phenomenon ``pushcut'' during RL training,
wherein the policy discovers previously unseen patterns beyond those seen in
the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL

</details>
