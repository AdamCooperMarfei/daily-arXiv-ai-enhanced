{"id": "2507.11621", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11621", "abs": "https://arxiv.org/abs/2507.11621", "authors": ["Tianyi Wang", "Yangyang Wang", "Jie Pan", "Junfeng Jiao", "Christian Claudel"], "title": "HCOMC: A Hierarchical Cooperative On-Ramp Merging Control Framework in Mixed Traffic Environment on Two-Lane Highways", "comment": "7 pages, 2 figures, 3 tables, accepted for IEEE International\n  Conference on Intelligent Transportation Systems (ITSC) 2025", "summary": "Highway on-ramp merging areas are common bottlenecks to traffic congestion\nand accidents. Currently, a cooperative control strategy based on connected and\nautomated vehicles (CAVs) is a fundamental solution to this problem. While CAVs\nare not fully widespread, it is necessary to propose a hierarchical cooperative\non-ramp merging control (HCOMC) framework for heterogeneous traffic flow on\ntwo-lane highways to address this gap. This paper extends longitudinal\ncar-following models based on the intelligent driver model and lateral\nlane-changing models using the quintic polynomial curve to account for\nhuman-driven vehicles (HDVs) and CAVs, comprehensively considering human\nfactors and cooperative adaptive cruise control. Besides, this paper proposes a\nHCOMC framework, consisting of a hierarchical cooperative planning model based\non the modified virtual vehicle model, a discretionary lane-changing model\nbased on game theory, and a multi-objective optimization model using the\nelitist non-dominated sorting genetic algorithm to ensure the safe, smooth, and\nefficient merging process. Then, the performance of our HCOMC is analyzed under\ndifferent traffic densities and CAV penetration rates through simulation. The\nfindings underscore our HCOMC's pronounced comprehensive advantages in\nenhancing the safety of group vehicles, stabilizing and expediting merging\nprocess, optimizing traffic efficiency, and economizing fuel consumption\ncompared with benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u534f\u4f5c\u7684\u531d\u9053\u5408\u6d41\u63a7\u5236\u6846\u67b6\uff08HCOMC\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u6df7\u5408\u4ea4\u901a\u6d41\u4e2d\u7684\u5408\u6d41\u95ee\u9898\uff0c\u7ed3\u5408\u7eb5\u5411\u8ddf\u8f66\u548c\u6a2a\u5411\u6362\u9053\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u9ad8\u901f\u516c\u8def\u531d\u9053\u5408\u6d41\u533a\u662f\u4ea4\u901a\u62e5\u5835\u548c\u4e8b\u6545\u7684\u9ad8\u53d1\u533a\uff0c\u5f53\u524d\u57fa\u4e8eCAV\u7684\u534f\u4f5c\u63a7\u5236\u7b56\u7565\u5c1a\u672a\u666e\u53ca\uff0c\u56e0\u6b64\u9700\u8981\u9488\u5bf9\u6df7\u5408\u4ea4\u901a\u6d41\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6269\u5c55\u4e86\u57fa\u4e8e\u667a\u80fd\u9a7e\u9a76\u5458\u6a21\u578b\u7684\u7eb5\u5411\u8ddf\u8f66\u6a21\u578b\u548c\u57fa\u4e8e\u4e94\u6b21\u591a\u9879\u5f0f\u66f2\u7ebf\u7684\u6a2a\u5411\u6362\u9053\u6a21\u578b\uff0c\u63d0\u51faHCOMC\u6846\u67b6\uff0c\u5305\u62ec\u5206\u5c42\u534f\u4f5c\u89c4\u5212\u3001\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u6362\u9053\u6a21\u578b\u548c\u591a\u76ee\u6807\u4f18\u5316\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cHCOMC\u5728\u63d0\u5347\u8f66\u8f86\u7fa4\u5b89\u5168\u6027\u3001\u7a33\u5b9a\u5408\u6d41\u8fc7\u7a0b\u3001\u4f18\u5316\u4ea4\u901a\u6548\u7387\u548c\u8282\u7701\u71c3\u6cb9\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "HCOMC\u4e3a\u6df7\u5408\u4ea4\u901a\u6d41\u4e2d\u7684\u531d\u9053\u5408\u6d41\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728CAV\u666e\u53ca\u7387\u8f83\u4f4e\u65f6\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.11623", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11623", "abs": "https://arxiv.org/abs/2507.11623", "authors": ["Alan Papalia", "Charles Dawson", "Laurentiu L. Anton", "Norhan Magdy Bayomi", "Bianca Champenois", "Jung-Hoon Cho", "Levi Cai", "Joseph DelPreto", "Kristen Edwards", "Bilha-Catherine Githinji", "Cameron Hickert", "Vindula Jayawardana", "Matthew Kramer", "Shreyaa Raghavan", "David Russell", "Shide Salimi", "Jingnan Shi", "Soumya Sudhakar", "Yanwei Wang", "Shouyi Wang", "Luca Carlone", "Vijay Kumar", "Daniela Rus", "John E. Fernandez", "Cathy Wu", "George Kantor", "Derek Young", "Hanumant Singh"], "title": "A Roadmap for Climate-Relevant Robotics Research", "comment": null, "summary": "Climate change is one of the defining challenges of the 21st century, and\nmany in the robotics community are looking for ways to contribute. This paper\npresents a roadmap for climate-relevant robotics research, identifying\nhigh-impact opportunities for collaboration between roboticists and experts\nacross climate domains such as energy, the built environment, transportation,\nindustry, land use, and Earth sciences. These applications include problems\nsuch as energy systems optimization, construction, precision agriculture,\nbuilding envelope retrofits, autonomous trucking, and large-scale environmental\nmonitoring. Critically, we include opportunities to apply not only physical\nrobots but also the broader robotics toolkit - including planning, perception,\ncontrol, and estimation algorithms - to climate-relevant problems. A central\ngoal of this roadmap is to inspire new research directions and collaboration by\nhighlighting specific, actionable problems at the intersection of robotics and\nclimate. This work represents a collaboration between robotics researchers and\ndomain experts in various climate disciplines, and it serves as an invitation\nto the robotics community to bring their expertise to bear on urgent climate\npriorities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6c14\u5019\u76f8\u5173\u673a\u5668\u4eba\u7814\u7a76\u7684\u8def\u7ebf\u56fe\uff0c\u65e8\u5728\u901a\u8fc7\u673a\u5668\u4eba\u6280\u672f\u4e0e\u6c14\u5019\u9886\u57df\u7684\u5408\u4f5c\u89e3\u51b3\u9ad8\u5f71\u54cd\u529b\u95ee\u9898\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u662f21\u4e16\u7eaa\u7684\u91cd\u8981\u6311\u6218\uff0c\u673a\u5668\u4eba\u793e\u533a\u5e0c\u671b\u901a\u8fc7\u6280\u672f\u8d21\u732e\u5e94\u5bf9\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u673a\u5668\u4eba\u6280\u672f\u4e0e\u6c14\u5019\u9886\u57df\uff08\u5982\u80fd\u6e90\u3001\u5efa\u7b51\u73af\u5883\u3001\u4ea4\u901a\u7b49\uff09\u7684\u5408\u4f5c\u673a\u4f1a\uff0c\u63d0\u51fa\u5177\u4f53\u5e94\u7528\u65b9\u5411\u3002", "result": "\u660e\u786e\u4e86\u673a\u5668\u4eba\u6280\u672f\u5728\u80fd\u6e90\u4f18\u5316\u3001\u7cbe\u51c6\u519c\u4e1a\u3001\u73af\u5883\u76d1\u6d4b\u7b49\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u6fc0\u53d1\u673a\u5668\u4eba\u793e\u533a\u7684\u65b0\u7814\u7a76\u65b9\u5411\uff0c\u63a8\u52a8\u8de8\u5b66\u79d1\u5408\u4f5c\u4ee5\u5e94\u5bf9\u6c14\u5019\u6311\u6218\u3002"}}
{"id": "2507.11716", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11716", "abs": "https://arxiv.org/abs/2507.11716", "authors": ["Yifan Xu", "Qianwei Wang", "Jordan Lillie", "Vineet Kamat", "Carol Menassa", "Clive D'Souza"], "title": "CoNav Chair: Development and Evaluation of a Shared Control based Wheelchair for the Built Environment", "comment": "13 pages, 10 figures", "summary": "As the global population of people with disabilities (PWD) continues to grow,\nso will the need for mobility solutions that promote independent living and\nsocial integration. Wheelchairs are vital for the mobility of PWD in both\nindoor and outdoor environments. The current SOTA in powered wheelchairs is\nbased on either manually controlled or fully autonomous modes of operation,\noffering limited flexibility and often proving difficult to navigate in\nspatially constrained environments. Moreover, research on robotic wheelchairs\nhas focused predominantly on complete autonomy or improved manual control;\napproaches that can compromise efficiency and user trust. To overcome these\nchallenges, this paper introduces the CoNav Chair, a smart wheelchair based on\nthe Robot Operating System (ROS) and featuring shared control navigation and\nobstacle avoidance capabilities that are intended to enhance navigational\nefficiency, safety, and ease of use for the user. The paper outlines the CoNav\nChair's design and presents a preliminary usability evaluation comparing three\ndistinct navigation modes, namely, manual, shared, and fully autonomous,\nconducted with 21 healthy, unimpaired participants traversing an indoor\nbuilding environment. Study findings indicated that the shared control\nnavigation framework had significantly fewer collisions and performed\ncomparably, if not superior to the autonomous and manual modes, on task\ncompletion time, trajectory length, and smoothness; and was perceived as being\nsafer and more efficient based on user reported subjective assessments of\nusability. Overall, the CoNav system demonstrated acceptable safety and\nperformance, laying the foundation for subsequent usability testing with end\nusers, namely, PWDs who rely on a powered wheelchair for mobility.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CoNav Chair\uff0c\u4e00\u79cd\u57fa\u4e8eROS\u7684\u667a\u80fd\u8f6e\u6905\uff0c\u7ed3\u5408\u5171\u4eab\u63a7\u5236\u548c\u969c\u788d\u7269\u907f\u969c\u529f\u80fd\uff0c\u65e8\u5728\u63d0\u9ad8\u5bfc\u822a\u6548\u7387\u3001\u5b89\u5168\u6027\u548c\u6613\u7528\u6027\u3002\u521d\u6b65\u8bc4\u4f30\u663e\u793a\u5171\u4eab\u63a7\u5236\u6a21\u5f0f\u5728\u78b0\u649e\u6b21\u6570\u3001\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u548c\u7528\u6237\u4e3b\u89c2\u8bc4\u4ef7\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u6b8b\u75be\u4eba\u53e3\u589e\u957f\uff0c\u73b0\u6709\u7535\u52a8\u8f6e\u6905\u7684\u5b8c\u5168\u81ea\u4e3b\u6216\u624b\u52a8\u63a7\u5236\u6a21\u5f0f\u7075\u6d3b\u6027\u4e0d\u8db3\uff0c\u96be\u4ee5\u5728\u7a7a\u95f4\u53d7\u9650\u73af\u5883\u4e2d\u5bfc\u822a\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5171\u4eab\u63a7\u5236\u6a21\u5f0f\u63d0\u5347\u8f6e\u6905\u7684\u5bfc\u822a\u6548\u7387\u548c\u7528\u6237\u4fe1\u4efb\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eROS\u7684CoNav Chair\uff0c\u7ed3\u5408\u5171\u4eab\u63a7\u5236\u548c\u969c\u788d\u7269\u907f\u969c\u529f\u80fd\u3002\u901a\u8fc721\u540d\u5065\u5eb7\u53c2\u4e0e\u8005\u5bf9\u6bd4\u624b\u52a8\u3001\u5171\u4eab\u548c\u5b8c\u5168\u81ea\u4e3b\u4e09\u79cd\u5bfc\u822a\u6a21\u5f0f\u5728\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5171\u4eab\u63a7\u5236\u6a21\u5f0f\u5728\u78b0\u649e\u6b21\u6570\u3001\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3001\u8f68\u8ff9\u957f\u5ea6\u548c\u5e73\u6ed1\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u7528\u6237\u4e3b\u89c2\u8bc4\u4ef7\u8ba4\u4e3a\u5176\u66f4\u5b89\u5168\u548c\u9ad8\u6548\u3002", "conclusion": "CoNav\u7cfb\u7edf\u5c55\u793a\u4e86\u826f\u597d\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd\uff0c\u4e3a\u540e\u7eed\u9488\u5bf9\u6b8b\u75be\u7528\u6237\u7684\u6d4b\u8bd5\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.11770", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11770", "abs": "https://arxiv.org/abs/2507.11770", "authors": ["Giang Nguyen", "Mihai Pomarlan", "Sascha Jongebloed", "Nils Leusmann", "Minh Nhat Vu", "Michael Beetz"], "title": "Generating Actionable Robot Knowledge Bases by Combining 3D Scene Graphs with Robot Ontologies", "comment": "8 pages, 7 figures, IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS2025)", "summary": "In robotics, the effective integration of environmental data into actionable\nknowledge remains a significant challenge due to the variety and\nincompatibility of data formats commonly used in scene descriptions, such as\nMJCF, URDF, and SDF. This paper presents a novel approach that addresses these\nchallenges by developing a unified scene graph model that standardizes these\nvaried formats into the Universal Scene Description (USD) format. This\nstandardization facilitates the integration of these scene graphs with robot\nontologies through semantic reporting, enabling the translation of complex\nenvironmental data into actionable knowledge essential for cognitive robotic\ncontrol. We evaluated our approach by converting procedural 3D environments\ninto USD format, which is then annotated semantically and translated into a\nknowledge graph to effectively answer competency questions, demonstrating its\nutility for real-time robotic decision-making. Additionally, we developed a\nweb-based visualization tool to support the semantic mapping process, providing\nusers with an intuitive interface to manage the 3D environment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u573a\u666f\u56fe\u6a21\u578b\uff0c\u5c06\u591a\u79cd\u673a\u5668\u4eba\u573a\u666f\u63cf\u8ff0\u683c\u5f0f\u6807\u51c6\u5316\u4e3aUSD\u683c\u5f0f\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u6807\u6ce8\u548c\u77e5\u8bc6\u56fe\u8c31\u8f6c\u6362\uff0c\u652f\u6301\u673a\u5668\u4eba\u5b9e\u65f6\u51b3\u7b56\u3002", "motivation": "\u673a\u5668\u4eba\u9886\u57df\u4e2d\uff0c\u73af\u5883\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u683c\u5f0f\u4e0d\u517c\u5bb9\u6027\u5bfc\u81f4\u6570\u636e\u6574\u5408\u56f0\u96be\uff0c\u5f71\u54cd\u673a\u5668\u4eba\u8ba4\u77e5\u63a7\u5236\u3002", "method": "\u5f00\u53d1\u7edf\u4e00\u573a\u666f\u56fe\u6a21\u578b\uff0c\u5c06MJCF\u3001URDF\u548cSDF\u7b49\u683c\u5f0f\u6807\u51c6\u5316\u4e3aUSD\u683c\u5f0f\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u6807\u6ce8\u548c\u77e5\u8bc6\u56fe\u8c31\u8f6c\u6362\u5b9e\u73b0\u6570\u636e\u6574\u5408\u3002", "result": "\u6210\u529f\u5c063D\u73af\u5883\u6570\u636e\u8f6c\u6362\u4e3aUSD\u683c\u5f0f\u5e76\u751f\u6210\u77e5\u8bc6\u56fe\u8c31\uff0c\u652f\u6301\u5b9e\u65f6\u673a\u5668\u4eba\u51b3\u7b56\uff0c\u540c\u65f6\u5f00\u53d1\u4e86\u53ef\u89c6\u5316\u5de5\u5177\u8f85\u52a9\u8bed\u4e49\u6620\u5c04\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u73af\u5883\u6570\u636e\u6574\u5408\u95ee\u9898\uff0c\u4e3a\u8ba4\u77e5\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.11840", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11840", "abs": "https://arxiv.org/abs/2507.11840", "authors": ["Gaofeng Li", "Ruize Wang", "Peisen Xu", "Qi Ye", "Jiming Chen"], "title": "The Developments and Challenges towards Dexterous and Embodied Robotic Manipulation: A Survey", "comment": null, "summary": "Achieving human-like dexterous robotic manipulation remains a central goal\nand a pivotal challenge in robotics. The development of Artificial Intelligence\n(AI) has allowed rapid progress in robotic manipulation. This survey summarizes\nthe evolution of robotic manipulation from mechanical programming to embodied\nintelligence, alongside the transition from simple grippers to multi-fingered\ndexterous hands, outlining key characteristics and main challenges. Focusing on\nthe current stage of embodied dexterous manipulation, we highlight recent\nadvances in two critical areas: dexterous manipulation data collection (via\nsimulation, human demonstrations, and teleoperation) and skill-learning\nframeworks (imitation and reinforcement learning). Then, based on the overview\nof the existing data collection paradigm and learning framework, three key\nchallenges restricting the development of dexterous robotic manipulation are\nsummarized and discussed.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4ece\u673a\u68b0\u7f16\u7a0b\u5230\u5177\u8eab\u667a\u80fd\u7684\u6f14\u53d8\uff0c\u91cd\u70b9\u63a2\u8ba8\u4e86\u5f53\u524d\u5177\u8eab\u7075\u5de7\u64cd\u4f5c\u7684\u5173\u952e\u6311\u6218\u548c\u6700\u65b0\u8fdb\u5c55\u3002", "motivation": "\u5b9e\u73b0\u7c7b\u4eba\u7684\u7075\u5de7\u673a\u5668\u4eba\u64cd\u4f5c\u662f\u673a\u5668\u4eba\u5b66\u7684\u6838\u5fc3\u76ee\u6807\u548c\u5173\u952e\u6311\u6218\uff0cAI\u7684\u53d1\u5c55\u63a8\u52a8\u4e86\u8fd9\u4e00\u9886\u57df\u7684\u5feb\u901f\u8fdb\u6b65\u3002", "method": "\u901a\u8fc7\u4eff\u771f\u3001\u4eba\u7c7b\u6f14\u793a\u548c\u8fdc\u7a0b\u64cd\u4f5c\u6536\u96c6\u7075\u5de7\u64cd\u4f5c\u6570\u636e\uff0c\u5e76\u91c7\u7528\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u6280\u80fd\u5b66\u4e60\u3002", "result": "\u603b\u7ed3\u4e86\u73b0\u6709\u6570\u636e\u6536\u96c6\u8303\u5f0f\u548c\u6280\u80fd\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u4e2a\u9650\u5236\u7075\u5de7\u673a\u5668\u4eba\u64cd\u4f5c\u53d1\u5c55\u7684\u5173\u952e\u6311\u6218\u3002", "conclusion": "\u7075\u5de7\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53d1\u5c55\u4ecd\u9762\u4e34\u6570\u636e\u6536\u96c6\u548c\u5b66\u4e60\u6846\u67b6\u7684\u6311\u6218\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u89e3\u51b3\u3002"}}
{"id": "2507.11852", "categories": ["cs.RO", "cs.CV", "93C85", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.11852", "abs": "https://arxiv.org/abs/2507.11852", "authors": ["Mohammed Hassanin", "Mohammad Abu Alsheikh", "Carlos C. N. Kuhn", "Damith Herath", "Dinh Thai Hoang", "Ibrahim Radwan"], "title": "Towards Autonomous Riding: A Review of Perception, Planning, and Control in Intelligent Two-Wheelers", "comment": "17 pages", "summary": "The rapid adoption of micromobility solutions, particularly two-wheeled\nvehicles like e-scooters and e-bikes, has created an urgent need for reliable\nautonomous riding (AR) technologies. While autonomous driving (AD) systems have\nmatured significantly, AR presents unique challenges due to the inherent\ninstability of two-wheeled platforms, limited size, limited power, and\nunpredictable environments, which pose very serious concerns about road users'\nsafety. This review provides a comprehensive analysis of AR systems by\nsystematically examining their core components, perception, planning, and\ncontrol, through the lens of AD technologies. We identify critical gaps in\ncurrent AR research, including a lack of comprehensive perception systems for\nvarious AR tasks, limited industry and government support for such\ndevelopments, and insufficient attention from the research community. The\nreview analyses the gaps of AR from the perspective of AD to highlight\npromising research directions, such as multimodal sensor techniques for\nlightweight platforms and edge deep learning architectures. By synthesising\ninsights from AD research with the specific requirements of AR, this review\naims to accelerate the development of safe, efficient, and scalable autonomous\nriding systems for future urban mobility.", "AI": {"tldr": "\u7efc\u8ff0\u5206\u6790\u4e86\u81ea\u4e3b\u9a91\u884c\uff08AR\uff09\u6280\u672f\u7684\u73b0\u72b6\u4e0e\u6311\u6218\uff0c\u901a\u8fc7\u4e0e\u81ea\u52a8\u9a7e\u9a76\uff08AD\uff09\u6280\u672f\u7684\u5bf9\u6bd4\uff0c\u6307\u51fa\u4e86AR\u5728\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\u7cfb\u7edf\u4e2d\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5fae\u578b\u4ea4\u901a\u5de5\u5177\uff08\u5982\u7535\u52a8\u6ed1\u677f\u8f66\u548c\u7535\u52a8\u81ea\u884c\u8f66\uff09\u7684\u5feb\u901f\u666e\u53ca\uff0c\u5f00\u53d1\u53ef\u9760\u7684AR\u6280\u672f\u6210\u4e3a\u8feb\u5207\u9700\u6c42\uff0c\u4f46AR\u9762\u4e34\u72ec\u7279\u7684\u6311\u6218\uff0c\u5982\u4e24\u8f6e\u5e73\u53f0\u7684\u4e0d\u7a33\u5b9a\u6027\u3001\u5c3a\u5bf8\u548c\u529f\u7387\u9650\u5236\u4ee5\u53ca\u73af\u5883\u4e0d\u53ef\u9884\u6d4b\u6027\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790AR\u7684\u6838\u5fc3\u7ec4\u4ef6\uff08\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\uff09\uff0c\u5e76\u4eceAD\u6280\u672f\u7684\u89d2\u5ea6\u5ba1\u89c6AR\u7684\u73b0\u72b6\uff0c\u8bc6\u522b\u5f53\u524d\u7814\u7a76\u7684\u4e0d\u8db3\u3002", "result": "\u53d1\u73b0\u5f53\u524dAR\u7814\u7a76\u5b58\u5728\u611f\u77e5\u7cfb\u7edf\u4e0d\u5b8c\u5584\u3001\u884c\u4e1a\u548c\u653f\u5e9c\u652f\u6301\u4e0d\u8db3\u4ee5\u53ca\u7814\u7a76\u5173\u6ce8\u5ea6\u4f4e\u7b49\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u4f20\u611f\u5668\u6280\u672f\u548c\u8fb9\u7f18\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7b49\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408AD\u7814\u7a76\u7684\u7ecf\u9a8c\u4e0eAR\u7684\u7279\u6b8a\u9700\u6c42\uff0c\u65e8\u5728\u63a8\u52a8\u5b89\u5168\u3001\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684AR\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u4ee5\u652f\u6301\u672a\u6765\u57ce\u5e02\u51fa\u884c\u3002"}}
{"id": "2507.11880", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11880", "abs": "https://arxiv.org/abs/2507.11880", "authors": ["Jinyuan Liu", "Minglei Fu", "Ling Shi", "Chenguang Yang", "Wenan Zhang"], "title": "A Fast Method for Planning All Optimal Homotopic Configurations for Tethered Robots and Its Extended Applications", "comment": "37 pages, 33 figures", "summary": "Tethered robots play a pivotal role in specialized environments such as\ndisaster response and underground exploration, where their stable power supply\nand reliable communication offer unparalleled advantages. However, their motion\nplanning is severely constrained by tether length limitations and entanglement\nrisks, posing significant challenges to achieving optimal path planning. To\naddress these challenges, this study introduces CDT-TCS (Convex Dissection\nTopology-based Tethered Configuration Search), a novel algorithm that leverages\nCDT Encoding as a homotopy invariant to represent topological states of paths.\nBy integrating algebraic topology with geometric optimization, CDT-TCS\nefficiently computes the complete set of optimal feasible configurations for\ntethered robots at all positions in 2D environments through a single\ncomputation. Building on this foundation, we further propose three\napplication-specific algorithms: i) CDT-TPP for optimal tethered path planning,\nii) CDT-TMV for multi-goal visiting with tether constraints, iii) CDT-UTPP for\ndistance-optimal path planning of untethered robots. All theoretical results\nand propositions underlying these algorithms are rigorously proven and\nthoroughly discussed in this paper. Extensive simulations demonstrate that the\nproposed algorithms significantly outperform state-of-the-art methods in their\nrespective problem domains. Furthermore, real-world experiments on robotic\nplatforms validate the practicality and engineering value of the proposed\nframework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCDT-TCS\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u7cfb\u7559\u673a\u5668\u4eba\u5728\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u62d3\u6251\u7ea6\u675f\u95ee\u9898\uff0c\u5e76\u884d\u751f\u51fa\u4e09\u79cd\u5e94\u7528\u7b97\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7cfb\u7559\u673a\u5668\u4eba\u5728\u7279\u6b8a\u73af\u5883\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5176\u8fd0\u52a8\u89c4\u5212\u53d7\u9650\u4e8e\u7cfb\u7ef3\u957f\u5ea6\u548c\u7f20\u7ed5\u98ce\u9669\uff0c\u9700\u8981\u9ad8\u6548\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u5229\u7528CDT\u7f16\u7801\u4f5c\u4e3a\u540c\u4f26\u4e0d\u53d8\u91cf\u8868\u793a\u8def\u5f84\u62d3\u6251\u72b6\u6001\uff0c\u7ed3\u5408\u4ee3\u6570\u62d3\u6251\u548c\u51e0\u4f55\u4f18\u5316\uff0c\u63d0\u51faCDT-TCS\u7b97\u6cd5\u53ca\u5176\u4e09\u79cd\u5e94\u7528\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u5728\u5404\u81ea\u95ee\u9898\u9886\u57df\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5de5\u7a0b\u5b9e\u7528\u6027\u3002", "conclusion": "CDT-TCS\u53ca\u5176\u884d\u751f\u7b97\u6cd5\u4e3a\u7cfb\u7559\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u548c\u5de5\u7a0b\u4ef7\u503c\u3002"}}
{"id": "2507.11889", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11889", "abs": "https://arxiv.org/abs/2507.11889", "authors": ["Adnan Abdullah", "Alankrit Gupta", "Vaishnav Ramesh", "Shivali Patel", "Md Jahidul Islam"], "title": "NemeSys: An Online Underwater Explorer with Goal-Driven Adaptive Autonomy", "comment": "10 pages, V1", "summary": "Adaptive mission control and dynamic parameter reconfiguration are essential\nfor autonomous underwater vehicles (AUVs) operating in GPS-denied,\ncommunication-limited marine environments. However, most current AUV platforms\nexecute static, pre-programmed missions or rely on tethered connections and\nhigh-latency acoustic channels for mid-mission updates, significantly limiting\ntheir adaptability and responsiveness. In this paper, we introduce NemeSys, a\nnovel AUV system designed to support real-time mission reconfiguration through\ncompact optical and magnetoelectric (OME) signaling facilitated by floating\nbuoys. We present the full system design, control architecture, and a semantic\nmission encoding framework that enables interactive exploration and task\nadaptation via low-bandwidth communication. The proposed system is validated\nthrough analytical modeling, controlled experimental evaluations, and\nopen-water trials. Results confirm the feasibility of online mission adaptation\nand semantic task updates, highlighting NemeSys as an online AUV platform for\ngoal-driven adaptive autonomy in dynamic and uncertain underwater environments.", "AI": {"tldr": "NemeSys\u662f\u4e00\u79cd\u65b0\u578bAUV\u7cfb\u7edf\uff0c\u652f\u6301\u901a\u8fc7\u5149\u5b66\u548c\u78c1\u7535\u4fe1\u53f7\u5b9e\u65f6\u91cd\u65b0\u914d\u7f6e\u4efb\u52a1\uff0c\u9002\u7528\u4e8eGPS\u7f3a\u5931\u548c\u901a\u4fe1\u53d7\u9650\u7684\u6c34\u4e0b\u73af\u5883\u3002", "motivation": "\u5f53\u524dAUV\u5e73\u53f0\u901a\u5e38\u6267\u884c\u9759\u6001\u4efb\u52a1\u6216\u4f9d\u8d56\u9ad8\u5ef6\u8fdf\u901a\u4fe1\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6027\u548c\u54cd\u5e94\u80fd\u529b\u3002", "method": "\u63d0\u51faNemeSys\u7cfb\u7edf\uff0c\u5305\u62ec\u8bbe\u8ba1\u3001\u63a7\u5236\u67b6\u6784\u548c\u8bed\u4e49\u4efb\u52a1\u7f16\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u5e26\u5bbd\u901a\u4fe1\u5b9e\u73b0\u4ea4\u4e92\u5f0f\u63a2\u7d22\u548c\u4efb\u52a1\u9002\u5e94\u3002", "result": "\u901a\u8fc7\u5efa\u6a21\u3001\u5b9e\u9a8c\u548c\u5f00\u653e\u6c34\u57df\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u652f\u6301\u5728\u7ebf\u4efb\u52a1\u9002\u5e94\u548c\u8bed\u4e49\u66f4\u65b0\u3002", "conclusion": "NemeSys\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u6c34\u4e0b\u73af\u5883\u7684\u81ea\u9002\u5e94AUV\u5e73\u53f0\u3002"}}
{"id": "2507.11920", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11920", "abs": "https://arxiv.org/abs/2507.11920", "authors": ["Jeongyong Yang", "KwangBin Lee", "SooJean Han"], "title": "Hybrid Conformal Prediction-based Risk-Aware Model Predictive Planning in Dense, Uncertain Environments", "comment": null, "summary": "Real-time path planning in dense, uncertain environments remains a\nchallenging problem, as predicting the future motions of numerous dynamic\nobstacles is computationally burdensome and unrealistic. To address this, we\nintroduce Hybrid Prediction-based Risk-Aware Planning (HyPRAP), a\nprediction-based risk-aware path-planning framework which uses a hybrid\ncombination of models to predict local obstacle movement. HyPRAP uses a novel\nPrediction-based Collision Risk Index (P-CRI) to evaluate the risk posed by\neach obstacle, enabling the selective use of predictors based on whether the\nagent prioritizes high predictive accuracy or low computational prediction\noverhead. This selective routing enables the agent to focus on high-risk\nobstacles while ignoring or simplifying low-risk ones, making it suitable for\nenvironments with a large number of obstacles. Moreover, HyPRAP incorporates\nuncertainty quantification through hybrid conformal prediction by deriving\nconfidence bounds simultaneously achieved by multiple predictions across\ndifferent models. Theoretical analysis demonstrates that HyPRAP effectively\nbalances safety and computational efficiency by leveraging the diversity of\nprediction models. Extensive simulations validate these insights for more\ngeneral settings, confirming that HyPRAP performs better compared to single\npredictor methods, and P-CRI performs better over naive proximity-based risk\nassessment.", "AI": {"tldr": "HyPRAP\u662f\u4e00\u79cd\u57fa\u4e8e\u9884\u6d4b\u7684\u98ce\u9669\u611f\u77e5\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u6a21\u578b\u9884\u6d4b\u969c\u788d\u7269\u8fd0\u52a8\uff0c\u5e76\u4f7f\u7528P-CRI\u8bc4\u4f30\u98ce\u9669\uff0c\u5e73\u8861\u5b89\u5168\u6027\u4e0e\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5bc6\u96c6\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\u7684\u6311\u6218\uff0c\u907f\u514d\u9884\u6d4b\u5927\u91cf\u969c\u788d\u7269\u8fd0\u52a8\u7684\u9ad8\u8ba1\u7b97\u8d1f\u62c5\u3002", "method": "\u7ed3\u5408\u591a\u79cd\u9884\u6d4b\u6a21\u578b\uff0c\u5229\u7528P-CRI\u9009\u62e9\u6027\u8bc4\u4f30\u969c\u788d\u7269\u98ce\u9669\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u5171\u5f62\u9884\u6d4b\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u4eff\u771f\u8868\u660e\uff0cHyPRAP\u5728\u5b89\u5168\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u5355\u4e00\u9884\u6d4b\u65b9\u6cd5\uff0cP-CRI\u4f18\u4e8e\u57fa\u4e8e\u8ddd\u79bb\u7684\u98ce\u9669\u8bc4\u4f30\u3002", "conclusion": "HyPRAP\u5728\u590d\u6742\u73af\u5883\u4e2d\u6709\u6548\u5e73\u8861\u4e86\u5b89\u5168\u6027\u548c\u5b9e\u65f6\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u969c\u788d\u7269\u573a\u666f\u3002"}}
{"id": "2507.11938", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11938", "abs": "https://arxiv.org/abs/2507.11938", "authors": ["Hao Chen", "Takuya Kiyokawa", "Zhengtao Hu", "Weiwei Wan", "Kensuke Harada"], "title": "A Multi-Level Similarity Approach for Single-View Object Grasping: Matching, Planning, and Fine-Tuning", "comment": "Accepted by IEEE T-RO", "summary": "Grasping unknown objects from a single view has remained a challenging topic\nin robotics due to the uncertainty of partial observation. Recent advances in\nlarge-scale models have led to benchmark solutions such as GraspNet-1Billion.\nHowever, such learning-based approaches still face a critical limitation in\nperformance robustness for their sensitivity to sensing noise and environmental\nchanges. To address this bottleneck in achieving highly generalized grasping,\nwe abandon the traditional learning framework and introduce a new perspective:\nsimilarity matching, where similar known objects are utilized to guide the\ngrasping of unknown target objects. We newly propose a method that robustly\nachieves unknown-object grasping from a single viewpoint through three key\nsteps: 1) Leverage the visual features of the observed object to perform\nsimilarity matching with an existing database containing various object models,\nidentifying potential candidates with high similarity; 2) Use the candidate\nmodels with pre-existing grasping knowledge to plan imitative grasps for the\nunknown target object; 3) Optimize the grasp quality through a local\nfine-tuning process. To address the uncertainty caused by partial and noisy\nobservation, we propose a multi-level similarity matching framework that\nintegrates semantic, geometric, and dimensional features for comprehensive\nevaluation. Especially, we introduce a novel point cloud geometric descriptor,\nthe C-FPFH descriptor, which facilitates accurate similarity assessment between\npartial point clouds of observed objects and complete point clouds of database\nmodels. In addition, we incorporate the use of large language models, introduce\nthe semi-oriented bounding box, and develop a novel point cloud registration\napproach based on plane detection to enhance matching accuracy under\nsingle-view conditions. Videos are available at https://youtu.be/qQDIELMhQmk.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f3c\u6027\u5339\u914d\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5355\u89c6\u89d2\u6293\u53d6\u672a\u77e5\u7269\u4f53\uff0c\u901a\u8fc7\u591a\u7ea7\u7279\u5f81\u5339\u914d\u548c\u5c40\u90e8\u4f18\u5316\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u5b66\u4e60\u6846\u67b6\u5bf9\u611f\u77e5\u566a\u58f0\u548c\u73af\u5883\u53d8\u5316\u654f\u611f\uff0c\u6027\u80fd\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u6293\u53d6\u65b9\u6cd5\u3002", "method": "1) \u5229\u7528\u89c6\u89c9\u7279\u5f81\u8fdb\u884c\u76f8\u4f3c\u6027\u5339\u914d\uff1b2) \u57fa\u4e8e\u5019\u9009\u6a21\u578b\u7684\u6293\u53d6\u77e5\u8bc6\u89c4\u5212\u6a21\u4eff\u6293\u53d6\uff1b3) \u901a\u8fc7\u5c40\u90e8\u4f18\u5316\u63d0\u5347\u6293\u53d6\u8d28\u91cf\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ea7\u76f8\u4f3c\u6027\u5339\u914d\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u4e49\u3001\u51e0\u4f55\u548c\u5c3a\u5bf8\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u70b9\u4e91\u63cf\u8ff0\u7b26C-FPFH\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u901a\u8fc7\u76f8\u4f3c\u6027\u5339\u914d\u548c\u5c40\u90e8\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u89c6\u89d2\u6293\u53d6\u672a\u77e5\u7269\u4f53\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.11940", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11940", "abs": "https://arxiv.org/abs/2507.11940", "authors": ["Kanghyun Ryu", "Minjun Sung", "Piyush Gupta", "Jovin D'sa", "Faizan M. Tariq", "David Isele", "Sangjae Bae"], "title": "IANN-MPPI: Interaction-Aware Neural Network-Enhanced Model Predictive Path Integral Approach for Autonomous Driving", "comment": "To be published in The IEEE International Conference on Intelligent\n  Transportation Systems (ITSC) 2025", "summary": "Motion planning for autonomous vehicles (AVs) in dense traffic is\nchallenging, often leading to overly conservative behavior and unmet planning\nobjectives. This challenge stems from the AVs' limited ability to anticipate\nand respond to the interactive behavior of surrounding agents. Traditional\ndecoupled prediction and planning pipelines rely on non-interactive predictions\nthat overlook the fact that agents often adapt their behavior in response to\nthe AV's actions. To address this, we propose Interaction-Aware Neural\nNetwork-Enhanced Model Predictive Path Integral (IANN-MPPI) control, which\nenables interactive trajectory planning by predicting how surrounding agents\nmay react to each control sequence sampled by MPPI. To improve performance in\nstructured lane environments, we introduce a spline-based prior for the MPPI\nsampling distribution, enabling efficient lane-changing behavior. We evaluate\nIANN-MPPI in a dense traffic merging scenario, demonstrating its ability to\nperform efficient merging maneuvers. Our project website is available at\nhttps://sites.google.com/berkeley.edu/iann-mppi", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u4e92\u611f\u77e5\u7684\u795e\u7ecf\u7f51\u7edc\u589e\u5f3a\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206\u63a7\u5236\uff08IANN-MPPI\uff09\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5bc6\u96c6\u4ea4\u901a\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\uff0c\u901a\u8fc7\u9884\u6d4b\u5468\u56f4\u8f66\u8f86\u5bf9\u63a7\u5236\u5e8f\u5217\u7684\u53cd\u5e94\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u8f66\u9053\u53d8\u6362\u548c\u5408\u5e76\u3002", "motivation": "\u4f20\u7edf\u9884\u6d4b\u4e0e\u89c4\u5212\u65b9\u6cd5\u5ffd\u7565\u4e86\u5468\u56f4\u8f66\u8f86\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u884c\u4e3a\u7684\u4ea4\u4e92\u53cd\u5e94\uff0c\u5bfc\u81f4\u4fdd\u5b88\u884c\u4e3a\u548c\u89c4\u5212\u76ee\u6807\u672a\u8fbe\u6210\u3002", "method": "\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u5468\u56f4\u8f66\u8f86\u7684\u53cd\u5e94\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u6837\u6761\u7684MPPI\u91c7\u6837\u5206\u5e03\u5148\u9a8c\uff0c\u4f18\u5316\u8f66\u9053\u53d8\u6362\u884c\u4e3a\u3002", "result": "\u5728\u5bc6\u96c6\u4ea4\u901a\u5408\u5e76\u573a\u666f\u4e2d\uff0cIANN-MPPI\u80fd\u591f\u9ad8\u6548\u5b8c\u6210\u5408\u5e76\u64cd\u4f5c\u3002", "conclusion": "IANN-MPPI\u901a\u8fc7\u4ea4\u4e92\u611f\u77e5\u548c\u4f18\u5316\u91c7\u6837\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5bc6\u96c6\u4ea4\u901a\u4e2d\u7684\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2507.11974", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11974", "abs": "https://arxiv.org/abs/2507.11974", "authors": ["Waseem Akram", "Muhayy Ud Din", "Lyes Saad Soud", "Irfan Hussain"], "title": "A Review of Generative AI in Aquaculture: Foundations, Applications, and Future Directions for Smart and Sustainable Farming", "comment": null, "summary": "Generative Artificial Intelligence (GAI) has rapidly emerged as a\ntransformative force in aquaculture, enabling intelligent synthesis of\nmultimodal data, including text, images, audio, and simulation outputs for\nsmarter, more adaptive decision-making. As the aquaculture industry shifts\ntoward data-driven, automation and digital integration operations under the\nAquaculture 4.0 paradigm, GAI models offer novel opportunities across\nenvironmental monitoring, robotics, disease diagnostics, infrastructure\nplanning, reporting, and market analysis. This review presents the first\ncomprehensive synthesis of GAI applications in aquaculture, encompassing\nfoundational architectures (e.g., diffusion models, transformers, and retrieval\naugmented generation), experimental systems, pilot deployments, and real-world\nuse cases. We highlight GAI's growing role in enabling underwater perception,\ndigital twin modeling, and autonomous planning for remotely operated vehicle\n(ROV) missions. We also provide an updated application taxonomy that spans\nsensing, control, optimization, communication, and regulatory compliance.\nBeyond technical capabilities, we analyze key limitations, including limited\ndata availability, real-time performance constraints, trust and explainability,\nenvironmental costs, and regulatory uncertainty. This review positions GAI not\nmerely as a tool but as a critical enabler of smart, resilient, and\nenvironmentally aligned aquaculture systems.", "AI": {"tldr": "\u7efc\u8ff0\u63a2\u8ba8\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GAI\uff09\u5728\u6c34\u4ea7\u517b\u6b96\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5305\u62ec\u73af\u5883\u76d1\u6d4b\u3001\u673a\u5668\u4eba\u6280\u672f\u3001\u75be\u75c5\u8bca\u65ad\u7b49\uff0c\u5e76\u5206\u6790\u4e86\u5176\u6280\u672f\u67b6\u6784\u3001\u5b9e\u9645\u6848\u4f8b\u53ca\u9762\u4e34\u7684\u6311\u6218\u3002", "motivation": "\u968f\u7740\u6c34\u4ea7\u517b\u6b96\u5411\u6570\u636e\u9a71\u52a8\u548c\u81ea\u52a8\u5316\u8f6c\u578b\uff08Aquaculture 4.0\uff09\uff0cGAI\u4e3a\u884c\u4e1a\u63d0\u4f9b\u4e86\u667a\u80fd\u51b3\u7b56\u652f\u6301\u7684\u65b0\u673a\u9047\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0GAI\u7684\u57fa\u7840\u67b6\u6784\uff08\u5982\u6269\u6563\u6a21\u578b\u3001Transformer\u7b49\uff09\u3001\u5b9e\u9a8c\u7cfb\u7edf\u3001\u8bd5\u70b9\u90e8\u7f72\u548c\u5b9e\u9645\u7528\u4f8b\uff0c\u5206\u6790\u5176\u5728\u6c34\u4ea7\u517b\u6b96\u4e2d\u7684\u5e94\u7528\u3002", "result": "GAI\u5728\u6c34\u4e0b\u611f\u77e5\u3001\u6570\u5b57\u5b6a\u751f\u5efa\u6a21\u548c\u81ea\u4e3b\u89c4\u5212\u7b49\u65b9\u9762\u53d1\u6325\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u4ecd\u9762\u4e34\u6570\u636e\u4e0d\u8db3\u3001\u5b9e\u65f6\u6027\u80fd\u9650\u5236\u7b49\u6311\u6218\u3002", "conclusion": "GAI\u662f\u63a8\u52a8\u667a\u80fd\u3001\u97e7\u6027\u548c\u73af\u4fdd\u6c34\u4ea7\u517b\u6b96\u7cfb\u7edf\u7684\u5173\u952e\u5de5\u5177\uff0c\u4f46\u9700\u89e3\u51b3\u6280\u672f\u548c\u76d1\u7ba1\u95ee\u9898\u3002"}}
{"id": "2507.11991", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11991", "abs": "https://arxiv.org/abs/2507.11991", "authors": ["Juanran Wang", "Marc R. Schlichting", "Mykel J. Kochenderfer"], "title": "Robust Planning for Autonomous Vehicles with Diffusion-Based Failure Samplers", "comment": null, "summary": "High-risk traffic zones such as intersections are a major cause of\ncollisions. This study leverages deep generative models to enhance the safety\nof autonomous vehicles in an intersection context. We train a 1000-step\ndenoising diffusion probabilistic model to generate collision-causing sensor\nnoise sequences for an autonomous vehicle navigating a four-way intersection\nbased on the current relative position and velocity of an intruder. Using the\ngenerative adversarial architecture, the 1000-step model is distilled into a\nsingle-step denoising diffusion model which demonstrates fast inference speed\nwhile maintaining similar sampling quality. We demonstrate one possible\napplication of the single-step model in building a robust planner for the\nautonomous vehicle. The planner uses the single-step model to efficiently\nsample potential failure cases based on the currently measured traffic state to\ninform its decision-making. Through simulation experiments, the robust planner\ndemonstrates significantly lower failure rate and delay rate compared with the\nbaseline Intelligent Driver Model controller.", "AI": {"tldr": "\u5229\u7528\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u4ea4\u53c9\u8def\u53e3\u7684\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u5355\u6b65\u53bb\u566a\u6269\u6563\u6a21\u578b\u5feb\u901f\u751f\u6210\u6f5c\u5728\u78b0\u649e\u573a\u666f\uff0c\u4f18\u5316\u51b3\u7b56\u89c4\u5212\u5668\u3002", "motivation": "\u9ad8\u98ce\u9669\u4ea4\u901a\u533a\u57df\uff08\u5982\u4ea4\u53c9\u8def\u53e3\uff09\u662f\u78b0\u649e\u4e8b\u6545\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u7814\u7a76\u65e8\u5728\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u6b64\u7c7b\u573a\u666f\u4e2d\u7684\u5b89\u5168\u6027\u3002", "method": "\u8bad\u7ec31000\u6b65\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\u751f\u6210\u78b0\u649e\u76f8\u5173\u7684\u4f20\u611f\u5668\u566a\u58f0\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u67b6\u6784\u5c06\u5176\u84b8\u998f\u4e3a\u5355\u6b65\u6a21\u578b\uff0c\u7528\u4e8e\u5feb\u901f\u63a8\u7406\u3002", "result": "\u5355\u6b65\u6a21\u578b\u5728\u4fdd\u6301\u91c7\u6837\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u5feb\u901f\u63a8\u7406\uff0c\u57fa\u4e8e\u6b64\u6784\u5efa\u7684\u89c4\u5212\u5668\u5728\u4eff\u771f\u5b9e\u9a8c\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u5931\u8d25\u7387\u548c\u5ef6\u8fdf\u7387\u3002", "conclusion": "\u5355\u6b65\u53bb\u566a\u6269\u6563\u6a21\u578b\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u4ea4\u53c9\u8def\u53e3\u7684\u5b89\u5168\u51b3\u7b56\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12067", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12067", "abs": "https://arxiv.org/abs/2507.12067", "authors": ["Xing Tong", "Michele D. Simoni"], "title": "Robust Route Planning for Sidewalk Delivery Robots", "comment": null, "summary": "Sidewalk delivery robots are a promising solution for urban freight\ndistribution, reducing congestion compared to trucks and providing a safer,\nhigher-capacity alternative to drones. However, unreliable travel times on\nsidewalks due to pedestrian density, obstacles, and varying infrastructure\nconditions can significantly affect their efficiency. This study addresses the\nrobust route planning problem for sidewalk robots, explicitly accounting for\ntravel time uncertainty due to varying sidewalk conditions. Optimization is\nintegrated with simulation to reproduce the effect of obstacles and pedestrian\nflows and generate realistic travel times. The study investigates three\ndifferent approaches to derive uncertainty sets, including budgeted,\nellipsoidal, and support vector clustering (SVC)-based methods, along with a\ndistributionally robust method to solve the shortest path (SP) problem. A\nrealistic case study reproducing pedestrian patterns in Stockholm's city center\nis used to evaluate the efficiency of robust routing across various robot\ndesigns and environmental conditions. The results show that, when compared to a\nconventional SP, robust routing significantly enhances operational reliability\nunder variable sidewalk conditions. The Ellipsoidal and DRSP approaches\noutperform the other methods, yielding the most efficient paths in terms of\naverage and worst-case delay. Sensitivity analyses reveal that robust\napproaches consistently outperform the conventional SP, particularly for\nsidewalk delivery robots that are wider, slower, and have more conservative\nnavigation behaviors. These benefits are even more pronounced in adverse\nweather conditions and high pedestrian congestion scenarios.", "AI": {"tldr": "\u7814\u7a76\u9488\u5bf9\u4eba\u884c\u9053\u9001\u8d27\u673a\u5668\u4eba\u7684\u9c81\u68d2\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u4e0e\u6a21\u62df\u7ed3\u5408\uff0c\u89e3\u51b3\u56e0\u884c\u4eba\u5bc6\u5ea6\u548c\u969c\u788d\u7269\u5bfc\u81f4\u7684\u65c5\u884c\u65f6\u95f4\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u4eba\u884c\u9053\u9001\u8d27\u673a\u5668\u4eba\u662f\u57ce\u5e02\u8d27\u8fd0\u7684\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u65c5\u884c\u65f6\u95f4\u7684\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd\u5176\u6548\u7387\u3002", "method": "\u96c6\u6210\u4f18\u5316\u4e0e\u6a21\u62df\uff0c\u91c7\u7528\u9884\u7b97\u3001\u692d\u7403\u548c\u652f\u6301\u5411\u91cf\u805a\u7c7b\uff08SVC\uff09\u65b9\u6cd5\u53ca\u5206\u5e03\u9c81\u68d2\u65b9\u6cd5\u89e3\u51b3\u6700\u77ed\u8def\u5f84\u95ee\u9898\u3002", "result": "\u9c81\u68d2\u8def\u5f84\u89c4\u5212\u663e\u8457\u63d0\u5347\u64cd\u4f5c\u53ef\u9760\u6027\uff0c\u692d\u7403\u548cDRSP\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u9c81\u68d2\u65b9\u6cd5\u5728\u6076\u52a3\u5929\u6c14\u548c\u9ad8\u884c\u4eba\u5bc6\u5ea6\u4e0b\u6548\u679c\u66f4\u663e\u8457\uff0c\u5c24\u5176\u9002\u5408\u5bbd\u578b\u3001\u4f4e\u901f\u673a\u5668\u4eba\u3002"}}
{"id": "2507.12093", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12093", "abs": "https://arxiv.org/abs/2507.12093", "authors": ["David Rapado-Rincon", "Gert Kootstra"], "title": "Tree-SLAM: semantic object SLAM for efficient mapping of individual trees in orchards", "comment": "Paper submitted to Smart Agricultural Technology", "summary": "Accurate mapping of individual trees is an important component for precision\nagriculture in orchards, as it allows autonomous robots to perform tasks like\ntargeted operations or individual tree monitoring. However, creating these maps\nis challenging because GPS signals are often unreliable under dense tree\ncanopies. Furthermore, standard Simultaneous Localization and Mapping (SLAM)\napproaches struggle in orchards because the repetitive appearance of trees can\nconfuse the system, leading to mapping errors. To address this, we introduce\nTree-SLAM, a semantic SLAM approach tailored for creating maps of individual\ntrees in orchards. Utilizing RGB-D images, our method detects tree trunks with\nan instance segmentation model, estimates their location and re-identifies them\nusing a cascade-graph-based data association algorithm. These re-identified\ntrunks serve as landmarks in a factor graph framework that integrates noisy GPS\nsignals, odometry, and trunk observations. The system produces maps of\nindividual trees with a geo-localization error as low as 18 cm, which is less\nthan 20\\% of the planting distance. The proposed method was validated on\ndiverse datasets from apple and pear orchards across different seasons,\ndemonstrating high mapping accuracy and robustness in scenarios with unreliable\nGPS signals.", "AI": {"tldr": "Tree-SLAM\u662f\u4e00\u79cd\u9488\u5bf9\u679c\u56ed\u4e2d\u5355\u68f5\u6811\u5b9a\u4f4d\u7684\u8bed\u4e49SLAM\u65b9\u6cd5\uff0c\u901a\u8fc7RGB-D\u56fe\u50cf\u548c\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\u68c0\u6d4b\u6811\u5e72\uff0c\u7ed3\u5408GPS\u3001\u91cc\u7a0b\u8ba1\u548c\u6811\u5e72\u89c2\u6d4b\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5730\u56fe\u6784\u5efa\u3002", "motivation": "\u679c\u56ed\u4e2dGPS\u4fe1\u53f7\u4e0d\u53ef\u9760\u4e14\u6807\u51c6SLAM\u65b9\u6cd5\u56e0\u6811\u6728\u91cd\u590d\u5916\u89c2\u6613\u51fa\u9519\uff0c\u9700\u4e00\u79cd\u7cbe\u51c6\u7684\u5355\u68f5\u6811\u5b9a\u4f4d\u65b9\u6cd5\u4ee5\u652f\u6301\u81ea\u4e3b\u673a\u5668\u4eba\u4efb\u52a1\u3002", "method": "\u5229\u7528RGB-D\u56fe\u50cf\u68c0\u6d4b\u6811\u5e72\uff0c\u901a\u8fc7\u7ea7\u8054\u56fe\u6570\u636e\u5173\u8054\u7b97\u6cd5\u91cd\u8bc6\u522b\u6811\u5e72\uff0c\u7ed3\u5408\u56e0\u5b50\u56fe\u6846\u67b6\u6574\u5408GPS\u3001\u91cc\u7a0b\u8ba1\u548c\u6811\u5e72\u89c2\u6d4b\u3002", "result": "\u5728\u82f9\u679c\u548c\u68a8\u56ed\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5b9a\u4f4d\u8bef\u5dee\u4f4e\u81f318\u5398\u7c73\uff0c\u4f4e\u4e8e\u79cd\u690d\u8ddd\u79bb\u768420%\uff0c\u4e14\u5bf9\u4e0d\u53ef\u9760GPS\u4fe1\u53f7\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "Tree-SLAM\u5728\u679c\u56ed\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5355\u68f5\u6811\u5b9a\u4f4d\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u5b63\u8282\u548c\u6811\u79cd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.12148", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12148", "abs": "https://arxiv.org/abs/2507.12148", "authors": ["Xing Tong", "Michele D. Simoni", "Kaj Munhoz Arfvidsson", "Jonas M\u00e5rtensson"], "title": "Leveraging Sidewalk Robots for Walkability-Related Analyses", "comment": null, "summary": "Walkability is a key component of sustainable urban development, while\ncollecting detailed data on its related features remains challenging due to the\nhigh costs and limited scalability of traditional methods. Sidewalk delivery\nrobots, increasingly deployed in urban environments, offer a promising solution\nto these limitations. This paper explores how these robots can serve as mobile\ndata collection platforms, capturing sidewalk-level features related to\nwalkability in a scalable, automated, and real-time manner. A sensor-equipped\nrobot was deployed on a sidewalk network at KTH in Stockholm, completing 101\ntrips covering 900 segments. From the collected data, different typologies of\nfeatures are derived, including robot trip characteristics (e.g., speed,\nduration), sidewalk conditions (e.g., width, surface unevenness), and sidewalk\nutilization (e.g., pedestrian density). Their walkability-related implications\nwere investigated with a series of analyses. The results demonstrate that\npedestrian movement patterns are strongly influenced by sidewalk\ncharacteristics, with higher density, reduced width, and surface irregularity\nassociated with slower and more variable trajectories. Notably, robot speed\nclosely mirrors pedestrian behavior, highlighting its potential as a proxy for\nassessing pedestrian dynamics. The proposed framework enables continuous\nmonitoring of sidewalk conditions and pedestrian behavior, contributing to the\ndevelopment of more walkable, inclusive, and responsive urban environments.", "AI": {"tldr": "\u5229\u7528\u914d\u5907\u4f20\u611f\u5668\u7684\u673a\u5668\u4eba\u6536\u96c6\u4eba\u884c\u9053\u6570\u636e\uff0c\u5206\u6790\u5176\u5bf9\u6b65\u884c\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4eba\u884c\u9053\u7279\u5f81\u663e\u8457\u5f71\u54cd\u884c\u4eba\u884c\u4e3a\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u6536\u96c6\u6b65\u884c\u6027\u76f8\u5173\u6570\u636e\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u800c\u4eba\u884c\u9053\u914d\u9001\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5728\u65af\u5fb7\u54e5\u5c14\u6469KTH\u90e8\u7f72\u4f20\u611f\u5668\u673a\u5668\u4eba\uff0c\u5b8c\u6210101\u6b21\u884c\u7a0b\uff0c\u6536\u96c6\u4eba\u884c\u9053\u7279\u5f81\u6570\u636e\u5e76\u5206\u6790\u5176\u4e0e\u6b65\u884c\u6027\u7684\u5173\u7cfb\u3002", "result": "\u884c\u4eba\u884c\u4e3a\u53d7\u4eba\u884c\u9053\u7279\u5f81\uff08\u5982\u5bbd\u5ea6\u3001\u5bc6\u5ea6\u3001\u8868\u9762\u4e0d\u5e73\uff09\u663e\u8457\u5f71\u54cd\uff0c\u673a\u5668\u4eba\u901f\u5ea6\u53ef\u4f5c\u4e3a\u884c\u4eba\u52a8\u6001\u7684\u4ee3\u7406\u6307\u6807\u3002", "conclusion": "\u8be5\u6846\u67b6\u652f\u6301\u6301\u7eed\u76d1\u6d4b\u4eba\u884c\u9053\u6761\u4ef6\u548c\u884c\u4eba\u884c\u4e3a\uff0c\u6709\u52a9\u4e8e\u5efa\u8bbe\u66f4\u9002\u5b9c\u6b65\u884c\u3001\u5305\u5bb9\u6027\u5f3a\u7684\u57ce\u5e02\u73af\u5883\u3002"}}
{"id": "2507.12158", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12158", "abs": "https://arxiv.org/abs/2507.12158", "authors": ["Nawshin Mannan Proma", "Gricel V\u00e1zquez", "Sepeedeh Shahbeigi", "Arjun Badyal", "Victoria Hodge"], "title": "Probabilistic Safety Verification for an Autonomous Ground Vehicle: A Situation Coverage Grid Approach", "comment": "6 pages, 6 figures", "summary": "As industrial autonomous ground vehicles are increasingly deployed in\nsafety-critical environments, ensuring their safe operation under diverse\nconditions is paramount. This paper presents a novel approach for their safety\nverification based on systematic situation extraction, probabilistic modelling\nand verification. We build upon the concept of a situation coverage grid, which\nexhaustively enumerates environmental configurations relevant to the vehicle's\noperation. This grid is augmented with quantitative probabilistic data\ncollected from situation-based system testing, capturing probabilistic\ntransitions between situations. We then generate a probabilistic model that\nencodes the dynamics of both normal and unsafe system behaviour. Safety\nproperties extracted from hazard analysis and formalised in temporal logic are\nverified through probabilistic model checking against this model. The results\ndemonstrate that our approach effectively identifies high-risk situations,\nprovides quantitative safety guarantees, and supports compliance with\nregulatory standards, thereby contributing to the robust deployment of\nautonomous systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7cfb\u7edf\u6027\u60c5\u5883\u63d0\u53d6\u3001\u6982\u7387\u5efa\u6a21\u548c\u9a8c\u8bc1\u7684\u5de5\u4e1a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5b89\u5168\u9a8c\u8bc1\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6982\u7387\u6a21\u578b\u68c0\u67e5\u9a8c\u8bc1\u5b89\u5168\u6027\u3002", "motivation": "\u5de5\u4e1a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u9700\u786e\u4fdd\u5176\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u5b89\u5168\u8fd0\u884c\u3002", "method": "\u6784\u5efa\u60c5\u5883\u8986\u76d6\u7f51\u683c\uff0c\u7ed3\u5408\u6982\u7387\u6570\u636e\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u6982\u7387\u6a21\u578b\u68c0\u67e5\u9a8c\u8bc1\u5b89\u5168\u6027\u3002", "result": "\u65b9\u6cd5\u6709\u6548\u8bc6\u522b\u9ad8\u98ce\u9669\u60c5\u5883\uff0c\u63d0\u4f9b\u5b9a\u91cf\u5b89\u5168\u4fdd\u8bc1\uff0c\u652f\u6301\u5408\u89c4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u7a33\u5065\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2507.12174", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.12174", "abs": "https://arxiv.org/abs/2507.12174", "authors": ["Zhenmin Huang", "Yusen Xie", "Benshan Ma", "Shaojie Shen", "Jun Ma"], "title": "Fast and Scalable Game-Theoretic Trajectory Planning with Intentional Uncertainties", "comment": null, "summary": "Trajectory planning involving multi-agent interactions has been a\nlong-standing challenge in the field of robotics, primarily burdened by the\ninherent yet intricate interactions among agents. While game-theoretic methods\nare widely acknowledged for their effectiveness in managing multi-agent\ninteractions, significant impediments persist when it comes to accommodating\nthe intentional uncertainties of agents. In the context of intentional\nuncertainties, the heavy computational burdens associated with existing\ngame-theoretic methods are induced, leading to inefficiencies and poor\nscalability. In this paper, we propose a novel game-theoretic interactive\ntrajectory planning method to effectively address the intentional uncertainties\nof agents, and it demonstrates both high efficiency and enhanced scalability.\nAs the underpinning basis, we model the interactions between agents under\nintentional uncertainties as a general Bayesian game, and we show that its\nagent-form equivalence can be represented as a potential game under certain\nminor assumptions. The existence and attainability of the optimal interactive\ntrajectories are illustrated, as the corresponding Bayesian Nash equilibrium\ncan be attained by optimizing a unified optimization problem. Additionally, we\npresent a distributed algorithm based on the dual consensus alternating\ndirection method of multipliers (ADMM) tailored to the parallel solving of the\nproblem, thereby significantly improving the scalability. The attendant\noutcomes from simulations and experiments demonstrate that the proposed method\nis effective across a range of scenarios characterized by general forms of\nintentional uncertainties. Its scalability surpasses that of existing\ncentralized and decentralized baselines, allowing for real-time interactive\ntrajectory planning in uncertain game settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u6709\u6548\u5904\u7406\u610f\u56fe\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u4e2d\u7684\u610f\u56fe\u4e0d\u786e\u5b9a\u6027\u5bfc\u81f4\u73b0\u6709\u535a\u5f08\u8bba\u65b9\u6cd5\u8ba1\u7b97\u8d1f\u62c5\u91cd\uff0c\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u5c06\u610f\u56fe\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u4ea4\u4e92\u5efa\u6a21\u4e3a\u8d1d\u53f6\u65af\u535a\u5f08\uff0c\u5e76\u8bc1\u660e\u5176\u5728\u4e00\u5b9a\u6761\u4ef6\u4e0b\u53ef\u8f6c\u5316\u4e3a\u6f5c\u5728\u535a\u5f08\uff1b\u63d0\u51fa\u57fa\u4e8eADMM\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\u5e76\u884c\u6c42\u89e3\u3002", "result": "\u65b9\u6cd5\u5728\u591a\u79cd\u610f\u56fe\u4e0d\u786e\u5b9a\u6027\u573a\u666f\u4e0b\u6709\u6548\uff0c\u53ef\u6269\u5c55\u6027\u4f18\u4e8e\u73b0\u6709\u96c6\u4e2d\u5f0f\u548c\u5206\u6563\u5f0f\u57fa\u7ebf\uff0c\u652f\u6301\u5b9e\u65f6\u89c4\u5212\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u610f\u56fe\u4e0d\u786e\u5b9a\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u89c4\u5212\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.12194", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12194", "abs": "https://arxiv.org/abs/2507.12194", "authors": ["Hongming Shen", "Xun Chen", "Yulin Hui", "Zhenyu Wu", "Wei Wang", "Qiyang Lyu", "Tianchen Deng", "Danwei Wang"], "title": "UniLGL: Learning Uniform Place Recognition for FOV-limited/Panoramic LiDAR Global Localization", "comment": null, "summary": "Existing LGL methods typically consider only partial information (e.g.,\ngeometric features) from LiDAR observations or are designed for homogeneous\nLiDAR sensors, overlooking the uniformity in LGL. In this work, a uniform LGL\nmethod is proposed, termed UniLGL, which simultaneously achieves spatial and\nmaterial uniformity, as well as sensor-type uniformity. The key idea of the\nproposed method is to encode the complete point cloud, which contains both\ngeometric and material information, into a pair of BEV images (i.e., a spatial\nBEV image and an intensity BEV image). An end-to-end multi-BEV fusion network\nis designed to extract uniform features, equipping UniLGL with spatial and\nmaterial uniformity. To ensure robust LGL across heterogeneous LiDAR sensors, a\nviewpoint invariance hypothesis is introduced, which replaces the conventional\ntranslation equivariance assumption commonly used in existing LPR networks and\nsupervises UniLGL to achieve sensor-type uniformity in both global descriptors\nand local feature representations. Finally, based on the mapping between local\nfeatures on the 2D BEV image and the point cloud, a robust global pose\nestimator is derived that determines the global minimum of the global pose on\nSE(3) without requiring additional registration. To validate the effectiveness\nof the proposed uniform LGL, extensive benchmarks are conducted in real-world\nenvironments, and the results show that the proposed UniLGL is demonstratively\ncompetitive compared to other State-of-the-Art LGL methods. Furthermore, UniLGL\nhas been deployed on diverse platforms, including full-size trucks and agile\nMicro Aerial Vehicles (MAVs), to enable high-precision localization and mapping\nas well as multi-MAV collaborative exploration in port and forest environments,\ndemonstrating the applicability of UniLGL in industrial and field scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684LiDAR\u5168\u5c40\u5b9a\u4f4d\uff08LGL\uff09\u65b9\u6cd5UniLGL\uff0c\u901a\u8fc7\u7f16\u7801\u5b8c\u6574\u70b9\u4e91\u4fe1\u606f\u5e76\u8bbe\u8ba1\u591aBEV\u878d\u5408\u7f51\u7edc\uff0c\u5b9e\u73b0\u7a7a\u95f4\u3001\u6750\u8d28\u548c\u4f20\u611f\u5668\u7c7b\u578b\u7684\u5747\u5300\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709LGL\u65b9\u6cd5\u901a\u5e38\u4ec5\u8003\u8651\u90e8\u5206\u4fe1\u606f\u6216\u9488\u5bf9\u5355\u4e00\u4f20\u611f\u5668\uff0c\u5ffd\u7565\u4e86\u5747\u5300\u6027\u3002UniLGL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u70b9\u4e91\u7f16\u7801\u4e3a\u7a7a\u95f4\u548c\u5f3a\u5ea6BEV\u56fe\u50cf\uff0c\u8bbe\u8ba1\u591aBEV\u878d\u5408\u7f51\u7edc\u63d0\u53d6\u5747\u5300\u7279\u5f81\uff0c\u5f15\u5165\u89c6\u89d2\u4e0d\u53d8\u6027\u5047\u8bbe\u786e\u4fdd\u4f20\u611f\u5668\u7c7b\u578b\u5747\u5300\u6027\uff0c\u5e76\u63a8\u5bfc\u5168\u5c40\u59ff\u6001\u4f30\u8ba1\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660eUniLGL\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u79cd\u5e73\u53f0\u4e0a\u6210\u529f\u90e8\u7f72\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u548c\u91ce\u5916\u573a\u666f\u3002", "conclusion": "UniLGL\u901a\u8fc7\u7edf\u4e00\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86LGL\u7684\u6027\u80fd\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2507.12273", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12273", "abs": "https://arxiv.org/abs/2507.12273", "authors": ["Luca Garello", "Francesca Cocchella", "Alessandra Sciutti", "Manuel Catalano", "Francesco Rea"], "title": "Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction with an Agentic Robot", "comment": null, "summary": "Autonomous robots are increasingly being tested into public spaces to enhance\nuser experiences, particularly in cultural and educational settings. This paper\npresents the design, implementation, and evaluation of the autonomous museum\nguide robot Alter-Ego equipped with advanced navigation and interactive\ncapabilities. The robot leverages state-of-the-art Large Language Models (LLMs)\nto provide real-time, context aware question-and-answer (Q&A) interactions,\nallowing visitors to engage in conversations about exhibits. It also employs\nrobust simultaneous localization and mapping (SLAM) techniques, enabling\nseamless navigation through museum spaces and route adaptation based on user\nrequests. The system was tested in a real museum environment with 34\nparticipants, combining qualitative analysis of visitor-robot conversations and\nquantitative analysis of pre and post interaction surveys. Results showed that\nthe robot was generally well-received and contributed to an engaging museum\nexperience, despite some limitations in comprehension and responsiveness. This\nstudy sheds light on HRI in cultural spaces, highlighting not only the\npotential of AI-driven robotics to support accessibility and knowledge\nacquisition, but also the current limitations and challenges of deploying such\ntechnologies in complex, real-world environments.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u81ea\u4e3b\u535a\u7269\u9986\u5bfc\u89c8\u673a\u5668\u4ebaAlter-Ego\u7684\u8bbe\u8ba1\u4e0e\u8bc4\u4f30\uff0c\u7ed3\u5408LLMs\u548cSLAM\u6280\u672f\u63d0\u5347\u4e92\u52a8\u4f53\u9a8c\uff0c\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\u5176\u53d7\u6b22\u8fce\u4f46\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u7d22AI\u9a71\u52a8\u673a\u5668\u4eba\u5728\u6587\u5316\u7a7a\u95f4\u4e2d\u5982\u4f55\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u77e5\u8bc6\u83b7\u53d6\uff0c\u540c\u65f6\u63ed\u793a\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u57fa\u4e8eLLMs\u548cSLAM\u6280\u672f\u7684\u673a\u5668\u4eba\uff0c\u901a\u8fc734\u540d\u53c2\u4e0e\u8005\u7684\u5b9e\u5730\u6d4b\u8bd5\uff0c\u7ed3\u5408\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\u3002", "result": "\u673a\u5668\u4eba\u53d7\u5230\u666e\u904d\u6b22\u8fce\uff0c\u63d0\u5347\u4e86\u535a\u7269\u9986\u4f53\u9a8c\uff0c\u4f46\u5728\u7406\u89e3\u548c\u54cd\u5e94\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "conclusion": "AI\u673a\u5668\u4eba\u6709\u6f5c\u529b\u652f\u6301\u6587\u5316\u7a7a\u95f4\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u5b66\u4e60\uff0c\u4f46\u9700\u514b\u670d\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6280\u672f\u9650\u5236\u3002"}}
{"id": "2507.12391", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12391", "abs": "https://arxiv.org/abs/2507.12391", "authors": ["Jacinto Colan", "Ana Davila", "Yasuhisa Hasegawa"], "title": "Assessing the Value of Visual Input: A Benchmark of Multimodal Large Language Models for Robotic Path Planning", "comment": "Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)", "summary": "Large Language Models (LLMs) show potential for enhancing robotic path\nplanning. This paper assesses visual input's utility for multimodal LLMs in\nsuch tasks via a comprehensive benchmark. We evaluated 15 multimodal LLMs on\ngenerating valid and optimal paths in 2D grid environments, simulating\nsimplified robotic planning, comparing text-only versus text-plus-visual inputs\nacross varying model sizes and grid complexities. Our results indicate moderate\nsuccess rates on simpler small grids, where visual input or few-shot text\nprompting offered some benefits. However, performance significantly degraded on\nlarger grids, highlighting a scalability challenge. While larger models\ngenerally achieved higher average success, the visual modality was not\nuniversally dominant over well-structured text for these multimodal systems,\nand successful paths on simpler grids were generally of high quality. These\nresults indicate current limitations in robust spatial reasoning, constraint\nadherence, and scalable multimodal integration, identifying areas for future\nLLM development in robotic path planning.", "AI": {"tldr": "\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u89c6\u89c9\u8f93\u5165\u6548\u7528\uff0c\u53d1\u73b0\u89c6\u89c9\u8f93\u5165\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u6709\u4e00\u5b9a\u5e2e\u52a9\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u8f93\u5165\u5bf9\u591a\u6a21\u6001LLMs\u5728\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u6f5c\u5728\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f3015\u79cd\u591a\u6a21\u6001LLMs\u57282D\u7f51\u683c\u73af\u5883\u4e2d\u751f\u6210\u6709\u6548\u548c\u6700\u4f18\u8def\u5f84\u7684\u80fd\u529b\uff0c\u6bd4\u8f83\u6587\u672c\u4e0e\u6587\u672c\u52a0\u89c6\u89c9\u8f93\u5165\u7684\u6548\u679c\u3002", "result": "\u5728\u7b80\u5355\u5c0f\u7f51\u683c\u4e2d\u8868\u73b0\u5c1a\u53ef\uff0c\u89c6\u89c9\u8f93\u5165\u6216\u5c11\u91cf\u6587\u672c\u63d0\u793a\u6709\u4e00\u5b9a\u5e2e\u52a9\uff1b\u4f46\u5728\u5927\u7f51\u683c\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u89c6\u89c9\u6a21\u6001\u5e76\u672a\u666e\u904d\u4f18\u4e8e\u7ed3\u6784\u5316\u6587\u672c\u3002", "conclusion": "\u5f53\u524d\u591a\u6a21\u6001LLMs\u5728\u7a7a\u95f4\u63a8\u7406\u3001\u7ea6\u675f\u9075\u5faa\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2507.12407", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12407", "abs": "https://arxiv.org/abs/2507.12407", "authors": ["Svetlana Levit", "Marc Toussaint"], "title": "Regrasp Maps for Sequential Manipulation Planning", "comment": null, "summary": "We consider manipulation problems in constrained and cluttered settings,\nwhich require several regrasps at unknown locations. We propose to inform an\noptimization-based task and motion planning (TAMP) solver with possible regrasp\nareas and grasp sequences to speed up the search. Our main idea is to use a\nstate space abstraction, a regrasp map, capturing the combinations of available\ngrasps in different parts of the configuration space, and allowing us to\nprovide the solver with guesses for the mode switches and additional\nconstraints for the object placements. By interleaving the creation of regrasp\nmaps, their adaptation based on failed refinements, and solving TAMP\n(sub)problems, we are able to provide a robust search method for challenging\nregrasp manipulation problems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7684\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\uff08TAMP\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u72b6\u6001\u7a7a\u95f4\u62bd\u8c61\uff08regrasp map\uff09\u52a0\u901f\u641c\u7d22\uff0c\u89e3\u51b3\u53d7\u9650\u548c\u6742\u4e71\u73af\u5883\u4e2d\u7684\u591a\u6b21\u6293\u53d6\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5728\u53d7\u9650\u548c\u6742\u4e71\u73af\u5883\u4e2d\u9700\u8981\u591a\u6b21\u6293\u53d6\u4e14\u6293\u53d6\u4f4d\u7f6e\u672a\u77e5\u7684\u64cd\u7eb5\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u72b6\u6001\u7a7a\u95f4\u62bd\u8c61\uff08regrasp map\uff09\u4e3a\u4f18\u5316\u5668\u63d0\u4f9b\u6293\u53d6\u5e8f\u5217\u548c\u53ef\u80fd\u7684\u6293\u53d6\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u8c03\u6574regrasp map\u548c\u89e3\u51b3TAMP\u5b50\u95ee\u9898\u6765\u4f18\u5316\u641c\u7d22\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u641c\u7d22\u65b9\u6cd5\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u590d\u6742\u7684\u591a\u6b21\u6293\u53d6\u64cd\u7eb5\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408regrasp map\u548cTAMP\u6c42\u89e3\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u64cd\u7eb5\u6548\u7387\u3002"}}
{"id": "2507.12431", "categories": ["cs.RO", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2507.12431", "abs": "https://arxiv.org/abs/2507.12431", "authors": ["Connor Burgess", "Kyle Douin", "Amir Kordijazi"], "title": "Design and Development of an Automated Contact Angle Tester (ACAT) for Surface Wettability Measurement", "comment": "14 pages, 4 figures", "summary": "The Automated Contact Angle Tester (ACAT) is a fully integrated robotic work\ncell developed to automate the measurement of surface wettability on 3D-printed\nmaterials. Designed for precision, repeatability, and safety, ACAT addresses\nthe limitations of manual contact angle testing by combining programmable\nrobotics, precise liquid dispensing, and a modular software-hardware\narchitecture. The system is composed of three core subsystems: (1) an\nelectrical system including power, control, and safety circuits compliant with\nindustrial standards such as NEC 70, NFPA 79, and UL 508A; (2) a software\ncontrol system based on a Raspberry Pi and Python, featuring fault detection,\nGPIO logic, and operator interfaces; and (3) a mechanical system that includes\na 3-axis Cartesian robot, pneumatic actuation, and a precision liquid dispenser\nenclosed within a safety-certified frame. The ACAT enables high-throughput,\nautomated surface characterization and provides a robust platform for future\nintegration into smart manufacturing and materials discovery workflows. This\npaper details the design methodology, implementation strategies, and system\nintegration required to develop the ACAT platform.", "AI": {"tldr": "ACAT\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u6d4b\u91cf3D\u6253\u5370\u6750\u6599\u7684\u8868\u9762\u6da6\u6e7f\u6027\uff0c\u89e3\u51b3\u4e86\u624b\u52a8\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u624b\u52a8\u63a5\u89e6\u89d2\u6d4b\u8bd5\u5b58\u5728\u7cbe\u5ea6\u548c\u91cd\u590d\u6027\u95ee\u9898\uff0cACAT\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u63d0\u9ad8\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "method": "ACAT\u7531\u7535\u6c14\u7cfb\u7edf\u3001\u8f6f\u4ef6\u63a7\u5236\u7cfb\u7edf\u548c\u673a\u68b0\u7cfb\u7edf\u7ec4\u6210\uff0c\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u7b26\u5408\u5de5\u4e1a\u6807\u51c6\u3002", "result": "ACAT\u5b9e\u73b0\u4e86\u9ad8\u901a\u91cf\u3001\u81ea\u52a8\u5316\u7684\u8868\u9762\u8868\u5f81\uff0c\u4e3a\u667a\u80fd\u5236\u9020\u548c\u6750\u6599\u53d1\u73b0\u63d0\u4f9b\u4e86\u5e73\u53f0\u3002", "conclusion": "ACAT\u7684\u8bbe\u8ba1\u548c\u5b9e\u65bd\u5c55\u793a\u4e86\u5176\u5728\u81ea\u52a8\u5316\u8868\u9762\u6d4b\u8bd5\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u96c6\u6210\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.12440", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12440", "abs": "https://arxiv.org/abs/2507.12440", "authors": ["Ruihan Yang", "Qinxi Yu", "Yecheng Wu", "Rui Yan", "Borui Li", "An-Chieh Cheng", "Xueyan Zou", "Yunhao Fang", "Hongxu Yin", "Sifei Liu", "Song Han", "Yao Lu", "Xiaolong Wang"], "title": "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos", "comment": "More videos can be found on our website:\n  https://rchalyang.github.io/EgoVLA", "summary": "Real robot data collection for imitation learning has led to significant\nadvancements in robotic manipulation. However, the requirement for robot\nhardware in the process fundamentally constrains the scale of the data. In this\npaper, we explore training Vision-Language-Action (VLA) models using egocentric\nhuman videos. The benefit of using human videos is not only for their scale but\nmore importantly for the richness of scenes and tasks. With a VLA trained on\nhuman video that predicts human wrist and hand actions, we can perform Inverse\nKinematics and retargeting to convert the human actions to robot actions. We\nfine-tune the model using a few robot manipulation demonstrations to obtain the\nrobot policy, namely EgoVLA. We propose a simulation benchmark called Isaac\nHumanoid Manipulation Benchmark, where we design diverse bimanual manipulation\ntasks with demonstrations. We fine-tune and evaluate EgoVLA with Isaac Humanoid\nManipulation Benchmark and show significant improvements over baselines and\nablate the importance of human data. Videos can be found on our website:\nhttps://rchalyang.github.io/EgoVLA", "AI": {"tldr": "\u5229\u7528\u4eba\u7c7b\u89c6\u9891\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u9006\u8fd0\u52a8\u5b66\u548c\u91cd\u5b9a\u5411\u5c06\u4eba\u7c7b\u52a8\u4f5c\u8f6c\u6362\u4e3a\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\u3002", "motivation": "\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\u53d7\u786c\u4ef6\u9650\u5236\uff0c\u96be\u4ee5\u89c4\u6a21\u5316\uff1b\u4eba\u7c7b\u89c6\u9891\u6570\u636e\u4e30\u5bcc\u4e14\u573a\u666f\u591a\u6837\uff0c\u53ef\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\u3002", "method": "\u8bad\u7ec3VLA\u6a21\u578b\u9884\u6d4b\u4eba\u7c7b\u624b\u8155\u548c\u624b\u90e8\u52a8\u4f5c\uff0c\u901a\u8fc7\u9006\u8fd0\u52a8\u5b66\u548c\u91cd\u5b9a\u5411\u8f6c\u6362\u4e3a\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u5e76\u7528\u5c11\u91cf\u673a\u5668\u4eba\u6f14\u793a\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u5728Isaac Humanoid Manipulation Benchmark\u4e0a\uff0cEgoVLA\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e86\u4eba\u7c7b\u6570\u636e\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u4eba\u7c7b\u89c6\u9891\u6570\u636e\u53ef\u7528\u4e8e\u9ad8\u6548\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\uff0c\u4e3a\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
