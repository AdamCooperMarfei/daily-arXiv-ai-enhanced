{"id": "2508.08258", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08258", "abs": "https://arxiv.org/abs/2508.08258", "authors": ["Gerald Brantner"], "title": "Humanoid Robot Acrobatics Utilizing Complete Articulated Rigid Body Dynamics", "comment": null, "summary": "Endowing humanoid robots with the ability to perform highly dynamic motions\nakin to human-level acrobatics has been a long-standing challenge. Successfully\nperforming these maneuvers requires close consideration of the underlying\nphysics in both trajectory optimization for planning and control during\nexecution. This is particularly challenging due to humanoids' high\ndegree-of-freedom count and associated exponentially scaling complexities,\nwhich makes planning on the explicit equations of motion intractable. Typical\nworkarounds include linearization methods and model approximations. However,\nneither are sufficient because they produce degraded performance on the true\nrobotic system. This paper presents a control architecture comprising\ntrajectory optimization and whole-body control, intermediated by a matching\nmodel abstraction, that enables the execution of acrobatic maneuvers, including\nconstraint and posture behaviors, conditioned on the unabbreviated equations of\nmotion of the articulated rigid body model. A review of underlying modeling and\ncontrol methods is given, followed by implementation details including model\nabstraction, trajectory optimization and whole-body controller. The system's\neffectiveness is analyzed in simulation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8f68\u8ff9\u4f18\u5316\u548c\u5168\u8eab\u63a7\u5236\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u578b\u62bd\u8c61\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u9ad8\u52a8\u6001\u52a8\u4f5c\u7684\u6267\u884c\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u6267\u884c\u9ad8\u52a8\u6001\u52a8\u4f5c\uff08\u5982\u6742\u6280\uff09\u7684\u6311\u6218\uff0c\u514b\u670d\u9ad8\u81ea\u7531\u5ea6\u5e26\u6765\u7684\u590d\u6742\u6027\u3002", "method": "\u91c7\u7528\u8f68\u8ff9\u4f18\u5316\u548c\u5168\u8eab\u63a7\u5236\uff0c\u7ed3\u5408\u6a21\u578b\u62bd\u8c61\uff0c\u57fa\u4e8e\u5b8c\u6574\u521a\u4f53\u8fd0\u52a8\u65b9\u7a0b\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u67b6\u6784\u80fd\u591f\u6210\u529f\u6267\u884c\u590d\u6742\u52a8\u4f5c\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.08259", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08259", "abs": "https://arxiv.org/abs/2508.08259", "authors": ["Chun-Ming Yang", "Pranav A. Bhounsule"], "title": "Koopman Operator Based Linear Model Predictive Control for Quadruped Trotting", "comment": null, "summary": "Online optimal control of quadruped robots would enable them to adapt to\nvarying inputs and changing conditions in real time. A common way of achieving\nthis is linear model predictive control (LMPC), where a quadratic programming\n(QP) problem is formulated over a finite horizon with a quadratic cost and\nlinear constraints obtained by linearizing the equations of motion and solved\non the fly. However, the model linearization may lead to model inaccuracies. In\nthis paper, we use the Koopman operator to create a linear model of the\nquadrupedal system in high dimensional space which preserves the nonlinearity\nof the equations of motion. Then using LMPC, we demonstrate high fidelity\ntracking and disturbance rejection on a quadrupedal robot. This is the first\nwork that uses the Koopman operator theory for LMPC of quadrupedal locomotion.", "AI": {"tldr": "\u4f7f\u7528Koopman\u7b97\u5b50\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u6784\u5efa\u9ad8\u7ef4\u7ebf\u6027\u6a21\u578b\uff0c\u7ed3\u5408LMPC\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8ddf\u8e2a\u548c\u5e72\u6270\u6291\u5236\u3002", "motivation": "\u5728\u7ebf\u4f18\u5316\u63a7\u5236\u4f7f\u56db\u8db3\u673a\u5668\u4eba\u80fd\u5b9e\u65f6\u9002\u5e94\u53d8\u5316\u8f93\u5165\u548c\u6761\u4ef6\uff0c\u4f46\u4f20\u7edfLMPC\u7684\u7ebf\u6027\u5316\u6a21\u578b\u53ef\u80fd\u5bfc\u81f4\u4e0d\u51c6\u786e\u3002", "method": "\u5229\u7528Koopman\u7b97\u5b50\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u6784\u5efa\u4fdd\u7559\u975e\u7ebf\u6027\u7279\u6027\u7684\u7ebf\u6027\u6a21\u578b\uff0c\u518d\u7ed3\u5408LMPC\u3002", "result": "\u5b9e\u73b0\u4e86\u56db\u8db3\u673a\u5668\u4eba\u7684\u9ad8\u7cbe\u5ea6\u8ddf\u8e2a\u548c\u5e72\u6270\u6291\u5236\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5c06Koopman\u7b97\u5b50\u7406\u8bba\u5e94\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4ebaLMPC\u7684\u7814\u7a76\u3002"}}
{"id": "2508.08264", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08264", "abs": "https://arxiv.org/abs/2508.08264", "authors": ["Hadush Hailu", "Bruk Gebregziabher", "Prudhvi Raj"], "title": "Forecast-Driven MPC for Decentralized Multi-Robot Collision Avoidance", "comment": "8 pages, 4 figures and 1 table", "summary": "The Iterative Forecast Planner (IFP) is a geometric planning approach that\noffers lightweight computations, scalable, and reactive solutions for\nmulti-robot path planning in decentralized, communication-free settings.\nHowever, it struggles in symmetric configurations, where mirrored interactions\noften lead to collisions and deadlocks. We introduce eIFP-MPC, an optimized and\nextended version of IFP that improves robustness and path consistency in dense,\ndynamic environments. The method refines threat prioritization using a\ntime-to-collision heuristic, stabilizes path generation through cost-based\nvia-point selection, and ensures dynamic feasibility by incorporating model\npredictive control (MPC) into the planning process. These enhancements are\ntightly integrated into the IFP to preserve its efficiency while improving its\nadaptability and stability. Extensive simulations across symmetric and\nhigh-density scenarios show that eIFP-MPC significantly reduces oscillations,\nensures collision-free motion, and improves trajectory efficiency. The results\ndemonstrate that geometric planners can be strengthened through optimization,\nenabling robust performance at scale in complex multi-agent environments.", "AI": {"tldr": "eIFP-MPC\u662fIFP\u7684\u4f18\u5316\u7248\u672c\uff0c\u901a\u8fc7\u6539\u8fdb\u5a01\u80c1\u4f18\u5148\u7ea7\u3001\u8def\u5f84\u751f\u6210\u548c\u52a8\u6001\u53ef\u884c\u6027\uff0c\u63d0\u5347\u4e86\u591a\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3IFP\u5728\u5bf9\u79f0\u914d\u7f6e\u4e2d\u56e0\u955c\u50cf\u4ea4\u4e92\u5bfc\u81f4\u7684\u78b0\u649e\u548c\u6b7b\u9501\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u65f6\u95f4\u78b0\u649e\u542f\u53d1\u5f0f\u4f18\u5316\u5a01\u80c1\u4f18\u5148\u7ea7\uff0c\u901a\u8fc7\u6210\u672c\u9009\u62e9\u7684\u8def\u5f84\u70b9\u7a33\u5b9a\u8def\u5f84\u751f\u6210\uff0c\u5e76\u96c6\u6210\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u786e\u4fdd\u52a8\u6001\u53ef\u884c\u6027\u3002", "result": "\u5728\u5bf9\u79f0\u548c\u9ad8\u5bc6\u5ea6\u573a\u666f\u4e2d\u663e\u8457\u51cf\u5c11\u632f\u8361\uff0c\u786e\u4fdd\u65e0\u78b0\u649e\u8fd0\u52a8\u5e76\u63d0\u9ad8\u8f68\u8ff9\u6548\u7387\u3002", "conclusion": "\u51e0\u4f55\u89c4\u5212\u5668\u901a\u8fc7\u4f18\u5316\u53ef\u5728\u590d\u6742\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u7684\u5927\u89c4\u6a21\u6027\u80fd\u3002"}}
{"id": "2508.08269", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08269", "abs": "https://arxiv.org/abs/2508.08269", "authors": ["Sagar Verma"], "title": "emg2tendon: From sEMG Signals to Tendon Control in Musculoskeletal Hands", "comment": "Accepted in Robotics: Science and Systems (RSS 2025)", "summary": "Tendon-driven robotic hands offer unparalleled dexterity for manipulation\ntasks, but learning control policies for such systems presents unique\nchallenges. Unlike joint-actuated robotic hands, tendon-driven systems lack a\ndirect one-to-one mapping between motion capture (mocap) data and tendon\ncontrols, making the learning process complex and expensive. Additionally,\nvisual tracking methods for real-world applications are prone to occlusions and\ninaccuracies, further complicating joint tracking. Wrist-wearable surface\nelectromyography (sEMG) sensors present an inexpensive, robust alternative to\ncapture hand motion. However, mapping sEMG signals to tendon control remains a\nsignificant challenge despite the availability of EMG-to-pose data sets and\nregression-based models in the existing literature.\n  We introduce the first large-scale EMG-to-Tendon Control dataset for robotic\nhands, extending the emg2pose dataset, which includes recordings from 193\nsubjects, spanning 370 hours and 29 stages with diverse gestures. This dataset\nincorporates tendon control signals derived using the MyoSuite MyoHand model,\naddressing limitations such as invalid poses in prior methods. We provide three\nbaseline regression models to demonstrate emg2tendon utility and propose a\nnovel diffusion-based regression model for predicting tendon control from sEMG\nrecordings. This dataset and modeling framework marks a significant step\nforward for tendon-driven dexterous robotic manipulation, laying the groundwork\nfor scalable and accurate tendon control in robotic hands.\nhttps://emg2tendon.github.io/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9996\u4e2a\u5927\u89c4\u6a21EMG-to-Tendon Control\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u808c\u8171\u9a71\u52a8\u673a\u68b0\u624b\u63a7\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u56de\u5f52\u6a21\u578b\u3002", "motivation": "\u808c\u8171\u9a71\u52a8\u673a\u68b0\u624b\u63a7\u5236\u590d\u6742\u4e14\u7f3a\u4e4f\u76f4\u63a5\u6620\u5c04\uff0c\u73b0\u6709\u89c6\u89c9\u8ddf\u8e2a\u65b9\u6cd5\u6613\u53d7\u906e\u6321\u548c\u4e0d\u51c6\u786e\u5f71\u54cd\uff0csEMG\u4fe1\u53f7\u867d\u5ec9\u4ef7\u4f46\u6620\u5c04\u56f0\u96be\u3002", "method": "\u6269\u5c55emg2pose\u6570\u636e\u96c6\uff0c\u5f15\u5165\u808c\u8171\u63a7\u5236\u4fe1\u53f7\uff0c\u63d0\u4f9b\u4e09\u79cd\u57fa\u7ebf\u56de\u5f52\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u7684\u56de\u5f52\u6a21\u578b\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b193\u540d\u53d7\u8bd5\u8005\u3001370\u5c0f\u65f6\u6570\u636e\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u65e0\u6548\u59ff\u52bf\u95ee\u9898\uff0c\u6a21\u578b\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e3a\u808c\u8171\u9a71\u52a8\u673a\u68b0\u624b\u7684\u7cbe\u786e\u63a7\u5236\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u53ef\u6269\u5c55\u63a7\u5236\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.08303", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08303", "abs": "https://arxiv.org/abs/2508.08303", "authors": ["Yasuyuki Fujii", "Dinh Tuan Tran", "Joo-Ho Lee"], "title": "Evaluation of an Autonomous Surface Robot Equipped with a Transformable Mobility Mechanism for Efficient Mobility Control", "comment": "5 pages, 6 figures. Presented at the ICRA 2025 Workshop on REaCT:\n  Robotics for Environmental and Climate Assessment", "summary": "Efficient mobility and power consumption are critical for autonomous water\nsurface robots in long-term water environmental monitoring. This study develops\nand evaluates a transformable mobility mechanism for a water surface robot with\ntwo control modes: station-keeping and traveling to improve energy efficiency\nand maneuverability. Field experiments show that, in a round-trip task between\ntwo points, the traveling mode reduces power consumption by 10\\% and decreases\nthe total time required for travel by 5\\% compared to the station-keeping mode.\nThese results confirm the effectiveness of the transformable mobility mechanism\nfor enhancing operational efficiency in patrolling on water surface.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u53d8\u5f62\u7684\u79fb\u52a8\u673a\u5236\uff0c\u7528\u4e8e\u6c34\u9762\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u4e24\u79cd\u63a7\u5236\u6a21\u5f0f\uff08\u5b9a\u70b9\u4fdd\u6301\u548c\u79fb\u52a8\u6a21\u5f0f\uff09\u63d0\u5347\u80fd\u6e90\u6548\u7387\u548c\u673a\u52a8\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u79fb\u52a8\u6a21\u5f0f\u6bd4\u5b9a\u70b9\u6a21\u5f0f\u8282\u80fd10%\uff0c\u5e76\u51cf\u5c115%\u7684\u79fb\u52a8\u65f6\u95f4\u3002", "motivation": "\u63d0\u9ad8\u6c34\u9762\u673a\u5668\u4eba\u5728\u957f\u671f\u73af\u5883\u76d1\u6d4b\u4e2d\u7684\u79fb\u52a8\u6548\u7387\u548c\u80fd\u6e90\u6548\u7387\u3002", "method": "\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u4e00\u79cd\u53ef\u53d8\u5f62\u7684\u79fb\u52a8\u673a\u5236\uff0c\u5305\u542b\u4e24\u79cd\u63a7\u5236\u6a21\u5f0f\uff1a\u5b9a\u70b9\u4fdd\u6301\u548c\u79fb\u52a8\u6a21\u5f0f\u3002", "result": "\u79fb\u52a8\u6a21\u5f0f\u6bd4\u5b9a\u70b9\u6a21\u5f0f\u8282\u80fd10%\uff0c\u5e76\u51cf\u5c115%\u7684\u79fb\u52a8\u65f6\u95f4\u3002", "conclusion": "\u53ef\u53d8\u5f62\u7684\u79fb\u52a8\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u6c34\u9762\u673a\u5668\u4eba\u7684\u64cd\u4f5c\u6548\u7387\u3002"}}
{"id": "2508.08328", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08328", "abs": "https://arxiv.org/abs/2508.08328", "authors": ["Qiwei Liang", "Boyang Cai", "Rongyi He", "Hui Li", "Tao Teng", "Haihan Duan", "Changxin Huang", "Runhao Zeng"], "title": "Whole-Body Coordination for Dynamic Object Grasping with Legged Manipulators", "comment": null, "summary": "Quadrupedal robots with manipulators offer strong mobility and adaptability\nfor grasping in unstructured, dynamic environments through coordinated\nwhole-body control. However, existing research has predominantly focused on\nstatic-object grasping, neglecting the challenges posed by dynamic targets and\nthus limiting applicability in dynamic scenarios such as logistics sorting and\nhuman-robot collaboration. To address this, we introduce DQ-Bench, a new\nbenchmark that systematically evaluates dynamic grasping across varying object\nmotions, velocities, heights, object types, and terrain complexities, along\nwith comprehensive evaluation metrics. Building upon this benchmark, we propose\nDQ-Net, a compact teacher-student framework designed to infer grasp\nconfigurations from limited perceptual cues. During training, the teacher\nnetwork leverages privileged information to holistically model both the static\ngeometric properties and dynamic motion characteristics of the target, and\nintegrates a grasp fusion module to deliver robust guidance for motion\nplanning. Concurrently, we design a lightweight student network that performs\ndual-viewpoint temporal modeling using only the target mask, depth map, and\nproprioceptive state, enabling closed-loop action outputs without reliance on\nprivileged data. Extensive experiments on DQ-Bench demonstrate that DQ-Net\nachieves robust dynamic objects grasping across multiple task settings,\nsubstantially outperforming baseline methods in both success rate and\nresponsiveness.", "AI": {"tldr": "DQ-Bench\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u52a8\u6001\u6293\u53d6\u4efb\u52a1\uff0cDQ-Net\u662f\u4e00\u4e2a\u7d27\u51d1\u7684\u5e08\u751f\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6709\u9650\u611f\u77e5\u7ebf\u7d22\u4e2d\u63a8\u65ad\u6293\u53d6\u914d\u7f6e\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u7269\u4f53\u6293\u53d6\uff0c\u5ffd\u89c6\u4e86\u52a8\u6001\u76ee\u6807\u7684\u6311\u6218\uff0c\u9650\u5236\u4e86\u5728\u52a8\u6001\u573a\u666f\uff08\u5982\u7269\u6d41\u5206\u62e3\u548c\u4eba\u673a\u534f\u4f5c\uff09\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faDQ-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u52a8\u6001\u6293\u53d6\u4efb\u52a1\uff1b\u8bbe\u8ba1DQ-Net\u5e08\u751f\u6846\u67b6\uff0c\u6559\u5e08\u7f51\u7edc\u5229\u7528\u7279\u6743\u4fe1\u606f\u5efa\u6a21\u76ee\u6807\u9759\u6001\u548c\u52a8\u6001\u7279\u6027\uff0c\u5b66\u751f\u7f51\u7edc\u4ec5\u4f7f\u7528\u76ee\u6807\u63a9\u7801\u3001\u6df1\u5ea6\u56fe\u548c\u672c\u4f53\u72b6\u6001\u8fdb\u884c\u53cc\u89c6\u89d2\u65f6\u95f4\u5efa\u6a21\u3002", "result": "DQ-Net\u5728\u591a\u4e2a\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u52a8\u6001\u7269\u4f53\u6293\u53d6\uff0c\u6210\u529f\u7387\u548c\u54cd\u5e94\u901f\u5ea6\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DQ-Bench\u548cDQ-Net\u4e3a\u89e3\u51b3\u52a8\u6001\u6293\u53d6\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6269\u5c55\u4e86\u56db\u8db3\u673a\u5668\u4eba\u64cd\u7eb5\u5668\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.08473", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.08473", "abs": "https://arxiv.org/abs/2508.08473", "authors": ["Hossein B. Jond"], "title": "A Minimal Model for Emergent Collective Behaviors in Autonomous Robotic Multi-Agent Systems", "comment": null, "summary": "Collective behaviors such as swarming and flocking emerge from simple,\ndecentralized interactions in biological systems. Existing models, such as\nVicsek and Cucker-Smale, lack collision avoidance, whereas the Olfati-Saber\nmodel imposes rigid formations, limiting their applicability in swarm robotics.\nTo address these limitations, this paper proposes a minimal yet expressive\nmodel that governs agent dynamics using relative positions, velocities, and\nlocal density, modulated by two tunable parameters: the spatial offset and\nkinetic offset. The model achieves spatially flexible, collision-free behaviors\nthat reflect naturalistic group dynamics. Furthermore, we extend the framework\nto cognitive autonomous systems, enabling energy-aware phase transitions\nbetween swarming and flocking through adaptive control parameter tuning. This\ncognitively inspired approach offers a robust foundation for real-world\napplications in multi-robot systems, particularly autonomous aerial swarms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7fa4\u4f53\u884c\u4e3a\u6a21\u578b\uff0c\u901a\u8fc7\u76f8\u5bf9\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u5c40\u90e8\u5bc6\u5ea6\u8c03\u63a7\u667a\u80fd\u4f53\u52a8\u6001\uff0c\u5b9e\u73b0\u7075\u6d3b\u3001\u65e0\u78b0\u649e\u7684\u7fa4\u4f53\u884c\u4e3a\uff0c\u5e76\u6269\u5c55\u81f3\u8ba4\u77e5\u81ea\u4e3b\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\uff08\u5982Vicsek\u3001Cucker-Smale\uff09\u7f3a\u4e4f\u78b0\u649e\u907f\u514d\uff0cOlfati-Saber\u6a21\u578b\u5219\u9650\u5236\u7fa4\u4f53\u5f62\u6001\uff0c\u9650\u5236\u4e86\u5176\u5728\u7fa4\u4f53\u673a\u5668\u4eba\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u4f7f\u7528\u76f8\u5bf9\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u5c40\u90e8\u5bc6\u5ea6\u8c03\u63a7\u667a\u80fd\u4f53\u52a8\u6001\uff0c\u5f15\u5165\u7a7a\u95f4\u504f\u79fb\u548c\u52a8\u529b\u5b66\u504f\u79fb\u4e24\u4e2a\u53ef\u8c03\u53c2\u6570\u3002", "result": "\u5b9e\u73b0\u4e86\u7a7a\u95f4\u7075\u6d3b\u3001\u65e0\u78b0\u649e\u7684\u7fa4\u4f53\u884c\u4e3a\uff0c\u5e76\u6269\u5c55\u81f3\u8ba4\u77e5\u81ea\u4e3b\u7cfb\u7edf\uff0c\u652f\u6301\u80fd\u91cf\u611f\u77e5\u7684\u7fa4\u4f53\u4e0e\u96c6\u7fa4\u884c\u4e3a\u5207\u6362\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff08\u5982\u81ea\u4e3b\u7a7a\u4e2d\u7fa4\u4f53\uff09\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u57fa\u7840\u3002"}}
{"id": "2508.08507", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.08507", "abs": "https://arxiv.org/abs/2508.08507", "authors": ["Shaun Macdonald", "Salma ElSayed", "Mark McGill"], "title": "AZRA: Extending the Affective Capabilities of Zoomorphic Robots using Augmented Reality", "comment": "Companion of the 2025 ACM/IEEE International Conference on\n  Human-Robot Interaction (RO-MAN 2025)", "summary": "Zoomorphic robots could serve as accessible and practical alternatives for\nusers unable or unwilling to keep pets. However, their affective interactions\nare often simplistic and short-lived, limiting their potential for domestic\nadoption. In order to facilitate more dynamic and nuanced affective\ninteractions and relationships between users and zoomorphic robots we present\nAZRA, a novel augmented reality (AR) framework that extends the affective\ncapabilities of these robots without physical modifications. To demonstrate\nAZRA, we augment a zoomorphic robot, Petit Qoobo, with novel emotional displays\n(face, light, sound, thought bubbles) and interaction modalities (voice, touch,\nproximity, gaze). Additionally, AZRA features a computational model of emotion\nto calculate the robot's emotional responses, daily moods, evolving personality\nand needs. We highlight how AZRA can be used for rapid participatory\nprototyping and enhancing existing robots, then discuss implications on future\nzoomorphic robot development.", "AI": {"tldr": "AZRA\u662f\u4e00\u4e2a\u589e\u5f3a\u73b0\u5b9e\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u5c55\u60c5\u611f\u4ea4\u4e92\u80fd\u529b\u63d0\u5347\u4eff\u751f\u673a\u5668\u4eba\u4e0e\u7528\u6237\u7684\u4e92\u52a8\uff0c\u65e0\u9700\u7269\u7406\u4fee\u6539\u3002", "motivation": "\u4eff\u751f\u673a\u5668\u4eba\u7684\u60c5\u611f\u4ea4\u4e92\u901a\u5e38\u7b80\u5355\u4e14\u77ed\u6682\uff0c\u9650\u5236\u4e86\u5176\u5bb6\u5ead\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u901a\u8fc7AR\u6280\u672f\u4e3a\u4eff\u751f\u673a\u5668\u4ebaPetit Qoobo\u6dfb\u52a0\u60c5\u611f\u663e\u793a\u548c\u4ea4\u4e92\u65b9\u5f0f\uff0c\u5e76\u7ed3\u5408\u60c5\u611f\u8ba1\u7b97\u6a21\u578b\u3002", "result": "AZRA\u5c55\u793a\u4e86\u52a8\u6001\u60c5\u611f\u4ea4\u4e92\u548c\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u7684\u80fd\u529b\u3002", "conclusion": "AZRA\u4e3a\u672a\u6765\u4eff\u751f\u673a\u5668\u4eba\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u548c\u6f5c\u529b\u3002"}}
{"id": "2508.08574", "categories": ["cs.RO", "cs.MA", "I.2.9; I.2.11"], "pdf": "https://arxiv.org/pdf/2508.08574", "abs": "https://arxiv.org/abs/2508.08574", "authors": ["Ameya Agaskar", "Sriram Siva", "William Pickering", "Kyle O'Brien", "Charles Kekeh", "Ang Li", "Brianna Gallo Sarker", "Alicia Chua", "Mayur Nemade", "Charun Thattai", "Jiaming Di", "Isaac Iyengar", "Ramya Dharoor", "Dino Kirouani", "Jimmy Erskine", "Tamir Hegazy", "Scott Niekum", "Usman A. Khan", "Federico Pecora", "Joseph W. Durham"], "title": "DeepFleet: Multi-Agent Foundation Models for Mobile Robots", "comment": "25 pages, 10 figures, 2 tables", "summary": "We introduce DeepFleet, a suite of foundation models designed to support\ncoordination and planning for large-scale mobile robot fleets. These models are\ntrained on fleet movement data, including robot positions, goals, and\ninteractions, from hundreds of thousands of robots in Amazon warehouses\nworldwide. DeepFleet consists of four architectures that each embody a distinct\ninductive bias and collectively explore key points in the design space for\nmulti-agent foundation models: the robot-centric (RC) model is an\nautoregressive decision transformer operating on neighborhoods of individual\nrobots; the robot-floor (RF) model uses a transformer with cross-attention\nbetween robots and the warehouse floor; the image-floor (IF) model applies\nconvolutional encoding to a multi-channel image representation of the full\nfleet; and the graph-floor (GF) model combines temporal attention with graph\nneural networks for spatial relationships. In this paper, we describe these\nmodels and present our evaluation of the impact of these design choices on\nprediction task performance. We find that the robot-centric and graph-floor\nmodels, which both use asynchronous robot state updates and incorporate the\nlocalized structure of robot interactions, show the most promise. We also\npresent experiments that show that these two models can make effective use of\nlarger warehouses operation datasets as the models are scaled up.", "AI": {"tldr": "DeepFleet\u662f\u4e00\u5957\u7528\u4e8e\u5927\u89c4\u6a21\u79fb\u52a8\u673a\u5668\u4eba\u8f66\u961f\u534f\u8c03\u4e0e\u89c4\u5212\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5305\u542b\u56db\u79cd\u67b6\u6784\uff0c\u5176\u4e2d\u673a\u5668\u4eba\u4e2d\u5fc3\uff08RC\uff09\u548c\u56fe-\u5730\u677f\uff08GF\uff09\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u673a\u5668\u4eba\u8f66\u961f\u534f\u8c03\u4e0e\u89c4\u5212\u7684\u6311\u6218\uff0c\u5229\u7528\u4e9a\u9a6c\u900a\u4ed3\u5e93\u7684\u673a\u5668\u4eba\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u3002", "method": "\u8bbe\u8ba1\u4e86\u56db\u79cd\u6a21\u578b\u67b6\u6784\uff08RC\u3001RF\u3001IF\u3001GF\uff09\uff0c\u5206\u522b\u91c7\u7528\u4e0d\u540c\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u8bc4\u4f30\u5176\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "RC\u548cGF\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u80fd\u6709\u6548\u5229\u7528\u66f4\u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6\u3002", "conclusion": "RC\u548cGF\u6a21\u578b\u56e0\u5176\u5f02\u6b65\u72b6\u6001\u66f4\u65b0\u548c\u5c40\u90e8\u4ea4\u4e92\u7ed3\u6784\uff0c\u5728\u5927\u89c4\u6a21\u673a\u5668\u4eba\u8f66\u961f\u89c4\u5212\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2508.08576", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08576", "abs": "https://arxiv.org/abs/2508.08576", "authors": ["Deniz Karanfil", "Daniel Lindmark", "Martin Servin", "David Torick", "Bahram Ravani"], "title": "Developing a Calibrated Physics-Based Digital Twin for Construction Vehicles", "comment": null, "summary": "This paper presents the development of a calibrated digital twin of a wheel\nloader. A calibrated digital twin integrates a construction vehicle with a\nhigh-fidelity digital model allowing for automated diagnostics and optimization\nof operations as well as pre-planning simulations enhancing automation\ncapabilities. The high-fidelity digital model is a virtual twin of the physical\nwheel loader. It uses a physics-based multibody dynamic model of the wheel\nloader in the software AGX Dynamics. Interactions of the wheel loader's bucket\nwhile in use in construction can be simulated in the virtual model. Calibration\nmakes this simulation of high-fidelity which can enhance realistic planning for\nautomation of construction operations. In this work, a wheel loader was\ninstrumented with several sensors used to calibrate the digital model. The\ncalibrated digital twin was able to estimate the magnitude of the forces on the\nbucket base with high accuracy, providing a high-fidelity simulation.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u6821\u51c6\u7684\u6570\u5b57\u5b6a\u751f\u8f6e\u5f0f\u88c5\u8f7d\u673a\uff0c\u901a\u8fc7\u9ad8\u4fdd\u771f\u6570\u5b57\u6a21\u578b\u5b9e\u73b0\u81ea\u52a8\u5316\u8bca\u65ad\u3001\u64cd\u4f5c\u4f18\u5316\u548c\u9884\u89c4\u5212\u6a21\u62df\u3002", "motivation": "\u63d0\u5347\u5efa\u7b51\u64cd\u4f5c\u7684\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6a21\u62df\u548c\u4f18\u5316\u3002", "method": "\u4f7f\u7528AGX Dynamics\u8f6f\u4ef6\u4e2d\u7684\u591a\u4f53\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u7ed3\u5408\u4f20\u611f\u5668\u6821\u51c6\u6570\u5b57\u6a21\u578b\u3002", "result": "\u6821\u51c6\u540e\u7684\u6570\u5b57\u5b6a\u751f\u80fd\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\u94f2\u6597\u53d7\u529b\uff0c\u63d0\u4f9b\u9ad8\u4fdd\u771f\u6a21\u62df\u3002", "conclusion": "\u6821\u51c6\u6570\u5b57\u5b6a\u751f\u6280\u672f\u53ef\u6709\u6548\u63d0\u5347\u5efa\u7b51\u64cd\u4f5c\u7684\u81ea\u52a8\u5316\u89c4\u5212\u548c\u4f18\u5316\u80fd\u529b\u3002"}}
{"id": "2508.08607", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08607", "abs": "https://arxiv.org/abs/2508.08607", "authors": ["Justin London"], "title": "Autonomous Mobile Plant Watering Robot : A Kinematic Approach", "comment": null, "summary": "Plants need regular and the appropriate amount of watering to thrive and\nsurvive. While agricultural robots exist that can spray water on plants and\ncrops such as the , they are expensive and have limited mobility and/or\nfunctionality. We introduce a novel autonomous mobile plant watering robot that\nuses a 6 degree of freedom (DOF) manipulator, connected to a 4 wheel drive\nalloy chassis, to be able to hold a garden hose, recognize and detect plants,\nand to water them with the appropriate amount of water by being able to insert\na soil humidity/moisture sensor into the soil. The robot uses Jetson Nano and\nArduino microcontroller and real sense camera to perform computer vision to\ndetect plants using real-time YOLOv5 with the Pl@ntNet-300K dataset. The robot\nuses LIDAR for object and collision avoideance and does not need to move on a\npre-defined path and can keep track of which plants it has watered. We provide\nthe Denavit-Hartenberg (DH) Table, forward kinematics, differential driving\nkinematics, and inverse kinematics along with simulation and experiment results", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u578b\u81ea\u4e3b\u79fb\u52a8\u690d\u7269\u6d47\u6c34\u673a\u5668\u4eba\uff0c\u914d\u59076\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u548c4\u8f6e\u9a71\u52a8\u5e95\u76d8\uff0c\u7ed3\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u4f20\u611f\u5668\u6280\u672f\uff0c\u5b9e\u73b0\u667a\u80fd\u6d47\u6c34\u3002", "motivation": "\u73b0\u6709\u519c\u4e1a\u673a\u5668\u4eba\u6210\u672c\u9ad8\u4e14\u529f\u80fd\u6709\u9650\uff0c\u65e0\u6cd5\u6ee1\u8db3\u690d\u7269\u6d47\u6c34\u7684\u7075\u6d3b\u9700\u6c42\u3002", "method": "\u673a\u5668\u4eba\u4f7f\u7528Jetson Nano\u548cArduino\u5fae\u63a7\u5236\u5668\uff0c\u7ed3\u5408YOLOv5\u548cPl@ntNet-300K\u6570\u636e\u96c6\u8fdb\u884c\u690d\u7269\u8bc6\u522b\uff0c\u5229\u7528LIDAR\u907f\u969c\uff0c\u5e76\u901a\u8fc7\u673a\u68b0\u81c2\u548c\u571f\u58e4\u6e7f\u5ea6\u4f20\u611f\u5668\u5b9e\u73b0\u7cbe\u51c6\u6d47\u6c34\u3002", "result": "\u63d0\u4f9b\u4e86DH\u8868\u3001\u8fd0\u52a8\u5b66\u6a21\u578b\u53ca\u4eff\u771f\u4e0e\u5b9e\u9a8c\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u673a\u5668\u4eba\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u673a\u5668\u4eba\u5177\u6709\u4f4e\u6210\u672c\u3001\u9ad8\u7075\u6d3b\u6027\u548c\u667a\u80fd\u5316\u7684\u7279\u70b9\uff0c\u9002\u7528\u4e8e\u690d\u7269\u517b\u62a4\u3002"}}
{"id": "2508.08624", "categories": ["cs.RO", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.08624", "abs": "https://arxiv.org/abs/2508.08624", "authors": ["Chenxuan Liu", "He Li", "Zongze Li", "Shuai Wang", "Wei Xu", "Kejiang Ye", "Derrick Wing Kwan Ng", "Chengzhong Xu"], "title": "Communication Efficient Robotic Mixed Reality with Gaussian Splatting Cross-Layer Optimization", "comment": "14 pages, 18 figures, to appear in IEEE Transactions on Cognitive\n  Communications and Networking", "summary": "Realizing low-cost communication in robotic mixed reality (RoboMR) systems\npresents a challenge, due to the necessity of uploading high-resolution images\nthrough wireless channels. This paper proposes Gaussian splatting (GS) RoboMR\n(GSMR), which enables the simulator to opportunistically render a\nphoto-realistic view from the robot's pose by calling ``memory'' from a GS\nmodel, thus reducing the need for excessive image uploads. However, the GS\nmodel may involve discrepancies compared to the actual environments. To this\nend, a GS cross-layer optimization (GSCLO) framework is further proposed, which\njointly optimizes content switching (i.e., deciding whether to upload image or\nnot) and power allocation (i.e., adjusting to content profiles) across\ndifferent frames by minimizing a newly derived GSMR loss function. The GSCLO\nproblem is addressed by an accelerated penalty optimization (APO) algorithm\nthat reduces computational complexity by over $10$x compared to traditional\nbranch-and-bound and search algorithms. Moreover, variants of GSCLO are\npresented to achieve robust, low-power, and multi-robot GSMR. Extensive\nexperiments demonstrate that the proposed GSMR paradigm and GSCLO method\nachieve significant improvements over existing benchmarks on both wheeled and\nlegged robots in terms of diverse metrics in various scenarios. For the first\ntime, it is found that RoboMR can be achieved with ultra-low communication\ncosts, and mixture of data is useful for enhancing GS performance in dynamic\nscenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGSMR\u548cGSCLO\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u548c\u8de8\u5c42\u4f18\u5316\u964d\u4f4e\u673a\u5668\u4eba\u6df7\u5408\u73b0\u5b9e\u7cfb\u7edf\u7684\u901a\u4fe1\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u6df7\u5408\u73b0\u5b9e\u7cfb\u7edf\u4e2d\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u4f20\u5bfc\u81f4\u7684\u901a\u4fe1\u6210\u672c\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u6cfc\u6e85\uff08GS\uff09\u6280\u672f\u6e32\u67d3\u903c\u771f\u89c6\u56fe\uff0c\u7ed3\u5408GSCLO\u6846\u67b6\u4f18\u5316\u5185\u5bb9\u5207\u6362\u548c\u529f\u7387\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGSMR\u548cGSCLO\u5728\u591a\u79cd\u573a\u666f\u548c\u673a\u5668\u4eba\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u901a\u4fe1\u6210\u672c\u6781\u4f4e\u3002", "conclusion": "\u9996\u6b21\u8bc1\u660e\u673a\u5668\u4eba\u6df7\u5408\u73b0\u5b9e\u53ef\u5b9e\u73b0\u8d85\u4f4e\u901a\u4fe1\u6210\u672c\uff0c\u52a8\u6001\u573a\u666f\u4e2d\u6df7\u5408\u6570\u636e\u6709\u52a9\u4e8e\u63d0\u5347GS\u6027\u80fd\u3002"}}
{"id": "2508.08690", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08690", "abs": "https://arxiv.org/abs/2508.08690", "authors": ["Zhenjiang Wang", "Yunhua Jiang", "Zikun Zhen", "Yifan Jiang", "Yubin Tan", "Wubin Wang"], "title": "ZS-Puffin: Design, Modeling and Implementation of an Unmanned Aerial-Aquatic Vehicle with Amphibious Wings", "comment": "Accepted to IROS 2025", "summary": "Unmanned aerial-aquatic vehicles (UAAVs) can operate both in the air and\nunderwater, giving them broad application prospects. Inspired by the\ndual-function wings of puffins, we propose a UAAV with amphibious wings to\naddress the challenge posed by medium differences on the vehicle's propulsion\nsystem. The amphibious wing, redesigned based on a fixed-wing structure,\nfeatures a single degree of freedom in pitch and requires no additional\ncomponents. It can generate lift in the air and function as a flapping wing for\npropulsion underwater, reducing disturbance to marine life and making it\nenvironmentally friendly. Additionally, an artificial central pattern generator\n(CPG) is introduced to enhance the smoothness of the flapping motion. This\npaper presents the prototype, design details, and practical implementation of\nthis concept.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6d77\u96c0\u7fc5\u8180\u542f\u53d1\u7684\u4e24\u6816\u7ffc\u65e0\u4eba\u7a7a\u4e2d-\u6c34\u4e0b\u98de\u884c\u5668\uff08UAAV\uff09\uff0c\u901a\u8fc7\u5355\u81ea\u7531\u5ea6\u4fef\u4ef0\u8bbe\u8ba1\u5b9e\u73b0\u7a7a\u4e2d\u548c\u6c34\u4e0b\u63a8\u8fdb\uff0c\u51cf\u5c11\u5bf9\u6d77\u6d0b\u751f\u7269\u7684\u5e72\u6270\u3002", "motivation": "\u89e3\u51b3\u4ecb\u8d28\u5dee\u5f02\u5bf9\u98de\u884c\u5668\u63a8\u8fdb\u7cfb\u7edf\u7684\u6311\u6218\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u6d77\u6d0b\u73af\u5883\u7684\u5f71\u54cd\u3002", "method": "\u91cd\u65b0\u8bbe\u8ba1\u56fa\u5b9a\u7ffc\u7ed3\u6784\u4e3a\u4e24\u6816\u7ffc\uff0c\u5f15\u5165\u4eba\u5de5\u4e2d\u592e\u6a21\u5f0f\u751f\u6210\u5668\uff08CPG\uff09\u4f18\u5316\u6251\u7ffc\u8fd0\u52a8\u3002", "result": "\u6210\u529f\u5f00\u53d1\u51fa\u539f\u578b\u673a\uff0c\u9a8c\u8bc1\u4e86\u4e24\u6816\u7ffc\u5728\u7a7a\u4e2d\u548c\u6c34\u4e0b\u7684\u529f\u80fd\u3002", "conclusion": "\u4e24\u6816\u7ffc\u8bbe\u8ba1\u4e3aUAAV\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u73af\u4fdd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.08706", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08706", "abs": "https://arxiv.org/abs/2508.08706", "authors": ["Zhengxue Cheng", "Yiqian Zhang", "Wenkang Zhang", "Haoyu Li", "Keyu Wang", "Li Song", "Hengdi Zhang"], "title": "OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing", "comment": "15 pages, 7 figures, 8 tables", "summary": "Recent vision-language-action (VLA) models build upon vision-language\nfoundations, and have achieved promising results and exhibit the possibility of\ntask generalization in robot manipulation. However, due to the heterogeneity of\ntactile sensors and the difficulty of acquiring tactile data, current VLA\nmodels significantly overlook the importance of tactile perception and fail in\ncontact-rich tasks. To address this issue, this paper proposes OmniVTLA, a\nnovel architecture involving tactile sensing. Specifically, our contributions\nare threefold. First, our OmniVTLA features a dual-path tactile encoder\nframework. This framework enhances tactile perception across diverse\nvision-based and force-based tactile sensors by using a pretrained vision\ntransformer (ViT) and a semantically-aligned tactile ViT (SA-ViT). Second, we\nintroduce ObjTac, a comprehensive force-based tactile dataset capturing\ntextual, visual, and tactile information for 56 objects across 10 categories.\nWith 135K tri-modal samples, ObjTac supplements existing visuo-tactile\ndatasets. Third, leveraging this dataset, we train a semantically-aligned\ntactile encoder to learn a unified tactile representation, serving as a better\ninitialization for OmniVTLA. Real-world experiments demonstrate substantial\nimprovements over state-of-the-art VLA baselines, achieving 96.9% success rates\nwith grippers, (21.9% higher over baseline) and 100% success rates with\ndexterous hands (6.2% higher over baseline) in pick-and-place tasks. Besides,\nOmniVTLA significantly reduces task completion time and generates smoother\ntrajectories through tactile sensing compared to existing VLA.", "AI": {"tldr": "OmniVTLA\u662f\u4e00\u79cd\u7ed3\u5408\u89e6\u89c9\u611f\u77e5\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u89e6\u89c9\u7f16\u7801\u5668\u548c\u65b0\u6570\u636e\u96c6ObjTac\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u56e0\u89e6\u89c9\u4f20\u611f\u5668\u5f02\u6784\u6027\u548c\u6570\u636e\u83b7\u53d6\u56f0\u96be\uff0c\u5ffd\u89c6\u4e86\u89e6\u89c9\u611f\u77e5\u7684\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u5728\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u53cc\u8def\u5f84\u89e6\u89c9\u7f16\u7801\u5668\u6846\u67b6\uff08ViT\u548cSA-ViT\uff09\uff0c\u5e76\u5f15\u5165\u5305\u542b13.5\u4e07\u6837\u672c\u7684\u89e6\u89c9\u6570\u636e\u96c6ObjTac\uff0c\u8bad\u7ec3\u8bed\u4e49\u5bf9\u9f50\u7684\u89e6\u89c9\u7f16\u7801\u5668\u3002", "result": "\u5728\u771f\u5b9e\u5b9e\u9a8c\u4e2d\uff0cOmniVTLA\u5728\u6293\u53d6\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u63d0\u534721.9%\uff08\u5939\u722a\uff09\u548c6.2%\uff08\u7075\u5de7\u624b\uff09\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u7f29\u77ed\uff0c\u8f68\u8ff9\u66f4\u5e73\u6ed1\u3002", "conclusion": "OmniVTLA\u901a\u8fc7\u89e6\u89c9\u611f\u77e5\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2508.08707", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08707", "abs": "https://arxiv.org/abs/2508.08707", "authors": ["Haoran Ding", "Anqing Duan", "Zezhou Sun", "Leonel Rozo", "No\u00e9mie Jaquier", "Dezhen Song", "Yoshihiko Nakamura"], "title": "Towards Safe Imitation Learning via Potential Field-Guided Flow Matching", "comment": "8 pages, 6 figures, Accepted to IROS 2025", "summary": "Deep generative models, particularly diffusion and flow matching models, have\nrecently shown remarkable potential in learning complex policies through\nimitation learning. However, the safety of generated motions remains\noverlooked, particularly in complex environments with inherent obstacles. In\nthis work, we address this critical gap by proposing Potential Field-Guided\nFlow Matching Policy (PF2MP), a novel approach that simultaneously learns task\npolicies and extracts obstacle-related information, represented as a potential\nfield, from the same set of successful demonstrations. During inference, PF2MP\nmodulates the flow matching vector field via the learned potential field,\nenabling safe motion generation. By leveraging these complementary fields, our\napproach achieves improved safety without compromising task success across\ndiverse environments, such as navigation tasks and robotic manipulation\nscenarios. We evaluate PF2MP in both simulation and real-world settings,\ndemonstrating its effectiveness in task space and joint space control.\nExperimental results demonstrate that PF2MP enhances safety, achieving a\nsignificant reduction of collisions compared to baseline policies. This work\npaves the way for safer motion generation in unstructured and obstaclerich\nenvironments.", "AI": {"tldr": "PF2MP\u662f\u4e00\u79cd\u7ed3\u5408\u52bf\u573a\u5f15\u5bfc\u548c\u6d41\u5339\u914d\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u5b89\u5168\u8fd0\u52a8\u7b56\u7565\uff0c\u663e\u8457\u51cf\u5c11\u78b0\u649e\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\u751f\u6210\u7684\u8fd0\u52a8\u7b56\u7565\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b89\u5168\u6027\u4e0d\u8db3\uff0cPF2MP\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "PF2MP\u901a\u8fc7\u540c\u4e00\u7ec4\u6210\u529f\u6f14\u793a\u540c\u65f6\u5b66\u4e60\u4efb\u52a1\u7b56\u7565\u548c\u969c\u788d\u7269\u52bf\u573a\uff0c\u63a8\u7406\u65f6\u7528\u52bf\u573a\u8c03\u6574\u6d41\u5339\u914d\u5411\u91cf\u573a\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePF2MP\u5728\u4eff\u771f\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u5747\u80fd\u63d0\u9ad8\u5b89\u5168\u6027\uff0c\u663e\u8457\u51cf\u5c11\u78b0\u649e\u3002", "conclusion": "PF2MP\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b89\u5168\u8fd0\u52a8\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.08709", "categories": ["cs.RO", "cs.AR", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.08709", "abs": "https://arxiv.org/abs/2508.08709", "authors": ["Lukas Krupp", "Maximilian Sch\u00f6ffel", "Elias Biehl", "Norbert Wehn"], "title": "CRADLE: Conversational RTL Design Space Exploration with LLM-based Multi-Agent Systems", "comment": "Accepted for presentation at the 22nd International SoC Conference\n  (ISOCC 2025). Proceedings to be included in IEEE Xplore", "summary": "This paper presents CRADLE, a conversational framework for design space\nexploration of RTL designs using LLM-based multi-agent systems. Unlike existing\nrigid approaches, CRADLE enables user-guided flows with internal\nself-verification, correction, and optimization. We demonstrate the framework\nwith a generator-critic agent system targeting FPGA resource minimization using\nstate-of-the-art LLMs. Experimental results on the RTLLM benchmark show that\nCRADLE achieves significant reductions in resource usage with averages of 48%\nand 40% in LUTs and FFs across all benchmark designs.", "AI": {"tldr": "CRADLE\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u6846\u67b6\uff0c\u7528\u4e8eRTL\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\uff0c\u652f\u6301\u7528\u6237\u5f15\u5bfc\u3001\u81ea\u9a8c\u8bc1\u548c\u4f18\u5316\uff0c\u663e\u8457\u51cf\u5c11FPGA\u8d44\u6e90\u4f7f\u7528\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8fc7\u4e8e\u50f5\u5316\uff0cCRADLE\u65e8\u5728\u63d0\u4f9b\u66f4\u7075\u6d3b\u3001\u7528\u6237\u53cb\u597d\u7684\u8bbe\u8ba1\u63a2\u7d22\u6d41\u7a0b\u3002", "method": "\u91c7\u7528\u751f\u6210\u5668-\u6279\u8bc4\u5bb6\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7ed3\u5408\u5148\u8fdbLLM\u6280\u672f\uff0c\u4e13\u6ce8\u4e8eFPGA\u8d44\u6e90\u6700\u5c0f\u5316\u3002", "result": "\u5728RTLLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLUT\u548cFF\u8d44\u6e90\u4f7f\u7528\u5e73\u5747\u51cf\u5c1148%\u548c40%\u3002", "conclusion": "CRADLE\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548cLLM\u6280\u672f\uff0c\u6709\u6548\u4f18\u5316\u4e86RTL\u8bbe\u8ba1\u8d44\u6e90\u4f7f\u7528\u3002"}}
{"id": "2508.08743", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08743", "abs": "https://arxiv.org/abs/2508.08743", "authors": ["Haoyu Zhang", "Long Cheng"], "title": "Boosting Action-Information via a Variational Bottleneck on Unlabelled Robot Videos", "comment": null, "summary": "Learning from demonstrations (LfD) typically relies on large amounts of\naction-labeled expert trajectories, which fundamentally constrains the scale of\navailable training data. A promising alternative is to learn directly from\nunlabeled video demonstrations. However, we find that existing methods tend to\nencode latent actions that share little mutual information with the true robot\nactions, leading to suboptimal control performance. To address this limitation,\nwe introduce a novel framework that explicitly maximizes the mutual information\nbetween latent actions and true actions, even in the absence of action labels.\nOur method leverage the variational information-bottleneck to extract\naction-relevant representations while discarding task-irrelevant information.\nWe provide a theoretical analysis showing that our objective indeed maximizes\nthe mutual information between latent and true actions. Finally, we validate\nour approach through extensive experiments: first in simulated robotic\nenvironments and then on real-world robotic platforms, the experimental results\ndemonstrate that our method significantly enhances mutual information and\nconsistently improves policy performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5316\u6f5c\u5728\u52a8\u4f5c\u4e0e\u771f\u5b9e\u52a8\u4f5c\u7684\u4e92\u4fe1\u606f\uff0c\u76f4\u63a5\u4ece\u65e0\u6807\u7b7e\u89c6\u9891\u6f14\u793a\u4e2d\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u7b56\u7565\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6f14\u793a\u7684\u5b66\u4e60\uff08LfD\uff09\u4f9d\u8d56\u5927\u91cf\u5e26\u6807\u7b7e\u7684\u4e13\u5bb6\u8f68\u8ff9\uff0c\u9650\u5236\u4e86\u8bad\u7ec3\u6570\u636e\u7684\u89c4\u6a21\u3002\u76f4\u63a5\u4ece\u65e0\u6807\u7b7e\u89c6\u9891\u5b66\u4e60\u662f\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u63d0\u53d6\u7684\u6f5c\u5728\u52a8\u4f5c\u4e0e\u771f\u5b9e\u52a8\u4f5c\u4e92\u4fe1\u606f\u4f4e\uff0c\u5bfc\u81f4\u63a7\u5236\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5229\u7528\u53d8\u5206\u4fe1\u606f\u74f6\u9888\u6280\u672f\uff0c\u663e\u5f0f\u6700\u5927\u5316\u6f5c\u5728\u52a8\u4f5c\u4e0e\u771f\u5b9e\u52a8\u4f5c\u7684\u4e92\u4fe1\u606f\uff0c\u5373\u4f7f\u6ca1\u6709\u52a8\u4f5c\u6807\u7b7e\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u4e92\u4fe1\u606f\u548c\u7b56\u7565\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u6807\u7b7e\u89c6\u9891\u5b66\u4e60\u4e2d\u6f5c\u5728\u52a8\u4f5c\u4e0e\u771f\u5b9e\u52a8\u4f5c\u4e92\u4fe1\u606f\u4f4e\u7684\u95ee\u9898\uff0c\u4e3aLfD\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u6570\u636e\u5229\u7528\u65b9\u5f0f\u3002"}}
{"id": "2508.08748", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08748", "abs": "https://arxiv.org/abs/2508.08748", "authors": ["Muhammad A. Muttaqien", "Tomohiro Motoda", "Ryo Hanai", "Yukiyasu Domae"], "title": "Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT", "comment": null, "summary": "Robotic pick-and-place tasks in convenience stores pose challenges due to\ndense object arrangements, occlusions, and variations in object properties such\nas color, shape, size, and texture. These factors complicate trajectory\nplanning and grasping. This paper introduces a perception-action pipeline\nleveraging annotation-guided visual prompting, where bounding box annotations\nidentify both pickable objects and placement locations, providing structured\nspatial guidance. Instead of traditional step-by-step planning, we employ\nAction Chunking with Transformers (ACT) as an imitation learning algorithm,\nenabling the robotic arm to predict chunked action sequences from human\ndemonstrations. This facilitates smooth, adaptive, and data-driven\npick-and-place operations. We evaluate our system based on success rate and\nvisual analysis of grasping behavior, demonstrating improved grasp accuracy and\nadaptability in retail environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6807\u6ce8\u5f15\u5bfc\u89c6\u89c9\u63d0\u793a\u548c\u52a8\u4f5c\u5206\u5757\u6a21\u4eff\u5b66\u4e60\u7684\u673a\u5668\u4eba\u6293\u53d6-\u653e\u7f6e\u7cfb\u7edf\uff0c\u63d0\u5347\u4e86\u96f6\u552e\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u4fbf\u5229\u5e97\u4e2d\u7684\u673a\u5668\u4eba\u6293\u53d6-\u653e\u7f6e\u4efb\u52a1\u56e0\u7269\u4f53\u5bc6\u96c6\u3001\u906e\u6321\u548c\u5c5e\u6027\u591a\u6837\u800c\u590d\u6742\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002", "method": "\u91c7\u7528\u6807\u6ce8\u5f15\u5bfc\u89c6\u89c9\u63d0\u793a\u63d0\u4f9b\u7a7a\u95f4\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u52a8\u4f5c\u5206\u5757\u6a21\u4eff\u5b66\u4e60\uff08ACT\uff09\u7b97\u6cd5\u9884\u6d4b\u8fde\u7eed\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u7cfb\u7edf\u5728\u6293\u53d6\u6210\u529f\u7387\u548c\u9002\u5e94\u6027\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u96f6\u552e\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.08767", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.08767", "abs": "https://arxiv.org/abs/2508.08767", "authors": ["Kazuki Komura", "Kumi Ozaki", "Seiji Yamada"], "title": "Robot can reduce superior's dominance in group discussions with human social hierarchy", "comment": "8 pages, 7 figures. International Conference on Human-Agent\n  Interaction (HAI '24), November 24-27, 2024, Swansea, United Kingdom", "summary": "This study investigated whether robotic agents that deal with social\nhierarchical relationships can reduce the dominance of superiors and equalize\nparticipation among participants in discussions with hierarchical structures.\nThirty doctors and students having hierarchical relationship were gathered as\nparticipants, and an intervention experiment was conducted using a robot that\ncan encourage participants to speak depending on social hierarchy. These were\ncompared with strategies that intervened equally for all participants without\nconsidering hierarchy and with a no-action. The robots performed follow\nactions, showing backchanneling to speech, and encourage actions, prompting\nspeech from members with less speaking time, on the basis of the hierarchical\nrelationships among group members to equalize participation. The experimental\nresults revealed that the robot's actions could potentially influence the\nspeaking time among members, but it could not be conclusively stated that there\nwere significant differences between the robot's action conditions. However,\nthe results suggested that it might be possible to influence speaking time\nwithout decreasing the satisfaction of superiors. This indicates that in\ndiscussion scenarios where experienced superiors are likely to dominate,\ncontrolling the robot's backchanneling behavior could potentially suppress\ndominance and equalize participation among group members.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u673a\u5668\u4eba\u4ee3\u7406\u662f\u5426\u80fd\u901a\u8fc7\u793e\u4ea4\u5c42\u7ea7\u5173\u7cfb\u51cf\u5c11\u4e0a\u7ea7\u7684\u652f\u914d\u6027\u5e76\u5747\u8861\u8ba8\u8bba\u53c2\u4e0e\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u673a\u5668\u4eba\u884c\u4e3a\u53ef\u80fd\u5f71\u54cd\u53d1\u8a00\u65f6\u95f4\uff0c\u4f46\u672a\u663e\u8457\u5dee\u5f02\u3002\u673a\u5668\u4eba\u884c\u4e3a\u53ef\u80fd\u5728\u4e0d\u964d\u4f4e\u4e0a\u7ea7\u6ee1\u610f\u5ea6\u7684\u60c5\u51b5\u4e0b\u5747\u8861\u53c2\u4e0e\u3002", "motivation": "\u5728\u5c42\u7ea7\u7ed3\u6784\u7684\u8ba8\u8bba\u4e2d\uff0c\u4e0a\u7ea7\u53ef\u80fd\u4e3b\u5bfc\u53d1\u8a00\uff0c\u5bfc\u81f4\u53c2\u4e0e\u4e0d\u5747\u8861\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u673a\u5668\u4eba\u662f\u5426\u80fd\u901a\u8fc7\u793e\u4ea4\u5c42\u7ea7\u5e72\u9884\u51cf\u5c11\u8fd9\u79cd\u4e0d\u5e73\u7b49\u3002", "method": "\u62db\u52df30\u540d\u533b\u751f\u548c\u5b66\u751f\u8fdb\u884c\u5b9e\u9a8c\uff0c\u673a\u5668\u4eba\u6839\u636e\u5c42\u7ea7\u5173\u7cfb\u9f13\u52b1\u53d1\u8a00\uff0c\u5e76\u4e0e\u65e0\u5e72\u9884\u548c\u5747\u7b49\u5e72\u9884\u7b56\u7565\u5bf9\u6bd4\u3002", "result": "\u673a\u5668\u4eba\u884c\u4e3a\u53ef\u80fd\u5f71\u54cd\u53d1\u8a00\u65f6\u95f4\uff0c\u4f46\u672a\u663e\u8457\u5dee\u5f02\u3002\u673a\u5668\u4eba\u884c\u4e3a\u53ef\u80fd\u5728\u4e0d\u964d\u4f4e\u4e0a\u7ea7\u6ee1\u610f\u5ea6\u7684\u60c5\u51b5\u4e0b\u5747\u8861\u53c2\u4e0e\u3002", "conclusion": "\u5728\u5c42\u7ea7\u8ba8\u8bba\u4e2d\uff0c\u673a\u5668\u4eba\u901a\u8fc7\u63a7\u5236\u53cd\u9988\u884c\u4e3a\u53ef\u80fd\u6291\u5236\u4e0a\u7ea7\u652f\u914d\u6027\u5e76\u5747\u8861\u53c2\u4e0e\u3002"}}
{"id": "2508.08896", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08896", "abs": "https://arxiv.org/abs/2508.08896", "authors": ["Haoyu Zhao", "Linghao Zhuang", "Xingyue Zhao", "Cheng Zeng", "Haoran Xu", "Yuming Jiang", "Jun Cen", "Kexiang Wang", "Jiayan Guo", "Siteng Huang", "Xin Li", "Deli Zhao", "Hua Zou"], "title": "Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors", "comment": "13 pages, 8 figures", "summary": "A dexterous hand capable of generalizable grasping objects is fundamental for\nthe development of general-purpose embodied AI. However, previous methods focus\nnarrowly on low-level grasp stability metrics, neglecting affordance-aware\npositioning and human-like poses which are crucial for downstream manipulation.\nTo address these limitations, we propose AffordDex, a novel framework with\ntwo-stage training that learns a universal grasping policy with an inherent\nunderstanding of both motion priors and object affordances. In the first stage,\na trajectory imitator is pre-trained on a large corpus of human hand motions to\ninstill a strong prior for natural movement. In the second stage, a residual\nmodule is trained to adapt these general human-like motions to specific object\ninstances. This refinement is critically guided by two components: our Negative\nAffordance-aware Segmentation (NAA) module, which identifies functionally\ninappropriate contact regions, and a privileged teacher-student distillation\nprocess that ensures the final vision-based policy is highly successful.\nExtensive experiments demonstrate that AffordDex not only achieves universal\ndexterous grasping but also remains remarkably human-like in posture and\nfunctionally appropriate in contact location. As a result, AffordDex\nsignificantly outperforms state-of-the-art baselines across seen objects,\nunseen instances, and even entirely novel categories.", "AI": {"tldr": "AffordDex\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u624b\u90e8\u8fd0\u52a8\u548c\u7269\u4f53\u529f\u80fd\u611f\u77e5\uff0c\u5b9e\u73b0\u901a\u7528\u7075\u5de7\u6293\u53d6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u6293\u53d6\u7a33\u5b9a\u6027\uff0c\u5ffd\u7565\u4e86\u529f\u80fd\u611f\u77e5\u5b9a\u4f4d\u548c\u7c7b\u4eba\u59ff\u52bf\uff0c\u8fd9\u5bf9\u4e0b\u6e38\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\u3002", "method": "AffordDex\u5206\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u7b2c\u4e00\u9636\u6bb5\u6a21\u4eff\u4eba\u7c7b\u624b\u90e8\u8fd0\u52a8\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u6b8b\u5dee\u6a21\u5757\u548c\u529f\u80fd\u611f\u77e5\u6a21\u5757\uff08NAA\uff09\u9002\u5e94\u5177\u4f53\u7269\u4f53\u3002", "result": "AffordDex\u5728\u901a\u7528\u6293\u53d6\u3001\u7c7b\u4eba\u59ff\u52bf\u548c\u529f\u80fd\u63a5\u89e6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5df2\u77e5\u548c\u672a\u77e5\u7269\u4f53\u3002", "conclusion": "AffordDex\u901a\u8fc7\u7ed3\u5408\u8fd0\u52a8\u5148\u9a8c\u548c\u529f\u80fd\u611f\u77e5\uff0c\u5b9e\u73b0\u4e86\u66f4\u901a\u7528\u4e14\u7c7b\u4eba\u7684\u7075\u5de7\u6293\u53d6\u3002"}}
{"id": "2508.08982", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08982", "abs": "https://arxiv.org/abs/2508.08982", "authors": ["Seungeun Rho", "Kartik Garg", "Morgan Byrd", "Sehoon Ha"], "title": "Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion", "comment": "Conference on Robot Learning 2025", "summary": "Exploration is crucial for enabling legged robots to learn agile locomotion\nbehaviors that can overcome diverse obstacles. However, such exploration is\ninherently challenging, and we often rely on extensive reward engineering,\nexpert demonstrations, or curriculum learning - all of which limit\ngeneralizability. In this work, we propose Skill Discovery as Exploration\n(SDAX), a novel learning framework that significantly reduces human engineering\neffort. SDAX leverages unsupervised skill discovery to autonomously acquire a\ndiverse repertoire of skills for overcoming obstacles. To dynamically regulate\nthe level of exploration during training, SDAX employs a bi-level optimization\nprocess that autonomously adjusts the degree of exploration. We demonstrate\nthat SDAX enables quadrupedal robots to acquire highly agile behaviors\nincluding crawling, climbing, leaping, and executing complex maneuvers such as\njumping off vertical walls. Finally, we deploy the learned policy on real\nhardware, validating its successful transfer to the real world.", "AI": {"tldr": "SDAX\u6846\u67b6\u901a\u8fc7\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\u51cf\u5c11\u4eba\u5de5\u5de5\u7a0b\u9700\u6c42\uff0c\u4f7f\u56db\u8db3\u673a\u5668\u4eba\u81ea\u4e3b\u638c\u63e1\u591a\u79cd\u654f\u6377\u884c\u4e3a\u3002", "motivation": "\u63a2\u7d22\u5bf9\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u5b66\u4e60\u514b\u670d\u591a\u6837\u5316\u969c\u788d\u7684\u654f\u6377\u884c\u4e3a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u5de5\u7a0b\u6216\u4e13\u5bb6\u6f14\u793a\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faSDAX\u6846\u67b6\uff0c\u5229\u7528\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\u548c\u53cc\u5c42\u4f18\u5316\u52a8\u6001\u8c03\u8282\u63a2\u7d22\u7a0b\u5ea6\u3002", "result": "SDAX\u4f7f\u673a\u5668\u4eba\u638c\u63e1\u722c\u884c\u3001\u6500\u722c\u3001\u8df3\u8dc3\u7b49\u654f\u6377\u884c\u4e3a\uff0c\u5e76\u6210\u529f\u8fc1\u79fb\u5230\u771f\u5b9e\u786c\u4ef6\u3002", "conclusion": "SDAX\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\uff0c\u63d0\u5347\u673a\u5668\u4eba\u81ea\u4e3b\u5b66\u4e60\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.08983", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08983", "abs": "https://arxiv.org/abs/2508.08983", "authors": ["Ben Zandonati", "Tom\u00e1s Lozano-P\u00e9rez", "Leslie Pack Kaelbling"], "title": "Rational Inverse Reasoning", "comment": null, "summary": "Humans can observe a single, imperfect demonstration and immediately\ngeneralize to very different problem settings. Robots, in contrast, often\nrequire hundreds of examples and still struggle to generalize beyond the\ntraining conditions. We argue that this limitation arises from the inability to\nrecover the latent explanations that underpin intelligent behavior, and that\nthese explanations can take the form of structured programs consisting of\nhigh-level goals, sub-task decomposition, and execution constraints. In this\nwork, we introduce Rational Inverse Reasoning (RIR), a framework for inferring\nthese latent programs through a hierarchical generative model of behavior. RIR\nframes few-shot imitation as Bayesian program induction: a vision-language\nmodel iteratively proposes structured symbolic task hypotheses, while a\nplanner-in-the-loop inference scheme scores each by the likelihood of the\nobserved demonstration under that hypothesis. This loop yields a posterior over\nconcise, executable programs. We evaluate RIR on a suite of continuous\nmanipulation tasks designed to test one-shot and few-shot generalization across\nvariations in object pose, count, geometry, and layout. With as little as one\ndemonstration, RIR infers the intended task structure and generalizes to novel\nsettings, outperforming state-of-the-art vision-language model baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRational Inverse Reasoning (RIR)\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u751f\u6210\u6a21\u578b\u63a8\u65ad\u6f5c\u5728\u7a0b\u5e8f\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u4ece\u5c11\u91cf\u6f14\u793a\u4e2d\u6cdb\u5316\u5230\u65b0\u4efb\u52a1\u3002", "motivation": "\u4eba\u7c7b\u80fd\u4ece\u5355\u4e2a\u4e0d\u5b8c\u7f8e\u7684\u6f14\u793a\u4e2d\u6cdb\u5316\u5230\u4e0d\u540c\u4efb\u52a1\uff0c\u800c\u673a\u5668\u4eba\u9700\u8981\u5927\u91cf\u6570\u636e\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u7814\u7a76\u8ba4\u4e3a\u8fd9\u662f\u7531\u4e8e\u673a\u5668\u4eba\u65e0\u6cd5\u6062\u590d\u667a\u80fd\u884c\u4e3a\u7684\u6f5c\u5728\u89e3\u91ca\u3002", "method": "RIR\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u751f\u6210\u6a21\u578b\u63a8\u65ad\u6f5c\u5728\u7a0b\u5e8f\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u89c4\u5212\u5668\u8bc4\u5206\uff0c\u751f\u6210\u53ef\u6267\u884c\u7a0b\u5e8f\u3002", "result": "RIR\u5728\u8fde\u7eed\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u4ec5\u9700\u4e00\u6b21\u6f14\u793a\u5373\u53ef\u63a8\u65ad\u4efb\u52a1\u7ed3\u6784\u5e76\u6cdb\u5316\u5230\u65b0\u573a\u666f\uff0c\u4f18\u4e8e\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57fa\u7ebf\u3002", "conclusion": "RIR\u901a\u8fc7\u7a0b\u5e8f\u5f52\u7eb3\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u4ece\u5c11\u91cf\u6f14\u793a\u4e2d\u7684\u9ad8\u6548\u6cdb\u5316\uff0c\u4e3a\u667a\u80fd\u884c\u4e3a\u89e3\u91ca\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.08999", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.08999", "abs": "https://arxiv.org/abs/2508.08999", "authors": ["Chao Wang", "Michael Gienger", "Fan Zhang"], "title": "Generation of Real-time Robotic Emotional Expressions Learning from Human Demonstration in Mixed Reality", "comment": "4", "summary": "Expressive behaviors in robots are critical for effectively conveying their\nemotional states during interactions with humans. In this work, we present a\nframework that autonomously generates realistic and diverse robotic emotional\nexpressions based on expert human demonstrations captured in Mixed Reality\n(MR). Our system enables experts to teleoperate a virtual robot from a\nfirst-person perspective, capturing their facial expressions, head movements,\nand upper-body gestures, and mapping these behaviors onto corresponding robotic\ncomponents including eyes, ears, neck, and arms. Leveraging a\nflow-matching-based generative process, our model learns to produce coherent\nand varied behaviors in real-time in response to moving objects, conditioned\nexplicitly on given emotional states. A preliminary test validated the\neffectiveness of our approach for generating autonomous expressions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u73b0\u5b9e\uff08MR\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u6f14\u793a\u751f\u6210\u673a\u5668\u4eba\u60c5\u611f\u8868\u8fbe\uff0c\u5229\u7528\u6d41\u5339\u914d\u751f\u6210\u5b9e\u65f6\u591a\u6837\u5316\u884c\u4e3a\u3002", "motivation": "\u673a\u5668\u4eba\u60c5\u611f\u8868\u8fbe\u5bf9\u4e0e\u4eba\u7c7b\u4e92\u52a8\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u3002", "method": "\u4e13\u5bb6\u901a\u8fc7\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u8fdc\u7a0b\u64cd\u4f5c\u865a\u62df\u673a\u5668\u4eba\uff0c\u6355\u6349\u5176\u9762\u90e8\u8868\u60c5\u3001\u5934\u90e8\u52a8\u4f5c\u548c\u4e0a\u534a\u8eab\u624b\u52bf\uff0c\u5e76\u6620\u5c04\u5230\u673a\u5668\u4eba\u7ec4\u4ef6\uff1b\u4f7f\u7528\u6d41\u5339\u914d\u751f\u6210\u6a21\u578b\u5b9e\u65f6\u751f\u6210\u591a\u6837\u5316\u884c\u4e3a\u3002", "result": "\u521d\u6b65\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u751f\u6210\u81ea\u4e3b\u60c5\u611f\u8868\u8fbe\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u751f\u6210\u771f\u5b9e\u4e14\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u60c5\u611f\u8868\u8fbe\uff0c\u63d0\u5347\u4eba\u673a\u4e92\u52a8\u4f53\u9a8c\u3002"}}
{"id": "2508.09003", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.09003", "abs": "https://arxiv.org/abs/2508.09003", "authors": ["Filippo A. Spinelli", "Yifan Zhai", "Fang Nan", "Pascal Egli", "Julian Nubert", "Thilo Bleumer", "Lukas Miller", "Ferdinand Hofmann", "Marco Hutter"], "title": "Large Scale Robotic Material Handling: Learning, Planning, and Control", "comment": "Preliminary version, currently undergoing review process", "summary": "Bulk material handling involves the efficient and precise moving of large\nquantities of materials, a core operation in many industries, including cargo\nship unloading, waste sorting, construction, and demolition. These repetitive,\nlabor-intensive, and safety-critical operations are typically performed using\nlarge hydraulic material handlers equipped with underactuated grippers. In this\nwork, we present a comprehensive framework for the autonomous execution of\nlarge-scale material handling tasks. The system integrates specialized modules\nfor environment perception, pile attack point selection, path planning, and\nmotion control. The main contributions of this work are two reinforcement\nlearning-based modules: an attack point planner that selects optimal grasping\nlocations on the material pile to maximize removal efficiency and minimize the\nnumber of scoops, and a robust trajectory following controller that addresses\nthe precision and safety challenges associated with underactuated grippers in\nmovement, while utilizing their free-swinging nature to release material\nthrough dynamic throwing. We validate our framework through real-world\nexperiments on a 40 t material handler in a representative worksite, focusing\non two key tasks: high-throughput bulk pile management and high-precision truck\nloading. Comparative evaluations against human operators demonstrate the\nsystem's effectiveness in terms of precision, repeatability, and operational\nsafety. To the best of our knowledge, this is the first complete automation of\nmaterial handling tasks on a full scale.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5927\u89c4\u6a21\u7269\u6599\u642c\u8fd0\u4efb\u52a1\u7684\u81ea\u4e3b\u6267\u884c\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u611f\u77e5\u3001\u8def\u5f84\u89c4\u5212\u548c\u8fd0\u52a8\u63a7\u5236\u6a21\u5757\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6293\u53d6\u70b9\u548c\u8f68\u8ff9\u8ddf\u8e2a\u3002", "motivation": "\u7269\u6599\u642c\u8fd0\u662f\u8bb8\u591a\u884c\u4e1a\u4e2d\u7684\u6838\u5fc3\u64cd\u4f5c\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u64cd\u4f5c\uff0c\u6548\u7387\u4f4e\u4e14\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002", "method": "\u7cfb\u7edf\u6574\u5408\u4e86\u73af\u5883\u611f\u77e5\u3001\u6293\u53d6\u70b9\u9009\u62e9\u3001\u8def\u5f84\u89c4\u5212\u548c\u8fd0\u52a8\u63a7\u5236\u6a21\u5757\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6293\u53d6\u70b9\u548c\u8f68\u8ff9\u8ddf\u8e2a\u3002", "result": "\u572840\u5428\u7269\u6599\u642c\u8fd0\u8bbe\u5907\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7cfb\u7edf\u5728\u7cbe\u5ea6\u3001\u91cd\u590d\u6027\u548c\u5b89\u5168\u6027\u4e0a\u4f18\u4e8e\u4eba\u5de5\u64cd\u4f5c\u3002", "conclusion": "\u9996\u6b21\u5b9e\u73b0\u4e86\u5168\u89c4\u6a21\u7269\u6599\u642c\u8fd0\u4efb\u52a1\u7684\u5b8c\u6574\u81ea\u52a8\u5316\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.09071", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09071", "abs": "https://arxiv.org/abs/2508.09071", "authors": ["Lin Sun", "Bin Xie", "Yingfei Liu", "Hao Shi", "Tiancai Wang", "Jiale Cao"], "title": "GeoVLA: Empowering 3D Representations in Vision-Language-Action Models", "comment": "The project is visible at https://linsun449.github.io/GeoVLA/", "summary": "Vision-Language-Action (VLA) models have emerged as a promising approach for\nenabling robots to follow language instructions and predict corresponding\nactions.However, current VLA models mainly rely on 2D visual inputs, neglecting\nthe rich geometric information in the 3D physical world, which limits their\nspatial awareness and adaptability. In this paper, we present GeoVLA, a novel\nVLA framework that effectively integrates 3D information to advance robotic\nmanipulation. It uses a vision-language model (VLM) to process images and\nlanguage instructions,extracting fused vision-language embeddings. In parallel,\nit converts depth maps into point clouds and employs a customized point\nencoder, called Point Embedding Network, to generate 3D geometric embeddings\nindependently. These produced embeddings are then concatenated and processed by\nour proposed spatial-aware action expert, called 3D-enhanced Action Expert,\nwhich combines information from different sensor modalities to produce precise\naction sequences. Through extensive experiments in both simulation and\nreal-world environments, GeoVLA demonstrates superior performance and\nrobustness. It achieves state-of-the-art results in the LIBERO and ManiSkill2\nsimulation benchmarks and shows remarkable robustness in real-world tasks\nrequiring height adaptability, scale awareness and viewpoint invariance.", "AI": {"tldr": "GeoVLA\u662f\u4e00\u79cd\u65b0\u578b\u7684Vision-Language-Action\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u54083D\u51e0\u4f55\u4fe1\u606f\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u67092D\u89c6\u89c9\u8f93\u5165\u7684VLA\u6a21\u578b\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u4e3b\u8981\u4f9d\u8d562D\u89c6\u89c9\u8f93\u5165\uff0c\u5ffd\u7565\u4e863D\u7269\u7406\u4e16\u754c\u7684\u51e0\u4f55\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u7a7a\u95f4\u611f\u77e5\u548c\u9002\u5e94\u6027\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\u56fe\u50cf\u548c\u8bed\u8a00\u6307\u4ee4\uff0c\u540c\u65f6\u901a\u8fc7\u70b9\u4e91\u7f16\u7801\u5668\u63d0\u53d63D\u51e0\u4f55\u4fe1\u606f\uff0c\u6700\u7ec8\u901a\u8fc7\u7a7a\u95f4\u611f\u77e5\u52a8\u4f5c\u4e13\u5bb6\u751f\u6210\u7cbe\u786e\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5728LIBERO\u548cManiSkill2\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u9700\u8981\u9ad8\u5ea6\u9002\u5e94\u6027\u3001\u5c3a\u5ea6\u611f\u77e5\u548c\u89c6\u89d2\u4e0d\u53d8\u6027\u7684\u5b9e\u9645\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u9c81\u68d2\u6027\u3002", "conclusion": "GeoVLA\u901a\u8fc7\u6574\u54083D\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\uff0c\u4e3aVLA\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
