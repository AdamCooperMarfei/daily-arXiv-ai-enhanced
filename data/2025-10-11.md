<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 35]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [FLEET: Formal Language-Grounded Scheduling for Heterogeneous Robot Teams](https://arxiv.org/abs/2510.07417)
*Corban Rivera,Grayson Byrd,Meghan Booker,Bethany Kemp,Allison Gaines,Emma Holmes,James Uplinger,Celso M de Melo,David Handelman*

Main category: cs.RO

TL;DR: FLEET是一个混合式去中心化框架，将自然语言指令转化为优化的多机器人调度方案，结合LLM前端和形式化后端，在异构机器人团队协调任务中表现优于现有生成规划器。


<details>
  <summary>Details</summary>
Motivation: 解决异构机器人团队从自由形式自然语言指令进行协调的挑战，避免纯语言规划器的长期协调困难和幻觉问题，以及纯形式化方法需要封闭世界模型的限制。

Method: 使用LLM前端生成任务图（包含持续时间和优先级）和能力感知的机器人-任务适应度矩阵，形式化后端解决最小化完工时间问题，底层机器人通过智能闭环控制执行自由形式的子任务。

Result: 在多个自由形式语言引导的自主协调基准测试中，FLEET在异构任务的两智能体团队上比最先进的生成规划器成功率更高。消融实验显示MILP主要改进时间结构，而LLM派生的适应度对能力耦合任务至关重要。

Conclusion: LLM前端和形式化后端的结合提供了最高整体性能，并在具有不相关能力的四足机器人硬件试验中验证了向现实世界挑战的转化。

Abstract: Coordinating heterogeneous robot teams from free-form natural-language
instructions is hard. Language-only planners struggle with long-horizon
coordination and hallucination, while purely formal methods require
closed-world models. We present FLEET, a hybrid decentralized framework that
turns language into optimized multi-robot schedules. An LLM front-end produces
(i) a task graph with durations and precedence and (ii) a capability-aware
robot--task fitness matrix; a formal back-end solves a makespan-minimization
problem while the underlying robots execute their free-form subtasks with
agentic closed-loop control. Across multiple free-form language-guided autonomy
coordination benchmarks, FLEET improves success over state of the art
generative planners on two-agent teams across heterogeneous tasks. Ablations
show that mixed integer linear programming (MILP) primarily improves temporal
structure, while LLM-derived fitness is decisive for capability-coupled tasks;
together they deliver the highest overall performance. We demonstrate the
translation to real world challenges with hardware trials using a pair of
quadruped robots with disjoint capabilities.

</details>


### [2] [VeMo: A Lightweight Data-Driven Approach to Model Vehicle Dynamics](https://arxiv.org/abs/2510.07447)
*Girolamo Oddo,Roberto Nuca,Matteo Parsani*

Main category: cs.RO

TL;DR: 提出了一种基于门控循环单元(GRU)的轻量级编码器-解码器模型，用于在信息稀缺条件下预测高性能车辆的未来状态，仅使用车载测量的历史状态和驾驶员控制动作。


<details>
  <summary>Details</summary>
Motivation: 高性能车辆的动态建模通常需要详细的系统结构信息，但这些信息对于非设计者往往不可得。自主驾驶应用常在现有车辆上开发，面临信息稀缺的典型问题。

Method: 使用基于GRU层的轻量级编码器-解码器模型，通过车辆历史状态测量值和驾驶员控制动作来关联预测未来状态。模型完全数据驱动，不受物理约束限制。

Result: 在极端动态条件下，模型实现了最大平均相对误差低于2.6%。对感兴趣频率范围内的噪声输入数据表现出良好的鲁棒性。输出信号（纵向和横向加速度、横摆角速度、纵向速度）具有物理一致性。

Conclusion: 该数据驱动模型在信息稀缺条件下有效预测车辆动态状态，精度高且鲁棒性强，输出符合物理规律，适用于自主驾驶应用。

Abstract: Developing a dynamic model for a high-performance vehicle is a complex
problem that requires extensive structural information about the system under
analysis. This information is often unavailable to those who did not design the
vehicle and represents a typical issue in autonomous driving applications,
which are frequently developed on top of existing vehicles; therefore, vehicle
models are developed under conditions of information scarcity. This paper
proposes a lightweight encoder-decoder model based on Gate Recurrent Unit
layers to correlate the vehicle's future state with its past states, measured
onboard, and control actions the driver performs. The results demonstrate that
the model achieves a maximum mean relative error below 2.6% in extreme dynamic
conditions. It also shows good robustness when subject to noisy input data
across the interested frequency components. Furthermore, being entirely
data-driven and free from physical constraints, the model exhibits physical
consistency in the output signals, such as longitudinal and lateral
accelerations, yaw rate, and the vehicle's longitudinal velocity.

</details>


### [3] [HJCD-IK: GPU-Accelerated Inverse Kinematics through Batched Hybrid Jacobian Coordinate Descent](https://arxiv.org/abs/2510.07514)
*Cael Yasutake,Zachary Kingston,Brian Plancher*

Main category: cs.RO

TL;DR: HJCD-IK是一种基于GPU加速的混合逆运动学求解器，结合了方向感知的贪婪坐标下降初始化方案和基于雅可比矩阵的优化程序，在精度和速度方面都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统解析求解器受限于低自由度和特定拓扑结构，而数值优化方法计算成本高且容易陷入局部最优解。需要开发一种既快速又准确的通用IK求解器。

Method: 采用GPU加速的采样方法，结合方向感知的贪婪坐标下降初始化方案和雅可比矩阵优化程序，形成混合求解器。

Result: 相比现有技术，在收敛速度和整体精度方面都有显著提升，在精度-延迟帕累托前沿上始终找到最优解，通常实现数量级增益，并产生高质量样本的广泛分布。

Conclusion: HJCD-IK在逆运动学求解方面实现了显著的性能提升，为社区提供了开源代码。

Abstract: Inverse Kinematics (IK) is a core problem in robotics, in which joint
configurations are found to achieve a desired end-effector pose. Although
analytical solvers are fast and efficient, they are limited to systems with low
degrees-of-freedom and specific topological structures. Numerical
optimization-based approaches are more general, but suffer from high
computational costs and frequent convergence to spurious local minima. Recent
efforts have explored the use of GPUs to combine sampling and optimization to
enhance both the accuracy and speed of IK solvers. We build on this recent
literature and introduce HJCD-IK, a GPU-accelerated, sampling-based hybrid
solver that combines an orientation-aware greedy coordinate descent
initialization scheme with a Jacobian-based polishing routine. This design
enables our solver to improve both convergence speed and overall accuracy as
compared to the state-of-the-art, consistently finding solutions along the
accuracy-latency Pareto frontier and often achieving order-of-magnitude gains.
In addition, our method produces a broad distribution of high-quality samples,
yielding the lowest maximum mean discrepancy. We release our code open-source
for the benefit of the community.

</details>


### [4] [AVO: Amortized Value Optimization for Contact Mode Switching in Multi-Finger Manipulation](https://arxiv.org/abs/2510.07548)
*Adam Hung,Fan Yang,Abhinav Kumar,Sergio Aguilera Marinovic,Soshi Iba,Rana Soltani Zarrin,Dmitry Berenson*

Main category: cs.RO

TL;DR: 提出Amortized Value Optimization (AVO)方法，通过引入学习价值函数来预测未来任务性能，指导轨迹优化器选择有利于后续子任务的状态，从而解决灵巧操作任务中独立优化子任务导致的性能限制和计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作任务需要在不同接触模式间切换，传统方法将任务分解为独立优化的子任务，这限制了性能且计算昂贵，因为缺乏对未来子任务的信息可能导致系统陷入难以继续后续任务的状态。

Method: AVO引入学习价值函数预测总未来任务性能，将该价值函数纳入轨迹优化的成本函数中，通过价值函数梯度引导优化器选择最小化未来子任务成本的状态，有效桥接独立优化的子任务并减少在线计算需求。

Result: 在螺丝刀抓取和转动任务的仿真和真实实验中验证，即使使用50%更少的计算预算，相比没有价值函数的轨迹优化方法仍表现出改进的性能。

Conclusion: AVO通过价值函数指导轨迹优化，有效解决了灵巧操作任务中子任务独立优化导致的性能限制和计算效率问题，在减少计算开销的同时提升了任务性能。

Abstract: Dexterous manipulation tasks often require switching between different
contact modes, such as rolling, sliding, sticking, or non-contact contact
modes. When formulating dexterous manipulation tasks as a trajectory
optimization problem, a common approach is to decompose these tasks into
sub-tasks for each contact mode, which are each solved independently.
Optimizing each sub-task independently can limit performance, as optimizing
contact points, contact forces, or other variables without information about
future sub-tasks can place the system in a state from which it is challenging
to make progress on subsequent sub-tasks. Further, optimizing these sub-tasks
is very computationally expensive. To address these challenges, we propose
Amortized Value Optimization (AVO), which introduces a learned value function
that predicts the total future task performance. By incorporating this value
function into the cost of the trajectory optimization at each planning step,
the value function gradients guide the optimizer toward states that minimize
the cost in future sub-tasks. This effectively bridges separately optimized
sub-tasks, and accelerates the optimization by reducing the amount of online
computation needed. We validate AVO on a screwdriver grasping and turning task
in both simulation and real world experiments, and show improved performance
even with 50% less computational budget compared to trajectory optimization
without the value function.

</details>


### [5] [Inspection Planning Primitives with Implicit Models](https://arxiv.org/abs/2510.07611)
*Jingyang You,Hanna Kurniawati,Lashika Medagoda*

Main category: cs.RO

TL;DR: 提出了基于隐式模型的检查规划原语(IPIM)，使基于采样的检查规划器能够完全使用神经SDF表示进行规划，在保持轨迹质量的同时大幅减少内存使用。


<details>
  <summary>Details</summary>
Motivation: 基础设施老化和复杂性增加使得高效检查规划更为关键。现有基于采样的检查规划器虽然快速但内存消耗大，特别是对于大型复杂结构。隐式模型(如神经SDF)能高效表示复杂结构，但现有规划原语主要针对显式模型设计。

Method: 开发了一套名为IPIM的检查规划原语，使基于采样的检查规划器能够完全使用神经SDF表示，无需在隐式和显式模型间频繁转换。

Result: 在三个场景(包括具有9200万个三角网格面的复杂真实结构)的评估表明，即使使用基本的基于采样规划器配合IPIM，也能生成与最先进规划器质量相当的检查轨迹，同时内存使用减少高达70倍。

Conclusion: IPIM方法成功实现了基于隐式模型的检查规划，在保持轨迹质量的前提下显著降低了内存需求，为大型复杂基础设施的高效检查规划提供了可行方案。

Abstract: The aging and increasing complexity of infrastructures make efficient
inspection planning more critical in ensuring safety. Thanks to sampling-based
motion planning, many inspection planners are fast. However, they often require
huge memory. This is particularly true when the structure under inspection is
large and complex, consisting of many struts and pillars of various geometry
and sizes. Such structures can be represented efficiently using implicit
models, such as neural Signed Distance Functions (SDFs). However, most
primitive computations used in sampling-based inspection planner have been
designed to work efficiently with explicit environment models, which in turn
requires the planner to use explicit environment models or performs frequent
transformations between implicit and explicit environment models during
planning. This paper proposes a set of primitive computations, called
Inspection Planning Primitives with Implicit Models (IPIM), that enable
sampling-based inspection planners to entirely use neural SDFs representation
during planning. Evaluation on three scenarios, including inspection of a
complex real-world structure with over 92M triangular mesh faces, indicates
that even a rudimentary sampling-based planner with IPIM can generate
inspection trajectories of similar quality to those generated by the
state-of-the-art planner, while using up to 70x less memory than the
state-of-the-art inspection planner.

</details>


### [6] [GATO: GPU-Accelerated and Batched Trajectory Optimization for Scalable Edge Model Predictive Control](https://arxiv.org/abs/2510.07625)
*Alexander Du,Emre Adabag,Gabriel Bravo,Brian Plancher*

Main category: cs.RO

TL;DR: GATO是一个开源GPU加速的批量轨迹优化求解器，针对中等批量规模（几十到几百个求解）提供实时性能，相比CPU基线提速18-21倍，相比GPU基线提速1.4-16倍。


<details>
  <summary>Details</summary>
Motivation: 现有GPU加速方法要么并行化单个求解以满足实时性，要么在较慢速率下扩展到非常大的批量，或者通过限制模型通用性来实现速度。这为需要实时批量求解的现代MPC应用留下了性能空白。

Method: GATO结合了块级、warp级和线程级并行性，在求解内部和跨求解之间实现超高性能。该求解器在算法、软件和计算硬件方面进行了协同设计。

Result: 模拟基准测试显示，随着批量大小增加，相比CPU基线提速18-21倍，相比GPU基线提速1.4-16倍。案例研究展示了改进的扰动抑制和收敛行为，并在工业机械臂上进行了硬件验证。

Conclusion: GATO填补了中等批量规模实时求解的性能空白，通过协同设计方法实现了显著的性能提升，并开源以支持可重复性和采用。

Abstract: While Model Predictive Control (MPC) delivers strong performance across
robotics applications, solving the underlying (batches of) nonlinear trajectory
optimization (TO) problems online remains computationally demanding. Existing
GPU-accelerated approaches typically (i) parallelize a single solve to meet
real-time deadlines, (ii) scale to very large batches at slower-than-real-time
rates, or (iii) achieve speed by restricting model generality (e.g., point-mass
dynamics or a single linearization). This leaves a large gap in solver
performance for many state-of-the-art MPC applications that require real-time
batches of tens to low-hundreds of solves. As such, we present GATO, an open
source, GPU-accelerated, batched TO solver co-designed across algorithm,
software, and computational hardware to deliver real-time throughput for these
moderate batch size regimes. Our approach leverages a combination of block-,
warp-, and thread-level parallelism within and across solves for ultra-high
performance. We demonstrate the effectiveness of our approach through a
combination of: simulated benchmarks showing speedups of 18-21x over CPU
baselines and 1.4-16x over GPU baselines as batch size increases; case studies
highlighting improved disturbance rejection and convergence behavior; and
finally a validation on hardware using an industrial manipulator. We open
source GATO to support reproducibility and adoption.

</details>


### [7] [Differentiable Particle Optimization for Fast Sequential Manipulation](https://arxiv.org/abs/2510.07674)
*Lucas Chen,Shrutheesh Raman Iyer,Zachary Kingston*

Main category: cs.RO

TL;DR: SPaSM是一个完全GPU并行化的框架，通过优化的CUDA内核实现约束评估、采样和梯度优化，用于顺序机器人操作任务的端到端轨迹优化，相比现有方法实现4000倍加速。


<details>
  <summary>Details</summary>
Motivation: 顺序机器人操作任务需要在可能的高维配置空间中寻找满足多个物体交互几何约束的无碰撞轨迹，现有方法由于计算需求难以实现实时大规模求解，且GPU加速方法受限于CPU-GPU数据传输开销和复杂逻辑。

Method: 采用两阶段粒子优化策略：首先通过大规模并行采样解决放置约束，然后在关节空间中提升解决方案进行完整轨迹优化，将约束评估、采样和梯度优化编译为优化的CUDA内核。

Result: 在具有挑战性的基准测试中，求解时间达到毫秒级别，成功率为100%，相比现有方法实现4000倍加速。

Conclusion: SPaSM框架通过完全GPU并行化和端到端优化，显著提升了顺序机器人操作任务的求解效率，能够处理运动可行性约束放置选项的复杂场景。

Abstract: Sequential robot manipulation tasks require finding collision-free
trajectories that satisfy geometric constraints across multiple object
interactions in potentially high-dimensional configuration spaces. Solving
these problems in real-time and at large scales has remained out of reach due
to computational requirements. Recently, GPU-based acceleration has shown
promising results, but prior methods achieve limited performance due to CPU-GPU
data transfer overhead and complex logic that prevents full hardware
utilization. To this end, we present SPaSM (Sampling Particle optimization for
Sequential Manipulation), a fully GPU-parallelized framework that compiles
constraint evaluation, sampling, and gradient-based optimization into optimized
CUDA kernels for end-to-end trajectory optimization without CPU coordination.
The method consists of a two-stage particle optimization strategy: first
solving placement constraints through massively parallel sampling, then lifting
solutions to full trajectory optimization in joint space. Unlike hierarchical
approaches, SPaSM jointly optimizes object placements and robot trajectories to
handle scenarios where motion feasibility constrains placement options.
Experimental evaluation on challenging benchmarks demonstrates solution times
in the realm of $\textbf{milliseconds}$ with a 100% success rate; a
$4000\times$ speedup compared to existing approaches.

</details>


### [8] [EB-MBD: Emerging-Barrier Model-Based Diffusion for Safe Trajectory Optimization in Highly Constrained Environments](https://arxiv.org/abs/2510.07700)
*Raghav Mishra,Ian R. Manchester*

Main category: cs.RO

TL;DR: 提出基于新兴障碍函数的模型扩散约束方法，解决传统约束处理导致的性能下降问题，在2D避障和3D水下机械臂系统中显著提升解质量并大幅减少计算时间


<details>
  <summary>Details</summary>
Motivation: 模型扩散中的约束处理会导致灾难性性能下降，即使在简单2D系统中也因蒙特卡洛近似样本效率低而出现问题

Method: 提出新兴障碍模型扩散(EB-MBD)，使用逐步引入的障碍约束避免性能问题，无需计算昂贵的投影操作，通过分析采样活跃度来指导障碍参数调度选择

Result: 在2D碰撞避免和3D水下机械臂系统中，该方法相比模型扩散获得更低成本解，相比基于投影的方法计算时间减少数个数量级

Conclusion: 新兴障碍函数方法能有效处理模型扩散中的约束问题，显著提升性能同时大幅降低计算成本

Abstract: We propose enforcing constraints on Model-Based Diffusion by introducing
emerging barrier functions inspired by interior point methods. We show that
constraints on Model-Based Diffusion can lead to catastrophic performance
degradation, even on simple 2D systems due to sample inefficiency in the Monte
Carlo approximation of the score function. We introduce Emerging-Barrier
Model-Based Diffusion (EB-MBD) which uses progressively introduced barrier
constraints to avoid these problems, significantly improving solution quality,
without the need for computationally expensive operations such as projections.
We analyze the sampling liveliness of samples each iteration to inform barrier
parameter scheduling choice. We demonstrate results for 2D collision avoidance
and a 3D underwater manipulator system and show that our method achieves lower
cost solutions than Model-Based Diffusion, and requires orders of magnitude
less computation time than projection based methods.

</details>


### [9] [Probabilistically-Safe Bipedal Navigation over Uncertain Terrain via Conformal Prediction and Contraction Analysis](https://arxiv.org/abs/2510.07725)
*Kasidit Muenprasitivej,Ye Zhao,Glen Chou*

Main category: cs.RO

TL;DR: 提出了一种用于双足机器人在粗糙地形导航的概率安全规划控制框架，结合高斯过程和保形预测处理地形不确定性，通过收缩控制保证动态可行性和质心鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决双足机器人在不确定粗糙地形上安全导航的挑战，需要确保动态可行性和质心稳定性，同时处理地形高程的不确定性。

Method: 采用高斯过程回归估计地形高程，利用保形预测构建校准置信区间；提出基于收缩的可达管方法处理地形不确定性；设计基于收缩的飞轮扭矩控制律稳定线性倒立摆模型的角动量。

Result: 在MuJoCo中对Digit双足机器人进行物理仿真验证，证明了框架在给定置信水平下能够提供概率安全性和目标可达性保证。

Conclusion: 该规划框架能够有效确保双足机器人在不确定粗糙地形上的安全导航，同时保证状态收敛和管不变性，实现了概率安全性和目标可达性的形式化保证。

Abstract: We address the challenge of enabling bipedal robots to traverse rough terrain
by developing probabilistically safe planning and control strategies that
ensure dynamic feasibility and centroidal robustness under terrain uncertainty.
Specifically, we propose a high-level Model Predictive Control (MPC) navigation
framework for a bipedal robot with a specified confidence level of safety that
(i) enables safe traversal toward a desired goal location across a terrain map
with uncertain elevations, and (ii) formally incorporates uncertainty bounds
into the centroidal dynamics of locomotion control. To model the rough terrain,
we employ Gaussian Process (GP) regression to estimate elevation maps and
leverage Conformal Prediction (CP) to construct calibrated confidence intervals
that capture the true terrain elevation. Building on this, we formulate
contraction-based reachable tubes that explicitly account for terrain
uncertainty, ensuring state convergence and tube invariance. In addition, we
introduce a contraction-based flywheel torque control law for the reduced-order
Linear Inverted Pendulum Model (LIPM), which stabilizes the angular momentum
about the center-of-mass (CoM). This formulation provides both probabilistic
safety and goal reachability guarantees. For a given confidence level, we
establish the forward invariance of the proposed torque control law by
demonstrating exponential stabilization of the actual CoM phase-space
trajectory and the desired trajectory prescribed by the high-level planner.
Finally, we evaluate the effectiveness of our planning framework through
physics-based simulations of the Digit bipedal robot in MuJoCo.

</details>


### [10] [Injecting Hallucinations in Autonomous Vehicles: A Component-Agnostic Safety Evaluation Framework](https://arxiv.org/abs/2510.07749)
*Alexandre Moreira Nascimento,Gabriel Kenji Godoy Shimanuki,Lúcio Flavio Vismari,João Batista Camargo Jr,Jorge Rady de Almeida Jr,Paulo Sergio Cugnasca,Anna Carolina Muller Queiroz,Jeremy Noah Bailenson*

Main category: cs.RO

TL;DR: 该论文提出了一种将自动驾驶车辆感知故障重新定义为幻觉的概念，并开发了一个可配置、组件无关的幻觉注入框架，用于在仿真环境中测试自动驾驶系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的故障注入研究通常针对单一传感器或感知模块，导致难以推广或集成到统一仿真环境中的孤立框架。为了克服这一限制，需要一种更通用的方法来研究感知故障对自动驾驶安全性的影响。

Method: 将感知故障重新定义为幻觉（错误的感知），提出组件无关的幻觉注入框架，在开源仿真器中注入六种合理的幻觉类型，执行超过18,350次仿真测试。

Result: 统计验证了框架的有效性，并量化了每种幻觉类型对碰撞和险情的影响。某些幻觉（如感知延迟和漂移）显著增加了测试场景中的碰撞风险。

Conclusion: 该框架提供了一个可扩展、统计验证、组件无关且完全互操作的工具集，简化并加速了自动驾驶安全验证，即使对于具有新颖感知架构和组件的系统也适用，有助于缩短自动驾驶产品的上市时间。

Abstract: Perception failures in autonomous vehicles (AV) remain a major safety concern
because they are the basis for many accidents. To study how these failures
affect safety, researchers typically inject artificial faults into hardware or
software components and observe the outcomes. However, existing fault injection
studies often target a single sensor or machine perception (MP) module,
resulting in siloed frameworks that are difficult to generalize or integrate
into unified simulation environments. This work addresses that limitation by
reframing perception failures as hallucinations, false perceptions that distort
an AV situational awareness and may trigger unsafe control actions. Since
hallucinations describe only observable effects, this abstraction enables
analysis independent of specific sensors or algorithms, focusing instead on how
their faults manifest along the MP pipeline. Building on this concept, we
propose a configurable, component-agnostic hallucination injection framework
that induces six plausible hallucination types in an iterative open-source
simulator. More than 18,350 simulations were executed in which hallucinations
were injected while AVs crossed an unsignalized transverse street with traffic.
The results statistically validate the framework and quantify the impact of
each hallucination type on collisions and near misses. Certain hallucinations,
such as perceptual latency and drift, significantly increase the risk of
collision in the scenario tested, validating the proposed paradigm can stress
the AV system safety. The framework offers a scalable, statistically validated,
component agnostic, and fully interoperable toolset that simplifies and
accelerates AV safety validations, even those with novel MP architectures and
components. It can potentially reduce the time-to-market of AV and lay the
foundation for future research on fault tolerance, and resilient AV design.

</details>


### [11] [Trajectory Conditioned Cross-embodiment Skill Transfer](https://arxiv.org/abs/2510.07773)
*YuHang Tang,Yixuan Lou,Pengfei Han,Haoming Song,Xinyi Ye,Dong Wang,Bin Zhao*

Main category: cs.RO

TL;DR: TrajSkill是一个从人类演示视频直接学习机器人操作技能的框架，通过稀疏光流轨迹作为跨形态的运动线索，实现人类到机器人的技能迁移。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖配对数据集或手工设计奖励，限制了可扩展性和泛化能力。人类与机器人之间的形态差异是主要挑战。

Method: 将人类动作表示为稀疏光流轨迹，作为形态无关的运动线索。基于这些轨迹结合视觉和文本输入，联合合成时间一致的机器人操作视频并转换为可执行动作。

Result: 在MetaWorld仿真中，FVD降低39.6%，KVD降低36.6%，跨形态成功率提升16.7%。真实机器人厨房操作实验验证了方法的有效性。

Conclusion: TrajSkill能够有效实现从人类演示视频到机器人的跨形态技能迁移，在仿真和真实环境中都表现出色。

Abstract: Learning manipulation skills from human demonstration videos presents a
promising yet challenging problem, primarily due to the significant embodiment
gap between human body and robot manipulators. Existing methods rely on paired
datasets or hand-crafted rewards, which limit scalability and generalization.
We propose TrajSkill, a framework for Trajectory Conditioned Cross-embodiment
Skill Transfer, enabling robots to acquire manipulation skills directly from
human demonstration videos. Our key insight is to represent human motions as
sparse optical flow trajectories, which serve as embodiment-agnostic motion
cues by removing morphological variations while preserving essential dynamics.
Conditioned on these trajectories together with visual and textual inputs,
TrajSkill jointly synthesizes temporally consistent robot manipulation videos
and translates them into executable actions, thereby achieving cross-embodiment
skill transfer. Extensive experiments are conducted, and the results on
simulation data (MetaWorld) show that TrajSkill reduces FVD by 39.6\% and KVD
by 36.6\% compared with the state-of-the-art, and improves cross-embodiment
success rate by up to 16.7\%. Real-robot experiments in kitchen manipulation
tasks further validate the effectiveness of our approach, demonstrating
practical human-to-robot skill transfer across embodiments.

</details>


### [12] [IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction](https://arxiv.org/abs/2510.07778)
*Yandu Chen,Kefan Gu,Yuqing Wen,Yucheng Zhao,Tiancai Wang,Liqiang Nie*

Main category: cs.RO

TL;DR: IntentionVLA是一个视觉-语言-动作模型框架，通过课程训练范式提升机器人对人类意图的推理能力，在复杂交互任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在预训练阶段缺乏与具身场景相关的推理密集型任务，无法处理复杂现实交互中所需的隐式人类意图推理。

Method: 采用课程训练范式：首先使用精心设计的推理数据进行训练，结合意图推断、空间定位和紧凑具身推理；然后在微调阶段使用紧凑推理输出作为动作生成的上下文指导。

Result: 在直接指令下比π0模型成功率提高18%，在意图指令下比ECoT提高28%；在分布外意图任务中成功率是所有基线的两倍以上，零样本人机交互成功率40%。

Conclusion: IntentionVLA为下一代人机交互系统提供了有前景的范式，显著提升了机器人对人类意图的理解和响应能力。

Abstract: Vision-Language-Action (VLA) models leverage pretrained vision-language
models (VLMs) to couple perception with robotic control, offering a promising
path toward general-purpose embodied intelligence. However, current SOTA VLAs
are primarily pretrained on multimodal tasks with limited relevance to embodied
scenarios, and then finetuned to map explicit instructions to actions.
Consequently, due to the lack of reasoning-intensive pretraining and
reasoning-guided manipulation, these models are unable to perform implicit
human intention reasoning required for complex, real-world interactions. To
overcome these limitations, we propose \textbf{IntentionVLA}, a VLA framework
with a curriculum training paradigm and an efficient inference mechanism. Our
proposed method first leverages carefully designed reasoning data that combine
intention inference, spatial grounding, and compact embodied reasoning,
endowing the model with both reasoning and perception capabilities. In the
following finetuning stage, IntentionVLA employs the compact reasoning outputs
as contextual guidance for action generation, enabling fast inference under
indirect instructions. Experimental results show that IntentionVLA
substantially outperforms $\pi_0$, achieving 18\% higher success rates with
direct instructions and 28\% higher than ECoT under intention instructions. On
out-of-distribution intention tasks, IntentionVLA achieves over twice the
success rate of all baselines, and further enables zero-shot human-robot
interaction with 40\% success rate. These results highlight IntentionVLA as a
promising paradigm for next-generation human-robot interaction (HRI) systems.

</details>


### [13] [GM3: A General Physical Model for Micro-Mobility Vehicles](https://arxiv.org/abs/2510.07807)
*Grace Cai,Nithin Parepally,Laura Zheng,Ming C. Lin*

Main category: cs.RO

TL;DR: 提出了GM3（广义微移动模型），这是一个基于轮胎刷表示的物理模型，能够统一建模各种微移动车辆（包括单/双轨和多轮平台）的动力学特性，解决了现有模型忽略轮胎滑移、负载转移和骑手/车辆倾斜等问题。


<details>
  <summary>Details</summary>
Motivation: 当前主流的微移动车辆动力学建模工具依赖于运动自行车模型或其变体，或者特定模式的物理模型，这些模型无法捕捉轮胎滑移、负载转移和骑手/车辆倾斜等关键动力学特性，且缺乏统一的物理基础模型来覆盖各种常见的微移动车辆和车轮布局。

Method: 提出了基于轮胎刷表示的GM3模型，支持任意车轮配置；开发了交互式模型无关的仿真框架，将车辆/布局规范与动力学解耦，使用固定步长RK4积分、人在回路和脚本控制，以及实时轨迹跟踪和日志记录进行分析；在斯坦福无人机数据集的死亡圆环场景中对自行车、滑板车和手推车类别进行了实证验证。

Result: GM3模型能够更准确地捕捉微移动车辆的动力学特性，包括轮胎滑移、负载转移和骑手/车辆倾斜等效应，相比传统运动自行车模型和其他模型具有更好的性能表现。

Conclusion: GM3提供了一个统一的物理基础框架，能够准确建模各种微移动车辆的动力学行为，为自动驾驶系统训练和城市交通仿真提供了更可靠的建模工具。

Abstract: Modeling the dynamics of micro-mobility vehicles (MMV) is becoming
increasingly important for training autonomous vehicle systems and building
urban traffic simulations. However, mainstream tools rely on variants of the
Kinematic Bicycle Model (KBM) or mode-specific physics that miss tire slip,
load transfer, and rider/vehicle lean. To our knowledge, no unified,
physics-based model captures these dynamics across the full range of common
MMVs and wheel layouts. We propose the "Generalized Micro-mobility Model"
(GM3), a tire-level formulation based on the tire brush representation that
supports arbitrary wheel configurations, including single/double track and
multi-wheel platforms. We introduce an interactive model-agnostic simulation
framework that decouples vehicle/layout specification from dynamics to compare
the GM3 with the KBM and other models, consisting of fixed step RK4
integration, human-in-the-loop and scripted control, real-time trajectory
traces and logging for analysis. We also empirically validate the GM3 on the
Stanford Drone Dataset's deathCircle (roundabout) scene for biker, skater, and
cart classes.

</details>


### [14] [DM1: MeanFlow with Dispersive Regularization for 1-Step Robotic Manipulation](https://arxiv.org/abs/2510.07865)
*Guowei Zou,Haitao Wang,Hejun Wu,Yukun Qian,Yuhang Wang,Weibing Li*

Main category: cs.RO

TL;DR: DM1提出了一种结合分散正则化的流匹配框架，解决了现有基于流模型的策略存在的表示坍塌问题，在保持一步生成效率的同时显著提升了机器人操作的精确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于流模型的机器人操作策略存在表示坍塌问题，无法区分相似的视觉表示，导致在精确操作任务中失败。虽然流模型相比扩散模型具有一步生成的高采样效率优势，但表示坍塌限制了其在实际应用中的性能。

Method: DM1在MeanFlow框架中集成了分散正则化，通过在多个中间嵌入层应用不同变体的分散正则化，鼓励训练批次中的表示多样性，无需引入额外的网络模块或专门的训练过程。

Result: 在RoboMimic基准测试中，DM1实现了20-40倍的推理加速（0.07秒 vs 2-3.5秒），成功率提升了10-20个百分点，Lift任务达到99%成功率，相比基线的85%有显著提升。在真实机器人上的部署验证了从仿真到物理世界的有效迁移。

Conclusion: 这是首个利用表示正则化使基于流的策略在机器人操作中实现强大性能的工作，为高效和鲁棒的操作建立了一个简单而强大的方法。

Abstract: The ability to learn multi-modal action distributions is indispensable for
robotic manipulation policies to perform precise and robust control. Flow-based
generative models have recently emerged as a promising solution to learning
distributions of actions, offering one-step action generation and thus
achieving much higher sampling efficiency compared to diffusion-based methods.
However, existing flow-based policies suffer from representation collapse, the
inability to distinguish similar visual representations, leading to failures in
precise manipulation tasks. We propose DM1 (MeanFlow with Dispersive
Regularization for One-Step Robotic Manipulation), a novel flow matching
framework that integrates dispersive regularization into MeanFlow to prevent
collapse while maintaining one-step efficiency. DM1 employs multiple dispersive
regularization variants across different intermediate embedding layers,
encouraging diverse representations across training batches without introducing
additional network modules or specialized training procedures. Experiments on
RoboMimic benchmarks show that DM1 achieves 20-40 times faster inference (0.07s
vs. 2-3.5s) and improves success rates by 10-20 percentage points, with the
Lift task reaching 99% success over 85% of the baseline. Real-robot deployment
on a Franka Panda further validates that DM1 transfers effectively from
simulation to the physical world. To the best of our knowledge, this is the
first work to leverage representation regularization to enable flow-based
policies to achieve strong performance in robotic manipulation, establishing a
simple yet powerful approach for efficient and robust manipulation.

</details>


### [15] [USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots](https://arxiv.org/abs/2510.07869)
*Junwen Gu,Zhiheng wu,Pengxuan Si,Shuang Qiu,Yukai Feng,Luoyang Sun,Laien Luo,Lianyi Yu,Jian Wang,Zhengxing Wu*

Main category: cs.RO

TL;DR: 提出了USIM模拟数据集和U0模型，解决水下机器人多任务自主操作的数据稀缺问题，在多种任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 水下环境复杂，缺乏大规模高质量数据集，难以开发能自主执行多任务的水下智能机器人。

Method: 构建USIM模拟数据集（56.1万帧，1852条轨迹），提出U0视觉-语言-动作模型，集成双目视觉和多模态融合，加入卷积注意力感知增强模块。

Result: 在检查、避障、扫描等任务中成功率80%，移动操作任务中目标距离比基线方法减少21.2%。

Conclusion: VLA模型可有效应用于水下机器人，为可扩展数据集构建、任务自主性提升和智能通用水下机器人实现提供基础。

Abstract: Underwater environments present unique challenges for robotic operation,
including complex hydrodynamics, limited visibility, and constrained
communication. Although data-driven approaches have advanced embodied
intelligence in terrestrial robots and enabled task-specific autonomous
underwater robots, developing underwater intelligence capable of autonomously
performing multiple tasks remains highly challenging, as large-scale,
high-quality underwater datasets are still scarce. To address these
limitations, we introduce USIM, a simulation-based multi-task
Vision-Language-Action (VLA) dataset for underwater robots. USIM comprises over
561K frames from 1,852 trajectories, totaling approximately 15.6 hours of
BlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging from
visual navigation to mobile manipulation. Building upon this dataset, we
propose U0, a VLA model for general underwater robots, which integrates
binocular vision and other sensor modalities through multimodal fusion, and
further incorporates a convolution-attention-based perception focus enhancement
module (CAP) to improve spatial understanding and mobile manipulation. Across
tasks such as inspection, obstacle avoidance, scanning, and dynamic tracking,
the framework achieves a success rate of 80%, while in challenging mobile
manipulation tasks, it reduces the distance to the target by 21.2% compared
with baseline methods, demonstrating its effectiveness. USIM and U0 show that
VLA models can be effectively applied to underwater robotic applications,
providing a foundation for scalable dataset construction, improved task
autonomy, and the practical realization of intelligent general underwater
robots.

</details>


### [16] [Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception -- Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track](https://arxiv.org/abs/2510.07871)
*Erjia Xiao,Lingfeng Zhang,Yingbo Tang,Hao Cheng,Renjing Xu,Wenbo Ding,Lei Zhou,Long Chen,Hangjun Ye,Xiaoshuai Hao*

Main category: cs.RO

TL;DR: 本文介绍了参加IROS 2025 RoboSense挑战赛社会导航赛道的技术方案，通过在Falcon模型基础上增加主动风险感知模块，提升了在动态人群环境中导航的社会合规性，最终获得第二名。


<details>
  <summary>Details</summary>
Motivation: 解决自主代理在动态人群室内环境中安全、高效且符合社会规范的导航问题，特别是在仅使用机载传感器（RGB-D和里程计）且无全局地图的情况下。

Method: 在Falcon模型基础上引入主动风险感知模块，通过学习基于距离的碰撞风险评分来增强空间感知能力，实现更主动的避碰行为。

Result: 在Social-HM3D基准测试中，该方法提高了代理在拥挤室内场景中保持个人空间合规性的能力，在16支参赛队伍中获得第二名。

Conclusion: 主动风险感知模块有效提升了社会导航性能，证明了基于风险预测的方法在复杂动态环境中导航的可行性。

Abstract: In this report, we describe the technical details of our submission to the
IROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on
developing RGBD-based perception and navigation systems that enable autonomous
agents to navigate safely, efficiently, and socially compliantly in dynamic
human-populated indoor environments. The challenge requires agents to operate
from an egocentric perspective using only onboard sensors including RGB-D
observations and odometry, without access to global maps or privileged
information, while maintaining social norm compliance such as safe distances
and collision avoidance. Building upon the Falcon model, we introduce a
Proactive Risk Perception Module to enhance social navigation performance. Our
approach augments Falcon with collision risk understanding that learns to
predict distance-based collision risk scores for surrounding humans, which
enables the agent to develop more robust spatial awareness and proactive
collision avoidance behaviors. The evaluation on the Social-HM3D benchmark
demonstrates that our method improves the agent's ability to maintain personal
space compliance while navigating toward goals in crowded indoor scenes with
dynamic human agents, achieving 2nd place among 16 participating teams in the
challenge.

</details>


### [17] [Towards Proprioception-Aware Embodied Planning for Dual-Arm Humanoid Robots](https://arxiv.org/abs/2510.07882)
*Boyu Li,Siyuan He,Hang Xu,Haoqi Yuan,Yu Zang,Liwei Hu,Junpeng Yue,Zhenxiong Jiang,Pengbo Hu,Börje F. Karlsson,Yehui Tang,Zongqing Lu*

Main category: cs.RO

TL;DR: 提出了DualTHOR双臂人形机器人仿真器和Proprio-MLLM模型，通过增强本体感知信息来解决MLLMs在长时程双臂任务中的规划性能问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在双臂人形机器人的长时程任务中效果有限，主要因为缺乏专门的仿真平台和模型缺乏对机器人本体状态的感知能力。

Method: 开发了DualTHOR仿真器（包含连续过渡和应急机制），并提出了Proprio-MLLM模型，通过运动位置嵌入和跨空间编码器整合本体感知信息。

Result: 在实验中，Proprio-MLLM相比现有MLLMs在规划性能上平均提升了19.75%。

Conclusion: 该工作为推进人形机器人的具身智能提供了必要的仿真平台和有效模型。

Abstract: In recent years, Multimodal Large Language Models (MLLMs) have demonstrated
the ability to serve as high-level planners, enabling robots to follow complex
human instructions. However, their effectiveness, especially in long-horizon
tasks involving dual-arm humanoid robots, remains limited. This limitation
arises from two main challenges: (i) the absence of simulation platforms that
systematically support task evaluation and data collection for humanoid robots,
and (ii) the insufficient embodiment awareness of current MLLMs, which hinders
reasoning about dual-arm selection logic and body positions during planning. To
address these issues, we present DualTHOR, a new dual-arm humanoid simulator,
with continuous transition and a contingency mechanism. Building on this
platform, we propose Proprio-MLLM, a model that enhances embodiment awareness
by incorporating proprioceptive information with motion-based position
embedding and a cross-spatial encoder. Experiments show that, while existing
MLLMs struggle in this environment, Proprio-MLLM achieves an average
improvement of 19.75% in planning performance. Our work provides both an
essential simulation platform and an effective model to advance embodied
intelligence in humanoid robotics. The code is available at
https://anonymous.4open.science/r/DualTHOR-5F3B.

</details>


### [18] [Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation](https://arxiv.org/abs/2510.07975)
*Mingyang Sun,Jiude Wei,Qichen He,Donglin Wang,Cewu Lu,Jianhua Sun*

Main category: cs.RO

TL;DR: GRACE框架通过可执行分析概念（EAC）弥合视觉语言模型语义理解与机器人物理执行之间的差距，实现精确和通用的机器人操作。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在非结构化环境中进行精确和通用操作的基本挑战，弥合视觉语言模型高级语义理解与实际物理执行之间的"语义到物理"鸿沟。

Method: 引入GRACE框架，通过可执行分析概念（EAC）将自然语言指令和视觉信息转化为实例化的EAC，从中推导抓取姿态、力方向和物理可行的运动轨迹。

Result: 在模拟和真实环境中对各种铰接物体实现了强大的零样本泛化能力，无需任务特定训练。

Conclusion: GRACE提供了一个统一且可解释的接口，通过语义-物理接地有效实现了精确和可泛化的机器人操作。

Abstract: Enabling robots to perform precise and generalized manipulation in
unstructured environments remains a fundamental challenge in embodied AI. While
Vision-Language Models (VLMs) have demonstrated remarkable capabilities in
semantic reasoning and task planning, a significant gap persists between their
high-level understanding and the precise physical execution required for
real-world manipulation. To bridge this "semantic-to-physical" gap, we
introduce GRACE, a novel framework that grounds VLM-based reasoning through
executable analytic concepts (EAC)-mathematically defined blueprints that
encode object affordances, geometric constraints, and semantics of
manipulation. Our approach integrates a structured policy scaffolding pipeline
that turn natural language instructions and visual information into an
instantiated EAC, from which we derive grasp poses, force directions and plan
physically feasible motion trajectory for robot execution. GRACE thus provides
a unified and interpretable interface between high-level instruction
understanding and low-level robot control, effectively enabling precise and
generalizable manipulation through semantic-physical grounding. Extensive
experiments demonstrate that GRACE achieves strong zero-shot generalization
across a variety of articulated objects in both simulated and real-world
environments, without requiring task-specific training.

</details>


### [19] [Orientation Learning and Adaptation towards Simultaneous Incorporation of Multiple Local Constraints](https://arxiv.org/abs/2510.07986)
*Gaofeng Li,Peisen Xu,Ruize Wang,Qi Ye,Jiming Chen,Dezhen Song,Yanlong Huang*

Main category: cs.RO

TL;DR: 提出基于角度-轴空间的方向表示方法，通过加权平均机制在SO(3)流形上融合多条轨迹，同时处理多个局部约束，解决非欧几何带来的扭曲问题。


<details>
  <summary>Details</summary>
Motivation: 旋转群SO(3)是黎曼流形，其非欧几何特性导致局部约束的整合困难，特别是同时整合多个局部约束时存在扭曲问题。

Method: 基于角度-轴表示法提出SO(3)上的加权平均机制，在不同基点考虑不同局部约束生成多条轨迹，然后融合成平滑轨迹。

Result: 仿真和实验验证表明，该方法能适应任意期望路径点的方向，处理角加速度约束，同时整合多个局部约束实现额外收益，如获得更小的加速度成本。

Conclusion: 所提方法能解决扭曲问题，使现成的欧几里得学习算法在非欧空间中重新适用，有效整合多个局部约束。

Abstract: Orientation learning plays a pivotal role in many tasks. However, the
rotation group SO(3) is a Riemannian manifold. As a result, the distortion
caused by non-Euclidean geometric nature introduces difficulties to the
incorporation of local constraints, especially for the simultaneous
incorporation of multiple local constraints. To address this issue, we propose
the Angle-Axis Space-based orientation representation method to solve several
orientation learning problems, including orientation adaptation and
minimization of angular acceleration. Specifically, we propose a weighted
average mechanism in SO(3) based on the angle-axis representation method. Our
main idea is to generate multiple trajectories by considering different local
constraints at different basepoints. Then these multiple trajectories are fused
to generate a smooth trajectory by our proposed weighted average mechanism,
achieving the goal to incorporate multiple local constraints simultaneously.
Compared with existing solution, ours can address the distortion issue and make
the off-theshelf Euclidean learning algorithm be re-applicable in non-Euclidean
space. Simulation and Experimental evaluations validate that our solution can
not only adapt orientations towards arbitrary desired via-points and cope with
angular acceleration constraints, but also incorporate multiple local
constraints simultaneously to achieve extra benefits, e.g., achieving smaller
acceleration costs.

</details>


### [20] [FastUMI-100K: Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset](https://arxiv.org/abs/2510.08022)
*Kehui Liu,Zhongjie Jia,Yang Li,Zhaxizhuoma,Pengan Chen,Song Liu,Xin Liu,Pingrui Zhang,Haoming Song,Xinyi Ye,Nieqing Cao,Zhigang Wang,Jia Zeng,Dong Wang,Yan Ding,Bin Zhao,Xuelong Li*

Main category: cs.RO

TL;DR: FastUMI-100K是一个大规模UMI风格的多模态演示数据集，包含超过10万条轨迹，涵盖54个任务和数百种物体类型，旨在解决现有机器人演示数据集在可扩展性、轨迹平滑性和跨机器人适应性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人演示数据集主要依赖人类遥操作收集，在可扩展性、轨迹平滑性和不同机器人实体的适用性方面存在限制，无法满足日益复杂的现实世界操作任务需求。

Method: 使用FastUMI机器人系统收集数据，该系统采用模块化、硬件解耦的机械设计和集成轻量级跟踪系统，能够更高效地收集多模态演示数据，包括末端执行器状态、多视角鱼眼图像和文本注释。

Result: 实验结果表明，FastUMI-100K能够在各种基线算法上实现高策略成功率，证实了其在解决复杂动态操作挑战方面的鲁棒性、适应性和现实世界适用性。

Conclusion: FastUMI-100K提供了一个更可扩展、灵活和适应性的解决方案，满足了现实世界机器人演示数据的多样化需求，为数据驱动的机器人操作学习提供了高质量的大规模数据集。

Abstract: Data-driven robotic manipulation learning depends on large-scale,
high-quality expert demonstration datasets. However, existing datasets, which
primarily rely on human teleoperated robot collection, are limited in terms of
scalability, trajectory smoothness, and applicability across different robotic
embodiments in real-world environments. In this paper, we present FastUMI-100K,
a large-scale UMI-style multimodal demonstration dataset, designed to overcome
these limitations and meet the growing complexity of real-world manipulation
tasks. Collected by FastUMI, a novel robotic system featuring a modular,
hardware-decoupled mechanical design and an integrated lightweight tracking
system, FastUMI-100K offers a more scalable, flexible, and adaptable solution
to fulfill the diverse requirements of real-world robot demonstration data.
Specifically, FastUMI-100K contains over 100K+ demonstration trajectories
collected across representative household environments, covering 54 tasks and
hundreds of object types. Our dataset integrates multimodal streams, including
end-effector states, multi-view wrist-mounted fisheye images and textual
annotations. Each trajectory has a length ranging from 120 to 500 frames.
Experimental results demonstrate that FastUMI-100K enables high policy success
rates across various baseline algorithms, confirming its robustness,
adaptability, and real-world applicability for solving complex, dynamic
manipulation challenges. The source code and dataset will be released in this
link https://github.com/MrKeee/FastUMI-100K.

</details>


### [21] [Towards Reliable LLM-based Robot Planning via Combined Uncertainty Estimation](https://arxiv.org/abs/2510.08044)
*Shiyuan Yin,Chenjia Bai,Zihao Zhang,Junwei Jin,Xinxin Zhang,Chi Zhang,Xuelong Li*

Main category: cs.RO

TL;DR: 提出CURE方法，通过分解认知不确定性和内在不确定性来改进LLM在机器人规划中的可靠性，在厨房操作和桌面重排实验中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: LLM在机器人规划中存在幻觉问题，导致过度自信但可能不准确或不安全的计划。现有研究未充分区分认知和内在不确定性，限制了不确定性估计的效果。

Method: 提出CURE方法，将不确定性分解为认知不确定性和内在不确定性，其中认知不确定性进一步细分为任务清晰度和任务熟悉度。使用随机网络蒸馏和基于LLM特征的多层感知机回归头进行不确定性评估。

Result: 在厨房操作和桌面重排两个实验场景中验证，相比现有方法，CURE的不确定性估计与真实执行结果更加一致。

Conclusion: CURE方法通过分解不同类型的不确定性，显著提升了LLM在机器人规划中的可靠性，为安全可靠的具身规划提供了有效解决方案。

Abstract: Large language models (LLMs) demonstrate advanced reasoning abilities,
enabling robots to understand natural language instructions and generate
high-level plans with appropriate grounding. However, LLM hallucinations
present a significant challenge, often leading to overconfident yet potentially
misaligned or unsafe plans. While researchers have explored uncertainty
estimation to improve the reliability of LLM-based planning, existing studies
have not sufficiently differentiated between epistemic and intrinsic
uncertainty, limiting the effectiveness of uncertainty estimation. In this
paper, we present Combined Uncertainty estimation for Reliable Embodied
planning (CURE), which decomposes the uncertainty into epistemic and intrinsic
uncertainty, each estimated separately. Furthermore, epistemic uncertainty is
subdivided into task clarity and task familiarity for more accurate evaluation.
The overall uncertainty assessments are obtained using random network
distillation and multi-layer perceptron regression heads driven by LLM
features. We validated our approach in two distinct experimental settings:
kitchen manipulation and tabletop rearrangement experiments. The results show
that, compared to existing methods, our approach yields uncertainty estimates
that are more closely aligned with the actual execution outcomes.

</details>


### [22] [Beyond hospital reach: Autonomous lightweight ultrasound robot for liver sonography](https://arxiv.org/abs/2510.08106)
*Zihan Li,Yixiao Xu,Lei Zhang,Taiyu Han,Xinshan Yang,Yingni Wang,Mingxuan Liu,Shenghai Xin,Linxun Liu,Hongen Liao,Guochen Ning*

Main category: cs.RO

TL;DR: 开发了一个轻量级自主超声机器人系统，能够在资源有限地区自主获取专家级肝脏超声图像并检测病理


<details>
  <summary>Details</summary>
Motivation: 解决资源有限地区超声专家严重短缺的问题，肝脏超声需要专业技能来定位非连续平面，而专家资源不足

Method: 集成多模态感知与记忆注意力的AI代理，结合588克6自由度线驱动机器人，通过腹部安装增强运动鲁棒性

Result: 系统能够自主获取专家级标准肝脏超声平面，在患者中检测病理，包括高海拔城市西宁的病例，在快速运动个体和野外环境中表现有效

Conclusion: 这是首个在多种挑战性场景中实现自主超声检查的系统，有望改变医疗服务不足地区获得专家级诊断的途径

Abstract: Liver disease is a major global health burden. While ultrasound is the
first-line diagnostic tool, liver sonography requires locating multiple
non-continuous planes from positions where target structures are often not
visible, for biometric assessment and lesion detection, requiring significant
expertise. However, expert sonographers are severely scarce in resource-limited
regions. Here, we develop an autonomous lightweight ultrasound robot comprising
an AI agent that integrates multi-modal perception with memory attention for
localization of unseen target structures, and a 588-gram 6-degrees-of-freedom
cable-driven robot. By mounting on the abdomen, the system enhances robustness
against motion. Our robot can autonomously acquire expert-level standard liver
ultrasound planes and detect pathology in patients, including two from Xining,
a 2261-meter-altitude city with limited medical resources. Our system performs
effectively on rapid-motion individuals and in wilderness environments. This
work represents the first demonstration of autonomous sonography across
multiple challenging scenarios, potentially transforming access to expert-level
diagnostics in underserved regions.

</details>


### [23] [Accurate and Noise-Tolerant Extraction of Routine Logs in Robotic Process Automation (Extended Version)](https://arxiv.org/abs/2510.08118)
*Massimiliano de Leoni,Faizan Ahmed Khan,Simone Agostinelli*

Main category: cs.RO

TL;DR: 提出一种基于聚类的技术来从UI日志中提取常规日志，用于机器人流程自动化。该技术在存在噪声的情况下比现有方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有工作大多不直接关注模型发现，仅提取常规动作集，且未在包含噪声（人类执行的自然变异和偶然错误）的场景下评估。

Method: 采用基于聚类的技术从UI日志中提取常规日志，并在九个具有不同噪声水平的UI日志上进行实验。

Result: 与现有技术相比，该技术能够提取更准确的常规日志，特别是在存在噪声的情况下，通过标准评估指标验证了其优越性。

Conclusion: 该聚类技术能够有效处理噪声，提取高质量的常规日志，为机器人流程自动化提供了更好的支持。

Abstract: Robotic Process Mining focuses on the identification of the routine types
performed by human resources through a User Interface. The ultimate goal is to
discover routine-type models to enable robotic process automation. The
discovery of routine-type models requires the provision of a routine log.
Unfortunately, the vast majority of existing works do not directly focus on
enabling the model discovery, limiting themselves to extracting the set of
actions that are part of the routines. They were also not evaluated in
scenarios characterized by inconsistent routine execution, hereafter referred
to as noise, which reflects natural variability and occasional errors in human
performance. This paper presents a clustering-based technique that aims to
extract routine logs. Experiments were conducted on nine UI logs from the
literature with different levels of injected noise. Our technique was compared
with existing techniques, most of which are not meant to discover routine logs
but were adapted for the purpose. The results were evaluated through standard
state-of-the-art metrics, showing that we can extract more accurate routine
logs than what the state of the art could, especially in the presence of noise.

</details>


### [24] [NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions](https://arxiv.org/abs/2510.08173)
*Haolin Yang,Yuxing Long,Zhuoyuan Yu,Zihan Yang,Minghan Wang,Jiapeng Xu,Yihan Wang,Ziyan Yu,Wenzhe Cai,Lei Kang,Hao Dong*

Main category: cs.RO

TL;DR: 提出了NavSpace基准测试，包含6个任务类别和1,228个轨迹-指令对，用于评估导航代理的空间感知和推理能力，并提出了新的空间智能导航模型SNav。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注语义理解，但忽视了系统评估导航代理的空间感知和推理能力。

Method: 构建NavSpace基准测试，包含6个任务类别和1,228个轨迹-指令对；提出SNav空间智能导航模型。

Result: 评估了22个导航代理，包括最先进的导航模型和多模态大语言模型；SNav在NavSpace和真实机器人测试中优于现有导航代理。

Conclusion: NavSpace基准测试揭示了具身导航中的空间智能问题，SNav为未来工作建立了强基线。

Abstract: Instruction-following navigation is a key step toward embodied intelligence.
Prior benchmarks mainly focus on semantic understanding but overlook
systematically evaluating navigation agents' spatial perception and reasoning
capabilities. In this work, we introduce the NavSpace benchmark, which contains
six task categories and 1,228 trajectory-instruction pairs designed to probe
the spatial intelligence of navigation agents. On this benchmark, we
comprehensively evaluate 22 navigation agents, including state-of-the-art
navigation models and multimodal large language models. The evaluation results
lift the veil on spatial intelligence in embodied navigation. Furthermore, we
propose SNav, a new spatially intelligent navigation model. SNav outperforms
existing navigation agents on NavSpace and real robot tests, establishing a
strong baseline for future work.

</details>


### [25] [Evaluation of a Robust Control System in Real-World Cable-Driven Parallel Robots](https://arxiv.org/abs/2510.08270)
*Damir Nurtdinov,Aliaksei Korshuk,Alexei Kornaev,Alexander Maloletov*

Main category: cs.RO

TL;DR: 比较经典PID控制器与现代强化学习算法在欠约束线驱动并联机器人控制中的性能，发现TRPO在不同轨迹和较大时间间隔下表现最优。


<details>
  <summary>Details</summary>
Motivation: 评估经典和现代控制方法在现实世界线驱动并联机器人中的性能，特别关注欠约束系统和有限时间离散化的情况。

Method: 对经典PID控制器与强化学习算法（DDPG、PPO、TRPO）进行比较分析，评估其在各种轨迹和不同时间间隔下的控制性能。

Result: TRPO在所有方法中表现最佳，在各种轨迹上实现最低的均方根误差，并对较大控制更新时间间隔表现出鲁棒性。

Conclusion: TRPO在复杂机器人控制任务中具有作为鲁棒解决方案的潜力，特别适用于动态环境和未来传感器融合或混合控制策略应用。

Abstract: This study evaluates the performance of classical and modern control methods
for real-world Cable-Driven Parallel Robots (CDPRs), focusing on
underconstrained systems with limited time discretization. A comparative
analysis is conducted between classical PID controllers and modern
reinforcement learning algorithms, including Deep Deterministic Policy Gradient
(DDPG), Proximal Policy Optimization (PPO), and Trust Region Policy
Optimization (TRPO). The results demonstrate that TRPO outperforms other
methods, achieving the lowest root mean square (RMS) errors across various
trajectories and exhibiting robustness to larger time intervals between control
updates. TRPO's ability to balance exploration and exploitation enables stable
control in noisy, real-world environments, reducing reliance on high-frequency
sensor feedback and computational demands. These findings highlight TRPO's
potential as a robust solution for complex robotic control tasks, with
implications for dynamic environments and future applications in sensor fusion
or hybrid control strategies.

</details>


### [26] [Airy: Reading Robot Intent through Height and Sky](https://arxiv.org/abs/2510.08381)
*Baoyang Chen,Xian Xu,Huamin Qu*

Main category: cs.RO

TL;DR: Airy是一个艺术装置，通过两个强化学习训练的机械臂竞争抖床单，将复杂的多智能体AI决策转化为直观可理解的体验，使用竞争、具身熟悉性和传感器到感知映射等设计原则。


<details>
  <summary>Details</summary>
Motivation: 随着工业机器人进入共享人类空间，其不透明的决策过程威胁安全、信任和公共监督。该项目旨在探索复杂多智能体AI是否能变得直观可理解。

Method: 采用三个设计原则：竞争作为清晰指标（谁抖得更高）、具身熟悉性（观众能识别抖床单动作）、传感器到感知映射（通过森林和天气投影显示机器人合作或竞争关系）。

Result: 在五个国际展览中的观察表明，观众能够实时解读机器人的策略、冲突和合作，情绪反应与系统内部状态一致。

Conclusion: 该项目展示了感官隐喻如何将黑盒系统转变为公共界面，使复杂的AI决策变得直观可理解。

Abstract: As industrial robots move into shared human spaces, their opaque decision
making threatens safety, trust, and public oversight. This artwork, Airy, asks
whether complex multi agent AI can become intuitively understandable by staging
a competition between two reinforcement trained robot arms that snap a bedsheet
skyward. Building on three design principles, competition as a clear metric
(who lifts higher), embodied familiarity (audiences recognize fabric snapping),
and sensor to sense mapping (robot cooperation or rivalry shown through forest
and weather projections), the installation gives viewers a visceral way to read
machine intent. Observations from five international exhibitions indicate that
audiences consistently read the robots' strategies, conflict, and cooperation
in real time, with emotional reactions that mirror the system's internal state.
The project shows how sensory metaphors can turn a black box into a public
interface.

</details>


### [27] [Reliability of Single-Level Equality-Constrained Inverse Optimal Control](https://arxiv.org/abs/2510.08406)
*Filip Bečanović,Kosta Jovanović,Vincent Bonnet*

Main category: cs.RO

TL;DR: 本文提出了一种基于单级重构的逆最优控制方法，相比传统的双层方法计算速度提升15倍，同时对噪声具有更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有逆最优控制方法要么基于计算缓慢的双层过程，要么基于对噪声敏感的最优性条件违反最小化方法，需要一种既快速又鲁棒的新方法。

Method: 采用单级重构方法替代传统的双层逆最优控制框架，基于等式约束的最优控制模型来重构问题。

Result: 在平面到达任务的数值实验中，该方法对噪声表现出很强的鲁棒性，计算时间比传统双层实现减少15倍。

Conclusion: 单级重构方法在保持结果等效性的同时，显著提高了逆最优控制的计算效率和鲁棒性。

Abstract: Inverse optimal control (IOC) allows the retrieval of optimal cost function
weights, or behavioral parameters, from human motion. The literature on IOC
uses methods that are either based on a slow bilevel process or a fast but
noise-sensitive minimization of optimality condition violation. Assuming
equality-constrained optimal control models of human motion, this article
presents a faster but robust approach to solving IOC using a single-level
reformulation of the bilevel method and yields equivalent results. Through
numerical experiments in simulation, we analyze the robustness to noise of the
proposed single-level reformulation to the bilevel IOC formulation with a
human-like planar reaching task that is used across recent studies. The
approach shows resilience to very large levels of noise and reduces the
computation time of the IOC on this task by a factor of 15 when compared to a
classical bilevel implementation.

</details>


### [28] [Validation of collision-free spheres of Stewart-Gough platforms for constant orientations using the Application Programming Interface of a CAD software](https://arxiv.org/abs/2510.08408)
*Bibekananda Patra,Rajeevlochana G. Chittawadigi,Sandipan Bandyopadhyay*

Main category: cs.RO

TL;DR: 提出了一种使用CAD软件API验证6-6 Stewart-Gough平台机械手最大无碰撞球体尺寸的方法，通过自动化更新移动平台位置并检查腿部碰撞来验证CFS安全性。


<details>
  <summary>Details</summary>
Motivation: 需要验证6-6 Stewart-Gough平台机械手在给定移动平台方向下的最大无碰撞球体尺寸，确保机械手在工作空间内安全运行。

Method: 利用CAD软件API自动更新移动平台位置，在CFS表面包围的壳体内采样，检查每个姿态下腿部之间的相互碰撞情况。

Result: 该方法能够验证预计算CFS的安全性，并估计任何空间并联机械手的无碰撞球体。

Conclusion: 所提出的方法有效验证了Stewart-Gough平台机械手的无碰撞球体尺寸，为空间并联机械手的安全分析提供了实用工具。

Abstract: This paper presents a method of validation of the size of the largest
collision-free sphere (CFS) of a 6-6 Stewart-Gough platform manipulator (SGPM)
for a given orientation of its moving platform (MP) using the Application
Programming Interface (API) of a CAD software. The position of the MP is
updated via the API in an automated manner over a set of samples within a shell
enclosing the surface of the CFS. For each pose of the manipulator, each pair
of legs is investigated for mutual collisions. The CFS is considered safe or
validated iff none of the points falling inside the CFS lead to a collision
between any pair of legs. This approach can not only validate the safety of a
precomputed CFS, but also estimate the same for any spatial parallel
manipulator.

</details>


### [29] [Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered](https://arxiv.org/abs/2510.08464)
*Jason Jabbour,Dong-Ki Kim,Max Smith,Jay Patrikar,Radhika Ghosal,Youhui Wang,Ali Agha,Vijay Janapa Reddi,Shayegan Omidshafiei*

Main category: cs.RO

TL;DR: GLUESTICK是一种后剪枝恢复方法，通过在权重空间对密集和剪枝模型进行一次性插值计算校正项，恢复VLA模型剪枝后丢失的功能，同时保持稀疏性优势。


<details>
  <summary>Details</summary>
Motivation: VLA模型在资源受限硬件上部署困难，现有剪枝方法在机器人应用中会导致性能急剧下降和安全违规增加。

Method: 在权重空间对密集和剪枝模型进行一次性插值计算校正项，推理时每个剪枝层使用该校正恢复丢失能力，无需额外训练。

Result: 在多种VLA架构和操作导航任务中，GLUESTICK实现了竞争性的内存效率，同时显著恢复成功率并减少安全违规。

Conclusion: GLUESTICK提供了一种无需训练、与剪枝算法无关的高效恢复方法，在效率和准确性之间实现良好平衡。

Abstract: Vision-Language-Action (VLA) models have advanced robotic capabilities but
remain challenging to deploy on resource-limited hardware. Pruning has enabled
efficient compression of large language models (LLMs), yet it is largely
understudied in robotics. Surprisingly, we observe that pruning VLA models
leads to drastic degradation and increased safety violations. We introduce
GLUESTICK, a post-pruning recovery method that restores much of the original
model's functionality while retaining sparsity benefits. Our method performs a
one-time interpolation between the dense and pruned models in weight-space to
compute a corrective term. This correction is used during inference by each
pruned layer to recover lost capabilities with minimal overhead. GLUESTICK
requires no additional training, is agnostic to the pruning algorithm, and
introduces a single hyperparameter that controls the tradeoff between
efficiency and accuracy. Across diverse VLA architectures and tasks in
manipulation and navigation, GLUESTICK achieves competitive memory efficiency
while substantially recovering success rates and reducing safety violations.
Additional material can be found at: https://gluestick-vla.github.io/.

</details>


### [30] [DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos](https://arxiv.org/abs/2510.08475)
*Jhen Hsieh,Kuan-Hsun Tu,Kuo-Han Hung,Tsung-Wei Ke*

Main category: cs.RO

TL;DR: DexMan是一个自动化框架，可将人类视觉演示转换为仿真中人形机器人的双手灵巧操作技能，无需相机校准、深度传感器或3D对象资产。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只考虑简化的浮动手，且需要大量手动数据收集和昂贵的动作捕捉设备。DexMan旨在直接从第三方人类操作视频中学习，降低数据收集成本。

Method: 直接从人类操作视频中估计手-物体姿态，使用基于接触的奖励来改进策略学习，直接控制人形机器人进行双手操作。

Result: 在TACO基准测试中实现最先进的物体姿态估计性能（ADD-S和VSD分别提升0.08和0.12），在OakInk-v2上的成功率比先前方法提高19%。

Conclusion: DexMan能够从真实和合成视频中生成技能，无需手动数据收集和昂贵的动作捕捉，为训练通用灵巧操作创建大规模多样化数据集提供了可能。

Abstract: We present DexMan, an automated framework that converts human visual
demonstrations into bimanual dexterous manipulation skills for humanoid robots
in simulation. Operating directly on third-person videos of humans manipulating
rigid objects, DexMan eliminates the need for camera calibration, depth
sensors, scanned 3D object assets, or ground-truth hand and object motion
annotations. Unlike prior approaches that consider only simplified floating
hands, it directly controls a humanoid robot and leverages novel contact-based
rewards to improve policy learning from noisy hand-object poses estimated from
in-the-wild videos.
  DexMan achieves state-of-the-art performance in object pose estimation on the
TACO benchmark, with absolute gains of 0.08 and 0.12 in ADD-S and VSD.
Meanwhile, its reinforcement learning policy surpasses previous methods by 19%
in success rate on OakInk-v2. Furthermore, DexMan can generate skills from both
real and synthetic videos, without the need for manual data collection and
costly motion capture, and enabling the creation of large-scale, diverse
datasets for training generalist dexterous manipulation.

</details>


### [31] [R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation](https://arxiv.org/abs/2510.08547)
*Xiuwei Xu,Angyuan Ma,Hankun Li,Bingyao Yu,Zheng Zhu,Jie Zhou,Jiwen Lu*

Main category: cs.RO

TL;DR: 提出R2RGen框架，通过直接增强点云观测-动作对来生成真实世界数据，实现高效的数据增强，提升机器人操作策略的空间泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中空间泛化问题，现有方法存在显著的模拟到现实差距，且局限于固定基座和预定义相机视角等约束场景。

Method: 采用真实到真实的3D数据生成框架，包含场景和轨迹的细粒度解析标注机制、处理多对象组合和任务约束的分组增强策略，以及相机感知处理。

Result: R2RGen在大量实验中显著提升了数据效率，并展示了在移动操作中扩展和应用的强大潜力。

Conclusion: R2RGen作为无模拟器和渲染的即插即用框架，有效解决了空间泛化问题，为机器人操作提供了高效的数据增强方案。

Abstract: Towards the aim of generalized robotic manipulation, spatial generalization
is the most fundamental capability that requires the policy to work robustly
under different spatial distribution of objects, environment and agent itself.
To achieve this, substantial human demonstrations need to be collected to cover
different spatial configurations for training a generalized visuomotor policy
via imitation learning. Prior works explore a promising direction that
leverages data generation to acquire abundant spatially diverse data from
minimal source demonstrations. However, most approaches face significant
sim-to-real gap and are often limited to constrained settings, such as
fixed-base scenarios and predefined camera viewpoints. In this paper, we
propose a real-to-real 3D data generation framework (R2RGen) that directly
augments the pointcloud observation-action pairs to generate real-world data.
R2RGen is simulator- and rendering-free, thus being efficient and
plug-and-play. Specifically, given a single source demonstration, we introduce
an annotation mechanism for fine-grained parsing of scene and trajectory. A
group-wise augmentation strategy is proposed to handle complex multi-object
compositions and diverse task constraints. We further present camera-aware
processing to align the distribution of generated data with real-world 3D
sensor. Empirically, R2RGen substantially enhances data efficiency on extensive
experiments and demonstrates strong potential for scaling and application on
mobile manipulation.

</details>


### [32] [DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model](https://arxiv.org/abs/2510.08556)
*Xueyi Liu,He Wang,Li Yi*

Main category: cs.RO

TL;DR: 提出一种新颖的sim-to-real框架，通过联合动力学模型解决灵巧操作中的现实差距问题，使单一仿真训练策略能在真实世界中泛化到各种物体和条件。


<details>
  <summary>Details</summary>
Motivation: 灵巧物体旋转在机器人领域仍面临重大挑战，主要由于仿真到现实的策略迁移困难。复杂的接触丰富动力学造成了"现实差距"，限制了先前工作只能处理简单几何形状、有限物体尺寸和长宽比、受限手腕姿态或定制化手部。

Method: 核心方法是联合动力学模型，通过分解关节间动力学、将系统级影响压缩为低维变量，并学习每个关节从其自身动态配置中的演化来有效拟合有限的真实世界数据并相应调整仿真策略动作。配合全自动数据收集策略，以最少人工干预收集多样化的真实世界交互数据。

Result: 完整流水线展示了前所未有的泛化能力：单一策略成功旋转具有复杂形状（如动物）、高长宽比（达5.33）和小尺寸的挑战性物体，同时处理多样手腕方向和旋转轴。全面的真实世界评估和复杂任务的遥操作应用验证了方法的有效性和鲁棒性。

Conclusion: 该方法通过数据高效的联合动力学模型和自主数据收集，成功解决了灵巧操作中的sim-to-real挑战，实现了对广泛物体和条件的泛化能力。

Abstract: Achieving generalized in-hand object rotation remains a significant challenge
in robotics, largely due to the difficulty of transferring policies from
simulation to the real world. The complex, contact-rich dynamics of dexterous
manipulation create a "reality gap" that has limited prior work to constrained
scenarios involving simple geometries, limited object sizes and aspect ratios,
constrained wrist poses, or customized hands. We address this sim-to-real
challenge with a novel framework that enables a single policy, trained in
simulation, to generalize to a wide variety of objects and conditions in the
real world. The core of our method is a joint-wise dynamics model that learns
to bridge the reality gap by effectively fitting limited amount of real-world
collected data and then adapting the sim policy's actions accordingly. The
model is highly data-efficient and generalizable across different whole-hand
interaction distributions by factorizing dynamics across joints, compressing
system-wide influences into low-dimensional variables, and learning each
joint's evolution from its own dynamic profile, implicitly capturing these net
effects. We pair this with a fully autonomous data collection strategy that
gathers diverse, real-world interaction data with minimal human intervention.
Our complete pipeline demonstrates unprecedented generality: a single policy
successfully rotates challenging objects with complex shapes (e.g., animals),
high aspect ratios (up to 5.33), and small sizes, all while handling diverse
wrist orientations and rotation axes. Comprehensive real-world evaluations and
a teleoperation application for complex tasks validate the effectiveness and
robustness of our approach. Website: https://meowuu7.github.io/DexNDM/

</details>


### [33] [NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos](https://arxiv.org/abs/2510.08568)
*Hongyu Li,Lingfeng Sun,Yafei Hu,Duy Ta,Jennifer Barry,George Konidaris,Jiahui Fu*

Main category: cs.RO

TL;DR: NovaFlow是一个零样本机器人操作框架，能够将任务描述转换为可执行计划，无需演示或特定平台训练，支持刚性、关节和可变形物体的操作。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设任务分布内或需要特定平台数据微调，限制了跨平台迁移能力。

Method: 使用视频生成模型合成任务视频，通过感知模块提取3D物体流，为刚性物体计算相对位姿，为可变形物体提供基于粒子动力学模型的跟踪目标。

Result: 在桌面Franka机械臂和Spot四足移动机器人上验证了刚性、关节和可变形物体操作任务的有效零样本执行。

Conclusion: 通过解耦任务理解与底层控制，NovaFlow实现了跨平台的零样本操作能力。

Abstract: Enabling robots to execute novel manipulation tasks zero-shot is a central
goal in robotics. Most existing methods assume in-distribution tasks or rely on
fine-tuning with embodiment-matched data, limiting transfer across platforms.
We present NovaFlow, an autonomous manipulation framework that converts a task
description into an actionable plan for a target robot without any
demonstrations. Given a task description, NovaFlow synthesizes a video using a
video generation model and distills it into 3D actionable object flow using
off-the-shelf perception modules. From the object flow, it computes relative
poses for rigid objects and realizes them as robot actions via grasp proposals
and trajectory optimization. For deformable objects, this flow serves as a
tracking objective for model-based planning with a particle-based dynamics
model. By decoupling task understanding from low-level control, NovaFlow
naturally transfers across embodiments. We validate on rigid, articulated, and
deformable object manipulation tasks using a table-top Franka arm and a Spot
quadrupedal mobile robot, and achieve effective zero-shot execution without
demonstrations or embodiment-specific training. Project website:
https://novaflow.lhy.xyz/.

</details>


### [34] [Scalable Offline Metrics for Autonomous Driving](https://arxiv.org/abs/2510.08571)
*Animikh Aich,Adwait Kulkarni,Eshed Ohn-Bar*

Main category: cs.RO

TL;DR: 本文研究自动驾驶系统中感知规划模型的离线与在线评估差异，发现两者相关性比以往研究更差，提出基于认知不确定性的离线评估指标，显著提升了相关性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统的离线评估虽然安全经济，但难以准确预测在线性能，微小误差可能在测试时导致事故，这种关系在复杂城市驾驶场景中研究不足。

Method: 通过大量模拟实验分析离线与在线评估的相关性，提出基于认知不确定性的离线评估指标，并在真实世界环境中验证其泛化能力。

Result: 发现离线与在线评估的相关性比以往研究更差，提出的新离线指标相比先前指标提升了13%以上的相关性，在真实世界环境中效果更显著。

Conclusion: 当前驾驶策略评估实践和指标的有效性值得怀疑，基于认知不确定性的离线评估指标能更好预测在线性能，为自动驾驶系统评估提供了改进方向。

Abstract: Real-World evaluation of perception-based planning models for robotic
systems, such as autonomous vehicles, can be safely and inexpensively conducted
offline, i.e., by computing model prediction error over a pre-collected
validation dataset with ground-truth annotations. However, extrapolating from
offline model performance to online settings remains a challenge. In these
settings, seemingly minor errors can compound and result in test-time
infractions or collisions. This relationship is understudied, particularly
across diverse closed-loop metrics and complex urban maneuvers. In this work,
we revisit this undervalued question in policy evaluation through an extensive
set of experiments across diverse conditions and metrics. Based on analysis in
simulation, we find an even worse correlation between offline and online
settings than reported by prior studies, casting doubts on the validity of
current evaluation practices and metrics for driving policies. Next, we bridge
the gap between offline and online evaluation. We investigate an offline metric
based on epistemic uncertainty, which aims to capture events that are likely to
cause errors in closed-loop settings. The resulting metric achieves over 13%
improvement in correlation compared to previous offline metrics. We further
validate the generalization of our findings beyond the simulation environment
in real-world settings, where even greater gains are observed.

</details>


### [35] [BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data Generation](https://arxiv.org/abs/2510.08572)
*Rocktim Jyoti Das,Harsh Singh,Diana Turmakhan,Muhammad Abdullah Sohail,Mingfei Han,Preslav Nakov,Fabio Pizzati,Ivan Laptev*

Main category: cs.RO

TL;DR: BLAZER是一个从自动生成的训练数据中学习机器人操作策略的框架，利用LLM规划器的零样本能力在模拟环境中自动生成多样化操作任务的演示，成功示例用于微调LLM以提升规划能力。


<details>
  <summary>Details</summary>
Motivation: 机器人领域缺乏互联网规模的多样化任务演示数据，现有数据集受限于手动收集和整理，需要自动化的数据生成方法来扩展训练规模。

Method: 基于LLM规划器的零样本能力，在模拟器中自动生成多样化操作任务的演示，使用成功示例微调LLM以改进规划能力，无需人工监督。

Result: BLAZER显著改善了模拟和真实环境中的零样本操作能力，在训练任务之外的任务上也有提升，并支持LLM模型的下采样。

Conclusion: BLAZER框架通过自动生成训练数据有效解决了机器人领域数据稀缺问题，实现了从模拟到传感器操作的直接迁移，提升了LLM在机器人操作任务中的性能。

Abstract: Scaling data and models has played a pivotal role in the remarkable progress
of computer vision and language. Inspired by these domains, recent efforts in
robotics have similarly focused on scaling both data and model size to develop
more generalizable and robust policies. However, unlike vision and language,
robotics lacks access to internet-scale demonstrations across diverse robotic
tasks and environments. As a result, the scale of existing datasets typically
suffers from the need for manual data collection and curation. To address this
problem, here we propose BLAZER, a framework that learns manipulation policies
from automatically generated training data. We build on the zero-shot
capabilities of LLM planners and automatically generate demonstrations for
diverse manipulation tasks in simulation. Successful examples are then used to
finetune an LLM and to improve its planning capabilities without human
supervision. Notably, while BLAZER training requires access to the simulator's
state, we demonstrate direct transfer of acquired skills to sensor-based
manipulation. Through extensive experiments, we show BLAZER to significantly
improve zero-shot manipulation in both simulated and real environments.
Moreover, BLAZER improves on tasks outside of its training pool and enables
downscaling of LLM models. Our code and data will be made publicly available on
the project page.

</details>
