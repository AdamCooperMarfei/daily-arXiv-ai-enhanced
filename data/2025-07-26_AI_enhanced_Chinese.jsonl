{"id": "2507.17846", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17846", "abs": "https://arxiv.org/abs/2507.17846", "authors": ["Alison Bartsch", "Arvind Car", "Amir Barati Farimani"], "title": "PinchBot: Long-Horizon Deformable Manipulation with Guided Diffusion Policy", "comment": null, "summary": "Pottery creation is a complicated art form that requires dexterous, precise\nand delicate actions to slowly morph a block of clay to a meaningful, and often\nuseful 3D goal shape. In this work, we aim to create a robotic system that can\ncreate simple pottery goals with only pinch-based actions. This pinch pottery\ntask allows us to explore the challenges of a highly multi-modal and\nlong-horizon deformable manipulation task. To this end, we present PinchBot, a\ngoal-conditioned diffusion policy model that when combined with pre-trained 3D\npoint cloud embeddings, task progress prediction and collision-constrained\naction projection, is able to successfully create a variety of simple pottery\ngoals. For experimental videos and access to the demonstration dataset, please\nvisit our project website:\nhttps://sites.google.com/andrew.cmu.edu/pinchbot/home.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86PinchBot\uff0c\u4e00\u4e2a\u901a\u8fc7\u634f\u5408\u52a8\u4f5c\u5b8c\u6210\u7b80\u5355\u9676\u827a\u4efb\u52a1\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7ed3\u5408\u6269\u6563\u7b56\u7565\u6a21\u578b\u548c\u591a\u6a21\u6001\u6280\u672f\u3002", "motivation": "\u63a2\u7d22\u9ad8\u5ea6\u591a\u6a21\u6001\u548c\u957f\u65f6\u7a0b\u7684\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\uff0c\u5982\u9676\u827a\u5236\u4f5c\u3002", "method": "\u4f7f\u7528\u76ee\u6807\u6761\u4ef6\u6269\u6563\u7b56\u7565\u6a21\u578b\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u76843D\u70b9\u4e91\u5d4c\u5165\u3001\u4efb\u52a1\u8fdb\u5ea6\u9884\u6d4b\u548c\u78b0\u649e\u7ea6\u675f\u52a8\u4f5c\u6295\u5f71\u3002", "result": "PinchBot\u6210\u529f\u5b9e\u73b0\u4e86\u591a\u79cd\u7b80\u5355\u9676\u827a\u76ee\u6807\u7684\u5236\u4f5c\u3002", "conclusion": "PinchBot\u5c55\u793a\u4e86\u5728\u590d\u6742\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.17856", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.17856", "abs": "https://arxiv.org/abs/2507.17856", "authors": ["Dennis Benders", "Laura Ferranti", "Johannes K\u00f6hler"], "title": "A Step-by-step Guide on Nonlinear Model Predictive Control for Safe Mobile Robot Navigation", "comment": "51 pages, 3 figures", "summary": "Designing a Model Predictive Control (MPC) scheme that enables a mobile robot\nto safely navigate through an obstacle-filled environment is a complicated yet\nessential task in robotics. In this technical report, safety refers to ensuring\nthat the robot respects state and input constraints while avoiding collisions\nwith obstacles despite the presence of disturbances and measurement noise. This\nreport offers a step-by-step approach to implementing Nonlinear Model\nPredictive Control (NMPC) schemes addressing these safety requirements.\nNumerous books and survey papers provide comprehensive overviews of linear MPC\n(LMPC) \\cite{bemporad2007robust,kouvaritakis2016model}, NMPC\n\\cite{rawlings2017model,allgower2004nonlinear,mayne2014model,grune2017nonlinear,saltik2018outlook},\nand their applications in various domains, including robotics\n\\cite{nascimento2018nonholonomic,nguyen2021model,shi2021advanced,wei2022mpc}.\nThis report does not aim to replicate those exhaustive reviews. Instead, it\nfocuses specifically on NMPC as a foundation for safe mobile robot navigation.\nThe goal is to provide a practical and accessible path from theoretical\nconcepts to mathematical proofs and implementation, emphasizing safety and\nperformance guarantees. It is intended for researchers, robotics engineers, and\npractitioners seeking to bridge the gap between theoretical NMPC formulations\nand real-world robotic applications.\n  This report is not necessarily meant to remain fixed over time. If someone\nfinds an error in the presented theory, please reach out via the given email\naddresses. We are happy to update the document if necessary.", "AI": {"tldr": "\u8be5\u6280\u672f\u62a5\u544a\u4ecb\u7ecd\u4e86\u5982\u4f55\u901a\u8fc7\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u5b9e\u73b0\u79fb\u52a8\u673a\u5668\u4eba\u5728\u969c\u788d\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5bfc\u822a\uff0c\u5f3a\u8c03\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u7ed3\u5408\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u80fd\u591f\u786e\u4fdd\u79fb\u52a8\u673a\u5668\u4eba\u5728\u5e72\u6270\u548c\u566a\u58f0\u4e0b\u5b89\u5168\u5bfc\u822a\u7684MPC\u65b9\u6848\uff0c\u586b\u8865\u7406\u8bbaNMPC\u4e0e\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u91c7\u7528\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u65b9\u6cd5\uff0c\u9010\u6b65\u5b9e\u73b0\u5b89\u5168\u5bfc\u822a\uff0c\u5e76\u786e\u4fdd\u72b6\u6001\u548c\u8f93\u5165\u7ea6\u675f\u7684\u9075\u5b88\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u4ece\u7406\u8bba\u5230\u5b9e\u8df5\u7684NMPC\u5b9e\u73b0\u8def\u5f84\uff0c\u91cd\u70b9\u5728\u4e8e\u5b89\u5168\u6027\u548c\u6027\u80fd\u4fdd\u8bc1\u3002", "conclusion": "\u62a5\u544a\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5de5\u7a0b\u5e08\u63d0\u4f9b\u5b9e\u7528\u7684NMPC\u5b9e\u73b0\u6307\u5357\uff0c\u5e76\u6b22\u8fce\u53cd\u9988\u4ee5\u6301\u7eed\u6539\u8fdb\u3002"}}
{"id": "2507.18033", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18033", "abs": "https://arxiv.org/abs/2507.18033", "authors": ["Mingfeng Yuan", "Letian Wang", "Steven L. Waslander"], "title": "OpenNav: Open-World Navigation with Multimodal Large Language Models", "comment": null, "summary": "Pre-trained large language models (LLMs) have demonstrated strong\ncommon-sense reasoning abilities, making them promising for robotic navigation\nand planning tasks. However, despite recent progress, bridging the gap between\nlanguage descriptions and actual robot actions in the open-world, beyond merely\ninvoking limited predefined motion primitives, remains an open challenge. In\nthis work, we aim to enable robots to interpret and decompose complex language\ninstructions, ultimately synthesizing a sequence of trajectory points to\ncomplete diverse navigation tasks given open-set instructions and open-set\nobjects. We observe that multi-modal large language models (MLLMs) exhibit\nstrong cross-modal understanding when processing free-form language\ninstructions, demonstrating robust scene comprehension. More importantly,\nleveraging their code-generation capability, MLLMs can interact with\nvision-language perception models to generate compositional 2D bird-eye-view\nvalue maps, effectively integrating semantic knowledge from MLLMs with spatial\ninformation from maps to reinforce the robot's spatial understanding. To\nfurther validate our approach, we effectively leverage large-scale autonomous\nvehicle datasets (AVDs) to validate our proposed zero-shot vision-language\nnavigation framework in outdoor navigation tasks, demonstrating its capability\nto execute a diverse range of free-form natural language navigation\ninstructions while maintaining robustness against object detection errors and\nlinguistic ambiguities. Furthermore, we validate our system on a Husky robot in\nboth indoor and outdoor scenes, demonstrating its real-world robustness and\napplicability. Supplementary videos are available at\nhttps://trailab.github.io/OpenNav-website/", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u63d0\u5347\u673a\u5668\u4eba\u5bfc\u822a\u80fd\u529b\uff0c\u901a\u8fc7\u751f\u6210\u9e1f\u77b0\u56fe\u4ef7\u503c\u5730\u56fe\u5c06\u8bed\u4e49\u77e5\u8bc6\u4e0e\u7a7a\u95f4\u4fe1\u606f\u7ed3\u5408\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u96f6\u6837\u672c\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5e38\u8bc6\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5982\u4f55\u5c06\u5176\u4e0e\u673a\u5668\u4eba\u5b9e\u9645\u52a8\u4f5c\u7ed3\u5408\uff0c\u5c24\u5176\u662f\u5904\u7406\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u590d\u6742\u8bed\u8a00\u6307\u4ee4\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u5229\u7528MLLMs\u7684\u8de8\u6a21\u6001\u7406\u89e3\u548c\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u611f\u77e5\u6a21\u578b\u751f\u6210\u9e1f\u77b0\u56fe\u4ef7\u503c\u5730\u56fe\uff0c\u6574\u5408\u8bed\u4e49\u4e0e\u7a7a\u95f4\u4fe1\u606f\u3002", "result": "\u5728\u6237\u5916\u5bfc\u822a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\u7684\u591a\u6837\u6027\u6307\u4ee4\u6267\u884c\u80fd\u529b\uff0c\u5e76\u5728Husky\u673a\u5668\u4eba\u4e0a\u5c55\u793a\u4e86\u5ba4\u5185\u5916\u573a\u666f\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u590d\u6742\u8bed\u8a00\u6307\u4ee4\u5bfc\u822a\uff0c\u5c55\u73b0\u4e86MLLMs\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.18070", "categories": ["cs.RO", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.18070", "abs": "https://arxiv.org/abs/2507.18070", "authors": ["Behzad Zamani", "Jochen Trumpf", "Chris Manzie"], "title": "Modular Robot and Landmark Localisation Using Relative Bearing Measurements", "comment": "Submitted to RA-L", "summary": "In this paper we propose a modular nonlinear least squares filtering approach\nfor systems composed of independent subsystems. The state and error covariance\nestimate of each subsystem is updated independently, even when a relative\nmeasurement simultaneously depends on the states of multiple subsystems. We\nintegrate the Covariance Intersection (CI) algorithm as part of our solution in\norder to prevent double counting of information when subsystems share estimates\nwith each other. An alternative derivation of the CI algorithm based on least\nsquares estimation makes this integration possible. We particularise the\nproposed approach to the robot-landmark localization problem. In this problem,\nnoisy measurements of the bearing angle to a stationary landmark position\nmeasured relative to the SE(2) pose of a moving robot couple the estimation\nproblems for the robot pose and the landmark position. In a randomized\nsimulation study, we benchmark the proposed modular method against a monolithic\njoint state filter to elucidate their respective trade-offs. In this study we\nalso include variants of the proposed method that achieve a graceful\ndegradation of performance with reduced communication and bandwidth\nrequirements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u7528\u4e8e\u72ec\u7acb\u5b50\u7cfb\u7edf\u7ec4\u6210\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u534f\u65b9\u5dee\u4ea4\u96c6\u7b97\u6cd5\u9632\u6b62\u4fe1\u606f\u91cd\u590d\u8ba1\u7b97\uff0c\u5e76\u5728\u673a\u5668\u4eba-\u5730\u6807\u5b9a\u4f4d\u95ee\u9898\u4e2d\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u5b50\u7cfb\u7edf\u72b6\u6001\u4f30\u8ba1\u4e2d\u4fe1\u606f\u91cd\u590d\u8ba1\u7b97\u7684\u95ee\u9898\uff0c\u540c\u65f6\u964d\u4f4e\u901a\u4fe1\u548c\u5e26\u5bbd\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u7ed3\u5408\u534f\u65b9\u5dee\u4ea4\u96c6\u7b97\u6cd5\uff0c\u72ec\u7acb\u66f4\u65b0\u5404\u5b50\u7cfb\u7edf\u7684\u72b6\u6001\u548c\u8bef\u5dee\u534f\u65b9\u5dee\u4f30\u8ba1\u3002", "result": "\u5728\u673a\u5668\u4eba-\u5730\u6807\u5b9a\u4f4d\u95ee\u9898\u7684\u4eff\u771f\u7814\u7a76\u4e2d\uff0c\u6a21\u5757\u5316\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u8054\u5408\u72b6\u6001\u6ee4\u6ce2\u5668\uff0c\u4e14\u80fd\u4f18\u96c5\u5730\u964d\u4f4e\u901a\u4fe1\u9700\u6c42\u3002", "conclusion": "\u6a21\u5757\u5316\u6ee4\u6ce2\u65b9\u6cd5\u5728\u591a\u5b50\u7cfb\u7edf\u72b6\u6001\u4f30\u8ba1\u4e2d\u6709\u6548\uff0c\u517c\u987e\u6027\u80fd\u4e0e\u901a\u4fe1\u6548\u7387\u3002"}}
{"id": "2507.18138", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18138", "abs": "https://arxiv.org/abs/2507.18138", "authors": ["Min-Gyu Kim", "Dongyun Kang", "Hajun Kim", "Hae-Won Park"], "title": "A Modular Residual Learning Framework to Enhance Model-Based Approach for Robust Locomotion", "comment": "8 pages, IEEE RA-L accepted (July 2025)", "summary": "This paper presents a novel approach that combines the advantages of both\nmodel-based and learning-based frameworks to achieve robust locomotion. The\nresidual modules are integrated with each corresponding part of the model-based\nframework, a footstep planner and dynamic model designed using heuristics, to\ncomplement performance degradation caused by a model mismatch. By utilizing a\nmodular structure and selecting the appropriate learning-based method for each\nresidual module, our framework demonstrates improved control performance in\nenvironments with high uncertainty, while also achieving higher learning\nefficiency compared to baseline methods. Moreover, we observed that our\nproposed methodology not only enhances control performance but also provides\nadditional benefits, such as making nominal controllers more robust to\nparameter tuning. To investigate the feasibility of our framework, we\ndemonstrated residual modules combined with model predictive control in a real\nquadrupedal robot. Despite uncertainties beyond the simulation, the robot\nsuccessfully maintains balance and tracks the commanded velocity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u578b\u548c\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6b8b\u5dee\u6a21\u5757\u63d0\u5347\u9c81\u68d2\u8fd0\u52a8\u63a7\u5236\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6a21\u578b\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u5347\u9ad8\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e0b\u7684\u63a7\u5236\u6027\u80fd\u548c\u5b66\u4e60\u6548\u7387\u3002", "method": "\u5c06\u6b8b\u5dee\u6a21\u5757\u4e0e\u57fa\u4e8e\u6a21\u578b\u7684\u6846\u67b6\uff08\u5982\u6b65\u6001\u89c4\u5212\u548c\u52a8\u6001\u6a21\u578b\uff09\u7ed3\u5408\uff0c\u9009\u62e9\u9002\u5408\u7684\u5b66\u4e60\u65b9\u6cd5\u4f18\u5316\u6a21\u5757\u3002", "result": "\u5728\u771f\u5b9e\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u6210\u529f\u4fdd\u6301\u5e73\u8861\u5e76\u8ddf\u8e2a\u901f\u5ea6\u6307\u4ee4\uff0c\u63d0\u5347\u4e86\u63a7\u5236\u6027\u80fd\u548c\u53c2\u6570\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u7ed3\u5408\u6a21\u578b\u548c\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u9ad8\u4e0d\u786e\u5b9a\u6027\u73af\u5883\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.18160", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18160", "abs": "https://arxiv.org/abs/2507.18160", "authors": ["Luka \u0160iktar", "Branimir \u0106aran", "Bojan \u0160ekoranja", "Marko \u0160vaco"], "title": "Autonomous UAV Navigation for Search and Rescue Missions Using Computer Vision and Convolutional Neural Networks", "comment": "The paper is accepted and presented on the 34th International\n  Conference on Robotics in Alpe-Adria-Danube Region, RAAD 2025, Belgrade\n  Serbia", "summary": "In this paper, we present a subsystem, using Unmanned Aerial Vehicles (UAV),\nfor search and rescue missions, focusing on people detection, face recognition\nand tracking of identified individuals. The proposed solution integrates a UAV\nwith ROS2 framework, that utilizes multiple convolutional neural networks (CNN)\nfor search missions. System identification and PD controller deployment are\nperformed for autonomous UAV navigation. The ROS2 environment utilizes the\nYOLOv11 and YOLOv11-pose CNNs for tracking purposes, and the dlib library CNN\nfor face recognition. The system detects a specific individual, performs face\nrecognition and starts tracking. If the individual is not yet known, the UAV\noperator can manually locate the person, save their facial image and\nimmediately initiate the tracking process. The tracking process relies on\nspecific keypoints identified on the human body using the YOLOv11-pose CNN\nmodel. These keypoints are used to track a specific individual and maintain a\nsafe distance. To enhance accurate tracking, system identification is\nperformed, based on measurement data from the UAVs IMU. The identified system\nparameters are used to design PD controllers that utilize YOLOv11-pose to\nestimate the distance between the UAVs camera and the identified individual.\nThe initial experiments, conducted on 14 known individuals, demonstrated that\nthe proposed subsystem can be successfully used in real time. The next step\ninvolves implementing the system on a large experimental UAV for field use and\nintegrating autonomous navigation with GPS-guided control for rescue operations\nplanning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u673a\uff08UAV\uff09\u7684\u641c\u7d22\u4e0e\u6551\u63f4\u5b50\u7cfb\u7edf\uff0c\u7ed3\u5408ROS2\u6846\u67b6\u548c\u591a\u79cdCNN\u6a21\u578b\uff0c\u5b9e\u73b0\u4eba\u5458\u68c0\u6d4b\u3001\u4eba\u8138\u8bc6\u522b\u548c\u4e2a\u4f53\u8ddf\u8e2a\u3002", "motivation": "\u63d0\u5347\u641c\u7d22\u4e0e\u6551\u63f4\u4efb\u52a1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u73af\u5883\u4e2d\u5feb\u901f\u5b9a\u4f4d\u548c\u8ddf\u8e2a\u7279\u5b9a\u4e2a\u4f53\u3002", "method": "\u96c6\u6210UAV\u4e0eROS2\u6846\u67b6\uff0c\u4f7f\u7528YOLOv11\u548cYOLOv11-pose CNN\u8fdb\u884c\u8ddf\u8e2a\uff0cdlib\u5e93\u8fdb\u884c\u4eba\u8138\u8bc6\u522b\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u8bc6\u522b\u548cPD\u63a7\u5236\u5668\u5b9e\u73b0\u81ea\u4e3b\u5bfc\u822a\u3002", "result": "\u572814\u4e2a\u5df2\u77e5\u4e2a\u4f53\u7684\u5b9e\u9a8c\u4e2d\uff0c\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u5b9e\u65f6\u8ddf\u8e2a\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5177\u5907\u5b9e\u65f6\u5e94\u7528\u6f5c\u529b\uff0c\u4e0b\u4e00\u6b65\u5c06\u8fdb\u884c\u5927\u89c4\u6a21\u65e0\u4eba\u673a\u5b9e\u9a8c\u5e76\u6574\u5408GPS\u5bfc\u822a\u4ee5\u4f18\u5316\u6551\u63f4\u89c4\u5212\u3002"}}
{"id": "2507.18206", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18206", "abs": "https://arxiv.org/abs/2507.18206", "authors": ["Arup Kumar Sahoo", "Itzik Klein"], "title": "MoRPI-PINN: A Physics-Informed Framework for Mobile Robot Pure Inertial Navigation", "comment": "9 pages, 5 figures", "summary": "A fundamental requirement for full autonomy in mobile robots is accurate\nnavigation even in situations where satellite navigation or cameras are\nunavailable. In such practical situations, relying only on inertial sensors\nwill result in navigation solution drift due to the sensors' inherent noise and\nerror terms. One of the emerging solutions to mitigate drift is to maneuver the\nrobot in a snake-like slithering motion to increase the inertial\nsignal-to-noise ratio, allowing the regression of the mobile robot position. In\nthis work, we propose MoRPI-PINN as a physics-informed neural network framework\nfor accurate inertial-based mobile robot navigation. By embedding physical laws\nand constraints into the training process, MoRPI-PINN is capable of providing\nan accurate and robust navigation solution. Using real-world experiments, we\nshow accuracy improvements of over 85% compared to other approaches. MoRPI-PINN\nis a lightweight approach that can be implemented even on edge devices and used\nin any typical mobile robot application.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08MoRPI-PINN\uff09\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u79fb\u52a8\u673a\u5668\u4eba\u5728\u65e0\u536b\u661f\u6216\u6444\u50cf\u5934\u60c5\u51b5\u4e0b\u7684\u60ef\u6027\u5bfc\u822a\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u673a\u5668\u4eba\u5728\u65e0\u536b\u661f\u6216\u6444\u50cf\u5934\u8f85\u52a9\u65f6\uff0c\u4ec5\u4f9d\u8d56\u60ef\u6027\u4f20\u611f\u5668\u5bfc\u81f4\u7684\u5bfc\u822a\u6f02\u79fb\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff08MoRPI-PINN\uff09\uff0c\u901a\u8fc7\u5d4c\u5165\u7269\u7406\u5b9a\u5f8b\u548c\u7ea6\u675f\u6761\u4ef6\u6765\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cMoRPI-PINN\u7684\u5bfc\u822a\u7cbe\u5ea6\u6bd4\u5176\u4ed6\u65b9\u6cd5\u63d0\u9ad8\u4e8685%\u4ee5\u4e0a\u3002", "conclusion": "MoRPI-PINN\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\u3002"}}
{"id": "2507.18248", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18248", "abs": "https://arxiv.org/abs/2507.18248", "authors": ["Ines Frajtag", "Marko \u0160vaco", "Filip \u0160uligoj"], "title": "Evaluation of facial landmark localization performance in a surgical setting", "comment": null, "summary": "The use of robotics, computer vision, and their applications is becoming\nincreasingly widespread in various fields, including medicine. Many face\ndetection algorithms have found applications in neurosurgery, ophthalmology,\nand plastic surgery. A common challenge in using these algorithms is variable\nlighting conditions and the flexibility of detection positions to identify and\nprecisely localize patients. The proposed experiment tests the MediaPipe\nalgorithm for detecting facial landmarks in a controlled setting, using a\nrobotic arm that automatically adjusts positions while the surgical light and\nthe phantom remain in a fixed position. The results of this study demonstrate\nthat the improved accuracy of facial landmark detection under surgical lighting\nsignificantly enhances the detection performance at larger yaw and pitch\nangles. The increase in standard deviation/dispersion occurs due to imprecise\ndetection of selected facial landmarks. This analysis allows for a discussion\non the potential integration of the MediaPipe algorithm into medical\nprocedures.", "AI": {"tldr": "\u8bba\u6587\u6d4b\u8bd5\u4e86MediaPipe\u7b97\u6cd5\u5728\u624b\u672f\u5149\u7167\u4e0b\u68c0\u6d4b\u9762\u90e8\u6807\u5fd7\u70b9\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5176\u5728\u5927\u89d2\u5ea6\u504f\u8f6c\u65f6\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u5b58\u5728\u7cbe\u5ea6\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u9762\u90e8\u68c0\u6d4b\u7b97\u6cd5\u5728\u624b\u672f\u4e2d\u56e0\u5149\u7167\u548c\u4f4d\u7f6e\u53d8\u5316\u5bfc\u81f4\u7684\u68c0\u6d4b\u7cbe\u5ea6\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u673a\u68b0\u81c2\u81ea\u52a8\u8c03\u6574\u4f4d\u7f6e\uff0c\u5728\u56fa\u5b9a\u624b\u672f\u5149\u7167\u548c\u6a21\u578b\u4e0b\u6d4b\u8bd5MediaPipe\u7b97\u6cd5\u7684\u9762\u90e8\u6807\u5fd7\u70b9\u68c0\u6d4b\u3002", "result": "\u7b97\u6cd5\u5728\u5927\u504f\u8f6c\u89d2\u4e0b\u68c0\u6d4b\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u6807\u51c6\u504f\u5dee\u589e\u52a0\uff0c\u8868\u660e\u90e8\u5206\u6807\u5fd7\u70b9\u68c0\u6d4b\u4e0d\u7cbe\u786e\u3002", "conclusion": "MediaPipe\u7b97\u6cd5\u5728\u533b\u7597\u7a0b\u5e8f\u4e2d\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u7cbe\u5ea6\u3002"}}
{"id": "2507.18262", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18262", "abs": "https://arxiv.org/abs/2507.18262", "authors": ["Chenyu Su", "Weiwei Shang", "Chen Qian", "Fei Zhang", "Shuang Cong"], "title": "ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation", "comment": "12 pages,9 figures", "summary": "Semantics-driven 3D spatial constraints align highlevel semantic\nrepresentations with low-level action spaces, facilitating the unification of\ntask understanding and execution in robotic manipulation. The synergistic\nreasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation\nModels (VFMs) enables cross-modal 3D spatial constraint construction.\nNevertheless, existing methods have three key limitations: (1) coarse semantic\ngranularity in constraint modeling, (2) lack of real-time closed-loop planning,\n(3) compromised robustness in semantically diverse environments. To address\nthese challenges, we propose ReSem3D, a unified manipulation framework for\nsemantically diverse environments, leveraging the synergy between VFMs and\nMLLMs to achieve fine-grained visual grounding and dynamically constructs\nhierarchical 3D spatial constraints for real-time manipulation. Specifically,\nthe framework is driven by hierarchical recursive reasoning in MLLMs, which\ninteract with VFMs to automatically construct 3D spatial constraints from\nnatural language instructions and RGB-D observations in two stages: part-level\nextraction and region-level refinement. Subsequently, these constraints are\nencoded as real-time optimization objectives in joint space, enabling reactive\nbehavior to dynamic disturbances. Extensive simulation and real-world\nexperiments are conducted in semantically rich household and sparse chemical\nlab environments. The results demonstrate that ReSem3D performs diverse\nmanipulation tasks under zero-shot conditions, exhibiting strong adaptability\nand generalization. Code and videos at https://resem3d.github.io.", "AI": {"tldr": "ReSem3D\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u8bed\u4e49\u9a71\u52a8\u76843D\u7a7a\u95f4\u7ea6\u675f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u591a\u6837\u6027\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u5efa\u6a21\u7c92\u5ea6\u3001\u5b9e\u65f6\u95ed\u73af\u89c4\u5212\u548c\u73af\u5883\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cReSem3D\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5229\u7528MLLMs\u548cVFMs\u7684\u534f\u540c\u4f5c\u7528\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\uff08\u90e8\u5206\u7ea7\u63d0\u53d6\u548c\u533a\u57df\u7ea7\u7ec6\u5316\uff09\u52a8\u6001\u6784\u5efa3D\u7a7a\u95f4\u7ea6\u675f\uff0c\u5e76\u5c06\u5176\u7f16\u7801\u4e3a\u5b9e\u65f6\u4f18\u5316\u76ee\u6807\u3002", "result": "\u5728\u4e30\u5bcc\u8bed\u4e49\u7684\u5bb6\u5ead\u73af\u5883\u548c\u7a00\u758f\u5316\u5b66\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\uff0cReSem3D\u5728\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ReSem3D\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8bed\u4e49\u9a71\u52a8\u548c\u5b9e\u65f6\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.18276", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18276", "abs": "https://arxiv.org/abs/2507.18276", "authors": ["Xiaojie Zhang", "Yuanfei Wang", "Ruihai Wu", "Kunqi Xu", "Yu Li", "Liuyu Xiang", "Hao Dong", "Zhaofeng He"], "title": "Adaptive Articulated Object Manipulation On The Fly with Foundation Model Reasoning and Part Grounding", "comment": "ICCV 2025", "summary": "Articulated objects pose diverse manipulation challenges for robots. Since\ntheir internal structures are not directly observable, robots must adaptively\nexplore and refine actions to generate successful manipulation trajectories.\nWhile existing works have attempted cross-category generalization in adaptive\narticulated object manipulation, two major challenges persist: (1) the\ngeometric diversity of real-world articulated objects complicates visual\nperception and understanding, and (2) variations in object functions and\nmechanisms hinder the development of a unified adaptive manipulation strategy.\nTo address these challenges, we propose AdaRPG, a novel framework that\nleverages foundation models to extract object parts, which exhibit greater\nlocal geometric similarity than entire objects, thereby enhancing visual\naffordance generalization for functional primitive skills. To support this, we\nconstruct a part-level affordance annotation dataset to train the affordance\nmodel. Additionally, AdaRPG utilizes the common knowledge embedded in\nfoundation models to reason about complex mechanisms and generate high-level\ncontrol codes that invoke primitive skill functions based on part affordance\ninference. Simulation and real-world experiments demonstrate AdaRPG's strong\ngeneralization ability across novel articulated object categories.", "AI": {"tldr": "AdaRPG\u6846\u67b6\u5229\u7528\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u7269\u4f53\u90e8\u4ef6\uff0c\u589e\u5f3a\u89c6\u89c9\u529f\u80fd\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u90e8\u4ef6\u7ea7\u529f\u80fd\u6807\u6ce8\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\uff0c\u5b9e\u73b0\u8de8\u7c7b\u522b\u94f0\u63a5\u7269\u4f53\u64cd\u4f5c\u7684\u6cdb\u5316\u3002", "motivation": "\u89e3\u51b3\u94f0\u63a5\u7269\u4f53\u64cd\u4f5c\u4e2d\u7684\u51e0\u4f55\u591a\u6837\u6027\u548c\u529f\u80fd\u673a\u5236\u5dee\u5f02\u95ee\u9898\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5bf9\u672a\u77e5\u7269\u4f53\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faAdaRPG\u6846\u67b6\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u90e8\u4ef6\u7ea7\u51e0\u4f55\u7279\u5f81\uff0c\u6784\u5efa\u90e8\u4ef6\u7ea7\u529f\u80fd\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u751f\u6210\u9ad8\u7ea7\u63a7\u5236\u4ee3\u7801\u8c03\u7528\u529f\u80fd\u6280\u80fd\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u8868\u660eAdaRPG\u5728\u65b0\u578b\u94f0\u63a5\u7269\u4f53\u7c7b\u522b\u4e0a\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AdaRPG\u901a\u8fc7\u90e8\u4ef6\u7ea7\u529f\u80fd\u63a8\u7406\u548c\u57fa\u7840\u6a21\u578b\u77e5\u8bc6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u94f0\u63a5\u7269\u4f53\u64cd\u4f5c\u7684\u6cdb\u5316\u95ee\u9898\u3002"}}
{"id": "2507.18317", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18317", "abs": "https://arxiv.org/abs/2507.18317", "authors": ["Chenglong Qian", "Yang Xu", "Xiufang Shi", "Jiming Chen", "Liang Li"], "title": "AF-RLIO: Adaptive Fusion of Radar-LiDAR-Inertial Information for Robust Odometry in Challenging Environments", "comment": null, "summary": "In robotic navigation, maintaining precise pose estimation and navigation in\ncomplex and dynamic environments is crucial. However, environmental challenges\nsuch as smoke, tunnels, and adverse weather can significantly degrade the\nperformance of single-sensor systems like LiDAR or GPS, compromising the\noverall stability and safety of autonomous robots. To address these challenges,\nwe propose AF-RLIO: an adaptive fusion approach that integrates 4D\nmillimeter-wave radar, LiDAR, inertial measurement unit (IMU), and GPS to\nleverage the complementary strengths of these sensors for robust odometry\nestimation in complex environments. Our method consists of three key modules.\nFirstly, the pre-processing module utilizes radar data to assist LiDAR in\nremoving dynamic points and determining when environmental conditions are\ndegraded for LiDAR. Secondly, the dynamic-aware multimodal odometry selects\nappropriate point cloud data for scan-to-map matching and tightly couples it\nwith the IMU using the Iterative Error State Kalman Filter. Lastly, the factor\ngraph optimization module balances weights between odometry and GPS data,\nconstructing a pose graph for optimization. The proposed approach has been\nevaluated on datasets and tested in real-world robotic environments,\ndemonstrating its effectiveness and advantages over existing methods in\nchallenging conditions such as smoke and tunnels.", "AI": {"tldr": "AF-RLIO\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u878d\u5408\u65b9\u6cd5\uff0c\u7ed3\u54084D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u3001LiDAR\u3001IMU\u548cGPS\uff0c\u63d0\u5347\u590d\u6742\u73af\u5883\u4e2d\u673a\u5668\u4eba\u5bfc\u822a\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u5355\u4f20\u611f\u5668\u7cfb\u7edf\uff08\u5982LiDAR\u6216GPS\uff09\u5728\u70df\u96fe\u3001\u96a7\u9053\u548c\u6076\u52a3\u5929\u6c14\u7b49\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u5305\u62ec\u9884\u5904\u7406\u6a21\u5757\u3001\u52a8\u6001\u611f\u77e5\u591a\u6a21\u6001\u91cc\u7a0b\u8ba1\u548c\u56e0\u5b50\u56fe\u4f18\u5316\u6a21\u5757\uff0c\u5206\u522b\u5904\u7406\u52a8\u6001\u70b9\u3001\u9009\u62e9\u70b9\u4e91\u6570\u636e\u5e76\u4f18\u5316\u4f4d\u59ff\u56fe\u3002", "result": "\u5728\u6570\u636e\u96c6\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u70df\u96fe\u548c\u96a7\u9053\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AF-RLIO\u901a\u8fc7\u591a\u4f20\u611f\u5668\u878d\u5408\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2507.18344", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18344", "abs": "https://arxiv.org/abs/2507.18344", "authors": ["Gyuhyeon Pak", "Hae Min Cho", "Euntai Kim"], "title": "G2S-ICP SLAM: Geometry-aware Gaussian Splatting ICP SLAM", "comment": "8 pages, 6 figures", "summary": "In this paper, we present a novel geometry-aware RGB-D Gaussian Splatting\nSLAM system, named G2S-ICP SLAM. The proposed method performs high-fidelity 3D\nreconstruction and robust camera pose tracking in real-time by representing\neach scene element using a Gaussian distribution constrained to the local\ntangent plane. This effectively models the local surface as a 2D Gaussian disk\naligned with the underlying geometry, leading to more consistent depth\ninterpretation across multiple viewpoints compared to conventional 3D\nellipsoid-based representations with isotropic uncertainty. To integrate this\nrepresentation into the SLAM pipeline, we embed the surface-aligned Gaussian\ndisks into a Generalized ICP framework by introducing anisotropic covariance\nprior without altering the underlying registration formulation. Furthermore we\npropose a geometry-aware loss that supervises photometric, depth, and normal\nconsistency. Our system achieves real-time operation while preserving both\nvisual and geometric fidelity. Extensive experiments on the Replica and\nTUM-RGBD datasets demonstrate that G2S-ICP SLAM outperforms prior SLAM systems\nin terms of localization accuracy, reconstruction completeness, while\nmaintaining the rendering quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u611f\u77e5\u7684RGB-D\u9ad8\u65af\u6cfc\u6e85SLAM\u7cfb\u7edfG2S-ICP SLAM\uff0c\u901a\u8fc7\u5c40\u90e8\u5207\u5e73\u9762\u7ea6\u675f\u7684\u9ad8\u65af\u5206\u5e03\u5b9e\u73b0\u9ad8\u4fdd\u771f3D\u91cd\u5efa\u548c\u5b9e\u65f6\u76f8\u673a\u4f4d\u59ff\u8ddf\u8e2a\u3002", "motivation": "\u4f20\u7edf3D\u692d\u7403\u8868\u793a\u5728\u5404\u5411\u540c\u6027\u4e0d\u786e\u5b9a\u6027\u4e0b\u591a\u89c6\u89d2\u6df1\u5ea6\u89e3\u91ca\u4e0d\u4e00\u81f4\uff0c\u9700\u6539\u8fdb\u4ee5\u63d0\u5347SLAM\u7cfb\u7edf\u7684\u5b9a\u4f4d\u548c\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u5c40\u90e8\u5207\u5e73\u9762\u5bf9\u9f50\u76842D\u9ad8\u65af\u76d8\u8868\u793a\u573a\u666f\u5143\u7d20\uff0c\u5d4c\u5165\u5e7f\u4e49ICP\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u51e0\u4f55\u611f\u77e5\u635f\u5931\u76d1\u7763\u5149\u5ea6\u3001\u6df1\u5ea6\u548c\u6cd5\u7ebf\u4e00\u81f4\u6027\u3002", "result": "\u5728Replica\u548cTUM-RGBD\u6570\u636e\u96c6\u4e0a\uff0cG2S-ICP SLAM\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u3001\u91cd\u5efa\u5b8c\u6574\u6027\u548c\u6e32\u67d3\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709SLAM\u7cfb\u7edf\u3002", "conclusion": "G2S-ICP SLAM\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u9ad8\u65af\u8868\u793a\u548c\u4f18\u5316\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u9ad8\u4fdd\u771f3D\u91cd\u5efa\u548c\u9c81\u68d2\u76f8\u673a\u8ddf\u8e2a\u3002"}}
{"id": "2507.18396", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.18396", "abs": "https://arxiv.org/abs/2507.18396", "authors": ["Yonghao Fu", "Cheng Hu", "Haokun Xiong", "Zhangpeng Bao", "Wenyuan Du", "Edoardo Ghignone", "Michele Magno", "Lei Xie", "Hongye Su"], "title": "Residual Koopman Model Predictive Control for Enhanced Vehicle Dynamics with Small On-Track Data Input", "comment": null, "summary": "In vehicle trajectory tracking tasks, the simplest approach is the Pure\nPursuit (PP) Control. However, this single-point preview tracking strategy\nfails to consider vehicle model constraints, compromising driving safety. Model\nPredictive Control (MPC) as a widely adopted control method, optimizes control\nactions by incorporating mechanistic models and physical constraints. While its\ncontrol performance critically depends on the accuracy of vehicle modeling.\nTraditional vehicle modeling approaches face inherent trade-offs between\ncapturing nonlinear dynamics and maintaining computational efficiency, often\nresulting in reduced control performance. To address these challenges, this\npaper proposes Residual Koopman Model Predictive Control (RKMPC) framework.\nThis method uses two linear MPC architecture to calculate control inputs: a\nLinear Model Predictive Control (LMPC) computes the baseline control input\nbased on the vehicle kinematic model, and a neural network-based RKMPC\ncalculates the compensation input. The final control command is obtained by\nadding these two components. This design preserves the reliability and\ninterpretability of traditional mechanistic model while achieving performance\noptimization through residual modeling. This method has been validated on the\nCarsim-Matlab joint simulation platform and a physical 1:10 scale F1TENTH\nracing car. Experimental results show that RKMPC requires only 20% of the\ntraining data needed by traditional Koopman Model Predictive Control (KMPC)\nwhile delivering superior tracking performance. Compared to traditional LMPC,\nRKMPC reduces lateral error by 11.7%-22.1%, decreases heading error by\n8.9%-15.8%, and improves front-wheel steering stability by up to 27.6%. The\nimplementation code is available at: https://github.com/ZJU-DDRX/Residual\nKoopman.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6b8b\u5deeKoopman\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08RKMPC\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u8f66\u8f86\u8f68\u8ff9\u8ddf\u8e2a\u4efb\u52a1\uff0c\u7ed3\u5408\u7ebf\u6027MPC\u548c\u795e\u7ecf\u7f51\u7edc\u8865\u507f\u8f93\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfPure Pursuit\u63a7\u5236\u672a\u8003\u8651\u8f66\u8f86\u6a21\u578b\u7ea6\u675f\uff0c\u800cMPC\u4f9d\u8d56\u4e8e\u7cbe\u786e\u7684\u8f66\u8f86\u5efa\u6a21\uff0c\u4f20\u7edf\u5efa\u6a21\u65b9\u6cd5\u5728\u975e\u7ebf\u6027\u52a8\u6001\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "\u91c7\u7528\u53cc\u7ebf\u6027MPC\u67b6\u6784\uff1a\u57fa\u4e8e\u8f66\u8f86\u8fd0\u52a8\u5b66\u6a21\u578b\u7684\u7ebf\u6027MPC\u8ba1\u7b97\u57fa\u7ebf\u8f93\u5165\uff0c\u795e\u7ecf\u7f51\u7edcRKMPC\u8ba1\u7b97\u8865\u507f\u8f93\u5165\uff0c\u4e24\u8005\u76f8\u52a0\u5f97\u5230\u6700\u7ec8\u63a7\u5236\u547d\u4ee4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRKMPC\u4ec5\u9700\u4f20\u7edfKMPC 20%\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6a2a\u5411\u8bef\u5dee\u51cf\u5c1111.7%-22.1%\uff0c\u822a\u5411\u8bef\u5dee\u51cf\u5c118.9%-15.8%\uff0c\u524d\u8f6e\u8f6c\u5411\u7a33\u5b9a\u6027\u63d0\u534727.6%\u3002", "conclusion": "RKMPC\u5728\u4fdd\u6301\u4f20\u7edf\u6a21\u578b\u53ef\u9760\u6027\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u6b8b\u5dee\u5efa\u6a21\u4f18\u5316\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8f66\u8f86\u8f68\u8ff9\u8ddf\u8e2a\u4efb\u52a1\u3002"}}
{"id": "2507.18436", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18436", "abs": "https://arxiv.org/abs/2507.18436", "authors": ["David Blanco-Mulero", "J\u00falia Borr\u00e0s", "Carme Torras"], "title": "Evaluating the Pre-Dressing Step: Unfolding Medical Garments Via Imitation Learning", "comment": "6 pages, 4 figures, 2 tables. Accepted to IEEE/RSJ IROS 2025. Project\n  website: https://sites.google.com/view/pre-dressing", "summary": "Robotic-assisted dressing has the potential to significantly aid both\npatients as well as healthcare personnel, reducing the workload and improving\nthe efficiency in clinical settings. While substantial progress has been made\nin robotic dressing assistance, prior works typically assume that garments are\nalready unfolded and ready for use. However, in medical applications gowns and\naprons are often stored in a folded configuration, requiring an additional\nunfolding step. In this paper, we introduce the pre-dressing step, the process\nof unfolding garments prior to assisted dressing. We leverage imitation\nlearning for learning three manipulation primitives, including both high and\nlow acceleration motions. In addition, we employ a visual classifier to\ncategorise the garment state as closed, partly opened, and fully opened. We\nconduct an empirical evaluation of the learned manipulation primitives as well\nas their combinations. Our results show that highly dynamic motions are not\neffective for unfolding freshly unpacked garments, where the combination of\nmotions can efficiently enhance the opening configuration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u4eba\u8f85\u52a9\u7a7f\u8863\u524d\u7684\u5c55\u5f00\u8863\u7269\u6b65\u9aa4\uff0c\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u89c6\u89c9\u5206\u7c7b\u5668\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540c\u52a8\u4f5c\u7ec4\u5408\u7684\u6548\u679c\u3002", "motivation": "\u533b\u7597\u73af\u5883\u4e2d\u8863\u7269\u901a\u5e38\u4ee5\u6298\u53e0\u72b6\u6001\u5b58\u653e\uff0c\u73b0\u6709\u7814\u7a76\u5047\u8bbe\u8863\u7269\u5df2\u5c55\u5f00\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u5c55\u5f00\u6b65\u9aa4\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u6a21\u4eff\u5b66\u4e60\u5b66\u4e60\u4e09\u79cd\u64cd\u4f5c\u52a8\u4f5c\uff08\u9ad8\u4f4e\u52a0\u901f\u5ea6\uff09\uff0c\u5e76\u4f7f\u7528\u89c6\u89c9\u5206\u7c7b\u5668\u5224\u65ad\u8863\u7269\u72b6\u6001\uff08\u95ed\u5408\u3001\u90e8\u5206\u5c55\u5f00\u3001\u5b8c\u5168\u5c55\u5f00\uff09\u3002", "result": "\u9ad8\u52a8\u6001\u52a8\u4f5c\u5bf9\u5c55\u5f00\u65b0\u8863\u7269\u6548\u679c\u4e0d\u4f73\uff0c\u800c\u52a8\u4f5c\u7ec4\u5408\u80fd\u6709\u6548\u6539\u5584\u5c55\u5f00\u6548\u679c\u3002", "conclusion": "\u52a8\u4f5c\u7ec4\u5408\u5728\u5c55\u5f00\u8863\u7269\u65f6\u66f4\u9ad8\u6548\uff0c\u4e3a\u673a\u5668\u4eba\u8f85\u52a9\u7a7f\u8863\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.18462", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18462", "abs": "https://arxiv.org/abs/2507.18462", "authors": ["Alghalya Al-Hajri", "Ejmen Al-Ubejdij", "Aiman Erbad", "Ali Safa"], "title": "A Novel Monte-Carlo Compressed Sensing and Dictionary Learning Method for the Efficient Path Planning of Remote Sensing Robots", "comment": null, "summary": "In recent years, Compressed Sensing (CS) has gained significant interest as a\ntechnique for acquiring high-resolution sensory data using fewer measurements\nthan traditional Nyquist sampling requires. At the same time, autonomous\nrobotic platforms such as drones and rovers have become increasingly popular\ntools for remote sensing and environmental monitoring tasks, including\nmeasurements of temperature, humidity, and air quality. Within this context,\nthis paper presents, to the best of our knowledge, the first investigation into\nhow the structure of CS measurement matrices can be exploited to design\noptimized sampling trajectories for robotic environmental data collection. We\npropose a novel Monte Carlo optimization framework that generates measurement\nmatrices designed to minimize both the robot's traversal path length and the\nsignal reconstruction error within the CS framework. Central to our approach is\nthe application of Dictionary Learning (DL) to obtain a data-driven sparsifying\ntransform, which enhances reconstruction accuracy while further reducing the\nnumber of samples that the robot needs to collect. We demonstrate the\neffectiveness of our method through experiments reconstructing $NO_2$ pollution\nmaps over the Gulf region. The results indicate that our approach can reduce\nrobot travel distance to less than $10\\%$ of a full-coverage path, while\nimproving reconstruction accuracy by over a factor of five compared to\ntraditional CS methods based on DCT and polynomial dictionaries, as well as by\na factor of two compared to previously-proposed Informative Path Planning (IPP)\nmethods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u538b\u7f29\u611f\u77e5\uff08CS\uff09\u6d4b\u91cf\u77e9\u9635\u7ed3\u6784\u4f18\u5316\u673a\u5668\u4eba\u73af\u5883\u6570\u636e\u91c7\u6837\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u4f18\u5316\u548c\u5b57\u5178\u5b66\u4e60\uff08DL\uff09\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u673a\u5668\u4eba\u8def\u5f84\u957f\u5ea6\u5e76\u63d0\u9ad8\u4e86\u4fe1\u53f7\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u538b\u7f29\u611f\u77e5\uff08CS\uff09\u5728\u51cf\u5c11\u91c7\u6837\u9700\u6c42\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u800c\u673a\u5668\u4eba\u5e73\u53f0\u5728\u73af\u5883\u76d1\u6d4b\u4e2d\u65e5\u76ca\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u4e24\u8005\uff0c\u4f18\u5316\u91c7\u6837\u8f68\u8ff9\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u8499\u7279\u5361\u6d1b\u4f18\u5316\u6846\u67b6\uff0c\u751f\u6210\u4f18\u5316\u7684CS\u6d4b\u91cf\u77e9\u9635\uff0c\u7ed3\u5408\u5b57\u5178\u5b66\u4e60\uff08DL\uff09\u63d0\u5347\u7a00\u758f\u53d8\u6362\uff0c\u51cf\u5c11\u91c7\u6837\u9700\u6c42\u5e76\u63d0\u9ad8\u91cd\u5efa\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u673a\u5668\u4eba\u8def\u5f84\u957f\u5ea6\u51cf\u5c11\u81f3\u5168\u8986\u76d6\u8def\u5f84\u768410%\u4ee5\u4e0b\uff0c\u91cd\u5efa\u7cbe\u5ea6\u6bd4\u4f20\u7edfCS\u65b9\u6cd5\u63d0\u9ad8\u4e94\u500d\uff0c\u6bd4\u73b0\u6709\u4fe1\u606f\u8def\u5f84\u89c4\u5212\uff08IPP\uff09\u65b9\u6cd5\u63d0\u9ad8\u4e24\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u73af\u5883\u6570\u636e\u91c7\u96c6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408CS\u548cDL\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.18502", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18502", "abs": "https://arxiv.org/abs/2507.18502", "authors": ["Sait Sovukluk", "Grazia Zambella", "Tobias Egle", "Christian Ott"], "title": "Experimental Comparison of Whole-Body Control Formulations for Humanoid Robots in Task Acceleration and Task Force Spaces", "comment": "This paper has been accepted for publication in 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025). -\n  Link to video: https://youtu.be/Nfm50ycz-FU", "summary": "This paper studies the experimental comparison of two different whole-body\ncontrol formulations for humanoid robots: inverse dynamics whole-body control\n(ID-WBC) and passivity-based whole-body control (PB-WBC). The two controllers\nfundamentally differ from each other as the first is formulated in task\nacceleration space and the latter is in task force space with passivity\nconsiderations. Even though both control methods predict stability under ideal\nconditions in closed-loop dynamics, their robustness against joint friction,\nsensor noise, unmodeled external disturbances, and non-perfect contact\nconditions is not evident. Therefore, we analyze and experimentally compare the\ntwo controllers on a humanoid robot platform through swing foot position and\norientation control, squatting with and without unmodeled additional weights,\nand jumping. We also relate the observed performance and characteristic\ndifferences with the controller formulations and highlight each controller's\nadvantages and disadvantages.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6bd4\u4e86\u4e24\u79cd\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u65b9\u6cd5\uff08ID-WBC\u548cPB-WBC\uff09\u7684\u5b9e\u9a8c\u6027\u80fd\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u5728\u975e\u7406\u60f3\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u4e24\u79cd\u63a7\u5236\u65b9\u6cd5\u5728\u5173\u8282\u6469\u64e6\u3001\u4f20\u611f\u5668\u566a\u58f0\u7b49\u975e\u7406\u60f3\u6761\u4ef6\u4e0b\u7684\u5b9e\u9645\u8868\u73b0\uff0c\u4ee5\u9a8c\u8bc1\u5176\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e24\u79cd\u63a7\u5236\u5668\u5728\u4eba\u5f62\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u8868\u73b0\uff0c\u5305\u62ec\u6446\u52a8\u811a\u4f4d\u7f6e\u63a7\u5236\u3001\u4e0b\u8e72\u548c\u8df3\u8dc3\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u4e24\u79cd\u63a7\u5236\u5668\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u5206\u6790\u4e86\u5176\u4f18\u52a3\u52bf\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9009\u62e9\u9002\u5408\u7279\u5b9a\u4efb\u52a1\u7684\u63a7\u5236\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4f9d\u636e\uff0c\u5e76\u6307\u51fa\u4e86\u6bcf\u79cd\u65b9\u6cd5\u7684\u9002\u7528\u573a\u666f\u3002"}}
