<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 17]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [In-Context Policy Adaptation via Cross-Domain Skill Diffusion](https://arxiv.org/abs/2509.04535)
*Minjong Yoo,Woo Kyung Kim,Honguk Woo*

Main category: cs.RO

TL;DR: 提出了ICPAD框架，使用扩散式技能学习实现跨域快速策略适应，无需模型更新且只需少量目标域数据


<details>
  <summary>Details</summary>
Motivation: 解决长时程多任务环境中策略跨域适应的挑战，特别是在模型无法更新且目标域数据有限的情况下

Method: 采用跨域技能扩散方案，学习领域无关的原型技能和领域基础技能适配器，结合动态领域提示机制

Result: 在Metaworld机器人操作和CARLA自动驾驶实验中，在环境动力学、智能体具身性和任务时程等不同跨域配置下均取得优异性能

Conclusion: ICPAD框架通过扩散式技能学习有效实现了跨域策略适应，为有限数据条件下的跨域强化学习提供了有效解决方案

Abstract: In this work, we present an in-context policy adaptation (ICPAD) framework
designed for long-horizon multi-task environments, exploring diffusion-based
skill learning techniques in cross-domain settings. The framework enables rapid
adaptation of skill-based reinforcement learning policies to diverse target
domains, especially under stringent constraints on no model updates and only
limited target domain data. Specifically, the framework employs a cross-domain
skill diffusion scheme, where domain-agnostic prototype skills and a
domain-grounded skill adapter are learned jointly and effectively from an
offline dataset through cross-domain consistent diffusion processes. The
prototype skills act as primitives for common behavior representations of
long-horizon policies, serving as a lingua franca to bridge different domains.
Furthermore, to enhance the in-context adaptation performance, we develop a
dynamic domain prompting scheme that guides the diffusion-based skill adapter
toward better alignment with the target domain. Through experiments with
robotic manipulation in Metaworld and autonomous driving in CARLA, we show that
our $\oursol$ framework achieves superior policy adaptation performance under
limited target domain data conditions for various cross-domain configurations
including differences in environment dynamics, agent embodiment, and task
horizon.

</details>


### [2] [Action Chunking with Transformers for Image-Based Spacecraft Guidance and Control](https://arxiv.org/abs/2509.04628)
*Alejandro Posadas-Nava,Andrea Scorsoglio,Luca Ghilardi,Roberto Furfaro,Richard Linares*

Main category: cs.RO

TL;DR: 使用仅100个专家示范训练的ACT方法，在宇航器GNC任务中实现了更高的准确性、更平滑的控制和更高的样本效率


<details>
  <summary>Details</summary>
Motivation: 解决宇航器导航和控制任务中样本效率低、控制不平滑的问题，通过模仿学习方法从有限的专家数据中学习高性能控制策略

Method: 采用Action Chunking with Transformers (ACT)方法，通过转换器网络将视觉和状态观测映射到推力和转矩命令，仅使用100个专家示范（相当于6300次环境交互）进行训练

Result: ACT方法生成了比训练4000万次交互的定制强化学习基线更平滑、更一致的轨迹，在国际空间站对接任务中实现了更高的准确性和控制性能

Conclusion: ACT方法在宇航器GNC领域表现出艰明的样本效率优势，能够从极其有限的专家数据中学习到高性能控制策略，为空间任务提供了更加高效、可靠的解决方案

Abstract: We present an imitation learning approach for spacecraft guidance,
navigation, and control(GNC) that achieves high performance from limited data.
Using only 100 expert demonstrations, equivalent to 6,300 environment
interactions, our method, which implements Action Chunking with Transformers
(ACT), learns a control policy that maps visual and state observations to
thrust and torque commands. ACT generates smoother, more consistent
trajectories than a meta-reinforcement learning (meta-RL) baseline trained with
40 million interactions. We evaluate ACT on a rendezvous task: in-orbit docking
with the International Space Station (ISS). We show that our approach achieves
greater accuracy, smoother control, and greater sample efficiency.

</details>


### [3] [Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement](https://arxiv.org/abs/2509.04645)
*Kallol Saha,Amber Li,Angela Rodriguez-Izquierdo,Lifan Yu,Ben Eisner,Maxim Likhachev,David Held*

Main category: cs.RO

TL;DR: SPOT是一种混合学习与规划方法，通过搜索点云对象变换序列来解决机器人长时程操作规划问题，无需离散化动作或对象关系


<details>
  <summary>Details</summary>
Motivation: 传统任务规划方法需要将连续状态和动作空间离散化为符号描述，这限制了在真实3D场景中的长时程操作规划能力

Method: 提出SPOT方法：从学习到的建议器中采样候选动作，在部分观测的点云上搜索从初始场景到目标满足场景的变换序列

Result: 在多对象重排任务中，SPOT在仿真和真实环境中都表现出成功的任务规划和执行能力，优于策略学习方法

Conclusion: 搜索式规划对于长时程机器人操作至关重要，SPOT通过结合学习模型和搜索策略，有效解决了高维连续动作空间的规划问题

Abstract: Long-horizon planning for robot manipulation is a challenging problem that
requires reasoning about the effects of a sequence of actions on a physical 3D
scene. While traditional task planning methods are shown to be effective for
long-horizon manipulation, they require discretizing the continuous state and
action space into symbolic descriptions of objects, object relationships, and
actions. Instead, we propose a hybrid learning-and-planning approach that
leverages learned models as domain-specific priors to guide search in
high-dimensional continuous action spaces. We introduce SPOT: Search over Point
cloud Object Transformations, which plans by searching for a sequence of
transformations from an initial scene point cloud to a goal-satisfying point
cloud. SPOT samples candidate actions from learned suggesters that operate on
partially observed point clouds, eliminating the need to discretize actions or
object relationships. We evaluate SPOT on multi-object rearrangement tasks,
reporting task planning success and task execution success in both simulation
and real-world environments. Our experiments show that SPOT generates
successful plans and outperforms a policy-learning approach. We also perform
ablations that highlight the importance of search-based planning.

</details>


### [4] [Surformer v2: A Multimodal Classifier for Surface Understanding from Touch and Vision](https://arxiv.org/abs/2509.04658)
*Manish Kansana,Sindhuja Penchala,Shahram Rahimi,Noorbakhsh Amiri Golilarz*

Main category: cs.RO

TL;DR: Surformer v2是一个改进的多模态表面材料分类架构，采用决策级融合机制整合视觉和触觉信息，在Touch and Go数据集上表现良好，适合实时机器人应用。


<details>
  <summary>Details</summary>
Motivation: 提升机器人触觉感知能力，通过多模态融合改善表面材料分类性能，为机器人操作和交互提供更好的感知支持。

Method: 基于Surformer v1框架改进，采用决策级融合机制：视觉分支使用CNN分类器(Efficient V-Net)，触觉分支使用仅编码器transformer模型，通过可学习的加权和组合输出logits进行融合。

Result: 在Touch and Go多模态数据集上表现良好，保持有竞争力的推理速度，适合实时机器人应用场景。

Conclusion: 决策级融合和基于transformer的触觉建模能有效增强多模态机器人感知中的表面理解能力。

Abstract: Multimodal surface material classification plays a critical role in advancing
tactile perception for robotic manipulation and interaction. In this paper, we
present Surformer v2, an enhanced multi-modal classification architecture
designed to integrate visual and tactile sensory streams through a
late(decision level) fusion mechanism. Building on our earlier Surformer v1
framework [1], which employed handcrafted feature extraction followed by
mid-level fusion architecture with multi-head cross-attention layers, Surformer
v2 integrates the feature extraction process within the model itself and shifts
to late fusion. The vision branch leverages a CNN-based classifier(Efficient
V-Net), while the tactile branch employs an encoder-only transformer model,
allowing each modality to extract modality-specific features optimized for
classification. Rather than merging feature maps, the model performs
decision-level fusion by combining the output logits using a learnable weighted
sum, enabling adaptive emphasis on each modality depending on data context and
training dynamics. We evaluate Surformer v2 on the Touch and Go dataset [2], a
multi-modal benchmark comprising surface images and corresponding tactile
sensor readings. Our results demonstrate that Surformer v2 performs well,
maintaining competitive inference speed, suitable for real-time robotic
applications. These findings underscore the effectiveness of decision-level
fusion and transformer-based tactile modeling for enhancing surface
understanding in multi-modal robotic perception.

</details>


### [5] [Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous Driving](https://arxiv.org/abs/2509.04712)
*Zhihao Zhang,Chengyang Peng,Ekim Yurtsever,Keith A. Redmill*

Main category: cs.RO

TL;DR: 提出了一种使用基于规则的演示策略来指导强化学习自动驾驶代理的方法，通过集成车道变换控制器与SAC算法来提高探索和学习效率


<details>
  <summary>Details</summary>
Motivation: 强化学习在自动驾驶控制中面临样本效率低和探索困难的问题，难以发现最优驾驶策略

Method: 将基于规则的车道变换控制器与Soft Actor Critic (SAC)算法集成，通过演示策略指导RL代理

Result: 该方法展示了改进的驾驶性能，并且可以扩展到其他能从演示指导中受益的驾驶场景

Conclusion: 即使不是高度优化的专家级控制器，演示策略也能有效提升RL驾驶代理的学习效果和探索能力

Abstract: Automated vehicle control using reinforcement learning (RL) has attracted
significant attention due to its potential to learn driving policies through
environment interaction. However, RL agents often face training challenges in
sample efficiency and effective exploration, making it difficult to discover an
optimal driving strategy. To address these issues, we propose guiding the RL
driving agent with a demonstration policy that need not be a highly optimized
or expert-level controller. Specifically, we integrate a rule-based lane change
controller with the Soft Actor Critic (SAC) algorithm to enhance exploration
and learning efficiency. Our approach demonstrates improved driving performance
and can be extended to other driving scenarios that can similarly benefit from
demonstration-based guidance.

</details>


### [6] [Hierarchical Reduced-Order Model Predictive Control for Robust Locomotion on Humanoid Robots](https://arxiv.org/abs/2509.04722)
*Adrian B. Ghansah,Sergio A. Esteban,Aaron D. Ames*

Main category: cs.RO

TL;DR: 基于降阶模型的层次控制框架，通过非线性MPC优化步长、步期和蹄关扭矩，并结合手臂和胸部动力学提升行走稳定性


<details>
  <summary>Details</summary>
Motivation: 确保人形机器人在多样环境中具有稳健的行走能力，需要高效计算的控制框架来实现多样化步行规划和更好的稳定性控制

Method: 使用ALIP模型的步到步动力学进行高层非线性MPC优化，通过线性MPC框架扩展SRB-MPC以包含简化的手臂和胸部动力学

Result: 在Unitree G1人形机器上验证，高层步行规划器40Hz运行，中层MPC以500Hz运行。适应性步行时间提高36%的推力恢复成功率，上身控制改善了航向干扰拒止能力，在草地、石板路和不平底垫等多样地形上实现稳健行走

Conclusion: 该层次控制框架通过结合降阶模型和上身动力学，实现了高效计算和稳健的人形机器人多样地形行走能力

Abstract: As humanoid robots enter real-world environments, ensuring robust locomotion
across diverse environments is crucial. This paper presents a computationally
efficient hierarchical control framework for humanoid robot locomotion based on
reduced-order models -- enabling versatile step planning and incorporating arm
and torso dynamics to better stabilize the walking. At the high level, we use
the step-to-step dynamics of the ALIP model to simultaneously optimize over
step periods, step lengths, and ankle torques via nonlinear MPC. The ALIP
trajectories are used as references to a linear MPC framework that extends the
standard SRB-MPC to also include simplified arm and torso dynamics. We validate
the performance of our approach through simulation and hardware experiments on
the Unitree G1 humanoid robot. In the proposed framework the high-level step
planner runs at 40 Hz and the mid-level MPC at 500 Hz using the onboard
mini-PC. Adaptive step timing increased the push recovery success rate by 36%,
and the upper body control improved the yaw disturbance rejection. We also
demonstrate robust locomotion across diverse indoor and outdoor terrains,
including grass, stone pavement, and uneven gym mats.

</details>


### [7] [Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics](https://arxiv.org/abs/2509.04737)
*Ryoga Oishi,Sho Sakaino,Toshiaki Tsuji*

Main category: cs.RO

TL;DR: 提出了一种基于语言指令的机器人动作生成模型，能够在线调整运动以适应人类的行为条件指令


<details>
  <summary>Details</summary>
Motivation: 在机器人学习中，通过语言指令协调动作变得越来越可行，但人类指令通常是定性的，需要探索满足不同条件的行为，现有方法难以在执行过程中适应指令变化

Method: 通过将演示分割成短序列，为特定修饰符类型分配弱监督标签，学习从修饰符指令到动作的映射

Result: 在擦拭和抓取放置任务中评估，结果显示该方法能够在线响应修饰符指令调整运动，而传统的批处理方法无法在执行过程中适应

Conclusion: 该方法成功实现了根据人类指令在线调整机器人动作的能力，解决了传统方法无法在执行过程中适应指令变化的问题

Abstract: In the field of robot learning, coordinating robot actions through language
instructions is becoming increasingly feasible. However, adapting actions to
human instructions remains challenging, as such instructions are often
qualitative and require exploring behaviors that satisfy varying conditions.
This paper proposes a motion generation model that adapts robot actions in
response to modifier directives human instructions imposing behavioral
conditions during task execution. The proposed method learns a mapping from
modifier directives to actions by segmenting demonstrations into short
sequences, assigning weakly supervised labels corresponding to specific
modifier types. We evaluated our method in wiping and pick and place tasks.
Results show that it can adjust motions online in response to modifier
directives, unlike conventional batch-based methods that cannot adapt during
execution.

</details>


### [8] [COMMET: A System for Human-Induced Conflicts in Mobile Manipulation of Everyday Tasks](https://arxiv.org/abs/2509.04836)
*Dongping Li,Shaoting Peng,John Pohovey,Katherine Rose Driggs-Campbell*

Main category: cs.RO

TL;DR: COMMET是一个处理家庭环境中人机冲突的系统，采用混合检测方法结合多模态检索和微调模型推理，利用GPT-4o总结用户偏好，在准确性和延迟方面优于GPT模型。


<details>
  <summary>Details</summary>
Motivation: 随着机器人从工业环境进入日常生活，动态不可预测的人类活动会与人机操作产生冲突，且由于社会属性的复杂性，解决方案需要基于用户个人偏好。

Method: 采用混合检测方法：多模态检索作为初始检测，对低置信度案例使用微调模型推理；利用GPT-4o从相关案例中总结用户偏好；设计用户友好的数据收集界面。

Result: 初步研究表明检测模块在准确性和延迟方面优于GPT模型；设计了有效的实际部署工作流程。

Conclusion: COMMET系统为解决家庭环境中的人机冲突提供了有效方案，通过混合检测和用户偏好学习，为未来家庭机器人研究提供了重要基础。

Abstract: Continuous advancements in robotics and AI are driving the integration of
robots from industry into everyday environments. However, dynamic and
unpredictable human activities in daily lives would directly or indirectly
conflict with robot actions. Besides, due to the social attributes of such
human-induced conflicts, solutions are not always unique and depend highly on
the user's personal preferences. To address these challenges and facilitate the
development of household robots, we propose COMMET, a system for human-induced
COnflicts in Mobile Manipulation of Everyday Tasks. COMMET employs a hybrid
detection approach, which begins with multi-modal retrieval and escalates to
fine-tuned model inference for low-confidence cases. Based on collected user
preferred options and settings, GPT-4o will be used to summarize user
preferences from relevant cases. In preliminary studies, our detection module
shows better accuracy and latency compared with GPT models. To facilitate
future research, we also design a user-friendly interface for user data
collection and demonstrate an effective workflow for real-world deployments.

</details>


### [9] [A Knowledge-Driven Diffusion Policy for End-to-End Autonomous Driving Based on Expert Routing](https://arxiv.org/abs/2509.04853)
*Chengkai Xu,Jiaqi Liu,Yicheng Guo,Peng Hang,Jian Sun*

Main category: cs.RO

TL;DR: KDP是一种基于知识驱动的扩散策略，通过扩散模型生成多模态动作序列，结合稀疏专家混合路由机制实现模块化知识组合，在自动驾驶中取得了更高的成功率和更平滑的控制效果。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶需要生成多模态动作、保持时间稳定性并在多样化场景中泛化，现有方法存在多模态崩溃、长时程一致性差和模块化适应性不足的问题。

Method: 提出KDP方法，结合生成扩散模型和稀疏专家混合路由机制。扩散组件生成时间相干的多模态动作序列，专家路由机制根据上下文激活专业化和可重用的专家。

Result: 在代表性驾驶场景中的广泛实验表明，KDP相比主流范式实现了更高的成功率、更低的碰撞风险和更平滑的控制。消融研究验证了稀疏专家激活和Transformer骨干网络的有效性。

Conclusion: 扩散模型与专家路由相结合为知识驱动的端到端自动驾驶提供了一个可扩展且可解释的范式，专家表现出结构化专业化和跨场景重用特性。

Abstract: End-to-end autonomous driving remains constrained by the need to generate
multi-modal actions, maintain temporal stability, and generalize across diverse
scenarios. Existing methods often collapse multi-modality, struggle with
long-horizon consistency, or lack modular adaptability. This paper presents
KDP, a knowledge-driven diffusion policy that integrates generative diffusion
modeling with a sparse mixture-of-experts routing mechanism. The diffusion
component generates temporally coherent and multi-modal action sequences, while
the expert routing mechanism activates specialized and reusable experts
according to context, enabling modular knowledge composition. Extensive
experiments across representative driving scenarios demonstrate that KDP
achieves consistently higher success rates, reduced collision risk, and
smoother control compared to prevailing paradigms. Ablation studies highlight
the effectiveness of sparse expert activation and the Transformer backbone, and
activation analyses reveal structured specialization and cross-scenario reuse
of experts. These results establish diffusion with expert routing as a scalable
and interpretable paradigm for knowledge-driven end-to-end autonomous driving.

</details>


### [10] [Towards an Accurate and Effective Robot Vision (The Problem of Topological Localization for Mobile Robots)](https://arxiv.org/abs/2509.04948)
*Emanuela Boros*

Main category: cs.RO

TL;DR: 这篇论文系统性评估了多种视觉描述子在办公环境中的顶点定位性能，包括颜色直方图、SIFT、ASIFT、RGB-SIFT和Bag-of-Visual-Words方法，并在ImageCLEF评测中验证了最优配置的效果。


<details>
  <summary>Details</summary>
Motivation: 解决移动机器人在办公环境中的顶点定位问题，应对视觉漏洞、感知模糊性、传感器噪声和照明变化带来的挑战。

Method: 使用单目标色相机获取图像，系统性比较多种视觉描述子、距离测量方法和分类器的性能，包括颜色直方图、SIFT、ASIFT、RGB-SIFT和Bag-of-Visual-Words方法。

Result: 结果显示适当配置外观描述子、相似性测量和分类器具有显著优势，并在ImageCLEF评估中成功识别新图像序列的最可能位置。

Conclusion: 将来工作将探索层次模型、排名方法和特征组合，以建立更稳健的定位系统，减少训练和运行时间，并避免维数灾难，最终实现多样化照明条件下的集成实时定位。

Abstract: Topological localization is a fundamental problem in mobile robotics, since
robots must be able to determine their position in order to accomplish tasks.
Visual localization and place recognition are challenging due to perceptual
ambiguity, sensor noise, and illumination variations. This work addresses
topological localization in an office environment using only images acquired
with a perspective color camera mounted on a robot platform, without relying on
temporal continuity of image sequences. We evaluate state-of-the-art visual
descriptors, including Color Histograms, SIFT, ASIFT, RGB-SIFT, and
Bag-of-Visual-Words approaches inspired by text retrieval. Our contributions
include a systematic, quantitative comparison of these features, distance
measures, and classifiers. Performance was analyzed using standard evaluation
metrics and visualizations, extending previous experiments. Results demonstrate
the advantages of proper configurations of appearance descriptors, similarity
measures, and classifiers. The quality of these configurations was further
validated in the Robot Vision task of the ImageCLEF evaluation campaign, where
the system identified the most likely location of novel image sequences. Future
work will explore hierarchical models, ranking methods, and feature
combinations to build more robust localization systems, reducing training and
runtime while avoiding the curse of dimensionality. Ultimately, this aims
toward integrated, real-time localization across varied illumination and longer
routes.

</details>


### [11] [Ground-Aware Octree-A* Hybrid Path Planning for Memory-Efficient 3D Navigation of Ground Vehicles](https://arxiv.org/abs/2509.04950)
*Byeong-Il Ham,Hyun-Bin Kim,Kyung-Soo Kim*

Main category: cs.RO

TL;DR: 基于八叉树结构的3D A*路径规划算法，通过高度罚分成本函数和节点压缩，实现了更高效的路径规划


<details>
  <summary>Details</summary>
Motivation: 无人地面车辆和腿式机器人需要在复杂地形中进行路径规划，并将障碍物不仅视为要避免的阻碍，而是可以利用的导航辅助工具

Method: 结合八叉树结构的改进垂3D A*算法，在成本函数中加入高度基础的罚分，允许算法利用可穿越障碍物来辅助移动，同时避免不可穿越障碍物

Result: 八叉树结构通过合并高分辨率节点减少了A*算法需要探索的节点数量，显著提高了计算效率和内存使用，支持实时路径规划

Conclusion: 该方法在确保最优路径的同时，显著减少了内存使用和计算时间，适用于实际环境中的实时3D路径规划

Abstract: In this paper, we propose a 3D path planning method that integrates the A*
algorithm with the octree structure. Unmanned Ground Vehicles (UGVs) and legged
robots have been extensively studied, enabling locomotion across a variety of
terrains. Advances in mobility have enabled obstacles to be regarded not only
as hindrances to be avoided, but also as navigational aids when beneficial. A
modified 3D A* algorithm generates an optimal path by leveraging obstacles
during the planning process. By incorporating a height-based penalty into the
cost function, the algorithm enables the use of traversable obstacles to aid
locomotion while avoiding those that are impassable, resulting in more
efficient and realistic path generation. The octree-based 3D grid map achieves
compression by merging high-resolution nodes into larger blocks, especially in
obstacle-free or sparsely populated areas. This reduces the number of nodes
explored by the A* algorithm, thereby improving computational efficiency and
memory usage, and supporting real-time path planning in practical environments.
Benchmark results demonstrate that the use of octree structure ensures an
optimal path while significantly reducing memory usage and computation time.

</details>


### [12] [DeGuV: Depth-Guided Visual Reinforcement Learning for Generalization and Interpretability in Manipulation](https://arxiv.org/abs/2509.04970)
*Tien Pham,Xinyun Chi,Khang Nguyen,Manfred Huber,Angelo Cangelosi*

Main category: cs.RO

TL;DR: DeGuV是一个强化学习框架，通过可学习的掩码网络从深度输入中提取关键视觉信息，结合对比学习和稳定的Q值估计，在提升泛化能力的同时保持样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决RL智能体从视觉输入学习复杂任务时，泛化到新环境的挑战。传统数据增强方法往往牺牲样本效率和训练稳定性。

Method: 使用可学习的掩码网络从深度输入生成掩码，保留关键视觉信息；结合对比学习和稳定的Q值估计；在RL-ViGen基准上使用Franka Emika机器人进行评估。

Result: 在零-shot sim-to-real迁移中表现出色，在泛化能力和样本效率方面均优于最先进方法，同时通过突出显示视觉输入中最相关区域提高了可解释性。

Conclusion: DeGuV框架有效解决了RL视觉泛化问题，在保持样本效率的同时显著提升了泛化性能和训练稳定性，为机器人应用提供了实用的解决方案。

Abstract: Reinforcement learning (RL) agents can learn to solve complex tasks from
visual inputs, but generalizing these learned skills to new environments
remains a major challenge in RL application, especially robotics. While data
augmentation can improve generalization, it often compromises sample efficiency
and training stability. This paper introduces DeGuV, an RL framework that
enhances both generalization and sample efficiency. In specific, we leverage a
learnable masker network that produces a mask from the depth input, preserving
only critical visual information while discarding irrelevant pixels. Through
this, we ensure that our RL agents focus on essential features, improving
robustness under data augmentation. In addition, we incorporate contrastive
learning and stabilize Q-value estimation under augmentation to further enhance
sample efficiency and training stability. We evaluate our proposed method on
the RL-ViGen benchmark using the Franka Emika robot and demonstrate its
effectiveness in zero-shot sim-to-real transfer. Our results show that DeGuV
outperforms state-of-the-art methods in both generalization and sample
efficiency while also improving interpretability by highlighting the most
relevant regions in the visual input

</details>


### [13] [Lyapunov-Based Deep Learning Control for Robots with Unknown Jacobian](https://arxiv.org/abs/2509.04984)
*Koji Matsuno,Chien Chern Cheah*

Main category: cs.RO

TL;DR: 提出了一种基于Lyapunov稳定性分析的端到端深度学习控制框架，解决深度学习黑盒问题，确保机器人运动控制的实时稳定性和安全性


<details>
  <summary>Details</summary>
Motivation: 深度学习在机器人控制应用中存在黑盒问题，缺乏可解释性和稳定性保证，这在需要高可靠性的实时机器人控制中构成重大挑战

Method: 采用模块化学习方法实时更新所有层权重，基于Lyapunov类分析确保系统稳定性，建立可集成到现有机器人控制理论中的理论框架

Result: 在工业机器人上的实验结果表明，所提出的深度学习控制器能够以稳定方式实现实时运动控制，有效解决了深度学习的黑盒问题

Conclusion: 该方法为深度学习在实时机器人应用中的部署提供了关键基础，展示了在保证稳定性前提下实施实时深度学习控制策略的可能性

Abstract: Deep learning, with its exceptional learning capabilities and flexibility,
has been widely applied in various applications. However, its black-box nature
poses a significant challenge in real-time robotic applications, particularly
in robot control, where trustworthiness and robustness are critical in ensuring
safety. In robot motion control, it is essential to analyze and ensure system
stability, necessitating the establishment of methodologies that address this
need. This paper aims to develop a theoretical framework for end-to-end deep
learning control that can be integrated into existing robot control theories.
The proposed control algorithm leverages a modular learning approach to update
the weights of all layers in real time, ensuring system stability based on
Lyapunov-like analysis. Experimental results on industrial robots are presented
to illustrate the performance of the proposed deep learning controller. The
proposed method offers an effective solution to the black-box problem in deep
learning, demonstrating the possibility of deploying real-time deep learning
strategies for robot kinematic control in a stable manner. This achievement
provides a critical foundation for future advancements in deep learning based
real-time robotic applications.

</details>


### [14] [FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies](https://arxiv.org/abs/2509.04996)
*Moritz Reuss,Hongyi Zhou,Marcel Rühle,Ömer Erdinç Yağmurlu,Fabian Otto,Rudolf Lioutikov*

Main category: cs.RO

TL;DR: FLOWER是一个高效的950M参数视觉-语言-动作模型，通过中间模态融合和全局自适应层归一化技术，在减少计算资源的同时实现了与大型模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散的VLA策略需要数十亿参数模型和大量数据集，计算成本和资源需求过高，限制了实际机器人部署。

Method: 采用中间模态融合技术（可剪枝50%的LLM层）和动作特定的Global-AdaLN条件化（减少20%参数），构建高效的950M参数VLA模型。

Result: 在200 H100 GPU小时内完成预训练，在10个仿真和真实世界基准的190个任务中表现优异，在CALVIN ABC基准上达到4.53的新SOTA。

Conclusion: FLOWER证明了通过精心设计的架构优化，可以在显著降低计算资源的同时保持VLA策略的竞争性能，为实际机器人应用提供了可行的解决方案。

Abstract: Developing efficient Vision-Language-Action (VLA) policies is crucial for
practical robotics deployment, yet current approaches face prohibitive
computational costs and resource requirements. Existing diffusion-based VLA
policies require multi-billion-parameter models and massive datasets to achieve
strong performance. We tackle this efficiency challenge with two contributions:
intermediate-modality fusion, which reallocates capacity to the diffusion head
by pruning up to $50\%$ of LLM layers, and action-specific Global-AdaLN
conditioning, which cuts parameters by $20\%$ through modular adaptation. We
integrate these advances into a novel 950 M-parameter VLA called FLOWER.
Pretrained in just 200 H100 GPU hours, FLOWER delivers competitive performance
with bigger VLAs across $190$ tasks spanning ten simulation and real-world
benchmarks and demonstrates robustness across diverse robotic embodiments. In
addition, FLOWER achieves a new SoTA of 4.53 on the CALVIN ABC benchmark.
Demos, code and pretrained weights are available at
https://intuitive-robots.github.io/flower_vla/.

</details>


### [15] [Pointing-Guided Target Estimation via Transformer-Based Attention](https://arxiv.org/abs/2509.05031)
*Luca Müller,Hassan Ali,Philipp Allgeuer,Lukáš Gajdošech,Stefan Wermter*

Main category: cs.RO

TL;DR: 提出MM-ITF多模态转换器架构，通过注意力机制将2D指向手势映射到物体位置，准确预测人类指向意图，实现直观的人机协作。


<details>
  <summary>Details</summary>
Motivation: 指向手势是人类非语言交流的基本形式，在HRI中机器人需要预测人类意图并做出适当响应，但目前缺乏有效的指向意图识别方法。

Method: 提出多模态交互转换器(MM-ITF)模块化架构，利用跨模态注意力机制处理NICOL机器人的桌面场景，将2D指向手势映射到物体位置并计算可能性分数。

Result: 方法能够仅使用单目RGB数据准确预测目标物体，并引入补丁混淆矩阵评估模型在不同候选物体位置的预测性能。

Conclusion: MM-ITF架构实现了直观可访问的人机协作，为基于指向手势的HRI提供了有效解决方案，代码已开源。

Abstract: Deictic gestures, like pointing, are a fundamental form of non-verbal
communication, enabling humans to direct attention to specific objects or
locations. This capability is essential in Human-Robot Interaction (HRI), where
robots should be able to predict human intent and anticipate appropriate
responses. In this work, we propose the Multi-Modality Inter-TransFormer
(MM-ITF), a modular architecture to predict objects in a controlled tabletop
scenario with the NICOL robot, where humans indicate targets through natural
pointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing
gestures to object locations, assigns a likelihood score to each, and
identifies the most likely target. Our results demonstrate that the method can
accurately predict the intended object using monocular RGB data, thus enabling
intuitive and accessible human-robot collaboration. To evaluate the
performance, we introduce a patch confusion matrix, providing insights into the
model's predictions across candidate object locations. Code available at:
https://github.com/lucamuellercode/MMITF.

</details>


### [16] [Shared Autonomy through LLMs and Reinforcement Learning for Applications to Ship Hull Inspections](https://arxiv.org/abs/2509.05042)
*Cristiano Caissutti,Estelle Gerbier,Ehsan Khorrambakht,Paolo Marinelli,Andrea Munafo',Andrea Caiti*

Main category: cs.RO

TL;DR: 该论文提出了一种用于异构海洋机器人舰队的多层次共享自主架构，结合大型语言模型、人在环交互框架和行为树任务管理器，旨在降低操作员认知负荷并提高人机协作的透明度和适应性。


<details>
  <summary>Details</summary>
Motivation: 海洋环境中复杂、高风险和不确定性的特点需要有效的人机协作，共享自主性在机器人系统中具有广阔前景，特别是在异构海洋机器人舰队中需要先进的交互和协调方法。

Method: 采用三种互补方法：(1)集成大型语言模型(LLMs)实现直观的高级任务规范和船体检查任务支持；(2)在多智能体设置中实施人在环交互框架，实现自适应和意图感知协调；(3)开发基于行为树的模块化任务管理器，提供可解释和灵活的任务控制。

Result: 在模拟和真实湖泊环境中的初步结果表明，该多层架构能够降低操作员认知负荷、增强透明度，并改善与人类意图的自适应行为对齐。

Conclusion: 这项研究为安全关键的海事机器人应用建立了模块化和可扩展的基础，用于构建可信赖的人机协作自主系统，未来工作将重点放在组件完全集成、协调机制优化和在港口操作场景中的验证。

Abstract: Shared autonomy is a promising paradigm in robotic systems, particularly
within the maritime domain, where complex, high-risk, and uncertain
environments necessitate effective human-robot collaboration. This paper
investigates the interaction of three complementary approaches to advance
shared autonomy in heterogeneous marine robotic fleets: (i) the integration of
Large Language Models (LLMs) to facilitate intuitive high-level task
specification and support hull inspection missions, (ii) the implementation of
human-in-the-loop interaction frameworks in multi-agent settings to enable
adaptive and intent-aware coordination, and (iii) the development of a modular
Mission Manager based on Behavior Trees to provide interpretable and flexible
mission control. Preliminary results from simulation and real-world lake-like
environments demonstrate the potential of this multi-layered architecture to
reduce operator cognitive load, enhance transparency, and improve adaptive
behaviour alignment with human intent. Ongoing work focuses on fully
integrating these components, refining coordination mechanisms, and validating
the system in operational port scenarios. This study contributes to
establishing a modular and scalable foundation for trustworthy,
human-collaborative autonomy in safety-critical maritime robotics applications.

</details>


### [17] [Robust Model Predictive Control Design for Autonomous Vehicles with Perception-based Observers](https://arxiv.org/abs/2509.05201)
*Nariman Niknejad,Gokul S. Sankar,Bahare Kiumarsi,Hamidreza Modares*

Main category: cs.RO

TL;DR: 提出了一个鲁棒模型预测控制框架，专门处理深度学习感知模块中的非高斯噪声，使用约束zonotopes进行状态估计，并通过线性规划和Minkowski-Lyapunov方法确保稳定性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设感知误差为零均值高斯噪声，但实际深度学习感知模块存在有偏、重尾的非高斯噪声，需要更准确的噪声量化来保证反馈控制的安全性。

Method: 使用约束zonotopes进行基于集合的状态估计，将鲁棒MPC重构为线性规划问题，采用Minkowski-Lyapunov成本函数和松弛变量，通过椭球近似推导最大稳定终端集和反馈增益。

Result: 在全方位移动机器人上的仿真和硬件实验表明，该方法在重尾噪声条件下提供稳定准确的控制性能，在状态估计误差边界和整体控制性能方面显著优于传统高斯噪声设计。

Conclusion: 该感知感知MPC框架有效处理了深度学习感知中的非高斯噪声问题，为安全反馈控制提供了可靠的解决方案，在机器人应用中表现出优越性能。

Abstract: This paper presents a robust model predictive control (MPC) framework that
explicitly addresses the non-Gaussian noise inherent in deep learning-based
perception modules used for state estimation. Recognizing that accurate
uncertainty quantification of the perception module is essential for safe
feedback control, our approach departs from the conventional assumption of
zero-mean noise quantification of the perception error. Instead, it employs
set-based state estimation with constrained zonotopes to capture biased,
heavy-tailed uncertainties while maintaining bounded estimation errors. To
improve computational efficiency, the robust MPC is reformulated as a linear
program (LP), using a Minkowski-Lyapunov-based cost function with an added
slack variable to prevent degenerate solutions. Closed-loop stability is
ensured through Minkowski-Lyapunov inequalities and contractive zonotopic
invariant sets. The largest stabilizing terminal set and its corresponding
feedback gain are then derived via an ellipsoidal approximation of the
zonotopes. The proposed framework is validated through both simulations and
hardware experiments on an omnidirectional mobile robot along with a camera and
a convolutional neural network-based perception module implemented within a
ROS2 framework. The results demonstrate that the perception-aware MPC provides
stable and accurate control performance under heavy-tailed noise conditions,
significantly outperforming traditional Gaussian-noise-based designs in terms
of both state estimation error bounding and overall control performance.

</details>
