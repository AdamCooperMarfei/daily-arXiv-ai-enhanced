{"id": "2508.14096", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14096", "abs": "https://arxiv.org/abs/2508.14096", "authors": ["Zhanxi Xie", "Baili Lu", "Yanzhao Gu", "Zikun Li", "Junhao Wei", "Ngai Cheong"], "title": "Research on UAV Applications in Public Administration: Based on an Improved RRT Algorithm", "comment": null, "summary": "This study investigates the application of unmanned aerial vehicles (UAVs) in\npublic management, focusing on optimizing path planning to address challenges\nsuch as energy consumption, obstacle avoidance, and airspace constraints. As\nUAVs transition from 'technical tools' to 'governance infrastructure', driven\nby advancements in low-altitude economy policies and smart city demands,\nefficient path planning becomes critical. The research proposes an enhanced\nRapidly-exploring Random Tree algorithm (dRRT), incorporating four strategies:\nTarget Bias (to accelerate convergence), Dynamic Step Size (to balance\nexploration and obstacle navigation), Detour Priority (to prioritize horizontal\ndetours over vertical ascents), and B-spline smoothing (to enhance path\nsmoothness). Simulations in a 500 m3 urban environment with randomized\nbuildings demonstrate dRRT's superiority over traditional RRT, A*, and Ant\nColony Optimization (ACO). Results show dRRT achieves a 100\\% success rate with\nan average runtime of 0.01468s, shorter path lengths, fewer waypoints, and\nsmoother trajectories (maximum yaw angles <45{\\deg}). Despite improvements,\nlimitations include increased computational overhead from added mechanisms and\npotential local optima due to goal biasing. The study highlights dRRT's\npotential for efficient UAV deployment in public management scenarios like\nemergency response and traffic monitoring, while underscoring the need for\nintegration with real-time obstacle avoidance frameworks. This work contributes\nto interdisciplinary advancements in urban governance, robotics, and\ncomputational optimization.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u6539\u8fdb\u7684dRRT\u7b97\u6cd5\uff0c\u901a\u8fc7\u56db\u79cd\u7b56\u7565\u4f18\u5316\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\uff0c\u5728500m\u00b3\u57ce\u5e02\u73af\u5883\u4e2d\u76f8\u6bd4\u4f20\u7edf\u7b97\u6cd5\u8868\u73b0\u66f4\u4f18\uff0c\u6210\u529f\u7387\u8fbe100%\uff0c\u5e73\u5747\u8fd0\u884c\u65f6\u95f40.01468\u79d2\uff0c\u8def\u5f84\u66f4\u77ed\u66f4\u5e73\u6ed1\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u4ece'\u6280\u672f\u5de5\u5177'\u5411'\u6cbb\u7406\u57fa\u7840\u8bbe\u65bd'\u8f6c\u578b\uff0c\u4f4e\u7a7a\u7ecf\u6d4e\u653f\u7b56\u548c\u667a\u6167\u57ce\u5e02\u9700\u6c42\u63a8\u52a8\u4e0b\uff0c\u9700\u8981\u89e3\u51b3\u80fd\u8017\u3001\u907f\u969c\u548c\u7a7a\u57df\u9650\u5236\u7b49\u6311\u6218\uff0c\u9ad8\u6548\u8def\u5f84\u89c4\u5212\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u589e\u5f3a\u578b\u5feb\u901f\u63a2\u7d22\u968f\u673a\u6811\u7b97\u6cd5(dRRT)\uff0c\u5305\u542b\u56db\u79cd\u7b56\u7565\uff1a\u76ee\u6807\u504f\u7f6e(\u52a0\u901f\u6536\u655b)\u3001\u52a8\u6001\u6b65\u957f(\u5e73\u8861\u63a2\u7d22\u4e0e\u907f\u969c)\u3001\u7ed5\u884c\u4f18\u5148(\u4f18\u5148\u6c34\u5e73\u7ed5\u884c\u800c\u975e\u5782\u76f4\u722c\u5347)\u3001B\u6837\u6761\u5e73\u6ed1(\u63d0\u5347\u8def\u5f84\u5e73\u6ed1\u5ea6)\u3002", "result": "\u5728\u968f\u673a\u5efa\u7b51\u7269\u7684500m\u00b3\u57ce\u5e02\u73af\u5883\u6a21\u62df\u4e2d\uff0cdRRT\u76f8\u6bd4\u4f20\u7edfRRT\u3001A*\u548c\u8681\u7fa4\u7b97\u6cd5\u8868\u73b0\u66f4\u4f18\uff1a100%\u6210\u529f\u7387\u3001\u5e73\u5747\u8fd0\u884c\u65f6\u95f40.01468\u79d2\u3001\u66f4\u77ed\u8def\u5f84\u3001\u66f4\u5c11\u822a\u70b9\u3001\u66f4\u5e73\u6ed1\u8f68\u8ff9(\u6700\u5927\u504f\u822a\u89d2<45\u5ea6)\u3002", "conclusion": "dRRT\u5728\u516c\u5171\u7ba1\u7406\u573a\u666f\u5982\u5e94\u6025\u54cd\u5e94\u548c\u4ea4\u901a\u76d1\u63a7\u4e2d\u5177\u6709\u9ad8\u6548\u90e8\u7f72\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u589e\u52a0\u548c\u76ee\u6807\u504f\u7f6e\u53ef\u80fd\u5bfc\u81f4\u5c40\u90e8\u6700\u4f18\u7684\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e0e\u5b9e\u65f6\u907f\u969c\u6846\u67b6\u96c6\u6210\u3002\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u57ce\u5e02\u6cbb\u7406\u3001\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u4f18\u5316\u7684\u8de8\u5b66\u79d1\u53d1\u5c55\u3002"}}
{"id": "2508.14098", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14098", "abs": "https://arxiv.org/abs/2508.14098", "authors": ["Pranay Dugar", "Mohitvishnu S. Gadde", "Jonah Siekmann", "Yesh Godse", "Aayam Shrestha", "Alan Fern"], "title": "No More Marching: Learning Humanoid Locomotion for Short-Range SE(2) Targets", "comment": null, "summary": "Humanoids operating in real-world workspaces must frequently execute\ntask-driven, short-range movements to SE(2) target poses. To be practical,\nthese transitions must be fast, robust, and energy efficient. While\nlearning-based locomotion has made significant progress, most existing methods\noptimize for velocity-tracking rather than direct pose reaching, resulting in\ninefficient, marching-style behavior when applied to short-range tasks. In this\nwork, we develop a reinforcement learning approach that directly optimizes\nhumanoid locomotion for SE(2) targets. Central to this approach is a new\nconstellation-based reward function that encourages natural and efficient\ntarget-oriented movement. To evaluate performance, we introduce a benchmarking\nframework that measures energy consumption, time-to-target, and footstep count\non a distribution of SE(2) goals. Our results show that the proposed approach\nconsistently outperforms standard methods and enables successful transfer from\nsimulation to hardware, highlighting the importance of targeted reward design\nfor practical short-range humanoid locomotion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4eba\u5f62\u673a\u5668\u4ebaSE(2)\u76ee\u6807\u4f4d\u59ff\u76f4\u63a5\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u661f\u5ea7\u5f0f\u5956\u52b1\u51fd\u6570\u5b9e\u73b0\u66f4\u81ea\u7136\u9ad8\u6548\u7684\u77ed\u8ddd\u79bb\u79fb\u52a8", "motivation": "\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f18\u5316\u901f\u5ea6\u8ddf\u8e2a\u800c\u975e\u76f4\u63a5\u4f4d\u59ff\u5230\u8fbe\uff0c\u5bfc\u81f4\u77ed\u8ddd\u79bb\u4efb\u52a1\u65f6\u51fa\u73b0\u4f4e\u6548\u7684\"\u884c\u8fdb\u5f0f\"\u884c\u4e3a\uff0c\u9700\u8981\u66f4\u5b9e\u7528\u7684\u77ed\u8ddd\u79bb\u8fd0\u52a8\u65b9\u6848", "method": "\u5f00\u53d1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u91c7\u7528\u65b0\u7684\u661f\u5ea7\u5f0f\u5956\u52b1\u51fd\u6570\u76f4\u63a5\u4f18\u5316\u4eba\u5f62\u673a\u5668\u4eba\u5bf9SE(2)\u76ee\u6807\u7684\u8fd0\u52a8\u63a7\u5236", "result": "\u5728\u80fd\u8017\u3001\u5230\u8fbe\u65f6\u95f4\u548c\u6b65\u6570\u7b49\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u6807\u51c6\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u5b9e\u73b0\u4ece\u4eff\u771f\u5230\u786c\u4ef6\u7684\u8fc1\u79fb", "conclusion": "\u9488\u5bf9\u6027\u7684\u5956\u52b1\u8bbe\u8ba1\u5bf9\u4e8e\u5b9e\u7528\u7684\u77ed\u8ddd\u79bb\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u81f3\u5173\u91cd\u8981"}}
{"id": "2508.14099", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14099", "abs": "https://arxiv.org/abs/2508.14099", "authors": ["Michal Ciebielski", "Victor Dh\u00e9din", "Majid Khadiv"], "title": "Task and Motion Planning for Humanoid Loco-manipulation", "comment": null, "summary": "This work presents an optimization-based task and motion planning (TAMP)\nframework that unifies planning for locomotion and manipulation through a\nshared representation of contact modes. We define symbolic actions as contact\nmode changes, grounding high-level planning in low-level motion. This enables a\nunified search that spans task, contact, and motion planning while\nincorporating whole-body dynamics, as well as all constraints between the\nrobot, the manipulated object, and the environment. Results on a humanoid\nplatform show that our method can generate a broad range of physically\nconsistent loco-manipulation behaviors over long action sequences requiring\ncomplex reasoning. To the best of our knowledge, this is the first work that\nenables the resolution of an integrated TAMP formulation with fully acyclic\nplanning and whole body dynamics with actuation constraints for the humanoid\nloco-manipulation problem.", "AI": {"tldr": "\u57fa\u4e8e\u63a5\u89e6\u6a21\u5f0f\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u7edf\u4e00\u8ba1\u5212\u4eba\u5f62\u673a\u5668\u4eba\u7684\u79fb\u52a8\u548c\u64cd\u4f5c\u4efb\u52a1", "motivation": "\u89e3\u51b3\u4efb\u52a1\u3001\u63a5\u89e6\u548c\u8fd0\u52a8\u89c4\u5212\u7684\u5206\u5272\u95ee\u9898\uff0c\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u79fb\u52a8-\u64cd\u4f5c\u4efb\u52a1\u7684\u7edf\u4e00\u89c4\u5212", "method": "\u901a\u8fc7\u63a5\u89e6\u6a21\u5f0f\u53d8\u66f4\u5b9a\u4e49\u7b26\u53f7\u52a8\u4f5c\uff0c\u5728\u5305\u542b\u5168\u8eab\u52a8\u529b\u5b66\u548c\u7ea6\u675f\u7684\u7edf\u4e00\u641c\u7d22\u7a7a\u95f4\u4e2d\u8fdb\u884c\u89c4\u5212", "result": "\u80fd\u591f\u751f\u6210\u5e7f\u6cdb\u7684\u7269\u7406\u4e00\u81f4\u7684\u79fb\u52a8-\u64cd\u4f5c\u884c\u4e3a\uff0c\u5904\u7406\u9700\u8981\u590d\u6742\u63a8\u7406\u7684\u957f\u5e8f\u5217\u4efb\u52a1", "conclusion": "\u9996\u6b21\u5b9e\u73b0\u4e86\u5305\u542b\u5168\u8eab\u52a8\u529b\u5b66\u548c\u9a71\u52a8\u7ea6\u675f\u7684\u5b8c\u5168\u65e0\u73af\u89c4\u5212\u7684\u4eba\u5f62\u673a\u5668\u4eba\u79fb\u52a8-\u64cd\u4f5c\u95ee\u9898\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.14100", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14100", "abs": "https://arxiv.org/abs/2508.14100", "authors": ["Nilay Kushawaha", "Carlo Alessi", "Lorenzo Fruzzetti", "Egidio Falotico"], "title": "Domain Translation of a Soft Robotic Arm using Conditional Cycle Generative Adversarial Network", "comment": "Accepted at IEEE International Conference on Robotic Systems and\n  Applications", "summary": "Deep learning provides a powerful method for modeling the dynamics of soft\nrobots, offering advantages over traditional analytical approaches that require\nprecise knowledge of the robot's structure, material properties, and other\nphysical characteristics. Given the inherent complexity and non-linearity of\nthese systems, extracting such details can be challenging. The mappings learned\nin one domain cannot be directly transferred to another domain with different\nphysical properties. This challenge is particularly relevant for soft robots,\nas their materials gradually degrade over time. In this paper, we introduce a\ndomain translation framework based on a conditional cycle generative\nadversarial network (CCGAN) to enable knowledge transfer from a source domain\nto a target domain. Specifically, we employ a dynamic learning approach to\nadapt a pose controller trained in a standard simulation environment to a\ndomain with tenfold increased viscosity. Our model learns from input pressure\nsignals conditioned on corresponding end-effector positions and orientations in\nboth domains. We evaluate our approach through trajectory-tracking experiments\nacross five distinct shapes and further assess its robustness under noise\nperturbations and periodicity tests. The results demonstrate that CCGAN-GP\neffectively facilitates cross-domain skill transfer, paving the way for more\nadaptable and generalizable soft robotic controllers.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6761\u4ef6\u5faa\u73af\u751f\u6210\u5bf9\u6297\u7f51\u7edc(CCGAN)\u7684\u9886\u57df\u8f6c\u6362\u6846\u67b6\uff0c\u5b9e\u73b0\u8f6f\u673a\u5668\u4eba\u4ece\u6e90\u9886\u57df\u5230\u76ee\u6807\u9886\u57df\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u89e3\u51b3\u4e86\u6750\u6599\u9000\u5316\u5e26\u6765\u7684\u63a7\u5236\u6311\u6218\u3002", "motivation": "\u8f6f\u673a\u5668\u4eba\u6750\u6599\u968f\u65f6\u95f4\u9010\u6e10\u9000\u5316\uff0c\u4f20\u7edf\u5206\u6790\u65b9\u6cd5\u9700\u8981\u7cbe\u786e\u7684\u7269\u7406\u7279\u6027\u77e5\u8bc6\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4e0d\u540c\u7269\u7406\u7279\u6027\u7684\u9886\u57df\u95f4\u65e0\u6cd5\u76f4\u63a5\u8fc1\u79fb\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u5faa\u73af\u751f\u6210\u5bf9\u6297\u7f51\u7edc(CCGAN)\uff0c\u901a\u8fc7\u52a8\u6001\u5b66\u4e60\u65b9\u6cd5\u5c06\u6807\u51c6\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\u7684\u59ff\u6001\u63a7\u5236\u5668\u9002\u914d\u5230\u7c98\u5ea6\u589e\u52a0\u5341\u500d\u7684\u9886\u57df\uff0c\u6a21\u578b\u5b66\u4e60\u57fa\u4e8e\u672b\u7aef\u6267\u884c\u5668\u4f4d\u7f6e\u548c\u65b9\u5411\u7684\u8f93\u5165\u538b\u529b\u4fe1\u53f7\u3002", "result": "\u5728\u4e94\u79cd\u4e0d\u540c\u5f62\u72b6\u7684\u8f68\u8ff9\u8ddf\u8e2a\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u566a\u58f0\u6270\u52a8\u548c\u5468\u671f\u6027\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "CCGAN-GP\u80fd\u591f\u6709\u6548\u4fc3\u8fdb\u8de8\u9886\u57df\u6280\u80fd\u8fc1\u79fb\uff0c\u4e3a\u5f00\u53d1\u66f4\u5177\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u8f6f\u673a\u5668\u4eba\u63a7\u5236\u5668\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.14105", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14105", "abs": "https://arxiv.org/abs/2508.14105", "authors": ["Jahid Chowdhury Choton", "John Woods", "William Hsu"], "title": "Efficient Environment Design for Multi-Robot Navigation via Continuous Control", "comment": "12 pages, 3 figures, conference", "summary": "Multi-robot navigation and path planning in continuous state and action\nspaces with uncertain environments remains an open challenge. Deep\nReinforcement Learning (RL) is one of the most popular paradigms for solving\nthis task, but its real-world application has been limited due to sample\ninefficiency and long training periods. Moreover, the existing works using RL\nfor multi-robot navigation lack formal guarantees while designing the\nenvironment. In this paper, we introduce an efficient and highly customizable\nenvironment for continuous-control multi-robot navigation, where the robots\nmust visit a set of regions of interest (ROIs) by following the shortest paths.\nThe task is formally modeled as a Markov Decision Process (MDP). We describe\nthe multi-robot navigation task as an optimization problem and relate it to\nfinding an optimal policy for the MDP. We crafted several variations of the\nenvironment and measured the performance using both gradient and non-gradient\nbased RL methods: A2C, PPO, TRPO, TQC, CrossQ and ARS. To show real-world\napplicability, we deployed our environment to a 3-D agricultural field with\nuncertainties using the CoppeliaSim robot simulator and measured the robustness\nby running inference on the learned models. We believe our work will guide the\nresearchers on how to develop MDP-based environments that are applicable to\nreal-world systems and solve them using the existing state-of-the-art RL\nmethods with limited resources and within reasonable time periods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u5b9a\u5236\u7684\u591a\u673a\u5668\u4eba\u5bfc\u822a\u73af\u5883\uff0c\u5c06\u8fde\u7eed\u63a7\u5236\u7684\u591a\u673a\u5668\u4eba\u5bfc\u822a\u4efb\u52a1\u5efa\u6a21\u4e3aMDP\uff0c\u5e76\u4f7f\u7528\u591a\u79cdRL\u65b9\u6cd5\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\uff0c\u57283D\u519c\u4e1a\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5b9e\u9645\u5e94\u7528\u6027\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u5728\u8fde\u7eed\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u5bfc\u822a\u548c\u8def\u5f84\u89c4\u5212\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u6d41\u884c\uff0c\u4f46\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u3001\u8bad\u7ec3\u65f6\u95f4\u957f\u7684\u95ee\u9898\uff0c\u4e14\u73b0\u6709\u5de5\u4f5c\u7f3a\u4e4f\u5f62\u5f0f\u5316\u4fdd\u8bc1\u3002", "method": "\u5c06\u591a\u673a\u5668\u4eba\u5bfc\u822a\u4efb\u52a1\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(MDP)\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9ad8\u6548\u53ef\u5b9a\u5236\u7684\u73af\u5883\uff0c\u673a\u5668\u4eba\u9700\u8981\u6cbf\u6700\u77ed\u8def\u5f84\u8bbf\u95ee\u611f\u5174\u8da3\u533a\u57df\u3002\u4f7f\u7528\u4e86A2C\u3001PPO\u3001TRPO\u3001TQC\u3001CrossQ\u548cARS\u7b49\u68af\u5ea6\u4e0e\u975e\u68af\u5ea6RL\u65b9\u6cd5\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\uff0c\u5e76\u5728CoppeliaSim\u673a\u5668\u4eba\u6a21\u62df\u5668\u76843D\u519c\u4e1a\u573a\u666f\u4e2d\u8fdb\u884c\u90e8\u7f72\u9a8c\u8bc1\u3002", "result": "\u5f00\u53d1\u4e86\u591a\u4e2a\u73af\u5883\u53d8\u4f53\u5e76\u6d4b\u91cf\u4e86\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5728\u6709\u9650\u8d44\u6e90\u548c\u5408\u7406\u65f6\u95f4\u8303\u56f4\u5185\u4f7f\u7528\u73b0\u6709\u6700\u5148\u8fdbRL\u65b9\u6cd5\u89e3\u51b3\u5b9e\u9645\u4e16\u754c\u7cfb\u7edf\u7684\u80fd\u529b\u3002\u901a\u8fc7\u5728\u5b66\u4e60\u6a21\u578b\u4e0a\u8fd0\u884c\u63a8\u7406\u6765\u6d4b\u91cf\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5982\u4f55\u5f00\u53d1\u9002\u7528\u4e8e\u5b9e\u9645\u4e16\u754c\u7cfb\u7edf\u7684MDP\u57fa\u7840\u73af\u5883\uff0c\u5e76\u4f7f\u7528\u73b0\u6709\u6700\u5148\u8fdbRL\u65b9\u6cd5\u5728\u6709\u9650\u8d44\u6e90\u548c\u5408\u7406\u65f6\u95f4\u5185\u89e3\u51b3\u95ee\u9898\u7684\u6307\u5bfc\u3002"}}
{"id": "2508.14120", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14120", "abs": "https://arxiv.org/abs/2508.14120", "authors": ["Yuhang Lin", "Yijia Xie", "Jiahong Xie", "Yuehao Huang", "Ruoyu Wang", "Jiajun Lv", "Yukai Ma", "Xingxing Zuo"], "title": "SimGenHOI: Physically Realistic Whole-Body Humanoid-Object Interaction via Generative Modeling and Reinforcement Learning", "comment": null, "summary": "Generating physically realistic humanoid-object interactions (HOI) is a\nfundamental challenge in robotics. Existing HOI generation approaches, such as\ndiffusion-based models, often suffer from artifacts such as implausible\ncontacts, penetrations, and unrealistic whole-body actions, which hinder\nsuccessful execution in physical environments. To address these challenges, we\nintroduce SimGenHOI, a unified framework that combines the strengths of\ngenerative modeling and reinforcement learning to produce controllable and\nphysically plausible HOI. Our HOI generative model, based on Diffusion\nTransformers (DiT), predicts a set of key actions conditioned on text prompts,\nobject geometry, sparse object waypoints, and the initial humanoid pose. These\nkey actions capture essential interaction dynamics and are interpolated into\nsmooth motion trajectories, naturally supporting long-horizon generation. To\nensure physical realism, we design a contact-aware whole-body control policy\ntrained with reinforcement learning, which tracks the generated motions while\ncorrecting artifacts such as penetration and foot sliding. Furthermore, we\nintroduce a mutual fine-tuning strategy, where the generative model and the\ncontrol policy iteratively refine each other, improving both motion realism and\ntracking robustness. Extensive experiments demonstrate that SimGenHOI generates\nrealistic, diverse, and physically plausible humanoid-object interactions,\nachieving significantly higher tracking success rates in simulation and\nenabling long-horizon manipulation tasks. Code will be released upon acceptance\non our project page: https://xingxingzuo.github.io/simgen_hoi.", "AI": {"tldr": "SimGenHOI\u662f\u4e00\u4e2a\u7ed3\u5408\u751f\u6210\u5efa\u6a21\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u53ef\u63a7\u4e14\u7269\u7406\u5408\u7406\u7684\u4eba\u5f62\u673a\u5668\u4eba-\u7269\u4f53\u4ea4\u4e92\uff0c\u901a\u8fc7\u6269\u6563\u53d8\u6362\u5668\u9884\u6d4b\u5173\u952e\u52a8\u4f5c\uff0c\u5e76\u4f7f\u7528\u63a5\u89e6\u611f\u77e5\u7684\u5168\u8eab\u63a7\u5236\u7b56\u7565\u786e\u4fdd\u7269\u7406\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u5f62\u673a\u5668\u4eba-\u7269\u4f53\u4ea4\u4e92\u751f\u6210\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u6269\u6563\u7684\u6a21\u578b\uff09\u7ecf\u5e38\u5b58\u5728\u4e0d\u5408\u7406\u7684\u63a5\u89e6\u3001\u7a7f\u900f\u548c\u4e0d\u771f\u5b9e\u7684\u5168\u8eab\u52a8\u4f5c\u7b49\u4f2a\u5f71\uff0c\u963b\u788d\u4e86\u5728\u7269\u7406\u73af\u5883\u4e2d\u7684\u6210\u529f\u6267\u884c\u3002", "method": "\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u7684\u751f\u6210\u6a21\u578b\u9884\u6d4b\u5173\u952e\u52a8\u4f5c\uff0c\u7ed3\u5408\u63a5\u89e6\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u7b56\u7565\u8ddf\u8e2a\u751f\u6210\u7684\u8fd0\u52a8\u5e76\u4fee\u6b63\u4f2a\u5f71\uff0c\u91c7\u7528\u76f8\u4e92\u5fae\u8c03\u7b56\u7565\u8fed\u4ee3\u4f18\u5316\u751f\u6210\u6a21\u578b\u548c\u63a7\u5236\u7b56\u7565\u3002", "result": "SimGenHOI\u751f\u6210\u4e86\u771f\u5b9e\u3001\u591a\u6837\u4e14\u7269\u7406\u5408\u7406\u7684\u4eba\u5f62\u673a\u5668\u4eba-\u7269\u4f53\u4ea4\u4e92\uff0c\u5728\u6a21\u62df\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u8ddf\u8e2a\u6210\u529f\u7387\uff0c\u5e76\u652f\u6301\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4eba\u5f62\u673a\u5668\u4eba-\u7269\u4f53\u4ea4\u4e92\u751f\u6210\u4e2d\u7684\u7269\u7406\u771f\u5b9e\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u751f\u6210\u5efa\u6a21\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u751f\u6210\u548c\u63a7\u5236\u3002"}}
{"id": "2508.14185", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.14185", "abs": "https://arxiv.org/abs/2508.14185", "authors": ["Evanns Morales-Cuadrado", "Luke Baird", "Yorai Wardi", "Samuel Coogan"], "title": "Lightweight Tracking Control for Computationally Constrained Aerial Systems with the Newton-Raphson Method", "comment": null, "summary": "We investigate the performance of a lightweight tracking controller, based on\na flow version of the Newton-Raphson method, applied to a miniature blimp and a\nmid-size quadrotor. This tracking technique has been shown to enjoy theoretical\nguarantees of performance and has been applied with success in simulation\nstudies and on mobile robots with simple motion models. This paper investigates\nthe technique through real-world flight experiments on aerial hardware\nplatforms subject to realistic deployment and onboard computational\nconstraints. The technique's performance is assessed in comparison with the\nestablished control frameworks of feedback linearization for the blimp, and\nnonlinear model predictive control for both quadrotor and blimp. The\nperformance metrics under consideration are (i) root mean square error of\nflight trajectories with respect to target trajectories, (ii) algorithms'\ncomputation times, and (iii) CPU energy consumption associated with the control\nalgorithms. The experimental findings show that the Newton-Raphson flow-based\ntracking controller achieves comparable or superior tracking performance to the\nbaseline methods with substantially reduced computation time and energy\nexpenditure.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u57fa\u4e8e\u725b\u987f-\u62c9\u5f17\u68ee\u6d41\u65b9\u6cd5\u7684\u8f7b\u91cf\u7ea7\u8ddf\u8e2a\u63a7\u5236\u5668\u5728\u5c0f\u578b\u98de\u8247\u548c\u4e2d\u578b\u56db\u65cb\u7ffc\u4e0a\u7684\u5b9e\u9645\u6027\u80fd\uff0c\u4e0e\u53cd\u9988\u7ebf\u6027\u5316\u548c\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7b49\u57fa\u51c6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u4fdd\u6301\u76f8\u5f53\u6216\u66f4\u597d\u8ddf\u8e2a\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u65f6\u95f4\u548c\u80fd\u8017\u3002", "motivation": "\u7814\u7a76\u8f7b\u91cf\u7ea7\u8ddf\u8e2a\u63a7\u5236\u5668\u5728\u771f\u5b9e\u7a7a\u4e2d\u786c\u4ef6\u5e73\u53f0\u4e0a\u7684\u6027\u80fd\uff0c\u8fd9\u4e9b\u5e73\u53f0\u9762\u4e34\u5b9e\u9645\u90e8\u7f72\u548c\u673a\u8f7d\u8ba1\u7b97\u7ea6\u675f\uff0c\u9700\u8981\u9a8c\u8bc1\u8be5\u6280\u672f\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u725b\u987f-\u62c9\u5f17\u68ee\u6d41\u65b9\u6cd5\u7684\u8ddf\u8e2a\u63a7\u5236\u5668\uff0c\u5728\u5c0f\u578b\u98de\u8247\u548c\u4e2d\u578b\u56db\u65cb\u7ffc\u4e0a\u8fdb\u884c\u771f\u5b9e\u98de\u884c\u5b9e\u9a8c\uff0c\u4e0e\u53cd\u9988\u7ebf\u6027\u5316\uff08\u98de\u8247\uff09\u548c\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08\u4e24\u8005\uff09\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u725b\u987f-\u62c9\u5f17\u68ee\u6d41\u8ddf\u8e2a\u63a7\u5236\u5668\u5728\u98de\u884c\u8f68\u8ff9\u5747\u65b9\u6839\u8bef\u5dee\u65b9\u9762\u8fbe\u5230\u76f8\u5f53\u6216\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u540c\u65f6\u8ba1\u7b97\u65f6\u95f4\u663e\u8457\u51cf\u5c11\uff0cCPU\u80fd\u8017\u5927\u5e45\u964d\u4f4e\u3002", "conclusion": "\u8be5\u8f7b\u91cf\u7ea7\u8ddf\u8e2a\u63a7\u5236\u5668\u5728\u5b9e\u9645\u7a7a\u4e2d\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u826f\u597d\u8ddf\u8e2a\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u673a\u8f7d\u7cfb\u7edf\u3002"}}
{"id": "2508.14235", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14235", "abs": "https://arxiv.org/abs/2508.14235", "authors": ["Omar Mostafa", "Nikolaos Evangeliou", "Anthony Tzes"], "title": "SLAM-based Safe Indoor Exploration Strategy", "comment": "5 pages, 8 figures. Published in the 2025 11th International\n  Conference on Automation, Robotics, and Applications (ICARA)", "summary": "This paper suggests a 2D exploration strategy for a planar space cluttered\nwith obstacles. Rather than using point robots capable of adjusting their\nposition and altitude instantly, this research is tailored to classical agents\nwith circular footprints that cannot control instantly their pose. Inhere, a\nself-balanced dual-wheeled differential drive system is used to explore the\nplace. The system is equipped with linear accelerometers and angular\ngyroscopes, a 3D-LiDAR, and a forward-facing RGB-D camera. The system performs\nRTAB-SLAM using the IMU and the LiDAR, while the camera is used for loop\nclosures. The mobile agent explores the planar space using a safe skeleton\napproach that places the agent as far as possible from the static obstacles.\nDuring the exploration strategy, the heading is towards any offered openings of\nthe space. This space exploration strategy has as its highest priority the\nagent's safety in avoiding the obstacles followed by the exploration of\nundetected space. Experimental studies with a ROS-enabled mobile agent are\npresented indicating the path planning strategy while exploring the space.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5706\u5f62\u8db3\u8ff9\u673a\u5668\u4eba\u76842D\u63a2\u7d22\u7b56\u7565\uff0c\u4f7f\u7528\u5b89\u5168\u9aa8\u67b6\u65b9\u6cd5\u4fdd\u6301\u4e0e\u969c\u788d\u7269\u6700\u5927\u8ddd\u79bb\uff0c\u4f18\u5148\u8003\u8651\u5b89\u5168\u6027\u5e76\u63a2\u7d22\u672a\u68c0\u6d4b\u7a7a\u95f4", "motivation": "\u9488\u5bf9\u65e0\u6cd5\u77ac\u65f6\u63a7\u5236\u59ff\u6001\u7684\u7ecf\u5178\u5706\u5f62\u8db3\u8ff9\u673a\u5668\u4eba\uff0c\u5728\u969c\u788d\u7269\u5bc6\u96c6\u7684\u5e73\u9762\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u5b89\u5168\u6709\u6548\u7684\u63a2\u7d22\uff0c\u800c\u975e\u4f7f\u7528\u53ef\u77ac\u65f6\u8c03\u6574\u4f4d\u7f6e\u548c\u9ad8\u5ea6\u7684\u70b9\u673a\u5668\u4eba", "method": "\u91c7\u7528\u81ea\u5e73\u8861\u53cc\u8f6e\u5dee\u901f\u9a71\u52a8\u7cfb\u7edf\uff0c\u914d\u5907IMU\u30013D-LiDAR\u548cRGB-D\u76f8\u673a\u3002\u4f7f\u7528RTAB-SLAM\u8fdb\u884c\u5b9a\u4f4d\u5efa\u56fe\uff0c\u901a\u8fc7\u5b89\u5168\u9aa8\u67b6\u65b9\u6cd5\u5c06\u673a\u5668\u4eba\u7f6e\u4e8e\u79bb\u9759\u6001\u969c\u788d\u7269\u6700\u8fdc\u7684\u4f4d\u7f6e\uff0c\u671d\u5411\u7a7a\u95f4\u5f00\u53e3\u65b9\u5411\u63a2\u7d22", "result": "\u901a\u8fc7ROS\u79fb\u52a8\u673a\u5668\u4eba\u8fdb\u884c\u4e86\u5b9e\u9a8c\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u5728\u7a7a\u95f4\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u7b56\u7565", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u969c\u788d\u7269\u5bc6\u96c6\u73af\u5883\u4e2d\u7684\u5b89\u5168\u63a2\u7d22\uff0c\u4f18\u5148\u4fdd\u969c\u673a\u5668\u4eba\u5b89\u5168\u7684\u540c\u65f6\u6709\u6548\u63a2\u7d22\u672a\u63a2\u6d4b\u533a\u57df"}}
{"id": "2508.14258", "categories": ["cs.RO", "physics.bio-ph"], "pdf": "https://arxiv.org/pdf/2508.14258", "abs": "https://arxiv.org/abs/2508.14258", "authors": ["Daegyun Choi", "Alhim Vera", "Donghoon Kim"], "title": "Adapting Biological Reflexes for Dynamic Reorientation in Space Manipulator Systems", "comment": "18 pages, 11 figures, 2025 AAS/AIAA Astrodynamics Specialist\n  Conference", "summary": "Robotic arms mounted on spacecraft, known as space manipulator systems\n(SMSs), are critical for enabling on-orbit assembly, satellite servicing, and\ndebris removal. However, controlling these systems in microgravity remains a\nsignificant challenge due to the dynamic coupling between the manipulator and\nthe spacecraft base. This study explores the potential of using biological\ninspiration to address this issue, focusing on animals, particularly lizards,\nthat exhibit mid-air righting reflexes. Based on similarities between SMSs and\nthese animals in terms of behavior, morphology, and environment, their\nair-righting motion trajectories are extracted from high-speed video recordings\nusing computer vision techniques. These trajectories are analyzed within a\nmulti-objective optimization framework to identify the key behavioral goals and\nassess their relative importance. The resulting motion profiles are then\napplied as reference trajectories for SMS control, with baseline controllers\nused to track them. The findings provide a step toward translating evolved\nanimal behaviors into interpretable, adaptive control strategies for space\nrobotics, with implications for improving maneuverability and robustness in\nfuture missions.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790\u873b\u8734\u7a7a\u4e2d\u8c03\u6574\u53cd\u5c04\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5c06\u751f\u7269\u884c\u4e3a\u8f6c\u5316\u4e3a\u7a7a\u95f4\u64cd\u7eb5\u5668\u7cfb\u7edf\u7684\u63a7\u5236\u7b56\u7565\uff0c\u63d0\u9ad8\u5fae\u91cd\u529b\u73af\u5883\u4e0b\u7684\u673a\u52a8\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u7a7a\u95f4\u64cd\u7eb5\u5668\u7cfb\u7edf\u5728\u5fae\u91cd\u529b\u73af\u5883\u4e0b\u7684\u63a7\u5236\u6311\u6218\uff0c\u7279\u522b\u662f\u64cd\u7eb5\u5668\u4e0e\u5b9a\u5b50\u57fa\u5e95\u4e4b\u95f4\u7684\u52a8\u529b\u8026\u5408\u95ee\u9898\u3002", "method": "\u4ece\u9ad8\u901f\u5f55\u50cf\u4e2d\u63d0\u53d6\u873b\u8734\u7a7a\u4e2d\u8c03\u6574\u53cd\u5c04\u8fd0\u52a8\u8f68\u8ff9\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\u5206\u6790\u5173\u952e\u884c\u4e3a\u76ee\u6807\uff0c\u5e76\u5c06\u8f68\u8ff9\u5e94\u7528\u4e3a\u7a7a\u95f4\u64cd\u7eb5\u5668\u63a7\u5236\u7684\u53c2\u8003\u8f68\u8ff9\u3002", "result": "\u5f97\u5230\u4e86\u53ef\u89e3\u91ca\u7684\u9002\u5e94\u6027\u63a7\u5236\u7b56\u7565\uff0c\u4e3a\u7a7a\u95f4\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6539\u5584\u673a\u52a8\u6027\u548c\u7a33\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "conclusion": "\u751f\u7269\u53cd\u5c04\u884c\u4e3a\u53ef\u4ee5\u6210\u529f\u8f6c\u5316\u4e3a\u7a7a\u95f4\u64cd\u7eb5\u5668\u7cfb\u7edf\u7684\u63a7\u5236\u7b56\u7565\uff0c\u4e3a\u672a\u6765\u4efb\u52a1\u7684\u673a\u52a8\u6027\u548c\u7a33\u5065\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.14355", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14355", "abs": "https://arxiv.org/abs/2508.14355", "authors": ["Guodong Yao", "Hao Wang", "Qing Chang"], "title": "D$^2$-LIO: Enhanced Optimization for LiDAR-IMU Odometry Considering Directional Degeneracy", "comment": "7 page, 2 figures", "summary": "LiDAR-inertial odometry (LIO) plays a vital role in achieving accurate\nlocalization and mapping, especially in complex environments. However, the\npresence of LiDAR feature degeneracy poses a major challenge to reliable state\nestimation. To overcome this issue, we propose an enhanced LIO framework that\nintegrates adaptive outlier-tolerant correspondence with a scan-to-submap\nregistration strategy. The core contribution lies in an adaptive outlier\nremoval threshold, which dynamically adjusts based on point-to-sensor distance\nand the motion amplitude of platform. This mechanism improves the robustness of\nfeature matching in varying conditions. Moreover, we introduce a flexible\nscan-to-submap registration method that leverages IMU data to refine pose\nestimation, particularly in degenerate geometric configurations. To further\nenhance localization accuracy, we design a novel weighting matrix that fuses\nIMU preintegration covariance with a degeneration metric derived from the\nscan-to-submap process. Extensive experiments conducted in both indoor and\noutdoor environments-characterized by sparse or degenerate features-demonstrate\nthat our method consistently outperforms state-of-the-art approaches in terms\nof both robustness and accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u578bLiDAR-\u60ef\u6027\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5f02\u5e38\u503c\u5bb9\u5fcd\u5bf9\u5e94\u548c\u626b\u63cf\u5230\u5b50\u56fe\u914d\u51c6\u7b56\u7565\uff0c\u89e3\u51b3LiDAR\u7279\u5f81\u9000\u5316\u95ee\u9898\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "LiDAR-\u60ef\u6027\u91cc\u7a0b\u8ba1\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u548c\u5efa\u56fe\u81f3\u5173\u91cd\u8981\uff0c\u4f46LiDAR\u7279\u5f81\u9000\u5316\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u4e86\u72b6\u6001\u4f30\u8ba1\u7684\u53ef\u9760\u6027\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u5f02\u5e38\u503c\u53bb\u9664\u9608\u503c\uff08\u6839\u636e\u70b9\u4f20\u611f\u5668\u8ddd\u79bb\u548c\u5e73\u53f0\u8fd0\u52a8\u5e45\u5ea6\u52a8\u6001\u8c03\u6574\uff09\uff0c\u7ed3\u5408\u7075\u6d3b\u7684\u626b\u63cf\u5230\u5b50\u56fe\u914d\u51c6\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u65b0\u578b\u6743\u91cd\u77e9\u9635\u878d\u5408IMU\u9884\u79ef\u5206\u534f\u65b9\u5dee\u548c\u9000\u5316\u5ea6\u91cf\u3002", "result": "\u5728\u5ba4\u5185\u5916\u7a00\u758f\u6216\u9000\u5316\u7279\u5f81\u73af\u5883\u4e2d\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u548c\u7cbe\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u589e\u5f3a\u578bLIO\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u5f02\u5e38\u503c\u5904\u7406\u548c\u626b\u63cf\u5230\u5b50\u56fe\u914d\u51c6\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7279\u5f81\u9000\u5316\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u53ef\u9760\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.14379", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.14379", "abs": "https://arxiv.org/abs/2508.14379", "authors": ["Chia-Han Yeh", "Tse-Sheng Nan", "Risto Vuorio", "Wei Hung", "Hung-Yen Wu", "Shao-Hua Sun", "Ping-Chun Hsieh"], "title": "Action-Constrained Imitation Learning", "comment": "Published in ICML 2025", "summary": "Policy learning under action constraints plays a central role in ensuring\nsafe behaviors in various robot control and resource allocation applications.\nIn this paper, we study a new problem setting termed Action-Constrained\nImitation Learning (ACIL), where an action-constrained imitator aims to learn\nfrom a demonstrative expert with larger action space. The fundamental challenge\nof ACIL lies in the unavoidable mismatch of occupancy measure between the\nexpert and the imitator caused by the action constraints. We tackle this\nmismatch through \\textit{trajectory alignment} and propose DTWIL, which\nreplaces the original expert demonstrations with a surrogate dataset that\nfollows similar state trajectories while adhering to the action constraints.\nSpecifically, we recast trajectory alignment as a planning problem and solve it\nvia Model Predictive Control, which aligns the surrogate trajectories with the\nexpert trajectories based on the Dynamic Time Warping (DTW) distance. Through\nextensive experiments, we demonstrate that learning from the dataset generated\nby DTWIL significantly enhances performance across multiple robot control tasks\nand outperforms various benchmark imitation learning algorithms in terms of\nsample efficiency. Our code is publicly available at\nhttps://github.com/NYCU-RL-Bandits-Lab/ACRL-Baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u52a8\u4f5c\u7ea6\u675f\u6a21\u4eff\u5b66\u4e60(ACIL)\u6846\u67b6DTWIL\uff0c\u901a\u8fc7\u52a8\u6001\u65f6\u95f4\u89c4\u6574\u8ddd\u79bb\u8fdb\u884c\u8f68\u8ff9\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e13\u5bb6\u4e0e\u6a21\u4eff\u8005\u52a8\u4f5c\u7a7a\u95f4\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u63a7\u5236\u548c\u8d44\u6e90\u5206\u914d\u7b49\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\uff0c\u52a8\u4f5c\u7ea6\u675f\u4e0b\u7684\u7b56\u7565\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u65e0\u6cd5\u5904\u7406\u4e13\u5bb6\u4e0e\u6a21\u4eff\u8005\u52a8\u4f5c\u7a7a\u95f4\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u5360\u7528\u5ea6\u91cf\u4e0d\u5339\u914d\u3002", "method": "\u63d0\u51faDTWIL\u65b9\u6cd5\uff0c\u5c06\u8f68\u8ff9\u5bf9\u9f50\u91cd\u65b0\u8868\u8ff0\u4e3a\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u751f\u6210\u9075\u5faa\u7c7b\u4f3c\u72b6\u6001\u8f68\u8ff9\u4f46\u7b26\u5408\u52a8\u4f5c\u7ea6\u675f\u7684\u66ff\u4ee3\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u52a8\u6001\u65f6\u95f4\u89c4\u6574\u8ddd\u79bb\u8fdb\u884c\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4eceDTWIL\u751f\u6210\u7684\u6570\u636e\u96c6\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u591a\u4e2a\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5728\u6837\u672c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u5404\u79cd\u57fa\u51c6\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\u3002", "conclusion": "DTWIL\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u4f5c\u7ea6\u675f\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u8f68\u8ff9\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.14380", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14380", "abs": "https://arxiv.org/abs/2508.14380", "authors": ["Nicole Fronda", "Phil Smith", "Bardh Hoxha", "Yash Pant", "Houssam Abbas"], "title": "Fair-CoPlan: Negotiated Flight Planning with Fair Deconfliction for Urban Air Mobility", "comment": "Accepted to IEEE International Conference on Intelligent\n  Transportation Systems (ITSC) 2025", "summary": "Urban Air Mobility (UAM) is an emerging transportation paradigm in which\nUncrewed Aerial Systems (UAS) autonomously transport passengers and goods in\ncities. The UAS have different operators with different, sometimes competing\ngoals, yet must share the airspace. We propose a negotiated, semi-distributed\nflight planner that optimizes UAS' flight lengths {\\em in a fair manner}.\nCurrent flight planners might result in some UAS being given disproportionately\nshorter flight paths at the expense of others. We introduce Fair-CoPlan, a\nplanner in which operators and a Provider of Service to the UAM (PSU) together\ncompute \\emph{fair} flight paths. Fair-CoPlan has three steps: First, the PSU\nconstrains take-off and landing choices for flights based on capacity at and\naround vertiports. Then, operators plan independently under these constraints.\nFinally, the PSU resolves any conflicting paths, optimizing for path length\nfairness. By fairly spreading the cost of deconfliction Fair-CoPlan encourages\nwider participation in UAM, ensures safety of the airspace and the areas below\nit, and promotes greater operator flexibility. We demonstrate Fair-CoPlan\nthrough simulation experiments and find fairer outcomes than a non-fair planner\nwith minor delays as a trade-off.", "AI": {"tldr": "Fair-CoPlan\u662f\u4e00\u4e2a\u7528\u4e8e\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u7684\u516c\u5e73\u534f\u5546\u5f0f\u98de\u884c\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u8fd0\u8425\u5546\u548c\u670d\u52a1\u63d0\u4f9b\u5546\u5408\u4f5c\u8ba1\u7b97\u516c\u5e73\u7684\u98de\u884c\u8def\u5f84\uff0c\u5728\u4fdd\u8bc1\u5b89\u5168\u7684\u540c\u65f6\u5b9e\u73b0\u8def\u5f84\u957f\u5ea6\u7684\u516c\u5e73\u5206\u914d", "motivation": "\u5f53\u524d\u98de\u884c\u89c4\u5212\u5668\u53ef\u80fd\u5bfc\u81f4\u67d0\u4e9b\u65e0\u4eba\u673a\u83b7\u5f97\u4e0d\u6210\u6bd4\u4f8b\u7684\u8f83\u77ed\u98de\u884c\u8def\u5f84\uff0c\u800c\u727a\u7272\u5176\u4ed6\u65e0\u4eba\u673a\u7684\u5229\u76ca\uff0c\u9700\u8981\u4e00\u79cd\u516c\u5e73\u7684\u65b9\u5f0f\u6765\u4f18\u5316\u98de\u884c\u8def\u5f84\u5206\u914d", "method": "\u91c7\u7528\u4e09\u6b65\u6cd5\uff1a1)\u670d\u52a1\u63d0\u4f9b\u5546\u57fa\u4e8e\u5782\u76f4\u8d77\u964d\u573a\u5bb9\u91cf\u7ea6\u675f\u8d77\u964d\u9009\u62e9\uff1b2)\u8fd0\u8425\u5546\u5728\u7ea6\u675f\u4e0b\u72ec\u7acb\u89c4\u5212\uff1b3)\u670d\u52a1\u63d0\u4f9b\u5546\u89e3\u51b3\u8def\u5f84\u51b2\u7a81\uff0c\u4f18\u5316\u8def\u5f84\u957f\u5ea6\u516c\u5e73\u6027", "result": "\u5b9e\u9a8c\u8868\u660eFair-CoPlan\u6bd4\u975e\u516c\u5e73\u89c4\u5212\u5668\u4ea7\u751f\u66f4\u516c\u5e73\u7684\u7ed3\u679c\uff0c\u4ee5\u8f7b\u5fae\u5ef6\u8fdf\u4e3a\u4ee3\u4ef7\u5b9e\u73b0\u66f4\u597d\u7684\u516c\u5e73\u6027", "conclusion": "Fair-CoPlan\u901a\u8fc7\u516c\u5e73\u5206\u644a\u51b2\u7a81\u89e3\u51b3\u6210\u672c\uff0c\u9f13\u52b1\u66f4\u5e7f\u6cdb\u7684\u53c2\u4e0e\uff0c\u786e\u4fdd\u7a7a\u57df\u5b89\u5168\uff0c\u5e76\u63d0\u5347\u8fd0\u8425\u5546\u7075\u6d3b\u6027"}}
{"id": "2508.14381", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14381", "abs": "https://arxiv.org/abs/2508.14381", "authors": ["Nicole Fronda", "Bardh Hoxha", "Houssam Abbas"], "title": "FiReFly: Fair Distributed Receding Horizon Planning for Multiple UAVs", "comment": "Accepted to IEEE International Conference on Intelligent\n  Transportation Systems (ITSC) 2025", "summary": "We propose injecting notions of fairness into multi-robot motion planning.\nWhen robots have competing interests, it is important to optimize for some kind\nof fairness in their usage of resources. In this work, we explore how the\nrobots' energy expenditures might be fairly distributed among them, while\nmaintaining mission success. We formulate a distributed fair motion planner and\nintegrate it with safe controllers in a algorithm called FiReFly. For simulated\nreach-avoid missions, FiReFly produces fairer trajectories and improves mission\nsuccess rates over a non-fair planner. We find that real-time performance is\nachievable up to 15 UAVs, and that scaling up to 50 UAVs is possible with\ntrade-offs between runtime and fairness improvements.", "AI": {"tldr": "\u63d0\u51faFiReFly\u7b97\u6cd5\uff0c\u5728\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u4e2d\u6ce8\u5165\u516c\u5e73\u6027\u6982\u5ff5\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u516c\u5e73\u8fd0\u52a8\u89c4\u5212\u5668\u4f18\u5316\u673a\u5668\u4eba\u80fd\u91cf\u6d88\u8017\u7684\u516c\u5e73\u5206\u914d\uff0c\u5728\u6a21\u62df\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u516c\u5e73\u7684\u8f68\u8ff9\u548c\u66f4\u9ad8\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u673a\u5668\u4eba\u5b58\u5728\u7ade\u4e89\u5229\u76ca\u65f6\uff0c\u9700\u8981\u4f18\u5316\u8d44\u6e90\u4f7f\u7528\u7684\u516c\u5e73\u6027\uff0c\u7279\u522b\u662f\u5728\u80fd\u91cf\u6d88\u8017\u65b9\u9762\u5b9e\u73b0\u516c\u5e73\u5206\u914d\uff0c\u540c\u65f6\u4fdd\u8bc1\u4efb\u52a1\u6210\u529f\u3002", "method": "\u5236\u5b9a\u5206\u5e03\u5f0f\u516c\u5e73\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u5e76\u4e0e\u5b89\u5168\u63a7\u5236\u5668\u96c6\u6210\uff0c\u5f62\u6210FiReFly\u7b97\u6cd5\u3002\u7b97\u6cd5\u5173\u6ce8\u673a\u5668\u4eba\u80fd\u91cf\u6d88\u8017\u7684\u516c\u5e73\u5206\u5e03\u3002", "result": "\u5728\u6a21\u62df\u5230\u8fbe\u89c4\u907f\u4efb\u52a1\u4e2d\uff0cFiReFly\u76f8\u6bd4\u975e\u516c\u5e73\u89c4\u5212\u5668\u4ea7\u751f\u4e86\u66f4\u516c\u5e73\u7684\u8f68\u8ff9\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002\u5b9e\u65f6\u6027\u80fd\u53ef\u8fbe15\u67b6\u65e0\u4eba\u673a\uff0c\u6269\u5c55\u523050\u67b6\u65e0\u4eba\u673a\u9700\u8981\u5728\u8fd0\u884c\u65f6\u95f4\u548c\u516c\u5e73\u6027\u6539\u8fdb\u4e4b\u95f4\u6743\u8861\u3002", "conclusion": "FiReFly\u7b97\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u516c\u5e73\u6027\u4f18\u5316\uff0c\u5728\u4fdd\u8bc1\u4efb\u52a1\u6210\u529f\u7684\u540c\u65f6\u6539\u5584\u4e86\u80fd\u91cf\u6d88\u8017\u7684\u516c\u5e73\u5206\u914d\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.14383", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.14383", "abs": "https://arxiv.org/abs/2508.14383", "authors": ["Haitong Ma", "Bo Dai", "Zhaolin Ren", "Yebin Wang", "Na Li"], "title": "Offline Imitation Learning upon Arbitrary Demonstrations by Pre-Training Dynamics Representations", "comment": "7 pages, 5 figures", "summary": "Limited data has become a major bottleneck in scaling up offline imitation\nlearning (IL). In this paper, we propose enhancing IL performance under limited\nexpert data by introducing a pre-training stage that learns dynamics\nrepresentations, derived from factorizations of the transition dynamics. We\nfirst theoretically justify that the optimal decision variable of offline IL\nlies in the representation space, significantly reducing the parameters to\nlearn in the downstream IL. Moreover, the dynamics representations can be\nlearned from arbitrary data collected with the same dynamics, allowing the\nreuse of massive non-expert data and mitigating the limited data issues. We\npresent a tractable loss function inspired by noise contrastive estimation to\nlearn the dynamics representations at the pre-training stage. Experiments on\nMuJoCo demonstrate that our proposed algorithm can mimic expert policies with\nas few as a single trajectory. Experiments on real quadrupeds show that we can\nleverage pre-trained dynamics representations from simulator data to learn to\nwalk from a few real-world demonstrations.", "AI": {"tldr": "\u901a\u8fc7\u9884\u8bad\u7ec3\u5b66\u4e60\u52a8\u6001\u8868\u5f81\u6765\u63d0\u5347\u6709\u9650\u4e13\u5bb6\u6570\u636e\u4e0b\u7684\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u6027\u80fd\uff0c\u5229\u7528\u4efb\u610f\u975e\u4e13\u5bb6\u6570\u636e\u5b66\u4e60\u52a8\u6001\u8868\u5f81\uff0c\u51cf\u5c11\u4e0b\u6e38\u6a21\u4eff\u5b66\u4e60\u7684\u53c2\u6570\u5b66\u4e60\u91cf", "motivation": "\u6709\u9650\u6570\u636e\u5df2\u6210\u4e3a\u6269\u5c55\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u9700\u8981\u89e3\u51b3\u4e13\u5bb6\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898", "method": "\u5f15\u5165\u9884\u8bad\u7ec3\u9636\u6bb5\u5b66\u4e60\u52a8\u6001\u8868\u5f81\uff0c\u4f7f\u7528\u566a\u58f0\u5bf9\u6bd4\u4f30\u8ba1\u7684\u635f\u5931\u51fd\u6570\uff0c\u4ece\u4efb\u610f\u5177\u6709\u76f8\u540c\u52a8\u6001\u7684\u6570\u636e\u4e2d\u5b66\u4e60\u8868\u5f81", "result": "\u5728MuJoCo\u4e0a\u4ec5\u7528\u5355\u6761\u8f68\u8ff9\u5c31\u80fd\u6a21\u4eff\u4e13\u5bb6\u7b56\u7565\uff0c\u5728\u771f\u5b9e\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u53ef\u5229\u7528\u6a21\u62df\u5668\u9884\u8bad\u7ec3\u7684\u52a8\u6001\u8868\u5f81\u4ece\u5c11\u91cf\u771f\u5b9e\u6f14\u793a\u4e2d\u5b66\u4e60\u884c\u8d70", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u9650\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u8868\u5f81\u9884\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u5c0f\u6570\u636e\u573a\u666f\u4e0b\u7684\u6027\u80fd"}}
{"id": "2508.14387", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14387", "abs": "https://arxiv.org/abs/2508.14387", "authors": ["Yuxiao Zhu", "Junfeng Chen", "Xintong Zhang", "Meng Guo", "Zhongkui Li"], "title": "DEXTER-LLM: Dynamic and Explainable Coordination of Multi-Robot Systems in Unknown Environments via Large Language Models", "comment": "submitted to IROS 2025", "summary": "Online coordination of multi-robot systems in open and unknown environments\nfaces significant challenges, particularly when semantic features detected\nduring operation dynamically trigger new tasks. Recent large language model\n(LLMs)-based approaches for scene reasoning and planning primarily focus on\none-shot, end-to-end solutions in known environments, lacking both dynamic\nadaptation capabilities for online operation and explainability in the\nprocesses of planning. To address these issues, a novel framework (DEXTER-LLM)\nfor dynamic task planning in unknown environments, integrates four modules: (i)\na mission comprehension module that resolves partial ordering of tasks\nspecified by natural languages or linear temporal logic formulas (LTL); (ii) an\nonline subtask generator based on LLMs that improves the accuracy and\nexplainability of task decomposition via multi-stage reasoning; (iii) an\noptimal subtask assigner and scheduler that allocates subtasks to robots via\nsearch-based optimization; and (iv) a dynamic adaptation and human-in-the-loop\nverification module that implements multi-rate, event-based updates for both\nsubtasks and their assignments, to cope with new features and tasks detected\nonline. The framework effectively combines LLMs' open-world reasoning\ncapabilities with the optimality of model-based assignment methods,\nsimultaneously addressing the critical issue of online adaptability and\nexplainability. Experimental evaluations demonstrate exceptional performances,\nwith 100% success rates across all scenarios, 160 tasks and 480 subtasks\ncompleted on average (3 times the baselines), 62% less queries to LLMs during\nadaptation, and superior plan quality (2 times higher) for compound tasks.\nProject page at https://tcxm.github.io/DEXTER-LLM/", "AI": {"tldr": "DEXTER-LLM\u662f\u4e00\u4e2a\u7528\u4e8e\u672a\u77e5\u73af\u5883\u4e2d\u52a8\u6001\u4efb\u52a1\u89c4\u5212\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u4e86LLM\u7684\u5f00\u653e\u4e16\u754c\u63a8\u7406\u80fd\u529b\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5728\u7ebf\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8eLLM\u7684\u573a\u666f\u63a8\u7406\u548c\u89c4\u5212\u65b9\u6cd5\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7f3a\u4e4f\u52a8\u6001\u9002\u5e94\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8bed\u4e49\u7279\u5f81\u52a8\u6001\u89e6\u53d1\u65b0\u4efb\u52a1\u65f6\u7684\u6311\u6218", "method": "\u96c6\u6210\u56db\u4e2a\u6a21\u5757\uff1a\u4efb\u52a1\u7406\u89e3\u6a21\u5757\u3001\u57fa\u4e8eLLM\u7684\u5728\u7ebf\u5b50\u4efb\u52a1\u751f\u6210\u5668\u3001\u6700\u4f18\u5b50\u4efb\u52a1\u5206\u914d\u5668\u548c\u8c03\u5ea6\u5668\u3001\u52a8\u6001\u9002\u5e94\u548c\u4eba\u673a\u9a8c\u8bc1\u6a21\u5757\uff0c\u5b9e\u73b0\u591a\u901f\u7387\u4e8b\u4ef6\u9a71\u52a8\u7684\u66f4\u65b0\u673a\u5236", "result": "\u5b9e\u9a8c\u663e\u793a100%\u6210\u529f\u7387\uff0c\u5e73\u5747\u5b8c\u6210160\u4e2a\u4efb\u52a1\u548c480\u4e2a\u5b50\u4efb\u52a1\uff08\u57fa\u7ebf3\u500d\uff09\uff0c\u9002\u5e94\u8fc7\u7a0b\u4e2dLLM\u67e5\u8be2\u51cf\u5c1162%\uff0c\u590d\u5408\u4efb\u52a1\u89c4\u5212\u8d28\u91cf\u63d0\u9ad82\u500d", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5c06LLM\u7684\u5f00\u653e\u4e16\u754c\u63a8\u7406\u80fd\u529b\u4e0e\u6a21\u578b\u5316\u4f18\u5316\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5728\u7ebf\u52a8\u6001\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u6311\u6218"}}
{"id": "2508.14441", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14441", "abs": "https://arxiv.org/abs/2508.14441", "authors": ["Yijin Chen", "Wenqiang Xu", "Zhenjun Yu", "Tutian Tang", "Yutong Li", "Siqiong Yao", "Cewu Lu"], "title": "FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile Shortcut Policy", "comment": null, "summary": "Dexterous in-hand manipulation is a long-standing challenge in robotics due\nto complex contact dynamics and partial observability. While humans synergize\nvision and touch for such tasks, robotic approaches often prioritize one\nmodality, therefore limiting adaptability. This paper introduces Flow Before\nImitation (FBI), a visuotactile imitation learning framework that dynamically\nfuses tactile interactions with visual observations through motion dynamics.\nUnlike prior static fusion methods, FBI establishes a causal link between\ntactile signals and object motion via a dynamics-aware latent model. FBI\nemploys a transformer-based interaction module to fuse flow-derived tactile\nfeatures with visual inputs, training a one-step diffusion policy for real-time\nexecution. Extensive experiments demonstrate that the proposed method\noutperforms the baseline methods in both simulation and the real world on two\ncustomized in-hand manipulation tasks and three standard dexterous manipulation\ntasks. Code, models, and more results are available in the website\nhttps://sites.google.com/view/dex-fbi.", "AI": {"tldr": "FBI\u662f\u4e00\u4e2a\u89c6\u89c9\u89e6\u89c9\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8fd0\u52a8\u52a8\u529b\u5b66\u52a8\u6001\u878d\u5408\u89e6\u89c9\u4ea4\u4e92\u548c\u89c6\u89c9\u89c2\u5bdf\uff0c\u7528\u4e8e\u7075\u5de7\u7684\u624b\u5185\u64cd\u4f5c\u4efb\u52a1", "motivation": "\u7075\u5de7\u7684\u624b\u5185\u64cd\u4f5c\u7531\u4e8e\u590d\u6742\u7684\u63a5\u89e6\u52a8\u529b\u5b66\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u4eba\u7c7b\u80fd\u591f\u534f\u540c\u4f7f\u7528\u89c6\u89c9\u548c\u89e6\u89c9\uff0c\u4f46\u673a\u5668\u4eba\u65b9\u6cd5\u5f80\u5f80\u4f18\u5148\u8003\u8651\u4e00\u79cd\u6a21\u6001\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6027", "method": "\u63d0\u51faFlow Before Imitation (FBI)\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u4ea4\u4e92\u6a21\u5757\u878d\u5408\u6d41\u52a8\u884d\u751f\u7684\u89e6\u89c9\u7279\u5f81\u4e0e\u89c6\u89c9\u8f93\u5165\uff0c\u8bad\u7ec3\u4e00\u6b65\u6269\u6563\u7b56\u7565\u8fdb\u884c\u5b9e\u65f6\u6267\u884c\u3002\u5efa\u7acb\u89e6\u89c9\u4fe1\u53f7\u4e0e\u7269\u4f53\u8fd0\u52a8\u4e4b\u95f4\u7684\u56e0\u679c\u8054\u7cfb", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u5b9a\u5236\u7684\u624b\u5185\u64cd\u4f5c\u4efb\u52a1\u548c\u4e09\u4e2a\u6807\u51c6\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "FBI\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u878d\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u6a21\u6001\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7075\u5de7\u624b\u5185\u64cd\u4f5c\u7684\u6311\u6218\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd"}}
{"id": "2508.14542", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14542", "abs": "https://arxiv.org/abs/2508.14542", "authors": ["Weize Li", "Zhengxiao Han", "Lixin Xu", "Xiangyu Chen", "Harrison Bounds", "Chenrui Zhang", "Yifan Xu"], "title": "Taming VR Teleoperation and Learning from Demonstration for Multi-Task Bimanual Table Service Manipulation", "comment": "Technical report of First-place/Champion solution at IEEE ICRA 2025\n  What Bimanuals Can Do (WBCD) Challenge - Table Services Track", "summary": "This technical report presents the champion solution of the Table Service\nTrack in the ICRA 2025 What Bimanuals Can Do (WBCD) competition. We tackled a\nseries of demanding tasks under strict requirements for speed, precision, and\nreliability: unfolding a tablecloth (deformable-object manipulation), placing a\npizza onto the table (pick-and-place), and opening and closing a food container\nwith the lid. Our solution combines VR-based teleoperation and Learning from\nDemonstrations (LfD) to balance robustness and autonomy. Most subtasks were\nexecuted through high-fidelity remote teleoperation, while the pizza placement\nwas handled by an ACT-based policy trained from 100 in-person teleoperated\ndemonstrations with randomized initial configurations. By carefully integrating\nscoring rules, task characteristics, and current technical capabilities, our\napproach achieved both high efficiency and reliability, ultimately securing the\nfirst place in the competition.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ICRA 2025 WBCD\u7ade\u8d5b\u4e2dTable Service Track\u7684\u51a0\u519b\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408VR\u9065\u64cd\u4f5c\u548c\u5b66\u4e60\u6f14\u793a\u6280\u672f\uff0c\u6210\u529f\u5b8c\u6210\u684c\u5e03\u5c55\u5f00\u3001\u62ab\u8428\u653e\u7f6e\u548c\u98df\u54c1\u5bb9\u5668\u5f00\u5173\u7b49\u590d\u6742\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u5728\u901f\u5ea6\u3001\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u4e25\u683c\u8981\u6c42\u4e0b\u7684\u4e00\u7cfb\u5217\u590d\u6742\u53cc\u624b\u64cd\u4f5c\u4efb\u52a1\uff0c\u5305\u62ec\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\u3001\u7cbe\u786e\u62fe\u53d6\u653e\u7f6e\u7b49\u6311\u6218\u3002", "method": "\u91c7\u7528VR\u9065\u64cd\u4f5c\u548c\u5b66\u4e60\u6f14\u793a(LfD)\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5927\u90e8\u5206\u5b50\u4efb\u52a1\u901a\u8fc7\u9ad8\u4fdd\u771f\u8fdc\u7a0b\u9065\u64cd\u4f5c\u6267\u884c\uff0c\u62ab\u8428\u653e\u7f6e\u4efb\u52a1\u4f7f\u7528\u57fa\u4e8eACT\u7b56\u7565\u7684100\u6b21\u73b0\u573a\u9065\u64cd\u4f5c\u6f14\u793a\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u7cbe\u5fc3\u6574\u5408\u8bc4\u5206\u89c4\u5219\u3001\u4efb\u52a1\u7279\u6027\u548c\u5f53\u524d\u6280\u672f\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7387\u548c\u53ef\u9760\u6027\uff0c\u6700\u7ec8\u83b7\u5f97\u7ade\u8d5b\u7b2c\u4e00\u540d\u3002", "conclusion": "VR\u9065\u64cd\u4f5c\u4e0e\u5b66\u4e60\u6f14\u793a\u7684\u6df7\u5408\u65b9\u6cd5\u5728\u590d\u6742\u53cc\u624b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e73\u8861\u4e86\u9c81\u68d2\u6027\u548c\u81ea\u4e3b\u6027\uff0c\u4e3a\u7c7b\u4f3c\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.14554", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14554", "abs": "https://arxiv.org/abs/2508.14554", "authors": ["Xinkai Liang", "Yigu Ge", "Yangxi Shi", "Haoyu Yang", "Xu Cao", "Hao Fang"], "title": "EAROL: Environmental Augmented Perception-Aware Planning and Robust Odometry via Downward-Mounted Tilted LiDAR", "comment": "Accepted by 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025). This work has been submitted to the IEEE for\n  possible publication", "summary": "To address the challenges of localization drift and perception-planning\ncoupling in unmanned aerial vehicles (UAVs) operating in open-top scenarios\n(e.g., collapsed buildings, roofless mazes), this paper proposes EAROL, a novel\nframework with a downward-mounted tilted LiDAR configuration (20{\\deg}\ninclination), integrating a LiDAR-Inertial Odometry (LIO) system and a\nhierarchical trajectory-yaw optimization algorithm. The hardware innovation\nenables constraint enhancement via dense ground point cloud acquisition and\nforward environmental awareness for dynamic obstacle detection. A\ntightly-coupled LIO system, empowered by an Iterative Error-State Kalman Filter\n(IESKF) with dynamic motion compensation, achieves high level 6-DoF\nlocalization accuracy in feature-sparse environments. The planner, augmented by\nenvironment, balancing environmental exploration, target tracking precision,\nand energy efficiency. Physical experiments demonstrate 81% tracking error\nreduction, 22% improvement in perceptual coverage, and near-zero vertical drift\nacross indoor maze and 60-meter-scale outdoor scenarios. This work proposes a\nhardware-algorithm co-design paradigm, offering a robust solution for UAV\nautonomy in post-disaster search and rescue missions. We will release our\nsoftware and hardware as an open-source package for the community. Video:\nhttps://youtu.be/7av2ueLSiYw.", "AI": {"tldr": "EAROL\u662f\u4e00\u4e2a\u9488\u5bf9\u65e0\u4eba\u673a\u5728\u5f00\u653e\u9876\u90e8\u573a\u666f\uff08\u5982\u574d\u584c\u5efa\u7b51\uff09\u7684\u786c\u4ef6\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u91c7\u752820\u5ea6\u503e\u659c\u7684\u4e0b\u5411LiDAR\u914d\u7f6e\uff0c\u7ed3\u5408LiDAR-\u60ef\u6027\u91cc\u7a0b\u8ba1\u548c\u5206\u5c42\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u5f00\u653e\u9876\u90e8\u573a\u666f\u4e2d\u7684\u5b9a\u4f4d\u6f02\u79fb\u95ee\u9898\u548c\u611f\u77e5-\u89c4\u5212\u8026\u5408\u6311\u6218\uff0c\u4e3a\u707e\u540e\u641c\u6551\u4efb\u52a1\u63d0\u4f9b\u53ef\u9760\u7684\u81ea\u4e3b\u98de\u884c\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u786c\u4ef6\u521b\u65b0\uff1a20\u5ea6\u503e\u659c\u7684\u4e0b\u5411LiDAR\u914d\u7f6e\uff0c\u5b9e\u73b0\u5bc6\u96c6\u5730\u9762\u70b9\u4e91\u91c7\u96c6\u548c\u524d\u5411\u73af\u5883\u611f\u77e5\uff1b\u5f00\u53d1\u7d27\u8026\u5408\u7684LiDAR-\u60ef\u6027\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff0c\u91c7\u7528\u8fed\u4ee3\u8bef\u5dee\u72b6\u6001\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u52a8\u6001\u8fd0\u52a8\u8865\u507f\uff1b\u8bbe\u8ba1\u5206\u5c42\u8f68\u8ff9-\u504f\u822a\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a\u8ddf\u8e2a\u8bef\u5dee\u51cf\u5c1181%\uff0c\u611f\u77e5\u8986\u76d6\u7387\u63d0\u534722%\uff0c\u5728\u5ba4\u5185\u8ff7\u5bab\u548c60\u7c73\u5c3a\u5ea6\u5ba4\u5916\u573a\u666f\u4e2d\u5b9e\u73b0\u8fd1\u4e4e\u96f6\u7684\u5782\u76f4\u6f02\u79fb\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e86\u786c\u4ef6\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u7684\u65b0\u8303\u5f0f\uff0c\u4e3a\u65e0\u4eba\u673a\u5728\u707e\u540e\u641c\u6551\u4efb\u52a1\u4e2d\u7684\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c06\u5f00\u6e90\u8f6f\u4ef6\u548c\u786c\u4ef6\u3002"}}
{"id": "2508.14610", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14610", "abs": "https://arxiv.org/abs/2508.14610", "authors": ["Junzhi Li", "Teng Long", "Jingliang Sun", "Jianxin Zhong"], "title": "TRUST-Planner: Topology-guided Robust Trajectory Planner for AAVs with Uncertain Obstacle Spatial-temporal Avoidance", "comment": null, "summary": "Despite extensive developments in motion planning of autonomous aerial\nvehicles (AAVs), existing frameworks faces the challenges of local minima and\ndeadlock in complex dynamic environments, leading to increased collision risks.\nTo address these challenges, we present TRUST-Planner, a topology-guided\nhierarchical planning framework for robust spatial-temporal obstacle avoidance.\nIn the frontend, a dynamic enhanced visible probabilistic roadmap (DEV-PRM) is\nproposed to rapidly explore topological paths for global guidance. The backend\nutilizes a uniform terminal-free minimum control polynomial (UTF-MINCO) and\ndynamic distance field (DDF) to enable efficient predictive obstacle avoidance\nand fast parallel computation. Furthermore, an incremental multi-branch\ntrajectory management framework is introduced to enable spatio-temporal\ntopological decision-making, while efficiently leveraging historical\ninformation to reduce replanning time. Simulation results show that\nTRUST-Planner outperforms baseline competitors, achieving a 96\\% success rate\nand millisecond-level computation efficiency in tested complex environments.\nReal-world experiments further validate the feasibility and practicality of the\nproposed method.", "AI": {"tldr": "TRUST-Planner\u662f\u4e00\u4e2a\u62d3\u6251\u5f15\u5bfc\u7684\u5c42\u6b21\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u589e\u5f3a\u53ef\u89c1\u6982\u7387\u8def\u7ebf\u56fe(DEV-PRM)\u63a2\u7d22\u62d3\u6251\u8def\u5f84\uff0c\u4f7f\u7528\u7edf\u4e00\u65e0\u7ec8\u7aef\u6700\u5c0f\u63a7\u5236\u591a\u9879\u5f0f(UTF-MINCO)\u548c\u52a8\u6001\u8ddd\u79bb\u573a(DDF)\u5b9e\u73b0\u9ad8\u6548\u9884\u6d4b\u907f\u969c\uff0c\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u8fbe\u523096%\u6210\u529f\u7387\u548c\u6beb\u79d2\u7ea7\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u81ea\u4e3b\u98de\u884c\u5668\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u9762\u4e34\u5c40\u90e8\u6700\u5c0f\u503c\u548c\u6b7b\u9501\u95ee\u9898\uff0c\u5bfc\u81f4\u78b0\u649e\u98ce\u9669\u589e\u52a0\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u65f6\u7a7a\u907f\u969c\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u62d3\u6251\u5f15\u5bfc\u7684\u5c42\u6b21\u89c4\u5212\u6846\u67b6\uff1a\u524d\u7aef\u4f7f\u7528DEV-PRM\u5feb\u901f\u63a2\u7d22\u62d3\u6251\u8def\u5f84\u63d0\u4f9b\u5168\u5c40\u5f15\u5bfc\uff1b\u540e\u7aef\u4f7f\u7528UTF-MINCO\u548cDDF\u5b9e\u73b0\u9ad8\u6548\u9884\u6d4b\u907f\u969c\u548c\u5e76\u884c\u8ba1\u7b97\uff1b\u5f15\u5165\u589e\u91cf\u591a\u5206\u652f\u8f68\u8ff9\u7ba1\u7406\u6846\u67b6\u8fdb\u884c\u65f6\u7a7a\u62d3\u6251\u51b3\u7b56\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793aTRUST-Planner\u5728\u590d\u6742\u73af\u5883\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8fbe\u523096%\u7684\u6210\u529f\u7387\u548c\u6beb\u79d2\u7ea7\u8ba1\u7b97\u6548\u7387\u3002\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "TRUST-Planner\u901a\u8fc7\u62d3\u6251\u5f15\u5bfc\u7684\u5c42\u6b21\u89c4\u5212\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u548c\u6b7b\u9501\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u9c81\u68d2\u7684\u65f6\u7a7a\u907f\u969c\uff0c\u4e3a\u81ea\u4e3b\u98de\u884c\u5668\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.14635", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14635", "abs": "https://arxiv.org/abs/2508.14635", "authors": ["Jo\u00e3o Vitor de Carvalho Silva", "Douglas G. Macharet"], "title": "Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination", "comment": null, "summary": "The ability to coordinate actions across multiple agents is critical for\nsolving complex, real-world problems. Large Language Models (LLMs) have shown\nstrong capabilities in communication, planning, and reasoning, raising the\nquestion of whether they can also support effective collaboration in\nmulti-agent settings. In this work, we investigate the use of LLM agents to\nsolve a structured victim rescue task that requires division of labor,\nprioritization, and cooperative planning. Agents operate in a fully known\ngraph-based environment and must allocate resources to victims with varying\nneeds and urgency levels. We systematically evaluate their performance using a\nsuite of coordination-sensitive metrics, including task success rate, redundant\nactions, room conflicts, and urgency-weighted efficiency. This study offers new\ninsights into the strengths and failure modes of LLMs in physically grounded\nmulti-agent collaboration tasks, contributing to future benchmarks and\narchitectural improvements.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u6551\u63f4\u4efb\u52a1\u4e2d\u7684\u534f\u4f5c\u80fd\u529b\uff0c\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u53d1\u73b0LLM\u5728\u5206\u5de5\u548c\u4f18\u5148\u7ea7\u89c4\u5212\u65b9\u9762\u5b58\u5728\u6f5c\u529b\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6c9f\u901a\u3001\u89c4\u5212\u548c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u662f\u5426\u80fd\u591f\u6709\u6548\u534f\u4f5c\u5c1a\u4e0d\u6e05\u695a\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5206\u5de5\u3001\u4f18\u5148\u7ea7\u6392\u5e8f\u548c\u5408\u4f5c\u89c4\u5212\u7684\u590d\u6742\u4efb\u52a1\u4e2d", "method": "\u4f7f\u7528LLM\u667a\u80fd\u4f53\u5728\u5b8c\u5168\u5df2\u77e5\u7684\u56fe\u57fa\u73af\u5883\u4e2d\u6267\u884c\u53d7\u5bb3\u8005\u6551\u63f4\u4efb\u52a1\uff0c\u8bc4\u4f30\u5176\u5728\u8d44\u6e90\u5206\u914d\u3001\u5206\u5de5\u5408\u4f5c\u65b9\u9762\u7684\u8868\u73b0\uff0c\u91c7\u7528\u534f\u8c03\u654f\u611f\u6307\u6807\uff08\u4efb\u52a1\u6210\u529f\u7387\u3001\u5197\u4f59\u884c\u52a8\u3001\u623f\u95f4\u51b2\u7a81\u3001\u7d27\u6025\u52a0\u6743\u6548\u7387\uff09\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30", "result": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u5728\u7269\u7406\u57fa\u7840\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u548c\u5931\u8d25\u6a21\u5f0f\uff0c\u4e3a\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cf\u5316\u7ed3\u679c", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u57fa\u51c6\u6d4b\u8bd5\u548c\u67b6\u6784\u6539\u8fdb\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8LLM\u5728\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u9886\u57df\u7684\u53d1\u5c55"}}
{"id": "2508.14636", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14636", "abs": "https://arxiv.org/abs/2508.14636", "authors": ["Sanjeev Ramkumar Sudha", "Marija Popovi\u0107", "Erlend M. Coates"], "title": "An Informative Planning Framework for Target Tracking and Active Mapping in Dynamic Environments with ASVs", "comment": "Submitted to IEEE Robotics and Automation Letters (RA-L)", "summary": "Mobile robot platforms are increasingly being used to automate information\ngathering tasks such as environmental monitoring. Efficient target tracking in\ndynamic environments is critical for applications such as search and rescue and\npollutant cleanups. In this letter, we study active mapping of floating targets\nthat drift due to environmental disturbances such as wind and currents. This is\na challenging problem as it involves predicting both spatial and temporal\nvariations in the map due to changing conditions. We propose an informative\npath planning framework to map an arbitrary number of moving targets with\ninitially unknown positions in dynamic environments. A key component of our\napproach is a spatiotemporal prediction network that predicts target position\ndistributions over time. We propose an adaptive planning objective for target\ntracking that leverages these predictions. Simulation experiments show that our\nproposed planning objective improves target tracking performance compared to\nexisting methods that consider only entropy reduction as the planning\nobjective. Finally, we validate our approach in field tests using an autonomous\nsurface vehicle, showcasing its ability to track targets in real-world\nmonitoring scenarios.", "AI": {"tldr": "\u57fa\u4e8e\u65f6\u7a7a\u9884\u6d4b\u7f51\u7edc\u7684\u6d3e\u751f\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u73af\u5883\u4e2d\u6d6e\u52a8\u76ee\u6807\u7684\u4e3b\u52a8\u7ed8\u56fe\u548c\u8ffd\u8e2a", "motivation": "\u52a8\u6001\u73af\u5883\u4e2d\u6d6e\u52a8\u76ee\u6807\u7684\u8ffd\u8e2a\u5728\u641c\u6551\u548c\u73af\u5883\u6e05\u7406\u7b49\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u9884\u6d4b\u76ee\u6807\u5728\u98ce\u3001\u6d41\u7b49\u5e72\u6270\u4e0b\u7684\u65f6\u7a7a\u53d8\u5316", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4fe1\u606f\u6d3e\u751f\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u5305\u542b\u65f6\u7a7a\u9884\u6d4b\u7f51\u7edc\u9884\u6d4b\u76ee\u6807\u4f4d\u7f6e\u5206\u5e03\uff0c\u4ee5\u53ca\u57fa\u4e8e\u9884\u6d4b\u7684\u9002\u5e94\u6027\u89c4\u5212\u76ee\u6807", "result": "\u6a21\u62df\u5b9e\u9a8c\u663e\u793a\uff0c\u4e0e\u4ec5\u8003\u8651\u71b5\u51cf\u7684\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u65b9\u6cd5\u7684\u76ee\u6807\u8ffd\u8e2a\u6027\u80fd\u66f4\u4f18\uff0c\u5e76\u901a\u8fc7\u81ea\u4e3b\u8868\u9762\u8f66\u8fdb\u884c\u4e86\u5b9e\u9645\u573a\u666f\u9a8c\u8bc1", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8ffd\u8e2a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6d6e\u52a8\u76ee\u6807\uff0c\u5728\u771f\u5b9e\u76d1\u6d4b\u573a\u666f\u4e2d\u5c55\u73b0\u4e86\u826f\u597d\u7684\u5e94\u7528\u6f5c\u529b"}}
{"id": "2508.14661", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14661", "abs": "https://arxiv.org/abs/2508.14661", "authors": ["Alexander Raab", "Stephan Weiss", "Alessandro Fornasier", "Christian Brommer", "Abdalrahman Ibrahim"], "title": "Consistent Pose Estimation of Unmanned Ground Vehicles through Terrain-Aided Multi-Sensor Fusion on Geometric Manifolds", "comment": null, "summary": "Aiming to enhance the consistency and thus long-term accuracy of Extended\nKalman Filters for terrestrial vehicle localization, this paper introduces the\nManifold Error State Extended Kalman Filter (M-ESEKF). By representing the\nrobot's pose in a space with reduced dimensionality, the approach ensures\nfeasible estimates on generic smooth surfaces, without introducing artificial\nconstraints or simplifications that may degrade a filter's performance. The\naccompanying measurement models are compatible with common loosely- and\ntightly-coupled sensor modalities and also implicitly account for the ground\ngeometry. We extend the formulation by introducing a novel correction scheme\nthat embeds additional domain knowledge into the sensor data, giving more\naccurate uncertainty approximations and further enhancing filter consistency.\nThe proposed estimator is seamlessly integrated into a validated modular state\nestimation framework, demonstrating compatibility with existing\nimplementations. Extensive Monte Carlo simulations across diverse scenarios and\ndynamic sensor configurations show that the M-ESEKF outperforms classical\nfilter formulations in terms of consistency and stability. Moreover, it\neliminates the need for scenario-specific parameter tuning, enabling its\napplication in a variety of real-world settings.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6d41\u5f62\u8bef\u5dee\u72b6\u6001\u7684\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668(M-ESEKF)\uff0c\u901a\u8fc7\u964d\u7ef4\u8868\u793a\u673a\u5668\u4eba\u4f4d\u59ff\uff0c\u5728\u4fdd\u6301\u901a\u7528\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u6ee4\u6ce2\u5668\u7684\u4e00\u81f4\u6027\u548c\u957f\u671f\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5728\u9646\u5730\u8f66\u8f86\u5b9a\u4f4d\u4e2d\u5b58\u5728\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5bfc\u81f4\u957f\u671f\u7cbe\u5ea6\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u901a\u7528\u5149\u6ed1\u8868\u9762\u4e0a\u63d0\u4f9b\u53ef\u884c\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u800c\u4e0d\u5f15\u5165\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\u7684\u4eba\u5de5\u7ea6\u675f\u6216\u7b80\u5316\u3002", "method": "\u4f7f\u7528\u6d41\u5f62\u8bef\u5dee\u72b6\u6001\u8868\u793a\u6cd5\uff0c\u5728\u964d\u7ef4\u7a7a\u95f4\u4e2d\u8868\u793a\u673a\u5668\u4eba\u4f4d\u59ff\u3002\u5f00\u53d1\u517c\u5bb9\u677e\u6563\u548c\u7d27\u5bc6\u8026\u5408\u4f20\u611f\u5668\u6a21\u5f0f\u7684\u6d4b\u91cf\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u65b0\u9896\u7684\u6821\u6b63\u65b9\u6848\u5c06\u9886\u57df\u77e5\u8bc6\u5d4c\u5165\u4f20\u611f\u5668\u6570\u636e\u4e2d\u3002", "result": "\u5e7f\u6cdb\u7684\u8499\u7279\u5361\u6d1b\u6a21\u62df\u663e\u793a\uff0cM-ESEKF\u5728\u4e00\u81f4\u6027\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u7ecf\u5178\u6ee4\u6ce2\u5668\u516c\u5f0f\u3002\u6d88\u9664\u4e86\u573a\u666f\u7279\u5b9a\u53c2\u6570\u8c03\u4f18\u7684\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "conclusion": "M-ESEKF\u901a\u8fc7\u6d41\u5f62\u8868\u793a\u548c\u521b\u65b0\u7684\u6821\u6b63\u65b9\u6848\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u7684\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u9646\u5730\u8f66\u8f86\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.14763", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14763", "abs": "https://arxiv.org/abs/2508.14763", "authors": ["Sagar Parekh", "Casey Grothoff", "Ryan Wright", "Robin White", "Dylan P. Losey"], "title": "Safe and Transparent Robots for Human-in-the-Loop Meat Processing", "comment": null, "summary": "Labor shortages have severely affected the meat processing sector. Automated\ntechnology has the potential to support the meat industry, assist workers, and\nenhance job quality. However, existing automation in meat processing is highly\nspecialized, inflexible, and cost intensive. Instead of forcing manufacturers\nto buy a separate device for each step of the process, our objective is to\ndevelop general-purpose robotic systems that work alongside humans to perform\nmultiple meat processing tasks. Through a recently conducted survey of industry\nexperts, we identified two main challenges associated with integrating these\ncollaborative robots alongside human workers. First, there must be measures to\nensure the safety of human coworkers; second, the coworkers need to understand\nwhat the robot is doing. This paper addresses both challenges by introducing a\nsafety and transparency framework for general-purpose meat processing robots.\nFor safety, we implement a hand-detection system that continuously monitors\nnearby humans. This system can halt the robot in situations where the human\ncomes into close proximity of the operating robot. We also develop an\ninstrumented knife equipped with a force sensor that can differentiate contact\nbetween objects such as meat, bone, or fixtures. For transparency, we introduce\na method that detects the robot's uncertainty about its performance and uses an\nLED interface to communicate that uncertainty to the human. Additionally, we\ndesign a graphical interface that displays the robot's plans and allows the\nhuman to provide feedback on the planned cut. Overall, our framework can ensure\nsafe operation while keeping human workers in-the-loop about the robot's\nactions which we validate through a user study.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8089\u7c7b\u52a0\u5de5\u673a\u5668\u4eba\u7684\u5b89\u5168\u548c\u900f\u660e\u6027\u6846\u67b6\uff0c\u89e3\u51b3\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u5b89\u5168\u98ce\u9669\u548c\u6d88\u9664\u4eba\u7c7b\u5de5\u4f5c\u8005\u5bf9\u673a\u5668\u4eba\u884c\u4e3a\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u8089\u7c7b\u52a0\u5de5\u884c\u4e1a\u9762\u4e34\u4e25\u91cd\u7684\u52b3\u52a1\u77ed\u7f3a\u95ee\u9898\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u6280\u672f\u4e13\u95e8\u5316\u7a0b\u5ea6\u9ad8\u3001\u7075\u6d3b\u6027\u5dee\u3001\u6210\u672c\u9ad8\u3002\u9700\u8981\u5f00\u53d1\u901a\u7528\u578b\u534f\u4f5c\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u4f46\u4eba\u673a\u534f\u4f5c\u5b58\u5728\u5b89\u5168\u98ce\u9669\u548c\u900f\u660e\u6027\u4e24\u5927\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b89\u5168\u548c\u900f\u660e\u6027\u6846\u67b6\uff1a1\uff09\u624b\u90e8\u68c0\u6d4b\u7cfb\u7edf\u76d1\u63a7\u4eba\u5458\u5b89\u5168\uff1b2\uff09\u914d\u5907\u529b\u4f20\u611f\u5668\u7684\u4eea\u5668\u5316\u5200\u5177\u533a\u5206\u5207\u5272\u7269\u4f53\uff1b3\uff09LED\u754c\u9762\u901a\u4fe1\u673a\u5668\u4eba\u4e0d\u786e\u5b9a\u6027\uff1b4\uff09\u56fe\u5f62\u754c\u9762\u663e\u793a\u673a\u5668\u4eba\u8ba1\u5212\u5e76\u5141\u8bb8\u4eba\u7c7b\u53cd\u9988\u3002", "result": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u786e\u4fdd\u5b89\u5168\u8fd0\u884c\uff0c\u540c\u65f6\u4fdd\u6301\u4eba\u7c7b\u5de5\u4f5c\u8005\u5bf9\u673a\u5668\u4eba\u884c\u4e3a\u7684\u77e5\u60c5\u6743\u3002", "conclusion": "\u8be5\u5b89\u5168\u548c\u900f\u660e\u6027\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u901a\u7528\u578b\u8089\u7c7b\u52a0\u5de5\u673a\u5668\u4eba\u5728\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u884c\u4e1a\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\u3002"}}
