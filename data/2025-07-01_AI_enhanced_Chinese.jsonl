{"id": "2506.22466", "categories": ["cs.RO", "cs.CY", "I.2.9; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.22466", "abs": "https://arxiv.org/abs/2506.22466", "authors": ["Marcel Heisler", "Christian Becker-Asano"], "title": "Conversations with Andrea: Visitors' Opinions on Android Robots in a Museum", "comment": "To be published in IEEE RO-MAN 2025 conference proceedings; for\n  videos check https://ai.hdm-stuttgart.de/humanoid-lab", "summary": "The android robot Andrea was set up at a public museum in Germany for six\nconsecutive days to have conversations with visitors, fully autonomously. No\nspecific context was given, so visitors could state their opinions regarding\npossible use-cases in structured interviews, without any bias. Additionally the\n44 interviewees were asked for their general opinions of the robot, their\nreasons (not) to interact with it and necessary improvements for future use.\nThe android's voice and wig were changed between different days of operation to\ngive varying cues regarding its gender. This did not have a significant impact\non the positive overall perception of the robot. Most visitors want the robot\nto provide information about exhibits in the future, while opinions on other\nroles, like a receptionist, were both wanted and explicitly not wanted by\ndifferent visitors. Speaking more languages (than only English) and faster\nresponse times were the improvements most desired. These findings from the\ninterviews are in line with an analysis of the system logs, which revealed,\nthat after chitchat and personal questions, most of the 4436 collected requests\nasked for information related to the museum and to converse in a different\nlanguage. The valuable insights gained from these real-world interactions are\nnow used to improve the system to become a useful real-world application.", "AI": {"tldr": "Andrea\u673a\u5668\u4eba\u535a\u7269\u9986\u5b9e\u9a8c\u603b\u7ed3", "motivation": "\u7814\u7a76\u516c\u4f17\u5bf9\u81ea\u4e3b\u5bf9\u8bdd\u673a\u5668\u4eba\u7684\u63a5\u53d7\u5ea6\u53ca\u6f5c\u5728\u5e94\u7528\u573a\u666f\u3002", "method": "\u5728\u535a\u7269\u9986\u90e8\u7f72Andrea\u673a\u5668\u4eba\u516d\u5929\uff0c\u6536\u96c644\u540d\u8bbf\u5ba2\u7684\u7ed3\u6784\u5316\u8bbf\u8c08\u6570\u636e\uff0c\u5e76\u5206\u67904436\u6761\u7cfb\u7edf\u65e5\u5fd7\u3002", "result": "\u673a\u5668\u4eba\u603b\u4f53\u8bc4\u4ef7\u79ef\u6781\uff0c\u6027\u522b\u7ebf\u7d22\u65e0\u663e\u8457\u5f71\u54cd\uff1b\u8bbf\u5ba2\u5e0c\u671b\u5176\u63d0\u4f9b\u5c55\u54c1\u4fe1\u606f\uff0c\u4f46\u5bf9\u5176\u4ed6\u89d2\u8272\u610f\u89c1\u4e0d\u4e00\uff1b\u6539\u8fdb\u9700\u6c42\u5305\u62ec\u591a\u8bed\u8a00\u652f\u6301\u548c\u66f4\u5feb\u54cd\u5e94\u3002", "conclusion": "\u5b9e\u9a8c\u4e3a\u6539\u8fdb\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\uff0c\u63a8\u52a8\u5176\u6210\u4e3a\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.22473", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22473", "abs": "https://arxiv.org/abs/2506.22473", "authors": ["Fernando Diaz Ledezma", "Valentin Marcel", "Matej Hoffmann"], "title": "Unsupervised Discovery of Behavioral Primitives from Sensorimotor Dynamic Functional Connectivity", "comment": "8 pages with 6 figures", "summary": "The movements of both animals and robots give rise to streams of\nhigh-dimensional motor and sensory information. Imagine the brain of a newborn\nor the controller of a baby humanoid robot trying to make sense of unprocessed\nsensorimotor time series. Here, we present a framework for studying the dynamic\nfunctional connectivity between the multimodal sensory signals of a robotic\nagent to uncover an underlying structure. Using instantaneous mutual\ninformation, we capture the time-varying functional connectivity (FC) between\nproprioceptive, tactile, and visual signals, revealing the sensorimotor\nrelationships. Using an infinite relational model, we identified sensorimotor\nmodules and their evolving connectivity. To further interpret these dynamic\ninteractions, we employed non-negative matrix factorization, which decomposed\nthe connectivity patterns into additive factors and their corresponding\ntemporal coefficients. These factors can be considered the agent's motion\nprimitives or movement synergies that the agent can use to make sense of its\nsensorimotor space and later for behavior selection. In the future, the method\ncan be deployed in robot learning as well as in the analysis of human movement\ntrajectories or brain signals.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6790\u673a\u5668\u4eba\u591a\u6a21\u6001\u611f\u5b98\u4fe1\u53f7\u52a8\u6001\u529f\u80fd\u8fde\u63a5\u7684\u65b9\u6cd5\uff0c\u63ed\u793a\u5176\u5e95\u5c42\u7ed3\u6784\uff0c\u5e76\u5e94\u7528\u4e8e\u884c\u4e3a\u9009\u62e9\u3002", "motivation": "\u7814\u7a76\u9ad8\u7ef4\u611f\u5b98\u8fd0\u52a8\u4fe1\u606f\u7684\u52a8\u6001\u529f\u80fd\u8fde\u63a5\uff0c\u5e2e\u52a9\u673a\u5668\u4eba\u6216\u65b0\u751f\u513f\u7406\u89e3\u672a\u5904\u7406\u7684\u611f\u5b98\u8fd0\u52a8\u65f6\u95f4\u5e8f\u5217\u3002", "method": "\u4f7f\u7528\u77ac\u65f6\u4e92\u4fe1\u606f\u6355\u6349\u52a8\u6001\u529f\u80fd\u8fde\u63a5\uff0c\u7ed3\u5408\u65e0\u9650\u5173\u7cfb\u6a21\u578b\u548c\u975e\u8d1f\u77e9\u9635\u5206\u89e3\u8bc6\u522b\u4f20\u611f\u5668\u8fd0\u52a8\u6a21\u5757\u53ca\u5176\u6f14\u5316\u3002", "result": "\u63ed\u793a\u4e86\u4f20\u611f\u5668\u8fd0\u52a8\u5173\u7cfb\uff0c\u5206\u89e3\u51fa\u8fd0\u52a8\u57fa\u5143\u6216\u534f\u540c\u4f5c\u7528\uff0c\u53ef\u7528\u4e8e\u884c\u4e3a\u9009\u62e9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u672a\u6765\u53ef\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u5b66\u4e60\u548c\u4eba\u7c7b\u8fd0\u52a8\u6216\u8111\u4fe1\u53f7\u5206\u6790\u3002"}}
{"id": "2506.22494", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22494", "abs": "https://arxiv.org/abs/2506.22494", "authors": ["Shihong Ling", "Yue Wan", "Xiaowei Jia", "Na Du"], "title": "DriveBLIP2: Attention-Guided Explanation Generation for Complex Driving Scenarios", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. 7 pages, 3 figures", "summary": "This paper introduces a new framework, DriveBLIP2, built upon the BLIP2-OPT\narchitecture, to generate accurate and contextually relevant explanations for\nemerging driving scenarios. While existing vision-language models perform well\nin general tasks, they encounter difficulties in understanding complex,\nmulti-object environments, particularly in real-time applications such as\nautonomous driving, where the rapid identification of key objects is crucial.\nTo address this limitation, an Attention Map Generator is proposed to highlight\nsignificant objects relevant to driving decisions within critical video frames.\nBy directing the model's focus to these key regions, the generated attention\nmap helps produce clear and relevant explanations, enabling drivers to better\nunderstand the vehicle's decision-making process in critical situations.\nEvaluations on the DRAMA dataset reveal significant improvements in explanation\nquality, as indicated by higher BLEU, ROUGE, CIDEr, and SPICE scores compared\nto baseline models. These findings underscore the potential of targeted\nattention mechanisms in vision-language models for enhancing explainability in\nreal-time autonomous driving.", "AI": {"tldr": "DriveBLIP2\u6846\u67b6\u57fa\u4e8eBLIP2-OPT\u67b6\u6784\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u56fe\u751f\u6210\u5668\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u89e3\u91ca\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u591a\u76ee\u6807\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u5728\u5b9e\u65f6\u81ea\u52a8\u9a7e\u9a76\u4e2d\u96be\u4ee5\u5feb\u901f\u8bc6\u522b\u5173\u952e\u5bf9\u8c61\u3002", "method": "\u63d0\u51fa\u6ce8\u610f\u529b\u56fe\u751f\u6210\u5668\uff0c\u7a81\u51fa\u5173\u952e\u89c6\u9891\u5e27\u4e2d\u5bf9\u9a7e\u9a76\u51b3\u7b56\u91cd\u8981\u7684\u5bf9\u8c61\u3002", "result": "\u5728DRAMA\u6570\u636e\u96c6\u4e0a\uff0cDriveBLIP2\u5728BLEU\u3001ROUGE\u3001CIDEr\u548cSPICE\u5206\u6570\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u5b9a\u5411\u6ce8\u610f\u529b\u673a\u5236\u53ef\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u65f6\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u89e3\u91ca\u80fd\u529b\u3002"}}
{"id": "2506.22572", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22572", "abs": "https://arxiv.org/abs/2506.22572", "authors": ["Mrunmayi Mungekar", "Sanjith Menon", "M. Ravi Shankar", "M. Khalid Jawed"], "title": "Directed Shape Morphing using Kirigami-enhanced Thermoplastics", "comment": "Software and Data: https://github.com/structuresComp/Shrinky-Dink", "summary": "We present a simple, accessible method for autonomously transforming flat\nplastic sheets into intricate three-dimensional structures using only uniform\nheating and common tools such as household ovens and scissors. Our approach\ncombines heat-shrinkable thermoplastics with Kirigami patterns tailored to the\ntarget 3D shape, creating bilayer composites that morph into a wide range of\ncomplex structures, e.g., bowls, pyramids, and even custom ergonomic surfaces\nlike mouse covers. Critically, the transformation is driven by a\nlow-information stimulus (uniform heat) yet produces highly intricate shapes\nthrough programmed geometric design. The morphing behavior, confirmed by finite\nelement simulations, arises from strain mismatch between the contracting\nthermoplastic layer and the constraining Kirigami layer. By decoupling material\ncomposition from mechanical response, this method avoids detailed process\ncontrol and enables a broad class of self-morphing structures, offering a\nversatile platform for adaptive design and scalable manufacturing.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u65b9\u6cd5\uff0c\u5229\u7528\u5747\u5300\u52a0\u70ed\u548c\u5e38\u89c1\u5de5\u5177\uff08\u5982\u5bb6\u7528\u70e4\u7bb1\u548c\u526a\u5200\uff09\u5c06\u5e73\u9762\u5851\u6599\u7247\u81ea\u4e3b\u8f6c\u5316\u4e3a\u590d\u6742\u4e09\u7ef4\u7ed3\u6784\u3002", "motivation": "\u63a2\u7d22\u4e00\u79cd\u65e0\u9700\u590d\u6742\u63a7\u5236\u5373\u53ef\u5b9e\u73b0\u590d\u6742\u5f62\u72b6\u81ea\u53d8\u5f62\u7684\u901a\u7528\u5236\u9020\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u70ed\u6536\u7f29\u70ed\u5851\u6027\u5851\u6599\u4e0e\u5b9a\u5236Kirigami\u56fe\u6848\uff0c\u5f62\u6210\u53cc\u5c42\u590d\u5408\u6750\u6599\uff0c\u901a\u8fc7\u5747\u5300\u52a0\u70ed\u9a71\u52a8\u53d8\u5f62\u3002", "result": "\u6210\u529f\u5236\u9020\u51fa\u591a\u79cd\u590d\u6742\u7ed3\u6784\uff08\u5982\u7897\u3001\u91d1\u5b57\u5854\u3001\u9f20\u6807\u76d6\uff09\uff0c\u5e76\u901a\u8fc7\u6709\u9650\u5143\u6a21\u62df\u9a8c\u8bc1\u53d8\u5f62\u673a\u5236\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u51e0\u4f55\u8bbe\u8ba1\u5b9e\u73b0\u4f4e\u4fe1\u606f\u523a\u6fc0\u4e0b\u7684\u9ad8\u590d\u6742\u5ea6\u53d8\u5f62\uff0c\u4e3a\u81ea\u9002\u5e94\u8bbe\u8ba1\u548c\u89c4\u6a21\u5316\u5236\u9020\u63d0\u4f9b\u4e86\u901a\u7528\u5e73\u53f0\u3002"}}
{"id": "2506.22593", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22593", "abs": "https://arxiv.org/abs/2506.22593", "authors": ["Antonello Longo", "Chanyoung Chung", "Matteo Palieri", "Sung-Kyun Kim", "Ali Agha", "Cataldo Guaragnella", "Shehryar Khattak"], "title": "Pixels-to-Graph: Real-time Integration of Building Information Models and Scene Graphs for Semantic-Geometric Human-Robot Understanding", "comment": "Paper accepted to 2025 IEEE International Conference on Automation\n  Science and Engineering (CASE)", "summary": "Autonomous robots are increasingly playing key roles as support platforms for\nhuman operators in high-risk, dangerous applications. To accomplish challenging\ntasks, an efficient human-robot cooperation and understanding is required.\nWhile typically robotic planning leverages 3D geometric information, human\noperators are accustomed to a high-level compact representation of the\nenvironment, like top-down 2D maps representing the Building Information Model\n(BIM). 3D scene graphs have emerged as a powerful tool to bridge the gap\nbetween human readable 2D BIM and the robot 3D maps. In this work, we introduce\nPixels-to-Graph (Pix2G), a novel lightweight method to generate structured\nscene graphs from image pixels and LiDAR maps in real-time for the autonomous\nexploration of unknown environments on resource-constrained robot platforms. To\nsatisfy onboard compute constraints, the framework is designed to perform all\noperation on CPU only. The method output are a de-noised 2D top-down\nenvironment map and a structure-segmented 3D pointcloud which are seamlessly\nconnected using a multi-layer graph abstracting information from object-level\nup to the building-level. The proposed method is quantitatively and\nqualitatively evaluated during real-world experiments performed using the NASA\nJPL NeBula-Spot legged robot to autonomously explore and map cluttered garage\nand urban office like environments in real-time.", "AI": {"tldr": "Pix2G\u65b9\u6cd5\u901a\u8fc7\u5b9e\u65f6\u751f\u6210\u573a\u666f\u56fe\uff0c\u8fde\u63a52D BIM\u4e0e3D\u5730\u56fe\uff0c\u652f\u6301\u8d44\u6e90\u53d7\u9650\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u63a2\u7d22\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u64cd\u4f5c\u5458\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u56e0\u73af\u5883\u8868\u793a\u5dee\u5f02\uff082D BIM\u4e0e3D\u5730\u56fe\uff09\u5bfc\u81f4\u7684\u534f\u4f5c\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51faPix2G\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u50cf\u50cf\u7d20\u548cLiDAR\u5730\u56fe\u5b9e\u65f6\u751f\u6210\u7ed3\u6784\u5316\u573a\u666f\u56fe\uff0c\u4ec5\u4f7f\u7528CPU\u6ee1\u8db3\u8d44\u6e90\u9650\u5236\u3002", "result": "\u751f\u6210\u53bb\u566a2D\u5730\u56fe\u548c\u7ed3\u6784\u5206\u52723D\u70b9\u4e91\uff0c\u901a\u8fc7\u591a\u5c42\u56fe\u8fde\u63a5\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "Pix2G\u4e3a\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u652f\u6301\u81ea\u4e3b\u63a2\u7d22\u4efb\u52a1\u3002"}}
{"id": "2506.22766", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22766", "abs": "https://arxiv.org/abs/2506.22766", "authors": ["Yiting Chen", "Kenneth Kimble", "Howard H. Qian", "Podshara Chanrungmaneekul", "Robert Seney", "Kaiyu Hang"], "title": "Robust Peg-in-Hole Assembly under Uncertainties via Compliant and Interactive Contact-Rich Manipulation", "comment": "Accepted to Robotics: Science and Systems (RSS) 2025; 16 pages, 10\n  figures", "summary": "Robust and adaptive robotic peg-in-hole assembly under tight tolerances is\ncritical to various industrial applications. However, it remains an open\nchallenge due to perceptual and physical uncertainties from contact-rich\ninteractions that easily exceed the allowed clearance. In this paper, we study\nhow to leverage contact between the peg and its matching hole to eliminate\nuncertainties in the assembly process under unstructured settings. By examining\nthe role of compliance under contact constraints, we present a manipulation\nsystem that plans collision-inclusive interactions for the peg to 1)\niteratively identify its task environment to localize the target hole and 2)\nexploit environmental contact constraints to refine insertion motions into the\ntarget hole without relying on precise perception, enabling a robust solution\nto peg-in-hole assembly. By conceptualizing the above process as the\ncomposition of funneling in different state spaces, we present a formal\napproach to constructing manipulation funnels as an uncertainty-absorbing\nparadigm for peg-in-hole assembly. The proposed system effectively generalizes\nacross diverse peg-in-hole scenarios across varying scales, shapes, and\nmaterials in a learning-free manner. Extensive experiments on a NIST Assembly\nTask Board (ATB) and additional challenging scenarios validate its robustness\nin real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a5\u89e6\u7ea6\u675f\u7684\u9c81\u68d2\u81ea\u9002\u5e94\u673a\u5668\u4eba\u63d2\u5b54\u88c5\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u78b0\u649e\u5305\u5bb9\u6027\u4ea4\u4e92\u548c\u64cd\u7eb5\u6f0f\u6597\u6982\u5ff5\uff0c\u89e3\u51b3\u4e86\u7d27\u5bc6\u516c\u5dee\u4e0b\u7684\u4e0d\u786e\u5b9a\u6027\u6311\u6218\u3002", "motivation": "\u5de5\u4e1a\u5e94\u7528\u4e2d\u7d27\u5bc6\u516c\u5dee\u4e0b\u7684\u63d2\u5b54\u88c5\u914d\u5b58\u5728\u611f\u77e5\u548c\u7269\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002", "method": "\u5229\u7528\u63a5\u89e6\u7ea6\u675f\u6d88\u9664\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u78b0\u649e\u5305\u5bb9\u6027\u4ea4\u4e92\u548c\u64cd\u7eb5\u6f0f\u6597\u6982\u5ff5\u5b9e\u73b0\u88c5\u914d\u3002", "result": "\u7cfb\u7edf\u5728\u591a\u79cd\u63d2\u5b54\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u4f9d\u8d56\u7cbe\u786e\u611f\u77e5\u6216\u5b66\u4e60\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7d27\u5bc6\u516c\u5dee\u4e0b\u7684\u63d2\u5b54\u88c5\u914d\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22769", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22769", "abs": "https://arxiv.org/abs/2506.22769", "authors": ["Changshi Zhou", "Feng Luan", "Jiarui Hu", "Shaoqiang Meng", "Zhipeng Wang", "Yanchao Dong", "Yanmin Zhou", "Bin He"], "title": "Learning Efficient Robotic Garment Manipulation with Standardization", "comment": null, "summary": "Garment manipulation is a significant challenge for robots due to the complex\ndynamics and potential self-occlusion of garments. Most existing methods of\nefficient garment unfolding overlook the crucial role of standardization of\nflattened garments, which could significantly simplify downstream tasks like\nfolding, ironing, and packing. This paper presents APS-Net, a novel approach to\ngarment manipulation that combines unfolding and standardization in a unified\nframework. APS-Net employs a dual-arm, multi-primitive policy with dynamic\nfling to quickly unfold crumpled garments and pick-and-place (p and p) for\nprecise alignment. The purpose of garment standardization during unfolding\ninvolves not only maximizing surface coverage but also aligning the garment's\nshape and orientation to predefined requirements. To guide effective robot\nlearning, we introduce a novel factorized reward function for standardization,\nwhich incorporates garment coverage (Cov), keypoint distance (KD), and\nintersection-over-union (IoU) metrics. Additionally, we introduce a spatial\naction mask and an Action Optimized Module to improve unfolding efficiency by\nselecting actions and operation points effectively. In simulation, APS-Net\noutperforms state-of-the-art methods for long sleeves, achieving 3.9 percent\nbetter coverage, 5.2 percent higher IoU, and a 0.14 decrease in KD (7.09\npercent relative reduction). Real-world folding tasks further demonstrate that\nstandardization simplifies the folding process. Project page: see\nhttps://hellohaia.github.io/APS/", "AI": {"tldr": "APS-Net\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5c55\u5f00\u548c\u6807\u51c6\u5316\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u670d\u88c5\u64cd\u4f5c\uff0c\u901a\u8fc7\u52a8\u6001\u6295\u63b7\u548c\u7cbe\u786e\u5bf9\u9f50\u5b9e\u73b0\u9ad8\u6548\u5c55\u5f00\u548c\u6807\u51c6\u5316\u3002", "motivation": "\u73b0\u6709\u670d\u88c5\u5c55\u5f00\u65b9\u6cd5\u5ffd\u89c6\u4e86\u6807\u51c6\u5316\u7684\u91cd\u8981\u6027\uff0c\u800c\u6807\u51c6\u5316\u80fd\u663e\u8457\u7b80\u5316\u540e\u7eed\u4efb\u52a1\uff08\u5982\u6298\u53e0\u3001\u71a8\u70eb\u548c\u5305\u88c5\uff09\u3002", "method": "\u91c7\u7528\u53cc\u81c2\u591a\u57fa\u5143\u7b56\u7565\uff0c\u7ed3\u5408\u52a8\u6001\u6295\u63b7\u548c\u7cbe\u786e\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u56e0\u5b50\u5316\u5956\u52b1\u51fd\u6570\uff08\u8986\u76d6\u7387\u3001\u5173\u952e\u70b9\u8ddd\u79bb\u548cIoU\uff09\u6307\u5bfc\u5b66\u4e60\u3002", "result": "\u5728\u4eff\u771f\u4e2d\uff0cAPS-Net\u5728\u957f\u8896\u670d\u88c5\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8986\u76d6\u7387\u63d0\u9ad83.9%\uff0cIoU\u63d0\u9ad85.2%\uff0c\u5173\u952e\u70b9\u8ddd\u79bb\u51cf\u5c110.14\u3002", "conclusion": "\u6807\u51c6\u5316\u7b80\u5316\u4e86\u6298\u53e0\u4efb\u52a1\uff0cAPS-Net\u5728\u670d\u88c5\u64cd\u4f5c\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.22788", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22788", "abs": "https://arxiv.org/abs/2506.22788", "authors": ["Xuao Hou", "Yongquan Jia", "Shijin Zhang", "Yuqiang Wu"], "title": "SPI-BoTER: Error Compensation for Industrial Robots via Sparse Attention Masking and Hybrid Loss with Spatial-Physical Information", "comment": null, "summary": "The widespread application of industrial robots in fields such as cutting and\nwelding has imposed increasingly stringent requirements on the trajectory\naccuracy of end-effectors. However, current error compensation methods face\nseveral critical challenges, including overly simplified mechanism modeling, a\nlack of physical consistency in data-driven approaches, and substantial data\nrequirements. These issues make it difficult to achieve both high accuracy and\nstrong generalization simultaneously. To address these challenges, this paper\nproposes a Spatial-Physical Informed Attention Residual Network (SPI-BoTER).\nThis method integrates the kinematic equations of the robotic manipulator with\na Transformer architecture enhanced by sparse self-attention masks. A\nparameter-adaptive hybrid loss function incorporating spatial and physical\ninformation is employed to iteratively optimize the network during training,\nenabling high-precision error compensation under small-sample conditions.\nAdditionally, inverse joint angle compensation is performed using a gradient\ndescent-based optimization method. Experimental results on a small-sample\ndataset from a UR5 robotic arm (724 samples, with a train:test:validation split\nof 8:1:1) demonstrate the superior performance of the proposed method. It\nachieves a 3D absolute positioning error of 0.2515 mm with a standard deviation\nof 0.15 mm, representing a 35.16\\% reduction in error compared to conventional\ndeep neural network (DNN) methods. Furthermore, the inverse angle compensation\nalgorithm converges to an accuracy of 0.01 mm within an average of 147\niterations. This study presents a solution that combines physical\ninterpretability with data adaptability for high-precision control of\nindustrial robots, offering promising potential for the reliable execution of\nprecision tasks in intelligent manufacturing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u6a21\u578b\u4e0eTransformer\u67b6\u6784\u7684\u65b9\u6cd5\uff08SPI-BoTER\uff09\uff0c\u7528\u4e8e\u5de5\u4e1a\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u7684\u9ad8\u7cbe\u5ea6\u8bef\u5dee\u8865\u507f\uff0c\u5728\u5c0f\u6837\u672c\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a\u673a\u5668\u4eba\u8f68\u8ff9\u7cbe\u5ea6\u8981\u6c42\u65e5\u76ca\u4e25\u683c\uff0c\u4f46\u73b0\u6709\u8bef\u5dee\u8865\u507f\u65b9\u6cd5\u5b58\u5728\u5efa\u6a21\u7b80\u5316\u3001\u6570\u636e\u9a71\u52a8\u7f3a\u4e4f\u7269\u7406\u4e00\u81f4\u6027\u53ca\u6570\u636e\u9700\u6c42\u5927\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4e0e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faSPI-BoTER\u65b9\u6cd5\uff0c\u878d\u5408\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u65b9\u7a0b\u4e0e\u7a00\u758f\u81ea\u6ce8\u610f\u529bTransformer\u67b6\u6784\uff0c\u91c7\u7528\u53c2\u6570\u81ea\u9002\u5e94\u6df7\u5408\u635f\u5931\u51fd\u6570\u8fed\u4ee3\u4f18\u5316\u7f51\u7edc\uff0c\u5e76\u7ed3\u5408\u68af\u5ea6\u4e0b\u964d\u8fdb\u884c\u9006\u5173\u8282\u89d2\u8865\u507f\u3002", "result": "\u5728UR5\u673a\u68b0\u81c2\u5c0f\u6837\u672c\u6570\u636e\u96c6\u4e0a\uff0c3D\u7edd\u5bf9\u5b9a\u4f4d\u8bef\u5dee\u4e3a0.2515 mm\uff08\u6807\u51c6\u5dee0.15 mm\uff09\uff0c\u8f83\u4f20\u7edfDNN\u65b9\u6cd5\u8bef\u5dee\u964d\u4f4e35.16%\uff1b\u9006\u89d2\u5ea6\u8865\u507f\u7b97\u6cd5\u5e73\u5747147\u6b21\u8fed\u4ee3\u6536\u655b\u81f30.01 mm\u7cbe\u5ea6\u3002", "conclusion": "SPI-BoTER\u7ed3\u5408\u7269\u7406\u53ef\u89e3\u91ca\u6027\u4e0e\u6570\u636e\u9002\u5e94\u6027\uff0c\u4e3a\u5de5\u4e1a\u673a\u5668\u4eba\u9ad8\u7cbe\u5ea6\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63a8\u52a8\u667a\u80fd\u5236\u9020\u4e2d\u7cbe\u5bc6\u4efb\u52a1\u7684\u53ef\u9760\u6267\u884c\u3002"}}
{"id": "2506.22827", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22827", "abs": "https://arxiv.org/abs/2506.22827", "authors": ["Andr\u00e9 Schakkal", "Ben Zandonati", "Zhutian Yang", "Navid Azizan"], "title": "Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation", "comment": "Accepted at the RSS 2025 Workshop on Robot Planning in the Era of\n  Foundation Models", "summary": "Enabling humanoid robots to reliably execute complex multi-step manipulation\ntasks is crucial for their effective deployment in industrial and household\nenvironments. This paper presents a hierarchical planning and control framework\ndesigned to achieve reliable multi-step humanoid manipulation. The proposed\nsystem comprises three layers: (1) a low-level RL-based controller responsible\nfor tracking whole-body motion targets; (2) a mid-level set of skill policies\ntrained via imitation learning that produce motion targets for different steps\nof a task; and (3) a high-level vision-language planning module that determines\nwhich skills should be executed and also monitors their completion in real-time\nusing pretrained vision-language models (VLMs). Experimental validation is\nperformed on a Unitree G1 humanoid robot executing a non-prehensile\npick-and-place task. Over 40 real-world trials, the hierarchical system\nachieved a 72.5% success rate in completing the full manipulation sequence.\nThese experiments confirm the feasibility of the proposed hierarchical system,\nhighlighting the benefits of VLM-based skill planning and monitoring for\nmulti-step manipulation scenarios. See https://vlp-humanoid.github.io/ for\nvideo demonstrations of the policy rollout.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u89c4\u5212\u4e0e\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u53ef\u9760\u7684\u591a\u6b65\u9aa4\u4eba\u5f62\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u3001\u6a21\u4eff\u5b66\u4e60\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "motivation": "\u4e3a\u4e86\u5728\u5de5\u4e1a\u548c\u5bb6\u5ead\u73af\u5883\u4e2d\u6709\u6548\u90e8\u7f72\u4eba\u5f62\u673a\u5668\u4eba\uff0c\u9700\u8981\u89e3\u51b3\u5176\u6267\u884c\u590d\u6742\u591a\u6b65\u9aa4\u64cd\u4f5c\u4efb\u52a1\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u5206\u4e3a\u4e09\u5c42\uff1a\u4f4e\u5c42RL\u63a7\u5236\u5668\u8ddf\u8e2a\u5168\u8eab\u8fd0\u52a8\u76ee\u6807\uff1b\u4e2d\u5c42\u6a21\u4eff\u5b66\u4e60\u6280\u80fd\u7b56\u7565\u751f\u6210\u4efb\u52a1\u6b65\u9aa4\u7684\u8fd0\u52a8\u76ee\u6807\uff1b\u9ad8\u5c42\u89c6\u89c9\u8bed\u8a00\u89c4\u5212\u6a21\u5757\u51b3\u5b9a\u6280\u80fd\u6267\u884c\u5e76\u5b9e\u65f6\u76d1\u63a7\u3002", "result": "\u5728Unitree G1\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u4e2d\uff0c\u7cfb\u7edf\u5728\u591a\u6b65\u9aa4\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8672.5%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u5206\u5c42\u7cfb\u7edf\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6280\u80fd\u89c4\u5212\u548c\u76d1\u63a7\u5728\u591a\u6b65\u9aa4\u64cd\u4f5c\u573a\u666f\u4e2d\u5177\u6709\u53ef\u884c\u6027\u3002"}}
{"id": "2506.22894", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22894", "abs": "https://arxiv.org/abs/2506.22894", "authors": ["Bei Zhou", "Baha Zarrouki", "Mattia Piccinini", "Cheng Hu", "Lei Xie", "Johannes Betz"], "title": "Safe Reinforcement Learning with a Predictive Safety Filter for Motion Planning and Control: A Drifting Vehicle Example", "comment": null, "summary": "Autonomous drifting is a complex and crucial maneuver for safety-critical\nscenarios like slippery roads and emergency collision avoidance, requiring\nprecise motion planning and control. Traditional motion planning methods often\nstruggle with the high instability and unpredictability of drifting,\nparticularly when operating at high speeds. Recent learning-based approaches\nhave attempted to tackle this issue but often rely on expert knowledge or have\nlimited exploration capabilities. Additionally, they do not effectively address\nsafety concerns during learning and deployment. To overcome these limitations,\nwe propose a novel Safe Reinforcement Learning (RL)-based motion planner for\nautonomous drifting. Our approach integrates an RL agent with model-based drift\ndynamics to determine desired drift motion states, while incorporating a\nPredictive Safety Filter (PSF) that adjusts the agent's actions online to\nprevent unsafe states. This ensures safe and efficient learning, and stable\ndrift operation. We validate the effectiveness of our method through\nsimulations on a Matlab-Carsim platform, demonstrating significant improvements\nin drift performance, reduced tracking errors, and computational efficiency\ncompared to traditional methods. This strategy promises to extend the\ncapabilities of autonomous vehicles in safety-critical maneuvers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6f02\u79fb\uff0c\u7ed3\u5408\u6a21\u578b\u6f02\u79fb\u52a8\u529b\u5b66\u548c\u9884\u6d4b\u5b89\u5168\u8fc7\u6ee4\u5668\uff08PSF\uff09\uff0c\u786e\u4fdd\u5b89\u5168\u9ad8\u6548\u7684\u5b66\u4e60\u548c\u7a33\u5b9a\u6f02\u79fb\u64cd\u4f5c\u3002", "motivation": "\u4f20\u7edf\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u5728\u9ad8\u901f\u6f02\u79fb\u65f6\u96be\u4ee5\u5e94\u5bf9\u4e0d\u7a33\u5b9a\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u7684\u63a2\u7d22\u80fd\u529b\u6709\u9650\u4e14\u5b89\u5168\u6027\u4e0d\u8db3\u3002", "method": "\u96c6\u6210RL\u4ee3\u7406\u4e0e\u6a21\u578b\u6f02\u79fb\u52a8\u529b\u5b66\uff0c\u901a\u8fc7PSF\u5728\u7ebf\u8c03\u6574\u52a8\u4f5c\u4ee5\u9632\u6b62\u4e0d\u5b89\u5168\u72b6\u6001\u3002", "result": "\u5728Matlab-Carsim\u5e73\u53f0\u4e0a\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u6f02\u79fb\u6027\u80fd\u3001\u51cf\u5c11\u8ddf\u8e2a\u8bef\u5dee\u5e76\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u671b\u6269\u5c55\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u80fd\u529b\u3002"}}
{"id": "2506.22942", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22942", "abs": "https://arxiv.org/abs/2506.22942", "authors": ["Kartik A. Pant", "Jaehyeok Kim", "James M. Goppert", "Inseok Hwang"], "title": "Energy-Constrained Resilient Multi-Robot Coverage Control", "comment": "6 pages, 4 figures", "summary": "The problem of multi-robot coverage control becomes significantly challenging\nwhen multiple robots leave the mission space simultaneously to charge their\nbatteries, disrupting the underlying network topology for communication and\nsensing. To address this, we propose a resilient network design and control\napproach that allows robots to achieve the desired coverage performance while\nsatisfying energy constraints and maintaining network connectivity throughout\nthe mission. We model the combined motion, energy, and network dynamics of the\nmultirobot systems (MRS) as a hybrid system with three modes, i.e., coverage,\nreturn-to-base, and recharge, respectively. We show that ensuring the energy\nconstraints can be transformed into designing appropriate guard conditions for\nmode transition between each of the three modes. Additionally, we present a\nsystematic procedure to design, maintain, and reconfigure the underlying\nnetwork topology using an energy-aware bearing rigid network design, enhancing\nthe structural resilience of the MRS even when a subset of robots departs to\ncharge their batteries. Finally, we validate our proposed method using\nnumerical simulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u673a\u5668\u4eba\u8986\u76d6\u63a7\u5236\u7684\u5f39\u6027\u7f51\u7edc\u8bbe\u8ba1\u4e0e\u63a7\u5236\u65b9\u6cd5\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u540c\u65f6\u79bb\u5f00\u4efb\u52a1\u7a7a\u95f4\u5145\u7535\u65f6\u7f51\u7edc\u62d3\u6251\u4e2d\u65ad\u7684\u95ee\u9898\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u8986\u76d6\u63a7\u5236\u4e2d\uff0c\u673a\u5668\u4eba\u540c\u65f6\u79bb\u5f00\u5145\u7535\u4f1a\u7834\u574f\u901a\u4fe1\u548c\u611f\u77e5\u7684\u7f51\u7edc\u62d3\u6251\uff0c\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u5c06\u673a\u5668\u4eba\u8fd0\u52a8\u3001\u80fd\u91cf\u548c\u7f51\u7edc\u52a8\u6001\u5efa\u6a21\u4e3a\u6df7\u5408\u7cfb\u7edf\uff0c\u8bbe\u8ba1\u6a21\u5f0f\u8f6c\u6362\u6761\u4ef6\u548c\u80fd\u91cf\u611f\u77e5\u7684\u8f74\u627f\u521a\u6027\u7f51\u7edc\u62d3\u6251\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u786e\u4fdd\u8986\u76d6\u6027\u80fd\u3001\u80fd\u91cf\u7ea6\u675f\u548c\u7f51\u7edc\u8fde\u901a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u589e\u5f3a\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7ed3\u6784\u5f39\u6027\uff0c\u652f\u6301\u673a\u5668\u4eba\u5145\u7535\u65f6\u7684\u4efb\u52a1\u8fde\u7eed\u6027\u3002"}}
{"id": "2506.22956", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22956", "abs": "https://arxiv.org/abs/2506.22956", "authors": ["David Rodr\u00edguez-Mart\u00ednez", "Dave van der Meer", "Junlin Song", "Abishek Bera", "C. J. P\u00e9rez-del-Pulgar", "Miguel Angel Olivares-Mendez"], "title": "SPICE-HL3: Single-Photon, Inertial, and Stereo Camera dataset for Exploration of High-Latitude Lunar Landscapes", "comment": "10 pages, 8 figures, dataset", "summary": "Exploring high-latitude lunar regions presents an extremely challenging\nvisual environment for robots. The low sunlight elevation angle and minimal\nlight scattering result in a visual field dominated by a high dynamic range\nfeaturing long, dynamic shadows. Reproducing these conditions on Earth requires\nsophisticated simulators and specialized facilities. We introduce a unique\ndataset recorded at the LunaLab from the SnT - University of Luxembourg, an\nindoor test facility designed to replicate the optical characteristics of\nmultiple lunar latitudes. Our dataset includes images, inertial measurements,\nand wheel odometry data from robots navigating seven distinct trajectories\nunder multiple illumination scenarios, simulating high-latitude lunar\nconditions from dawn to night time with and without the aid of headlights,\nresulting in 88 distinct sequences containing a total of 1.3M images. Data was\ncaptured using a stereo RGB-inertial sensor, a monocular monochrome camera, and\nfor the first time, a novel single-photon avalanche diode (SPAD) camera. We\nrecorded both static and dynamic image sequences, with robots navigating at\nslow (5 cm/s) and fast (50 cm/s) speeds. All data is calibrated, synchronized,\nand timestamped, providing a valuable resource for validating perception tasks\nfrom vision-based autonomous navigation to scientific imaging for future lunar\nmissions targeting high-latitude regions or those intended for robots operating\nacross perceptually degraded environments. The dataset can be downloaded from\nhttps://zenodo.org/records/13970078?preview=1, and a visual overview is\navailable at https://youtu.be/d7sPeO50_2I. All supplementary material can be\nfound at https://github.com/spaceuma/spice-hl3.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5728LunaLab\u8bbe\u65bd\u4e2d\u8bb0\u5f55\u7684\u9ad8\u7eac\u5ea6\u6708\u7403\u73af\u5883\u6a21\u62df\u6570\u636e\u96c6\uff0c\u5305\u542b\u56fe\u50cf\u3001\u60ef\u6027\u6d4b\u91cf\u548c\u8f6e\u5f0f\u91cc\u7a0b\u6570\u636e\uff0c\u7528\u4e8e\u9a8c\u8bc1\u611f\u77e5\u4efb\u52a1\u3002", "motivation": "\u9ad8\u7eac\u5ea6\u6708\u7403\u533a\u57df\u7684\u89c6\u89c9\u73af\u5883\u5bf9\u673a\u5668\u4eba\u6781\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u6a21\u62df\u4f4e\u592a\u9633\u9ad8\u5ea6\u89d2\u548c\u52a8\u6001\u9634\u5f71\u7684\u6761\u4ef6\u3002", "method": "\u4f7f\u7528\u7acb\u4f53RGB-\u60ef\u6027\u4f20\u611f\u5668\u3001\u5355\u8272\u76f8\u673a\u548c\u65b0\u578bSPAD\u76f8\u673a\uff0c\u5728\u591a\u79cd\u5149\u7167\u6761\u4ef6\u4e0b\u8bb0\u5f55\u673a\u5668\u4eba\u5bfc\u822a\u6570\u636e\u3002", "result": "\u751f\u6210\u4e8688\u4e2a\u5e8f\u5217\u5171130\u4e07\u5f20\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u9759\u6001\u548c\u52a8\u6001\u573a\u666f\uff0c\u9002\u7528\u4e8e\u611f\u77e5\u4efb\u52a1\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u672a\u6765\u6708\u7403\u4efb\u52a1\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u548c\u79d1\u5b66\u6210\u50cf\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2506.23023", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23023", "abs": "https://arxiv.org/abs/2506.23023", "authors": ["M. Youssef Abdelhamid", "Lennart Vater", "Zlatan Ajanovic"], "title": "Scenario-Based Hierarchical Reinforcement Learning for Automated Driving Decision Making", "comment": "6 pages, 10 figures, submitted to a conference", "summary": "Developing decision-making algorithms for highly automated driving systems\nremains challenging, since these systems have to operate safely in an open and\ncomplex environments. Reinforcement Learning (RL) approaches can learn\ncomprehensive decision policies directly from experience and already show\npromising results in simple driving tasks. However, current approaches fail to\nachieve generalizability for more complex driving tasks and lack learning\nefficiency. Therefore, we present Scenario-based Automated Driving\nReinforcement Learning (SAD-RL), the first framework that integrates\nReinforcement Learning (RL) of hierarchical policy in a scenario-based\nenvironment. A high-level policy selects maneuver templates that are evaluated\nand executed by a low-level control logic. The scenario-based environment\nallows to control the training experience for the agent and to explicitly\nintroduce challenging, but rate situations into the training process. Our\nexperiments show that an agent trained using the SAD-RL framework can achieve\nsafe behaviour in easy as well as challenging situations efficiently. Our\nablation studies confirmed that both HRL and scenario diversity are essential\nfor achieving these results.", "AI": {"tldr": "\u63d0\u51faSAD-RL\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u548c\u573a\u666f\u5316\u73af\u5883\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7b97\u6cd5\u7684\u901a\u7528\u6027\u548c\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9700\u5728\u590d\u6742\u5f00\u653e\u73af\u5883\u4e2d\u5b89\u5168\u8fd0\u884c\uff0c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u901a\u7528\u6027\u548c\u6548\u7387\u4e0d\u8db3\u3002", "method": "SAD-RL\u6846\u67b6\u7ed3\u5408\u5206\u5c42\u7b56\u7565\uff08\u9ad8\u5c42\u9009\u62e9\u6a21\u677f\uff0c\u4f4e\u5c42\u6267\u884c\uff09\u548c\u573a\u666f\u5316\u73af\u5883\uff0c\u63a7\u5236\u8bad\u7ec3\u4f53\u9a8c\u5e76\u5f15\u5165\u6311\u6218\u6027\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSAD-RL\u8bad\u7ec3\u7684\u4ee3\u7406\u80fd\u5728\u7b80\u5355\u548c\u590d\u6742\u573a\u666f\u4e2d\u9ad8\u6548\u5b9e\u73b0\u5b89\u5168\u884c\u4e3a\uff0c\u5206\u5c42\u5b66\u4e60\u548c\u573a\u666f\u591a\u6837\u6027\u662f\u5173\u952e\u3002", "conclusion": "SAD-RL\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u7b56\u7565\u548c\u573a\u666f\u5316\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7b97\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23078", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23078", "abs": "https://arxiv.org/abs/2506.23078", "authors": ["Zhaoxing Zhang", "Xiaoxiang Wang", "Chengliang Zhang", "Yangyang Guo", "Zikang Yuan", "Xin Yang"], "title": "Event-based Stereo Visual-Inertial Odometry with Voxel Map", "comment": null, "summary": "The event camera, renowned for its high dynamic range and exceptional\ntemporal resolution, is recognized as an important sensor for visual odometry.\nHowever, the inherent noise in event streams complicates the selection of\nhigh-quality map points, which critically determine the precision of state\nestimation. To address this challenge, we propose Voxel-ESVIO, an event-based\nstereo visual-inertial odometry system that utilizes voxel map management,\nwhich efficiently filter out high-quality 3D points. Specifically, our\nmethodology utilizes voxel-based point selection and voxel-aware point\nmanagement to collectively optimize the selection and updating of map points on\na per-voxel basis. These synergistic strategies enable the efficient retrieval\nof noise-resilient map points with the highest observation likelihood in\ncurrent frames, thereby ensureing the state estimation accuracy. Extensive\nevaluations on three public benchmarks demonstrate that our Voxel-ESVIO\noutperforms state-of-the-art methods in both accuracy and computational\nefficiency.", "AI": {"tldr": "Voxel-ESVIO\u662f\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u7acb\u4f53\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f53\u7d20\u5730\u56fe\u7ba1\u7406\u9ad8\u6548\u7b5b\u9009\u9ad8\u8d28\u91cf3D\u70b9\uff0c\u63d0\u5347\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u5353\u8d8a\u65f6\u95f4\u5206\u8fa8\u7387\u4f7f\u5176\u6210\u4e3a\u89c6\u89c9\u91cc\u7a0b\u8ba1\u7684\u91cd\u8981\u4f20\u611f\u5668\uff0c\u4f46\u4e8b\u4ef6\u6d41\u4e2d\u7684\u56fa\u6709\u566a\u58f0\u5f71\u54cd\u4e86\u9ad8\u8d28\u91cf\u5730\u56fe\u70b9\u7684\u9009\u62e9\uff0c\u8fdb\u800c\u5f71\u54cd\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faVoxel-ESVIO\u7cfb\u7edf\uff0c\u91c7\u7528\u4f53\u7d20\u5730\u56fe\u7ba1\u7406\uff0c\u901a\u8fc7\u4f53\u7d20\u70b9\u9009\u62e9\u548c\u4f53\u7d20\u611f\u77e5\u70b9\u7ba1\u7406\u4f18\u5316\u5730\u56fe\u70b9\u7684\u9009\u62e9\u548c\u66f4\u65b0\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVoxel-ESVIO\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Voxel-ESVIO\u901a\u8fc7\u4f53\u7d20\u7ba1\u7406\u7b56\u7565\u6709\u6548\u7b5b\u9009\u566a\u58f0\u9c81\u68d2\u7684\u5730\u56fe\u70b9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u72b6\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.23114", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23114", "abs": "https://arxiv.org/abs/2506.23114", "authors": ["Zhanxiang Cao", "Buqing Nie", "Yang Zhang", "Yue Gao"], "title": "Minimizing Acoustic Noise: Enhancing Quiet Locomotion for Quadruped Robots in Indoor Applications", "comment": "8 pages,6 figures, IROS2025", "summary": "Recent advancements in quadruped robot research have significantly improved\ntheir ability to traverse complex and unstructured outdoor environments.\nHowever, the issue of noise generated during locomotion is generally\noverlooked, which is critically important in noise-sensitive indoor\nenvironments, such as service and healthcare settings, where maintaining low\nnoise levels is essential. This study aims to optimize the acoustic noise\ngenerated by quadruped robots during locomotion through the development of\nadvanced motion control algorithms. To achieve this, we propose a novel\napproach that minimizes noise emissions by integrating optimized gait design\nwith tailored control strategies. This method achieves an average noise\nreduction of approximately 8 dBA during movement, thereby enhancing the\nsuitability of quadruped robots for deployment in noise-sensitive indoor\nenvironments. Experimental results demonstrate the effectiveness of this\napproach across various indoor settings, highlighting the potential of\nquadruped robots for quiet operation in noise-sensitive environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u6b65\u6001\u8bbe\u8ba1\u548c\u63a7\u5236\u7b56\u7565\uff0c\u964d\u4f4e\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u8fd0\u52a8\u65f6\u7684\u566a\u97f3\uff0c\u5e73\u5747\u51cf\u5c11\u7ea68 dBA\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u566a\u97f3\u654f\u611f\u7684\u5ba4\u5185\u73af\u5883\u3002", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u7684\u79fb\u52a8\u80fd\u529b\u5df2\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5176\u8fd0\u52a8\u65f6\u4ea7\u751f\u7684\u566a\u97f3\u95ee\u9898\u5728\u566a\u97f3\u654f\u611f\u7684\u5ba4\u5185\u73af\u5883\uff08\u5982\u670d\u52a1\u548c\u533b\u7597\u573a\u6240\uff09\u4e2d\u88ab\u5ffd\u89c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6b65\u6001\u8bbe\u8ba1\u548c\u5b9a\u5236\u63a7\u5236\u7b56\u7565\u6765\u6700\u5c0f\u5316\u566a\u97f3\u6392\u653e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5ba4\u5185\u73af\u5883\u4e2d\u6709\u6548\uff0c\u5e73\u5747\u566a\u97f3\u964d\u4f4e\u4e86\u7ea68 dBA\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u566a\u97f3\u654f\u611f\u73af\u5883\u4e2d\u5b9e\u73b0\u5b89\u9759\u64cd\u4f5c\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.23125", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23125", "abs": "https://arxiv.org/abs/2506.23125", "authors": ["Zhanxiang Cao", "Yang Zhang", "Buqing Nie", "Huangxuan Lin", "Haoyang Li", "Yue Gao"], "title": "Learning Motion Skills with Adaptive Assistive Curriculum Force in Humanoid Robots", "comment": "8 pages, 8 figures", "summary": "Learning policies for complex humanoid tasks remains both challenging and\ncompelling. Inspired by how infants and athletes rely on external support--such\nas parental walkers or coach-applied guidance--to acquire skills like walking,\ndancing, and performing acrobatic flips, we propose A2CF: Adaptive Assistive\nCurriculum Force for humanoid motion learning. A2CF trains a dual-agent system,\nin which a dedicated assistive force agent applies state-dependent forces to\nguide the robot through difficult initial motions and gradually reduces\nassistance as the robot's proficiency improves. Across three\nbenchmarks--bipedal walking, choreographed dancing, and backflip--A2CF achieves\nconvergence 30% faster than baseline methods, lowers failure rates by over 40%,\nand ultimately produces robust, support-free policies. Real-world experiments\nfurther demonstrate that adaptively applied assistive forces significantly\naccelerate the acquisition of complex skills in high-dimensional robotic\ncontrol.", "AI": {"tldr": "A2CF\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u8f85\u52a9\u529b\u52a0\u901f\u4eba\u5f62\u673a\u5668\u4eba\u5b66\u4e60\u590d\u6742\u52a8\u4f5c\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5feb30%\uff0c\u5931\u8d25\u7387\u964d\u4f4e40%\u3002", "motivation": "\u53d7\u5a74\u513f\u548c\u8fd0\u52a8\u5458\u4f9d\u8d56\u5916\u90e8\u652f\u6301\u5b66\u4e60\u6280\u80fd\u7684\u542f\u53d1\uff0c\u63d0\u51faA2CF\u65b9\u6cd5\u4ee5\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u590d\u6742\u4efb\u52a1\u5b66\u4e60\u96be\u9898\u3002", "method": "\u8bad\u7ec3\u53cc\u4ee3\u7406\u7cfb\u7edf\uff0c\u8f85\u52a9\u529b\u4ee3\u7406\u6839\u636e\u72b6\u6001\u65bd\u52a0\u529b\u5e76\u9010\u6b65\u51cf\u5c11\u8f85\u52a9\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u719f\u7ec3\u5ea6\u3002", "result": "\u5728\u884c\u8d70\u3001\u821e\u8e48\u548c\u540e\u7a7a\u7ffb\u4efb\u52a1\u4e2d\uff0cA2CF\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6536\u655b\u5feb30%\uff0c\u5931\u8d25\u7387\u964d\u4f4e40%\u3002", "conclusion": "\u81ea\u9002\u5e94\u8f85\u52a9\u529b\u663e\u8457\u52a0\u901f\u9ad8\u7ef4\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u590d\u6742\u6280\u80fd\u7684\u83b7\u53d6\u3002"}}
{"id": "2506.23126", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23126", "abs": "https://arxiv.org/abs/2506.23126", "authors": ["Suning Huang", "Qianzhong Chen", "Xiaohan Zhang", "Jiankai Sun", "Mac Schwager"], "title": "ParticleFormer: A 3D Point Cloud World Model for Multi-Object, Multi-Material Robotic Manipulation", "comment": null, "summary": "3D world models (i.e., learning-based 3D dynamics models) offer a promising\napproach to generalizable robotic manipulation by capturing the underlying\nphysics of environment evolution conditioned on robot actions. However,\nexisting 3D world models are primarily limited to single-material dynamics\nusing a particle-based Graph Neural Network model, and often require\ntime-consuming 3D scene reconstruction to obtain 3D particle tracks for\ntraining. In this work, we present ParticleFormer, a Transformer-based point\ncloud world model trained with a hybrid point cloud reconstruction loss,\nsupervising both global and local dynamics features in multi-material,\nmulti-object robot interactions. ParticleFormer captures fine-grained\nmulti-object interactions between rigid, deformable, and flexible materials,\ntrained directly from real-world robot perception data without an elaborate\nscene reconstruction. We demonstrate the model's effectiveness both in 3D scene\nforecasting tasks, and in downstream manipulation tasks using a Model\nPredictive Control (MPC) policy. In addition, we extend existing dynamics\nlearning benchmarks to include diverse multi-material, multi-object interaction\nscenarios. We validate our method on six simulation and three real-world\nexperiments, where it consistently outperforms leading baselines by achieving\nsuperior dynamics prediction accuracy and less rollout error in downstream\nvisuomotor tasks. Experimental videos are available at\nhttps://particleformer.github.io/.", "AI": {"tldr": "ParticleFormer\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u70b9\u4e91\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u70b9\u4e91\u91cd\u5efa\u635f\u5931\u8bad\u7ec3\uff0c\u80fd\u591f\u6355\u6349\u591a\u6750\u6599\u3001\u591a\u7269\u4f53\u4ea4\u4e92\u7684\u7cbe\u7ec6\u52a8\u6001\u7279\u5f81\uff0c\u65e0\u9700\u590d\u6742\u573a\u666f\u91cd\u5efa\u3002", "motivation": "\u73b0\u67093D\u4e16\u754c\u6a21\u578b\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u6750\u6599\u52a8\u6001\uff0c\u4e14\u9700\u8981\u8017\u65f6\u76843D\u573a\u666f\u91cd\u5efa\u3002ParticleFormer\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u76f4\u63a5\u5229\u7528\u771f\u5b9e\u673a\u5668\u4eba\u611f\u77e5\u6570\u636e\u8bad\u7ec3\u3002", "method": "\u91c7\u7528Transformer\u67b6\u6784\u548c\u6df7\u5408\u70b9\u4e91\u91cd\u5efa\u635f\u5931\uff0c\u76d1\u7763\u5168\u5c40\u548c\u5c40\u90e8\u52a8\u6001\u7279\u5f81\uff0c\u9002\u7528\u4e8e\u591a\u6750\u6599\u3001\u591a\u7269\u4f53\u4ea4\u4e92\u3002", "result": "\u57283D\u573a\u666f\u9884\u6d4b\u548c\u4e0b\u6e38\u64cd\u63a7\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u5747\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ParticleFormer\u5728\u591a\u6750\u6599\u3001\u591a\u7269\u4f53\u4ea4\u4e92\u7684\u52a8\u6001\u9884\u6d4b\u548c\u673a\u5668\u4eba\u64cd\u63a7\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2506.23129", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23129", "abs": "https://arxiv.org/abs/2506.23129", "authors": ["Hossein B. Jond", "Logan Beaver", "Martin Jirou\u0161ek", "Naiemeh Ahmadlou", "Veli Bak\u0131rc\u0131o\u011flu", "Martin Saska"], "title": "Flatness-based Finite-Horizon Multi-UAV Formation Trajectory Planning and Directionally Aware Collision Avoidance Tracking", "comment": null, "summary": "Collision-free optimal formation control of unmanned aerial vehicle (UAV)\nteams is challenging. The state-of-the-art optimal control approaches often\nrely on numerical methods sensitive to initial guesses. This paper presents an\ninnovative collision-free finite-time formation control scheme for multiple\nUAVs leveraging the differential flatness of the UAV dynamics, eliminating the\nneed for numerical methods. We formulate a finite-time optimal control problem\nto plan a formation trajectory for feasible initial states. This formation\ntrajectory planning optimal control problem involves a collective performance\nindex to meet the formation requirements of achieving relative positions and\nvelocity consensus. It is solved by applying Pontryagin's principle.\nSubsequently, a collision-constrained regulating problem is addressed to ensure\ncollision-free tracking of the planned formation trajectory. The tracking\nproblem incorporates a directionally aware collision avoidance strategy that\nprioritizes avoiding UAVs in the forward path and relative approach. It assigns\nlower priority to those on the sides with an oblique relative approach and\ndisregards UAVs behind and not in the relative approach. The simulation results\nfor a four-UAV team (re)formation problem confirm the efficacy of the proposed\ncontrol scheme.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fae\u5206\u5e73\u5766\u5ea6\u7684\u65e0\u4eba\u673a\u7f16\u961f\u63a7\u5236\u65b9\u6848\uff0c\u907f\u514d\u4e86\u6570\u503c\u65b9\u6cd5\u4f9d\u8d56\uff0c\u901a\u8fc7\u6709\u9650\u65f6\u95f4\u6700\u4f18\u63a7\u5236\u548c\u78b0\u649e\u7ea6\u675f\u8c03\u8282\u5b9e\u73b0\u65e0\u78b0\u649e\u7f16\u961f\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u7f16\u961f\u63a7\u5236\u4e2d\u521d\u59cb\u731c\u6d4b\u654f\u611f\u548c\u78b0\u649e\u907f\u514d\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u5fae\u5206\u5e73\u5766\u6027\u8bbe\u8ba1\u6709\u9650\u65f6\u95f4\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u7ed3\u5408\u78b0\u649e\u7ea6\u675f\u8c03\u8282\u548c\u65b9\u5411\u611f\u77e5\u907f\u969c\u7b56\u7565\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u56db\u65e0\u4eba\u673a\u7f16\u961f\u95ee\u9898\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u6570\u503c\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5b9e\u73b0\u65e0\u78b0\u649e\u7f16\u961f\u63a7\u5236\u3002"}}
{"id": "2506.23152", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23152", "abs": "https://arxiv.org/abs/2506.23152", "authors": ["Youzhuo Wang", "Jiayi Ye", "Chuyang Xiao", "Yiming Zhong", "Heng Tao", "Hang Yu", "Yumeng Liu", "Jingyi Yu", "Yuexin Ma"], "title": "DexH2R: A Benchmark for Dynamic Dexterous Grasping in Human-to-Robot Handover", "comment": null, "summary": "Handover between a human and a dexterous robotic hand is a fundamental yet\nchallenging task in human-robot collaboration. It requires handling dynamic\nenvironments and a wide variety of objects and demands robust and adaptive\ngrasping strategies. However, progress in developing effective dynamic\ndexterous grasping methods is limited by the absence of high-quality,\nreal-world human-to-robot handover datasets. Existing datasets primarily focus\non grasping static objects or rely on synthesized handover motions, which\ndiffer significantly from real-world robot motion patterns, creating a\nsubstantial gap in applicability. In this paper, we introduce DexH2R, a\ncomprehensive real-world dataset for human-to-robot handovers, built on a\ndexterous robotic hand. Our dataset captures a diverse range of interactive\nobjects, dynamic motion patterns, rich visual sensor data, and detailed\nannotations. Additionally, to ensure natural and human-like dexterous motions,\nwe utilize teleoperation for data collection, enabling the robot's movements to\nalign with human behaviors and habits, which is a crucial characteristic for\nintelligent humanoid robots. Furthermore, we propose an effective solution,\nDynamicGrasp, for human-to-robot handover and evaluate various state-of-the-art\napproaches, including auto-regressive models and diffusion policy methods,\nproviding a thorough comparison and analysis. We believe our benchmark will\ndrive advancements in human-to-robot handover research by offering a\nhigh-quality dataset, effective solutions, and comprehensive evaluation\nmetrics.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86DexH2R\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u4eba\u673a\u4ea4\u63a5\u4efb\u52a1\u4e2d\u9ad8\u8d28\u91cf\u771f\u5b9e\u6570\u636e\u7f3a\u4e4f\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51faDynamicGrasp\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4eba\u673a\u4ea4\u63a5\u4efb\u52a1\u56e0\u7f3a\u4e4f\u771f\u5b9e\u6570\u636e\u96c6\u800c\u53d7\u9650\uff0c\u73b0\u6709\u6570\u636e\u591a\u4e3a\u9759\u6001\u6216\u5408\u6210\uff0c\u4e0e\u5b9e\u9645\u673a\u5668\u4eba\u8fd0\u52a8\u6a21\u5f0f\u5dee\u5f02\u5927\u3002", "method": "\u901a\u8fc7\u9065\u64cd\u4f5c\u6536\u96c6\u6570\u636e\uff0c\u6784\u5efaDexH2R\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51faDynamicGrasp\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b\u591a\u6837\u4ea4\u4e92\u5bf9\u8c61\u3001\u52a8\u6001\u8fd0\u52a8\u6a21\u5f0f\u548c\u4e30\u5bcc\u89c6\u89c9\u6570\u636e\uff0cDynamicGrasp\u5728\u4ea4\u63a5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DexH2R\u6570\u636e\u96c6\u548cDynamicGrasp\u65b9\u6848\u5c06\u63a8\u52a8\u4eba\u673a\u4ea4\u63a5\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.23164", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23164", "abs": "https://arxiv.org/abs/2506.23164", "authors": ["Maarten Hugenholtz", "Anna Meszaros", "Jens Kober", "Zlatan Ajanovic"], "title": "Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models", "comment": "12 pages, 8 figures, submitted to a journal", "summary": "Autonomous Vehicle decisions rely on multimodal prediction models that\naccount for multiple route options and the inherent uncertainty in human\nbehavior. However, models can suffer from mode collapse, where only the most\nlikely mode is predicted, posing significant safety risks. While existing\nmethods employ various strategies to generate diverse predictions, they often\noverlook the diversity in interaction modes among agents. Additionally,\ntraditional metrics for evaluating prediction models are dataset-dependent and\ndo not evaluate inter-agent interactions quantitatively. To our knowledge, none\nof the existing metrics explicitly evaluates mode collapse. In this paper, we\npropose a novel evaluation framework that assesses mode collapse in joint\ntrajectory predictions, focusing on safety-critical interactions. We introduce\nmetrics for mode collapse, mode correctness, and coverage, emphasizing the\nsequential dimension of predictions. By testing four multi-agent trajectory\nprediction models, we demonstrate that mode collapse indeed happens. When\nlooking at the sequential dimension, although prediction accuracy improves\ncloser to interaction events, there are still cases where the models are unable\nto predict the correct interaction mode, even just before the interaction mode\nbecomes inevitable. We hope that our framework can help researchers gain new\ninsights and advance the development of more consistent and accurate prediction\nmodels, thus enhancing the safety of autonomous driving systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u591a\u6a21\u6001\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u76f8\u5173\u6307\u6807\uff0c\u4ee5\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u9884\u6d4b\u4eba\u7c7b\u884c\u4e3a\u65f6\u53ef\u80fd\u56e0\u6a21\u5f0f\u5d29\u6e83\u800c\u4ec5\u9884\u6d4b\u6700\u53ef\u80fd\u6a21\u5f0f\uff0c\u5ffd\u89c6\u4ea4\u4e92\u591a\u6837\u6027\uff0c\u4e14\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u672a\u5b9a\u91cf\u8bc4\u4f30\u4ea4\u4e92\u6a21\u5f0f\u6216\u6a21\u5f0f\u5d29\u6e83\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u6a21\u5f0f\u5d29\u6e83\u3001\u6a21\u5f0f\u6b63\u786e\u6027\u548c\u8986\u76d6\u7387\u7684\u6307\u6807\uff0c\u7279\u522b\u5173\u6ce8\u9884\u6d4b\u7684\u65f6\u5e8f\u7ef4\u5ea6\u3002", "result": "\u6d4b\u8bd5\u4e86\u56db\u79cd\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\uff0c\u53d1\u73b0\u6a21\u5f0f\u5d29\u6e83\u786e\u5b9e\u5b58\u5728\uff0c\u4e14\u5373\u4f7f\u63a5\u8fd1\u4ea4\u4e92\u4e8b\u4ef6\u65f6\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u9ad8\uff0c\u4ecd\u65e0\u6cd5\u9884\u6d4b\u6b63\u786e\u7684\u4ea4\u4e92\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u7814\u7a76\u8005\u6df1\u5165\u7406\u89e3\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u63a8\u52a8\u5f00\u53d1\u66f4\u4e00\u81f4\u548c\u51c6\u786e\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6027\u3002"}}
{"id": "2506.23316", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23316", "abs": "https://arxiv.org/abs/2506.23316", "authors": ["Zhenghao Peng", "Yuxin Liu", "Bolei Zhou"], "title": "InfGen: Scenario Generation as Next Token Group Prediction", "comment": null, "summary": "Realistic and interactive traffic simulation is essential for training and\nevaluating autonomous driving systems. However, most existing data-driven\nsimulation methods rely on static initialization or log-replay data, limiting\ntheir ability to model dynamic, long-horizon scenarios with evolving agent\npopulations. We propose InfGen, a scenario generation framework that outputs\nagent states and trajectories in an autoregressive manner. InfGen represents\nthe entire scene as a sequence of tokens, including traffic light signals,\nagent states, and motion vectors, and uses a transformer model to simulate\ntraffic over time. This design enables InfGen to continuously insert new agents\ninto traffic, supporting infinite scene generation. Experiments demonstrate\nthat InfGen produces realistic, diverse, and adaptive traffic behaviors.\nFurthermore, reinforcement learning policies trained in InfGen-generated\nscenarios achieve superior robustness and generalization, validating its\nutility as a high-fidelity simulation environment for autonomous driving. More\ninformation is available at https://metadriverse.github.io/infgen/.", "AI": {"tldr": "InfGen\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u4ea4\u901a\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u652f\u6301\u52a8\u6001\u3001\u957f\u65f6\u7a0b\u7684\u4ea4\u901a\u6a21\u62df\uff0c\u5e76\u80fd\u6301\u7eed\u63d2\u5165\u65b0\u8f66\u8f86\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9a71\u52a8\u7684\u4ea4\u901a\u6a21\u62df\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u521d\u59cb\u5316\u6216\u65e5\u5fd7\u56de\u653e\u6570\u636e\uff0c\u96be\u4ee5\u6a21\u62df\u52a8\u6001\u3001\u957f\u65f6\u7a0b\u7684\u4ea4\u901a\u573a\u666f\u3002", "method": "InfGen\u5c06\u6574\u4e2a\u573a\u666f\u8868\u793a\u4e3a\u5305\u542b\u4ea4\u901a\u4fe1\u53f7\u3001\u8f66\u8f86\u72b6\u6001\u548c\u8fd0\u52a8\u5411\u91cf\u7684\u4ee4\u724c\u5e8f\u5217\uff0c\u901a\u8fc7Transformer\u6a21\u578b\u8fdb\u884c\u81ea\u56de\u5f52\u6a21\u62df\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cInfGen\u80fd\u751f\u6210\u771f\u5b9e\u3001\u591a\u6837\u4e14\u81ea\u9002\u5e94\u7684\u4ea4\u901a\u884c\u4e3a\uff0c\u4e14\u5728\u5176\u751f\u6210\u7684\u573a\u666f\u4e2d\u8bad\u7ec3\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "InfGen\u662f\u4e00\u4e2a\u9ad8\u4fdd\u771f\u5ea6\u7684\u81ea\u52a8\u9a7e\u9a76\u6a21\u62df\u73af\u5883\uff0c\u652f\u6301\u65e0\u9650\u573a\u666f\u751f\u6210\u3002"}}
{"id": "2506.23326", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23326", "abs": "https://arxiv.org/abs/2506.23326", "authors": ["Sang-Yoep Lee", "Leonardo Zamora Yanez", "Jacob Rogatinsky", "Vi T. Vo", "Tanvi Shingade", "Tommaso Ranzani"], "title": "Simplifying Data-Driven Modeling of the Volume-Flow-Pressure Relationship in Hydraulic Soft Robotic Actuators", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Soft robotic systems are known for their flexibility and adaptability, but\ntraditional physics-based models struggle to capture their complex, nonlinear\nbehaviors. This study explores a data-driven approach to modeling the\nvolume-flow-pressure relationship in hydraulic soft actuators, focusing on\nlow-complexity models with high accuracy. We perform regression analysis on a\nstacked balloon actuator system using exponential, polynomial, and neural\nnetwork models with or without autoregressive inputs. The results demonstrate\nthat simpler models, particularly multivariate polynomials, effectively predict\npressure dynamics with fewer parameters. This research offers a practical\nsolution for real-time soft robotics applications, balancing model complexity\nand computational efficiency. Moreover, the approach may benefit various\ntechniques that require explicit analytical models.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u6db2\u538b\u8f6f\u6267\u884c\u5668\u7684\u4f53\u79ef-\u6d41\u91cf-\u538b\u529b\u5173\u7cfb\uff0c\u91cd\u70b9\u5728\u4e8e\u4f4e\u590d\u6742\u5ea6\u9ad8\u7cbe\u5ea6\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u7269\u7406\u6a21\u578b\u96be\u4ee5\u6355\u6349\u8f6f\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u590d\u6742\u975e\u7ebf\u6027\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u56de\u5f52\u5206\u6790\uff0c\u6bd4\u8f83\u4e86\u6307\u6570\u3001\u591a\u9879\u5f0f\u548c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff08\u542b\u6216\u4e0d\u542b\u81ea\u56de\u5f52\u8f93\u5165\uff09\u5728\u5806\u53e0\u6c14\u7403\u6267\u884c\u5668\u7cfb\u7edf\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8f83\u7b80\u5355\u7684\u6a21\u578b\uff08\u5c24\u5176\u662f\u591a\u5143\u591a\u9879\u5f0f\uff09\u80fd\u4ee5\u8f83\u5c11\u53c2\u6570\u6709\u6548\u9884\u6d4b\u538b\u529b\u52a8\u6001\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5b9e\u65f6\u8f6f\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u6a21\u578b\u590d\u6742\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u53ef\u80fd\u9002\u7528\u4e8e\u5176\u4ed6\u9700\u8981\u663e\u5f0f\u89e3\u6790\u6a21\u578b\u7684\u6280\u672f\u3002"}}
{"id": "2506.23333", "categories": ["cs.RO", "cs.CG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2506.23333", "abs": "https://arxiv.org/abs/2506.23333", "authors": ["Javier Garcia", "Jonas Friemel", "Ramin Kosfeld", "Michael Yannuzzi", "Peter Kramer", "Christian Rieck", "Christian Scheffer", "Arne Schmidt", "Harm Kube", "Dan Biediger", "S\u00e1ndor P. Fekete", "Aaron T. Becker"], "title": "Moving Matter: Using a Single, Simple Robot to Reconfigure a Connected Set of Building Blocks", "comment": "8 pages, 12 figures. To appear in the proceedings of the 2025 IEEE\n  21st International Conference on Automation Science and Engineering (CASE\n  2025)", "summary": "We implement and evaluate different methods for the reconfiguration of a\nconnected arrangement of tiles into a desired target shape, using a single\nactive robot that can move along the tile structure. This robot can pick up,\ncarry, or drop off one tile at a time, but it must maintain a single connected\nconfiguration at all times.\n  Becker et al. (CCCG 2025) recently proposed an algorithm that uses histograms\nas canonical intermediate configurations, guaranteeing performance within a\nconstant factor of the optimal solution if the start and target configuration\nare well-separated. We implement and evaluate this algorithm, both in a\nsimulated and practical setting, using an inchworm type robot to compare it\nwith two existing heuristic algorithms.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u5355\u4e2a\u673a\u5668\u4eba\u91cd\u65b0\u914d\u7f6e\u8fde\u63a5\u74f7\u7816\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86\u57fa\u4e8e\u76f4\u65b9\u56fe\u7684\u7b97\u6cd5\uff0c\u5e76\u4e0e\u73b0\u6709\u542f\u53d1\u5f0f\u7b97\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u9ad8\u6548\u5730\u5c06\u8fde\u63a5\u7684\u74f7\u7816\u7ed3\u6784\u91cd\u65b0\u914d\u7f6e\u4e3a\u76ee\u6807\u5f62\u72b6\uff0c\u540c\u65f6\u4fdd\u6301\u7ed3\u6784\u7684\u8fde\u901a\u6027\u3002", "method": "\u5b9e\u73b0\u5e76\u8bc4\u4f30\u4e86Becker\u7b49\u4eba\u63d0\u51fa\u7684\u57fa\u4e8e\u76f4\u65b9\u56fe\u7684\u7b97\u6cd5\uff0c\u4f7f\u7528\u6a21\u62df\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728\u6a21\u62df\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u4e0e\u4e24\u79cd\u542f\u53d1\u5f0f\u7b97\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "\u57fa\u4e8e\u76f4\u65b9\u56fe\u7684\u7b97\u6cd5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff08\u8d77\u59cb\u548c\u76ee\u6807\u914d\u7f6e\u5206\u79bb\u826f\u597d\uff09\u80fd\u4fdd\u8bc1\u63a5\u8fd1\u6700\u4f18\u89e3\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23346", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.23346", "abs": "https://arxiv.org/abs/2506.23346", "authors": ["Hao Wang", "Armand Jordana", "Ludovic Righetti", "Somil Bansal"], "title": "Safe and Performant Deployment of Autonomous Systems via Model Predictive Control and Hamilton-Jacobi Reachability Analysis", "comment": "RSS 2025 Workshop on Reliable Robotics", "summary": "While we have made significant algorithmic developments to enable autonomous\nsystems to perform sophisticated tasks, it remains difficult for them to\nperform tasks effective and safely. Most existing approaches either fail to\nprovide any safety assurances or substantially compromise task performance for\nsafety. In this work, we develop a framework, based on model predictive control\n(MPC) and Hamilton-Jacobi (HJ) reachability, to optimize task performance for\nautonomous systems while respecting the safety constraints. Our framework\nguarantees recursive feasibility for the MPC controller, and it is scalable to\nhigh-dimensional systems. We demonstrate the effectiveness of our framework\nwith two simulation studies using a 4D Dubins Car and a 6 Dof Kuka iiwa\nmanipulator, and the experiments show that our framework significantly improves\nthe safety constraints satisfaction of the systems over the baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMPC\u548cHJ\u53ef\u8fbe\u6027\u7684\u6846\u67b6\uff0c\u4f18\u5316\u81ea\u4e3b\u7cfb\u7edf\u7684\u4efb\u52a1\u6027\u80fd\u5e76\u786e\u4fdd\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u540c\u65f6\u9ad8\u6548\u5b8c\u6210\u4efb\u52a1\uff0c\u9700\u8981\u4e00\u79cd\u517c\u987e\u6027\u80fd\u4e0e\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u548cHamilton-Jacobi\uff08HJ\uff09\u53ef\u8fbe\u6027\u7406\u8bba\uff0c\u786e\u4fdd\u9012\u5f52\u53ef\u884c\u6027\u5e76\u9002\u7528\u4e8e\u9ad8\u7ef4\u7cfb\u7edf\u3002", "result": "\u57284D Dubins Car\u548c6 Dof Kuka iiwa\u673a\u68b0\u81c2\u7684\u4eff\u771f\u5b9e\u9a8c\u4e2d\uff0c\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u8bc1\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u4e3b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u9ad8\u7ef4\u7cfb\u7edf\u3002"}}
{"id": "2506.23351", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.23351", "abs": "https://arxiv.org/abs/2506.23351", "authors": ["Tianxing Chen", "Kaixuan Wang", "Zhaohui Yang", "Yuhao Zhang", "Zanxin Chen", "Baijun Chen", "Wanxi Dong", "Ziyuan Liu", "Dong Chen", "Tianshuo Yang", "Haibao Yu", "Xiaokang Yang", "Yusen Qin", "Zhiqiang Xie", "Yao Mu", "Ping Luo", "Tian Nian", "Weiliang Deng", "Yiheng Ge", "Yibin Liu", "Zixuan Li", "Dehui Wang", "Zhixuan Liang", "Haohui Xie", "Rijie Zeng", "Yunfei Ge", "Peiqing Cong", "Guannan He", "Zhaoming Han", "Ruocheng Yin", "Jingxiang Guo", "Lunkai Lin", "Tianling Xu", "Hongzhe Bi", "Xuewu Lin", "Tianwei Lin", "Shujie Luo", "Keyu Li", "Ziyan Zhao", "Ke Fan", "Heyang Xu", "Bo Peng", "Wenlong Gao", "Dongjiang Li", "Feng Jin", "Hui Shen", "Jinming Li", "Chaowei Cui", "Yuchen", "Yaxin Peng", "Lingdong Zeng", "Wenlong Dong", "Tengfei Li", "Weijie Ke", "Jun Chen", "Erdemt Bao", "Tian Lan", "Tenglong Liu", "Jin Yang", "Huiping Zhuang", "Baozhi Jia", "Shuai Zhang", "Zhengfeng Zou", "Fangheng Guan", "Tianyi Jia", "Ke Zhou", "Hongjiu Zhang", "Yating Han", "Cheng Fang", "Yixian Zou", "Chongyang Xu", "Qinglun Zhang", "Shen Cheng", "Xiaohe Wang", "Ping Tan", "Haoqiang Fan", "Shuaicheng Liu", "Jiaheng Chen", "Chuxuan Huang", "Chengliang Lin", "Kaijun Luo", "Boyu Yue", "Yi Liu", "Jinyu Chen", "Zichang Tan", "Liming Deng", "Shuo Xu", "Zijian Cai", "Shilong Yin", "Hao Wang", "Hongshan Liu", "Tianyang Li", "Long Shi", "Ran Xu", "Huilin Xu", "Zhengquan Zhang", "Congsheng Xu", "Jinchang Yang", "Feng Xu"], "title": "Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop", "comment": "Challenge Webpage:\n  https://robotwin-benchmark.github.io/cvpr-2025-challenge/", "summary": "Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in\nrobotics, driven by the need for autonomous systems that can perceive, reason,\nand act in complex physical environments. While single-arm systems have shown\nstrong task performance, collaborative dual-arm systems are essential for\nhandling more intricate tasks involving rigid, deformable, and\ntactile-sensitive objects. To advance this goal, we launched the RoboTwin\nDual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on\nthe RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot\nplatform, the competition consisted of three stages: Simulation Round 1,\nSimulation Round 2, and a final Real-World Round. Participants totally tackled\n17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based\nscenarios. The challenge attracted 64 global teams and over 400 participants,\nproducing top-performing solutions like SEM and AnchorDP3 and generating\nvaluable insights into generalizable bimanual policy learning. This report\noutlines the competition setup, task design, evaluation methodology, key\nfindings and future direction, aiming to support future research on robust and\ngeneralizable bimanual manipulation policies. The Challenge Webpage is\navailable at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86RoboTwin\u53cc\u81c2\u534f\u4f5c\u6311\u6218\u8d5b\uff0c\u65e8\u5728\u63a8\u52a8\u53cc\u81c2\u673a\u5668\u4eba\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u7814\u7a76\uff0c\u5438\u5f15\u4e86\u5168\u740364\u652f\u56e2\u961f\u53c2\u4e0e\uff0c\u5e76\u63d0\u51fa\u4e86\u901a\u7528\u53cc\u81c2\u7b56\u7565\u5b66\u4e60\u7684\u65b0\u89c1\u89e3\u3002", "motivation": "\u63a8\u52a8\u81ea\u4e3b\u7cfb\u7edf\u5728\u590d\u6742\u7269\u7406\u73af\u5883\u4e2d\u7684\u611f\u77e5\u3001\u63a8\u7406\u548c\u884c\u52a8\u80fd\u529b\uff0c\u7279\u522b\u662f\u53cc\u81c2\u534f\u4f5c\u7cfb\u7edf\u5728\u521a\u6027\u3001\u53ef\u53d8\u5f62\u548c\u89e6\u89c9\u654f\u611f\u7269\u4f53\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u57fa\u4e8eRoboTwin\u4eff\u771f\u5e73\u53f0\u548cAgileX COBOT-Magic\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u9636\u6bb5\u7684\u6bd4\u8d5b\uff0c\u5305\u62ec\u4eff\u771f\u548c\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u3002", "result": "\u5438\u5f15\u4e8664\u652f\u5168\u7403\u56e2\u961f\uff0c\u4ea7\u751f\u4e86\u5982SEM\u548cAnchorDP3\u7b49\u4f18\u79c0\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u901a\u7528\u53cc\u81c2\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002", "conclusion": "\u6311\u6218\u8d5b\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6846\u67b6\u548c\u65b9\u5411\uff0c\u652f\u6301\u5f00\u53d1\u9c81\u68d2\u4e14\u901a\u7528\u7684\u53cc\u81c2\u64cd\u4f5c\u7b56\u7565\u3002"}}
{"id": "2506.23369", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23369", "abs": "https://arxiv.org/abs/2506.23369", "authors": ["Xiao'ao Song", "Konstantinos Karydis"], "title": "GS-NBV: a Geometry-based, Semantics-aware Viewpoint Planning Algorithm for Avocado Harvesting under Occlusions", "comment": "Accepted for publication in CASE 2025, 6 pages, 8 figures", "summary": "Efficient identification of picking points is critical for automated fruit\nharvesting. Avocados present unique challenges owing to their irregular shape,\nweight, and less-structured growing environments, which require specific\nviewpoints for successful harvesting. We propose a geometry-based,\nsemantics-aware viewpoint-planning algorithm to address these challenges. The\nplanning process involves three key steps: viewpoint sampling, evaluation, and\nexecution. Starting from a partially occluded view, the system first detects\nthe fruit, then leverages geometric information to constrain the viewpoint\nsearch space to a 1D circle, and uniformly samples four points to balance the\nefficiency and exploration. A new picking score metric is introduced to\nevaluate the viewpoint suitability and guide the camera to the next-best view.\nWe validate our method through simulation against two state-of-the-art\nalgorithms. Results show a 100% success rate in two case studies with\nsignificant occlusions, demonstrating the efficiency and robustness of our\napproach. Our code is available at https://github.com/lineojcd/GSNBV", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u548c\u8bed\u4e49\u7684\u89c6\u70b9\u89c4\u5212\u7b97\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u91c7\u6458\u4e0d\u89c4\u5219\u5f62\u72b6\u7684\u725b\u6cb9\u679c\uff0c\u901a\u8fc7\u91c7\u6837\u3001\u8bc4\u4f30\u548c\u6267\u884c\u4e09\u6b65\u5b9e\u73b0\u9ad8\u6548\u91c7\u6458\u3002", "motivation": "\u725b\u6cb9\u679c\u7684\u4e0d\u89c4\u5219\u5f62\u72b6\u548c\u751f\u957f\u73af\u5883\u589e\u52a0\u4e86\u81ea\u52a8\u5316\u91c7\u6458\u7684\u96be\u5ea6\uff0c\u9700\u8981\u7279\u5b9a\u89c6\u70b9\u624d\u80fd\u6210\u529f\u91c7\u6458\u3002", "method": "\u7b97\u6cd5\u5305\u62ec\u89c6\u70b9\u91c7\u6837\u3001\u8bc4\u4f30\u548c\u6267\u884c\u4e09\u6b65\uff0c\u5229\u7528\u51e0\u4f55\u4fe1\u606f\u7ea6\u675f\u641c\u7d22\u7a7a\u95f4\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u91c7\u6458\u8bc4\u5206\u6307\u6807\u3002", "result": "\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0c\u7b97\u6cd5\u5728\u906e\u6321\u4e25\u91cd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86100%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u9c81\u68d2\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u5316\u725b\u6cb9\u679c\u91c7\u6458\u3002"}}
{"id": "2506.23400", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.23400", "abs": "https://arxiv.org/abs/2506.23400", "authors": ["Yifei Li", "Joshua A. Robbins", "Guha Manogharan", "Herschel C. Pangborn", "Ilya Kovalenko"], "title": "A Model Predictive Control Framework to Enhance Safety and Quality in Mobile Additive Manufacturing Systems", "comment": "2025 IEEE 21st International Conference on Automation Science and\n  Engineering", "summary": "In recent years, the demand for customized, on-demand production has grown in\nthe manufacturing sector. Additive Manufacturing (AM) has emerged as a\npromising technology to enhance customization capabilities, enabling greater\nflexibility, reduced lead times, and more efficient material usage. However,\ntraditional AM systems remain constrained by static setups and human worker\ndependencies, resulting in long lead times and limited scalability. Mobile\nrobots can improve the flexibility of production systems by transporting\nproducts to designated locations in a dynamic environment. By integrating AM\nsystems with mobile robots, manufacturers can optimize travel time for\npreparatory tasks and distributed printing operations. Mobile AM robots have\nbeen deployed for on-site production of large-scale structures, but often\nneglect critical print quality metrics like surface roughness. Additionally,\nthese systems do not have the precision necessary for producing small,\nintricate components. We propose a model predictive control framework for a\nmobile AM platform that ensures safe navigation on the plant floor while\nmaintaining high print quality in a dynamic environment. Three case studies are\nused to test the feasibility and reliability of the proposed systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u79fb\u52a8\u589e\u6750\u5236\u9020\u5e73\u53f0\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u589e\u6750\u5236\u9020\u7cfb\u7edf\u7684\u9759\u6001\u8bbe\u7f6e\u548c\u4eba\u5de5\u4f9d\u8d56\u95ee\u9898\uff0c\u540c\u65f6\u786e\u4fdd\u9ad8\u6253\u5370\u8d28\u91cf\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u5236\u9020\u4e1a\u5bf9\u5b9a\u5236\u5316\u3001\u6309\u9700\u751f\u4ea7\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u4f20\u7edf\u589e\u6750\u5236\u9020\u7cfb\u7edf\u53d7\u9650\u4e8e\u9759\u6001\u8bbe\u7f6e\u548c\u4eba\u5de5\u4f9d\u8d56\uff0c\u5bfc\u81f4\u751f\u4ea7\u5468\u671f\u957f\u4e14\u6269\u5c55\u6027\u6709\u9650\u3002\u79fb\u52a8\u673a\u5668\u4eba\u53ef\u4ee5\u63d0\u5347\u751f\u4ea7\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u3002", "method": "\u901a\u8fc7\u5c06\u589e\u6750\u5236\u9020\u7cfb\u7edf\u4e0e\u79fb\u52a8\u673a\u5668\u4eba\u96c6\u6210\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u786e\u4fdd\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b89\u5168\u5bfc\u822a\u5e76\u7ef4\u6301\u9ad8\u6253\u5370\u8d28\u91cf\u3002", "result": "\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u6240\u63d0\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u79fb\u52a8\u589e\u6750\u5236\u9020\u5e73\u53f0\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u540c\u65f6\u4f18\u5316\u751f\u4ea7\u7075\u6d3b\u6027\u548c\u6253\u5370\u8d28\u91cf\u3002"}}
{"id": "2506.23433", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23433", "abs": "https://arxiv.org/abs/2506.23433", "authors": ["Tim Puphal", "Vipul Ramtekkar", "Kenji Nishimiya"], "title": "Risk-Based Filtering of Valuable Driving Situations in the Waymo Open Motion Dataset", "comment": null, "summary": "Improving automated vehicle software requires driving data rich in valuable\nroad user interactions. In this paper, we propose a risk-based filtering\napproach that helps identify such valuable driving situations from large\ndatasets. Specifically, we use a probabilistic risk model to detect high-risk\nsituations. Our method stands out by considering a) first-order situations\n(where one vehicle directly influences another and induces risk) and b)\nsecond-order situations (where influence propagates through an intermediary\nvehicle). In experiments, we show that our approach effectively selects\nvaluable driving situations in the Waymo Open Motion Dataset. Compared to the\ntwo baseline interaction metrics of Kalman difficulty and Tracks-To-Predict\n(TTP), our filtering approach identifies complex and complementary situations,\nenriching the quality in automated vehicle testing. The risk data is made\nopen-source: https://github.com/HRI-EU/RiskBasedFiltering.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u98ce\u9669\u7684\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5927\u578b\u6570\u636e\u96c6\u4e2d\u8bc6\u522b\u6709\u4ef7\u503c\u7684\u9a7e\u9a76\u573a\u666f\uff0c\u91cd\u70b9\u5173\u6ce8\u9ad8\u98ce\u9669\u7684\u4e00\u9636\u548c\u4e8c\u9636\u4ea4\u4e92\u3002", "motivation": "\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u8f6f\u4ef6\u9700\u8981\u5305\u542b\u4e30\u5bcc\u9053\u8def\u7528\u6237\u4ea4\u4e92\u7684\u9a7e\u9a76\u6570\u636e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u7b5b\u9009\u51fa\u8fd9\u4e9b\u6709\u4ef7\u503c\u7684\u60c5\u5883\u3002", "method": "\u4f7f\u7528\u6982\u7387\u98ce\u9669\u6a21\u578b\u68c0\u6d4b\u9ad8\u98ce\u9669\u9a7e\u9a76\u60c5\u5883\uff0c\u7279\u522b\u5173\u6ce8\u4e00\u9636\u548c\u4e8c\u9636\u4ea4\u4e92\u3002", "result": "\u5728Waymo Open Motion Dataset\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6307\u6807\uff08Kalman\u96be\u5ea6\u548cTTP\uff09\uff0c\u80fd\u8bc6\u522b\u66f4\u590d\u6742\u548c\u4e92\u8865\u7684\u60c5\u5883\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u6570\u636e\u7684\u8d28\u91cf\uff0c\u76f8\u5173\u98ce\u9669\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.23514", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.23514", "abs": "https://arxiv.org/abs/2506.23514", "authors": ["Sai Krishna Ghanta", "Ramviyas Parasuraman"], "title": "MGPRL: Distributed Multi-Gaussian Processes for Wi-Fi-based Multi-Robot Relative Localization in Large Indoor Environments", "comment": "Accepted to IROS 2025", "summary": "Relative localization is a crucial capability for multi-robot systems\noperating in GPS-denied environments. Existing approaches for multi-robot\nrelative localization often depend on costly or short-range sensors like\ncameras and LiDARs. Consequently, these approaches face challenges such as high\ncomputational overhead (e.g., map merging) and difficulties in disjoint\nenvironments. To address this limitation, this paper introduces MGPRL, a novel\ndistributed framework for multi-robot relative localization using convex-hull\nof multiple Wi-Fi access points (AP). To accomplish this, we employ\nco-regionalized multi-output Gaussian Processes for efficient Radio Signal\nStrength Indicator (RSSI) field prediction and perform uncertainty-aware\nmulti-AP localization, which is further coupled with weighted convex hull-based\nalignment for robust relative pose estimation. Each robot predicts the RSSI\nfield of the environment by an online scan of APs in its environment, which are\nutilized for position estimation of multiple APs. To perform relative\nlocalization, each robot aligns the convex hull of its predicted AP locations\nwith that of the neighbor robots. This approach is well-suited for devices with\nlimited computational resources and operates solely on widely available Wi-Fi\nRSSI measurements without necessitating any dedicated pre-calibration or\noffline fingerprinting. We rigorously evaluate the performance of the proposed\nMGPRL in ROS simulations and demonstrate it with real-world experiments,\ncomparing it against multiple state-of-the-art approaches. The results showcase\nthat MGPRL outperforms existing methods in terms of localization accuracy and\ncomputational efficiency. Finally, we open source MGPRL as a ROS package\nhttps://github.com/herolab-uga/MGPRL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWi-Fi\u4fe1\u53f7\u7684\u591a\u673a\u5668\u4eba\u76f8\u5bf9\u5b9a\u4f4d\u6846\u67b6MGPRL\uff0c\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\u548c\u51f8\u5305\u5bf9\u9f50\u5b9e\u73b0\u9ad8\u6548\u5b9a\u4f4d\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u6216\u77ed\u7a0b\u4f20\u611f\u5668\uff08\u5982\u6444\u50cf\u5934\u548cLiDAR\uff09\u4ee5\u53ca\u9ad8\u8ba1\u7b97\u5f00\u9500\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u591a\u8f93\u51fa\u9ad8\u65af\u8fc7\u7a0b\u9884\u6d4bWi-Fi\u4fe1\u53f7\u5f3a\u5ea6\uff0c\u7ed3\u5408\u51f8\u5305\u5bf9\u9f50\u8fdb\u884c\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u3002", "result": "MGPRL\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MGPRL\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u7684\u5206\u5e03\u5f0f\u5b9a\u4f4d\u65b9\u6848\uff0c\u65e0\u9700\u9884\u6821\u51c6\u6216\u79bb\u7ebf\u6307\u7eb9\u3002"}}
{"id": "2506.23573", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23573", "abs": "https://arxiv.org/abs/2506.23573", "authors": ["Siddhartha Mondal", "Avik Mitra", "Chayan Sarkar"], "title": "Online Human Action Detection during Escorting", "comment": "Accepted in IEEE RO-MAN '25", "summary": "The deployment of robot assistants in large indoor spaces has seen\nsignificant growth, with escorting tasks becoming a key application. However,\nmost current escorting robots primarily rely on navigation-focused strategies,\nassuming that the person being escorted will follow without issue. In crowded\nenvironments, this assumption often falls short, as individuals may struggle to\nkeep pace, become obstructed, get distracted, or need to stop unexpectedly. As\na result, conventional robotic systems are often unable to provide effective\nescorting services due to their limited understanding of human movement\ndynamics. To address these challenges, an effective escorting robot must\ncontinuously detect and interpret human actions during the escorting process\nand adjust its movement accordingly. However, there is currently no existing\ndataset designed specifically for human action detection in the context of\nescorting. Given that escorting often occurs in crowded environments, where\nother individuals may enter the robot's camera view, the robot also needs to\nidentify the specific human it is escorting (the subject) before predicting\ntheir actions. Since no existing model performs both person re-identification\nand action prediction in real-time, we propose a novel neural network\narchitecture that can accomplish both tasks. This enables the robot to adjust\nits speed dynamically based on the escortee's movements and seamlessly resume\nescorting after any disruption. In comparative evaluations against strong\nbaselines, our system demonstrates superior efficiency and effectiveness,\nshowcasing its potential to significantly improve robotic escorting services in\ncomplex, real-world scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u5b9e\u65f6\u884c\u4eba\u91cd\u8bc6\u522b\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u5728\u62e5\u6324\u73af\u5883\u4e2d\u7684\u62a4\u9001\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u62a4\u9001\u673a\u5668\u4eba\u4e3b\u8981\u4f9d\u8d56\u5bfc\u822a\u7b56\u7565\uff0c\u5047\u8bbe\u88ab\u62a4\u9001\u8005\u4f1a\u987a\u5229\u8ddf\u968f\uff0c\u4f46\u5728\u62e5\u6324\u73af\u5883\u4e2d\u8fd9\u4e00\u5047\u8bbe\u5e38\u4e0d\u6210\u7acb\uff0c\u5bfc\u81f4\u670d\u52a1\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u540c\u65f6\u5b8c\u6210\u884c\u4eba\u91cd\u8bc6\u522b\u548c\u52a8\u4f5c\u9884\u6d4b\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u52a8\u6001\u8c03\u6574\u901f\u5ea6\u5e76\u5e94\u5bf9\u4e2d\u65ad\u3002", "result": "\u5728\u5bf9\u6bd4\u8bc4\u4f30\u4e2d\uff0c\u8be5\u7cfb\u7edf\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6548\u7387\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6709\u671b\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u62a4\u9001\u670d\u52a1\u80fd\u529b\u3002"}}
{"id": "2506.23614", "categories": ["cs.RO", "cs.CG"], "pdf": "https://arxiv.org/pdf/2506.23614", "abs": "https://arxiv.org/abs/2506.23614", "authors": ["Jing Huang", "Hao Su", "Kwok Wai Samuel Au"], "title": "Passage-traversing optimal path planning with sampling-based algorithms", "comment": "30 pages, 22 figures, 6 tables, journal paper", "summary": "This paper introduces a new paradigm of optimal path planning, i.e.,\npassage-traversing optimal path planning (PTOPP), that optimizes paths'\ntraversed passages for specified optimization objectives. In particular, PTOPP\nis utilized to find the path with optimal accessible free space along its\nentire length, which represents a basic requirement for paths in robotics. As\npassages are places where free space shrinks and becomes constrained, the core\nidea is to leverage the path's passage traversal status to characterize its\naccessible free space comprehensively. To this end, a novel passage detection\nand free space decomposition method using proximity graphs is proposed,\nenabling fast detection of sparse but informative passages and environment\ndecompositions. Based on this preprocessing, optimal path planning with\naccessible free space objectives or constraints is formulated as PTOPP problems\ncompatible with sampling-based optimal planners. Then, sampling-based\nalgorithms for PTOPP, including their dependent primitive procedures, are\ndeveloped leveraging partitioned environments for fast passage traversal check.\nAll these methods are implemented and thoroughly tested for effectiveness and\nefficiency validation. Compared to existing approaches, such as clearance-based\nmethods, PTOPP demonstrates significant advantages in configurability, solution\noptimality, and efficiency, addressing prior limitations and incapabilities. It\nis believed to provide an efficient and versatile solution to accessible free\nspace optimization over conventional avenues and more generally, to a broad\nclass of path planning problems that can be formulated as PTOPP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6700\u4f18\u8def\u5f84\u89c4\u5212\u8303\u5f0fPTOPP\uff0c\u901a\u8fc7\u4f18\u5316\u8def\u5f84\u7a7f\u8d8a\u7684\u901a\u9053\u6765\u5b9e\u73b0\u7279\u5b9a\u76ee\u6807\uff0c\u7279\u522b\u9002\u7528\u4e8e\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4e2d\u81ea\u7531\u7a7a\u95f4\u4f18\u5316\u7684\u9700\u6c42\u3002", "motivation": "\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4e2d\uff0c\u81ea\u7531\u7a7a\u95f4\u7684\u4f18\u5316\u662f\u4e00\u4e2a\u57fa\u672c\u9700\u6c42\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u5728\u901a\u9053\uff08\u81ea\u7531\u7a7a\u95f4\u6536\u7f29\u5904\uff09\u7684\u5904\u7406\u4e0a\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u90bb\u8fd1\u56fe\u7684\u901a\u9053\u68c0\u6d4b\u548c\u81ea\u7531\u7a7a\u95f4\u5206\u89e3\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u91c7\u6837\u7b97\u6cd5\u89e3\u51b3PTOPP\u95ee\u9898\u3002", "result": "PTOPP\u5728\u53ef\u914d\u7f6e\u6027\u3001\u89e3\u7684\u6700\u4f18\u6027\u548c\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u95f4\u9699\u7684\u65b9\u6cd5\uff09\u3002", "conclusion": "PTOPP\u4e3a\u81ea\u7531\u7a7a\u95f4\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\u3002"}}
{"id": "2506.23624", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.23624", "abs": "https://arxiv.org/abs/2506.23624", "authors": ["Max Grobbel", "Tristan Schneider", "S\u00f6ren Hohmann"], "title": "Towards Universal Shared Control in Teleoperation Without Haptic Feedback", "comment": "5 pages, submitted to IEEE Telepresence 2025 conference", "summary": "Teleoperation with non-haptic VR controllers deprives human operators of\ncritical motion feedback. We address this by embedding a multi-objective\noptimization problem that converts user input into collision-free UR5e joint\ntrajectories while actively suppressing liquid slosh in a glass. The controller\nmaintains 13 ms average planning latency, confirming real-time performance and\nmotivating the augmentation of this teleoperation approach to further\nobjectives.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u5c06\u7528\u6237\u8f93\u5165\u8f6c\u6362\u4e3a\u65e0\u78b0\u649eUR5e\u5173\u8282\u8f68\u8ff9\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u6291\u5236\u73bb\u7483\u4e2d\u6db2\u4f53\u6643\u52a8\uff0c\u5b9e\u73b0\u4e8613\u6beb\u79d2\u7684\u5e73\u5747\u89c4\u5212\u5ef6\u8fdf\u3002", "motivation": "\u975e\u89e6\u89c9VR\u63a7\u5236\u5668\u5265\u593a\u4e86\u64cd\u4f5c\u5458\u7684\u8fd0\u52a8\u53cd\u9988\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u63d0\u4f9b\u5b9e\u65f6\u53cd\u9988\u5e76\u6291\u5236\u6db2\u4f53\u6643\u52a8\u3002", "method": "\u5d4c\u5165\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5c06\u7528\u6237\u8f93\u5165\u8f6c\u6362\u4e3a\u65e0\u78b0\u649eUR5e\u5173\u8282\u8f68\u8ff9\uff0c\u5e76\u4e3b\u52a8\u6291\u5236\u6db2\u4f53\u6643\u52a8\u3002", "result": "\u63a7\u5236\u5668\u5b9e\u73b0\u4e8613\u6beb\u79d2\u7684\u5e73\u5747\u89c4\u5212\u5ef6\u8fdf\uff0c\u9a8c\u8bc1\u4e86\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u7528\u4e8e\u8fdb\u4e00\u6b65\u6269\u5c55\u5176\u4ed6\u76ee\u6807\u7684\u8fdc\u7a0b\u64cd\u4f5c\u3002"}}
{"id": "2506.23723", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23723", "abs": "https://arxiv.org/abs/2506.23723", "authors": ["Jozsef Palmieri", "Paolo Di Lillo", "Stefano Chiaverini", "Alessandro Marino"], "title": "A comprehensive control architecture for semi-autonomous dual-arm robots in agriculture settings", "comment": null, "summary": "The adoption of mobile robotic platforms in complex environments, such as\nagricultural settings, requires these systems to exhibit a flexible yet\neffective architecture that integrates perception and control. In such\nscenarios, several tasks need to be accomplished simultaneously, ranging from\nmanaging robot limits to performing operational tasks and handling human\ninputs. The purpose of this paper is to present a comprehensive control\narchitecture for achieving complex tasks such as robotized harvesting in\nvineyards within the framework of the European project CANOPIES. In detail, a\n16-DOF dual-arm mobile robot is employed, controlled via a Hierarchical\nQuadratic Programming (HQP) approach capable of handling both equality and\ninequality constraints at various priorities to harvest grape bunches selected\nby the perception system developed within the project. Furthermore, given the\ncomplexity of the scenario and the uncertainty in the perception system, which\ncould potentially lead to collisions with the environment, the handling of\ninteraction forces is necessary. Remarkably, this was achieved using the same\nHQP framework. This feature is further leveraged to enable semi-autonomous\noperations, allowing a human operator to assist the robotic counterpart in\ncompleting harvesting tasks. Finally, the obtained results are validated\nthrough extensive testing conducted first in a laboratory environment to prove\nindividual functionalities, then in a real vineyard, encompassing both\nautonomous and semi-autonomous grape harvesting operations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u590d\u6742\u519c\u4e1a\u73af\u5883\uff08\u5982\u8461\u8404\u56ed\uff09\u7684\u79fb\u52a8\u673a\u5668\u4eba\u63a7\u5236\u67b6\u6784\uff0c\u91c7\u7528\u5206\u5c42\u4e8c\u6b21\u89c4\u5212\uff08HQP\uff09\u65b9\u6cd5\uff0c\u540c\u65f6\u652f\u6301\u81ea\u4e3b\u548c\u534a\u81ea\u4e3b\u64cd\u4f5c\u3002", "motivation": "\u5728\u590d\u6742\u519c\u4e1a\u73af\u5883\u4e2d\uff0c\u79fb\u52a8\u673a\u5668\u4eba\u9700\u8981\u7075\u6d3b\u4e14\u9ad8\u6548\u5730\u6574\u5408\u611f\u77e5\u4e0e\u63a7\u5236\uff0c\u4ee5\u5b8c\u6210\u591a\u9879\u4efb\u52a1\uff08\u5982\u8461\u8404\u91c7\u6458\uff09\uff0c\u540c\u65f6\u5904\u7406\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u548c\u4eba\u673a\u4ea4\u4e92\u3002", "method": "\u4f7f\u752816\u81ea\u7531\u5ea6\u53cc\u81c2\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u901a\u8fc7HQP\u65b9\u6cd5\u5904\u7406\u4f18\u5148\u7ea7\u4e0d\u540c\u7684\u7b49\u5f0f\u548c\u4e0d\u7b49\u5f0f\u7ea6\u675f\uff0c\u5e76\u7ed3\u5408\u611f\u77e5\u7cfb\u7edf\u9009\u62e9\u8461\u8404\u4e32\u3002", "result": "\u5728\u5b9e\u9a8c\u5ba4\u548c\u771f\u5b9e\u8461\u8404\u56ed\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u529f\u80fd\uff0c\u5305\u62ec\u81ea\u4e3b\u548c\u534a\u81ea\u4e3b\u91c7\u6458\uff0c\u6210\u529f\u5904\u7406\u4e86\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u73af\u5883\u4ea4\u4e92\u3002", "conclusion": "\u63d0\u51fa\u7684HQP\u63a7\u5236\u67b6\u6784\u5728\u590d\u6742\u519c\u4e1a\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u7075\u6d3b\u7684\u4efb\u52a1\u6267\u884c\u548c\u4eba\u673a\u534f\u4f5c\u3002"}}
{"id": "2506.23725", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23725", "abs": "https://arxiv.org/abs/2506.23725", "authors": ["Atharva Gundawar", "Som Sagar", "Ransalu Senanayake"], "title": "PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?", "comment": null, "summary": "Vision-Language Models (VLMs) are increasingly pivotal for generalist robot\nmanipulation, enabling tasks such as physical reasoning, policy generation, and\nfailure detection. However, their proficiency in these high-level applications\noften assumes a deep understanding of low-level physical prerequisites, a\ncapability that remains largely unverified. For robots to perform actions\nreliably, they must comprehend intrinsic object properties (e.g., material,\nweight), action affordances (e.g., graspable, stackable), and physical\nconstraints (e.g., stability, reachability, or an object's state, such as being\nclosed). Despite the widespread use of VLMs in manipulation tasks, we argue\nthat off-the-shelf models may lack this granular, physically grounded\nunderstanding, as such prerequisites are often overlooked during training.\n  To address this critical gap, we introduce PAC Bench, a comprehensive\nbenchmark designed to systematically evaluate VLMs on their understanding of\ncore Properties, Affordances, and Constraints (PAC) from a task executability\nperspective. PAC Bench features a diverse dataset with over 30,000 annotations,\ncomprising 673 real-world images (115 object classes, 15 property types, and 1\nto 3 affordances defined per class), 100 real-world humanoid-view scenarios,\nand 120 unique simulated constraint scenarios across four tasks.\n  Our evaluations reveal significant gaps in the ability of current VLMs to\ngrasp fundamental physical concepts, highlighting limitations in their\nsuitability for reliable robot manipulation and pointing to key areas for\ntargeted research. PAC Bench also serves as a standardized benchmark for\nrigorously evaluating physical reasoning in VLMs and guiding the development of\nmore robust, physically grounded models for robotic applications.\n  Project Page: https://pacbench.github.io/", "AI": {"tldr": "PAC Bench\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u7269\u7406\u5c5e\u6027\u3001\u529f\u80fd\u6027\u548c\u7ea6\u675f\uff08PAC\uff09\u7406\u89e3\u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u7269\u7406\u57fa\u7840\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u5c3d\u7ba1VLMs\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5bf9\u4f4e\u5c42\u6b21\u7269\u7406\u524d\u63d0\uff08\u5982\u7269\u4f53\u5c5e\u6027\u3001\u529f\u80fd\u6027\u548c\u7ea6\u675f\uff09\u7684\u7406\u89e3\u5c1a\u672a\u9a8c\u8bc1\uff0c\u53ef\u80fd\u5f71\u54cd\u4efb\u52a1\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faPAC Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u591a\u6837\u5316\u6570\u636e\u96c6\uff0830,000+\u6807\u6ce8\u3001673\u5f20\u771f\u5b9e\u56fe\u50cf\u3001100\u4e2a\u4eba\u7c7b\u89c6\u89d2\u573a\u666f\u548c120\u4e2a\u6a21\u62df\u7ea6\u675f\u573a\u666f\uff09\uff0c\u7cfb\u7edf\u8bc4\u4f30VLMs\u7684PAC\u7406\u89e3\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u5f53\u524dVLMs\u5728\u57fa\u7840\u7269\u7406\u6982\u5ff5\u7406\u89e3\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u4e0d\u9002\u5408\u53ef\u9760\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "conclusion": "PAC Bench\u4e3a\u8bc4\u4f30VLMs\u7684\u7269\u7406\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u5e76\u6307\u5bfc\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u7269\u7406\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2506.23739", "categories": ["cs.RO", "cs.CE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.23739", "abs": "https://arxiv.org/abs/2506.23739", "authors": ["Lisa Marie Otto", "Michael Kaiser", "Daniel Seebacher", "Steffen M\u00fcller"], "title": "Validation of AI-Based 3D Human Pose Estimation in a Cyber-Physical Environment", "comment": "6 pages, 5 figures, Preprint for 2025 IEEE IAVVC (International\n  Automated Vehicle Validation Conference)", "summary": "Ensuring safe and realistic interactions between automated driving systems\nand vulnerable road users (VRUs) in urban environments requires advanced\ntesting methodologies. This paper presents a test environment that combines a\nVehiclein-the-Loop (ViL) test bench with a motion laboratory, demonstrating the\nfeasibility of cyber-physical (CP) testing of vehicle-pedestrian and\nvehicle-cyclist interactions. Building upon previous work focused on pedestrian\nlocalization, we further validate a human pose estimation (HPE) approach\nthrough a comparative analysis of real-world (RW) and virtual representations\nof VRUs. The study examines the perception of full-body motion using a\ncommercial monocular camera-based 3Dskeletal detection AI. The virtual scene is\ngenerated in Unreal Engine 5, where VRUs are animated in real time and\nprojected onto a screen to stimulate the camera. The proposed stimulation\ntechnique ensures the correct perspective, enabling realistic vehicle\nperception. To assess the accuracy and consistency of HPE across RW and CP\ndomains, we analyze the reliability of detections as well as variations in\nmovement trajectories and joint estimation stability. The validation includes\ndynamic test scenarios where human avatars, both walking and cycling, are\nmonitored under controlled conditions. Our results show a strong alignment in\nHPE between RW and CP test conditions for stable motion patterns, while notable\ninaccuracies persist under dynamic movements and occlusions, particularly for\ncomplex cyclist postures. These findings contribute to refining CP testing\napproaches for evaluating next-generation AI-based vehicle perception and to\nenhancing interaction models of automated vehicles and VRUs in CP environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8f66\u8f86\u5728\u73af\u6d4b\u8bd5\u53f0\u548c\u8fd0\u52a8\u5b9e\u9a8c\u5ba4\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u7528\u4e8e\u9a8c\u8bc1\u8f66\u8f86\u4e0e\u884c\u4eba/\u9a91\u884c\u8005\u4ea4\u4e92\u7684\u53ef\u884c\u6027\uff0c\u5e76\u901a\u8fc7\u6bd4\u8f83\u771f\u5b9e\u4e16\u754c\u4e0e\u865a\u62df\u573a\u666f\u4e2d\u7684\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff08HPE\uff09\u6765\u8bc4\u4f30\u611f\u77e5\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e0e\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRUs\uff09\u7684\u5b89\u5168\u548c\u771f\u5b9e\u4ea4\u4e92\uff0c\u9700\u8981\u5148\u8fdb\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u8f66\u8f86\u5728\u73af\u6d4b\u8bd5\u53f0\u548c\u8fd0\u52a8\u5b9e\u9a8c\u5ba4\uff0c\u5229\u7528Unreal Engine 5\u751f\u6210\u865a\u62df\u573a\u666f\uff0c\u5b9e\u65f6\u6295\u5f71VRUs\u52a8\u753b\u4ee5\u523a\u6fc0\u6444\u50cf\u5934\uff0c\u5e76\u901a\u8fc7\u5546\u4e1a\u5355\u76ee\u6444\u50cf\u5934AI\u8fdb\u884c3D\u9aa8\u9abc\u68c0\u6d4b\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5728\u7a33\u5b9a\u8fd0\u52a8\u6a21\u5f0f\u4e0b\uff0cHPE\u5728\u771f\u5b9e\u4e16\u754c\u4e0e\u865a\u62df\u573a\u666f\u4e2d\u8868\u73b0\u4e00\u81f4\uff0c\u4f46\u5728\u52a8\u6001\u8fd0\u52a8\u548c\u906e\u6321\u60c5\u51b5\u4e0b\uff08\u5c24\u5176\u662f\u590d\u6742\u9a91\u884c\u8005\u59ff\u52bf\uff09\u5b58\u5728\u663e\u8457\u8bef\u5dee\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6539\u8fdb\u4e0b\u4e00\u4ee3\u57fa\u4e8eAI\u7684\u8f66\u8f86\u611f\u77e5\u6d4b\u8bd5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53c2\u8003\uff0c\u5e76\u4f18\u5316\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0eVRUs\u5728\u865a\u62df\u73af\u5883\u4e2d\u7684\u4ea4\u4e92\u6a21\u578b\u3002"}}
{"id": "2506.23768", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23768", "abs": "https://arxiv.org/abs/2506.23768", "authors": ["Vittorio La Barbera", "Steven Bohez", "Leonard Hasenclever", "Yuval Tassa", "John R. Hutchinson"], "title": "Motion Tracking with Muscles: Predictive Control of a Parametric Musculoskeletal Canine Model", "comment": null, "summary": "We introduce a novel musculoskeletal model of a dog, procedurally generated\nfrom accurate 3D muscle meshes. Accompanying this model is a motion\ncapture-based locomotion task compatible with a variety of control algorithms,\nas well as an improved muscle dynamics model designed to enhance convergence in\ndifferentiable control frameworks. We validate our approach by comparing\nsimulated muscle activation patterns with experimentally obtained\nelectromyography (EMG) data from previous canine locomotion studies. This work\naims to bridge gaps between biomechanics, robotics, and computational\nneuroscience, offering a robust platform for researchers investigating muscle\nactuation and neuromuscular control.We plan to release the full model along\nwith the retargeted motion capture clips to facilitate further research and\ndevelopment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u808c\u8089\u7f51\u683c\u7684\u72d7\u9aa8\u9abc\u808c\u8089\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u4e86\u8fd0\u52a8\u6355\u6349\u4efb\u52a1\u548c\u6539\u8fdb\u7684\u808c\u8089\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u6a21\u62df\u808c\u8089\u6fc0\u6d3b\u6a21\u5f0f\u4e0e\u5b9e\u9a8cEMG\u6570\u636e\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u586b\u8865\u751f\u7269\u529b\u5b66\u3001\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u808c\u8089\u9a71\u52a8\u548c\u795e\u7ecf\u808c\u8089\u63a7\u5236\u63d0\u4f9b\u5e73\u53f0\u3002", "method": "\u4f7f\u75283D\u808c\u8089\u7f51\u683c\u751f\u6210\u9aa8\u9abc\u808c\u8089\u6a21\u578b\uff0c\u7ed3\u5408\u8fd0\u52a8\u6355\u6349\u4efb\u52a1\u548c\u6539\u8fdb\u7684\u808c\u8089\u52a8\u529b\u5b66\u6a21\u578b\u3002", "result": "\u6a21\u62df\u808c\u8089\u6fc0\u6d3b\u6a21\u5f0f\u4e0e\u5b9e\u9a8cEMG\u6570\u636e\u4e00\u81f4\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\uff0c\u672a\u6765\u5c06\u516c\u5f00\u6a21\u578b\u548c\u8fd0\u52a8\u6355\u6349\u6570\u636e\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2506.23771", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23771", "abs": "https://arxiv.org/abs/2506.23771", "authors": ["Guizhe Jin", "Zhuoren Li", "Bo Leng", "Ran Yu", "Lu Xiong"], "title": "Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving", "comment": "8 pages, Submitted to IEEE Robotics and Automation Letters", "summary": "Reinforcement Learning (RL) is increasingly used in autonomous driving (AD)\nand shows clear advantages. However, most RL-based AD methods overlook policy\nstructure design. An RL policy that only outputs short-timescale vehicle\ncontrol commands results in fluctuating driving behavior due to fluctuations in\nnetwork outputs, while one that only outputs long-timescale driving goals\ncannot achieve unified optimality of driving behavior and control. Therefore,\nwe propose a multi-timescale hierarchical reinforcement learning approach. Our\napproach adopts a hierarchical policy structure, where high- and low-level RL\npolicies are unified-trained to produce long-timescale motion guidance and\nshort-timescale control commands, respectively. Therein, motion guidance is\nexplicitly represented by hybrid actions to capture multimodal driving\nbehaviors on structured road and support incremental low-level extend-state\nupdates. Additionally, a hierarchical safety mechanism is designed to ensure\nmulti-timescale safety. Evaluation in simulator-based and HighD dataset-based\nhighway multi-lane scenarios demonstrates that our approach significantly\nimproves AD performance, effectively increasing driving efficiency, action\nconsistency and safety.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u65f6\u95f4\u5c3a\u5ea6\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\uff0c\u7ed3\u5408\u957f\u65f6\u8fd0\u52a8\u6307\u5bfc\u548c\u77ed\u65f6\u63a7\u5236\u547d\u4ee4\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5ffd\u7565\u4e86\u7b56\u7565\u7ed3\u6784\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u884c\u4e3a\u6ce2\u52a8\u6216\u65e0\u6cd5\u7edf\u4e00\u4f18\u5316\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7b56\u7565\u7ed3\u6784\uff0c\u9ad8\u4f4e\u5c42\u7b56\u7565\u7edf\u4e00\u8bad\u7ec3\uff0c\u5206\u522b\u751f\u6210\u957f\u65f6\u8fd0\u52a8\u6307\u5bfc\u548c\u77ed\u65f6\u63a7\u5236\u547d\u4ee4\uff0c\u5e76\u8bbe\u8ba1\u5206\u5c42\u5b89\u5168\u673a\u5236\u3002", "result": "\u5728\u6a21\u62df\u5668\u548cHighD\u6570\u636e\u96c6\u7684\u9ad8\u901f\u516c\u8def\u591a\u8f66\u9053\u573a\u666f\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9a7e\u9a76\u6548\u7387\u3001\u884c\u4e3a\u4e00\u81f4\u6027\u548c\u5b89\u5168\u6027\u3002", "conclusion": "\u591a\u65f6\u95f4\u5c3a\u5ea6\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u884c\u4e3a\u6ce2\u52a8\u548c\u7edf\u4e00\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2506.23781", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.23781", "abs": "https://arxiv.org/abs/2506.23781", "authors": ["Savvas Papaioannou", "Panayiotis Kolios", "Christos G. Panayiotou", "Marios M. Polycarpou"], "title": "Data-Driven Predictive Planning and Control for Aerial 3D Inspection with Back-face Elimination", "comment": "2025 European Control Conference (ECC), Thessaloniki, Greece, 24-27\n  June 2025", "summary": "Automated inspection with Unmanned Aerial Systems (UASs) is a transformative\ncapability set to revolutionize various application domains. However, this task\nis inherently complex, as it demands the seamless integration of perception,\nplanning, and control which existing approaches often treat separately.\nMoreover, it requires accurate long-horizon planning to predict action\nsequences, in contrast to many current techniques, which tend to be myopic. To\novercome these limitations, we propose a 3D inspection approach that unifies\nperception, planning, and control within a single data-driven predictive\ncontrol framework. Unlike traditional methods that rely on known UAS dynamic\nmodels, our approach requires only input-output data, making it easily\napplicable to off-the-shelf black-box UASs. Our method incorporates back-face\nelimination, a visibility determination technique from 3D computer graphics,\ndirectly into the control loop, thereby enabling the online generation of\naccurate, long-horizon 3D inspection trajectories.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u9884\u6d4b\u63a7\u5236\u76843D\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7edf\u4e00\u611f\u77e5\u3001\u89c4\u5212\u4e0e\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u73b0\u6210\u65e0\u4eba\u673a\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5206\u79bb\uff0c\u4e14\u7f3a\u4e4f\u957f\u65f6\u7a0b\u89c4\u5212\u80fd\u529b\uff0c\u96be\u4ee5\u6ee1\u8db3\u65e0\u4eba\u673a\u81ea\u52a8\u68c0\u6d4b\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u54083D\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e2d\u7684\u80cc\u9762\u6d88\u9664\u6280\u672f\uff0c\u5b9e\u73b0\u5728\u7ebf\u751f\u6210\u957f\u65f6\u7a0b3D\u68c0\u6d4b\u8f68\u8ff9\u3002", "result": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5df2\u77e5\u65e0\u4eba\u673a\u52a8\u6001\u6a21\u578b\uff0c\u4ec5\u9700\u8f93\u5165\u8f93\u51fa\u6570\u636e\uff0c\u9002\u7528\u4e8e\u73b0\u6210\u9ed1\u76d2\u65e0\u4eba\u673a\u7cfb\u7edf\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u65e0\u4eba\u673a\u81ea\u52a8\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23919", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23919", "abs": "https://arxiv.org/abs/2506.23919", "authors": ["Haonan Chen", "Bangjun Wang", "Jingxiang Guo", "Tianrui Zhang", "Yiwen Hou", "Xuchuan Huang", "Chenrui Tie", "Lin Shao"], "title": "World4Omni: A Zero-Shot Framework from Image Generation World Model to Robotic Manipulation", "comment": null, "summary": "Improving data efficiency and generalization in robotic manipulation remains\na core challenge. We propose a novel framework that leverages a pre-trained\nmultimodal image-generation model as a world model to guide policy learning. By\nexploiting its rich visual-semantic representations and strong generalization\nacross diverse scenes, the model generates open-ended future state predictions\nthat inform downstream manipulation. Coupled with zero-shot low-level control\nmodules, our approach enables general-purpose robotic manipulation without\ntask-specific training. Experiments in both simulation and real-world\nenvironments demonstrate that our method achieves effective performance across\na wide range of manipulation tasks with no additional data collection or\nfine-tuning. Supplementary materials are available on our website:\nhttps://world4omni.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u6307\u5bfc\u7b56\u7565\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u5b9e\u73b0\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u7684\u9ad8\u6548\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u7684\u6838\u5fc3\u6311\u6218\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\u6a21\u578b\u751f\u6210\u672a\u6765\u72b6\u6001\u9884\u6d4b\uff0c\u7ed3\u5408\u96f6\u6837\u672c\u4f4e\u7ea7\u63a7\u5236\u6a21\u5757\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u591a\u79cd\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6536\u96c6\u6216\u5fae\u8c03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23944", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23944", "abs": "https://arxiv.org/abs/2506.23944", "authors": ["Fuhang Kuang", "Jiacheng You", "Yingdong Hu", "Tong Zhang", "Chuan Wen", "Yang Gao"], "title": "Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning", "comment": null, "summary": "Imitation learning models for robotic tasks typically rely on multi-modal\ninputs, such as RGB images, language, and proprioceptive states. While\nproprioception is intuitively important for decision-making and obstacle\navoidance, simply incorporating all proprioceptive states leads to a surprising\ndegradation in imitation learning performance. In this work, we identify the\nunderlying issue as the proprioception shift problem, where the distributions\nof proprioceptive states diverge significantly between training and deployment.\nTo address this challenge, we propose a domain adaptation framework that\nbridges the gap by utilizing rollout data collected during deployment. Using\nWasserstein distance, we quantify the discrepancy between expert and rollout\nproprioceptive states and minimize this gap by adding noise to both sets of\nstates, proportional to the Wasserstein distance. This strategy enhances\nrobustness against proprioception shifts by aligning the training and\ndeployment distributions. Experiments on robotic manipulation tasks demonstrate\nthe efficacy of our method, enabling the imitation policy to leverage\nproprioception while mitigating its adverse effects. Our approach outperforms\nthe naive solution which discards proprioception, and other baselines designed\nto address distributional shifts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u4e2d\u672c\u4f53\u611f\u53d7\u504f\u79fb\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u57df\u9002\u5e94\u6846\u67b6\u548cWasserstein\u8ddd\u79bb\u5bf9\u9f50\u8bad\u7ec3\u4e0e\u90e8\u7f72\u5206\u5e03\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u591a\u6a21\u6001\u8f93\u5165\uff0c\u4f46\u76f4\u63a5\u5f15\u5165\u6240\u6709\u672c\u4f53\u611f\u53d7\u72b6\u6001\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u662f\u7531\u4e8e\u8bad\u7ec3\u548c\u90e8\u7f72\u65f6\u672c\u4f53\u611f\u53d7\u72b6\u6001\u7684\u5206\u5e03\u5dee\u5f02\uff08\u672c\u4f53\u611f\u53d7\u504f\u79fb\u95ee\u9898\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57df\u9002\u5e94\u6846\u67b6\uff0c\u5229\u7528\u90e8\u7f72\u671f\u95f4\u6536\u96c6\u7684\u6570\u636e\uff0c\u901a\u8fc7Wasserstein\u8ddd\u79bb\u91cf\u5316\u4e13\u5bb6\u4e0e\u90e8\u7f72\u672c\u4f53\u611f\u53d7\u72b6\u6001\u7684\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u6dfb\u52a0\u566a\u58f0\u6700\u5c0f\u5316\u8fd9\u79cd\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u4f18\u4e8e\u76f4\u63a5\u4e22\u5f03\u672c\u4f53\u611f\u53d7\u72b6\u6001\u6216\u5176\u4ed6\u5904\u7406\u5206\u5e03\u504f\u79fb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u672c\u4f53\u611f\u53d7\u504f\u79fb\u95ee\u9898\uff0c\u4f7f\u6a21\u4eff\u7b56\u7565\u80fd\u591f\u5229\u7528\u672c\u4f53\u611f\u53d7\u4fe1\u606f\u5e76\u51cf\u5c11\u5176\u8d1f\u9762\u5f71\u54cd\u3002"}}
{"id": "2506.23999", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23999", "abs": "https://arxiv.org/abs/2506.23999", "authors": ["Zeyu Han", "Mengchi Cai", "Chaoyi Chen", "Qingwen Meng", "Guangwei Wang", "Ying Liu", "Qing Xu", "Jianqiang Wang", "Keqiang Li"], "title": "Predictive Risk Analysis and Safe Trajectory Planning for Intelligent and Connected Vehicles", "comment": null, "summary": "The safe trajectory planning of intelligent and connected vehicles is a key\ncomponent in autonomous driving technology. Modeling the environment risk\ninformation by field is a promising and effective approach for safe trajectory\nplanning. However, existing risk assessment theories only analyze the risk by\ncurrent information, ignoring future prediction. This paper proposes a\npredictive risk analysis and safe trajectory planning framework for intelligent\nand connected vehicles. This framework first predicts future trajectories of\nobjects by a local risk-aware algorithm, following with a\nspatiotemporal-discretised predictive risk analysis using the prediction\nresults. Then the safe trajectory is generated based on the predictive risk\nanalysis. Finally, simulation and vehicle experiments confirm the efficacy and\nreal-time practicability of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u7f51\u8054\u8f66\u8f86\u7684\u9884\u6d4b\u98ce\u9669\u5206\u6790\u4e0e\u5b89\u5168\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u672a\u6765\u9884\u6d4b\u4f18\u5316\u8f68\u8ff9\u89c4\u5212\u3002", "motivation": "\u73b0\u6709\u98ce\u9669\u8bc4\u4f30\u7406\u8bba\u4ec5\u57fa\u4e8e\u5f53\u524d\u4fe1\u606f\uff0c\u5ffd\u7565\u672a\u6765\u9884\u6d4b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u667a\u80fd\u7f51\u8054\u8f66\u8f86\u7684\u5b89\u5168\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5c40\u90e8\u98ce\u9669\u611f\u77e5\u7b97\u6cd5\u9884\u6d4b\u7269\u4f53\u672a\u6765\u8f68\u8ff9\uff0c\u5229\u7528\u9884\u6d4b\u7ed3\u679c\u8fdb\u884c\u65f6\u7a7a\u79bb\u6563\u5316\u7684\u9884\u6d4b\u98ce\u9669\u5206\u6790\uff0c\u5e76\u751f\u6210\u5b89\u5168\u8f68\u8ff9\u3002", "result": "\u4eff\u771f\u548c\u8f66\u8f86\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u65f6\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u667a\u80fd\u7f51\u8054\u8f66\u8f86\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u7684\u8f68\u8ff9\u89c4\u5212\u65b9\u6848\u3002"}}
{"id": "2506.24046", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.24046", "abs": "https://arxiv.org/abs/2506.24046", "authors": ["Olivia Richards", "Keith L. Obstein", "Nabil Simaan"], "title": "Exploring Accelerated Skill Acquisition via Tandem Training for Colonoscopy", "comment": null, "summary": "New endoscopists require a large volume of expert-proctored colonoscopies to\nattain minimal competency. Developing multi-fingered, synchronized control of a\ncolonoscope requires significant time and exposure to the device. Current\ntraining methods inhibit this development by relying on tool hand-off for\nexpert demonstrations. There is a need for colonoscopy training tools that\nenable in-hand expert guidance in real-time. We present a new concept of a\ntandem training system that uses a telemanipulated preceptor colonoscope to\nguide novice users as they perform a colonoscopy. This system is capable of\ndual-control and can automatically toggle between expert and novice control of\na standard colonoscope's angulation control wheels. Preliminary results from a\nuser study with novice and expert users show the effectiveness of this device\nas a skill acquisition tool. We believe that this device has the potential to\naccelerate skill acquisition for colonoscopy and, in the future, enable\nindividualized instruction and responsive teaching through bidirectional\nactuation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7ed3\u80a0\u955c\u8bad\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fdc\u7a0b\u64cd\u63a7\u7684\u5bfc\u5e08\u7ed3\u80a0\u955c\u5b9e\u65f6\u6307\u5bfc\u65b0\u624b\u64cd\u4f5c\uff0c\u52a0\u901f\u6280\u80fd\u5b66\u4e60\u3002", "motivation": "\u5f53\u524d\u7ed3\u80a0\u955c\u57f9\u8bad\u65b9\u6cd5\u4f9d\u8d56\u5de5\u5177\u4ea4\u63a5\uff0c\u9650\u5236\u4e86\u65b0\u624b\u5bf9\u8bbe\u5907\u7684\u719f\u7ec3\u5ea6\u53d1\u5c55\uff0c\u4e9f\u9700\u5b9e\u65f6\u6307\u5bfc\u5de5\u5177\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u53cc\u63a7\u7cfb\u7edf\uff0c\u53ef\u81ea\u52a8\u5207\u6362\u4e13\u5bb6\u548c\u65b0\u624b\u5bf9\u7ed3\u80a0\u955c\u89d2\u5ea6\u63a7\u5236\u8f6e\u7684\u64cd\u4f5c\u3002", "result": "\u521d\u6b65\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u6709\u6548\u63d0\u5347\u65b0\u624b\u6280\u80fd\u5b66\u4e60\u6548\u679c\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6709\u671b\u52a0\u901f\u7ed3\u80a0\u955c\u6280\u80fd\u5b66\u4e60\uff0c\u672a\u6765\u53ef\u5b9e\u73b0\u4e2a\u6027\u5316\u6307\u5bfc\u548c\u53cc\u5411\u64cd\u4f5c\u6559\u5b66\u3002"}}
