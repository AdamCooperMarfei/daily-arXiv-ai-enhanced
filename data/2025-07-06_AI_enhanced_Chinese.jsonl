{"id": "2507.02016", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02016", "abs": "https://arxiv.org/abs/2507.02016", "authors": ["Cong Wang", "Roberto Calandra", "Verena Kl\u00f6s"], "title": "Effective Explanations for Belief-Desire-Intention Robots: When and What to Explain", "comment": "Paper accepted at IEEE RO-MAN 2025; 6 pages", "summary": "When robots perform complex and context-dependent tasks in our daily lives,\ndeviations from expectations can confuse users. Explanations of the robot's\nreasoning process can help users to understand the robot intentions. However,\nwhen to provide explanations and what they contain are important to avoid user\nannoyance. We have investigated user preferences for explanation demand and\ncontent for a robot that helps with daily cleaning tasks in a kitchen. Our\nresults show that users want explanations in surprising situations and prefer\nconcise explanations that clearly state the intention behind the confusing\naction and the contextual factors that were relevant to this decision. Based on\nthese findings, we propose two algorithms to identify surprising actions and to\nconstruct effective explanations for Belief-Desire-Intention (BDI) robots. Our\nalgorithms can be easily integrated in the BDI reasoning process and pave the\nway for better human-robot interaction with context- and user-specific\nexplanations.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u7528\u6237\u5bf9\u673a\u5668\u4eba\u89e3\u91ca\u9700\u6c42\u548c\u5185\u5bb9\u7684\u504f\u597d\uff0c\u63d0\u51fa\u4e24\u79cd\u7b97\u6cd5\u7528\u4e8e\u8bc6\u522b\u610f\u5916\u884c\u4e3a\u5e76\u751f\u6210\u6709\u6548\u89e3\u91ca\u3002", "motivation": "\u673a\u5668\u4eba\u6267\u884c\u590d\u6742\u4efb\u52a1\u65f6\uff0c\u504f\u79bb\u9884\u671f\u7684\u884c\u4e3a\u4f1a\u4f7f\u7528\u6237\u56f0\u60d1\uff0c\u89e3\u91ca\u5176\u63a8\u7406\u8fc7\u7a0b\u6709\u52a9\u4e8e\u7528\u6237\u7406\u89e3\u610f\u56fe\uff0c\u4f46\u9700\u907f\u514d\u5f15\u8d77\u7528\u6237\u53cd\u611f\u3002", "method": "\u8c03\u67e5\u7528\u6237\u5bf9\u53a8\u623f\u6e05\u6d01\u673a\u5668\u4eba\u89e3\u91ca\u7684\u504f\u597d\uff0c\u63d0\u51fa\u4e24\u79cd\u7b97\u6cd5\u8bc6\u522b\u610f\u5916\u884c\u4e3a\u5e76\u6784\u5efa\u89e3\u91ca\u3002", "result": "\u7528\u6237\u5e0c\u671b\u5728\u610f\u5916\u60c5\u51b5\u4e0b\u83b7\u5f97\u89e3\u91ca\uff0c\u504f\u597d\u7b80\u6d01\u4e14\u660e\u786e\u8bf4\u660e\u610f\u56fe\u548c\u4e0a\u4e0b\u6587\u56e0\u7d20\u7684\u8868\u8fbe\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u53ef\u8f7b\u677e\u96c6\u6210\u5230BDI\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u4e3a\u66f4\u597d\u7684\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2507.02029", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.02029", "abs": "https://arxiv.org/abs/2507.02029", "authors": ["BAAI RoboBrain Team", "Mingyu Cao", "Huajie Tan", "Yuheng Ji", "Minglan Lin", "Zhiyu Li", "Zhou Cao", "Pengwei Wang", "Enshen Zhou", "Yi Han", "Yingbo Tang", "Xiangqi Xu", "Wei Guo", "Yaoxu Lyu", "Yijie Xu", "Jiayu Shi", "Cheng Chi", "Mengdi Zhao", "Xiaoshuai Hao", "Shanyu Rong", "Zhengliang Cai", "Bolun Zhang", "Shuyi Zhang", "Huaihai Lyu", "Mengfei Du", "Lingfeng Zhang", "Xi Feng", "Xiaodan Liu", "Yance Jiao", "Chenrui He", "Mengsi Lyu", "Zhuo Chen", "Yulong Ao", "Xue Sun", "Zheqi He", "Jingshu Zheng", "Xi Yang", "Donghai Shi", "Kunchang Xie", "Bochao Zhang", "Shaokai Nie", "Chunlei Men", "Yonghua Lin", "Zhongyuan Wang", "Tiejun Huang", "Shanghang Zhang"], "title": "RoboBrain 2.0 Technical Report", "comment": null, "summary": "We introduce RoboBrain 2.0, our latest generation of embodied vision-language\nfoundation models, designed to unify perception, reasoning, and planning for\ncomplex embodied tasks in physical environments. It comes in two variants: a\nlightweight 7B model and a full-scale 32B model, featuring a heterogeneous\narchitecture with a vision encoder and a language model. Despite its compact\nsize, RoboBrain 2.0 achieves strong performance across a wide spectrum of\nembodied reasoning tasks. On both spatial and temporal benchmarks, the 32B\nvariant achieves leading results, surpassing prior open-source and proprietary\nmodels. In particular, it supports key real-world embodied AI capabilities,\nincluding spatial understanding (e.g., affordance prediction, spatial\nreferring, trajectory forecasting) and temporal decision-making (e.g.,\nclosed-loop interaction, multi-agent long-horizon planning, and scene graph\nupdating). This report details the model architecture, data construction,\nmulti-stage training strategies, infrastructure and practical applications. We\nhope RoboBrain 2.0 advances embodied AI research and serves as a practical step\ntoward building generalist embodied agents. The code, checkpoint and benchmark\nare available at https://superrobobrain.github.io.", "AI": {"tldr": "RoboBrain 2.0\u662f\u4e00\u4e2a\u65b0\u578b\u7684\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u7269\u7406\u73af\u5883\u4e2d\u7684\u611f\u77e5\u3001\u63a8\u7406\u548c\u89c4\u5212\u4efb\u52a1\uff0c\u63d0\u4f9b7B\u548c32B\u4e24\u79cd\u7248\u672c\uff0c\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u65e8\u5728\u7edf\u4e00\u590d\u6742\u7269\u7406\u4efb\u52a1\u4e2d\u7684\u611f\u77e5\u3001\u63a8\u7406\u548c\u89c4\u5212\uff0c\u63a8\u52a8\u5177\u8eabAI\u7814\u7a76\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u5f02\u6784\u67b6\u6784\uff0c\u7ed3\u5408\u89c6\u89c9\u7f16\u7801\u5668\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u4f18\u5316\u3002", "result": "32B\u7248\u672c\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u9886\u5148\uff0c\u652f\u6301\u591a\u79cd\u73b0\u5b9eAI\u80fd\u529b\u3002", "conclusion": "RoboBrain 2.0\u4e3a\u5177\u8eabAI\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u5e76\u63a8\u52a8\u4e86\u901a\u7528\u5177\u8eab\u4ee3\u7406\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.02171", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02171", "abs": "https://arxiv.org/abs/2507.02171", "authors": ["Miroslav Cibula", "Krist\u00edna Malinovsk\u00e1", "Matthias Kerzel"], "title": "Towards Bio-Inspired Robotic Trajectory Planning via Self-Supervised RNN", "comment": "12 pages, 4 figures, 2 tables. To be published in 2025 International\n  Conference on Artificial Neural Networks (ICANN) proceedings. This research\n  was funded by the Horizon Europe project TERAIS, GA no. 101079338, and in\n  part by the Slovak Grant Agency for Science (VEGA), project 1/0373/23", "summary": "Trajectory planning in robotics is understood as generating a sequence of\njoint configurations that will lead a robotic agent, or its manipulator, from\nan initial state to the desired final state, thus completing a manipulation\ntask while considering constraints like robot kinematics and the environment.\nTypically, this is achieved via sampling-based planners, which are\ncomputationally intensive. Recent advances demonstrate that trajectory planning\ncan also be performed by supervised sequence learning of trajectories, often\nrequiring only a single or fixed number of passes through a neural\narchitecture, thus ensuring a bounded computation time. Such fully supervised\napproaches, however, perform imitation learning; they do not learn based on\nwhether the trajectories can successfully reach a goal, but try to reproduce\nobserved trajectories. In our work, we build on this approach and propose a\ncognitively inspired self-supervised learning scheme based on a recurrent\narchitecture for building a trajectory model. We evaluate the feasibility of\nthe proposed method on a task of kinematic planning for a robotic arm. The\nresults suggest that the model is able to learn to generate trajectories only\nusing given paired forward and inverse kinematics models, and indicate that\nthis novel method could facilitate planning for more complex manipulation tasks\nrequiring adaptive solutions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba4\u77e5\u542f\u53d1\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u8f68\u8ff9\u89c4\u5212\uff0c\u901a\u8fc7\u5faa\u73af\u67b6\u6784\u6784\u5efa\u8f68\u8ff9\u6a21\u578b\uff0c\u4ec5\u9700\u6b63\u5411\u548c\u9006\u5411\u8fd0\u52a8\u5b66\u6a21\u578b\u5373\u53ef\u751f\u6210\u8f68\u8ff9\u3002", "motivation": "\u4f20\u7edf\u91c7\u6837\u89c4\u5212\u5668\u8ba1\u7b97\u91cf\u5927\uff0c\u800c\u5b8c\u5168\u76d1\u7763\u65b9\u6cd5\u4ec5\u6a21\u4eff\u8f68\u8ff9\uff0c\u65e0\u6cd5\u786e\u4fdd\u76ee\u6807\u8fbe\u6210\u3002\u56e0\u6b64\uff0c\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u80fd\u81ea\u9002\u5e94\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6848\uff0c\u57fa\u4e8e\u5faa\u73af\u67b6\u6784\u6784\u5efa\u8f68\u8ff9\u6a21\u578b\uff0c\u5229\u7528\u6b63\u5411\u548c\u9006\u5411\u8fd0\u52a8\u5b66\u6a21\u578b\u751f\u6210\u8f68\u8ff9\u3002", "result": "\u6a21\u578b\u80fd\u4ec5\u901a\u8fc7\u7ed9\u5b9a\u8fd0\u52a8\u5b66\u6a21\u578b\u5b66\u4e60\u751f\u6210\u8f68\u8ff9\uff0c\u9002\u7528\u4e8e\u590d\u6742\u4efb\u52a1\u7684\u81ea\u9002\u5e94\u89c4\u5212\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u81ea\u9002\u5e94\u7684\u8f68\u8ff9\u89c4\u5212\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.02190", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02190", "abs": "https://arxiv.org/abs/2507.02190", "authors": ["Max Argus", "Jelena Bratulic", "Houman Masnavi", "Maxim Velikanov", "Nick Heppert", "Abhinav Valada", "Thomas Brox"], "title": "cVLA: Towards Efficient Camera-Space VLAs", "comment": "20 pages, 10 figures", "summary": "Vision-Language-Action (VLA) models offer a compelling framework for tackling\ncomplex robotic manipulation tasks, but they are often expensive to train. In\nthis paper, we propose a novel VLA approach that leverages the competitive\nperformance of Vision Language Models (VLMs) on 2D images to directly infer\nrobot end-effector poses in image frame coordinates. Unlike prior VLA models\nthat output low-level controls, our model predicts trajectory waypoints, making\nit both more efficient to train and robot embodiment agnostic. Despite its\nlightweight design, our next-token prediction architecture effectively learns\nmeaningful and executable robot trajectories. We further explore the\nunderutilized potential of incorporating depth images, inference-time\ntechniques such as decoding strategies, and demonstration-conditioned action\ngeneration. Our model is trained on a simulated dataset and exhibits strong\nsim-to-real transfer capabilities. We evaluate our approach using a combination\nof simulated and real data, demonstrating its effectiveness on a real robotic\nsystem.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8f7b\u91cf\u7ea7VLA\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u8f68\u8ff9\u70b9\u800c\u975e\u4f4e\u7ea7\u63a7\u5236\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u5e76\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u3002", "motivation": "\u4f20\u7edfVLA\u6a21\u578b\u8bad\u7ec3\u6210\u672c\u9ad8\uff0c\u4e14\u8f93\u51fa\u4f4e\u7ea7\u63a7\u5236\uff0c\u9650\u5236\u4e86\u5176\u6548\u7387\u548c\u901a\u7528\u6027\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece2D\u56fe\u50cf\u76f4\u63a5\u63a8\u65ad\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u4f4d\u59ff\uff0c\u9884\u6d4b\u8f68\u8ff9\u70b9\uff0c\u5e76\u63a2\u7d22\u6df1\u5ea6\u56fe\u50cf\u548c\u89e3\u7801\u7b56\u7565\u7684\u6f5c\u529b\u3002", "result": "\u6a21\u578b\u5728\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u5747\u8868\u73b0\u826f\u597d\uff0c\u5177\u6709\u5f3a\u5927\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u6709\u6548\u5b66\u4e60\u53ef\u6267\u884c\u8f68\u8ff9\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\uff0c\u5c55\u793a\u4e86VLA\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.02198", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.02198", "abs": "https://arxiv.org/abs/2507.02198", "authors": ["Surya Pratap Singh", "Tsimafei Lazouski", "Maani Ghaffari"], "title": "GPS-DRIFT: Marine Surface Robot Localization using IMU-GPS Fusion and Invariant Filtering", "comment": "6 pages", "summary": "This paper presents an extension of the DRIFT invariant state estimation\nframework, enabling robust fusion of GPS and IMU data for accurate pose and\nheading estimation. Originally developed for testing and usage on a marine\nautonomous surface vehicle (ASV), this approach can also be utilized on other\nmobile systems. Building upon the original proprioceptive only DRIFT algorithm,\nwe develop a symmetry-preserving sensor fusion pipeline utilizing the invariant\nextended Kalman filter (InEKF) to integrate global position updates from GPS\ndirectly into the correction step. Crucially, we introduce a novel heading\ncorrection mechanism that leverages GPS course-over-ground information in\nconjunction with IMU orientation, overcoming the inherent unobservability of\nyaw in dead-reckoning. The system was deployed and validated on a customized\nBlue Robotics BlueBoat, but the methodological focus is on the algorithmic\napproach to fusing exteroceptive and proprioceptive sensors for drift-free\nlocalization and reliable orientation estimation. This work provides an open\nsource solution for accurate yaw observation and localization in challenging or\nGPS-degraded conditions, and lays the groundwork for future experimental and\ncomparative studies.", "AI": {"tldr": "\u6269\u5c55DRIFT\u6846\u67b6\uff0c\u7ed3\u5408GPS\u548cIMU\u6570\u636e\uff0c\u5b9e\u73b0\u7cbe\u786e\u4f4d\u59ff\u548c\u822a\u5411\u4f30\u8ba1\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u79fb\u52a8\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u822a\u5411\u4e0d\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u5728GPS\u4fe1\u53f7\u5f31\u6216\u6311\u6218\u6027\u73af\u5883\u4e2d\u7684\u7cbe\u786e\u5b9a\u4f4d\u548c\u822a\u5411\u4f30\u8ba1\u3002", "method": "\u57fa\u4e8eDRIFT\u7b97\u6cd5\uff0c\u4f7f\u7528\u4e0d\u53d8\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08InEKF\uff09\u878d\u5408GPS\u548cIMU\u6570\u636e\uff0c\u5f15\u5165\u65b0\u7684\u822a\u5411\u6821\u6b63\u673a\u5236\u3002", "result": "\u5728BlueBoat\u4e0a\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u65e0\u6f02\u79fb\u5b9a\u4f4d\u548c\u53ef\u9760\u822a\u5411\u4f30\u8ba1\u3002", "conclusion": "\u63d0\u4f9b\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u5b9e\u9a8c\u548c\u6bd4\u8f83\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2507.02245", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.02245", "abs": "https://arxiv.org/abs/2507.02245", "authors": ["Minghao Ning", "Yufeng Yang", "Keqi Shu", "Shucheng Huang", "Jiaming Zhong", "Maryam Salehi", "Mahdi Rahmani", "Yukun Lu", "Chen Sun", "Aladdin Saleh", "Ehsan Hashemi", "Amir Khajepour"], "title": "CoInfra: A Large-Scale Cooperative Infrastructure Perception System and Dataset in Adverse Weather", "comment": "This paper has been submitted to the IEEE Transactions on Robotics\n  for review", "summary": "We present CoInfra, a large-scale cooperative infrastructure perception\nsystem and dataset designed to advance robust multi-agent perception under\nreal-world and adverse weather conditions. The CoInfra system includes 14 fully\nsynchronized sensor nodes, each equipped with dual RGB cameras and a LiDAR,\ndeployed across a shared region and operating continuously to capture all\ntraffic participants in real-time. A robust, delay-aware synchronization\nprotocol and a scalable system architecture that supports real-time data\nfusion, OTA management, and remote monitoring are provided in this paper. On\nthe other hand, the dataset was collected in different weather scenarios,\nincluding sunny, rainy, freezing rain, and heavy snow and includes 195k LiDAR\nframes and 390k camera images from 8 infrastructure nodes that are globally\ntime-aligned and spatially calibrated. Furthermore, comprehensive 3D bounding\nbox annotations for five object classes (i.e., car, bus, truck, person, and\nbicycle) are provided in both global and individual node frames, along with\nhigh-definition maps for contextual understanding. Baseline experiments\ndemonstrate the trade-offs between early and late fusion strategies, the\nsignificant benefits of HD map integration are discussed. By openly releasing\nour dataset, codebase, and system documentation at\nhttps://github.com/NingMingHao/CoInfra, we aim to enable reproducible research\nand drive progress in infrastructure-supported autonomous driving, particularly\nin challenging, real-world settings.", "AI": {"tldr": "CoInfra\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u534f\u4f5c\u57fa\u7840\u8bbe\u65bd\u611f\u77e5\u7cfb\u7edf\u548c\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63d0\u5347\u591a\u667a\u80fd\u4f53\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u611f\u77e5\u80fd\u529b\u3002\u7cfb\u7edf\u5305\u542b14\u4e2a\u540c\u6b65\u4f20\u611f\u5668\u8282\u70b9\uff0c\u6570\u636e\u96c6\u6db5\u76d6\u591a\u79cd\u5929\u6c14\u573a\u666f\uff0c\u5e76\u63d0\u4f9b3D\u6807\u6ce8\u548c\u9ad8\u6e05\u5730\u56fe\u3002", "motivation": "\u63a8\u52a8\u5728\u771f\u5b9e\u4e16\u754c\u548c\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u591a\u667a\u80fd\u4f53\u611f\u77e5\u7814\u7a76\uff0c\u652f\u6301\u57fa\u7840\u8bbe\u65bd\u8f85\u52a9\u7684\u81ea\u52a8\u9a7e\u9a76\u53d1\u5c55\u3002", "method": "\u5f00\u53d1\u4e86\u540c\u6b65\u4f20\u611f\u5668\u8282\u70b9\u7cfb\u7edf\uff0c\u91c7\u96c6\u591a\u5929\u6c14\u573a\u666f\u6570\u636e\uff0c\u63d0\u4f9b\u5b9e\u65f6\u6570\u636e\u878d\u5408\u548c\u8fdc\u7a0b\u7ba1\u7406\u529f\u80fd\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b195k LiDAR\u5e27\u548c390k\u76f8\u673a\u56fe\u50cf\uff0c\u6807\u6ce8\u4e865\u7c7b\u5bf9\u8c61\uff0c\u5b9e\u9a8c\u5c55\u793a\u4e86\u65e9\u671f\u4e0e\u665a\u671f\u878d\u5408\u7b56\u7565\u7684\u6743\u8861\u3002", "conclusion": "\u901a\u8fc7\u516c\u5f00\u6570\u636e\u96c6\u548c\u7cfb\u7edf\u6587\u6863\uff0c\u4fc3\u8fdb\u53ef\u91cd\u590d\u7814\u7a76\uff0c\u63a8\u52a8\u57fa\u7840\u8bbe\u65bd\u652f\u6301\u7684\u81ea\u52a8\u9a7e\u9a76\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8fdb\u6b65\u3002"}}
{"id": "2507.02313", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.02313", "abs": "https://arxiv.org/abs/2507.02313", "authors": ["Zengjie Zhang", "Giannis Badakis", "Michalis Galanis", "Adem Bavar\u015fi", "Edwin van Hassel", "Mohsen Alirezaei", "Sofie Haesaert"], "title": "A Vehicle-in-the-Loop Simulator with AI-Powered Digital Twins for Testing Automated Driving Controllers", "comment": null, "summary": "Simulators are useful tools for testing automated driving controllers.\nVehicle-in-the-loop (ViL) tests and digital twins (DTs) are widely used\nsimulation technologies to facilitate the smooth deployment of controllers to\nphysical vehicles. However, conventional ViL tests rely on full-size vehicles,\nrequiring large space and high expenses. Also, physical-model-based DT suffers\nfrom the reality gap caused by modeling imprecision. This paper develops a\ncomprehensive and practical simulator for testing automated driving controllers\nenhanced by scaled physical cars and AI-powered DT models. The scaled cars\nallow for saving space and expenses of simulation tests. The AI-powered DT\nmodels ensure superior simulation fidelity. Moreover, the simulator integrates\nwell with off-the-shelf software and control algorithms, making it easy to\nextend. We use a filtered control benchmark with formal safety guarantees to\nshowcase the capability of the simulator in validating automated driving\ncontrollers. Experimental studies are performed to showcase the efficacy of the\nsimulator, implying its great potential in validating control solutions for\nautonomous vehicles and intelligent traffic.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7f29\u6bd4\u7269\u7406\u8f66\u548cAI\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u7684\u81ea\u52a8\u9a7e\u9a76\u63a7\u5236\u5668\u6d4b\u8bd5\u6a21\u62df\u5668\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u7a7a\u95f4\u3001\u6210\u672c\u53ca\u7cbe\u5ea6\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8f66\u8f86\u5728\u73af\u6d4b\u8bd5\u9700\u8981\u5927\u7a7a\u95f4\u548c\u9ad8\u6210\u672c\uff0c\u800c\u57fa\u4e8e\u7269\u7406\u6a21\u578b\u7684\u6570\u5b57\u5b6a\u751f\u5b58\u5728\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408\u7f29\u6bd4\u7269\u7406\u8f66\u548cAI\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u7684\u6a21\u62df\u5668\uff0c\u652f\u6301\u73b0\u6709\u8f6f\u4ef6\u548c\u63a7\u5236\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u62df\u5668\u5728\u9a8c\u8bc1\u81ea\u52a8\u9a7e\u9a76\u63a7\u5236\u5668\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u5176\u9ad8\u6548\u6027\u548c\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u6a21\u62df\u5668\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u667a\u80fd\u4ea4\u901a\u63a7\u5236\u9a8c\u8bc1\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.02328", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02328", "abs": "https://arxiv.org/abs/2507.02328", "authors": ["Gabriel O. Flores-Aquino", "Octavio Gutierrez-Frias", "Juan Irving Vasquez"], "title": "Path Planning using a One-shot-sampling Skeleton Map", "comment": null, "summary": "Path planning algorithms aim to compute a collision-free path, and many works\nfocus on finding the optimal distance path. However, for some applications, a\nmore suitable approach is to balance response time, safety of the paths, and\npath length. In this context, a skeleton map is a useful tool in graph-based\nschemes, as it provides an intrinsic representation of free configuration\nspace. However, skeletonization algorithms are very resource-intensive, being\nprimarily oriented towards image processing tasks. We propose an efficient\npath-planning methodology that finds safe paths within an acceptable processing\ntime. This methodology leverages a Deep Denoising Auto-Encoder (DDAE) based on\nU-Net architecture to compute a skeletonized version of the navigation map,\nwhich we refer to as SkelUnet. The SkelUnet network facilitates exploration of\nthe entire workspace through one-shot sampling (OSS), as opposed to the\niterative process used by exact algorithms or the probabilistic sampling\nprocess. SkelUnet is trained and tested on a dataset consisting of 12,500\nbi-dimensional dungeon maps. The motion planning methodology is evaluated in a\nsimulation environment for an Unmanned Aerial Vehicle (UAV) using 250\npreviously unseen maps, and assessed with various navigation metrics to\nquantify the navigability of the computed paths. The results demonstrate that\nusing SkelUnet to construct a roadmap offers significant advantages, such as\nconnecting all regions of free workspace, providing safer paths, and reducing\nprocessing times. These characteristics make this method particularly suitable\nfor mobile service robots in structured environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u53bb\u566a\u81ea\u7f16\u7801\u5668\uff08DDAE\uff09\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5SkelUnet\uff0c\u7528\u4e8e\u9ad8\u6548\u751f\u6210\u9aa8\u67b6\u5730\u56fe\uff0c\u5e73\u8861\u54cd\u5e94\u65f6\u95f4\u3001\u5b89\u5168\u6027\u548c\u8def\u5f84\u957f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u9aa8\u67b6\u5316\u7b97\u6cd5\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u4e3b\u8981\u7528\u4e8e\u56fe\u50cf\u5904\u7406\uff0c\u800c\u8def\u5f84\u89c4\u5212\u9700\u8981\u5e73\u8861\u591a\u76ee\u6807\uff0c\u56e0\u6b64\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u57fa\u4e8eU-Net\u67b6\u6784\u7684DDAE\uff08SkelUnet\uff09\u751f\u6210\u9aa8\u67b6\u5730\u56fe\uff0c\u5e76\u901a\u8fc7\u4e00\u6b21\u6027\u91c7\u6837\uff08OSS\uff09\u63a2\u7d22\u5de5\u4f5c\u7a7a\u95f4\u3002", "result": "\u572812,500\u5f20\u5730\u56fe\u4e0a\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c250\u5f20\u65b0\u5730\u56fe\u4e0a\u9a8c\u8bc1\uff0cSkelUnet\u663e\u8457\u63d0\u5347\u8def\u5f84\u5b89\u5168\u6027\u5e76\u51cf\u5c11\u5904\u7406\u65f6\u95f4\u3002", "conclusion": "SkelUnet\u9002\u7528\u4e8e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u79fb\u52a8\u670d\u52a1\u673a\u5668\u4eba\uff0c\u63d0\u4f9b\u9ad8\u6548\u3001\u5b89\u5168\u7684\u8def\u5f84\u89c4\u5212\u3002"}}
{"id": "2507.02400", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.02400", "abs": "https://arxiv.org/abs/2507.02400", "authors": ["Maximilian Zipfl", "Pascal Zwick", "Patrick Schulz", "Marc Rene Zofka", "Albert Schotschneider", "Helen Gremmelmaier", "Nikolai Polley", "Ferdinand M\u00fctsch", "Kevin Simon", "Fabian Gottselig", "Michael Frey", "Sergio Marschall", "Akim Stark", "Maximilian M\u00fcller", "Marek Wehmer", "Mihai Kocsis", "Dominic Waldenmayer", "Florian Schnepf", "Erik Heinrich", "Sabrina Pletz", "Matthias K\u00f6lle", "Karin Langbein-Euchner", "Alexander Viehl", "Raoul Z\u00f6llner", "J. Marius Z\u00f6llner"], "title": "DigiT4TAF -- Bridging Physical and Digital Worlds for Future Transportation Systems", "comment": "Accepted at the IEEE IAVVC 2025 Conference", "summary": "In the future, mobility will be strongly shaped by the increasing use of\ndigitalization. Not only will individual road users be highly interconnected,\nbut also the road and associated infrastructure. At that point, a Digital Twin\nbecomes particularly appealing because, unlike a basic simulation, it offers a\ncontinuous, bilateral connection linking the real and virtual environments.\nThis paper describes the digital reconstruction used to develop the Digital\nTwin of the Test Area Autonomous Driving-Baden-W\\\"urttemberg (TAF-BW), Germany.\nThe TAF-BW offers a variety of different road sections, from high-traffic urban\nintersections and tunnels to multilane motorways. The test area is equipped\nwith a comprehensive Vehicle-to-Everything (V2X) communication infrastructure\nand multiple intelligent intersections equipped with camera sensors to\nfacilitate real-time traffic flow monitoring. The generation of authentic data\nas input for the Digital Twin was achieved by extracting object lists at the\nintersections. This process was facilitated by the combined utilization of\ncamera images from the intelligent infrastructure and LiDAR sensors mounted on\na test vehicle. Using a unified interface, recordings from real-world\ndetections of traffic participants can be resimulated. Additionally, the\nsimulation framework's design and the reconstruction process is discussed. The\nresulting framework is made publicly available for download and utilization at:\nhttps://digit4taf-bw.fzi.de The demonstration uses two case studies to\nillustrate the application of the digital twin and its interfaces: the analysis\nof traffic signal systems to optimize traffic flow and the simulation of\nsecurity-related scenarios in the communications sector.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5fb7\u56fdTAF-BW\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u533a\u7684\u6570\u5b57\u5b6a\u751f\u5f00\u53d1\uff0c\u901a\u8fc7\u667a\u80fd\u57fa\u7840\u8bbe\u65bd\u548c\u6d4b\u8bd5\u8f66\u8f86\u4f20\u611f\u5668\u751f\u6210\u771f\u5b9e\u6570\u636e\uff0c\u5e76\u516c\u5f00\u4e86\u4eff\u771f\u6846\u67b6\u3002", "motivation": "\u6570\u5b57\u5316\u548c\u4e92\u8054\u4e92\u901a\u5bf9\u672a\u6765\u4ea4\u901a\u81f3\u5173\u91cd\u8981\uff0c\u6570\u5b57\u5b6a\u751f\u6280\u672f\u80fd\u5b9e\u73b0\u771f\u5b9e\u4e0e\u865a\u62df\u73af\u5883\u7684\u6301\u7eed\u53cc\u5411\u8fde\u63a5\u3002", "method": "\u5229\u7528\u667a\u80fd\u57fa\u7840\u8bbe\u65bd\u7684\u6444\u50cf\u5934\u548c\u6d4b\u8bd5\u8f66\u8f86\u7684LiDAR\u4f20\u611f\u5668\u63d0\u53d6\u5bf9\u8c61\u5217\u8868\uff0c\u751f\u6210\u771f\u5b9e\u6570\u636e\u8f93\u5165\u6570\u5b57\u5b6a\u751f\uff0c\u5e76\u901a\u8fc7\u7edf\u4e00\u63a5\u53e3\u91cd\u73b0\u5b9e\u65f6\u4ea4\u901a\u68c0\u6d4b\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u6570\u5b57\u5b6a\u751f\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u5c55\u793a\u4e86\u5176\u5728\u4ea4\u901a\u4fe1\u53f7\u4f18\u5316\u548c\u901a\u4fe1\u5b89\u5168\u6a21\u62df\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u6280\u672f\u4e3a\u4ea4\u901a\u7ba1\u7406\u548c\u5b89\u5168\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0cTAF-BW\u7684\u6846\u67b6\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.02430", "categories": ["cs.RO", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.02430", "abs": "https://arxiv.org/abs/2507.02430", "authors": ["Maryem Fadili", "Mohamed Anis Ghaoui", "Louis Lecrosnier", "Steve Pechberti", "Redouane Khemmar"], "title": "A Late Collaborative Perception Framework for 3D Multi-Object and Multi-Source Association and Fusion", "comment": null, "summary": "In autonomous driving, recent research has increasingly focused on\ncollaborative perception based on deep learning to overcome the limitations of\nindividual perception systems. Although these methods achieve high accuracy,\nthey rely on high communication bandwidth and require unrestricted access to\neach agent's object detection model architecture and parameters. These\nconstraints pose challenges real-world autonomous driving scenarios, where\ncommunication limitations and the need to safeguard proprietary models hinder\npractical implementation. To address this issue, we introduce a novel late\ncollaborative framework for 3D multi-source and multi-object fusion, which\noperates solely on shared 3D bounding box attributes-category, size, position,\nand orientation-without necessitating direct access to detection models. Our\nframework establishes a new state-of-the-art in late fusion, achieving up to\nfive times lower position error compared to existing methods. Additionally, it\nreduces scale error by a factor of 7.5 and orientation error by half, all while\nmaintaining perfect 100% precision and recall when fusing detections from\nheterogeneous perception systems. These results highlight the effectiveness of\nour approach in addressing real-world collaborative perception challenges,\nsetting a new benchmark for efficient and scalable multi-agent fusion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u540e\u671f\u534f\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e3D\u591a\u6e90\u591a\u76ee\u6807\u878d\u5408\uff0c\u4ec5\u9700\u5171\u4eab3D\u8fb9\u754c\u6846\u5c5e\u6027\uff0c\u65e0\u9700\u76f4\u63a5\u8bbf\u95ee\u68c0\u6d4b\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4f4d\u7f6e\u3001\u5c3a\u5ea6\u548c\u65b9\u5411\u8bef\u5dee\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u534f\u4f5c\u611f\u77e5\u65b9\u6cd5\u5bf9\u9ad8\u901a\u4fe1\u5e26\u5bbd\u548c\u6a21\u578b\u53c2\u6570\u8bbf\u95ee\u7684\u4f9d\u8d56\uff0c\u4ee5\u9002\u5e94\u5b9e\u9645\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u901a\u4fe1\u9650\u5236\u548c\u6a21\u578b\u4fdd\u62a4\u9700\u6c42\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u4ec5\u57fa\u4e8e\u5171\u4eab3D\u8fb9\u754c\u6846\u5c5e\u6027\uff08\u7c7b\u522b\u3001\u5927\u5c0f\u3001\u4f4d\u7f6e\u548c\u65b9\u5411\uff09\u7684\u540e\u671f\u534f\u4f5c\u6846\u67b6\uff0c\u65e0\u9700\u76f4\u63a5\u8bbf\u95ee\u68c0\u6d4b\u6a21\u578b\u3002", "result": "\u5728\u540e\u671f\u878d\u5408\u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u4f73\u6027\u80fd\uff0c\u4f4d\u7f6e\u8bef\u5dee\u964d\u4f4e5\u500d\uff0c\u5c3a\u5ea6\u8bef\u5dee\u51cf\u5c117.5\u500d\uff0c\u65b9\u5411\u8bef\u5dee\u51cf\u534a\uff0c\u540c\u65f6\u4fdd\u6301100%\u7684\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5b9e\u9645\u534f\u4f5c\u611f\u77e5\u7684\u6311\u6218\uff0c\u4e3a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u591a\u667a\u80fd\u4f53\u878d\u5408\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2507.02438", "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.02438", "abs": "https://arxiv.org/abs/2507.02438", "authors": ["Shivam Chaubey", "Francesco Verdoja", "Shankar Deka", "Ville Kyrki"], "title": "MISC: Minimal Intervention Shared Control with Guaranteed Safety under Non-Convex Constraints", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Shared control combines human intention with autonomous decision-making, from\nlow-level safety overrides to high-level task guidance, enabling systems that\nadapt to users while ensuring safety and performance. This enhances task\neffectiveness and user experience across domains such as assistive robotics,\nteleoperation, and autonomous driving. However, existing shared control\nmethods, based on e.g. Model Predictive Control, Control Barrier Functions, or\nlearning-based control, struggle with feasibility, scalability, or safety\nguarantees, particularly since the user input is unpredictable.\n  To address these challenges, we propose an assistive controller framework\nbased on Constrained Optimal Control Problem that incorporates an\noffline-computed Control Invariant Set, enabling online computation of control\nactions that ensure feasibility, strict constraint satisfaction, and minimal\noverride of user intent. Moreover, the framework can accommodate structured\nclass of non-convex constraints, which are common in real-world scenarios. We\nvalidate the approach through a large-scale user study with 66\nparticipants--one of the most extensive in shared control research--using a\ncomputer game environment to assess task load, trust, and perceived control, in\naddition to performance. The results show consistent improvements across all\nthese aspects without compromising safety and user intent.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ea6\u675f\u6700\u4f18\u63a7\u5236\u95ee\u9898\u7684\u8f85\u52a9\u63a7\u5236\u5668\u6846\u67b6\uff0c\u7ed3\u5408\u79bb\u7ebf\u8ba1\u7b97\u7684\u63a7\u5236\u4e0d\u53d8\u96c6\uff0c\u786e\u4fdd\u53ef\u884c\u6027\u3001\u4e25\u683c\u7ea6\u675f\u6ee1\u8db3\u548c\u6700\u5c0f\u5316\u7528\u6237\u610f\u56fe\u8986\u76d6\u3002", "motivation": "\u73b0\u6709\u5171\u4eab\u63a7\u5236\u65b9\u6cd5\u5728\u53ef\u884c\u6027\u3001\u53ef\u6269\u5c55\u6027\u6216\u5b89\u5168\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u7528\u6237\u8f93\u5165\u4e0d\u53ef\u9884\u6d4b\u3002", "method": "\u91c7\u7528\u7ea6\u675f\u6700\u4f18\u63a7\u5236\u95ee\u9898\u6846\u67b6\uff0c\u7ed3\u5408\u79bb\u7ebf\u8ba1\u7b97\u7684\u63a7\u5236\u4e0d\u53d8\u96c6\uff0c\u652f\u6301\u5728\u7ebf\u8ba1\u7b97\u63a7\u5236\u52a8\u4f5c\uff0c\u5e76\u80fd\u5904\u7406\u975e\u51f8\u7ea6\u675f\u3002", "result": "\u901a\u8fc7\u5927\u89c4\u6a21\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\uff0c\u5728\u4efb\u52a1\u8d1f\u8f7d\u3001\u4fe1\u4efb\u3001\u611f\u77e5\u63a7\u5236\u548c\u6027\u80fd\u65b9\u9762\u5747\u6709\u6539\u5584\uff0c\u4e14\u4e0d\u727a\u7272\u5b89\u5168\u6027\u548c\u7528\u6237\u610f\u56fe\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5171\u4eab\u63a7\u5236\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2507.02447", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.02447", "abs": "https://arxiv.org/abs/2507.02447", "authors": ["Xiang Zhou", "Xinyu Zhang", "Qingrui Zhang"], "title": "HAC-LOCO: Learning Hierarchical Active Compliance Control for Quadruped Locomotion under Continuous External Disturbances", "comment": "8 pages, 7 Figures", "summary": "Despite recent remarkable achievements in quadruped control, it remains\nchallenging to ensure robust and compliant locomotion in the presence of\nunforeseen external disturbances. Existing methods prioritize locomotion\nrobustness over compliance, often leading to stiff, high-frequency motions, and\nenergy inefficiency. This paper, therefore, presents a two-stage hierarchical\nlearning framework that can learn to take active reactions to external force\ndisturbances based on force estimation. In the first stage, a velocity-tracking\npolicy is trained alongside an auto-encoder to distill historical\nproprioceptive features. A neural network-based estimator is learned through\nsupervised learning, which estimates body velocity and external forces based on\nproprioceptive measurements. In the second stage, a compliance action module,\ninspired by impedance control, is learned based on the pre-trained encoder and\npolicy. This module is employed to actively adjust velocity commands in\nresponse to external forces based on real-time force estimates. With the\ncompliance action module, a quadruped robot can robustly handle minor\ndisturbances while appropriately yielding to significant forces, thus striking\na balance between robustness and compliance. Simulations and real-world\nexperiments have demonstrated that our method has superior performance in terms\nof robustness, energy efficiency, and safety. Experiment comparison shows that\nour method outperforms the state-of-the-art RL-based locomotion controllers.\nAblation studies are given to show the critical roles of the compliance action\nmodule.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u5206\u5c42\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u529b\u4f30\u8ba1\u5b9e\u73b0\u5bf9\u5916\u90e8\u5e72\u6270\u7684\u4e3b\u52a8\u53cd\u5e94\uff0c\u5e73\u8861\u4e86\u9c81\u68d2\u6027\u548c\u67d4\u987a\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8fc7\u4e8e\u6ce8\u91cd\u9c81\u68d2\u6027\u800c\u5ffd\u7565\u4e86\u67d4\u987a\u6027\uff0c\u5bfc\u81f4\u8fd0\u52a8\u50f5\u786c\u4e14\u80fd\u8017\u9ad8\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3\u901f\u5ea6\u8ddf\u8e2a\u7b56\u7565\u548c\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u672c\u4f53\u611f\u89c9\u7279\u5f81\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5b66\u4e60\u57fa\u4e8e\u963b\u6297\u63a7\u5236\u7684\u67d4\u987a\u52a8\u4f5c\u6a21\u5757\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u3001\u80fd\u6548\u548c\u5b89\u5168\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709RL\u63a7\u5236\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5e73\u8861\u4e86\u9c81\u68d2\u6027\u548c\u67d4\u987a\u6027\uff0c\u9002\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u5e94\u5bf9\u590d\u6742\u5e72\u6270\u3002"}}
{"id": "2507.02521", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.02521", "abs": "https://arxiv.org/abs/2507.02521", "authors": ["Ayodeji O. Abioye", "Jayati Deshmukh", "Athina Georgara", "Dominic Price", "Tuyen Nguyen", "Aleksandra Landowska", "Amel Bennaceur", "Joel E. Fischer", "Sarvapali D. Ramchurn"], "title": "Safe and Socially Aware Multi-Robot Coordination in Multi-Human Social Care Settings", "comment": "3 pages, 1 figure. Accepted for poster presentation at the UK AI\n  Research Symposium (UKAIR) 2025, themed \"A Festival of Ideas\", being held in\n  Newcastle from 8th - 9th September, 2025. https://www.ukairs.ac.uk/", "summary": "This research investigates strategies for multi-robot coordination in\nmulti-human environments. It proposes a multi-objective learning-based\ncoordination approach to addressing the problem of path planning, navigation,\ntask scheduling, task allocation, and human-robot interaction in multi-human\nmulti-robot (MHMR) settings.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u591a\u673a\u5668\u4eba\u534f\u8c03\u7b56\u7565\uff0c\u63d0\u51fa\u57fa\u4e8e\u591a\u76ee\u6807\u5b66\u4e60\u7684\u534f\u8c03\u65b9\u6cd5\uff0c\u89e3\u51b3\u591a\u4eba\u7c7b\u591a\u673a\u5668\u4eba\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u3001\u5bfc\u822a\u3001\u4efb\u52a1\u8c03\u5ea6\u548c\u5206\u914d\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u591a\u4eba\u7c7b\u591a\u673a\u5668\u4eba\u73af\u5883\u4e2d\u7684\u590d\u6742\u534f\u8c03\u95ee\u9898\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "method": "\u91c7\u7528\u591a\u76ee\u6807\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u8def\u5f84\u89c4\u5212\u3001\u5bfc\u822a\u3001\u4efb\u52a1\u8c03\u5ea6\u548c\u5206\u914d\u7b49\u6280\u672f\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u534f\u8c03\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u591a\u4eba\u7c7b\u591a\u673a\u5668\u4eba\u73af\u5883\u3002", "conclusion": "\u591a\u76ee\u6807\u5b66\u4e60\u65b9\u6cd5\u662f\u89e3\u51b3\u591a\u4eba\u7c7b\u591a\u673a\u5668\u4eba\u534f\u8c03\u95ee\u9898\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2507.02547", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.02547", "abs": "https://arxiv.org/abs/2507.02547", "authors": ["Yuhao Jiang", "Fuchen Chen", "Jamie Paik", "Daniel M. Aukes"], "title": "Vibration of Soft, Twisted Beams for Under-Actuated Quadrupedal Locomotion", "comment": "This manuscript is under revision for possible publication in the\n  IEEE/ASME Transactions on Mechatronics. Copyright may be transferred to IEEE\n  if the manuscript is accepted for publication, without further notice.\n  Supplementary videos: https://youtu.be/T3d6FT3Rx-s,\n  https://youtu.be/nPQrhKlN02E", "summary": "Under-actuated compliant robotic systems offer a promising approach to\nmitigating actuation and control challenges by harnessing pre-designed,\nembodied dynamic behaviors. This paper presents Flix-Walker, a novel,\nuntethered, centimeter-scale quadrupedal robot inspired by compliant\nunder-actuated mechanisms. Flix-Walker employs flexible, helix-shaped beams as\nlegs, which are actuated by vibrations from just two motors to achieve three\ndistinct mobility modes. We analyze the actuation parameters required to\ngenerate various locomotion modes through both simulation and prototype\nexperiments. The effects of system and environmental variations on locomotion\nperformance are examined, and we propose a generic metric for selecting control\nparameters that produce robust and functional motions. Experiments validate the\neffectiveness and robustness of these actuation parameters within a closed-loop\ncontrol framework, demonstrating reliable trajectory-tracking and\nself-navigation capabilities.", "AI": {"tldr": "Flix-Walker\u662f\u4e00\u79cd\u65b0\u578b\u65e0\u7ef3\u5398\u7c73\u7ea7\u56db\u8db3\u673a\u5668\u4eba\uff0c\u5229\u7528\u87ba\u65cb\u5f62\u67d4\u6027\u817f\u548c\u632f\u52a8\u9a71\u52a8\u5b9e\u73b0\u591a\u79cd\u8fd0\u52a8\u6a21\u5f0f\u3002", "motivation": "\u901a\u8fc7\u9884\u8bbe\u8ba1\u7684\u67d4\u6027\u52a8\u6001\u884c\u4e3a\u89e3\u51b3\u9a71\u52a8\u548c\u63a7\u5236\u6311\u6218\u3002", "method": "\u4f7f\u7528\u87ba\u65cb\u5f62\u67d4\u6027\u817f\uff0c\u4ec5\u9700\u4e24\u4e2a\u7535\u673a\u632f\u52a8\u9a71\u52a8\uff0c\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u5206\u6790\u9a71\u52a8\u53c2\u6570\u3002", "result": "\u9a8c\u8bc1\u4e86\u9a71\u52a8\u53c2\u6570\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5b9e\u73b0\u53ef\u9760\u8f68\u8ff9\u8ddf\u8e2a\u548c\u81ea\u4e3b\u5bfc\u822a\u3002", "conclusion": "Flix-Walker\u5c55\u793a\u4e86\u67d4\u6027\u9a71\u52a8\u5728\u5c0f\u578b\u673a\u5668\u4eba\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.02600", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.02600", "abs": "https://arxiv.org/abs/2507.02600", "authors": ["Qiaojun Yu", "Xibin Yuan", "Yu jiang", "Junting Chen", "Dongzhe Zheng", "Ce Hao", "Yang You", "Yixing Chen", "Yao Mu", "Liu Liu", "Cewu Lu"], "title": "ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and Manipulation of Articulated Objects", "comment": "Accepted by IROS 2025", "summary": "Articulated object manipulation remains a critical challenge in robotics due\nto the complex kinematic constraints and the limited physical reasoning of\nexisting methods. In this work, we introduce ArtGS, a novel framework that\nextends 3D Gaussian Splatting (3DGS) by integrating visual-physical modeling\nfor articulated object understanding and interaction. ArtGS begins with\nmulti-view RGB-D reconstruction, followed by reasoning with a vision-language\nmodel (VLM) to extract semantic and structural information, particularly the\narticulated bones. Through dynamic, differentiable 3DGS-based rendering, ArtGS\noptimizes the parameters of the articulated bones, ensuring physically\nconsistent motion constraints and enhancing the manipulation policy. By\nleveraging dynamic Gaussian splatting, cross-embodiment adaptability, and\nclosed-loop optimization, ArtGS establishes a new framework for efficient,\nscalable, and generalizable articulated object modeling and manipulation.\nExperiments conducted in both simulation and real-world environments\ndemonstrate that ArtGS significantly outperforms previous methods in joint\nestimation accuracy and manipulation success rates across a variety of\narticulated objects. Additional images and videos are available on the project\nwebsite: https://sites.google.com/view/artgs/home", "AI": {"tldr": "ArtGS\u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9-\u7269\u7406\u5efa\u6a21\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc73D\u9ad8\u65af\u6cfc\u6e85\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u5173\u8282\u7269\u4f53\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5173\u8282\u4f30\u8ba1\u548c\u64cd\u4f5c\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5173\u8282\u7269\u4f53\u64cd\u4f5c\u4e2d\u56e0\u590d\u6742\u8fd0\u52a8\u7ea6\u675f\u548c\u6709\u9650\u7269\u7406\u63a8\u7406\u80fd\u529b\u800c\u9762\u4e34\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u591a\u89c6\u89d2RGB-D\u91cd\u5efa\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u8bed\u4e49\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u53ef\u5fae3D\u9ad8\u65af\u6cfc\u6e85\u4f18\u5316\u5173\u8282\u53c2\u6570\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\uff0cArtGS\u5728\u5173\u8282\u4f30\u8ba1\u7cbe\u5ea6\u548c\u64cd\u4f5c\u6210\u529f\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ArtGS\u4e3a\u5173\u8282\u7269\u4f53\u5efa\u6a21\u548c\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u901a\u7528\u7684\u65b0\u6846\u67b6\u3002"}}
{"id": "2507.02672", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02672", "abs": "https://arxiv.org/abs/2507.02672", "authors": ["Qingyu Fan", "Yinghao Cai", "Chao Li", "Chunting Jiao", "Xudong Zheng", "Tao Lu", "Bin Liang", "Shuo Wang"], "title": "MISCGrasp: Leveraging Multiple Integrated Scales and Contrastive Learning for Enhanced Volumetric Grasping", "comment": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS), 2025", "summary": "Robotic grasping faces challenges in adapting to objects with varying shapes\nand sizes. In this paper, we introduce MISCGrasp, a volumetric grasping method\nthat integrates multi-scale feature extraction with contrastive feature\nenhancement for self-adaptive grasping. We propose a query-based interaction\nbetween high-level and low-level features through the Insight Transformer,\nwhile the Empower Transformer selectively attends to the highest-level\nfeatures, which synergistically strikes a balance between focusing on fine\ngeometric details and overall geometric structures. Furthermore, MISCGrasp\nutilizes multi-scale contrastive learning to exploit similarities among\npositive grasp samples, ensuring consistency across multi-scale features.\nExtensive experiments in both simulated and real-world environments demonstrate\nthat MISCGrasp outperforms baseline and variant methods in tabletop\ndecluttering tasks. More details are available at https://miscgrasp.github.io/.", "AI": {"tldr": "MISCGrasp\u662f\u4e00\u79cd\u4f53\u79ef\u6293\u53d6\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u5bf9\u6bd4\u7279\u5f81\u589e\u5f3a\u5b9e\u73b0\u81ea\u9002\u5e94\u6293\u53d6\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u673a\u5668\u4eba\u6293\u53d6\u5728\u9002\u5e94\u4e0d\u540c\u5f62\u72b6\u548c\u5927\u5c0f\u7684\u7269\u4f53\u65f6\u9762\u4e34\u6311\u6218\u3002", "method": "\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u4e0e\u5bf9\u6bd4\u7279\u5f81\u589e\u5f3a\uff0c\u901a\u8fc7Insight Transformer\u548cEmpower Transformer\u5b9e\u73b0\u7279\u5f81\u4ea4\u4e92\u4e0e\u9009\u62e9\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\uff0cMISCGrasp\u5728\u684c\u9762\u6574\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MISCGrasp\u5728\u591a\u5c3a\u5ea6\u7279\u5f81\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u534f\u540c\u4f5c\u7528\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u81ea\u9002\u5e94\u6293\u53d6\u3002"}}
{"id": "2507.02700", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.02700", "abs": "https://arxiv.org/abs/2507.02700", "authors": ["M\u00e1t\u00e9 B. Vizi", "D\u00e9nes T\u00e1k\u00e1cs", "G\u00e1bor St\u00e9p\u00e1n", "G\u00e1bor Orosz"], "title": "Integrating path-planning and control for robotic unicycles", "comment": null, "summary": "This article focuses on integrating path-planning and control with\nspecializing on the unique needs of robotic unicycles. A unicycle design is\npresented which is capable of accelerating/breaking and carrying out a variety\nof maneuvers. The proposed path-planning method segments the path into straight\nand curved path sections dedicated for accelerating/breaking and turning\nmaneuvers, respectively. The curvature profiles of the curved sections are\noptimized while considering the control performance and the slipping limits of\nthe wheel. The performance of the proposed integrated approach is demonstrated\nvia numerical simulations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u673a\u5668\u4eba\u72ec\u8f6e\u8f66\u7684\u8def\u5f84\u89c4\u5212\u4e0e\u63a7\u5236\u96c6\u6210\u65b9\u6cd5\uff0c\u4f18\u5316\u4e86\u8def\u5f84\u5206\u6bb5\u548c\u66f2\u7387\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u673a\u5668\u4eba\u72ec\u8f6e\u8f66\u7684\u72ec\u7279\u9700\u6c42\uff0c\u7814\u7a76\u5982\u4f55\u96c6\u6210\u8def\u5f84\u89c4\u5212\u4e0e\u63a7\u5236\uff0c\u4ee5\u5b9e\u73b0\u52a0\u901f\u3001\u5236\u52a8\u548c\u591a\u79cd\u673a\u52a8\u64cd\u4f5c\u3002", "method": "\u5c06\u8def\u5f84\u5206\u6bb5\u4e3a\u76f4\u7ebf\u548c\u66f2\u7ebf\u90e8\u5206\uff0c\u5206\u522b\u7528\u4e8e\u52a0\u901f/\u5236\u52a8\u548c\u8f6c\u5411\u64cd\u4f5c\uff0c\u5e76\u4f18\u5316\u66f2\u7ebf\u90e8\u5206\u7684\u66f2\u7387\uff0c\u540c\u65f6\u8003\u8651\u63a7\u5236\u6027\u80fd\u548c\u8f66\u8f6e\u6253\u6ed1\u9650\u5236\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u96c6\u6210\u4e86\u8def\u5f84\u89c4\u5212\u4e0e\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u72ec\u8f6e\u8f66\u7684\u673a\u52a8\u9700\u6c42\u3002"}}
{"id": "2507.02708", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.02708", "abs": "https://arxiv.org/abs/2507.02708", "authors": ["Ananya Rao", "Alyssa Hargis", "David Wettergreen", "Howie Choset"], "title": "Optimizing Start Locations in Ergodic Search for Disaster Response", "comment": null, "summary": "In disaster response scenarios, deploying robotic teams effectively is\ncrucial for improving situational awareness and enhancing search and rescue\noperations. The use of robots in search and rescue has been studied but the\nquestion of where to start robot deployments has not been addressed. This work\naddresses the problem of optimally selecting starting locations for robots with\nheterogeneous capabilities by formulating a joint optimization problem. To\ndetermine start locations, this work adds a constraint to the ergodic\noptimization framework whose minimum assigns robots to start locations. This\nbecomes a little more challenging when the robots are heterogeneous (equipped\nwith different sensing and motion modalities) because not all robots start at\nthe same location, and a more complex adaptation of the aforementioned\nconstraint is applied. Our method assumes access to potential starting\nlocations, which can be obtained from expert knowledge or aerial imagery. We\nexperimentally evaluate the efficacy of our joint optimization approach by\ncomparing it to baseline methods that use fixed starting locations for all\nrobots. Our experimental results show significant gains in coverage\nperformance, with average improvements of 35.98% on synthetic data and 31.91%\non real-world data for homogeneous and heterogeneous teams, in terms of the\nergodic metric.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u673a\u5668\u4eba\u56e2\u961f\u5728\u707e\u96be\u54cd\u5e94\u4e2d\u8d77\u59cb\u90e8\u7f72\u4f4d\u7f6e\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u90e8\u7f72\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5728\u707e\u96be\u54cd\u5e94\u4e2d\uff0c\u673a\u5668\u4eba\u56e2\u961f\u7684\u90e8\u7f72\u5bf9\u63d0\u9ad8\u60c5\u5883\u611f\u77e5\u548c\u641c\u6551\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5982\u4f55\u9009\u62e9\u8d77\u59cb\u4f4d\u7f6e\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5f15\u5165\u7ea6\u675f\u6761\u4ef6\u786e\u5b9a\u5f02\u6784\u673a\u5668\u4eba\u7684\u8d77\u59cb\u4f4d\u7f6e\uff0c\u5e76\u5229\u7528\u4e13\u5bb6\u77e5\u8bc6\u6216\u822a\u62cd\u56fe\u50cf\u83b7\u53d6\u6f5c\u5728\u8d77\u59cb\u70b9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u5206\u522b\u5b9e\u73b0\u4e8635.98%\u548c31.91%\u7684\u8986\u76d6\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u56e2\u961f\u5728\u707e\u96be\u54cd\u5e94\u4e2d\u7684\u8986\u76d6\u6027\u80fd\uff0c\u4e3a\u5f02\u6784\u673a\u5668\u4eba\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.02761", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.02761", "abs": "https://arxiv.org/abs/2507.02761", "authors": ["Long Xu", "Choilam Wong", "Mengke Zhang", "Junxiao Lin", "Fei Gao"], "title": "Trajectory Optimization for Differential Drive Mobile Manipulators via Topological Paths Search and Arc Length-Yaw Parameterization", "comment": "Technical Report", "summary": "We present an efficient hierarchical motion planning pipeline for\ndifferential drive mobile manipulators. Our approach first searches for\nmultiple collisionfree and topologically distinct paths for the mobile base to\nextract the space in which optimal solutions may exist. Further sampling and\noptimization are then conducted in parallel to explore feasible whole-body\ntrajectories. For trajectory optimization, we employ polynomial trajectories\nand arc length-yaw parameterization, enabling efficient handling of the\nnonholonomic dynamics while ensuring optimality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5206\u5c42\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u5dee\u901f\u9a71\u52a8\u79fb\u52a8\u673a\u68b0\u81c2\uff0c\u901a\u8fc7\u591a\u8def\u5f84\u641c\u7d22\u548c\u5e76\u884c\u4f18\u5316\u5b9e\u73b0\u6700\u4f18\u8f68\u8ff9\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u5dee\u901f\u9a71\u52a8\u79fb\u52a8\u673a\u68b0\u81c2\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u6700\u4f18\u6027\u3002", "method": "\u91c7\u7528\u5206\u5c42\u89c4\u5212\uff0c\u5148\u641c\u7d22\u591a\u8def\u5f84\uff0c\u518d\u5e76\u884c\u91c7\u6837\u548c\u4f18\u5316\uff0c\u4f7f\u7528\u591a\u9879\u5f0f\u8f68\u8ff9\u548c\u5f27\u957f-\u504f\u822a\u53c2\u6570\u5316\u5904\u7406\u975e\u5b8c\u6574\u52a8\u529b\u5b66\u3002", "result": "\u80fd\u591f\u9ad8\u6548\u751f\u6210\u65e0\u78b0\u649e\u4e14\u6700\u4f18\u7684\u5168\u8eab\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u6700\u4f18\u6027\uff0c\u9002\u7528\u4e8e\u5dee\u901f\u9a71\u52a8\u79fb\u52a8\u673a\u68b0\u81c2\u7684\u8fd0\u52a8\u89c4\u5212\u3002"}}
{"id": "2507.02864", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02864", "abs": "https://arxiv.org/abs/2507.02864", "authors": ["Renhao Wang", "Haoran Geng", "Tingle Li", "Feishi Wang", "Gopala Anumanchipalli", "Philipp Wu", "Trevor Darrell", "Boyi Li", "Pieter Abbeel", "Jitendra Malik", "Alexei A. Efros"], "title": "MultiGen: Using Multimodal Generation in Simulation to Learn Multimodal Policies in Real", "comment": null, "summary": "Robots must integrate multiple sensory modalities to act effectively in the\nreal world. Yet, learning such multimodal policies at scale remains\nchallenging. Simulation offers a viable solution, but while vision has\nbenefited from high-fidelity simulators, other modalities (e.g. sound) can be\nnotoriously difficult to simulate. As a result, sim-to-real transfer has\nsucceeded primarily in vision-based tasks, with multimodal transfer still\nlargely unrealized. In this work, we tackle these challenges by introducing\nMultiGen, a framework that integrates large-scale generative models into\ntraditional physics simulators, enabling multisensory simulation. We showcase\nour framework on the dynamic task of robot pouring, which inherently relies on\nmultimodal feedback. By synthesizing realistic audio conditioned on simulation\nvideo, our method enables training on rich audiovisual trajectories -- without\nany real robot data. We demonstrate effective zero-shot transfer to real-world\npouring with novel containers and liquids, highlighting the potential of\ngenerative modeling to both simulate hard-to-model modalities and close the\nmultimodal sim-to-real gap.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMultiGen\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u751f\u6210\u6a21\u578b\u4e0e\u4f20\u7edf\u7269\u7406\u6a21\u62df\u5668\u7ed3\u5408\uff0c\u5b9e\u73b0\u591a\u611f\u5b98\u6a21\u62df\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u7b56\u7565\u5b66\u4e60\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u673a\u5668\u4eba\u9700\u8981\u6574\u5408\u591a\u611f\u5b98\u4fe1\u606f\uff0c\u4f46\u591a\u6a21\u6001\u7b56\u7565\u7684\u5927\u89c4\u6a21\u5b66\u4e60\u4ecd\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u58f0\u97f3\u7b49\u96be\u4ee5\u6a21\u62df\u7684\u6a21\u6001\u3002", "method": "\u5f15\u5165MultiGen\u6846\u67b6\uff0c\u5229\u7528\u751f\u6210\u6a21\u578b\u5408\u6210\u57fa\u4e8e\u6a21\u62df\u89c6\u9891\u7684\u903c\u771f\u97f3\u9891\uff0c\u5b9e\u73b0\u65e0\u9700\u771f\u5b9e\u6570\u636e\u7684\u591a\u6a21\u6001\u8bad\u7ec3\u3002", "result": "\u5728\u673a\u5668\u4eba\u5012\u6c34\u4efb\u52a1\u4e2d\uff0c\u5c55\u793a\u4e86\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u7684\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u65b0\u5bb9\u5668\u548c\u6db2\u4f53\u3002", "conclusion": "\u751f\u6210\u6a21\u578b\u80fd\u6709\u6548\u6a21\u62df\u96be\u4ee5\u5efa\u6a21\u7684\u6a21\u6001\uff0c\u7f29\u5c0f\u591a\u6a21\u6001\u6a21\u62df\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002"}}
