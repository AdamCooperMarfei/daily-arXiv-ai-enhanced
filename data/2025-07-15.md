<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 45]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [OTAS: Open-vocabulary Token Alignment for Outdoor Segmentation](https://arxiv.org/abs/2507.08851)
*Simon Schwaiger,Stefan Thalhammer,Wilfried Wöber,Gerald Steinbauer-Wagner*

Main category: cs.RO

TL;DR: OTAS是一种开放词汇标记对齐方法，用于户外分割，通过从预训练视觉模型的输出标记中提取语义结构，解决了开放词汇分割模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 理解开放世界语义对机器人规划和控制在非结构化户外环境中至关重要，但现有方法依赖对象中心分割先验，在户外环境中常因语义模糊和边界不清而失效。

Method: OTAS通过聚类单视图和多视图中的语义相似结构，并将其与语言对齐，构建几何一致的特征场，支持开放词汇分割查询。该方法无需场景特定微调，零样本运行，速度达17 fps。

Result: OTAS在Off-Road Freespace Detection数据集上略优于微调和开放词汇2D分割方法，在TartanAir的3D分割中比开放词汇映射方法提升151% IoU。

Conclusion: OTAS适用于机器人应用，代码和ROS节点将在论文接受后公开。

Abstract: Understanding open-world semantics is critical for robotic planning and
control, particularly in unstructured outdoor environments. Current
vision-language mapping approaches rely on object-centric segmentation priors,
which often fail outdoors due to semantic ambiguities and indistinct semantic
class boundaries. We propose OTAS - an Open-vocabulary Token Alignment method
for Outdoor Segmentation. OTAS overcomes the limitations of open-vocabulary
segmentation models by extracting semantic structure directly from the output
tokens of pretrained vision models. By clustering semantically similar
structures across single and multiple views and grounding them in language,
OTAS reconstructs a geometrically consistent feature field that supports
open-vocabulary segmentation queries. Our method operates zero-shot, without
scene-specific fine-tuning, and runs at up to ~17 fps. OTAS provides a minor
IoU improvement over fine-tuned and open-vocabulary 2D segmentation methods on
the Off-Road Freespace Detection dataset. Our model achieves up to a 151% IoU
improvement over open-vocabulary mapping methods in 3D segmentation on
TartanAir. Real-world reconstructions demonstrate OTAS' applicability to
robotic applications. The code and ROS node will be made publicly available
upon paper acceptance.

</details>


### [2] [AirScape: An Aerial Generative World Model with Motion Controllability](https://arxiv.org/abs/2507.08885)
*Baining Zhao,Rongze Tang,Mingyuan Jia,Ziyou Wang,Fanghang Man,Xin Zhang,Yu Shang,Weichen Zhang,Chen Gao,Wei Wu,Xin Wang,Xinlei Chen,Yong Li*

Main category: cs.RO

TL;DR: AirScape是首个为六自由度空中代理设计的世界模型，通过视觉输入和运动意图预测未来观察序列。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在三维空间中预测自身运动意图结果的基本问题，探索更通用的空间想象能力。

Method: 构建包含11k视频-意图对的数据集，采用两阶段训练计划训练基础模型。

Result: 开发出可控且符合物理时空约束的世界模型。

Conclusion: AirScape为空中代理提供了一种有效的空间想象和预测工具。

Abstract: How to enable robots to predict the outcomes of their own motion intentions
in three-dimensional space has been a fundamental problem in embodied
intelligence. To explore more general spatial imagination capabilities, here we
present AirScape, the first world model designed for six-degree-of-freedom
aerial agents. AirScape predicts future observation sequences based on current
visual inputs and motion intentions. Specifically, we construct an dataset for
aerial world model training and testing, which consists of 11k video-intention
pairs. This dataset includes first-person-view videos capturing diverse drone
actions across a wide range of scenarios, with over 1,000 hours spent
annotating the corresponding motion intentions. Then we develop a two-phase
training schedule to train a foundation model -- initially devoid of embodied
spatial knowledge -- into a world model that is controllable by motion
intentions and adheres to physical spatio-temporal constraints.

</details>


### [3] [End-to-End Generation of City-Scale Vectorized Maps by Crowdsourced Vehicles](https://arxiv.org/abs/2507.08901)
*Zebang Feng,Miao Fan,Bao Liu,Shengtong Xu,Haoyi Xiong*

Main category: cs.RO

TL;DR: EGC-VMAP是一个端到端框架，通过众包车辆数据生成高精度城市级矢量化地图，显著提升地图准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统LiDAR制图成本高且慢，单车感知方法在恶劣条件下缺乏准确性和鲁棒性。

Method: 采用Trip-Aware Transformer架构，融合多车、多时态地图元素，结合分层匹配和多目标损失。

Result: 在大规模多城市数据集上验证，性能优于单车基线，手动标注成本降低90%。

Conclusion: EGC-VMAP为城市级地图提供了一种可扩展、经济高效的解决方案。

Abstract: High-precision vectorized maps are indispensable for autonomous driving, yet
traditional LiDAR-based creation is costly and slow, while single-vehicle
perception methods lack accuracy and robustness, particularly in adverse
conditions. This paper introduces EGC-VMAP, an end-to-end framework that
overcomes these limitations by generating accurate, city-scale vectorized maps
through the aggregation of data from crowdsourced vehicles. Unlike prior
approaches, EGC-VMAP directly fuses multi-vehicle, multi-temporal map elements
perceived onboard vehicles using a novel Trip-Aware Transformer architecture
within a unified learning process. Combined with hierarchical matching for
efficient training and a multi-objective loss, our method significantly
enhances map accuracy and structural robustness compared to single-vehicle
baselines. Validated on a large-scale, multi-city real-world dataset, EGC-VMAP
demonstrates superior performance, enabling a scalable, cost-effective solution
for city-wide mapping with a reported 90\% reduction in manual annotation
costs.

</details>


### [4] [Multimodal HD Mapping for Intersections by Intelligent Roadside Units](https://arxiv.org/abs/2507.08903)
*Zhongzhang Chen,Miao Fan,Shengtong Xu,Mengmeng Yang,Kun Jiang,Xiangzeng Liu,Haoyi Xiong*

Main category: cs.RO

TL;DR: 论文提出了一种基于摄像头-LiDAR融合的新框架，利用智能路边单元（IRUs）生成高精度语义地图，并发布了RS-seq数据集。该方法在语义分割任务中显著优于单模态方法。


<details>
  <summary>Details</summary>
Motivation: 传统车辆方法在复杂交叉路口的高清语义地图生成中存在遮挡和视角限制问题，需要多模态数据融合解决方案。

Method: 采用两阶段融合框架，结合摄像头的高分辨率纹理和LiDAR的精确几何数据，通过模态特定特征提取和跨模态语义整合实现。

Result: 多模态方法在RS-seq数据集上的mIoU比图像单模态高4%，比点云单模态高18%。

Conclusion: 研究为基于IRU的高清语义地图生成提供了基准方法，并为基础设施辅助自动驾驶系统的未来研究提供了有价值的数据集。

Abstract: High-definition (HD) semantic mapping of complex intersections poses
significant challenges for traditional vehicle-based approaches due to
occlusions and limited perspectives. This paper introduces a novel camera-LiDAR
fusion framework that leverages elevated intelligent roadside units (IRUs).
Additionally, we present RS-seq, a comprehensive dataset developed through the
systematic enhancement and annotation of the V2X-Seq dataset. RS-seq includes
precisely labelled camera imagery and LiDAR point clouds collected from
roadside installations, along with vectorized maps for seven intersections
annotated with detailed features such as lane dividers, pedestrian crossings,
and stop lines. This dataset facilitates the systematic investigation of
cross-modal complementarity for HD map generation using IRU data. The proposed
fusion framework employs a two-stage process that integrates modality-specific
feature extraction and cross-modal semantic integration, capitalizing on camera
high-resolution texture and precise geometric data from LiDAR. Quantitative
evaluations using the RS-seq dataset demonstrate that our multimodal approach
consistently surpasses unimodal methods. Specifically, compared to unimodal
baselines evaluated on the RS-seq dataset, the multimodal approach improves the
mean Intersection-over-Union (mIoU) for semantic segmentation by 4\% over the
image-only results and 18\% over the point cloud-only results. This study
establishes a baseline methodology for IRU-based HD semantic mapping and
provides a valuable dataset for future research in infrastructure-assisted
autonomous driving systems.

</details>


### [5] [Towards Human-level Dexterity via Robot Learning](https://arxiv.org/abs/2507.09117)
*Gagan Khandate*

Main category: cs.RO

TL;DR: 论文探讨了如何通过强化学习和模仿学习克服多指机器人操作中的基本限制，以实现更高水平的灵巧性。


<details>
  <summary>Details</summary>
Motivation: 人类灵巧智能的复杂性及其在机器人领域的实现是一个长期目标，但现有计算方法存在根本性限制。

Method: 采用结构化探索的强化学习方法，并结合基于采样的规划和视觉触觉人类演示的模仿学习技术。

Result: 开发了一种高效的强化学习框架，能够有效克服随机探索的局限性，并引入新的模仿学习范式。

Conclusion: 通过直接解决计算传感器运动学习的根本限制，论文为多指机器人操作提供了有效的学习方法和框架。

Abstract: Dexterous intelligence -- the ability to perform complex interactions with
multi-fingered hands -- is a pinnacle of human physical intelligence and
emergent higher-order cognitive skills. However, contrary to Moravec's paradox,
dexterous intelligence in humans appears simple only superficially. Many
million years were spent co-evolving the human brain and hands including rich
tactile sensing. Achieving human-level dexterity with robotic hands has long
been a fundamental goal in robotics and represents a critical milestone toward
general embodied intelligence. In this pursuit, computational sensorimotor
learning has made significant progress, enabling feats such as arbitrary
in-hand object reorientation. However, we observe that achieving higher levels
of dexterity requires overcoming very fundamental limitations of computational
sensorimotor learning.
  I develop robot learning methods for highly dexterous multi-fingered
manipulation by directly addressing these limitations at their root cause.
Chiefly, through key studies, this disseration progressively builds an
effective framework for reinforcement learning of dexterous multi-fingered
manipulation skills. These methods adopt structured exploration, effectively
overcoming the limitations of random exploration in reinforcement learning. The
insights gained culminate in a highly effective reinforcement learning that
incorporates sampling-based planning for direct exploration. Additionally, this
thesis explores a new paradigm of using visuo-tactile human demonstrations for
dexterity, introducing corresponding imitation learning techniques.

</details>


### [6] [Online 3D Bin Packing with Fast Stability Validation and Stable Rearrangement Planning](https://arxiv.org/abs/2507.09123)
*Ziyan Gao,Lijun Wang,Yuntao Kong,Nak Young Chong*

Main category: cs.RO

TL;DR: 提出了一种结合包装策略、结构稳定性验证和启发式规划的新框架，解决了在线装箱问题中结构稳定性和安全重新配置的不足。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法虽能提高容积利用率，但无法确保箱体结构稳定性，且缺乏安全重新配置机制。

Method: 引入负载可承载凸多边形（LBCP）进行稳定性验证，并提出稳定重排规划（SRP）模块以安全重排物品。

Result: 实验表明LBCP验证高效且通用，SRP在节省重排成本方面表现优越。

Conclusion: 该方法为实际工业和物流应用提供了鲁棒且实用的自动化装箱解决方案。

Abstract: The Online Bin Packing Problem (OBPP) is a sequential decision-making task in
which each item must be placed immediately upon arrival, with no knowledge of
future arrivals. Although recent deep-reinforcement-learning methods achieve
superior volume utilization compared with classical heuristics, the learned
policies cannot ensure the structural stability of the bin and lack mechanisms
for safely reconfiguring the bin when a new item cannot be placed directly. In
this work, we propose a novel framework that integrates packing policy with
structural stability validation and heuristic planning to overcome these
limitations. Specifically, we introduce the concept of Load Bearable Convex
Polygon (LBCP), which provides a computationally efficient way to identify
stable loading positions that guarantee no bin collapse. Additionally, we
present Stable Rearrangement Planning (SRP), a module that rearranges existing
items to accommodate new ones while maintaining overall stability. Extensive
experiments on standard OBPP benchmarks demonstrate the efficiency and
generalizability of our LBCP-based stability validation, as well as the
superiority of SRP in finding the effort-saving rearrangement plans. Our method
offers a robust and practical solution for automated packing in real-world
industrial and logistics applications.

</details>


### [7] [Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization](https://arxiv.org/abs/2507.09160)
*Jialei Huang,Shuo Wang,Fanqi Lin,Yihang Hu,Chuan Wen,Yang Gao*

Main category: cs.RO

TL;DR: Tactile-VLA框架融合视觉、语言、动作和触觉感知，通过混合力位控制器和推理模块实现精确物理交互，实验证明其在触觉感知任务中的有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 提升视觉-语言-动作（VLA）模型在物理交互中的能力，特别是在需要精细力控制的接触丰富场景中。

Method: 提出Tactile-VLA框架，结合混合力位控制器和触觉反馈推理模块，通过少量演示激活VLM的先验知识。

Result: 实验验证了框架在触觉感知指令跟随、触觉常识利用和自适应触觉推理三方面的有效性，并实现零样本泛化。

Conclusion: Tactile-VLA通过触觉反馈和VLM先验知识的结合，显著提升了机器人在接触丰富任务中的表现。

Abstract: Vision-Language-Action (VLA) models have shown remarkable achievements,
driven by the rich implicit knowledge of their vision-language components.
However, achieving generalist robotic agents demands precise grounding into
physical interactions, especially in contact-rich scenarios where fine-grained
force control is essential. We advance VLAs' implicit knowledge beyond
identifying what to do, towards guiding how to physically interact with real
world. This paper introduces Tactile-VLA, a novel framework that deeply fuses
vision, language, action, and tactile sensing. This framework incorporates a
hybrid position-force controller to translate the model's intentions into
precise physical actions and a reasoning module that allows the robot to adapt
its strategy based on tactile feedback. Experiments demonstrate Tactile-VLA's
effectiveness and generalizability in three key aspects: (1) enabling
tactile-aware instruction following, (2) utilizing tactile-relevant
commonsense, and (3) facilitating adaptive tactile-involved reasoning. A key
finding is that the VLM's prior knowledge already contains semantic
understanding of physical interaction; by connecting it to the robot's tactile
sensors with only a few demonstrations, we can activate this prior knowledge to
achieve zero-shot generalization in contact-rich tasks.

</details>


### [8] [PRAG: Procedural Action Generator](https://arxiv.org/abs/2507.09167)
*Michal Vavrecka,Radoslav Skoviera,Gabriela Sejnova,Karla Stepanova*

Main category: cs.RO

TL;DR: 提出了一种用于机器人多步接触丰富操作任务程序化构建的新方法，通过符号和物理验证生成可解任务。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作任务中多步接触丰富任务的生成问题，确保任务逻辑和物理可行性。

Method: 输入原子动作、对象和空间谓词，通过符号和物理验证生成可解任务序列。

Result: 生成了数百万个独特的可解多步任务，适用于机器人训练框架。

Conclusion: 该方法能高效生成可解任务，支持机器人训练和任务语义相似性评估。

Abstract: We present a novel approach for the procedural construction of multi-step
contact-rich manipulation tasks in robotics. Our generator takes as input
user-defined sets of atomic actions, objects, and spatial predicates and
outputs solvable tasks of a given length for the selected robotic environment.
The generator produces solvable tasks by constraining all possible
(nonsolvable) combinations by symbolic and physical validation. The symbolic
validation checks each generated sequence for logical and operational
consistency, and also the suitability of object-predicate relations. Physical
validation checks whether tasks can be solved in the selected robotic
environment. Only the tasks that passed both validators are retained. The
output from the generator can be directly interfaced with any existing
framework for training robotic manipulation tasks, or it can be stored as a
dataset of curated robotic tasks with detailed information about each task.
This is beneficial for RL training as there are dense reward functions and
initial and goal states paired with each subgoal. It allows the user to measure
the semantic similarity of all generated tasks. We tested our generator on
sequences of up to 15 actions resulting in millions of unique solvable
multi-step tasks.

</details>


### [9] [DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA](https://arxiv.org/abs/2507.09176)
*Han Ye,Yuqiang Jin,Jinyuan Liu,Tao Li,Wen-An Zhang,Minglei Fu*

Main category: cs.RO

TL;DR: 提出了一种无需目标的多LiDAR外参标定框架，通过LiDAR束调整和自适应加权机制实现高精度标定。


<details>
  <summary>Details</summary>
Motivation: 多LiDAR系统的外参标定对3D地图重建至关重要，传统方法依赖重叠视场或精确初始参数，限制了实用性。

Method: 结合LiDAR束调整（LBA）和鲁棒迭代优化，构建参考点云图并通过联合LBA优化外参。

Result: 在CARLA仿真和真实场景中，平均平移误差5mm，旋转误差0.2°，初始误差容忍度达0.4m/30°。

Conclusion: 该方法在精度和鲁棒性上优于现有技术，无需专用基础设施或手动调参，代码开源。

Abstract: Accurate extrinsic calibration of multiple LiDARs is crucial for improving
the foundational performance of three-dimensional (3D) map reconstruction
systems. This paper presents a novel targetless extrinsic calibration framework
for multi-LiDAR systems that does not rely on overlapping fields of view or
precise initial parameter estimates. Unlike conventional calibration methods
that require manual annotations or specific reference patterns, our approach
introduces a unified optimization framework by integrating LiDAR bundle
adjustment (LBA) optimization with robust iterative refinement. The proposed
method constructs an accurate reference point cloud map via continuous scanning
from the target LiDAR and sliding-window LiDAR bundle adjustment, while
formulating extrinsic calibration as a joint LBA optimization problem. This
method effectively mitigates cumulative mapping errors and achieves
outlier-resistant parameter estimation through an adaptive weighting mechanism.
Extensive evaluations in both the CARLA simulation environment and real-world
scenarios demonstrate that our method outperforms state-of-the-art calibration
techniques in both accuracy and robustness. Experimental results show that for
non-overlapping sensor configurations, our framework achieves an average
translational error of 5 mm and a rotational error of 0.2{\deg}, with an
initial error tolerance of up to 0.4 m/30{\deg}. Moreover, the calibration
process operates without specialized infrastructure or manual parameter tuning.
The code is open source and available on GitHub
(\underline{https://github.com/Silentbarber/DLBAcalib})

</details>


### [10] [Informed Hybrid Zonotope-based Motion Planning Algorithm](https://arxiv.org/abs/2507.09309)
*Peng Xie,Johannes Betz,Amr Alanwar*

Main category: cs.RO

TL;DR: HZ-MP是一种基于混合Zonotope的运动规划器，通过分解无障碍空间和低维面采样，解决了非凸自由空间中的路径规划问题，避免了现有方法的过度采样问题。


<details>
  <summary>Details</summary>
Motivation: 非凸自由空间中的路径规划问题由于NP难的特性而极具挑战性，现有方法在狭窄间隙或目标受限场景中表现不佳。

Method: HZ-MP采用混合Zonotope分解无障碍空间，结合低维面采样和椭球体启发式引导，实现高效探索。

Result: HZ-MP在有限时间内收敛到接近最优的轨迹，适用于高维复杂场景，并具有概率完备性和渐进最优性。

Conclusion: HZ-MP为复杂环境中的路径规划提供了一种高效且可靠的方法。

Abstract: Optimal path planning in nonconvex free spaces is notoriously challenging, as
formulating such problems as mixed-integer linear programs (MILPs) is NP-hard.
We propose HZ-MP, an informed Hybrid Zonotope-based Motion Planner, as an
alternative approach that decomposes the obstacle-free space and performs
low-dimensional face sampling guided by an ellipsotope heuristic, enabling
focused exploration along promising transit regions. This structured
exploration eliminates the excessive, unreachable sampling that degrades
existing informed planners such as AIT* and EIT* in narrow gaps or boxed-goal
scenarios. We prove that HZ-MP is probabilistically complete and asymptotically
optimal. It converges to near-optimal trajectories in finite time and scales to
high-dimensional cluttered scenes.

</details>


### [11] [Unified Linear Parametric Map Modeling and Perception-aware Trajectory Planning for Mobile Robotics](https://arxiv.org/abs/2507.09340)
*Hongyu Nie,Xingyu Li,Xu Liu,Zhaotong Tan,Sen Mei,Wenbo Su*

Main category: cs.RO

TL;DR: 论文提出了一种名为RMRP的轻量级线性参数化地图构建方法，结合RPATR框架，解决了无人机和地面机器人在复杂环境中的导航问题。


<details>
  <summary>Details</summary>
Motivation: 大规模复杂环境中，移动机器人的自主导航面临计算负担重、传感器遮挡和地形不规则等挑战，缺乏感知感知策略。

Method: 通过高维空间映射和稀疏随机投影降维构建轻量级地图，提出RPATR框架，结合ESDF和梯度优化实现安全高效导航。

Result: 在多种场景中验证，RMRP和RPATR在时间、内存和精度上表现优越，支持高速无人机和地面机器人的高效安全导航。

Conclusion: RMRP和RPATR为复杂环境中的机器人导航提供了理论保障和实用解决方案，代码将开源以促进合作。

Abstract: Autonomous navigation in mobile robots, reliant on perception and planning,
faces major hurdles in large-scale, complex environments. These include heavy
computational burdens for mapping, sensor occlusion failures for UAVs, and
traversal challenges on irregular terrain for UGVs, all compounded by a lack of
perception-aware strategies. To address these challenges, we introduce Random
Mapping and Random Projection (RMRP). This method constructs a lightweight
linear parametric map by first mapping data to a high-dimensional space,
followed by a sparse random projection for dimensionality reduction. Our novel
Residual Energy Preservation Theorem provides theoretical guarantees for this
process, ensuring critical geometric properties are preserved. Based on this
map, we propose the RPATR (Robust Perception-Aware Trajectory Planner)
framework. For UAVs, our method unifies grid and Euclidean Signed Distance
Field (ESDF) maps. The front-end uses an analytical occupancy gradient to
refine initial paths for safety and smoothness, while the back-end uses a
closed-form ESDF for trajectory optimization. Leveraging the trained RMRP
model's generalization, the planner predicts unobserved areas for proactive
navigation. For UGVs, the model characterizes terrain and provides closed-form
gradients, enabling online planning to circumvent large holes. Validated in
diverse scenarios, our framework demonstrates superior mapping performance in
time, memory, and accuracy, and enables computationally efficient, safe
navigation for high-speed UAVs and UGVs. The code will be released to foster
community collaboration.

</details>


### [12] [C-ZUPT: Stationarity-Aided Aerial Hovering](https://arxiv.org/abs/2507.09344)
*Daniel Engelsman,Itzik Klein*

Main category: cs.RO

TL;DR: 提出了一种用于空中导航的受控零速度更新（C-ZUPT）方法，通过识别准静态平衡来减少惯性漂移，提高导航稳定性。


<details>
  <summary>Details</summary>
Motivation: 卫星和摄像头定位在复杂环境中受限，惯性传感器易受偏差和噪声影响，导致精度快速下降。

Method: 引入C-ZUPT方法，通过不确定性阈值识别准静态平衡，提供精确速度更新。

Result: C-ZUPT显著减少惯性漂移和控制能耗，提升导航稳定性，延长飞行时间。

Conclusion: C-ZUPT为资源受限的空中系统提供了高效导航解决方案。

Abstract: Autonomous systems across diverse domains have underscored the need for
drift-resilient state estimation. Although satellite-based positioning and
cameras are widely used, they often suffer from limited availability in many
environments. As a result, positioning must rely solely on inertial sensors,
leading to rapid accuracy degradation over time due to sensor biases and noise.
To counteract this, alternative update sources-referred to as information
aiding-serve as anchors of certainty. Among these, the zero-velocity update
(ZUPT) is particularly effective in providing accurate corrections during
stationary intervals, though it is restricted to surface-bound platforms. This
work introduces a controlled ZUPT (C-ZUPT) approach for aerial navigation and
control, independent of surface contact. By defining an uncertainty threshold,
C-ZUPT identifies quasi-static equilibria to deliver precise velocity updates
to the estimation filter. Extensive validation confirms that these
opportunistic, high-quality updates significantly reduce inertial drift and
control effort. As a result, C-ZUPT mitigates filter divergence and enhances
navigation stability, enabling more energy-efficient hovering and substantially
extending sustained flight-key advantages for resource-constrained aerial
systems.

</details>


### [13] [Constrained Style Learning from Imperfect Demonstrations under Task Optimality](https://arxiv.org/abs/2507.09371)
*Kehan Wen,Chenhao Li,Junzhe He,Marco Hutter*

Main category: cs.RO

TL;DR: 论文提出了一种基于约束马尔可夫决策过程（CMDP）的方法，用于在机器人任务中平衡任务性能和风格模仿质量，解决了现有方法因依赖不完整或不切实际的演示而牺牲任务性能的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在合成风格化动作时，通常依赖与任务目标紧密对齐的专家演示，但实际演示往往不完整或不切实际，导致风格模仿以牺牲任务性能为代价。

Method: 将问题建模为约束马尔可夫决策过程（CMDP），优化风格模仿目标的同时通过约束保持接近最优的任务性能，并引入自适应调整的拉格朗日乘数来选择性模仿演示。

Result: 在多个机器人平台和任务中验证了方法的有效性，实现了稳健的任务性能和高保真的风格学习，在ANYmal-D硬件上展示了14.5%的机械能耗降低和更敏捷的步态模式。

Conclusion: 提出的方法通过CMDP框架和自适应拉格朗日乘数，成功实现了风格模仿与任务性能的平衡，具有实际应用价值。

Abstract: Learning from demonstration has proven effective in robotics for acquiring
natural behaviors, such as stylistic motions and lifelike agility, particularly
when explicitly defining style-oriented reward functions is challenging.
Synthesizing stylistic motions for real-world tasks usually requires balancing
task performance and imitation quality. Existing methods generally depend on
expert demonstrations closely aligned with task objectives. However, practical
demonstrations are often incomplete or unrealistic, causing current methods to
boost style at the expense of task performance. To address this issue, we
propose formulating the problem as a constrained Markov Decision Process
(CMDP). Specifically, we optimize a style-imitation objective with constraints
to maintain near-optimal task performance. We introduce an adaptively
adjustable Lagrangian multiplier to guide the agent to imitate demonstrations
selectively, capturing stylistic nuances without compromising task performance.
We validate our approach across multiple robotic platforms and tasks,
demonstrating both robust task performance and high-fidelity style learning. On
ANYmal-D hardware we show a 14.5% drop in mechanical energy and a more agile
gait pattern, showcasing real-world benefits.

</details>


### [14] [Real-Time Adaptive Motion Planning via Point Cloud-Guided, Energy-Based Diffusion and Potential Fields](https://arxiv.org/abs/2507.09383)
*Wondmgezahu Teshome,Kian Behzad,Octavia Camps,Michael Everett,Milad Siami,Mario Sznaier*

Main category: cs.RO

TL;DR: 提出了一种结合能量扩散模型和人工势场的运动规划框架，用于复杂环境中的实时轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 解决追逃问题，实现在复杂环境中实时生成鲁棒轨迹的需求。

Method: 利用点云直接处理障碍物信息，结合无分类器引导训练和局部势场采样增强避障能力。

Result: 在动态场景中，系统通过扩散模型生成初始轨迹，并通过势场适应持续优化，在部分可观测的追逃场景中表现良好。

Conclusion: 该框架在复杂环境中实现了高效的实时轨迹规划，适用于动态和部分可观测的场景。

Abstract: Motivated by the problem of pursuit-evasion, we present a motion planning
framework that combines energy-based diffusion models with artificial potential
fields for robust real time trajectory generation in complex environments. Our
approach processes obstacle information directly from point clouds, enabling
efficient planning without requiring complete geometric representations. The
framework employs classifier-free guidance training and integrates local
potential fields during sampling to enhance obstacle avoidance. In dynamic
scenarios, the system generates initial trajectories using the diffusion model
and continuously refines them through potential field-based adaptation,
demonstrating effective performance in pursuit-evasion scenarios with partial
pursuer observability.

</details>


### [15] [Influence of Static and Dynamic Downwash Interactions on Multi-Quadrotor Systems](https://arxiv.org/abs/2507.09463)
*Anoop Kiran,Nora Ayanian,Kenneth Breuer*

Main category: cs.RO

TL;DR: 该论文通过数据驱动的方法分析了多旋翼无人机间的下洗效应，提出了优化编队和控制的策略。


<details>
  <summary>Details</summary>
Motivation: 多旋翼无人机在近距离飞行时因下洗效应导致性能下降，传统保守策略限制了其应用范围。

Method: 使用力和力矩测量以及粒子图像测速技术（PIV）量化单机和多机配置中的下洗效应。

Result: 数据可用于优化编队、扩展操作范围并提高多机控制的鲁棒性。

Conclusion: 研究为多旋翼无人机的协调控制和密集环境应用提供了物理基础。

Abstract: Flying multiple quadrotors in close proximity presents a significant
challenge due to complex aerodynamic interactions, particularly downwash
effects that are known to destabilize vehicles and degrade performance.
Traditionally, multi-quadrotor systems rely on conservative strategies, such as
collision avoidance zones around the robot volume, to circumvent this effect.
This restricts their capabilities by requiring a large volume for the operation
of a multi-quadrotor system, limiting their applicability in dense
environments. This work provides a comprehensive, data-driven analysis of the
downwash effect, with a focus on characterizing, analyzing, and understanding
forces, moments, and velocities in both single and multi-quadrotor
configurations. We use measurements of forces and torques to characterize
vehicle interactions, and particle image velocimetry (PIV) to quantify the
spatial features of the downwash wake for a single quadrotor and an interacting
pair of quadrotors. This data can be used to inform physics-based strategies
for coordination, leverage downwash for optimized formations, expand the
envelope of operation, and improve the robustness of multi-quadrotor control.

</details>


### [16] [Unmanned Aerial Vehicle (UAV) Data-Driven Modeling Software with Integrated 9-Axis IMUGPS Sensor Fusion and Data Filtering Algorithm](https://arxiv.org/abs/2507.09464)
*Azfar Azdi Arfakhsyad,Aufa Nasywa Rahman,Larasati Kinanti,Ahmad Ataka Awwalur Rizqi,Hannan Nur Muhammad*

Main category: cs.RO

TL;DR: 提出了一种基于低成本传感器的无人机数据驱动建模软件，通过传感器融合和滤波算法提高数据质量，实现高精度模型可视化。


<details>
  <summary>Details</summary>
Motivation: 无人机作为多功能平台需要精确建模支持开发测试，而低成本传感器和高效数据处理是关键。

Method: 利用IMU数据（四元数表示避免万向节锁）和GPS与加速度计数据融合（结合稳定坐标与高频更新），通过滤波算法和传感器融合技术优化数据。

Result: 软件能高精度、流畅地实时渲染无人机的方向和位置。

Conclusion: 该软件通过低成本传感器和高效数据处理，实现了无人机建模的高精度和实时性。

Abstract: Unmanned Aerial Vehicles (UAV) have emerged as versatile platforms, driving
the demand for accurate modeling to support developmental testing. This paper
proposes data-driven modeling software for UAV. Emphasizes the utilization of
cost-effective sensors to obtain orientation and location data subsequently
processed through the application of data filtering algorithms and sensor
fusion techniques to improve the data quality to make a precise model
visualization on the software. UAV's orientation is obtained using processed
Inertial Measurement Unit (IMU) data and represented using Quaternion
Representation to avoid the gimbal lock problem. The UAV's location is
determined by combining data from the Global Positioning System (GPS), which
provides stable geographic coordinates but slower data update frequency, and
the accelerometer, which has higher data update frequency but integrating it to
get position data is unstable due to its accumulative error. By combining data
from these two sensors, the software is able to calculate and continuously
update the UAV's real-time position during its flight operations. The result
shows that the software effectively renders UAV orientation and position with
high degree of accuracy and fluidity

</details>


### [17] [mmE-Loc: Facilitating Accurate Drone Landing with Ultra-High-Frequency Localization](https://arxiv.org/abs/2507.09469)
*Haoyang Wang,Jingao Xu,Xinyu Luo,Ting Zhang,Xuecheng Chen,Ruiyang Duan,Jialong Chen,Yunhao Liu,Jianfeng Zheng,Weijie Hong,Xinlei Chen*

Main category: cs.RO

TL;DR: 论文提出了一种名为mmE-Loc的高精度、低延迟地面定位系统，结合事件相机和毫米波雷达，用于无人机精确降落。


<details>
  <summary>Details</summary>
Motivation: 传统帧相机采样频率低，与毫米波雷达不匹配，限制了系统性能。

Method: 采用事件相机与毫米波雷达结合，提出两个创新模块：一致性协作跟踪和图自适应联合优化。

Result: 实验表明，mmE-Loc在精度和延迟上显著优于现有方法。

Conclusion: mmE-Loc系统通过多模态融合和优化模块，实现了无人机精确降落的高效解决方案。

Abstract: For precise, efficient, and safe drone landings, ground platforms should
real-time, accurately locate descending drones and guide them to designated
spots. While mmWave sensing combined with cameras improves localization
accuracy, lower sampling frequency of traditional frame cameras compared to
mmWave radar creates bottlenecks in system throughput. In this work, we upgrade
traditional frame camera with event camera, a novel sensor that harmonizes in
sampling frequency with mmWave radar within ground platform setup, and
introduce mmE-Loc, a high-precision, low-latency ground localization system
designed for precise drone landings. To fully exploit the \textit{temporal
consistency} and \textit{spatial complementarity} between these two modalities,
we propose two innovative modules: \textit{(i)} the Consistency-instructed
Collaborative Tracking module, which further leverages the drone's physical
knowledge of periodic micro-motions and structure for accurate measurements
extraction, and \textit{(ii)} the Graph-informed Adaptive Joint Optimization
module, which integrates drone motion information for efficient sensor fusion
and drone localization. Real-world experiments conducted in landing scenarios
with a drone delivery company demonstrate that mmE-Loc significantly
outperforms state-of-the-art methods in both accuracy and latency.

</details>


### [18] [TruckV2X: A Truck-Centered Perception Dataset](https://arxiv.org/abs/2507.09505)
*Tenghui Xie,Zhiying Song,Fuxi Wen,Jun Li,Guangzhao Liu,Zijian Zhao*

Main category: cs.RO

TL;DR: 论文介绍了首个以卡车为中心的大规模协同感知数据集TruckV2X，旨在解决卡车感知中的盲区和遮挡问题，并促进多智能体自动驾驶卡车系统的发展。


<details>
  <summary>Details</summary>
Motivation: 卡车自动驾驶面临独特的感知挑战，如盲区和动态拖车运动，现有数据集缺乏针对重型车辆的协同感知配置。

Method: 提出TruckV2X数据集，包含多模态感知（LiDAR和摄像头）和多智能体协作（牵引车、拖车、CAV和RSU）。

Result: 数据集为开发协同感知系统提供了基础，增强了遮挡处理能力，并加速了多智能体自动驾驶卡车系统的部署。

Conclusion: TruckV2X填补了重型车辆协同感知数据集的空白，为未来研究提供了基准和方向。

Abstract: Autonomous trucking offers significant benefits, such as improved safety and
reduced costs, but faces unique perception challenges due to trucks' large size
and dynamic trailer movements. These challenges include extensive blind spots
and occlusions that hinder the truck's perception and the capabilities of other
road users. To address these limitations, cooperative perception emerges as a
promising solution. However, existing datasets predominantly feature light
vehicle interactions or lack multi-agent configurations for heavy-duty vehicle
scenarios. To bridge this gap, we introduce TruckV2X, the first large-scale
truck-centered cooperative perception dataset featuring multi-modal sensing
(LiDAR and cameras) and multi-agent cooperation (tractors, trailers, CAVs, and
RSUs). We further investigate how trucks influence collaborative perception
needs, establishing performance benchmarks while suggesting research priorities
for heavy vehicle perception. The dataset provides a foundation for developing
cooperative perception systems with enhanced occlusion handling capabilities,
and accelerates the deployment of multi-agent autonomous trucking systems. The
TruckV2X dataset is available at
https://huggingface.co/datasets/XieTenghu1/TruckV2X.

</details>


### [19] [Self-supervised Pretraining for Integrated Prediction and Planning of Automated Vehicles](https://arxiv.org/abs/2507.09537)
*Yangang Ren,Guojian Zhan,Chen Lv,Jun Li,Fenghua Liang,Keqiang Li*

Main category: cs.RO

TL;DR: Plan-MAE是一个基于掩码自编码器的预测与规划预训练框架，通过三个任务学习场景理解，显著提升了自动驾驶车辆的轨迹规划性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖模仿学习，忽视了场景理解对生成更全面轨迹的潜力。Plan-MAE旨在通过预训练框架解决这一问题。

Method: Plan-MAE通过三个任务（道路网络重建、代理轨迹建模、导航路线捕捉）学习场景理解，并加入局部子规划任务对齐车辆动态。预训练后微调下游任务。

Result: 实验表明，Plan-MAE在规划指标上大幅优于现有方法，可作为学习型运动规划器的重要预训练步骤。

Conclusion: Plan-MAE通过场景理解的预训练，显著提升了自动驾驶车辆的预测与规划能力。

Abstract: Predicting the future of surrounding agents and accordingly planning a safe,
goal-directed trajectory are crucial for automated vehicles. Current methods
typically rely on imitation learning to optimize metrics against the ground
truth, often overlooking how scene understanding could enable more holistic
trajectories. In this paper, we propose Plan-MAE, a unified pretraining
framework for prediction and planning that capitalizes on masked autoencoders.
Plan-MAE fuses critical contextual understanding via three dedicated tasks:
reconstructing masked road networks to learn spatial correlations, agent
trajectories to model social interactions, and navigation routes to capture
destination intents. To further align vehicle dynamics and safety constraints,
we incorporate a local sub-planning task predicting the ego-vehicle's near-term
trajectory segment conditioned on earlier segment. This pretrained model is
subsequently fine-tuned on downstream tasks to jointly generate the prediction
and planning trajectories. Experiments on large-scale datasets demonstrate that
Plan-MAE outperforms current methods on the planning metrics by a large margin
and can serve as an important pre-training step for learning-based motion
planner.

</details>


### [20] [On the Importance of Neural Membrane Potential Leakage for LIDAR-based Robot Obstacle Avoidance using Spiking Neural Networks](https://arxiv.org/abs/2507.09538)
*Zainab Ali,Lujayn Al-Amir,Ali Safa*

Main category: cs.RO

TL;DR: 论文研究了基于脉冲神经网络（SNN）的机器人导航与避障，重点探讨了神经元膜泄漏对SNN处理LIDAR数据精度的影响，并通过实验验证了其与CNN相当的精度。


<details>
  <summary>Details</summary>
Motivation: 由于SNN在神经形态硬件中具有高精度、低内存和计算复杂度的优势，适合用于资源有限的自主机器人应用（如无人机和漫游车）。

Method: 搭建了配备LIDAR的机器人平台，收集带标签的LIDAR数据及人工操作的控制命令，研究了SNN中神经元膜泄漏对避障精度的影响。

Result: 通过调整LIF神经元的膜电位泄漏常数，SNN在避障任务中达到了与CNN相当的精度。

Conclusion: 论文首次系统研究了神经元膜泄漏对SNN处理LIDAR数据的重要性，并开源了LIDAR数据集以促进未来研究。

Abstract: Using neuromorphic computing for robotics applications has gained much
attention in recent year due to the remarkable ability of Spiking Neural
Networks (SNNs) for high-precision yet low memory and compute complexity
inference when implemented in neuromorphic hardware. This ability makes SNNs
well-suited for autonomous robot applications (such as in drones and rovers)
where battery resources and payload are typically limited. Within this context,
this paper studies the use of SNNs for performing direct robot navigation and
obstacle avoidance from LIDAR data. A custom robot platform equipped with a
LIDAR is set up for collecting a labeled dataset of LIDAR sensing data together
with the human-operated robot control commands used for obstacle avoidance.
Crucially, this paper provides what is, to the best of our knowledge, a first
focused study about the importance of neuron membrane leakage on the SNN
precision when processing LIDAR data for obstacle avoidance. It is shown that
by carefully tuning the membrane potential leakage constant of the spiking
Leaky Integrate-and-Fire (LIF) neurons used within our SNN, it is possible to
achieve on-par robot control precision compared to the use of a non-spiking
Convolutional Neural Network (CNN). Finally, the LIDAR dataset collected during
this work is released as open-source with the hope of benefiting future
research.

</details>


### [21] [IteraOptiRacing: A Unified Planning-Control Framework for Real-time Autonomous Racing for Iterative Optimal Performance](https://arxiv.org/abs/2507.09714)
*Yifan Zeng,Yihan Li,Suiyi He,Koushil Sreenath,Jun Zeng*

Main category: cs.RO

TL;DR: 提出了一种基于i2LQR的统一规划控制策略IteraOptiRacing，用于自动驾驶赛车环境中与其他赛车竞争，优化圈速并避免碰撞。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶赛车环境中，需要一种既能优化圈速又能避免与动态障碍物碰撞的策略。

Method: 基于i2LQR的统一策略，利用历史数据迭代优化轨迹，实现低计算负担和并行计算能力。

Result: 仿真结果表明，该策略在随机生成的动态环境中优于现有方法，实现了无碰撞且时间最优的轨迹。

Conclusion: IteraOptiRacing策略在实时性和性能上表现优异，适用于竞争性赛车场景。

Abstract: This paper presents a unified planning-control strategy for competing with
other racing cars called IteraOptiRacing in autonomous racing environments.
This unified strategy is proposed based on Iterative Linear Quadratic Regulator
for Iterative Tasks (i2LQR), which can improve lap time performance in the
presence of surrounding racing obstacles. By iteratively using the ego car's
historical data, both obstacle avoidance for multiple moving cars and time cost
optimization are considered in this unified strategy, resulting in
collision-free and time-optimal generated trajectories. The algorithm's
constant low computation burden and suitability for parallel computing enable
real-time operation in competitive racing scenarios. To validate its
performance, simulations in a high-fidelity simulator are conducted with
multiple randomly generated dynamic agents on the track. Results show that the
proposed strategy outperforms existing methods across all randomly generated
autonomous racing scenarios, enabling enhanced maneuvering for the ego racing
car.

</details>


### [22] [Visual Homing in Outdoor Robots Using Mushroom Body Circuits and Learning Walks](https://arxiv.org/abs/2507.09725)
*Gabriel G. Gattaux,Julien R. Serres,Franck Ruffier,Antoine Wystrach*

Main category: cs.RO

TL;DR: 蚂蚁通过少量视觉输入和学习行走实现稳健的视觉归巢，启发了一种基于蘑菇体（MB）架构的自主导航解决方案，并在真实环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 蚂蚁的视觉归巢行为具有高效性和鲁棒性，为自主导航提供了生物启发。研究旨在将蘑菇体模型首次应用于视觉归巢，验证其在实际场景中的可行性。

Method: 采用侧向化蘑菇体架构，通过角路径积分信号分类全景视图，分为“目标在左”和“目标在右”记忆库。通过四个实验逐步验证：仿真、解耦学习行走归巢、随机行走归巢和精确停止行为。

Result: 系统在自然户外环境中实现了稳健的视觉归巢，内存占用低于9 kB，运行频率为8 Hz，功能上类似于机器人中的基于路点的位置控制。

Conclusion: 该研究提供了一种基于生物学的资源高效视觉归巢解决方案，验证了蘑菇体架构在自主导航中的潜力。

Abstract: Ants achieve robust visual homing with minimal sensory input and only a few
learning walks, inspiring biomimetic solutions for autonomous navigation. While
Mushroom Body (MB) models have been used in robotic route following, they have
not yet been applied to visual homing. We present the first real-world
implementation of a lateralized MB architecture for visual homing onboard a
compact autonomous car-like robot. We test whether the sign of the angular path
integration (PI) signal can categorize panoramic views, acquired during
learning walks and encoded in the MB, into "goal on the left" and "goal on the
right" memory banks, enabling robust homing in natural outdoor settings. We
validate this approach through four incremental experiments: (1) simulation
showing attractor-like nest dynamics; (2) real-world homing after decoupled
learning walks, producing nest search behavior; (3) homing after random walks
using noisy PI emulated with GPS-RTK; and (4) precise stopping-at-the-goal
behavior enabled by a fifth MB Output Neuron (MBON) encoding goal-views to
control velocity. This mimics the accurate homing behavior of ants and
functionally resembles waypoint-based position control in robotics, despite
relying solely on visual input. Operating at 8 Hz on a Raspberry Pi 4 with
32x32 pixel views and a memory footprint under 9 kB, our system offers a
biologically grounded, resource-efficient solution for autonomous visual
homing.

</details>


### [23] [Active Probing with Multimodal Predictions for Motion Planning](https://arxiv.org/abs/2507.09822)
*Darshan Gadginmath,Farhad Nawaz,Minjun Sung,Faizan M Tariq,Sangjae Bae,David Isele,Fabio Pasqualetti,Jovin Dsa*

Main category: cs.RO

TL;DR: 论文提出了一种结合轨迹规划、多模态预测和主动探测的统一框架，用于增强不确定性下的决策能力，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 动态环境中导航需要处理其他代理行为的不确定性，现有方法在多模态预测和主动探测的结合上存在不足。

Method: 开发了一种新的风险度量方法，通过混合模型整合多模态预测不确定性，并引入主动探测机制以减少预测模糊性。

Result: 在MetaDrive仿真环境中验证，框架能成功处理复杂交通场景，并在多种行为模型下表现稳健。

Conclusion: 该框架为现实世界中的自主导航挑战提供了广泛适用的解决方案。

Abstract: Navigation in dynamic environments requires autonomous systems to reason
about uncertainties in the behavior of other agents. In this paper, we
introduce a unified framework that combines trajectory planning with multimodal
predictions and active probing to enhance decision-making under uncertainty. We
develop a novel risk metric that seamlessly integrates multimodal prediction
uncertainties through mixture models. When these uncertainties follow a
Gaussian mixture distribution, we prove that our risk metric admits a
closed-form solution, and is always finite, thus ensuring analytical
tractability. To reduce prediction ambiguity, we incorporate an active probing
mechanism that strategically selects actions to improve its estimates of
behavioral parameters of other agents, while simultaneously handling multimodal
uncertainties. We extensively evaluate our framework in autonomous navigation
scenarios using the MetaDrive simulation environment. Results demonstrate that
our active probing approach successfully navigates complex traffic scenarios
with uncertain predictions. Additionally, our framework shows robust
performance across diverse traffic agent behavior models, indicating its broad
applicability to real-world autonomous navigation challenges. Code and videos
are available at
https://darshangm.github.io/papers/active-probing-multimodal-predictions/.

</details>


### [24] [Multi-residual Mixture of Experts Learning for Cooperative Control in Multi-vehicle Systems](https://arxiv.org/abs/2507.09836)
*Vindula Jayawardana,Sirui Li,Yashar Farid,Cathy Wu*

Main category: cs.RO

TL;DR: 论文提出了一种名为MRMEL的新框架，用于设计通用的拉格朗日交通控制策略，通过动态选择最优策略并学习残差修正，显著提升了自动驾驶车辆在多样化交通场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆作为移动执行器在交通流控制中具有潜力，但传统固定执行器（如交通信号）无法适应多样化交通场景，设计通用的控制策略面临挑战。

Method: 提出MRMEL框架，结合残差强化学习和专家混合模型，动态选择最优策略并学习残差修正。

Result: 在真实交通场景的案例研究中，MRMEL实现了比基线方法额外4%-9%的车辆排放减少。

Conclusion: MRMEL通过动态策略选择和残差修正，显著提升了自动驾驶车辆在多样化交通场景中的控制性能。

Abstract: Autonomous vehicles (AVs) are becoming increasingly popular, with their
applications now extending beyond just a mode of transportation to serving as
mobile actuators of a traffic flow to control flow dynamics. This contrasts
with traditional fixed-location actuators, such as traffic signals, and is
referred to as Lagrangian traffic control. However, designing effective
Lagrangian traffic control policies for AVs that generalize across traffic
scenarios introduces a major challenge. Real-world traffic environments are
highly diverse, and developing policies that perform robustly across such
diverse traffic scenarios is challenging. It is further compounded by the joint
complexity of the multi-agent nature of traffic systems, mixed motives among
participants, and conflicting optimization objectives subject to strict
physical and external constraints. To address these challenges, we introduce
Multi-Residual Mixture of Expert Learning (MRMEL), a novel framework for
Lagrangian traffic control that augments a given suboptimal nominal policy with
a learned residual while explicitly accounting for the structure of the traffic
scenario space. In particular, taking inspiration from residual reinforcement
learning, MRMEL augments a suboptimal nominal AV control policy by learning a
residual correction, but at the same time dynamically selects the most suitable
nominal policy from a pool of nominal policies conditioned on the traffic
scenarios and modeled as a mixture of experts. We validate MRMEL using a case
study in cooperative eco-driving at signalized intersections in Atlanta, Dallas
Fort Worth, and Salt Lake City, with real-world data-driven traffic scenarios.
The results show that MRMEL consistently yields superior performance-achieving
an additional 4%-9% reduction in aggregate vehicle emissions relative to the
strongest baseline in each setting.

</details>


### [25] [AdvGrasp: Adversarial Attacks on Robotic Grasping from a Physical Perspective](https://arxiv.org/abs/2507.09857)
*Xiaofei Wang,Mingliang Han,Tianyu Hao,Cegang Li,Yunbo Zhao,Keke Tang*

Main category: cs.RO

TL;DR: AdvGrasp框架从物理角度对机器人抓取进行对抗攻击，通过变形物体形状降低抓取性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注神经网络预测而忽略抓取的物理原理，AdvGrasp填补了这一空白。

Method: 通过变形物体形状增加重力扭矩并降低稳定性，系统性地削弱抓取能力。

Result: 实验验证了AdvGrasp的有效性，实际应用展示了其鲁棒性。

Conclusion: AdvGrasp为机器人抓取的鲁棒性评估和改进提供了新视角。

Abstract: Adversarial attacks on robotic grasping provide valuable insights into
evaluating and improving the robustness of these systems. Unlike studies that
focus solely on neural network predictions while overlooking the physical
principles of grasping, this paper introduces AdvGrasp, a framework for
adversarial attacks on robotic grasping from a physical perspective.
Specifically, AdvGrasp targets two core aspects: lift capability, which
evaluates the ability to lift objects against gravity, and grasp stability,
which assesses resistance to external disturbances. By deforming the object's
shape to increase gravitational torque and reduce stability margin in the
wrench space, our method systematically degrades these two key grasping
metrics, generating adversarial objects that compromise grasp performance.
Extensive experiments across diverse scenarios validate the effectiveness of
AdvGrasp, while real-world validations demonstrate its robustness and practical
applicability

</details>


### [26] [Customize Harmonic Potential Fields via Hybrid Optimization over Homotopic Paths](https://arxiv.org/abs/2507.09858)
*Shuaikang Wang,Tiecheng Guo,Meng Guo*

Main category: cs.RO

TL;DR: 提出了一种新方法，通过混合优化算法自动生成具有特定拓扑特性的谐波势场路径，适用于复杂工作空间。


<details>
  <summary>Details</summary>
Motivation: 现有谐波势场方法难以自定义路径的拓扑特性，限制了其在复杂环境中的应用。

Method: 采用混合优化算法，搜索路径的同伦类，并通过投影梯度下降优化权重参数，同时利用微分同胚变换简化设计。

Result: 方法在仿真和硬件实验中验证了其有效性，能够生成满足特定同伦特性的导航势场。

Conclusion: 该方法为复杂环境中机器人导航提供了灵活且安全的解决方案。

Abstract: Safe navigation within a workspace is a fundamental skill for autonomous
robots to accomplish more complex tasks. Harmonic potentials are artificial
potential fields that are analytical, globally convergent and provably free of
local minima. Thus, it has been widely used for generating safe and reliable
robot navigation control policies. However, most existing methods do not allow
customization of the harmonic potential fields nor the resulting paths,
particularly regarding their topological properties. In this paper, we propose
a novel method that automatically finds homotopy classes of paths that can be
generated by valid harmonic potential fields. The considered complex workspaces
can be as general as forest worlds consisting of numerous overlapping
star-obstacles. The method is based on a hybrid optimization algorithm that
searches over homotopy classes, selects the structure of each tree-of-stars
within the forest, and optimizes over the continuous weight parameters for each
purged tree via the projected gradient descent. The key insight is to transform
the forest world to the unbounded point world via proper diffeomorphic
transformations. It not only facilitates a simpler design of the
multi-directional D-signature between non-homotopic paths, but also retain the
safety and convergence properties. Extensive simulations and hardware
experiments are conducted for non-trivial scenarios, where the navigation
potentials are customized for desired homotopic properties. Project page:
https://shuaikang-wang.github.io/CustFields.

</details>


### [27] [Demonstrating the Octopi-1.5 Visual-Tactile-Language Model](https://arxiv.org/abs/2507.09985)
*Samson Yu,Kelvin Lin,Harold Soh*

Main category: cs.RO

TL;DR: Octopi-1.5是一个最新的视觉-触觉-语言模型，能够处理多部分触觉信号，并通过检索增强生成（RAG）模块提升任务性能，支持实时学习新物体。


<details>
  <summary>Details</summary>
Motivation: 触觉对人类和机器人至关重要，尤其在灵巧操作、材料识别和视觉遮挡场景中。Octopi-1.5旨在提升触觉推理能力并推动视觉-触觉-语言模型（VTLM）的发展。

Method: Octopi-1.5扩展了前代模型功能，支持多部分触觉信号处理，并引入RAG模块优化任务性能。通过手持触觉接口TMI（配备GelSight和TAC-02传感器）实现用户交互。

Result: 模型能够通过触觉输入和常识知识解决推理任务，例如识别抓取物体并推荐处理方式。RAG模块还支持实时学习新物体。

Conclusion: Octopi-1.5展示了VTLM的进展与潜力，同时通过开源代码和设计文件促进领域发展。

Abstract: Touch is recognized as a vital sense for humans and an equally important
modality for robots, especially for dexterous manipulation, material
identification, and scenarios involving visual occlusion. Building upon very
recent work in touch foundation models, this demonstration will feature
Octopi-1.5, our latest visual-tactile-language model. Compared to its
predecessor, Octopi-1.5 introduces the ability to process tactile signals from
multiple object parts and employs a simple retrieval-augmented generation (RAG)
module to improve performance on tasks and potentially learn new objects
on-the-fly. The system can be experienced live through a new handheld
tactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile
sensors. This convenient and accessible setup allows users to interact with
Octopi-1.5 without requiring a robot. During the demonstration, we will
showcase Octopi-1.5 solving tactile inference tasks by leveraging tactile
inputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5
will identify objects being grasped and respond to follow-up queries about how
to handle it (e.g., recommending careful handling for soft fruits). We also
plan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items.
With live interactions, this demonstration aims to highlight both the progress
and limitations of VTLMs such as Octopi-1.5 and to foster further interest in
this exciting field. Code for Octopi-1.5 and design files for the TMI gripper
are available at https://github.com/clear-nus/octopi-1.5.

</details>


### [28] [Ariel Explores: Vision-based underwater exploration and inspection via generalist drone-level autonomy](https://arxiv.org/abs/2507.10003)
*Mohit Singh,Mihir Dharmadhikari,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文介绍了一种基于视觉的水下探索和检查自主解决方案，集成在Ariel水下机器人中，通过多摄像头视觉-惯性状态估计和学习型机器人速度预测方法提升鲁棒性，并在潜艇干船坞中进行了实地测试。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够在视觉条件恶劣的水下环境中稳定运行的自主探索和检查系统，以提升水下机器人的自主性和适应性。

Method: 采用多摄像头视觉-惯性状态估计方法，结合学习型机器人速度预测技术，增强系统对视觉退化的鲁棒性。

Result: 实地测试表明，状态估计解决方案具有鲁棒性，路径规划技术在不同机器人平台上具有通用性。

Conclusion: 该系统在水下环境中表现出色，为水下自主探索和检查提供了可靠的技术支持。

Abstract: This work presents a vision-based underwater exploration and inspection
autonomy solution integrated into Ariel, a custom vision-driven underwater
robot. Ariel carries a $5$ camera and IMU based sensing suite, enabling a
refraction-aware multi-camera visual-inertial state estimation method aided by
a learning-based proprioceptive robot velocity prediction method that enhances
robustness against visual degradation. Furthermore, our previously developed
and extensively field-verified autonomous exploration and general visual
inspection solution is integrated on Ariel, providing aerial drone-level
autonomy underwater. The proposed system is field-tested in a submarine dry
dock in Trondheim under challenging visual conditions. The field demonstration
shows the robustness of the state estimation solution and the generalizability
of the path planning techniques across robot embodiments.

</details>


### [29] [Finetuning Deep Reinforcement Learning Policies with Evolutionary Strategies for Control of Underactuated Robots](https://arxiv.org/abs/2507.10030)
*Marco Calì,Alberto Sinigaglia,Niccolò Turcato,Ruggero Carli,Gian Antonio Susto*

Main category: cs.RO

TL;DR: 提出一种结合深度强化学习和进化策略的方法，用于优化欠驱动机器人的控制策略，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在复杂控制问题中表现优异，但策略可能需要进一步优化以实现特定任务目标。

Method: 先用SAC训练代理，再用SNES进化策略直接优化原始评分。

Result: 实验表明，进化微调显著提升性能，控制器优于基线，达到竞赛任务的高分。

Conclusion: 结合RL和ES的方法有效提升欠驱动机器人控制的性能和鲁棒性。

Abstract: Deep Reinforcement Learning (RL) has emerged as a powerful method for
addressing complex control problems, particularly those involving underactuated
robotic systems. However, in some cases, policies may require refinement to
achieve optimal performance and robustness aligned with specific task
objectives. In this paper, we propose an approach for fine-tuning Deep RL
policies using Evolutionary Strategies (ES) to enhance control performance for
underactuated robots. Our method involves initially training an RL agent with
Soft-Actor Critic (SAC) using a surrogate reward function designed to
approximate complex specific scoring metrics. We subsequently refine this
learned policy through a zero-order optimization step employing the Separable
Natural Evolution Strategy (SNES), directly targeting the original score.
Experimental evaluations conducted in the context of the 2nd AI Olympics with
RealAIGym at IROS 2024 demonstrate that our evolutionary fine-tuning
significantly improves agent performance while maintaining high robustness. The
resulting controllers outperform established baselines, achieving competitive
scores for the competition tasks.

</details>


### [30] [MP-RBFN: Learning-based Vehicle Motion Primitives using Radial Basis Function Networks](https://arxiv.org/abs/2507.10047)
*Marc Kaufeld,Mattia Piccinini,Johannes Betz*

Main category: cs.RO

TL;DR: MP-RBFN是一种基于径向基函数网络的新方法，用于高效学习自动驾驶中的运动基元，结合了采样方法的高性能和优化方法的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统运动规划方法（基于优化）计算成本高，而基于采样的方法虽高效但对轨迹几何形状有限制。MP-RBFN旨在结合两者的优势。

Method: MP-RBFN利用径向基函数网络，将采样方法的高保真轨迹生成与车辆动力学的精确描述相结合。

Result: 实验表明，MP-RBFN在生成优化运动基元时的精度比现有半解析方法高7倍，且推理时间短。

Conclusion: MP-RBFN在运动规划中表现出色，已开源并集成到采样轨迹规划器中，具有实际应用价值。

Abstract: This research introduces MP-RBFN, a novel formulation leveraging Radial Basis
Function Networks for efficiently learning Motion Primitives derived from
optimal control problems for autonomous driving. While traditional motion
planning approaches based on optimization are highly accurate, they are often
computationally prohibitive. In contrast, sampling-based methods demonstrate
high performance but impose constraints on the geometric shape of trajectories.
MP-RBFN combines the strengths of both by coupling the high-fidelity trajectory
generation of sampling-based methods with an accurate description of vehicle
dynamics. Empirical results show compelling performance compared to previous
methods, achieving a precise description of motion primitives at low inference
times. MP-RBFN yields a seven times higher accuracy in generating optimized
motion primitives compared to existing semi-analytic approaches. We demonstrate
the practical applicability of MP-RBFN for motion planning by integrating the
method into a sampling-based trajectory planner. MP-RBFN is available as
open-source software at https://github.com/TUM-AVS/RBFN-Motion-Primitives.

</details>


### [31] [Hand Gesture Recognition for Collaborative Robots Using Lightweight Deep Learning in Real-Time Robotic Systems](https://arxiv.org/abs/2507.10055)
*Muhtadin,I Wayan Agus Darmawan,Muhammad Hilmi Rusydiansyah,I Ketut Eddy Purnama,Chastine Fatichah,Mauridhi Hery Purnomo*

Main category: cs.RO

TL;DR: 轻量级深度学习手势识别系统，用于自然控制协作机器人，模型仅1,103参数，22KB大小，准确率93.5%。通过量化和剪枝优化至7KB，成功在UR5机器人上实时测试。


<details>
  <summary>Details</summary>
Motivation: 实现直接自然的人机交互，避免使用额外设备如操纵杆或传感器。

Method: 基于深度学习的手势识别系统，采用量化和剪枝技术优化模型。

Result: 模型准确率93.5%，优化后仅7KB，成功在UR5机器人上实时运行。

Conclusion: 轻量级模型可实现准确响应手势控制，为受限环境中自然人机交互提供新可能。

Abstract: Direct and natural interaction is essential for intuitive human-robot
collaboration, eliminating the need for additional devices such as joysticks,
tablets, or wearable sensors. In this paper, we present a lightweight deep
learning-based hand gesture recognition system that enables humans to control
collaborative robots naturally and efficiently. This model recognizes eight
distinct hand gestures with only 1,103 parameters and a compact size of 22 KB,
achieving an accuracy of 93.5%. To further optimize the model for real-world
deployment on edge devices, we applied quantization and pruning using
TensorFlow Lite, reducing the final model size to just 7 KB. The system was
successfully implemented and tested on a Universal Robot UR5 collaborative
robot within a real-time robotic framework based on ROS2. The results
demonstrate that even extremely lightweight models can deliver accurate and
responsive hand gesture-based control for collaborative robots, opening new
possibilities for natural human-robot interaction in constrained environments.

</details>


### [32] [TGLD: A Trust-Aware Game-Theoretic Lane-Changing Decision Framework for Automated Vehicles in Heterogeneous Traffic](https://arxiv.org/abs/2507.10075)
*Jie Pan,Tianyi Wang,Yangyang Wang,Junfeng Jiao,Christian Claudel*

Main category: cs.RO

TL;DR: 论文提出了一种信任感知的博弈论换道决策框架（TGLD），通过动态评估人类驾驶车辆的信任水平，优化自动驾驶车辆的换道策略，提高效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆（AVs）需要与人类驾驶车辆（HVs）在异构交通环境中有效协作，但现有换道框架忽视了HVs的动态信任水平，导致行为预测不准确。

Method: 1. 构建多车辆联盟博弈模型，结合AVs的全协作和HVs的部分协作行为；2. 开发在线信任评估方法，动态估计HVs的信任水平；3. 考虑社会兼容性目标，最小化对周围车辆的干扰。

Result: 实验验证表明，TGLD框架能根据HVs的信任水平和驾驶风格调整策略，显著提高换道效率、安全性及交互透明度。

Conclusion: TGLD框架通过信任机制实现了更高效、安全和适应性强的AV-HV交互，为异构交通环境提供了实用解决方案。

Abstract: Automated vehicles (AVs) face a critical need to adopt socially compatible
behaviors and cooperate effectively with human-driven vehicles (HVs) in
heterogeneous traffic environment. However, most existing lane-changing
frameworks overlook HVs' dynamic trust levels, limiting their ability to
accurately predict human driver behaviors. To address this gap, this study
proposes a trust-aware game-theoretic lane-changing decision (TGLD) framework.
First, we formulate a multi-vehicle coalition game, incorporating fully
cooperative interactions among AVs and partially cooperative behaviors from HVs
informed by real-time trust evaluations. Second, we develop an online trust
evaluation method to dynamically estimate HVs' trust levels during
lane-changing interactions, guiding AVs to select context-appropriate
cooperative maneuvers. Lastly, social compatibility objectives are considered
by minimizing disruption to surrounding vehicles and enhancing the
predictability of AV behaviors, thereby ensuring human-friendly and
context-adaptive lane-changing strategies. A human-in-the-loop experiment
conducted in a highway on-ramp merging scenario validates our TGLD approach.
Results show that AVs can effectively adjust strategies according to different
HVs' trust levels and driving styles. Moreover, incorporating a trust mechanism
significantly improves lane-changing efficiency, maintains safety, and
contributes to transparent and adaptive AV-HV interactions.

</details>


### [33] [Unscented Kalman Filter with a Nonlinear Propagation Model for Navigation Applications](https://arxiv.org/abs/2507.10082)
*Amit Levy,Itzik Klein*

Main category: cs.RO

TL;DR: 提出了一种改进无迹卡尔曼滤波中sigma点传播的方法，提高了滤波精度和导航性能。


<details>
  <summary>Details</summary>
Motivation: 无迹卡尔曼滤波在导航应用中广泛使用，但其sigma点传播对滤波稳定性至关重要，需改进以提高精度。

Method: 通过非线性动态模型传播sigma点，优化导航误差状态向量的预测。

Result: 使用自主水下车辆的真实传感器数据验证了方法的有效性，提高了导航性能。

Conclusion: 新方法显著提升了无迹卡尔曼滤波的精度和导航表现。

Abstract: The unscented Kalman filter is a nonlinear estimation algorithm commonly used
in navigation applications. The prediction of the mean and covariance matrix is
crucial to the stable behavior of the filter. This prediction is done by
propagating the sigma points according to the dynamic model at hand. In this
paper, we introduce an innovative method to propagate the sigma points
according to the nonlinear dynamic model of the navigation error state vector.
This improves the filter accuracy and navigation performance. We demonstrate
the benefits of our proposed approach using real sensor data recorded by an
autonomous underwater vehicle during several scenarios.

</details>


### [34] [Foundation Model Driven Robotics: A Comprehensive Review](https://arxiv.org/abs/2507.10087)
*Muhammad Tayyab Khan,Ammar Waheed*

Main category: cs.RO

TL;DR: 综述探讨了基础模型（如LLMs和VLMs）在机器人领域的应用，分析了其优势、局限及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 基础模型在机器人领域的快速发展带来了新的可能性，但现有研究多关注孤立能力，缺乏系统级策略和实际可行性评估。

Method: 通过结构化综述，分类应用领域（如仿真驱动设计、开放世界执行等），并分析集成策略和实际挑战。

Result: 基础模型在语义理解、推理和多模态泛化方面表现优异，但仍面临实时操作、安全风险和计算限制等瓶颈。

Conclusion: 未来研究需关注更鲁棒、可解释和具身化的模型，以弥合语义推理与物理智能之间的差距。

Abstract: The rapid emergence of foundation models, particularly Large Language Models
(LLMs) and Vision-Language Models (VLMs), has introduced a transformative
paradigm in robotics. These models offer powerful capabilities in semantic
understanding, high-level reasoning, and cross-modal generalization, enabling
significant advances in perception, planning, control, and human-robot
interaction. This critical review provides a structured synthesis of recent
developments, categorizing applications across simulation-driven design,
open-world execution, sim-to-real transfer, and adaptable robotics. Unlike
existing surveys that emphasize isolated capabilities, this work highlights
integrated, system-level strategies and evaluates their practical feasibility
in real-world environments. Key enabling trends such as procedural scene
generation, policy generalization, and multimodal reasoning are discussed
alongside core bottlenecks, including limited embodiment, lack of multimodal
data, safety risks, and computational constraints. Through this lens, this
paper identifies both the architectural strengths and critical limitations of
foundation model-based robotics, highlighting open challenges in real-time
operation, grounding, resilience, and trust. The review concludes with a
roadmap for future research aimed at bridging semantic reasoning and physical
intelligence through more robust, interpretable, and embodied models.

</details>


### [35] [Physics-Informed Neural Networks with Unscented Kalman Filter for Sensorless Joint Torque Estimation in Humanoid Robots](https://arxiv.org/abs/2507.10105)
*Ines Sorrentino,Giulio Romualdi,Lorenzo Moretti,Silvio Traversaro,Daniele Pucci*

Main category: cs.RO

TL;DR: 提出了一种无需关节扭矩传感器的人形机器人全身扭矩控制框架，结合物理信息神经网络（PINNs）和无迹卡尔曼滤波（UKF），实验验证了其在扭矩跟踪、能效和抗干扰方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人在无关节扭矩传感器情况下的扭矩控制问题，提高扭矩跟踪精度和系统鲁棒性。

Method: 使用PINNs建模非线性摩擦，UKF估计关节扭矩，实时扭矩控制架构结合两者。

Result: 在ergoCub机器人上验证，优于现有RNEA方法，扭矩跟踪更准、能效更高、抗干扰更强。

Conclusion: 该方法为无传感器扭矩控制提供了可扩展、实用的解决方案，适用于动态环境。

Abstract: This paper presents a novel framework for whole-body torque control of
humanoid robots without joint torque sensors, designed for systems with
electric motors and high-ratio harmonic drives. The approach integrates
Physics-Informed Neural Networks (PINNs) for friction modeling and Unscented
Kalman Filtering (UKF) for joint torque estimation, within a real-time torque
control architecture. PINNs estimate nonlinear static and dynamic friction from
joint and motor velocity readings, capturing effects like motor actuation
without joint movement. The UKF utilizes PINN-based friction estimates as
direct measurement inputs, improving torque estimation robustness. Experimental
validation on the ergoCub humanoid robot demonstrates improved torque tracking
accuracy, enhanced energy efficiency, and superior disturbance rejection
compared to the state-of-the-art Recursive Newton-Euler Algorithm (RNEA), using
a dynamic balancing experiment. The framework's scalability is shown by
consistent performance across robots with similar hardware but different
friction characteristics, without re-identification. Furthermore, a comparative
analysis with position control highlights the advantages of the proposed torque
control approach. The results establish the method as a scalable and practical
solution for sensorless torque control in humanoid robots, ensuring torque
tracking, adaptability, and stability in dynamic environments.

</details>


### [36] [Simulations and experiments with assemblies of fiber-reinforced soft actuators](https://arxiv.org/abs/2507.10121)
*Seung Hyun Kim,Jiamiao Guo,Arman Tekinalp,Heng-Sheng Chang,Ugur Akcal,Tixian Wang,Darren Biskup,Benjamin Walt,Girish Chowdhary,Girish Krishnan,Prashant G. Mehta,Mattia Gazzola*

Main category: cs.RO

TL;DR: 开发了一种用于纤维增强弹性体软连续臂（SCAs）的仿真框架，并结合视频跟踪系统进行实验测试和控制设计。


<details>
  <summary>Details</summary>
Motivation: 软连续臂（SCAs）因其机械顺应性在多个领域有应用潜力，但其非线性行为难以控制，限制了实际应用。

Method: 开发了一个模块化组装纤维增强弹性体（FREEs）的仿真框架，并集成视频跟踪系统进行实验和控制设计。

Result: 通过仿真和实验测试，验证了该框架对SCAs非线性行为的控制能力。

Conclusion: 该框架为SCAs的实际应用提供了有效的控制和测试工具。

Abstract: Soft continuum arms (SCAs) promise versatile manipulation through mechanical
compliance, for assistive devices, agriculture, search applications, or
surgery. However, SCAs' real-world use is challenging, partly due to their
hard-to-control non-linear behavior. Here, a simulation framework for SCAs
modularly assembled out of fiber reinforced elastomeric enclosures (FREEs) is
developed and integrated with a video-tracking system for experimental testing
and control design.

</details>


### [37] [Probabilistic Human Intent Prediction for Mobile Manipulation: An Evaluation with Human-Inspired Constraints](https://arxiv.org/abs/2507.10131)
*Cesar Alan Contreras,Manolis Chiou,Alireza Rastegarpanah,Michal Szulik,Rustam Stolkin*

Main category: cs.RO

TL;DR: GUIDER是一个双阶段概率框架，用于机器人推断人类意图，在导航和操作阶段均表现优异。


<details>
  <summary>Details</summary>
Motivation: 提高人机协作中机器人对人类意图的准确推断能力，避免限制人类控制或引发冲突。

Method: GUIDER采用双阶段框架：导航阶段结合控制器速度和占用网格生成协同地图；操作阶段融合U2Net、FastSAM和几何抓取可行性测试，实时更新对象概率。

Result: 在25次试验中，GUIDER导航稳定性达93-100%，操作稳定性达94-100%，显著优于基线方法。

Conclusion: GUIDER验证了双阶段框架的有效性，显著提升了移动操作任务中的意图推断能力。

Abstract: Accurate inference of human intent enables human-robot collaboration without
constraining human control or causing conflicts between humans and robots. We
present GUIDER (Global User Intent Dual-phase Estimation for Robots), a
probabilistic framework that enables a robot to estimate the intent of human
operators. GUIDER maintains two coupled belief layers, one tracking navigation
goals and the other manipulation goals. In the Navigation phase, a Synergy Map
blends controller velocity with an occupancy grid to rank interaction areas.
Upon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.
The Manipulation phase combines U2Net saliency, FastSAM instance saliency, and
three geometric grasp-feasibility tests, with an end-effector kinematics-aware
update rule that evolves object probabilities in real-time. GUIDER can
recognize areas and objects of intent without predefined goals. We evaluated
GUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and
compared it with two baselines, one for navigation and one for manipulation.
Across the 25 trials, GUIDER achieved a median stability of 93-100% during
navigation, compared with 60-100% for the BOIR baseline, with an improvement of
39.5% in a redirection scenario (T5). During manipulation, stability reached
94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a
redirection task (T3). In geometry-constrained trials (manipulation), GUIDER
recognized the object intent three times earlier than Trajectron (median
remaining time to confident prediction 23.6 s vs 7.8 s). These results validate
our dual-phase framework and show improvements in intent inference in both
phases of mobile manipulation tasks.

</details>


### [38] [Robust RL Control for Bipedal Locomotion with Closed Kinematic Chains](https://arxiv.org/abs/2507.10164)
*Egor Maslennikov,Eduard Zaliaev,Nikita Dudorov,Oleg Shamanin,Karanov Dmitry,Gleb Afanasev,Alexey Burkov,Egor Lygin,Simeon Nedelchev,Evgeny Ponomarev*

Main category: cs.RO

TL;DR: 提出了一种强化学习框架，显式结合闭链动力学，显著提升了双足机器人的运动控制鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法将闭链机构简化为串行模型，导致仿真到现实的迁移性能下降，无法捕捉关节耦合等关键特性。

Method: 采用对称感知损失函数、对抗训练和针对性网络正则化，结合闭链动力学进行训练。

Result: 在自定义机器人TopA上验证，实现了跨多种地形的稳定运动，性能显著优于简化模型方法。

Conclusion: 显式结合闭链动力学的强化学习框架能有效提升双足机器人的运动控制鲁棒性。

Abstract: Developing robust locomotion controllers for bipedal robots with closed
kinematic chains presents unique challenges, particularly since most
reinforcement learning (RL) approaches simplify these parallel mechanisms into
serial models during training. We demonstrate that this simplification
significantly impairs sim-to-real transfer by failing to capture essential
aspects such as joint coupling, friction dynamics, and motor-space control
characteristics. In this work, we present an RL framework that explicitly
incorporates closed-chain dynamics and validate it on our custom-built robot
TopA. Our approach enhances policy robustness through symmetry-aware loss
functions, adversarial training, and targeted network regularization.
Experimental results demonstrate that our integrated approach achieves stable
locomotion across diverse terrains, significantly outperforming methods based
on simplified kinematic models.

</details>


### [39] [REACT: Real-time Entanglement-Aware Coverage Path Planning for Tethered Underwater Vehicles](https://arxiv.org/abs/2507.10204)
*Abdelhakim Amer,Mohit Mehindratta,Yury Brodskiy,Bilal Wehbe,Erdal Kayacan*

Main category: cs.RO

TL;DR: REACT框架通过实时几何模型和路径规划，解决了水下机器人因缆绳缠绕导致的检查效率问题，显著提升了任务完成速度和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统水下机器人检查复杂结构时，缆绳缠绕风险限制了效率和安全性。

Method: REACT结合了基于几何的缆绳模型（SDF地图）和实时路径重规划策略，主动避免缠绕。

Result: 仿真和实际实验显示，REACT比传统规划器快20%，且能完成全部任务，而基线规划器因缠绕失败。

Conclusion: REACT框架有效解决了缆绳缠绕问题，提升了水下检查的效率和安全性。

Abstract: Inspection of complex underwater structures with tethered underwater vehicles
is often hindered by the risk of tether entanglement. We propose REACT
(real-time entanglement-aware coverage path planning for tethered underwater
vehicles), a framework designed to overcome this limitation. REACT comprises a
fast geometry-based tether model using the signed distance field (SDF) map for
accurate, real-time simulation of taut tether configurations around arbitrary
structures in 3D. This model enables an efficient online replanning strategy by
enforcing a maximum tether length constraint, thereby actively preventing
entanglement. By integrating REACT into a coverage path planning framework, we
achieve safe and optimal inspection paths, previously challenging due to tether
constraints. The complete REACT framework's efficacy is validated in a pipe
inspection scenario, demonstrating safe, entanglement-free navigation and
full-coverage inspection. Simulation results show that REACT achieves complete
coverage while maintaining tether constraints and completing the total mission
20% faster than conventional planners, despite a longer inspection time due to
proactive avoidance of entanglement that eliminates extensive post-mission
disentanglement. Real-world experiments confirm these benefits, where REACT
completes the full mission, while the baseline planner fails due to physical
tether entanglement.

</details>


### [40] [Prompt Informed Reinforcement Learning for Visual Coverage Path Planning](https://arxiv.org/abs/2507.10284)
*Venkat Margapuri*

Main category: cs.RO

TL;DR: 论文提出了一种名为PIRL的新方法，结合大型语言模型（如GPT-3.5）的零样本推理能力和好奇心驱动的强化学习，用于无人机视觉覆盖路径规划。PIRL通过动态调整奖励函数，显著提升了覆盖率和电池效率。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法依赖于特定环境的奖励设计，缺乏语义适应性。PIRL旨在利用语言模型的语义反馈，动态优化奖励函数，以解决复杂空间探索任务中的覆盖和效率问题。

Method: PIRL结合了大型语言模型的零样本推理能力和好奇心驱动的强化学习（PPO），通过动态调整奖励函数来指导无人机的位置和相机调整。

Result: PIRL在OpenAI Gym和Webots模拟器中表现优异，视觉覆盖率分别提高了14%和27%，电池效率提升25%，冗余降低18%。

Conclusion: PIRL展示了语言模型在强化学习中的潜力，为机器人任务中自然语言先验的整合提供了新方向。

Abstract: Visual coverage path planning with unmanned aerial vehicles (UAVs) requires
agents to strategically coordinate UAV motion and camera control to maximize
coverage, minimize redundancy, and maintain battery efficiency. Traditional
reinforcement learning (RL) methods rely on environment-specific reward
formulations that lack semantic adaptability. This study proposes
Prompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates
the zero-shot reasoning ability and in-context learning capability of large
language models with curiosity-driven RL. PIRL leverages semantic feedback from
an LLM, GPT-3.5, to dynamically shape the reward function of the Proximal
Policy Optimization (PPO) RL policy guiding the agent in position and camera
adjustments for optimal visual coverage. The PIRL agent is trained using OpenAI
Gym and evaluated in various environments. Furthermore, the sim-to-real-like
ability and zero-shot generalization of the agent are tested by operating the
agent in Webots simulator which introduces realistic physical dynamics. Results
show that PIRL outperforms multiple learning-based baselines such as PPO with
static rewards, PPO with exploratory weight initialization, imitation learning,
and an LLM-only controller. Across different environments, PIRL outperforms the
best-performing baseline by achieving up to 14% higher visual coverage in
OpenAI Gym and 27% higher in Webots, up to 25% higher battery efficiency, and
up to 18\% lower redundancy, depending on the environment. The results
highlight the effectiveness of LLM-guided reward shaping in complex spatial
exploration tasks and suggest a promising direction for integrating natural
language priors into RL for robotics.

</details>


### [41] [TOP: Trajectory Optimization via Parallel Optimization towards Constant Time Complexity](https://arxiv.org/abs/2507.10290)
*Jiajun Yu,Nanhe Chen,Guodong Liu,Chao Xu,Fei Gao,Yanjun Cao*

Main category: cs.RO

TL;DR: 提出基于CADMM算法的轨迹优化框架，通过并行计算解决大规模长轨迹优化问题，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹优化方法在处理大规模长轨迹时效率不足，并行计算的应用尚未充分解决该问题。

Method: 使用CADMM算法将轨迹分解为多段并行求解，引入闭式解和数值解处理约束。

Result: 框架将每次迭代的时间复杂度降至O(1)，实验显示效率和平滑性优于现有方法，大规模轨迹提速十倍。

Conclusion: 该框架在并行计算架构（如GPU）上表现优异，适用于超大规模轨迹优化。

Abstract: Optimization has been widely used to generate smooth trajectories for motion
planning. However, existing trajectory optimization methods show weakness when
dealing with large-scale long trajectories. Recent advances in parallel
computing have accelerated optimization in some fields, but how to efficiently
solve trajectory optimization via parallelism remains an open question. In this
paper, we propose a novel trajectory optimization framework based on the
Consensus Alternating Direction Method of Multipliers (CADMM) algorithm, which
decomposes the trajectory into multiple segments and solves the subproblems in
parallel. The proposed framework reduces the time complexity to O(1) per
iteration to the number of segments, compared to O(N) of the state-of-the-art
(SOTA) approaches. Furthermore, we introduce a closed-form solution that
integrates convex linear and quadratic constraints to speed up the
optimization, and we also present numerical solutions for general inequality
constraints. A series of simulations and experiments demonstrate that our
approach outperforms the SOTA approach in terms of efficiency and smoothness.
Especially for a large-scale trajectory, with one hundred segments, achieving
over a tenfold speedup. To fully explore the potential of our algorithm on
modern parallel computing architectures, we deploy our framework on a GPU and
show high performance with thousands of segments.

</details>


### [42] [Polygonal Obstacle Avoidance Combining Model Predictive Control and Fuzzy Logic](https://arxiv.org/abs/2507.10310)
*Michael Schröder,Eric Schöneberg,Daniel Görges,Hans D. Schotten*

Main category: cs.RO

TL;DR: 论文提出了一种将离散的占用网格地图转化为连续可微函数的方法，以解决MPC中障碍物避让约束的兼容性问题。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在受限环境中导航时，通常使用离散的占用网格地图表示障碍物，但MPC需要连续可微的约束函数，两者不兼容。

Method: 通过将障碍物定义为多边形（半空间的交集），并使用模糊逻辑将逻辑运算符转化为不等式约束，使其适用于标准MPC。

Result: 在仿真中成功测试了基于MPC的轨迹规划器，验证了方法的有效性。

Conclusion: 该方法不仅适用于导航任务，还可用于MPC中实现逻辑或语言约束。

Abstract: In practice, navigation of mobile robots in confined environments is often
done using a spatially discrete cost-map to represent obstacles. Path following
is a typical use case for model predictive control (MPC), but formulating
constraints for obstacle avoidance is challenging in this case. Typically the
cost and constraints of an MPC problem are defined as closed-form functions and
typical solvers work best with continuously differentiable functions. This is
contrary to spatially discrete occupancy grid maps, in which a grid's value
defines the cost associated with occupancy. This paper presents a way to
overcome this compatibility issue by re-formulating occupancy grid maps to
continuously differentiable functions to be embedded into the MPC scheme as
constraints. Each obstacle is defined as a polygon -- an intersection of
half-spaces. Any half-space is a linear inequality representing one edge of a
polygon. Using AND and OR operators, the combined set of all obstacles and
therefore the obstacle avoidance constraints can be described. The key
contribution of this paper is the use of fuzzy logic to re-formulate such
constraints that include logical operators as inequality constraints which are
compatible with standard MPC formulation. The resulting MPC-based trajectory
planner is successfully tested in simulation. This concept is also applicable
outside of navigation tasks to implement logical or verbal constraints in MPC.

</details>


### [43] [Raci-Net: Ego-vehicle Odometry Estimation in Adverse Weather Conditions](https://arxiv.org/abs/2507.10376)
*Mohammadhossein Talebi,Pragyan Dahal,Davide Possenti,Stefano Arrigoni,Francesco Braghin*

Main category: cs.RO

TL;DR: 提出了一种基于深度学习的运动估计模型，融合视觉、惯性和毫米波雷达数据，提升恶劣环境下的里程估计精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态环境因素（如天气）下性能下降，需改进传感器融合技术以应对恶劣条件。

Method: 采用深度学习模型，动态调整各传感器贡献，利用雷达弥补视觉传感器在低能见度下的不足。

Result: 在Boreas数据集上的实验表明，模型在清晰和恶劣环境下均表现出鲁棒性和有效性。

Conclusion: 雷达在恶劣天气中的鲁棒性使其成为姿态估计系统的关键组件，尤其在视觉传感器性能下降时。

Abstract: Autonomous driving systems are highly dependent on sensors like cameras,
LiDAR, and inertial measurement units (IMU) to perceive the environment and
estimate their motion. Among these sensors, perception-based sensors are not
protected from harsh weather and technical failures. Although existing methods
show robustness against common technical issues like rotational misalignment
and disconnection, they often degrade when faced with dynamic environmental
factors like weather conditions. To address these problems, this research
introduces a novel deep learning-based motion estimator that integrates visual,
inertial, and millimeter-wave radar data, utilizing each sensor strengths to
improve odometry estimation accuracy and reliability under adverse
environmental conditions such as snow, rain, and varying light. The proposed
model uses advanced sensor fusion techniques that dynamically adjust the
contributions of each sensor based on the current environmental condition, with
radar compensating for visual sensor limitations in poor visibility. This work
explores recent advancements in radar-based odometry and highlights that radar
robustness in different weather conditions makes it a valuable component for
pose estimation systems, specifically when visual sensors are degraded.
Experimental results, conducted on the Boreas dataset, showcase the robustness
and effectiveness of the model in both clear and degraded environments.

</details>


### [44] [Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance](https://arxiv.org/abs/2507.10500)
*Kyungtae Han,Yitao Chen,Rohit Gupta,Onur Altintas*

Main category: cs.RO

TL;DR: SC-ADAS是一个结合生成式AI的模块化框架，通过自然语言对话和场景感知提升ADAS的交互性和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前ADAS缺乏场景理解和自然语言交互能力，限制了其在动态环境中的灵活性。

Method: 集成大型语言模型、视觉到文本解释和结构化函数调用，实现实时、可解释的驾驶员辅助。

Result: 在CARLA模拟器中验证了系统的可行性，但存在延迟和令牌增长等权衡。

Conclusion: SC-ADAS展示了结合对话推理、场景感知和模块化ADAS控制的潜力，支持下一代智能驾驶辅助。

Abstract: While autonomous driving technologies continue to advance, current Advanced
Driver Assistance Systems (ADAS) remain limited in their ability to interpret
scene context or engage with drivers through natural language. These systems
typically rely on predefined logic and lack support for dialogue-based
interaction, making them inflexible in dynamic environments or when adapting to
driver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a
modular framework that integrates Generative AI components including large
language models, vision-to-text interpretation, and structured function calling
to enable real-time, interpretable, and adaptive driver assistance. SC-ADAS
supports multi-turn dialogue grounded in visual and sensor context, allowing
natural language recommendations and driver-confirmed ADAS control. Implemented
in the CARLA simulator with cloud-based Generative AI, the system executes
confirmed user intents as structured ADAS commands without requiring model
fine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and
revisited multi-turn interactions, highlighting trade-offs such as increased
latency from vision-based context retrieval and token growth from accumulated
dialogue history. These results demonstrate the feasibility of combining
conversational reasoning, scene perception, and modular ADAS control to support
the next generation of intelligent driver assistance.

</details>


### [45] [MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation](https://arxiv.org/abs/2507.10543)
*Juyi Sheng,Ziyi Wang,Peiming Li,Mengyuan Liu*

Main category: cs.RO

TL;DR: MP1是一种新型机器人学习方法，通过MeanFlow范式生成动作轨迹，避免了扩散模型的慢采样和Flow方法的架构限制，实现了快速且精确的轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 解决机器人学习中生成模型在采样速度和架构限制之间的权衡问题。

Method: 采用MeanFlow Identity直接学习区间平均速度，结合CFG提升轨迹可控性，并引入Dispersive Loss增强泛化能力。

Result: 在Adroit和Meta-World基准测试中，MP1平均任务成功率比DP3高10.2%，比FlowPolicy高7.3%，推理速度快19倍。

Conclusion: MP1在保持快速推理的同时，显著提升了任务成功率和泛化能力，适用于实际机器人操作场景。

Abstract: In robot manipulation, robot learning has become a prevailing approach.
However, generative models within this field face a fundamental trade-off
between the slow, iterative sampling of diffusion models and the architectural
constraints of faster Flow-based methods, which often rely on explicit
consistency losses. To address these limitations, we introduce MP1, which pairs
3D point-cloud inputs with the MeanFlow paradigm to generate action
trajectories in one network function evaluation (1-NFE). By directly learning
the interval-averaged velocity via the MeanFlow Identity, our policy avoids any
additional consistency constraints. This formulation eliminates numerical
ODE-solver errors during inference, yielding more precise trajectories. MP1
further incorporates CFG for improved trajectory controllability while
retaining 1-NFE inference without reintroducing structural constraints. Because
subtle scene-context variations are critical for robot learning, especially in
few-shot learning, we introduce a lightweight Dispersive Loss that repels state
embeddings during training, boosting generalization without slowing inference.
We validate our method on the Adroit and Meta-World benchmarks, as well as in
real-world scenarios. Experimental results show MP1 achieves superior average
task success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its
average inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster
than FlowPolicy. Our code is available at https://mp1-2254.github.io/.

</details>
