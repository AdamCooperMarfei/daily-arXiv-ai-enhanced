<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 28]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [On the causality between affective impact and coordinated human-robot reactions](https://arxiv.org/abs/2508.04834)
*Morten Roed Frederiksen,Kasper Støy*

Main category: cs.RO

TL;DR: 研究机器人通过主动分享对事件的反应是否影响人类对其情感影响的感知，发现共享反应和特定延迟时间能显著改变感知。


<details>
  <summary>Details</summary>
Motivation: 改善机器人在社交环境中的功能，探究其反应行为对人类感知的影响。

Method: 设计两个实验：一个隔离反应元素，另一个测试不同反应延迟时间；分别与84名和110名参与者互动。

Result: 共享事件反应显著改变感知（p<0.05）；近人类反应时间（200ms）最适合物理互动场景，100ms延迟则让人感觉对机器人影响最大。

Conclusion: 200ms延迟对小型非人形机器人最有效，100ms延迟则让人感觉对机器人影响最大。

Abstract: In an effort to improve how robots function in social contexts, this paper
investigates if a robot that actively shares a reaction to an event with a
human alters how the human perceives the robot's affective impact. To verify
this, we created two different test setups. One to highlight and isolate the
reaction element of affective robot expressions, and one to investigate the
effects of applying specific timing delays to a robot reacting to a physical
encounter with a human. The first test was conducted with two different groups
(n=84) of human observers, a test group and a control group both interacting
with the robot. The second test was performed with 110 participants using
increasingly longer reaction delays for the robot with every ten participants.
The results show a statistically significant change (p$<$.05) in perceived
affective impact for the robots when they react to an event shared with a human
observer rather than reacting at random. The result also shows for shared
physical interaction, the near-human reaction times from the robot are most
appropriate for the scenario. The paper concludes that a delay time around
200ms may render the biggest impact on human observers for small-sized
non-humanoid robots. It further concludes that a slightly shorter reaction time
around 100ms is most effective when the goal is to make the human observers
feel they made the biggest impact on the robot.

</details>


### [2] [INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM](https://arxiv.org/abs/2508.04931)
*Jin Wang,Weijie Wang,Boyuan Deng,Heng Zhang,Rui Dai,Nikos Tsagarakis*

Main category: cs.RO

TL;DR: INTENTION框架通过结合视觉语言模型和交互驱动记忆，赋予机器人直觉式交互和自主操作能力，解决传统方法在真实场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统机器人控制依赖精确模型和预定义动作，难以适应真实世界的复杂性和新任务。人类通过直觉和物理理解高效决策，启发研究者开发类似能力的机器人框架。

Method: 提出INTENTION框架，结合视觉语言模型（VLMs）的场景推理和交互驱动记忆（Memory Graph），并设计Intuitive Perceptor提取物理关系和功能。

Result: 机器人能在新场景中推断适当交互行为，无需重复指令，展现类似人类的适应性和决策能力。

Conclusion: INTENTION框架为机器人提供了直觉式交互和自主操作能力，显著提升了在多样化场景中的适应性和泛化能力。

Abstract: Traditional control and planning for robotic manipulation heavily rely on
precise physical models and predefined action sequences. While effective in
structured environments, such approaches often fail in real-world scenarios due
to modeling inaccuracies and struggle to generalize to novel tasks. In
contrast, humans intuitively interact with their surroundings, demonstrating
remarkable adaptability, making efficient decisions through implicit physical
understanding. In this work, we propose INTENTION, a novel framework enabling
robots with learned interactive intuition and autonomous manipulation in
diverse scenarios, by integrating Vision-Language Models (VLMs) based scene
reasoning with interaction-driven memory. We introduce Memory Graph to record
scenes from previous task interactions which embodies human-like understanding
and decision-making about different tasks in real world. Meanwhile, we design
an Intuitive Perceptor that extracts physical relations and affordances from
visual scenes. Together, these components empower robots to infer appropriate
interaction behaviors in new scenes without relying on repetitive instructions.
Videos: https://robo-intention.github.io

</details>


### [3] [Optimal Planning for Multi-Robot Simultaneous Area and Line Coverage Using Hierarchical Cyclic Merging Regulation](https://arxiv.org/abs/2508.04981)
*Tianyuan Zheng,Jingang Yi,Kaiyan Yu*

Main category: cs.RO

TL;DR: 论文提出了一种基于层次循环合并调节（HCMR）的双重覆盖问题最优规划算法，显著提升了路径长度和任务时间效率。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人在已知环境中同时覆盖线性特征和区域时的高效、无碰撞路径规划问题。

Method: 采用HCMR算法，结合莫尔斯理论分析流形附着过程，通过循环合并搜索和边序列反向传播生成最优路径。

Result: HCMR算法在路径长度上至少提升10.0%，任务时间平均减少16.9%，且确保无冲突操作。

Conclusion: HCMR算法在固定扫描方向下具有最优性，显著优于现有规划方法。

Abstract: The double coverage problem focuses on determining efficient, collision-free
routes for multiple robots to simultaneously cover linear features (e.g.,
surface cracks or road routes) and survey areas (e.g., parking lots or local
regions) in known environments. In these problems, each robot carries two
functional roles: service (linear feature footprint coverage) and exploration
(complete area coverage). Service has a smaller operational footprint but
incurs higher costs (e.g., time) compared to exploration. We present optimal
planning algorithms for the double coverage problems using hierarchical cyclic
merging regulation (HCMR). To reduce the complexity for optimal planning
solutions, we analyze the manifold attachment process during graph traversal
from a Morse theory perspective. We show that solutions satisfying minimum path
length and collision-free constraints must belong to a Morse-bounded
collection. To identify this collection, we introduce the HCMR algorithm. In
HCMR, cyclic merging search regulates traversal behavior, while edge sequence
back propagation converts these regulations into graph edge traversal
sequences. Incorporating balanced partitioning, the optimal sequence is
selected to generate routes for each robot. We prove the optimality of the HCMR
algorithm under a fixed sweep direction. The multi-robot simulation results
demonstrate that the HCMR algorithm significantly improves planned path length
by at least 10.0%, reduces task time by at least 16.9% in average, and ensures
conflict-free operation compared to other state-of-the-art planning methods.

</details>


### [4] [Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots](https://arxiv.org/abs/2508.04994)
*Wenjie Hu,Ye Zhou,Hann Woei Ho*

Main category: cs.RO

TL;DR: 论文提出了一种分层DDPG（HDDPG）算法，通过高低级策略结合改进迷宫导航任务中的性能，显著提升了成功率和平均奖励。


<details>
  <summary>Details</summary>
Motivation: 传统DDPG算法在迷宫导航中因稀疏奖励、低效探索和长时规划问题表现不佳，需改进以提升效率和稳定性。

Method: HDDPG采用高低级策略：高级策略生成子目标，低级策略执行动作，结合离策略校正、自适应噪声和优化奖励函数。

Result: 实验表明，HDDPG在迷宫导航任务中成功率和平均奖励分别提升至少56.59%和519.03。

Conclusion: HDDPG有效解决了传统DDPG的局限性，显著提升了迷宫导航任务的性能。

Abstract: Maze navigation is a fundamental challenge in robotics, requiring agents to
traverse complex environments efficiently. While the Deep Deterministic Policy
Gradient (DDPG) algorithm excels in control tasks, its performance in maze
navigation suffers from sparse rewards, inefficient exploration, and
long-horizon planning difficulties, often leading to low success rates and
average rewards, sometimes even failing to achieve effective navigation. To
address these limitations, this paper proposes an efficient Hierarchical DDPG
(HDDPG) algorithm, which includes high-level and low-level policies. The
high-level policy employs an advanced DDPG framework to generate intermediate
subgoals from a long-term perspective and on a higher temporal scale. The
low-level policy, also powered by the improved DDPG algorithm, generates
primitive actions by observing current states and following the subgoal
assigned by the high-level policy. The proposed method enhances stability with
off-policy correction, refining subgoal assignments by relabeling historical
experiences. Additionally, adaptive parameter space noise is utilized to
improve exploration, and a reshaped intrinsic-extrinsic reward function is
employed to boost learning efficiency. Further optimizations, including
gradient clipping and Xavier initialization, are employed to improve
robustness. The proposed algorithm is rigorously evaluated through numerical
simulation experiments executed using the Robot Operating System (ROS) and
Gazebo. Regarding the three distinct final targets in autonomous maze
navigation tasks, HDDPG significantly overcomes the limitations of standard
DDPG and its variants, improving the success rate by at least 56.59% and
boosting the average reward by a minimum of 519.03 compared to baseline
algorithms.

</details>


### [5] [MAG-Nav: Language-Driven Object Navigation Leveraging Memory-Reserved Active Grounding](https://arxiv.org/abs/2508.05021)
*Weifan Zhang,Tingguang Li,Yuzhen Liu*

Main category: cs.RO

TL;DR: 提出了一种基于视觉语言模型的导航框架，通过主动视角调整和历史记忆回溯提升未知环境中的视觉导航能力，无需微调即可泛化到多样语言描述。


<details>
  <summary>Details</summary>
Motivation: 解决智能机器人在未知环境中仅依赖自然语言描述进行视觉导航的挑战，提升视觉-语言基础能力。

Method: 结合视角主动调整（动态优化视觉输入）和历史记忆回溯（保留并重新评估不确定观察），在零样本设置下工作。

Result: 在HM3D数据集上优于现有方法，并在真实四足机器人上验证了实用性。

Conclusion: 该框架通过主动感知和记忆机制显著提升了复杂环境中的导航性能，展示了零样本泛化的潜力。

Abstract: Visual navigation in unknown environments based solely on natural language
descriptions is a key capability for intelligent robots. In this work, we
propose a navigation framework built upon off-the-shelf Visual Language Models
(VLMs), enhanced with two human-inspired mechanisms: perspective-based active
grounding, which dynamically adjusts the robot's viewpoint for improved visual
inspection, and historical memory backtracking, which enables the system to
retain and re-evaluate uncertain observations over time. Unlike existing
approaches that passively rely on incidental visual inputs, our method actively
optimizes perception and leverages memory to resolve ambiguity, significantly
improving vision-language grounding in complex, unseen environments. Our
framework operates in a zero-shot manner, achieving strong generalization to
diverse and open-ended language descriptions without requiring labeled data or
model fine-tuning. Experimental results on Habitat-Matterport 3D (HM3D) show
that our method outperforms state-of-the-art approaches in language-driven
object navigation. We further demonstrate its practicality through real-world
deployment on a quadruped robot, achieving robust and effective navigation
performance.

</details>


### [6] [Benchmarking Shortcutting Techniques for Multi-Robot-Arm Motion Planning](https://arxiv.org/abs/2508.05027)
*Philip Huang,Yorai Shaoul,Jiaoyang Li*

Main category: cs.RO

TL;DR: 本文研究了多机械臂运动规划中的优化方法，比较了现有捷径技术的优缺点，并提出了两种组合策略以实现最佳性能与运行时间的平衡。


<details>
  <summary>Details</summary>
Motivation: 多机械臂系统的高维度和潜在的碰撞问题使得高质量运动规划具有挑战性，传统方法常产生不理想的运动轨迹，而现有捷径技术的具体方法和性能影响缺乏清晰描述。

Method: 通过定量比较现有捷径技术，分析其优缺点，并提出两种组合策略以优化性能与运行时间的权衡。

Result: 研究提供了对多机械臂轨迹优化的全面分析，展示了不同捷径技术的效果，并验证了组合策略的有效性。

Conclusion: 本文为多机械臂运动规划提供了实用的优化方法，通过组合现有捷径技术实现了更好的性能与效率平衡。

Abstract: Generating high-quality motion plans for multiple robot arms is challenging
due to the high dimensionality of the system and the potential for inter-arm
collisions. Traditional motion planning methods often produce motions that are
suboptimal in terms of smoothness and execution time for multi-arm systems.
Post-processing via shortcutting is a common approach to improve motion quality
for efficient and smooth execution. However, in multi-arm scenarios, optimizing
one arm's motion must not introduce collisions with other arms. Although
existing multi-arm planning works often use some form of shortcutting
techniques, their exact methodology and impact on performance are often vaguely
described. In this work, we present a comprehensive study quantitatively
comparing existing shortcutting methods for multi-arm trajectories across
diverse simulated scenarios. We carefully analyze the pros and cons of each
shortcutting method and propose two simple strategies for combining these
methods to achieve the best performance-runtime tradeoff. Video, code, and
dataset are available at https://philip-huang.github.io/mr-shortcut/.

</details>


### [7] [A Vision-Based Collision Sensing Method for Stable Circular Object Grasping with A Soft Gripper System](https://arxiv.org/abs/2508.05040)
*Boyang Zhang,Jiahui Zuo,Zeyu Duan,Fumin Zhang*

Main category: cs.RO

TL;DR: 提出了一种基于视觉的传感模块，用于检测碰撞以保持软夹持系统的稳定抓取。


<details>
  <summary>Details</summary>
Motivation: 外部碰撞对机器人执行器抓取圆形物体构成风险，需要一种解决方案来确保稳定抓取。

Method: 使用眼在手相机监测手指和抓取物体的运动，并开发了碰撞丰富的抓取策略。

Result: 实验验证了系统能即时响应碰撞，并能精确检测碰撞方向和大小。

Conclusion: 该系统能有效提升软夹持器在动态抓取中的稳定性和安全性。

Abstract: External collisions to robot actuators typically pose risks to grasping
circular objects. This work presents a vision-based sensing module capable of
detecting collisions to maintain stable grasping with a soft gripper system.
The system employs an eye-in-palm camera with a broad field of view to
simultaneously monitor the motion of fingers and the grasped object.
Furthermore, we have developed a collision-rich grasping strategy to ensure the
stability and security of the entire dynamic grasping process. A physical soft
gripper was manufactured and affixed to a collaborative robotic arm to evaluate
the performance of the collision detection mechanism. An experiment regarding
testing the response time of the mechanism confirmed the system has the
capability to react to the collision instantaneously. A dodging test was
conducted to demonstrate the gripper can detect the direction and scale of
external collisions precisely.

</details>


### [8] [Examining the legibility of humanoid robot arm movements in a pointing task](https://arxiv.org/abs/2508.05104)
*Andrej Lúčny,Matilde Antonj,Carlo Mazzola,Hana Hornáčková,Ana Farić,Kristína Malinovská,Michal Vavrecka,Igor Farkaš*

Main category: cs.RO

TL;DR: 研究探讨人形机器人手臂动作的可读性，通过实验验证了多模态优越性和视觉主导假说。


<details>
  <summary>Details</summary>
Motivation: 提升人机交互中机器人动作的可读性，使人类能更准确地预测机器人意图。

Method: 使用NICO人形机器人进行实验，观察参与者对不同手臂动作（如凝视、指向）及截断轨迹的预测能力。

Result: 实验支持多模态优越性和视觉主导假说，表明人类依赖多种线索预测机器人意图。

Conclusion: 机器人动作设计应结合多种线索（如凝视和指向），以提高其可读性和交互效果。

Abstract: Human--robot interaction requires robots whose actions are legible, allowing
humans to interpret, predict, and feel safe around them. This study
investigates the legibility of humanoid robot arm movements in a pointing task,
aiming to understand how humans predict robot intentions from truncated
movements and bodily cues. We designed an experiment using the NICO humanoid
robot, where participants observed its arm movements towards targets on a
touchscreen. Robot cues varied across conditions: gaze, pointing, and pointing
with congruent or incongruent gaze. Arm trajectories were stopped at 60\% or
80\% of their full length, and participants predicted the final target. We
tested the multimodal superiority and ocular primacy hypotheses, both of which
were supported by the experiment.

</details>


### [9] [From Canada to Japan: How 10,000 km Affect User Perception in Robot Teleoperation](https://arxiv.org/abs/2508.05143)
*Siméon Capy,Thomas M. Kwok,Kevin Joseph,Yuichiro Kawasumi,Koichi Nagashima,Tomoya Sasaki,Yue Hu,Eiichi Yoshida*

Main category: cs.RO

TL;DR: 研究探讨了远程机器人操作（RTo）中距离对用户感知的影响，发现远程与本地操作在感知上无显著差异。


<details>
  <summary>Details</summary>
Motivation: 探索远程机器人操作在老年护理中的潜力，研究非专家用户对长距离RTo的感知变化。

Method: 设计了包含问卷调查的协议，并使用ROS和Unity构建软件架构。

Result: 远程与本地机器人操作在用户感知上无显著差异。

Conclusion: 远程机器人操作可能是传统本地控制的有效替代方案。

Abstract: Robot teleoperation (RTo) has emerged as a viable alternative to local
control, particularly when human intervention is still necessary. This research
aims to study the distance effect on user perception in RTo, exploring the
potential of teleoperated robots for older adult care. We propose an evaluation
of non-expert users' perception of long-distance RTo, examining how their
perception changes before and after interaction, as well as comparing it to
that of locally operated robots. We have designed a specific protocol
consisting of multiple questionnaires, along with a dedicated software
architecture using the Robotics Operating System (ROS) and Unity. The results
revealed no statistically significant differences between the local and remote
robot conditions, suggesting that robots may be a viable alternative to
traditional local control.

</details>


### [10] [Chemist Eye: A Visual Language Model-Powered System for Safety Monitoring and Robot Decision-Making in Self-Driving Laboratories](https://arxiv.org/abs/2508.05148)
*Francisco Munguia-Galeano,Zhengxue Zhou,Satheeshkumar Veeramani,Hatem Fakhruldeen,Louis Longley,Rob Clowes,Andrew I. Cooper*

Main category: cs.RO

TL;DR: Chemist Eye是一个分布式安全监控系统，用于提升自驱动实验室（SDL）的安全性和情境感知能力，通过视觉语言模型（VLM）实现实时决策和通信。


<details>
  <summary>Details</summary>
Motivation: 自驱动实验室（SDL）中机器人集成和自动化可能增加安全风险，如火灾和PPE合规问题，需要更高效的安全监控系统。

Method: 系统整合RGB、深度和红外摄像头，利用VLM进行决策，实时监控安全事件，并与移动机器人和第三方消息平台集成。

Result: 在真实SDL环境中测试，Chemist Eye对安全隐患的识别和决策性能分别达到97%和95%。

Conclusion: Chemist Eye能有效提升SDL的安全管理能力，减少潜在风险。

Abstract: The integration of robotics and automation into self-driving laboratories
(SDLs) can introduce additional safety complexities, in addition to those that
already apply to conventional research laboratories. Personal protective
equipment (PPE) is an essential requirement for ensuring the safety and
well-being of workers in laboratories, self-driving or otherwise. Fires are
another important risk factor in chemical laboratories. In SDLs, fires that
occur close to mobile robots, which use flammable lithium batteries, could have
increased severity. Here, we present Chemist Eye, a distributed safety
monitoring system designed to enhance situational awareness in SDLs. The system
integrates multiple stations equipped with RGB, depth, and infrared cameras,
designed to monitor incidents in SDLs. Chemist Eye is also designed to spot
workers who have suffered a potential accident or medical emergency, PPE
compliance and fire hazards. To do this, Chemist Eye uses decision-making
driven by a vision-language model (VLM). Chemist Eye is designed for seamless
integration, enabling real-time communication with robots. Based on the VLM
recommendations, the system attempts to drive mobile robots away from potential
fire locations, exits, or individuals not wearing PPE, and issues audible
warnings where necessary. It also integrates with third-party messaging
platforms to provide instant notifications to lab personnel. We tested Chemist
Eye with real-world data from an SDL equipped with three mobile robots and
found that the spotting of possible safety hazards and decision-making
performances reached 97 % and 95 %, respectively.

</details>


### [11] [FCBV-Net: Category-Level Robotic Garment Smoothing via Feature-Conditioned Bimanual Value Prediction](https://arxiv.org/abs/2508.05153)
*Mohammed Daba,Jing Qiu*

Main category: cs.RO

TL;DR: FCBV-Net通过预训练的几何特征和任务特定策略学习，提升了机器人服装平滑任务的类别级泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器人服装操作中因高维度和类别内变化导致的泛化问题。

Method: 使用3D点云和预训练的几何特征，训练FCBV-Net预测双手机器人动作价值。

Result: 在模拟实验中，FCBV-Net表现优于基线方法，泛化能力更强。

Conclusion: 几何理解与动作价值学习的解耦有助于提升类别级泛化能力。

Abstract: Category-level generalization for robotic garment manipulation, such as
bimanual smoothing, remains a significant hurdle due to high dimensionality,
complex dynamics, and intra-category variations. Current approaches often
struggle, either overfitting with concurrently learned visual features for a
specific instance or, despite category-level perceptual generalization, failing
to predict the value of synergistic bimanual actions. We propose the
Feature-Conditioned Bimanual Value Network (FCBV-Net), operating on 3D point
clouds to specifically enhance category-level policy generalization for garment
smoothing. FCBV-Net conditions bimanual action value prediction on pre-trained,
frozen dense geometric features, ensuring robustness to intra-category garment
variations. Trainable downstream components then learn a task-specific policy
using these static features. In simulated GarmentLab experiments with the
CLOTH3D dataset, FCBV-Net demonstrated superior category-level generalization.
It exhibited only an 11.5% efficiency drop (Steps80) on unseen garments
compared to 96.2% for a 2D image-based baseline, and achieved 89% final
coverage, outperforming an 83% coverage from a 3D correspondence-based baseline
that uses identical per-point geometric features but a fixed primitive. These
results highlight that the decoupling of geometric understanding from bimanual
action value learning enables better category-level generalization.

</details>


### [12] [Learning to See and Act: Task-Aware View Planning for Robotic Manipulation](https://arxiv.org/abs/2508.05186)
*Yongjie Bai,Zhouxia Wang,Yang Liu,Weixing Chen,Ziliang Chen,Mingtong Dai,Yongsen Zheng,Lingbo Liu,Guanbin Li,Liang Lin*

Main category: cs.RO

TL;DR: 提出了一种任务感知视角规划（TAVP）框架，通过主动视角规划和任务特定表征学习，提升3D感知和任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型依赖静态视角和共享视觉编码器，限制了3D感知并导致任务干扰，影响鲁棒性和泛化性。

Method: TAVP结合主动视角规划和任务特定表征学习，使用高效探索策略和混合专家（MoE）视觉编码器。

Result: 在RLBench任务上，TAVP显著优于固定视角方法，生成更完整和区分性强的视觉表征。

Conclusion: TAVP通过任务感知的视觉表征学习，显著提升了动作预测能力和任务泛化性。

Abstract: Recent vision-language-action (VLA) models for multi-task robotic
manipulation commonly rely on static viewpoints and shared visual encoders,
which limit 3D perception and cause task interference, hindering robustness and
generalization. In this work, we propose Task-Aware View Planning (TAVP), a
framework designed to overcome these challenges by integrating active view
planning with task-specific representation learning. TAVP employs an efficient
exploration policy, accelerated by a novel pseudo-environment, to actively
acquire informative views. Furthermore, we introduce a Mixture-of-Experts (MoE)
visual encoder to disentangle features across different tasks, boosting both
representation fidelity and task generalization. By learning to see the world
in a task-aware way, TAVP generates more complete and discriminative visual
representations, demonstrating significantly enhanced action prediction across
a wide array of manipulation challenges. Extensive experiments on RLBench tasks
show that our proposed TAVP model achieves superior performance over
state-of-the-art fixed-view approaches. Visual results and code are provided
at: https://hcplab-sysu.github.io/TAVP.

</details>


### [13] [Dancing with a Robot: An Experimental Study of Child-Robot Interaction in a Performative Art Setting](https://arxiv.org/abs/2508.05208)
*Victor Ngo,Rachel,Ramchurn,Roma Patel,Alan Chamberlain,Ayse Kucukyilmaz*

Main category: cs.RO

TL;DR: 论文评估了儿童与自主机器人NED的互动，发现三个关键挑战，并强调优化人机交互系统的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究儿童与机器人NED在艺术表演中的互动，以了解如何提升人机交互体验。

Method: 通过观察分析18名儿童与NED的互动，总结设计要素和互动挑战。

Result: 发现儿童对机器人充满好奇，但存在互动启动、机器人表达不足和期望未满足等问题。

Conclusion: 需优化人机交互系统，考虑观众能力和期望，以创造更有意义的体验。

Abstract: This paper presents an evaluation of 18 children's in-the-wild experiences
with the autonomous robot arm performer NED (Never-Ending Dancer) within the
Thingamabobas installation, showcased across the UK. We detail NED's design,
including costume, behaviour, and human interactions, all integral to the
installation. Our observational analysis revealed three key challenges in
child-robot interactions: 1) Initiating and maintaining engagement, 2) Lack of
robot expressivity and reciprocity, and 3) Unmet expectations. Our findings
show that children are naturally curious, and adept at interacting with a
robotic art performer. However, our observations emphasise the critical need to
optimise human-robot interaction (HRI) systems through careful consideration of
audience's capabilities, perceptions, and expectations, within the performative
arts context, to enable engaging and meaningful experiences, especially for
young audiences.

</details>


### [14] [Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction](https://arxiv.org/abs/2508.05294)
*Sahar Salimpour,Lei Fu,Farhad Keramat,Leonardo Militano,Giovanni Toffetti,Harry Edelman,Jorge Peña Queralta*

Main category: cs.RO

TL;DR: 本文综述了基础模型（如LLMs和VLMs）在机器人自主性和人机交互中的应用，探讨了VLAs和BLMs如何提升机器人能力，并提出了分类模型集成方法的分类法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索基础模型如何推动机器人系统的智能化和多功能化，特别是在代理架构中的应用。

Method: 方法包括对现有研究的综述，分类模型集成方法，并对不同解决方案中代理的角色进行比较分析。

Result: 结果表明，代理架构使机器人能够通过自然语言指令进行推理、调用API、规划任务序列，并在操作和诊断中提供协助。

Conclusion: 结论指出，基础模型和代理架构正在快速推动机器人技术的发展，社区驱动项目和工业框架也显示出新兴趋势。

Abstract: Foundation models, including large language models (LLMs) and vision-language
models (VLMs), have recently enabled novel approaches to robot autonomy and
human-robot interfaces. In parallel, vision-language-action models (VLAs) or
large behavior models (BLMs) are increasing the dexterity and capabilities of
robotic systems. This survey paper focuses on those words advancing towards
agentic applications and architectures. This includes initial efforts exploring
GPT-style interfaces to tooling, as well as more complex system where AI agents
are coordinators, planners, perception actors, or generalist interfaces. Such
agentic architectures allow robots to reason over natural language
instructions, invoke APIs, plan task sequences, or assist in operations and
diagnostics. In addition to peer-reviewed research, due to the fast-evolving
nature of the field, we highlight and include community-driven projects, ROS
packages, and industrial frameworks that show emerging trends. We propose a
taxonomy for classifying model integration approaches and present a comparative
analysis of the role that agents play in different solutions in today's
literature.

</details>


### [15] [GhostShell: Streaming LLM Function Calls for Concurrent Embodied Programming](https://arxiv.org/abs/2508.05298)
*Jian Gong,Youwei Huang,Bo Yuan,Ming Zhu,Juncheng Zhan,Jinke Wang,Hang Shu,Mingyue Xiong,Yanjun Ye,Yufan Zu,Yang Zhou,Yihan Ding,Xuannian Chen,Xingyu Lu,Runjie Ban,Bingchao Huang,Fusen Liu*

Main category: cs.RO

TL;DR: GhostShell利用LLMs实现实时行为编程，通过动态功能接口和多通道调度器协调机器人行为，显著提升响应速度和任务正确率。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖预定义行为序列，无法灵活适应实时需求，GhostShell旨在通过LLMs实现动态、并发的行为编程。

Method: 采用流式XML功能令牌解析器、动态功能接口映射器和多通道调度器，支持同步和异步功能调用。

Result: 在34个实际任务中，GhostShell的行为正确率高达0.85，响应速度比原生API快66倍，且在长时多模态任务中表现稳健。

Conclusion: GhostShell为实时机器人行为编程提供了高效、灵活的解决方案，具有广泛的应用潜力。

Abstract: We present GhostShell, a novel approach that leverages Large Language Models
(LLMs) to enable streaming and concurrent behavioral programming for embodied
systems. In contrast to conventional methods that rely on pre-scheduled action
sequences or behavior trees, GhostShell drives embodied systems to act
on-the-fly by issuing function calls incrementally as tokens are streamed from
the LLM. GhostShell features a streaming XML function token parser, a dynamic
function interface mapper, and a multi-channel scheduler that orchestrates
intra-channel synchronous and inter-channel asynchronous function calls,
thereby coordinating serial-parallel embodied actions across multiple robotic
components as directed by the LLM. We evaluate GhostShell on our robot
prototype COCO through comprehensive grounded experiments across 34 real-world
interaction tasks and multiple LLMs. The results demonstrate that our approach
achieves state-of-the-art Behavioral Correctness Metric of 0.85 with Claude-4
Sonnet and up to 66X faster response times compared to LLM native function
calling APIs. GhostShell also proves effective in long-horizon multimodal
tasks, demonstrating strong robustness and generalization.

</details>


### [16] [Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control](https://arxiv.org/abs/2508.05342)
*Shunlei Li,Longsen Gao,Jin Wang,Chang Che,Xi Xiao,Jiuwen Cao,Yingbai Hu,Hamid Reza Karimi*

Main category: cs.RO

TL;DR: GF-VLA框架通过信息论和场景图实现双臂机器人从人类视频中学习任务级推理和执行，表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决传统低级别轨迹模仿在机器人学习中的泛化能力不足问题。

Method: 提取任务相关的手和物体信息，编码为场景图，结合语言条件变换器生成行为树和运动指令。

Result: 在双臂任务中，实现了95%的图准确率、93%的子任务分割，任务成功率高达90%。

Conclusion: GF-VLA在空间和语义变化中表现出强大的泛化能力和鲁棒性。

Abstract: Teaching robots dexterous skills from human videos remains challenging due to
the reliance on low-level trajectory imitation, which fails to generalize
across object types, spatial layouts, and manipulator configurations. We
propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables
dual-arm robotic systems to perform task-level reasoning and execution directly
from RGB and Depth human demonstrations. GF-VLA first extracts
Shannon-information-based cues to identify hands and objects with the highest
task relevance, then encodes these cues into temporally ordered scene graphs
that capture both hand-object and object-object interactions. These graphs are
fused with a language-conditioned transformer that generates hierarchical
behavior trees and interpretable Cartesian motion commands. To improve
execution efficiency in bimanual settings, we further introduce a cross-hand
selection policy that infers optimal gripper assignment without explicit
geometric reasoning. We evaluate GF-VLA on four structured dual-arm block
assembly tasks involving symbolic shape construction and spatial
generalization. Experimental results show that the information-theoretic scene
representation achieves over 95 percent graph accuracy and 93 percent subtask
segmentation, supporting the LLM planner in generating reliable and
human-readable task policies. When executed by the dual-arm robot, these
policies yield 94 percent grasp success, 89 percent placement accuracy, and 90
percent overall task success across stacking, letter-building, and geometric
reconfiguration scenarios, demonstrating strong generalization and robustness
across diverse spatial and semantic variations.

</details>


### [17] [Affecta-Context: The Context-Guided Behavior Adaptation Framework](https://arxiv.org/abs/2508.05359)
*Morten Roed Frederiksen,Kasper Støy*

Main category: cs.RO

TL;DR: Affecta-context框架通过物理上下文信息指导社交机器人行为适应，包括上下文表示和行为优先级学习，并在实验中验证了其泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为社交机器人提供一种基于物理上下文的行为适应框架，以优化其在不同环境中的行为表现。

Method: 框架分为两部分：上下文表示和行为优先级学习。通过聚类物理上下文，并在每个上下文中学习行为优先级。

Result: 在72次交互实验中，机器人成功学习并泛化行为优先级，适应新物理上下文。

Conclusion: Affecta-context框架有效提升了社交机器人在不同环境中的行为适应能力。

Abstract: This paper presents Affecta-context, a general framework to facilitate
behavior adaptation for social robots. The framework uses information about the
physical context to guide its behaviors in human-robot interactions. It
consists of two parts: one that represents encountered contexts and one that
learns to prioritize between behaviors through human-robot interactions. As
physical contexts are encountered the framework clusters them by their measured
physical properties. In each context, the framework learns to prioritize
between behaviors to optimize the physical attributes of the robot's behavior
in line with its current environment and the preferences of the users it
interacts with. This paper illlustrates the abilities of the Affecta-context
framework by enabling a robot to autonomously learn the prioritization of
discrete behaviors. This was achieved by training across 72 interactions in two
different physical contexts with 6 different human test participants. The paper
demonstrates the trained Affecta-context framework by verifying the robot's
ability to generalize over the input and to match its behaviors to a previously
unvisited physical context.

</details>


### [18] [A Multi-view Landmark Representation Approach with Application to GNSS-Visual-Inertial Odometry](https://arxiv.org/abs/2508.05368)
*Tong Hua,Jiale Han,Wei Ouyang*

Main category: cs.RO

TL;DR: 提出一种多视角仅姿态估计方法，用于提高GNSS-视觉-惯性里程计（GVIO）的效率，通过紧密耦合的视觉测量模型减少计算负担。


<details>
  <summary>Details</summary>
Motivation: 传统IEKF在联合优化相机姿态和地标时计算负担高，限制了其在多传感器融合中的适用性。

Method: 提出一种多视角仅姿态估计方法，推导出直接关联地标表示与多相机姿态的视觉测量模型，并应用于基于滤波的GVIO。

Result: 仿真和实际实验表明，该方法在效率和精度上优于传统方法。

Conclusion: 所提出的方法显著提高了GVIO的效率，同时保持了高精度。

Abstract: Invariant Extended Kalman Filter (IEKF) has been a significant technique in
vision-aided sensor fusion. However, it usually suffers from high computational
burden when jointly optimizing camera poses and the landmarks. To improve its
efficiency and applicability for multi-sensor fusion, we present a multi-view
pose-only estimation approach with its application to GNSS-Visual-Inertial
Odometry (GVIO) in this paper. Our main contribution is deriving a visual
measurement model which directly associates landmark representation with
multiple camera poses and observations. Such a pose-only measurement is proven
to be tightly-coupled between landmarks and poses, and maintain a perfect null
space that is independent of estimated poses. Finally, we apply the proposed
approach to a filter based GVIO with a novel feature management strategy. Both
simulation tests and real-world experiments are conducted to demonstrate the
superiority of the proposed method in terms of efficiency and accuracy.

</details>


### [19] [Robots can defuse high-intensity conflict situations](https://arxiv.org/abs/2508.05373)
*Morten Roed Frederiksen,Kasper Støy*

Main category: cs.RO

TL;DR: 研究探讨了高强度人机冲突中机器人如何通过五种情感表达方式化解冲突，发现所有方式均有效，但运动方式表现不同。


<details>
  <summary>Details</summary>
Motivation: 理解机器人如何通过情感表达化解人类对其的敌意，尤其是在机器人表现不佳时。

Method: 使用定制的情感机器人和105名测试参与者模拟冲突场景，测试五种情感表达方式的效果。

Result: 所有表达方式均能成功化解冲突，运动方式表现显著不同，但参与者对机器人受影响程度的感知相似。

Conclusion: 化解高强度冲突更需机器人具备社会情境意识和反应能力，而非特定表达方式。

Abstract: This paper investigates the specific scenario of high-intensity
confrontations between humans and robots, to understand how robots can defuse
the conflict. It focuses on the effectiveness of using five different affective
expression modalities as main drivers for defusing the conflict. The aim is to
discover any strengths or weaknesses in using each modality to mitigate the
hostility that people feel towards a poorly performing robot. The defusing of
the situation is accomplished by making the robot better at acknowledging the
conflict and by letting it express remorse. To facilitate the tests, we used a
custom affective robot in a simulated conflict situation with 105 test
participants. The results show that all tested expression modalities can
successfully be used to defuse the situation and convey an acknowledgment of
the confrontation. The ratings were remarkably similar, but the movement
modality was different (ANON p$<$.05) than the other modalities. The test
participants also had similar affective interpretations on how impacted the
robot was of the confrontation across all expression modalities. This indicates
that defusing a high-intensity interaction may not demand special attention to
the expression abilities of the robot, but rather require attention to the
abilities of being socially aware of the situation and reacting in accordance
with it.

</details>


### [20] [Real-Time Iteration Scheme for Diffusion Policy](https://arxiv.org/abs/2508.05396)
*Yufei Duan,Hang Yin,Danica Kragic*

Main category: cs.RO

TL;DR: 论文提出了一种基于实时迭代方案（RTI）的新方法，显著降低了扩散策略的推理时间，无需额外训练或策略重新设计。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在机器人操作任务中表现优异，但其迭代去噪过程导致推理时间长，限制了其在延迟敏感任务中的应用。

Method: 借鉴最优控制中的RTI方案，利用前一时间步的解作为后续迭代的初始猜测，并提出基于缩放的方法处理离散动作。

Result: 实验表明，该方法大幅减少了推理时间，同时保持了与完整去噪步骤相当的总体性能。

Conclusion: 该方法为预训练扩散模型的集成提供了无缝解决方案，尤其适用于资源密集型大模型。

Abstract: Diffusion Policies have demonstrated impressive performance in robotic
manipulation tasks. However, their long inference time, resulting from an
extensive iterative denoising process, and the need to execute an action chunk
before the next prediction to maintain consistent actions limit their
applicability to latency-critical tasks or simple tasks with a short cycle
time. While recent methods explored distillation or alternative policy
structures to accelerate inference, these often demand additional training,
which can be resource-intensive for large robotic models. In this paper, we
introduce a novel approach inspired by the Real-Time Iteration (RTI) Scheme, a
method from optimal control that accelerates optimization by leveraging
solutions from previous time steps as initial guesses for subsequent
iterations. We explore the application of this scheme in diffusion inference
and propose a scaling-based method to effectively handle discrete actions, such
as grasping, in robotic manipulation. The proposed scheme significantly reduces
runtime computational costs without the need for distillation or policy
redesign. This enables a seamless integration into many pre-trained
diffusion-based models, in particular, to resource-demanding large models. We
also provide theoretical conditions for the contractivity which could be useful
for estimating the initial denoising step. Quantitative results from extensive
simulation experiments show a substantial reduction in inference time, with
comparable overall performance compared with Diffusion Policy using full-step
denoising. Our project page with additional resources is available at:
https://rti-dp.github.io/.

</details>


### [21] [DistillDrive: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model](https://arxiv.org/abs/2508.05402)
*Rui Yu,Xianghang Zhang,Runkai Zhao,Huaicheng Yan,Meng Wang*

Main category: cs.RO

TL;DR: DistillDrive是一种基于知识蒸馏的端到端自动驾驶模型，通过多样化实例模仿增强多模式运动特征学习，显著降低碰撞率并提升闭环性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶研究过度关注自车状态，缺乏规划导向的理解，限制了决策过程的鲁棒性。

Method: 利用基于结构化场景表示的规划模型作为教师模型，结合强化学习和生成建模优化状态到决策的映射。

Result: 在nuScenes和NAVSIM数据集上验证，碰撞率降低50%，闭环性能提升3分。

Conclusion: DistillDrive通过知识蒸馏和多目标学习显著提升了自动驾驶的鲁棒性和性能。

Abstract: End-to-end autonomous driving has been recently seen rapid development,
exerting a profound influence on both industry and academia. However, the
existing work places excessive focus on ego-vehicle status as their sole
learning objectives and lacks of planning-oriented understanding, which limits
the robustness of the overall decision-making prcocess. In this work, we
introduce DistillDrive, an end-to-end knowledge distillation-based autonomous
driving model that leverages diversified instance imitation to enhance
multi-mode motion feature learning. Specifically, we employ a planning model
based on structured scene representations as the teacher model, leveraging its
diversified planning instances as multi-objective learning targets for the
end-to-end model. Moreover, we incorporate reinforcement learning to enhance
the optimization of state-to-decision mappings, while utilizing generative
modeling to construct planning-oriented instances, fostering intricate
interactions within the latent space. We validate our model on the nuScenes and
NAVSIM datasets, achieving a 50\% reduction in collision rate and a 3-point
improvement in closed-loop performance compared to the baseline model. Code and
model are publicly available at https://github.com/YuruiAI/DistillDrive

</details>


### [22] [Computational Design and Fabrication of Modular Robots with Untethered Control](https://arxiv.org/abs/2508.05410)
*Manas Bhargava,Takefumi Hiraki,Malina Strugaru,Michal Piovarci,Chiara Daraio,Daisuke Iwai,Bernd Bickel*

Main category: cs.RO

TL;DR: 提出了一种基于分布式驱动的机器人设计框架，结合3D打印骨骼和液晶弹性体肌肉，实现了模块化组装和无线控制，并通过计算工具优化设计和运动。


<details>
  <summary>Details</summary>
Motivation: 模仿自然生物通过肌肉骨骼系统实现的多功能性和适应性，解决现有软机器人系统功能单一、依赖有线控制的问题。

Method: 利用3D打印骨骼和液晶弹性体（LCE）肌肉作为轻量级驱动器，开发了红外响应的LCE杆，实现无线分布式控制。通过计算工具优化骨骼图和运动步态。

Result: 构建了多种机器人，展示了复杂形状变形、多样化控制方案和环境适应性。

Conclusion: 该系统结合模块化材料、无线分布式控制和计算设计，推动了机器人向生物体能力的迈进。

Abstract: Natural organisms use distributed actuation via their musculoskeletal systems
to adapt their gait for traversing diverse terrains or to morph their bodies to
perform varied tasks. A longstanding challenge in the field of robotics is to
mimic this extensive adaptability and range of motion. This has led humans to
develop various soft robotic systems that emulate natural organisms. However,
such systems are generally optimized for a single functionality, lack the
ability to change form or function on demand, or are often tethered to bulky
control systems. To address these challenges, we present our framework for
designing and controlling robots that mimic nature's blueprint by utilizing
distributed actuation. We propose a novel building block that combines
3D-printed bones with liquid crystal elastomer (LCE) muscles as lightweight
actuators and enables the modular assembly of musculoskeletal robots. We
developed LCE rods that contract in response to infrared radiation, thereby
achieving local and untethered control over the distributed network of bones,
which in turn results in global deformation of the robot. Furthermore, to
capitalize on the extensive design space, we develop two computational tools:
one to optimize the robot's skeletal graph, enabling multiple target
deformations, and another to co-optimize the skeletal designs and control gaits
to achieve target locomotion. We validate our system by building several robots
that show complex shape morphing, varying control schemes, and adaptability to
their environment. Our system integrates advances in modular material building,
untethered and distributed control, and computational design to introduce a new
generation of robots that brings us closer to the capabilities of living
organisms.

</details>


### [23] [Do Robots Really Need Anthropomorphic Hands?](https://arxiv.org/abs/2508.05415)
*Alexander Fabisch,Wadhah Zai El Amri,Chandandeep Singh,Nicolás Navarro-Guerrero*

Main category: cs.RO

TL;DR: 论文探讨了人类手的灵巧性是否应为机器人手的理想目标，分析了现有机器人手的复杂性与其实际技能的关系，并提出了简化设计的可能性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨人类手的灵巧性是否应为机器人手的终极目标，以及是否可以通过简化设计实现类似甚至更高的操作能力。

Method: 方法包括对人类手的概述、商业化机器人手的比较，以及对机器人手机制和技能的全面系统综述。

Result: 研究发现，手腕灵活性和手指外展/内收对操作能力至关重要，而增加手指数量或自由度并非必要。三指设计是简单性与灵巧性的良好折中。

Conclusion: 结论表明，人类手并非机器人手的唯一理想模型，非人形设计或更多手指可能提供更高的灵巧性。

Abstract: Human manipulation skills represent a pinnacle of their voluntary motor
functions, requiring the coordination of many degrees of freedom and processing
of high-dimensional sensor input to achieve such a high level of dexterity.
Thus, we set out to answer whether the human hand, with its associated
biomechanical properties, sensors, and control mechanisms, is an ideal that we
should strive for in robotics-do we really need anthropomorphic robotic hands?
  This survey can help practitioners to make the trade-off between hand
complexity and potential manipulation skills. We provide an overview of the
human hand, a comparison of commercially available robotic and prosthetic
hands, and a systematic review of hand mechanisms and skills that they are
capable of. This leads to follow-up questions. What is the minimum requirement
for mechanisms and sensors to implement most skills that a robot needs? What is
missing to reach human-level dexterity? Can we improve upon human dexterity?
  Although complex five-fingered hands are often used as the ultimate goal for
robotic manipulators, they are not necessary for all tasks. We found that wrist
flexibility and finger abduction/adduction are important for manipulation
capabilities. On the contrary, increasing the number of fingers, actuators, or
degrees of freedom is often not necessary. Three fingers are a good compromise
between simplicity and dexterity. Non-anthropomorphic hand designs with two
opposing pairs of fingers or human hands with six fingers can further increase
dexterity, suggesting that the human hand may not be the optimum.

</details>


### [24] [Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation](https://arxiv.org/abs/2508.05535)
*Albert Yu,Chengshu Li,Luca Macesanu,Arnav Balaji,Ruchira Ray,Raymond Mooney,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: MICoBot是一种用于人机协作的混合主动对话系统，通过三层决策机制优化任务分配，显著提升任务成功率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 解决人机协作中因人类伙伴行为变化、意愿差异和能力理解不足导致的适应性问题。

Method: 采用混合主动对话范式，结合元规划器、任务分配器和动作执行器三层决策机制，优化任务分配和协作策略。

Result: 在仿真和真实环境中验证，显著优于纯LLM基线和其他任务分配模型。

Conclusion: MICoBot能有效适应多样化人类伙伴，提升协作效率和用户体验。

Abstract: Effective robotic systems for long-horizon human-robot collaboration must
adapt to a wide range of human partners, whose physical behavior, willingness
to assist, and understanding of the robot's capabilities may change over time.
This demands a tightly coupled communication loop that grants both agents the
flexibility to propose, accept, or decline requests as they coordinate toward
completing the task effectively. We apply a Mixed-Initiative dialog paradigm to
Collaborative human-roBot teaming and propose MICoBot, a system that handles
the common scenario where both agents, using natural language, take initiative
in formulating, accepting, or rejecting proposals on who can best complete
different steps of a task. To handle diverse, task-directed dialog, and find
successful collaborative strategies that minimize human effort, MICoBot makes
decisions at three levels: (1) a meta-planner considers human dialog to
formulate and code a high-level collaboration strategy, (2) a planner optimally
allocates the remaining steps to either agent based on the robot's capabilities
(measured by a simulation-pretrained affordance model) and the human's
estimated availability to help, and (3) an action executor decides the
low-level actions to perform or words to say to the human. Our extensive
evaluations in simulation and real-world -- on a physical robot with 18 unique
human participants over 27 hours -- demonstrate the ability of our method to
effectively collaborate with diverse human users, yielding significantly
improved task success and user experience than a pure LLM baseline and other
agent allocation models. See additional videos and materials at
https://robin-lab.cs.utexas.edu/MicoBot/.

</details>


### [25] [CleanUpBench: Embodied Sweeping and Grasping Benchmark](https://arxiv.org/abs/2508.05543)
*Wenbo Li,Guanting Chen,Tao Zhao,Jiyao Wang,Tianxin Hu,Yuwen Liao,Weixiang Guo,Shenghai Yuan*

Main category: cs.RO

TL;DR: CleanUpBench是一个用于评估移动清洁机器人在现实室内清洁任务中的基准测试，填补了学术研究与实际应用之间的空白。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试多针对复杂人形代理或大规模模拟，缺乏对实际可行的移动清洁机器人的系统评估。

Method: 基于NVIDIA Isaac Sim构建，模拟配备清扫机制和六自由度机械臂的机器人，评估任务完成、空间效率、运动质量等。

Result: 提供了手动设计环境和程序生成布局，支持基线代理和比较研究。

Conclusion: CleanUpBench为日常场景中的具身智能提供了可扩展的测试平台。

Abstract: Embodied AI benchmarks have advanced navigation, manipulation, and reasoning,
but most target complex humanoid agents or large-scale simulations that are far
from real-world deployment. In contrast, mobile cleaning robots with dual mode
capabilities, such as sweeping and grasping, are rapidly emerging as realistic
and commercially viable platforms. However, no benchmark currently exists that
systematically evaluates these agents in structured, multi-target cleaning
tasks, revealing a critical gap between academic research and real-world
applications. We introduce CleanUpBench, a reproducible and extensible
benchmark for evaluating embodied agents in realistic indoor cleaning
scenarios. Built on NVIDIA Isaac Sim, CleanUpBench simulates a mobile service
robot equipped with a sweeping mechanism and a six-degree-of-freedom robotic
arm, enabling interaction with heterogeneous objects. The benchmark includes
manually designed environments and one procedurally generated layout to assess
generalization, along with a comprehensive evaluation suite covering task
completion, spatial efficiency, motion quality, and control performance. To
support comparative studies, we provide baseline agents based on heuristic
strategies and map-based planning. CleanUpBench bridges the gap between
low-level skill evaluation and full-scene testing, offering a scalable testbed
for grounded, embodied intelligence in everyday settings.

</details>


### [26] [Robust adaptive fuzzy sliding mode control for trajectory tracking for of cylindrical manipulator](https://arxiv.org/abs/2508.05584)
*Van Cuong Pham,Minh Hai Tran,Phuc Anh Nguyen,Ngoc Son Vu,Nga Nguyen Thi*

Main category: cs.RO

TL;DR: 提出了一种鲁棒自适应模糊滑模控制（AFSMC）方法，用于提升圆柱形机器人机械臂的轨迹跟踪性能，适用于CNC和3D打印等领域。


<details>
  <summary>Details</summary>
Motivation: 传统方法在轨迹跟踪精度、稳定性和抗干扰能力方面存在不足，需要一种更鲁棒且自适应的控制方法。

Method: 结合模糊逻辑与滑模控制（SMC），模糊逻辑用于近似系统的不确定性动态，SMC确保强鲁棒性。

Result: MATLAB/Simulink仿真显示，AFSMC在轨迹跟踪精度、稳定性和抗干扰能力上显著优于传统方法。

Conclusion: AFSMC在机器人机械臂控制中表现出高效性，有助于提升工业机器人应用的精度。

Abstract: This research proposes a robust adaptive fuzzy sliding mode control (AFSMC)
approach to enhance the trajectory tracking performance of cylindrical robotic
manipulators, extensively utilized in applications such as CNC and 3D printing.
The proposed approach integrates fuzzy logic with sliding mode control (SMC) to
bolster adaptability and robustness, with fuzzy logic approximating the
uncertain dynamics of the system, while SMC ensures strong performance.
Simulation results in MATLAB/Simulink demonstrate that AFSMC significantly
improves trajectory tracking accuracy, stability, and disturbance rejection
compared to traditional methods. This research underscores the effectiveness of
AFSMC in controlling robotic manipulators, contributing to enhanced precision
in industrial robotic applications.

</details>


### [27] [Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling](https://arxiv.org/abs/2508.05634)
*Jianpeng Yao,Xiaopan Zhang,Yu Xia,Zejin Wang,Amit K. Roy-Chowdhury,Jiachen Li*

Main category: cs.RO

TL;DR: 提出一种通过预测不确定性增强观察的强化学习方法，提升移动机器人在人群导航中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习训练的移动机器人在面对分布外场景时性能下降的问题。

Method: 利用自适应共形推理生成预测不确定性估计，并通过约束强化学习指导机器人行为。

Result: 在分布内场景中，成功率提升8.80%，碰撞减少3.72倍，侵入轨迹减少2.43倍；在分布外场景中表现更强鲁棒性。

Conclusion: 该方法在真实机器人实验中验证了其安全性和鲁棒性。

Abstract: Mobile robots navigating in crowds trained using reinforcement learning are
known to suffer performance degradation when faced with out-of-distribution
scenarios. We propose that by properly accounting for the uncertainties of
pedestrians, a robot can learn safe navigation policies that are robust to
distribution shifts. Our method augments agent observations with prediction
uncertainty estimates generated by adaptive conformal inference, and it uses
these estimates to guide the agent's behavior through constrained reinforcement
learning. The system helps regulate the agent's actions and enables it to adapt
to distribution shifts. In the in-distribution setting, our approach achieves a
96.93% success rate, which is over 8.80% higher than the previous
state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times
fewer intrusions into ground-truth human future trajectories. In three
out-of-distribution scenarios, our method shows much stronger robustness when
facing distribution shifts in velocity variations, policy changes, and
transitions from individual to group dynamics. We deploy our method on a real
robot, and experiments show that the robot makes safe and robust decisions when
interacting with both sparse and dense crowds. Our code and videos are
available on https://gen-safe-nav.github.io/.

</details>


### [28] [Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation](https://arxiv.org/abs/2508.05635)
*Yue Liao,Pengfei Zhou,Siyuan Huang,Donglin Yang,Shengcong Chen,Yuxin Jiang,Yue Hu,Jingbin Cai,Si Liu,Jianlan Luo,Liliang Chen,Shuicheng Yan,Maoqing Yao,Guanghui Ren*

Main category: cs.RO

TL;DR: Genie Envisioner (GE) 是一个集成了策略学习、评估和仿真的统一机器人操作平台，基于视频生成框架。


<details>
  <summary>Details</summary>
Motivation: 为机器人操作提供一个统一的、可扩展的基础平台，支持指令驱动的通用智能体开发。

Method: GE-Base 是一个大规模、指令条件的视频扩散模型，GE-Act 通过轻量级解码器将潜在表示映射为可执行动作，GE-Sim 作为神经模拟器生成高保真模拟数据。

Result: GE 平台实现了跨多样化的机器人体的精确和泛化策略推断，并提供了标准化评估基准 EWMBench。

Conclusion: Genie Envisioner 是一个可扩展且实用的基础平台，适用于指令驱动的通用智能体开发。

Abstract: We introduce Genie Envisioner (GE), a unified world foundation platform for
robotic manipulation that integrates policy learning, evaluation, and
simulation within a single video-generative framework. At its core, GE-Base is
a large-scale, instruction-conditioned video diffusion model that captures the
spatial, temporal, and semantic dynamics of real-world robotic interactions in
a structured latent space. Built upon this foundation, GE-Act maps latent
representations to executable action trajectories through a lightweight,
flow-matching decoder, enabling precise and generalizable policy inference
across diverse embodiments with minimal supervision. To support scalable
evaluation and training, GE-Sim serves as an action-conditioned neural
simulator, producing high-fidelity rollouts for closed-loop policy development.
The platform is further equipped with EWMBench, a standardized benchmark suite
measuring visual fidelity, physical consistency, and instruction-action
alignment. Together, these components establish Genie Envisioner as a scalable
and practical foundation for instruction-driven, general-purpose embodied
intelligence. All code, models, and benchmarks will be released publicly.

</details>
