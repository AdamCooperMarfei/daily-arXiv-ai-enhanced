<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 29]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Feedback-MPPI: Fast Sampling-Based MPC via Rollout Differentiation -- Adios low-level controllers](https://arxiv.org/abs/2506.14855)
*Tommaso Belvedere,Michael Ziegltrum,Giulio Turrisi,Valerio Modugno*

Main category: cs.RO

TL;DR: F-MPPI通过结合局部线性反馈增益改进标准MPPI，提升实时控制性能，适用于复杂机器人任务。


<details>
  <summary>Details</summary>
Motivation: 标准MPPI在实时高频机器人控制中因计算需求高而受限，需改进以适应复杂场景。

Method: 引入F-MPPI框架，通过灵敏度分析计算局部线性反馈增益，实现快速闭环校正。

Result: 在四足机器人和四旋翼平台上验证，F-MPPI显著提升控制性能和稳定性。

Conclusion: F-MPPI通过局部反馈实现高效实时控制，适用于复杂机器人系统。

Abstract: Model Predictive Path Integral control is a powerful sampling-based approach
suitable for complex robotic tasks due to its flexibility in handling nonlinear
dynamics and non-convex costs. However, its applicability in real-time,
highfrequency robotic control scenarios is limited by computational demands.
This paper introduces Feedback-MPPI (F-MPPI), a novel framework that augments
standard MPPI by computing local linear feedback gains derived from sensitivity
analysis inspired by Riccati-based feedback used in gradient-based MPC. These
gains allow for rapid closed-loop corrections around the current state without
requiring full re-optimization at each timestep. We demonstrate the
effectiveness of F-MPPI through simulations and real-world experiments on two
robotic platforms: a quadrupedal robot performing dynamic locomotion on uneven
terrain and a quadrotor executing aggressive maneuvers with onboard
computation. Results illustrate that incorporating local feedback significantly
improves control performance and stability, enabling robust, high-frequency
operation suitable for complex robotic systems.

</details>


### [2] [Towards Perception-based Collision Avoidance for UAVs when Guiding the Visually Impaired](https://arxiv.org/abs/2506.14857)
*Suman Raj,Swapnil Padhi,Ruchi Bhoot,Prince Modi,Yogesh Simmhan*

Main category: cs.RO

TL;DR: 无人机通过机载传感器结合机器学习和计算机视觉算法实现自主导航，应用于农业、物流和灾害管理等领域。本文探讨无人机如何辅助视障人士在城市户外环境中导航，提出了一种基于感知的路径规划系统，结合全局规划器。


<details>
  <summary>Details</summary>
Motivation: 研究无人机辅助视障人士导航的潜力，解决其在复杂城市环境中的移动问题。

Method: 提出几何问题表示和多DNN框架，用于无人机和视障人士的障碍物避让，结合局部感知规划和全局GPS地图规划。

Result: 在校园环境中测试了三种场景（人行道、停车区、拥挤街道），验证了算法的可行性。

Conclusion: 系统在复杂城市环境中有效辅助视障人士导航，展示了无人机技术的实用价值。

Abstract: Autonomous navigation by drones using onboard sensors combined with machine
learning and computer vision algorithms is impacting a number of domains,
including agriculture, logistics, and disaster management. In this paper, we
examine the use of drones for assisting visually impaired people (VIPs) in
navigating through outdoor urban environments. Specifically, we present a
perception-based path planning system for local planning around the
neighborhood of the VIP, integrated with a global planner based on GPS and maps
for coarse planning. We represent the problem using a geometric formulation and
propose a multi DNN based framework for obstacle avoidance of the UAV as well
as the VIP. Our evaluations conducted on a drone human system in a university
campus environment verifies the feasibility of our algorithms in three
scenarios; when the VIP walks on a footpath, near parked vehicles, and in a
crowded street.

</details>


### [3] [Efficient and Real-Time Motion Planning for Robotics Using Projection-Based Optimization](https://arxiv.org/abs/2506.14865)
*Xuemin Chi,Hakan Girgin,Tobias Löw,Yangyang Xie,Teng Xue,Jihao Huang,Cheng Hu,Zhitao Liu,Sylvain Calinon*

Main category: cs.RO

TL;DR: 提出了一种高效的一阶方法ALSPG，通过几何投影优化机器人运动生成，显著提升实时性能。


<details>
  <summary>Details</summary>
Motivation: 机器人运动生成涉及复杂几何和多行为需求，现有方法在特定问题或几何约束利用上不足。

Method: 采用ALSPG方法，结合欧几里得投影、Minkowski和基函数，利用几何约束而非完整约束和梯度。

Result: ALSPG在实时性能上显著优于现有方法，并在无约束情况下与二阶方法（如iLQR）竞争。

Conclusion: ALSPG通过几何约束优化，有效提升机器人运动生成的效率和实用性，实验验证了其效果。

Abstract: Generating motions for robots interacting with objects of various shapes is a
complex challenge, further complicated by the robot geometry and multiple
desired behaviors. While current robot programming tools (such as inverse
kinematics, collision avoidance, and manipulation planning) often treat these
problems as constrained optimization, many existing solvers focus on specific
problem domains or do not exploit geometric constraints effectively. We propose
an efficient first-order method, Augmented Lagrangian Spectral Projected
Gradient Descent (ALSPG), which leverages geometric projections via Euclidean
projections, Minkowski sums, and basis functions. We show that by using
geometric constraints rather than full constraints and gradients, ALSPG
significantly improves real-time performance. Compared to second-order methods
like iLQR, ALSPG remains competitive in the unconstrained case. We validate our
method through toy examples and extensive simulations, and demonstrate its
effectiveness on a 7-axis Franka robot, a 6-axis P-Rob robot and a 1:10 scale
car in real-world experiments. Source codes, experimental data and videos are
available on the project webpage: https://sites.google.com/view/alspg-oc

</details>


### [4] [FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild Personalization](https://arxiv.org/abs/2506.14968)
*Rajat Kumar Jenamani,Tom Silver,Ben Dodson,Shiqin Tong,Anthony Song,Yuting Yang,Ziang Liu,Benjamin Howe,Aimee Whitneck,Tapomayukh Bhattacharjee*

Main category: cs.RO

TL;DR: FEAST是一个灵活的用餐辅助系统，旨在满足个体护理对象的独特需求，通过模块化硬件、多样化交互方法和参数化行为树实现个性化。


<details>
  <summary>Details</summary>
Motivation: 解决家庭用餐辅助中因活动多样性、情境复杂性和用户偏好差异带来的挑战。

Method: 采用模块化硬件、多样化交互方法和参数化行为树，结合大型语言模型实现安全透明的个性化。

Result: FEAST在透明和安全的个性化方面表现优异，优于固定定制化的基准系统，并在实际用户研究中验证了其适用性。

Conclusion: FEAST成功满足了个体护理对象的需求，展示了其在真实环境中的实用性和生态效度。

Abstract: Physical caregiving robots hold promise for improving the quality of life of
millions worldwide who require assistance with feeding. However, in-home meal
assistance remains challenging due to the diversity of activities (e.g.,
eating, drinking, mouth wiping), contexts (e.g., socializing, watching TV),
food items, and user preferences that arise during deployment. In this work, we
propose FEAST, a flexible mealtime-assistance system that can be personalized
in-the-wild to meet the unique needs of individual care recipients. Developed
in collaboration with two community researchers and informed by a formative
study with a diverse group of care recipients, our system is guided by three
key tenets for in-the-wild personalization: adaptability, transparency, and
safety. FEAST embodies these principles through: (i) modular hardware that
enables switching between assisted feeding, drinking, and mouth-wiping, (ii)
diverse interaction methods, including a web interface, head gestures, and
physical buttons, to accommodate diverse functional abilities and preferences,
and (iii) parameterized behavior trees that can be safely and transparently
adapted using a large language model. We evaluate our system based on the
personalization requirements identified in our formative study, demonstrating
that FEAST offers a wide range of transparent and safe adaptations and
outperforms a state-of-the-art baseline limited to fixed customizations. To
demonstrate real-world applicability, we conduct an in-home user study with two
care recipients (who are community researchers), feeding them three meals each
across three diverse scenarios. We further assess FEAST's ecological validity
by evaluating with an Occupational Therapist previously unfamiliar with the
system. In all cases, users successfully personalize FEAST to meet their
individual needs and preferences. Website: https://emprise.cs.cornell.edu/feast

</details>


### [5] [Time-Optimized Safe Navigation in Unstructured Environments through Learning Based Depth Completion](https://arxiv.org/abs/2506.14975)
*Jeffrey Mao,Raghuram Cauligi Srinivas,Steven Nogar,Giuseppe Loianno*

Main category: cs.RO

TL;DR: 提出了一种基于轻量传感器的四旋翼无人机实时导航系统，结合视觉深度估计和路径规划，实现复杂环境中的安全自主飞行。


<details>
  <summary>Details</summary>
Motivation: 四旋翼无人机在复杂环境中实现自主导航面临传感器限制和实时计算的挑战，需要轻量且高效的解决方案。

Method: 使用立体和单目学习融合的视觉深度估计构建密集3D地图，并开发快速全局路径规划和轨迹生成框架。

Result: 系统在计算效率和避障能力上优于现有方法，并通过室内外实验验证了其有效性。

Conclusion: 该系统为四旋翼无人机在未知环境中的安全自主导航提供了高效解决方案。

Abstract: Quadrotors hold significant promise for several applications such as
agriculture, search and rescue, and infrastructure inspection. Achieving
autonomous operation requires systems to navigate safely through complex and
unfamiliar environments. This level of autonomy is particularly challenging due
to the complexity of such environments and the need for real-time decision
making especially for platforms constrained by size, weight, and power (SWaP),
which limits flight time and precludes the use of bulky sensors like Light
Detection and Ranging (LiDAR) for mapping. Furthermore, computing globally
optimal, collision-free paths and translating them into time-optimized, safe
trajectories in real time adds significant computational complexity. To address
these challenges, we present a fully onboard, real-time navigation system that
relies solely on lightweight onboard sensors. Our system constructs a dense 3D
map of the environment using a novel visual depth estimation approach that
fuses stereo and monocular learning-based depth, yielding longer-range, denser,
and less noisy depth maps than conventional stereo methods. Building on this
map, we introduce a novel planning and trajectory generation framework capable
of rapidly computing time-optimal global trajectories. As the map is
incrementally updated with new depth information, our system continuously
refines the trajectory to maintain safety and optimality. Both our planner and
trajectory generator outperforms state-of-the-art methods in terms of
computational efficiency and guarantee obstacle-free trajectories. We validate
our system through robust autonomous flight experiments in diverse indoor and
outdoor environments, demonstrating its effectiveness for safe navigation in
previously unknown settings.

</details>


### [6] [Six-DoF Hand-Based Teleoperation for Omnidirectional Aerial Robots](https://arxiv.org/abs/2506.15009)
*Jinjie Li,Jiaxuan Li,Kotaro Kaneko,Liming Shu,Moju Zhao*

Main category: cs.RO

TL;DR: 提出一种利用人类手部全向性和灵活性的空中遥操作系统，通过运动追踪和数据手套实现多模式交互，提升空中机械操作的效率。


<details>
  <summary>Details</summary>
Motivation: 现有遥操作方法未能充分利用全向旋转的自由度，且未充分挖掘人类手指的灵活性。

Method: 系统包括肩部和手部运动追踪标记及数据手套，设计四种交互模式以适应不同任务。

Result: 在真实阀门转动任务中验证了系统的有效性，各模式均对操作有贡献。

Conclusion: 该框架将人类灵活性与空中机器人结合，为复杂环境中的遥操作提供了新思路。

Abstract: Omnidirectional aerial robots offer full 6-DoF independent control over
position and orientation, making them popular for aerial manipulation. Although
advancements in robotic autonomy, operating by human remains essential in
complex aerial environments. Existing teleoperation approaches for multirotors
fail to fully leverage the additional DoFs provided by omnidirectional
rotation. Additionally, the dexterity of human fingers should be exploited for
more engaged interaction. In this work, we propose an aerial teleoperation
system that brings the omnidirectionality of human hands into the unbounded
aerial workspace. Our system includes two motion-tracking marker sets -- one on
the shoulder and one on the hand -- along with a data glove to capture hand
gestures. Using these inputs, we design four interaction modes for different
tasks, including Spherical Mode and Cartesian Mode for long-range moving as
well as Operation Mode and Locking Mode for precise manipulation, where the
hand gestures are utilized for seamless mode switching. We evaluate our system
on a valve-turning task in real world, demonstrating how each mode contributes
to effective aerial manipulation. This interaction framework bridges human
dexterity with aerial robotics, paving the way for enhanced teleoperated aerial
manipulation in unstructured environments.

</details>


### [7] [Context Matters: Learning Generalizable Rewards via Calibrated Features](https://arxiv.org/abs/2506.15012)
*Alexandra Forsey-Smerek,Julie Shah,Andreea Bobu*

Main category: cs.RO

TL;DR: 论文提出了一种显式建模上下文不变偏好与上下文相关特征显著性的方法，显著提高了奖励学习的样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法将每个新上下文视为独立任务，导致数据需求大且效率低。本文观察到上下文影响的是特征显著性而非偏好本身，因此提出显式分离建模以提高效率。

Method: 引入校准特征来捕捉上下文对特征显著性的影响，并设计专门的配对比较查询以高效学习。

Result: 实验显示，该方法比基线方法少用10倍偏好查询即可达到相同奖励准确度，低数据量下性能提升15%。用户研究验证了其有效性。

Conclusion: 显式分离建模偏好与特征显著性能显著提升奖励学习的适应性和个性化能力。

Abstract: A key challenge in reward learning from human input is that desired agent
behavior often changes based on context. Traditional methods typically treat
each new context as a separate task with its own reward function. For example,
if a previously ignored stove becomes too hot to be around, the robot must
learn a new reward from scratch, even though the underlying preference for
prioritizing safety over efficiency remains unchanged. We observe that context
influences not the underlying preference itself, but rather the
$\textit{saliency}$--or importance--of reward features. For instance, stove
heat affects the importance of the robot's proximity, yet the human's safety
preference stays the same. Existing multi-task and meta IRL methods learn
context-dependent representations $\textit{implicitly}$--without distinguishing
between preferences and feature importance--resulting in substantial data
requirements. Instead, we propose $\textit{explicitly}$ modeling
context-invariant preferences separately from context-dependent feature
saliency, creating modular reward representations that adapt to new contexts.
To achieve this, we introduce $\textit{calibrated features}$--representations
that capture contextual effects on feature saliency--and present specialized
paired comparison queries that isolate saliency from preference for efficient
learning. Experiments with simulated users show our method significantly
improves sample efficiency, requiring 10x fewer preference queries than
baselines to achieve equivalent reward accuracy, with up to 15% better
performance in low-data regimes (5-10 queries). An in-person user study (N=12)
demonstrates that participants can effectively teach their unique personal
contextual preferences using our method, enabling more adaptable and
personalized reward learning.

</details>


### [8] [Assigning Multi-Robot Tasks to Multitasking Robots](https://arxiv.org/abs/2506.15032)
*Winston Smith,Andrew Boateng,Taha Shaheen,Yu Zhang*

Main category: cs.RO

TL;DR: 提出了一种新的任务分配框架，支持多任务机器人，考虑了物理约束，并通过加权MAX-SAT和贪心启发式方法实现高效分配。


<details>
  <summary>Details</summary>
Motivation: 现有任务分配方法假设机器人单任务运行，效率低下或不切实际，需支持多任务以提高效率。

Method: 提出任务分配框架，考虑多任务物理约束，采用加权MAX-SAT编译和贪心启发式方法。

Result: 在合成域和模拟场景中验证了多任务的优势，并通过物理实验展示了实际效率提升。

Conclusion: 多任务分配框架显著提升了任务效率，适用于复杂任务交互和实际场景。

Abstract: One simplifying assumption in existing and well-performing task allocation
methods is that the robots are single-tasking: each robot operates on a single
task at any given time. While this assumption is harmless to make in some
situations, it can be inefficient or even infeasible in others. In this paper,
we consider assigning multi-robot tasks to multitasking robots. The key
contribution is a novel task allocation framework that incorporates the
consideration of physical constraints introduced by multitasking. This is in
contrast to the existing work where such constraints are largely ignored. After
formulating the problem, we propose a compilation to weighted MAX-SAT, which
allows us to leverage existing solvers for a solution. A more efficient greedy
heuristic is then introduced. For evaluation, we first compare our methods with
a modern baseline that is efficient for single-tasking robots to validate the
benefits of multitasking in synthetic domains. Then, using a site-clearing
scenario in simulation, we further illustrate the complex task interaction
considered by the multitasking robots in our approach to demonstrate its
performance. Finally, we demonstrate a physical experiment to show how
multitasking enabled by our approach can benefit task efficiency in a realistic
setting.

</details>


### [9] [EmojiVoice: Towards long-term controllable expressivity in robot speech](https://arxiv.org/abs/2506.15085)
*Paige Tuttösí,Shivam Mehta,Zachary Syvenky,Bermet Burkanova,Gustav Eje Henter,Angelica Lim*

Main category: cs.RO

TL;DR: EmojiVoice是一个免费的、可定制的文本转语音工具包，旨在为社交机器人提供长期变化的表达性语音。


<details>
  <summary>Details</summary>
Motivation: 社交机器人通常使用单调的‘表达性’语音，缺乏人类语音中的长期变化，而现有的大模型文本转语音系统难以离线部署。

Method: 通过emoji提示实现细粒度控制表达性，并利用轻量级Matcha-TTS实时生成语音。

Result: 在讲故事任务中，变化的emoji提示提高了语音的感知和表达性，但在助手场景中表达性语音并不受欢迎。

Conclusion: EmojiVoice为社交机器人提供了灵活的表达性语音生成方案，但需根据场景调整表达性。

Abstract: Humans vary their expressivity when speaking for extended periods to maintain
engagement with their listener. Although social robots tend to be deployed with
``expressive'' joyful voices, they lack this long-term variation found in human
speech. Foundation model text-to-speech systems are beginning to mimic the
expressivity in human speech, but they are difficult to deploy offline on
robots. We present EmojiVoice, a free, customizable text-to-speech (TTS)
toolkit that allows social roboticists to build temporally variable, expressive
speech on social robots. We introduce emoji-prompting to allow fine-grained
control of expressivity on a phase level and use the lightweight Matcha-TTS
backbone to generate speech in real-time. We explore three case studies: (1) a
scripted conversation with a robot assistant, (2) a storytelling robot, and (3)
an autonomous speech-to-speech interactive agent. We found that using varied
emoji prompting improved the perception and expressivity of speech over a long
period in a storytelling task, but expressive voice was not preferred in the
assistant use case.

</details>


### [10] [3D Vision-tactile Reconstruction from Infrared and Visible Images for Robotic Fine-grained Tactile Perception](https://arxiv.org/abs/2506.15087)
*Yuankai Lin,Xiaofan Lu,Jiahui Chen,Hua Yang*

Main category: cs.RO

TL;DR: 论文提出了一种改进的视觉触觉传感器（VTS），通过引入棱镜和近红外相机扩展成像通道，解决了曲面触觉感知中的光照不足、重建模糊等问题。


<details>
  <summary>Details</summary>
Motivation: 为了实现类人触觉感知，需要将传统平面VTS扩展到仿生曲面结构，但面临光照不足、重建模糊等技术挑战。

Method: 开发了GelSplitter3D，结合棱镜和近红外相机；提出基于CAD的光度立体神经网络校准触觉几何；设计带边界约束的深度先验信息法修正表面积分误差。

Result: 触觉感知性能提升，法线估计精度提高40%，传感器形状在抓取和操作任务中表现出优势。

Conclusion: 该方法显著提升了曲面触觉传感器的性能，为类人指尖设计提供了有效解决方案。

Abstract: To achieve human-like haptic perception in anthropomorphic grippers, the
compliant sensing surfaces of vision tactile sensor (VTS) must evolve from
conventional planar configurations to biomimetically curved topographies with
continuous surface gradients. However, planar VTSs have challenges when
extended to curved surfaces, including insufficient lighting of surfaces,
blurring in reconstruction, and complex spatial boundary conditions for surface
structures. With an end goal of constructing a human-like fingertip, our
research (i) develops GelSplitter3D by expanding imaging channels with a prism
and a near-infrared (NIR) camera, (ii) proposes a photometric stereo neural
network with a CAD-based normal ground truth generation method to calibrate
tactile geometry, and (iii) devises a normal integration method with boundary
constraints of depth prior information to correcting the cumulative error of
surface integrals. We demonstrate better tactile sensing performance, a 40$\%$
improvement in normal estimation accuracy, and the benefits of sensor shapes in
grasping and manipulation tasks.

</details>


### [11] [DyNaVLM: Zero-Shot Vision-Language Navigation System with Dynamic Viewpoints and Self-Refining Graph Memory](https://arxiv.org/abs/2506.15096)
*Zihe Ji,Huangxuan Lin,Yue Gao*

Main category: cs.RO

TL;DR: DyNaVLM是一种端到端的视觉语言导航框架，利用视觉语言模型（VLM）实现自由目标选择，通过自优化图记忆提升决策能力，无需任务特定训练即可在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于固定的角度或距离间隔，限制了导航的灵活性和适应性。DyNaVLM旨在通过视觉语言推理实现自由目标选择，并提升导航的鲁棒性和泛化能力。

Method: DyNaVLM采用自优化图记忆存储对象位置，支持分布式图更新实现跨机器人记忆共享，并通过检索增强提升VLM的决策能力。系统无需任务特定训练或微调。

Result: DyNaVLM在GOAT和ObjectNav基准测试中表现优异，并通过真实世界测试验证了其鲁棒性和泛化能力。

Conclusion: DyNaVLM通过动态动作空间、协作图记忆和无训练部署的创新，为可扩展的机器人导航建立了新范式，弥合了离散VLN任务与连续现实导航之间的差距。

Abstract: We present DyNaVLM, an end-to-end vision-language navigation framework using
Vision-Language Models (VLM). In contrast to prior methods constrained by fixed
angular or distance intervals, our system empowers agents to freely select
navigation targets via visual-language reasoning. At its core lies a
self-refining graph memory that 1) stores object locations as executable
topological relations, 2) enables cross-robot memory sharing through
distributed graph updates, and 3) enhances VLM's decision-making via retrieval
augmentation. Operating without task-specific training or fine-tuning, DyNaVLM
demonstrates high performance on GOAT and ObjectNav benchmarks. Real-world
tests further validate its robustness and generalization. The system's three
innovations: dynamic action space formulation, collaborative graph memory, and
training-free deployment, establish a new paradigm for scalable embodied robot,
bridging the gap between discrete VLN tasks and continuous real-world
navigation.

</details>


### [12] [I Know You're Listening: Adaptive Voice for HRI](https://arxiv.org/abs/2506.15107)
*Paige Tuttösí*

Main category: cs.RO

TL;DR: 论文探讨了为语言教学机器人开发任务特定的合成语音，提出了三种贡献：轻量级且富有表现力的语音、适应环境的语音调整，以及针对第二语言学习者的清晰语音模式。


<details>
  <summary>Details</summary>
Motivation: 现有研究在语言教学机器人中缺乏任务特定的合成语音，这可能影响教学效果。

Method: 1. 使用微调的Matcha-TTS和表情符号提示创建表现力强的语音；2. 调整语音以适应物理和社交环境；3. 开发针对L2学习者的清晰语音模式。

Result: 1. 语音更具表现力且适合长时间使用；2. 环境适应性调整使语音更合适；3. 清晰模式提高了L2学习者的理解能力。

Conclusion: 提出的方法显著提升了语言教学机器人的语音效果，尤其在表现力、环境适应性和L2学习者支持方面。

Abstract: While the use of social robots for language teaching has been explored, there
remains limited work on a task-specific synthesized voices for language
teaching robots. Given that language is a verbal task, this gap may have severe
consequences for the effectiveness of robots for language teaching tasks. We
address this lack of L2 teaching robot voices through three contributions: 1.
We address the need for a lightweight and expressive robot voice. Using a
fine-tuned version of Matcha-TTS, we use emoji prompting to create an
expressive voice that shows a range of expressivity over time. The voice can
run in real time with limited compute resources. Through case studies, we found
this voice more expressive, socially appropriate, and suitable for long periods
of expressive speech, such as storytelling. 2. We explore how to adapt a
robot's voice to physical and social ambient environments to deploy our voices
in various locations. We found that increasing pitch and pitch rate in noisy
and high-energy environments makes the robot's voice appear more appropriate
and makes it seem more aware of its current environment. 3. We create an
English TTS system with improved clarity for L2 listeners using known
linguistic properties of vowels that are difficult for these listeners. We used
a data-driven, perception-based approach to understand how L2 speakers use
duration cues to interpret challenging words with minimal tense (long) and lax
(short) vowels in English. We found that the duration of vowels strongly
influences the perception for L2 listeners and created an "L2 clarity mode" for
Matcha-TTS that applies a lengthening to tense vowels while leaving lax vowels
unchanged. Our clarity mode was found to be more respectful, intelligible, and
encouraging than base Matcha-TTS while reducing transcription errors in these
challenging tense/lax minimal pairs.

</details>


### [13] [VIMS: A Visual-Inertial-Magnetic-Sonar SLAM System in Underwater Environments](https://arxiv.org/abs/2506.15126)
*Bingbing Zhang,Huan Yin,Shuo Liu,Fumin Zhang,Wen Xu*

Main category: cs.RO

TL;DR: 提出了一种新型水下SLAM系统VIMS，结合低成本单波束声纳和高采样率磁力计，解决了水下环境中的尺度估计和闭环问题，提升了鲁棒性和精度。


<details>
  <summary>Details</summary>
Motivation: 传统视觉-惯性状态估计器在水下环境中面临尺度估计和闭环的挑战，需要更鲁棒的解决方案。

Method: 利用单波束声纳改进尺度估计，结合磁力计和磁场线圈实现视觉-磁力分层闭环识别，平衡局部特征跟踪与闭环计算。

Result: 实验表明VIMS显著提升了水下状态估计的鲁棒性和精度。

Conclusion: VIMS为水下导航提供了一种高效且经济的解决方案。

Abstract: In this study, we present a novel simultaneous localization and mapping
(SLAM) system, VIMS, designed for underwater navigation. Conventional
visual-inertial state estimators encounter significant practical challenges in
perceptually degraded underwater environments, particularly in scale estimation
and loop closing. To address these issues, we first propose leveraging a
low-cost single-beam sonar to improve scale estimation. Then, VIMS integrates a
high-sampling-rate magnetometer for place recognition by utilizing magnetic
signatures generated by an economical magnetic field coil. Building on this, a
hierarchical scheme is developed for visual-magnetic place recognition,
enabling robust loop closure. Furthermore, VIMS achieves a balance between
local feature tracking and descriptor-based loop closing, avoiding additional
computational burden on the front end. Experimental results highlight the
efficacy of the proposed VIMS, demonstrating significant improvements in both
the robustness and accuracy of state estimation within underwater environments.

</details>


### [14] [Booster Gym: An End-to-End Reinforcement Learning Framework for Humanoid Robot Locomotion](https://arxiv.org/abs/2506.15132)
*Yushi Wang,Penghui Chen,Xinyu Han,Feng Wu,Mingguo Zhao*

Main category: cs.RO

TL;DR: 论文提出了一套完整的强化学习代码框架，用于简化人形机器人运动策略从仿真到现实的迁移过程。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在人形机器人运动控制方面取得进展，但将仿真训练的策略迁移到现实机器人仍具挑战性。

Method: 开发了一个涵盖训练到部署全流程的代码框架，包括强化学习方法、领域随机化、奖励函数设计及并行结构处理。

Result: 在Booster T1机器人上验证了框架有效性，实现了全向行走、抗干扰和地形适应能力。

Conclusion: 该框架为机器人社区提供了便捷工具，有望加速人形机器人发展。

Abstract: Recent advancements in reinforcement learning (RL) have led to significant
progress in humanoid robot locomotion, simplifying the design and training of
motion policies in simulation. However, the numerous implementation details
make transferring these policies to real-world robots a challenging task. To
address this, we have developed a comprehensive code framework that covers the
entire process from training to deployment, incorporating common RL training
methods, domain randomization, reward function design, and solutions for
handling parallel structures. This library is made available as a community
resource, with detailed descriptions of its design and experimental results. We
validate the framework on the Booster T1 robot, demonstrating that the trained
policies seamlessly transfer to the physical platform, enabling capabilities
such as omnidirectional walking, disturbance resistance, and terrain
adaptability. We hope this work provides a convenient tool for the robotics
community, accelerating the development of humanoid robots. The code can be
found in https://github.com/BoosterRobotics/booster_gym.

</details>


### [15] [TACT: Humanoid Whole-body Contact Manipulation through Deep Imitation Learning with Tactile Modality](https://arxiv.org/abs/2506.15146)
*Masaki Murooka,Takahiro Hoshi,Kensuke Fukumitsu,Shimpei Masuda,Marwan Hamze,Tomoya Sasaki,Mitsuharu Morisawa,Eiichi Yoshida*

Main category: cs.RO

TL;DR: 开发了一种基于触觉传感器的人形机器人全身接触操控系统TACT，通过模仿学习人类远程操作数据，结合视觉和触觉输入，提升操控的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 人形机器人全身接触操控能增强稳定性和减少负载，但面临运动生成计算成本高和广域接触测量困难等挑战。

Method: 开发了TACT策略，整合关节位置、视觉和触觉输入，并结合重定向和双足模型运动控制。

Result: 实验验证表明，TACT策略能使人形机器人RHP7 Kaleido在保持平衡和行走的同时完成全身接触操控。

Conclusion: 视觉和触觉输入的整合显著提升了涉及广域和精细接触的操控鲁棒性。

Abstract: Manipulation with whole-body contact by humanoid robots offers distinct
advantages, including enhanced stability and reduced load. On the other hand,
we need to address challenges such as the increased computational cost of
motion generation and the difficulty of measuring broad-area contact. We
therefore have developed a humanoid control system that allows a humanoid robot
equipped with tactile sensors on its upper body to learn a policy for
whole-body manipulation through imitation learning based on human teleoperation
data. This policy, named tactile-modality extended ACT (TACT), has a feature to
take multiple sensor modalities as input, including joint position, vision, and
tactile measurements. Furthermore, by integrating this policy with retargeting
and locomotion control based on a biped model, we demonstrate that the
life-size humanoid robot RHP7 Kaleido is capable of achieving whole-body
contact manipulation while maintaining balance and walking. Through detailed
experimental verification, we show that inputting both vision and tactile
modalities into the policy contributes to improving the robustness of
manipulation involving broad and delicate contact.

</details>


### [16] [Human Locomotion Implicit Modeling Based Real-Time Gait Phase Estimation](https://arxiv.org/abs/2506.15150)
*Yuanlong Ji,Xingbang Yang,Ruoqi Zhao,Qihan Ye,Quan Zheng,Yubo Fan*

Main category: cs.RO

TL;DR: 提出了一种基于IMU信号的步态相位估计神经网络，结合时间卷积和Transformer层，通过通道掩码重建预训练策略提升模型泛化能力，实验结果显示其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有步态相位估计方法在复杂地形变化时精度和鲁棒性不足的问题。

Method: 开发了一种基于隐式建模的神经网络，结合时间卷积和Transformer层，并采用通道掩码重建预训练策略。

Result: 在稳定地形和地形变化条件下均表现出色，步态相位RMSE和相位率MAE均优于基线方法。

Conclusion: 该方法为更智能和自适应的外骨骼系统奠定了基础，提升了人机交互的安全性和效率。

Abstract: Gait phase estimation based on inertial measurement unit (IMU) signals
facilitates precise adaptation of exoskeletons to individual gait variations.
However, challenges remain in achieving high accuracy and robustness,
particularly during periods of terrain changes. To address this, we develop a
gait phase estimation neural network based on implicit modeling of human
locomotion, which combines temporal convolution for feature extraction with
transformer layers for multi-channel information fusion. A channel-wise masked
reconstruction pre-training strategy is proposed, which first treats gait phase
state vectors and IMU signals as joint observations of human locomotion, thus
enhancing model generalization. Experimental results demonstrate that the
proposed method outperforms existing baseline approaches, achieving a gait
phase RMSE of $2.729 \pm 1.071%$ and phase rate MAE of $0.037 \pm 0.016%$ under
stable terrain conditions with a look-back window of 2 seconds, and a phase
RMSE of $3.215 \pm 1.303%$ and rate MAE of $0.050 \pm 0.023%$ under terrain
transitions. Hardware validation on a hip exoskeleton further confirms that the
algorithm can reliably identify gait cycles and key events, adapting to various
continuous motion scenarios. This research paves the way for more intelligent
and adaptive exoskeleton systems, enabling safer and more efficient human-robot
interaction across diverse real-world environments.

</details>


### [17] [Robust Instant Policy: Leveraging Student's t-Regression Model for Robust In-context Imitation Learning of Robot Manipulation](https://arxiv.org/abs/2506.15157)
*Hanbit Oh,Andrea M. Salcedo-Vázquez,Ixchel G. Ramirez-Alpizar,Yukiyasu Domae*

Main category: cs.RO

TL;DR: 论文提出了一种名为RIP的鲁棒上下文模仿学习算法，通过使用Student's t回归模型来减少LLM生成的幻觉轨迹，显著提高了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的上下文模仿学习方法存在幻觉问题，导致生成的轨迹偏离演示，影响可靠性。

Method: 提出RIP算法，利用Student's t分布聚合多个候选轨迹，忽略异常值（幻觉），生成鲁棒轨迹。

Result: 在仿真和真实环境中，RIP显著优于现有方法，任务成功率至少提高26%。

Conclusion: RIP通过鲁棒聚合方法有效解决了LLM的幻觉问题，适用于低数据场景的日常任务。

Abstract: Imitation learning (IL) aims to enable robots to perform tasks autonomously
by observing a few human demonstrations. Recently, a variant of IL, called
In-Context IL, utilized off-the-shelf large language models (LLMs) as instant
policies that understand the context from a few given demonstrations to perform
a new task, rather than explicitly updating network models with large-scale
demonstrations. However, its reliability in the robotics domain is undermined
by hallucination issues such as LLM-based instant policy, which occasionally
generates poor trajectories that deviate from the given demonstrations. To
alleviate this problem, we propose a new robust in-context imitation learning
algorithm called the robust instant policy (RIP), which utilizes a Student's
t-regression model to be robust against the hallucinated trajectories of
instant policies to allow reliable trajectory generation. Specifically, RIP
generates several candidate robot trajectories to complete a given task from an
LLM and aggregates them using the Student's t-distribution, which is beneficial
for ignoring outliers (i.e., hallucinations); thereby, a robust trajectory
against hallucinations is generated. Our experiments, conducted in both
simulated and real-world environments, show that RIP significantly outperforms
state-of-the-art IL methods, with at least $26\%$ improvement in task success
rates, particularly in low-data scenarios for everyday tasks. Video results
available at https://sites.google.com/view/robustinstantpolicy.

</details>


### [18] [SHeRLoc: Synchronized Heterogeneous Radar Place Recognition for Cross-Modal Localization](https://arxiv.org/abs/2506.15175)
*Hanjun Kim,Minwoo Jung,Wooseong Yang,Ayoung Kim*

Main category: cs.RO

TL;DR: SHeRLoc是一种专为异构雷达设计的深度网络，通过RCS极坐标匹配和多尺度特征聚合，显著提升了异构雷达地点识别的性能。


<details>
  <summary>Details</summary>
Motivation: 当前研究多局限于同质雷达传感器，忽视了异构雷达技术的整合与跨模态挑战，导致难以泛化到多样雷达数据类型。

Method: 提出SHeRLoc，利用RCS极坐标匹配对齐多模态雷达数据，采用分层最优传输特征聚合生成旋转鲁棒的多尺度描述符，并通过FFT相似性数据挖掘和自适应三元组损失实现FOV感知度量学习。

Result: 在公共数据集上，SHeRLoc将异构雷达地点识别的recall@1从低于0.1提升至0.9，性能优于现有方法。

Conclusion: SHeRLoc不仅适用于雷达，还可扩展至LiDAR，为跨模态地点识别和异构传感器SLAM开辟了新途径。

Abstract: Despite the growing adoption of radar in robotics, the majority of research
has been confined to homogeneous sensor types, overlooking the integration and
cross-modality challenges inherent in heterogeneous radar technologies. This
leads to significant difficulties in generalizing across diverse radar data
types, with modality-aware approaches that could leverage the complementary
strengths of heterogeneous radar remaining unexplored. To bridge these gaps, we
propose SHeRLoc, the first deep network tailored for heterogeneous radar, which
utilizes RCS polar matching to align multimodal radar data. Our hierarchical
optimal transport-based feature aggregation method generates rotationally
robust multi-scale descriptors. By employing FFT-similarity-based data mining
and adaptive margin-based triplet loss, SHeRLoc enables FOV-aware metric
learning. SHeRLoc achieves an order of magnitude improvement in heterogeneous
radar place recognition, increasing recall@1 from below 0.1 to 0.9 on a public
dataset and outperforming state of-the-art methods. Also applicable to LiDAR,
SHeRLoc paves the way for cross-modal place recognition and heterogeneous
sensor SLAM. The source code will be available upon acceptance.

</details>


### [19] [Context-Aware Deep Lagrangian Networks for Model Predictive Control](https://arxiv.org/abs/2506.15249)
*Lucas Schulze,Jan Peters,Oleg Arenz*

Main category: cs.RO

TL;DR: 论文提出了一种基于上下文感知的深度拉格朗日网络（DeLaN）方法，结合在线系统识别和模型预测控制（MPC），用于复杂环境中的机器人控制。实验表明，该方法显著降低了轨迹跟踪误差。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，全局模型难以应对大量不确定的物理对象，因此需要上下文感知的局部模型。物理一致性在单个上下文中仍然重要，尤其是用于MPC时。

Method: 扩展DeLaN为上下文感知模型，结合循环网络进行在线系统识别，并与MPC集成。此外，结合残差动力学模型利用机器人名义模型。

Result: 在7自由度机械臂的轨迹跟踪实验中，该方法将末端执行器跟踪误差降低了39%，优于基线方法的21%改进。

Conclusion: 上下文感知的DeLaN结合MPC和在线识别，显著提升了机器人控制的适应性和物理一致性。

Abstract: Controlling a robot based on physics-informed dynamic models, such as deep
Lagrangian networks (DeLaN), can improve the generalizability and
interpretability of the resulting behavior. However, in complex environments,
the number of objects to potentially interact with is vast, and their physical
properties are often uncertain. This complexity makes it infeasible to employ a
single global model. Therefore, we need to resort to online system
identification of context-aware models that capture only the currently relevant
aspects of the environment. While physical principles such as the conservation
of energy may not hold across varying contexts, ensuring physical plausibility
for any individual context-aware model can still be highly desirable,
particularly when using it for receding horizon control methods such as Model
Predictive Control (MPC). Hence, in this work, we extend DeLaN to make it
context-aware, combine it with a recurrent network for online system
identification, and integrate it with a MPC for adaptive, physics-informed
control. We also combine DeLaN with a residual dynamics model to leverage the
fact that a nominal model of the robot is typically available. We evaluate our
method on a 7-DOF robot arm for trajectory tracking under varying loads. Our
method reduces the end-effector tracking error by 39%, compared to a 21%
improvement achieved by a baseline that uses an extended Kalman filter.

</details>


### [20] [Offensive Robot Cybersecurity](https://arxiv.org/abs/2506.15343)
*Víctor Mayoral-Vilches*

Main category: cs.RO

TL;DR: 论文提出了一种通过自动化赋能进攻性安全方法的新框架，强调通过理解攻击者战术和提前识别漏洞来提升机器人安全性。


<details>
  <summary>Details</summary>
Motivation: 机器人安全需要从攻击者角度出发，提前发现漏洞以构建有效防御。

Method: 结合机器学习和博弈论，优化漏洞识别与利用流程，并开发基于认知引擎的安全工具。

Result: 提出了一种新型网络安全认知引擎架构，支持机器人自主实施进攻性安全策略。

Conclusion: 研究不仅提升了机器人防御能力，还为未来自主防御系统奠定了基础。

Abstract: Offensive Robot Cybersecurity introduces a groundbreaking approach by
advocating for offensive security methods empowered by means of automation. It
emphasizes the necessity of understanding attackers' tactics and identifying
vulnerabilities in advance to develop effective defenses, thereby improving
robots' security posture. This thesis leverages a decade of robotics
experience, employing Machine Learning and Game Theory to streamline the
vulnerability identification and exploitation process. Intrinsically, the
thesis uncovers a profound connection between robotic architecture and
cybersecurity, highlighting that the design and creation aspect of robotics
deeply intertwines with its protection against attacks. This duality -- whereby
the architecture that shapes robot behavior and capabilities also necessitates
a defense mechanism through offensive and defensive cybersecurity strategies --
creates a unique equilibrium. Approaching cybersecurity with a dual perspective
of defense and attack, rooted in an understanding of systems architecture, has
been pivotal. Through comprehensive analysis, including ethical considerations,
the development of security tools, and executing cyber attacks on robot
software, hardware, and industry deployments, this thesis proposes a novel
architecture for cybersecurity cognitive engines. These engines, powered by
advanced game theory and machine learning, pave the way for autonomous
offensive cybersecurity strategies for robots, marking a significant shift
towards self-defending robotic systems. This research not only underscores the
importance of offensive measures in enhancing robot cybersecurity but also sets
the stage for future advancements where robots are not just resilient to cyber
threats but are equipped to autonomously safeguard themselves.

</details>


### [21] [Comparison of Innovative Strategies for the Coverage Problem: Path Planning, Search Optimization, and Applications in Underwater Robotics](https://arxiv.org/abs/2506.15376)
*Ahmed Ibrahim,Francisco F. C. Rego,Éric Busvelle*

Main category: cs.RO

TL;DR: 论文研究了水下滑翔机的覆盖路径规划策略，比较了TSP、MST和OCP三种方法，发现OCP在时间受限时更优，但计算成本高；MST速度快但效果较差。


<details>
  <summary>Details</summary>
Motivation: 提高水下滑翔机在放射性源探测任务中的效率，同时确保安全导航。

Method: 评估了TSP、MST和OCP三种路径规划方法，通过MATLAB模拟比较处理时间、未覆盖区域、路径长度和遍历时间。

Result: OCP在时间受限时表现最佳但计算成本高；MST速度快但效果较差。

Conclusion: 根据任务优先级选择算法，平衡效率和计算可行性。

Abstract: In many applications, including underwater robotics, the coverage problem
requires an autonomous vehicle to systematically explore a defined area while
minimizing redundancy and avoiding obstacles. This paper investigates coverage
path planning strategies to enhance the efficiency of underwater gliders,
particularly in maximizing the probability of detecting a radioactive source
while ensuring safe navigation.
  We evaluate three path-planning approaches: the Traveling Salesman Problem
(TSP), Minimum Spanning Tree (MST), and Optimal Control Problem (OCP).
Simulations were conducted in MATLAB, comparing processing time, uncovered
areas, path length, and traversal time. Results indicate that OCP is preferable
when traversal time is constrained, although it incurs significantly higher
computational costs. Conversely, MST-based approaches provide faster but less
optimal solutions. These findings offer insights into selecting appropriate
algorithms based on mission priorities, balancing efficiency and computational
feasibility.

</details>


### [22] [Efficient Navigation Among Movable Obstacles using a Mobile Manipulator via Hierarchical Policy Learning](https://arxiv.org/abs/2506.15380)
*Taegeun Yang,Jiwoo Hwang,Jeil Jeong,Minsung Yoon,Sung-Eui Yoon*

Main category: cs.RO

TL;DR: 提出了一种分层强化学习框架，用于移动机械臂在动态障碍物环境中的高效导航，结合交互式障碍物属性估计和结构化推动策略。


<details>
  <summary>Details</summary>
Motivation: 解决在动态障碍物环境中导航时，如何高效估计障碍物属性并执行稳定推动的问题。

Method: 采用分层强化学习框架，高层策略生成考虑环境约束和路径跟踪的推动命令，低层策略通过协调全身运动精确执行。

Result: 实验显示，相比基线方法，该方法在任务成功率、路径长度和到达时间上均有提升。

Conclusion: 该方法有效提高了动态障碍物环境中的导航效率，并通过消融实验和定性分析验证了其准确性和可靠性。

Abstract: We propose a hierarchical reinforcement learning (HRL) framework for
efficient Navigation Among Movable Obstacles (NAMO) using a mobile manipulator.
Our approach combines interaction-based obstacle property estimation with
structured pushing strategies, facilitating the dynamic manipulation of
unforeseen obstacles while adhering to a pre-planned global path. The
high-level policy generates pushing commands that consider environmental
constraints and path-tracking objectives, while the low-level policy precisely
and stably executes these commands through coordinated whole-body movements.
Comprehensive simulation-based experiments demonstrate improvements in
performing NAMO tasks, including higher success rates, shortened traversed path
length, and reduced goal-reaching times, compared to baselines. Additionally,
ablation studies assess the efficacy of each component, while a qualitative
analysis further validates the accuracy and reliability of the real-time
obstacle property estimation.

</details>


### [23] [MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System](https://arxiv.org/abs/2506.15402)
*Miaoxin Pan,Jinnan Li,Yaowen Zhang,Yi Yang,Yufeng Yue*

Main category: cs.RO

TL;DR: MCOO-SLAM是一种多相机全方位物体SLAM系统，通过环绕视角相机配置实现复杂户外场景中的鲁棒、一致且语义丰富的建图。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM方法依赖RGB-D或单目相机，视野窄、易受遮挡且深度感知有限，导致物体建模不准确和数据关联不可靠。

Method: 结合点特征和物体级地标，引入语义-几何-时间融合策略，设计全方位闭环模块，构建层次化3D场景图。

Result: 实验显示MCOO-SLAM在定位和物体级建图方面表现优异，对遮挡、姿态变化和环境复杂性具有更强鲁棒性。

Conclusion: MCOO-SLAM为复杂户外场景提供了更可靠、语义丰富的SLAM解决方案。

Abstract: Object-level SLAM offers structured and semantically meaningful environment
representations, making it more interpretable and suitable for high-level
robotic tasks. However, most existing approaches rely on RGB-D sensors or
monocular views, which suffer from narrow fields of view, occlusion
sensitivity, and limited depth perception-especially in large-scale or outdoor
environments. These limitations often restrict the system to observing only
partial views of objects from limited perspectives, leading to inaccurate
object modeling and unreliable data association. In this work, we propose
MCOO-SLAM, a novel Multi-Camera Omnidirectional Object SLAM system that fully
leverages surround-view camera configurations to achieve robust, consistent,
and semantically enriched mapping in complex outdoor scenarios. Our approach
integrates point features and object-level landmarks enhanced with
open-vocabulary semantics. A semantic-geometric-temporal fusion strategy is
introduced for robust object association across multiple views, leading to
improved consistency and accurate object modeling, and an omnidirectional loop
closure module is designed to enable viewpoint-invariant place recognition
using scene-level descriptors. Furthermore, the constructed map is abstracted
into a hierarchical 3D scene graph to support downstream reasoning tasks.
Extensive experiments in real-world demonstrate that MCOO-SLAM achieves
accurate localization and scalable object-level mapping with improved
robustness to occlusion, pose variation, and environmental complexity.

</details>


### [24] [SurfAAV: Design and Implementation of a Novel Multimodal Surfing Aquatic-Aerial Vehicle](https://arxiv.org/abs/2506.15450)
*Kun Liu,Junhao Xiao,Hao Lin,Yue Cao,Hui Peng,Kaihong Huang,Huimin Lu*

Main category: cs.RO

TL;DR: 提出了一种新型多模态水上-空中机器人SurfAAV，整合了水下、水面和空中运动能力，无需浮力调节系统，实现了高效运动和灵活任务执行。


<details>
  <summary>Details</summary>
Motivation: 现有水上-空中机器人难以同时高效完成水下、水面和空中运动，需要一种更灵活的设计。

Method: 设计了差分推力矢量水翼，结合控制算法，实现水下导航、水面滑行和空中飞行。

Result: SurfAAV水面滑行速度达7.96 m/s，水下速度3.1 m/s，性能优于现有机器人。

Conclusion: SurfAAV为多模态运动提供了新解决方案，验证了其高效性和灵活性。

Abstract: Despite significant advancements in the research of aquatic-aerial robots,
existing configurations struggle to efficiently perform underwater, surface,
and aerial movement simultaneously. In this paper, we propose a novel
multimodal surfing aquatic-aerial vehicle, SurfAAV, which efficiently
integrates underwater navigation, surface gliding, and aerial flying
capabilities. Thanks to the design of the novel differential thrust vectoring
hydrofoil, SurfAAV can achieve efficient surface gliding and underwater
navigation without the need for a buoyancy adjustment system. This design
provides flexible operational capabilities for both surface and underwater
tasks, enabling the robot to quickly carry out underwater monitoring
activities. Additionally, when it is necessary to reach another water body,
SurfAAV can switch to aerial mode through a gliding takeoff, flying to the
target water area to perform corresponding tasks. The main contribution of this
letter lies in proposing a new solution for underwater, surface, and aerial
movement, designing a novel hybrid prototype concept, developing the required
control laws, and validating the robot's ability to successfully perform
surface gliding and gliding takeoff. SurfAAV achieves a maximum surface gliding
speed of 7.96 m/s and a maximum underwater speed of 3.1 m/s. The prototype's
surface gliding maneuverability and underwater cruising maneuverability both
exceed those of existing aquatic-aerial vehicles.

</details>


### [25] [Real-Time Initialization of Unknown Anchors for UWB-aided Navigation](https://arxiv.org/abs/2506.15518)
*Giulio Delama,Igor Borowski,Roland Jung,Stephan Weiss*

Main category: cs.RO

TL;DR: 提出一种实时初始化未知UWB锚点的框架，适用于UWB辅助导航系统，通过自动检测和校准提升鲁棒性和实用性。


<details>
  <summary>Details</summary>
Motivation: 解决UWB锚点在导航系统中需手动设置的问题，提升自动化水平和适应性。

Method: 结合在线PDOP估计、轻量级异常检测和自适应鲁棒核优化，实现自动初始化。

Result: 在自主叉车和四旋翼机器人上验证，显示初始化鲁棒且定位误差低。

Conclusion: 方法显著优于现有技术，开源代码便于实际应用。

Abstract: This paper presents a framework for the real-time initialization of unknown
Ultra-Wideband (UWB) anchors in UWB-aided navigation systems. The method is
designed for localization solutions where UWB modules act as supplementary
sensors. Our approach enables the automatic detection and calibration of
previously unknown anchors during operation, removing the need for manual
setup. By combining an online Positional Dilution of Precision (PDOP)
estimation, a lightweight outlier detection method, and an adaptive robust
kernel for non-linear optimization, our approach significantly improves
robustness and suitability for real-world applications compared to
state-of-the-art. In particular, we show that our metric which triggers an
initialization decision is more conservative than current ones commonly based
on initial linear or non-linear initialization guesses. This allows for better
initialization geometry and subsequently lower initialization errors. We
demonstrate the proposed approach on two different mobile robots: an autonomous
forklift and a quadcopter equipped with a UWB-aided Visual-Inertial Odometry
(VIO) framework. The results highlight the effectiveness of the proposed method
with robust initialization and low positioning error. We open-source our code
in a C++ library including a ROS wrapper.

</details>


### [26] [Aerial Grasping via Maximizing Delta-Arm Workspace Utilization](https://arxiv.org/abs/2506.15539)
*Haoran Chen,Weiliang Deng,Biyu Ye,Yifan Xiong,Ximin Lyu*

Main category: cs.RO

TL;DR: 提出了一种新的空中抓取规划框架，通过优化机械臂轨迹和任务约束，最大化工作空间利用率，提升操作效率。


<details>
  <summary>Details</summary>
Motivation: 工作空间限制了机械臂系统的操作能力和运动范围，最大化工作空间利用率可以提高空中操作任务的灵活性和效率。

Method: 利用多层感知机（MLP）映射位置点到可行性概率，并采用可逆残差网络（RevNet）近似复杂的前向运动学，消除工作空间约束。

Result: 通过仿真和真实实验验证了方法的有效性。

Conclusion: 提出的框架显著提升了空中抓取任务的工作空间利用率和操作效率。

Abstract: The workspace limits the operational capabilities and range of motion for the
systems with robotic arms. Maximizing workspace utilization has the potential
to provide more optimal solutions for aerial manipulation tasks, increasing the
system's flexibility and operational efficiency. In this paper, we introduce a
novel planning framework for aerial grasping that maximizes workspace
utilization. We formulate an optimization problem to optimize the aerial
manipulator's trajectory, incorporating task constraints to achieve efficient
manipulation. To address the challenge of incorporating the delta arm's
non-convex workspace into optimization constraints, we leverage a Multilayer
Perceptron (MLP) to map position points to feasibility
probabilities.Furthermore, we employ Reversible Residual Networks (RevNet) to
approximate the complex forward kinematics of the delta arm, utilizing
efficient model gradients to eliminate workspace constraints. We validate our
methods in simulations and real-world experiments to demonstrate their
effectiveness.

</details>


### [27] [GRIM: Task-Oriented Grasping with Conditioning on Generative Examples](https://arxiv.org/abs/2506.15607)
*Shailesh,Alok Raj,Nayan Kumar,Priya Shukla,Andrew Melnik,Micheal Beetz,Gora Chand Nandi*

Main category: cs.RO

TL;DR: GRIM是一种无需训练的任务导向抓取框架，通过几何线索和PCA降维特征进行粗对齐，再通过任务无关的几何稳定抓取进行细化，表现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 任务导向抓取（TOG）需要理解任务语义、对象功能及抓取约束，现有方法泛化能力有限。

Method: GRIM框架分两步：1）基于几何和PCA降维特征的粗对齐；2）任务无关的几何稳定抓取细化。

Result: GRIM仅需少量示例即可实现鲁棒性能，泛化能力强。

Conclusion: GRIM为任务导向抓取提供了一种高效且无需训练的解决方案。

Abstract: Task-Oriented Grasping (TOG) presents a significant challenge, requiring a
nuanced understanding of task semantics, object affordances, and the functional
constraints dictating how an object should be grasped for a specific task. To
address these challenges, we introduce GRIM (Grasp Re-alignment via Iterative
Matching), a novel training-free framework for task-oriented grasping.
Initially, a coarse alignment strategy is developed using a combination of
geometric cues and principal component analysis (PCA)-reduced DINO features for
similarity scoring. Subsequently, the full grasp pose associated with the
retrieved memory instance is transferred to the aligned scene object and
further refined against a set of task-agnostic, geometrically stable grasps
generated for the scene object, prioritizing task compatibility. In contrast to
existing learning-based methods, GRIM demonstrates strong generalization
capabilities, achieving robust performance with only a small number of
conditioning examples.

</details>


### [28] [Vision in Action: Learning Active Perception from Human Demonstrations](https://arxiv.org/abs/2506.15666)
*Haoyu Xiong,Xiaomeng Xu,Jimmy Wu,Yifan Hou,Jeannette Bohg,Shuran Song*

Main category: cs.RO

TL;DR: ViA是一个用于双手机器人操作的活动感知系统，通过学习人类演示的任务相关感知策略，结合6自由度机器人颈部和VR远程操作界面，显著优于基线系统。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够学习人类主动感知策略的机器人系统，以解决复杂双手操作任务中的视觉遮挡问题。

Method: 使用6自由度机器人颈部实现灵活头部运动，设计VR远程操作界面以共享观察空间，并通过3D场景表示减少VR运动病。

Result: 在涉及视觉遮挡的三个复杂多阶段双手操作任务中，ViA显著优于基线系统。

Conclusion: ViA通过结合硬件设计和VR界面，成功学习了鲁棒的视觉运动策略，为复杂机器人操作任务提供了有效解决方案。

Abstract: We present Vision in Action (ViA), an active perception system for bimanual
robot manipulation. ViA learns task-relevant active perceptual strategies
(e.g., searching, tracking, and focusing) directly from human demonstrations.
On the hardware side, ViA employs a simple yet effective 6-DoF robotic neck to
enable flexible, human-like head movements. To capture human active perception
strategies, we design a VR-based teleoperation interface that creates a shared
observation space between the robot and the human operator. To mitigate VR
motion sickness caused by latency in the robot's physical movements, the
interface uses an intermediate 3D scene representation, enabling real-time view
rendering on the operator side while asynchronously updating the scene with the
robot's latest observations. Together, these design elements enable the
learning of robust visuomotor policies for three complex, multi-stage bimanual
manipulation tasks involving visual occlusions, significantly outperforming
baseline systems.

</details>


### [29] [Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos](https://arxiv.org/abs/2506.15680)
*Kaifeng Zhang,Baoyu Li,Kris Hauser,Yunzhu Li*

Main category: cs.RO

TL;DR: 提出了一种结合粒子与空间网格的神经动力学框架，用于建模可变形物体的动态行为，并通过实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 可变形物体的动态建模因物理特性多样且视觉信息有限而具有挑战性。

Method: 采用粒子-网格混合表示，结合高斯渲染，实现全学习化的数字孪生模型。

Result: 模型能够从稀疏视角的RGB-D数据中学习多样物体的动态，并在类别级别泛化，性能优于现有方法。

Conclusion: 该框架在有限视角下表现出色，并支持基于模型的规划任务。

Abstract: Modeling the dynamics of deformable objects is challenging due to their
diverse physical properties and the difficulty of estimating states from
limited visual information. We address these challenges with a neural dynamics
framework that combines object particles and spatial grids in a hybrid
representation. Our particle-grid model captures global shape and motion
information while predicting dense particle movements, enabling the modeling of
objects with varied shapes and materials. Particles represent object shapes,
while the spatial grid discretizes the 3D space to ensure spatial continuity
and enhance learning efficiency. Coupled with Gaussian Splattings for visual
rendering, our framework achieves a fully learning-based digital twin of
deformable objects and generates 3D action-conditioned videos. Through
experiments, we demonstrate that our model learns the dynamics of diverse
objects -- such as ropes, cloths, stuffed animals, and paper bags -- from
sparse-view RGB-D recordings of robot-object interactions, while also
generalizing at the category level to unseen instances. Our approach
outperforms state-of-the-art learning-based and physics-based simulators,
particularly in scenarios with limited camera views. Furthermore, we showcase
the utility of our learned models in model-based planning, enabling
goal-conditioned object manipulation across a range of tasks. The project page
is available at https://kywind.github.io/pgnd .

</details>
