<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 21]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Decision-Making-Based Path Planning for Autonomous UAVs: A Survey](https://arxiv.org/abs/2508.09304)
*Kelen C. Teixeira Vivaldini,Robert Pěnička,Martin Saska*

Main category: cs.RO

TL;DR: 综述探讨了自主无人机路径规划中的决策问题，总结了探索路径规划和信息路径规划的研究现状，并指出了该领域的挑战。


<details>
  <summary>Details</summary>
Motivation: 自主无人机需基于环境信息做出决策以应对系统与环境的不确定性，决策能力是其成功运行的关键。

Method: 综述分析了现有研究中决策在路径规划中的应用，重点关注数据建模和理解方式。

Result: 总结了探索路径规划和信息路径规划的研究现状，并分析了数据建模的特点。

Conclusion: 指出了该领域现有挑战，为未来研究提供了方向。

Abstract: One of the most critical features for the successful operation of autonomous
UAVs is the ability to make decisions based on the information acquired from
their surroundings. Each UAV must be able to make decisions during the flight
in order to deal with uncertainties in its system and the environment, and to
further act upon the information being received. Such decisions influence the
future behavior of the UAV, which is expressed as the path plan. Thus,
decision-making in path planning is an enabling technique for deploying
autonomous UAVs in real-world applications. This survey provides an overview of
existing studies that use aspects of decision-making in path planning,
presenting the research strands for Exploration Path Planning and Informative
Path Planning, and focusing on characteristics of how data have been modeled
and understood. Finally, we highlight the existing challenges for relevant
topics in this field.

</details>


### [2] [How Safe Will I Be Given What I Saw? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy](https://arxiv.org/abs/2508.09346)
*Zhenjiang Mao,Mrinall Eashaan Umasudhan,Ivan Ruchkin*

Main category: cs.RO

TL;DR: 提出了一种基于世界模型的校准安全预测框架，用于端到端视觉控制系统，解决了部分可观测性和分布偏移下的安全预测问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于模型的验证方法在可扩展性和低维状态模型访问上受限，而基于模型的方法缺乏可靠性保证。

Method: 利用变分自编码器和循环预测器从原始图像序列预测潜在轨迹，并估计满足安全属性的概率，同时引入校准机制和域适应技术。

Result: 实验表明，配备域适应的评估器在分布偏移下保持高准确性和低误报率，世界模型复合预测器在长时任务中表现更优。

Conclusion: 该框架为端到端视觉控制系统提供了理论校准保证和实际评估支持，显著提升了安全预测的可靠性。

Abstract: Autonomous robots that rely on deep neural network controllers pose critical
challenges for safety prediction, especially under partial observability and
distribution shift. Traditional model-based verification techniques are limited
in scalability and require access to low-dimensional state models, while
model-free methods often lack reliability guarantees. This paper addresses
these limitations by introducing a framework for calibrated safety prediction
in end-to-end vision-controlled systems, where neither the state-transition
model nor the observation model is accessible. Building on the foundation of
world models, we leverage variational autoencoders and recurrent predictors to
forecast future latent trajectories from raw image sequences and estimate the
probability of satisfying safety properties. We distinguish between monolithic
and composite prediction pipelines and introduce a calibration mechanism to
quantify prediction confidence. In long-horizon predictions from
high-dimensional observations, the forecasted inputs to the safety evaluator
can deviate significantly from the training distribution due to compounding
prediction errors and changing environmental conditions, leading to
miscalibrated risk estimates. To address this, we incorporate unsupervised
domain adaptation to ensure robustness of safety evaluation under distribution
shift in predictions without requiring manual labels. Our formulation provides
theoretical calibration guarantees and supports practical evaluation across
long prediction horizons. Experimental results on three benchmarks show that
our UDA-equipped evaluators maintain high accuracy and substantially lower
false positive rates under distribution shift. Similarly, world model-based
composite predictors outperform their monolithic counterparts on long-horizon
tasks, and our conformal calibration provides reliable statistical bounds.

</details>


### [3] [CLF-RL: Control Lyapunov Function Guided Reinforcement Learning](https://arxiv.org/abs/2508.09354)
*Kejun Li,Zachary Olkin,Yisong Yue,Aaron D. Ames*

Main category: cs.RO

TL;DR: 提出了一种基于模型轨迹生成和控制Lyapunov函数（CLF）的结构化奖励框架，用于强化学习（RL）策略训练，显著提升了双足机器人运动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统RL在双足机器人运动控制中常因奖励设计复杂和目标函数敏感性问题表现不佳，需要更高效的奖励设计方法。

Method: 结合线性倒立摆（LIP）模型和混合零动力学（HZD）预计算步态库生成参考轨迹，并利用CLF构建奖励函数，惩罚跟踪误差并促进快速收敛。

Result: 在仿真和Unitree G1机器人实验中，CLF-RL方法比基线RL策略和经典跟踪奖励RL表现更优，鲁棒性显著提升。

Conclusion: 结构化奖励框架有效解决了RL在双足机器人运动控制中的挑战，且训练后策略轻量，适用于实际部署。

Abstract: Reinforcement learning (RL) has shown promise in generating robust locomotion
policies for bipedal robots, but often suffers from tedious reward design and
sensitivity to poorly shaped objectives. In this work, we propose a structured
reward shaping framework that leverages model-based trajectory generation and
control Lyapunov functions (CLFs) to guide policy learning. We explore two
model-based planners for generating reference trajectories: a reduced-order
linear inverted pendulum (LIP) model for velocity-conditioned motion planning,
and a precomputed gait library based on hybrid zero dynamics (HZD) using
full-order dynamics. These planners define desired end-effector and joint
trajectories, which are used to construct CLF-based rewards that penalize
tracking error and encourage rapid convergence. This formulation provides
meaningful intermediate rewards, and is straightforward to implement once a
reference is available. Both the reference trajectories and CLF shaping are
used only during training, resulting in a lightweight policy at deployment. We
validate our method both in simulation and through extensive real-world
experiments on a Unitree G1 robot. CLF-RL demonstrates significantly improved
robustness relative to the baseline RL policy and better performance than a
classic tracking reward RL formulation.

</details>


### [4] [DAgger Diffusion Navigation: DAgger Boosted Diffusion Policy for Vision-Language Navigation](https://arxiv.org/abs/2508.09444)
*Haoxiang Shi,Xiang Deng,Zaijing Li,Gongwei Chen,Yaowei Wang,Liqiang Nie*

Main category: cs.RO

TL;DR: 论文提出了一种名为DifNav的端到端优化方法，通过统一的扩散策略解决了传统两阶段导航框架的全局次优化和性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段导航框架存在全局次优化和性能瓶颈问题，DifNav旨在通过端到端优化解决这些问题。

Method: DifNav采用条件扩散策略直接建模连续导航空间中的多模态动作分布，结合DAgger进行在线训练和专家轨迹增强。

Result: 实验表明，DifNav在导航性能上显著优于现有两阶段方法。

Conclusion: DifNav通过端到端优化和扩散策略，有效提升了导航性能，无需依赖两阶段框架。

Abstract: Vision-Language Navigation in Continuous Environments (VLN-CE) requires
agents to follow natural language instructions through free-form 3D spaces.
Existing VLN-CE approaches typically use a two-stage waypoint planning
framework, where a high-level waypoint predictor generates the navigable
waypoints, and then a navigation planner suggests the intermediate goals in the
high-level action space. However, this two-stage decomposition framework
suffers from: (1) global sub-optimization due to the proxy objective in each
stage, and (2) a performance bottleneck caused by the strong reliance on the
quality of the first-stage predicted waypoints. To address these limitations,
we propose DAgger Diffusion Navigation (DifNav), an end-to-end optimized VLN-CE
policy that unifies the traditional two stages, i.e. waypoint generation and
planning, into a single diffusion policy. Notably, DifNav employs a conditional
diffusion policy to directly model multi-modal action distributions over future
actions in continuous navigation space, eliminating the need for a waypoint
predictor while enabling the agent to capture multiple possible
instruction-following behaviors. To address the issues of compounding error in
imitation learning and enhance spatial reasoning in long-horizon navigation
tasks, we employ DAgger for online policy training and expert trajectory
augmentation, and use the aggregated data to further fine-tune the policy. This
approach significantly improves the policy's robustness and its ability to
recover from error states. Extensive experiments on benchmark datasets
demonstrate that, even without a waypoint predictor, the proposed method
substantially outperforms previous state-of-the-art two-stage waypoint-based
models in terms of navigation performance. Our code is available at:
https://github.com/Tokishx/DifNav.

</details>


### [5] [Reactive Model Predictive Contouring Control for Robot Manipulators](https://arxiv.org/abs/2508.09502)
*Junheon Yoon,Woo-Jeong Baek,Jaeheung Park*

Main category: cs.RO

TL;DR: 提出了一种基于RMPCC的机器人路径跟随框架，能在动态环境中以100Hz频率避开障碍、奇异点和自碰撞。


<details>
  <summary>Details</summary>
Motivation: 现有路径跟随方法依赖时间参数化，难以同时处理碰撞、奇异点避免和运动学限制，导致路径误差较大。

Method: 通过路径参数化参考路径，利用RMPCC优化，引入CBF避免碰撞和奇异点，采用雅可比线性化和高斯-牛顿Hessian近似实现高速求解。

Result: 实验表明，该方法在动态环境中表现优异，轮廓误差和机器人加速度均较低，性能优于现有方法10倍。

Conclusion: RMPCC框架在动态环境中高效解决了路径跟随问题，具有实际应用潜力。

Abstract: This contribution presents a robot path-following framework via Reactive
Model Predictive Contouring Control (RMPCC) that successfully avoids obstacles,
singularities and self-collisions in dynamic environments at 100 Hz. Many
path-following methods rely on the time parametrization, but struggle to handle
collision and singularity avoidance while adhering kinematic limits or other
constraints. Specifically, the error between the desired path and the actual
position can become large when executing evasive maneuvers. Thus, this paper
derives a method that parametrizes the reference path by a path parameter and
performs the optimization via RMPCC. In particular, Control Barrier Functions
(CBFs) are introduced to avoid collisions and singularities in dynamic
environments. A Jacobian-based linearization and Gauss-Newton Hessian
approximation enable solving the nonlinear RMPCC problem at 100 Hz,
outperforming state-of-the-art methods by a factor of 10. Experiments confirm
that the framework handles dynamic obstacles in real-world settings with low
contouring error and low robot acceleration.

</details>


### [6] [SMART-OC: A Real-time Time-risk Optimal Replanning Algorithm for Dynamic Obstacles and Spatio-temporally Varying Currents](https://arxiv.org/abs/2508.09508)
*Reema Raval,Shalabh Gupta*

Main category: cs.RO

TL;DR: 提出了一种名为SMART-OC的新算法，用于无人水面车辆（USV）在动态海洋环境中实时规划最优路径，结合障碍风险和到达时间。


<details>
  <summary>Details</summary>
Motivation: 海洋环境复杂多变，USV需要实时调整路径以避免碰撞并利用洋流高效导航。

Method: SMART-OC算法整合路径上的障碍风险与到达目标的时间成本，实现实时时间-风险最优路径规划。

Result: 仿真实验验证了SMART-OC的有效性，USV能够快速重新规划路径以避开障碍并利用洋流成功到达目标。

Conclusion: SMART-OC为USV在动态海洋环境中的安全高效导航提供了有效解决方案。

Abstract: Typical marine environments are highly complex with spatio-temporally varying
currents and dynamic obstacles, presenting significant challenges to Unmanned
Surface Vehicles (USVs) for safe and efficient navigation. Thus, the USVs need
to continuously adapt their paths with real-time information to avoid
collisions and follow the path of least resistance to the goal via exploiting
ocean currents. In this regard, we introduce a novel algorithm, called
Self-Morphing Adaptive Replanning Tree for dynamic Obstacles and Currents
(SMART-OC), that facilitates real-time time-risk optimal replanning in dynamic
environments. SMART-OC integrates the obstacle risks along a path with the time
cost to reach the goal to find the time-risk optimal path. The effectiveness of
SMART-OC is validated by simulation experiments, which demonstrate that the USV
performs fast replannings to avoid dynamic obstacles and exploit ocean currents
to successfully reach the goal.

</details>


### [7] [CaRoBio: 3D Cable Routing with a Bio-inspired Gripper Fingernail](https://arxiv.org/abs/2508.09558)
*Jiahui Zuo,Boyang Zhang,Fumin Zhang*

Main category: cs.RO

TL;DR: 提出了一种受鹰爪启发的指甲设计，用于机器人夹爪，以改进电缆的抓取和引导，并开发了一种端到端的3D电缆布线框架。


<details>
  <summary>Details</summary>
Motivation: 传统平行两指夹爪在电缆操作中存在过度挤压和张力问题，需要更高效的解决方案。

Method: 设计了鹰爪启发的指甲，结合视觉状态估计和离线轨迹规划，实现连续控制。

Result: 新框架在多种电缆和槽道测试中显著优于传统的拾取-放置策略。

Conclusion: 该框架为未来3D空间中的电缆布线操作提供了参考。

Abstract: The manipulation of deformable linear flexures has a wide range of
applications in industry, such as cable routing in automotive manufacturing and
textile production. Cable routing, as a complex multi-stage robot manipulation
scenario, is a challenging task for robot automation. Common parallel
two-finger grippers have the risk of over-squeezing and over-tension when
grasping and guiding cables. In this paper, a novel eagle-inspired fingernail
is designed and mounted on the gripper fingers, which helps with cable grasping
on planar surfaces and in-hand cable guiding operations. Then we present a
single-grasp end-to-end 3D cable routing framework utilizing the proposed
fingernails, instead of the common pick-and-place strategy. Continuous control
is achieved to efficiently manipulate cables through vision-based state
estimation of task configurations and offline trajectory planning based on
motion primitives. We evaluate the effectiveness of the proposed framework with
a variety of cables and channel slots, significantly outperforming the
pick-and-place manipulation process under equivalent perceptual conditions. Our
reconfigurable task setting and the proposed framework provide a reference for
future cable routing manipulations in 3D space.

</details>


### [8] [ESCoT: An Enhanced Step-based Coordinate Trajectory Planning Method for Multiple Car-like Robots](https://arxiv.org/abs/2508.09581)
*Junkai Jiang,Yihe Chen,Yibin Yang,Ruochen Li,Shaobing Xu,Jianqiang Wang*

Main category: cs.RO

TL;DR: ESCoT是一种改进的基于步骤的多车轨迹规划方法，通过协作规划和重复配置重规划提升性能，在稀疏和密集场景中均优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 多车轨迹规划（MVTP）是多机器人系统（MRS）中的关键挑战，具有广泛的应用需求。

Method: ESCoT采用协作规划和重复配置重规划策略，优化基于步骤的MVTP方法。

Result: 在稀疏场景中，ESCoT显著提升解的质量（最高70%改进），在密集场景中保持50%以上的成功率。

Conclusion: ESCoT有效解决了MVTP问题，扩展了基于步骤方法的能力，并通过实际机器人测试验证了实用性。

Abstract: Multi-vehicle trajectory planning (MVTP) is one of the key challenges in
multi-robot systems (MRSs) and has broad applications across various fields.
This paper presents ESCoT, an enhanced step-based coordinate trajectory
planning method for multiple car-like robots. ESCoT incorporates two key
strategies: collaborative planning for local robot groups and replanning for
duplicate configurations. These strategies effectively enhance the performance
of step-based MVTP methods. Through extensive experiments, we show that ESCoT
1) in sparse scenarios, significantly improves solution quality compared to
baseline step-based method, achieving up to 70% improvement in typical conflict
scenarios and 34% in randomly generated scenarios, while maintaining high
solving efficiency; and 2) in dense scenarios, outperforms all baseline
methods, maintains a success rate of over 50% even in the most challenging
configurations. The results demonstrate that ESCoT effectively solves MVTP,
further extending the capabilities of step-based methods. Finally, practical
robot tests validate the algorithm's applicability in real-world scenarios.

</details>


### [9] [HapticGiant: A Novel Very Large Kinesthetic Haptic Interface with Hierarchical Force Control](https://arxiv.org/abs/2508.09595)
*Michael Fennel,Markus Walker,Dominik Pikos,Uwe D. Hanebeck*

Main category: cs.RO

TL;DR: HapticGiant是一种新型的大规模动觉触觉接口，旨在匹配人类手臂特性并提供自然用户移动和完整触觉反馈。


<details>
  <summary>Details</summary>
Motivation: 当前动觉触觉接口存在工作空间有限、自由度不足和运动学不匹配等问题，限制了虚拟现实的沉浸感。

Method: 提出HapticGiant，采用新型导纳型力控制方案，利用分层优化渲染任意串行运动链和笛卡尔导纳。

Result: 实验证明HapticGiant及其控制方案有效，为高度沉浸式虚拟现实应用铺平道路。

Conclusion: HapticGiant通过匹配人类手臂特性和优化控制方案，显著提升了虚拟现实的触觉沉浸感。

Abstract: Research in virtual reality and haptic technologies has consistently aimed to
enhance immersion. While advanced head-mounted displays are now commercially
available, kinesthetic haptic interfaces still face challenges such as limited
workspaces, insufficient degrees of freedom, and kinematics not matching the
human arm. In this paper, we present HapticGiant, a novel large-scale
kinesthetic haptic interface designed to match the properties of the human arm
as closely as possible and to facilitate natural user locomotion while
providing full haptic feedback. The interface incorporates a novel
admittance-type force control scheme, leveraging hierarchical optimization to
render both arbitrary serial kinematic chains and Cartesian admittances.
Notably, the proposed control scheme natively accounts for system limitations,
including joint and Cartesian constraints, as well as singularities.
Experimental results demonstrate the effectiveness of HapticGiant and its
control scheme, paving the way for highly immersive virtual reality
applications.

</details>


### [10] [BEAVR: Bimanual, multi-Embodiment, Accessible, Virtual Reality Teleoperation System for Robots](https://arxiv.org/abs/2508.09606)
*Alejandro Posadas-Nava,Alejandro Carrasco,Richard Linares*

Main category: cs.RO

TL;DR: BEAVR是一个开源的、双手操作的、多体现的VR远程操作系统，用于机器人，支持实时控制、数据记录和策略学习。


<details>
  <summary>Details</summary>
Motivation: 设计BEAVR的目的是为了统一异构机器人平台的实时控制、数据记录和策略学习，提升远程操作的灵活性和效率。

Method: BEAVR采用零拷贝流架构实现低延迟（≤35ms），支持异步“思考-行动”控制循环和灵活的网络API，适用于多种机器人平台。

Result: 系统在多样化操作任务中表现优异，兼容主流视觉运动策略（如ACT、DiffusionPolicy、SmolVLA），并公开了代码和数据集。

Conclusion: BEAVR为机器人远程操作提供了一个高效、灵活且可扩展的解决方案，推动了异构机器人平台的研究和应用。

Abstract: \textbf{BEAVR} is an open-source, bimanual, multi-embodiment Virtual Reality
(VR) teleoperation system for robots, designed to unify real-time control, data
recording, and policy learning across heterogeneous robotic platforms. BEAVR
enables real-time, dexterous teleoperation using commodity VR hardware,
supports modular integration with robots ranging from 7-DoF manipulators to
full-body humanoids, and records synchronized multi-modal demonstrations
directly in the LeRobot dataset schema. Our system features a zero-copy
streaming architecture achieving $\leq$35\,ms latency, an asynchronous
``think--act'' control loop for scalable inference, and a flexible network API
optimized for real-time, multi-robot operation. We benchmark BEAVR across
diverse manipulation tasks and demonstrate its compatibility with leading
visuomotor policies such as ACT, DiffusionPolicy, and SmolVLA. All code is
publicly available, and datasets are released on Hugging Face\footnote{Code,
datasets, and VR app available at https://github.com/ARCLab-MIT/BEAVR-Bot.

</details>


### [11] [Interpretable Robot Control via Structured Behavior Trees and Large Language Models](https://arxiv.org/abs/2508.09621)
*Ingrid Maéva Chekam,Ines Pastor-Martinez,Ali Tourani,Jose Andres Millan-Romera,Laura Ribeiro,Pedro Miguel Bastos Soares,Holger Voos,Jose Luis Sanchez-Lopez*

Main category: cs.RO

TL;DR: 论文提出了一种结合大型语言模型（LLMs）和行为树的新框架，用于实现自然语言指令到机器人动作的转换，提升了人机交互的直观性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着智能机器人更多地融入人类环境，需要更直观、可靠且适应性强的人机交互界面，传统方法限制了动态环境中的可用性。

Method: 通过结合LLMs和行为树，将自然语言指令转化为可执行动作，支持模块化集成和感知功能（如人员跟踪和手势识别）。

Result: 实验结果显示，系统在真实场景中平均认知到执行的准确率约为94%。

Conclusion: 该框架为人机交互系统提供了实用且高效的解决方案，代码已开源。

Abstract: As intelligent robots become more integrated into human environments, there
is a growing need for intuitive and reliable Human-Robot Interaction (HRI)
interfaces that are adaptable and more natural to interact with. Traditional
robot control methods often require users to adapt to interfaces or memorize
predefined commands, limiting usability in dynamic, unstructured environments.
This paper presents a novel framework that bridges natural language
understanding and robotic execution by combining Large Language Models (LLMs)
with Behavior Trees. This integration enables robots to interpret natural
language instructions given by users and translate them into executable actions
by activating domain-specific plugins. The system supports scalable and modular
integration, with a primary focus on perception-based functionalities, such as
person tracking and hand gesture recognition. To evaluate the system, a series
of real-world experiments was conducted across diverse environments.
Experimental results demonstrate that the proposed approach is practical in
real-world scenarios, with an average cognition-to-execution accuracy of
approximately 94%, making a significant contribution to HRI systems and robots.
The complete source code of the framework is publicly available at
https://github.com/snt-arg/robot_suite.

</details>


### [12] [Immersive Teleoperation of Beyond-Human-Scale Robotic Manipulators: Challenges and Future Directions](https://arxiv.org/abs/2508.09700)
*Mahdi Hejrati,Jouni Mattila*

Main category: cs.RO

TL;DR: 本文探讨了超大型机器人操作器（BHSRMs）的远程操作挑战，重点关注安全性、感知匹配和沉浸感，并分析了触觉与视觉反馈系统的设计权衡。


<details>
  <summary>Details</summary>
Motivation: 随着BHSRMs在工业领域的应用增加，需要重新设计沉浸式界面以支持安全高效的人机协作。

Method: 分析了控制和界面层面的挑战，并通过实验比较了外骨骼和摇杆控制方案。

Result: 提出了触觉与视觉反馈系统的设计权衡，并展示了初步实验结果。

Conclusion: 未来研究方向包括开发新的评估工具、扩展策略和以人为中心的安全模型。

Abstract: Teleoperation of beyond-human-scale robotic manipulators (BHSRMs) presents
unique challenges that differ fundamentally from conventional human-scale
systems. As these platforms gain relevance in industrial domains such as
construction, mining, and disaster response, immersive interfaces must be
rethought to support scalable, safe, and effective human-robot collaboration.
This paper investigates the control, cognitive, and interface-level challenges
of immersive teleoperation in BHSRMs, with a focus on ensuring operator safety,
minimizing sensorimotor mismatch, and enhancing the sense of embodiment. We
analyze design trade-offs in haptic and visual feedback systems, supported by
early experimental comparisons of exoskeleton- and joystick-based control
setups. Finally, we outline key research directions for developing new
evaluation tools, scaling strategies, and human-centered safety models tailored
to large-scale robotic telepresence.

</details>


### [13] [FLARE: Agile Flights for Quadrotor Cable-Suspended Payload System via Reinforcement Learning](https://arxiv.org/abs/2508.09797)
*Dongcheng Cao,Jin Zhou,Xian Wang,Shuo Li*

Main category: cs.RO

TL;DR: FLARE是一个基于强化学习的框架，用于解决四旋翼悬挂负载系统的敏捷飞行问题，显著优于传统优化方法，并实现了零样本的仿真到现实迁移。


<details>
  <summary>Details</summary>
Motivation: 四旋翼悬挂负载系统的动力学复杂且非线性，传统优化方法计算成本高且难以处理电缆模式转换，限制了实时性和机动性。

Method: FLARE通过强化学习直接从高保真仿真中学习敏捷导航策略。

Result: 在三个挑战性场景中，FLARE比最先进的优化方法快3倍，并成功实现零样本的仿真到现实迁移。

Conclusion: FLARE展示了在实时性和安全性方面的显著优势，适用于实际应用。

Abstract: Agile flight for the quadrotor cable-suspended payload system is a formidable
challenge due to its underactuated, highly nonlinear, and hybrid dynamics.
Traditional optimization-based methods often struggle with high computational
costs and the complexities of cable mode transitions, limiting their real-time
applicability and maneuverability exploitation. In this letter, we present
FLARE, a reinforcement learning (RL) framework that directly learns agile
navigation policy from high-fidelity simulation. Our method is validated across
three designed challenging scenarios, notably outperforming a state-of-the-art
optimization-based approach by a 3x speedup during gate traversal maneuvers.
Furthermore, the learned policies achieve successful zero-shot sim-to-real
transfer, demonstrating remarkable agility and safety in real-world
experiments, running in real time on an onboard computer.

</details>


### [14] [Embodied Tactile Perception of Soft Objects Properties](https://arxiv.org/abs/2508.09836)
*Anirvan Dutta,Alexis WM Devillard,Zhihuan Zhang,Xiaoxiao Cheng,Etienne Burdet*

Main category: cs.RO

TL;DR: 研究探讨了机械顺应性、多模态传感和交互策略如何共同影响机器人触觉感知，提出了一种无监督的深度状态空间模型，证明多模态传感优于单模态传感。


<details>
  <summary>Details</summary>
Motivation: 为实现机器人具备类似人类的精细操作能力，需理解机械顺应性、多模态传感和交互策略如何共同塑造触觉感知。

Method: 使用模块化电子皮肤（e-Skin）和软波对象，通过按压、滑动等交互动作，结合提出的无监督深度状态空间模型（潜在滤波器）分析感知。

Result: 多模态传感表现优于单模态传感，揭示了电子皮肤机械特性与环境间的复杂交互作用。

Conclusion: 研究强调了结合时间动态分析机械特性和交互策略的重要性，为机器人触觉感知提供了通用且可解释的表征。

Abstract: To enable robots to develop human-like fine manipulation, it is essential to
understand how mechanical compliance, multi-modal sensing, and purposeful
interaction jointly shape tactile perception. In this study, we use a dedicated
modular e-Skin with tunable mechanical compliance and multi-modal sensing
(normal, shear forces and vibrations) to systematically investigate how sensing
embodiment and interaction strategies influence robotic perception of objects.
Leveraging a curated set of soft wave objects with controlled viscoelastic and
surface properties, we explore a rich set of palpation primitives-pressing,
precession, sliding that vary indentation depth, frequency, and directionality.
In addition, we propose the latent filter, an unsupervised, action-conditioned
deep state-space model of the sophisticated interaction dynamics and infer
causal mechanical properties into a structured latent space. This provides
generalizable and in-depth interpretable representation of how embodiment and
interaction determine and influence perception. Our investigation demonstrates
that multi-modal sensing outperforms uni-modal sensing. It highlights a nuanced
interaction between the environment and mechanical properties of e-Skin, which
should be examined alongside the interaction by incorporating temporal
dynamics.

</details>


### [15] [Whole-Body Bilateral Teleoperation with Multi-Stage Object Parameter Estimation for Wheeled Humanoid Locomanipulation](https://arxiv.org/abs/2508.09846)
*Donghoon Baek,Amartya Purushottam,Jason J. Choi,Joao Ramos*

Main category: cs.RO

TL;DR: 本文提出了一种面向轮式人形机器人的物体感知全身双边遥操作框架，结合在线多阶段物体惯性参数估计模块，显著提升了操作效率和动态同步能力。


<details>
  <summary>Details</summary>
Motivation: 旨在解决轮式人形机器人在搬运任务中因物体惯性参数未知而导致的动态同步和操作效率问题。

Method: 采用多阶段估计方法，结合视觉大小估计、视觉语言模型先验和分层采样策略，实时更新物体惯性参数。

Result: 实验验证了框架在实时搬运任务中的有效性，机器人能够处理约自身体重三分之一的负载。

Conclusion: 该框架通过动态参数估计和同步，提升了遥操作的动态性和操作效率，适用于复杂搬运任务。

Abstract: This paper presents an object-aware whole-body bilateral teleoperation
framework for wheeled humanoid loco-manipulation. This framework combines
whole-body bilateral teleoperation with an online multi-stage object inertial
parameter estimation module, which is the core technical contribution of this
work. The multi-stage process sequentially integrates a vision-based object
size estimator, an initial parameter guess generated by a large vision-language
model (VLM), and a decoupled hierarchical sampling strategy. The visual size
estimate and VLM prior offer a strong initial guess of the object's inertial
parameters, significantly reducing the search space for sampling-based
refinement and improving the overall estimation speed. A hierarchical strategy
first estimates mass and center of mass, then infers inertia from object size
to ensure physically feasible parameters, while a decoupled multi-hypothesis
scheme enhances robustness to VLM prior errors. Our estimator operates in
parallel with high-fidelity simulation and hardware, enabling real-time online
updates. The estimated parameters are then used to update the wheeled
humanoid's equilibrium point, allowing the operator to focus more on locomotion
and manipulation. This integration improves the haptic force feedback for
dynamic synchronization, enabling more dynamic whole-body teleoperation. By
compensating for object dynamics using the estimated parameters, the framework
also improves manipulation tracking while preserving compliant behavior. We
validate the system on a customized wheeled humanoid with a robotic gripper and
human-machine interface, demonstrating real-time execution of lifting,
delivering, and releasing tasks with a payload weighing approximately one-third
of the robot's body weight.

</details>


### [16] [Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes](https://arxiv.org/abs/2508.09855)
*Yuekun Wu,Yik Lung Pang,Andrea Cavallaro,Changjae Oh*

Main category: cs.RO

TL;DR: 提出一种仅从RGB图像训练人机交接任务策略的方法，无需真实机器人数据，利用高斯泼溅重建场景生成演示。


<details>
  <summary>Details</summary>
Motivation: 解决人机协作中视觉域差距问题，减少对真实机器人训练的依赖。

Method: 利用稀疏视图高斯泼溅重建场景生成图像-动作对，模拟机器人夹爪姿态变化。

Result: 在重建场景和真实实验中均表现良好，提升了人机交接的鲁棒性。

Conclusion: 该方法为人机交接任务提供了新的有效表示，促进了无缝协作。

Abstract: Human-robot teaming (HRT) systems often rely on large-scale datasets of human
and robot interactions, especially for close-proximity collaboration tasks such
as human-robot handovers. Learning robot manipulation policies from raw,
real-world image data requires a large number of robot-action trials in the
physical environment. Although simulation training offers a cost-effective
alternative, the visual domain gap between simulation and robot workspace
remains a major limitation. We introduce a method for training HRT policies,
focusing on human-to-robot handovers, solely from RGB images without the need
for real-robot training or real-robot data collection. The goal is to enable
the robot to reliably receive objects from a human with stable grasping while
avoiding collisions with the human hand. The proposed policy learner leverages
sparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes
to generate robot demonstrations containing image-action pairs captured with a
camera mounted on the robot gripper. As a result, the simulated camera pose
changes in the reconstructed scene can be directly translated into gripper pose
changes. Experiments in both Gaussian Splatting reconstructed scene and
real-world human-to-robot handover experiments demonstrate that our method
serves as a new and effective representation for the human-to-robot handover
task, contributing to more seamless and robust HRT.

</details>


### [17] [A Shank Angle-Based Control System Enables Soft Exoskeleton to Assist Human Non-Steady Locomotion](https://arxiv.org/abs/2508.09876)
*Xiaowei Tan,Weizhong Jiang,Bi Zhang,Wanxin Chen,Yiwen Zhao,Ning Li,Lianqing Liu,Xingang Zhao*

Main category: cs.RO

TL;DR: 该研究提出了一种基于小腿角度的外骨骼控制系统，能够实时协调人类步态，适应非线性步态变化，并在多种活动中动态调整辅助模式。


<details>
  <summary>Details</summary>
Motivation: 探索外骨骼在非线性步态（如行走、跑步、上下楼梯）中的效果，填补现有研究空白。

Method: 采用双高斯模型生成辅助模式，结合模型前馈控制方法，仅依赖IMU数据在线更新参数。

Result: 实验验证了控制系统的有效性、鲁棒性，并显示外骨骼辅助对用户生物力学和生理反应有积极影响。

Conclusion: 该系统为外骨骼在复杂步态活动中的应用提供了可行方案。

Abstract: Exoskeletons have been shown to effectively assist humans during steady
locomotion. However, their effects on non-steady locomotion, characterized by
nonlinear phase progression within a gait cycle, remain insufficiently
explored, particularly across diverse activities. This work presents a shank
angle-based control system that enables the exoskeleton to maintain real-time
coordination with human gait, even under phase perturbations, while dynamically
shaping assistance profiles to match the biological ankle moment patterns
across walking, running, stair negotiation tasks. The control system consists
of an assistance profile online generation method and a model-based feedforward
control method. The assistance profile is formulated as a dual-Gaussian model
with the shank angle as the independent variable. Leveraging only IMU
measurements, the model parameters are updated online each stride to adapt to
inter- and intra-individual biomechanical variability. The profile tracking
control employs a human-exoskeleton kinematics and stiffness model as a
feedforward component, reducing reliance on historical control data due to the
lack of clear and consistent periodicity in non-steady locomotion. Three
experiments were conducted using a lightweight soft exoskeleton with multiple
subjects. The results validated the effectiveness of each individual method,
demonstrated the robustness of the control system against gait perturbations
across various activities, and revealed positive biomechanical and
physiological responses of human users to the exoskeleton's mechanical
assistance.

</details>


### [18] [PPL: Point Cloud Supervised Proprioceptive Locomotion Reinforcement Learning for Legged Robots in Crawl Spaces](https://arxiv.org/abs/2508.09950)
*Bida Ma,Nuo Xu,Chenkun Qi,Xin Liu,Yule Mo,Jinkai Wang,Chunpeng Lu*

Main category: cs.RO

TL;DR: 提出了一种基于点云监督的强化学习方法，用于腿式机器人在狭窄空间中的运动，无需依赖外部感知传感器。


<details>
  <summary>Details</summary>
Motivation: 解决腿式机器人在狭窄空间中运动的挑战，现有方法因传感器噪声或仅依赖地面特征而受限。

Method: 设计状态估计网络，利用历史本体感知数据估计周围环境和碰撞状态；提出点云处理方法提取特征，并设计奖励函数指导运动。

Result: 实验表明，该方法在狭窄空间中表现出更敏捷的运动能力。

Conclusion: 该方法提升了腿式机器人在无外部传感器情况下的狭窄环境通过能力。

Abstract: The legged locomotion in spatially constrained structures (called crawl
spaces) is challenging. In crawl spaces, current exteroceptive locomotion
learning methods are limited by large noises and errors of the sensors in
possible low visibility conditions, and current proprioceptive locomotion
learning methods are difficult in traversing crawl spaces because only ground
features are inferred. In this study, a point cloud supervised proprioceptive
locomotion reinforcement learning method for legged robots in crawl spaces is
proposed. A state estimation network is designed to estimate the robot's
surrounding ground and spatial features as well as the robot's collision states
using historical proprioceptive sensor data. The point cloud is represented in
polar coordinate frame and a point cloud processing method is proposed to
efficiently extract the ground and spatial features that are used to supervise
the state estimation network learning. Comprehensive reward functions that
guide the robot to traverse through crawl spaces after collisions are designed.
Experiments demonstrate that, compared to existing methods, our method exhibits
more agile locomotion in crawl spaces. This study enhances the ability of
legged robots to traverse spatially constrained environments without requiring
exteroceptive sensors.

</details>


### [19] [GBC: Generalized Behavior-Cloning Framework for Whole-Body Humanoid Imitation](https://arxiv.org/abs/2508.09960)
*Yifei Yao,Chengyuan Luo,Jiaheng Du,Wentao He,Jun-Guo Lu*

Main category: cs.RO

TL;DR: 论文提出了通用行为克隆（GBC）框架，通过自适应数据管道、DAgger-MMPPO算法和开源平台，实现了从人类动作到机器人行为的统一解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人数据与算法不通用的问题，实现跨形态的通用控制。

Method: GBC框架包含自适应数据管道（利用可逆IK网络）、DAgger-MMPPO算法（基于MMTransformer架构）和开源平台Isaac Lab。

Result: 在多种异构人形机器人上验证了GBC的优异性能和泛化能力。

Conclusion: GBC首次实现了真正通用的人形机器人控制器，为领域提供了统一解决方案。

Abstract: The creation of human-like humanoid robots is hindered by a fundamental
fragmentation: data processing and learning algorithms are rarely universal
across different robot morphologies. This paper introduces the Generalized
Behavior Cloning (GBC) framework, a comprehensive and unified solution designed
to solve this end-to-end challenge. GBC establishes a complete pathway from
human motion to robot action through three synergistic innovations. First, an
adaptive data pipeline leverages a differentiable IK network to automatically
retarget any human MoCap data to any humanoid. Building on this foundation, our
novel DAgger-MMPPO algorithm with its MMTransformer architecture learns robust,
high-fidelity imitation policies. To complete the ecosystem, the entire
framework is delivered as an efficient, open-source platform based on Isaac
Lab, empowering the community to deploy the full workflow via simple
configuration scripts. We validate the power and generality of GBC by training
policies on multiple heterogeneous humanoids, demonstrating excellent
performance and transfer to novel motions. This work establishes the first
practical and unified pathway for creating truly generalized humanoid
controllers.

</details>


### [20] [Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model](https://arxiv.org/abs/2508.09971)
*Zihan Wang,Nina Mahmoudian*

Main category: cs.RO

TL;DR: 该论文提出了一种基于视觉的无人机自主河流跟踪方法，通过子模马尔可夫决策过程建模，并引入边际增益优势估计、语义动态模型和约束演员动态估计器架构，实现了更高效、安全的强化学习框架。


<details>
  <summary>Details</summary>
Motivation: 在密集河流环境中，GPS信号不可靠，视觉驱动的无人机自主河流跟踪对于救援、监视和环境监测等应用至关重要。

Method: 1. 引入边际增益优势估计（MGAE）优化奖励优势函数；2. 开发基于语义掩码的语义动态模型（SDM）；3. 提出约束演员动态估计器（CADE）架构，整合模型和安全强化学习。

Result: 仿真结果表明，MGAE比传统方法收敛更快且性能更优；SDM提供更准确的短期状态预测；CADE成功将安全约束整合到模型强化学习中。

Conclusion: 该方法通过软硬结合的平衡策略，显著提升了无人机在复杂环境中的自主跟踪能力和安全性。

Abstract: Vision-driven autonomous river following by Unmanned Aerial Vehicles is
critical for applications such as rescue, surveillance, and environmental
monitoring, particularly in dense riverine environments where GPS signals are
unreliable. We formalize river following as a coverage control problem in which
the reward function is submodular, yielding diminishing returns as more unique
river segments are visited, thereby framing the task as a Submodular Markov
Decision Process. First, we introduce Marginal Gain Advantage Estimation, which
refines the reward advantage function by using a sliding window baseline
computed from historical episodic returns, thus aligning the advantage
estimation with the agent's evolving recognition of action value in
non-Markovian settings. Second, we develop a Semantic Dynamics Model based on
patchified water semantic masks that provides more interpretable and
data-efficient short-term prediction of future observations compared to latent
vision dynamics models. Third, we present the Constrained Actor Dynamics
Estimator architecture, which integrates the actor, the cost estimator, and SDM
for cost advantage estimation to form a model-based SafeRL framework capable of
solving partially observable Constrained Submodular Markov Decision Processes.
Simulation results demonstrate that MGAE achieves faster convergence and
superior performance over traditional critic-based methods like Generalized
Advantage Estimation. SDM provides more accurate short-term state predictions
that enable the cost estimator to better predict potential violations. Overall,
CADE effectively integrates safety regulation into model-based RL, with the
Lagrangian approach achieving the soft balance of reward and safety during
training, while the safety layer enhances performance during inference by hard
action overlay.

</details>


### [21] [Masquerade: Learning from In-the-wild Human Videos using Data-Editing](https://arxiv.org/abs/2508.09976)
*Marion Lepert,Jiaying Fang,Jeannette Bohg*

Main category: cs.RO

TL;DR: Masquerade方法通过编辑人类视频生成机器人演示，显著提升了机器人策略的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 机器人操作研究面临数据稀缺问题，现有数据集远小于语言和视觉领域的数据集。

Method: 通过估计3D手部姿势、修复人类手臂并叠加机器人轨迹，将人类视频转化为机器人演示，并结合预训练和微调策略。

Result: 在三个未见场景的长时程双手机器人任务中，性能优于基线5-6倍。

Conclusion: 通过缩小视觉体现差距，人类视频成为提升机器人策略的宝贵数据源。

Abstract: Robot manipulation research still suffers from significant data scarcity:
even the largest robot datasets are orders of magnitude smaller and less
diverse than those that fueled recent breakthroughs in language and vision. We
introduce Masquerade, a method that edits in-the-wild egocentric human videos
to bridge the visual embodiment gap between humans and robots and then learns a
robot policy with these edited videos. Our pipeline turns each human video into
robotized demonstrations by (i) estimating 3-D hand poses, (ii) inpainting the
human arms, and (iii) overlaying a rendered bimanual robot that tracks the
recovered end-effector trajectories. Pre-training a visual encoder to predict
future 2-D robot keypoints on 675K frames of these edited clips, and continuing
that auxiliary loss while fine-tuning a diffusion policy head on only 50 robot
demonstrations per task, yields policies that generalize significantly better
than prior work. On three long-horizon, bimanual kitchen tasks evaluated in
three unseen scenes each, Masquerade outperforms baselines by 5-6x. Ablations
show that both the robot overlay and co-training are indispensable, and
performance scales logarithmically with the amount of edited human video. These
results demonstrate that explicitly closing the visual embodiment gap unlocks a
vast, readily available source of data from human videos that can be used to
improve robot policies.

</details>
