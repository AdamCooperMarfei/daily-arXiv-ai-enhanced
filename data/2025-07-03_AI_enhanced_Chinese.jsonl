{"id": "2507.01111", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.01111", "abs": "https://arxiv.org/abs/2507.01111", "authors": ["Haosen Xing", "Haoran Ma", "Sijin Zhang", "Hartmut Geyer"], "title": "Environment-Aware and Human-Cooperative Swing Control for Lower-Limb Prostheses in Diverse Obstacle Scenarios", "comment": null, "summary": "Current control strategies for powered lower limb prostheses often lack\nawareness of the environment and the user's intended interactions with it. This\nlimitation becomes particularly apparent in complex terrains. Obstacle\nnegotiation, a critical scenario exemplifying such challenges, requires both\nreal-time perception of obstacle geometry and responsiveness to user intention\nabout when and where to step over or onto, to dynamically adjust swing\ntrajectories. We propose a novel control strategy that fuses environmental\nawareness and human cooperativeness: an on-board depth camera detects obstacles\nahead of swing phase, prompting an elevated early-swing trajectory to ensure\nclearance, while late-swing control defers to natural biomechanical cues from\nthe user. This approach enables intuitive stepping strategies without requiring\nunnatural movement patterns. Experiments with three non-amputee participants\ndemonstrated 100 percent success across more than 150 step-overs and 30\nstep-ons with randomly placed obstacles of varying heights (4-16 cm) and\ndistances (15-70 cm). By effectively addressing obstacle navigation -- a\ngateway challenge for complex terrain mobility -- our system demonstrates\nadaptability to both environmental constraints and user intentions, with\npromising applications across diverse locomotion scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u63a7\u5236\u7b56\u7565\uff0c\u7ed3\u5408\u73af\u5883\u611f\u77e5\u4e0e\u7528\u6237\u610f\u56fe\uff0c\u4f18\u5316\u4e0b\u80a2\u5047\u80a2\u5728\u590d\u6742\u5730\u5f62\u4e2d\u7684\u969c\u788d\u7269\u8de8\u8d8a\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4e0b\u80a2\u5047\u80a2\u63a7\u5236\u7b56\u7565\u7f3a\u4e4f\u5bf9\u73af\u5883\u4e0e\u7528\u6237\u610f\u56fe\u7684\u611f\u77e5\uff0c\u5c24\u5176\u5728\u590d\u6742\u5730\u5f62\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u673a\u8f7d\u6df1\u5ea6\u76f8\u673a\u5b9e\u65f6\u68c0\u6d4b\u969c\u788d\u7269\uff0c\u7ed3\u5408\u7528\u6237\u751f\u7269\u529b\u5b66\u4fe1\u53f7\u52a8\u6001\u8c03\u6574\u6446\u52a8\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5728150\u591a\u6b21\u8de8\u8d8a\u548c30\u591a\u6b21\u8e0f\u4e0a\u969c\u788d\u7269\u7684\u6d4b\u8bd5\u4e2d\uff0c\u6210\u529f\u7387100%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6210\u529f\u89e3\u51b3\u4e86\u969c\u788d\u7269\u5bfc\u822a\u95ee\u9898\uff0c\u5c55\u73b0\u4e86\u9002\u5e94\u73af\u5883\u4e0e\u7528\u6237\u610f\u56fe\u7684\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8fd0\u52a8\u573a\u666f\u3002"}}
{"id": "2507.01125", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01125", "abs": "https://arxiv.org/abs/2507.01125", "authors": ["Keiko Nagami", "Timothy Chen", "Javier Yu", "Ola Shorinwa", "Maximilian Adang", "Carlyn Dougherty", "Eric Cristofalo", "Mac Schwager"], "title": "VISTA: Open-Vocabulary, Task-Relevant Robot Exploration with Online Semantic Gaussian Splatting", "comment": "9 pages, 4 figures", "summary": "We present VISTA (Viewpoint-based Image selection with Semantic Task\nAwareness), an active exploration method for robots to plan informative\ntrajectories that improve 3D map quality in areas most relevant for task\ncompletion. Given an open-vocabulary search instruction (e.g., \"find a\nperson\"), VISTA enables a robot to explore its environment to search for the\nobject of interest, while simultaneously building a real-time semantic 3D\nGaussian Splatting reconstruction of the scene. The robot navigates its\nenvironment by planning receding-horizon trajectories that prioritize semantic\nsimilarity to the query and exploration of unseen regions of the environment.\nTo evaluate trajectories, VISTA introduces a novel, efficient\nviewpoint-semantic coverage metric that quantifies both the geometric view\ndiversity and task relevance in the 3D scene. On static datasets, our coverage\nmetric outperforms state-of-the-art baselines, FisherRF and Bayes' Rays, in\ncomputation speed and reconstruction quality. In quadrotor hardware\nexperiments, VISTA achieves 6x higher success rates in challenging maps,\ncompared to baseline methods, while matching baseline performance in less\nchallenging maps. Lastly, we show that VISTA is platform-agnostic by deploying\nit on a quadrotor drone and a Spot quadruped robot. Open-source code will be\nreleased upon acceptance of the paper.", "AI": {"tldr": "VISTA\u662f\u4e00\u79cd\u673a\u5668\u4eba\u4e3b\u52a8\u63a2\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u4efb\u52a1\u611f\u77e5\u89c4\u5212\u8f68\u8ff9\uff0c\u63d0\u53473D\u5730\u56fe\u8d28\u91cf\uff0c\u4e13\u6ce8\u4e8e\u4efb\u52a1\u76f8\u5173\u533a\u57df\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u5f00\u653e\u8bcd\u6c47\u6307\u4ee4\u4e0b\u9ad8\u6548\u63a2\u7d22\u73af\u5883\u5e76\u6784\u5efa\u8bed\u4e493D\u5730\u56fe\u7684\u95ee\u9898\u3002", "method": "VISTA\u7ed3\u5408\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u672a\u63a2\u7d22\u533a\u57df\u4f18\u5148\u7ea7\uff0c\u89c4\u5212\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u9ad8\u6548\u7684\u89c6\u89d2-\u8bed\u4e49\u8986\u76d6\u5ea6\u91cf\u3002", "result": "\u5728\u9759\u6001\u6570\u636e\u96c6\u4e0a\uff0cVISTA\u5728\u8ba1\u7b97\u901f\u5ea6\u548c\u91cd\u5efa\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1b\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff0c\u6210\u529f\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "VISTA\u5e73\u53f0\u65e0\u5173\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u5f00\u6e90\u3002"}}
{"id": "2507.01143", "categories": ["cs.RO", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01143", "abs": "https://arxiv.org/abs/2507.01143", "authors": ["Reza Jalayer", "Masoud Jalayer", "Amirali Baniasadi"], "title": "A Review on Sound Source Localization in Robotics: Focusing on Deep Learning Methods", "comment": "35 pages", "summary": "Sound source localization (SSL) adds a spatial dimension to auditory\nperception, allowing a system to pinpoint the origin of speech, machinery\nnoise, warning tones, or other acoustic events, capabilities that facilitate\nrobot navigation, human-machine dialogue, and condition monitoring. While\nexisting surveys provide valuable historical context, they typically address\ngeneral audio applications and do not fully account for robotic constraints or\nthe latest advancements in deep learning. This review addresses these gaps by\noffering a robotics-focused synthesis, emphasizing recent progress in deep\nlearning methodologies. We start by reviewing classical methods such as Time\nDifference of Arrival (TDOA), beamforming, Steered-Response Power (SRP), and\nsubspace analysis. Subsequently, we delve into modern machine learning (ML) and\ndeep learning (DL) approaches, discussing traditional ML and neural networks\n(NNs), convolutional neural networks (CNNs), convolutional recurrent neural\nnetworks (CRNNs), and emerging attention-based architectures. The data and\ntraining strategy that are the two cornerstones of DL-based SSL are explored.\nStudies are further categorized by robot types and application domains to\nfacilitate researchers in identifying relevant work for their specific\ncontexts. Finally, we highlight the current challenges in SSL works in general,\nregarding environmental robustness, sound source multiplicity, and specific\nimplementation constraints in robotics, as well as data and learning strategies\nin DL-based SSL. Also, we sketch promising directions to offer an actionable\nroadmap toward robust, adaptable, efficient, and explainable DL-based SSL for\nnext-generation robots.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u673a\u5668\u4eba\u9886\u57df\u4e2d\u7684\u58f0\u6e90\u5b9a\u4f4d\uff08SSL\uff09\u6280\u672f\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u63a2\u8ba8\u4e86\u5f53\u524d\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u7efc\u8ff0\u591a\u5173\u6ce8\u901a\u7528\u97f3\u9891\u5e94\u7528\uff0c\u672a\u5145\u5206\u8003\u8651\u673a\u5668\u4eba\u9886\u57df\u7684\u9650\u5236\u6216\u6df1\u5ea6\u5b66\u4e60\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u56de\u987e\u4e86\u7ecf\u5178\u65b9\u6cd5\uff08\u5982TDOA\u3001\u6ce2\u675f\u6210\u5f62\u7b49\uff09\u548c\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982CNN\u3001CRNN\u7b49\uff09\uff0c\u5e76\u5206\u6790\u4e86\u6570\u636e\u4e0e\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u603b\u7ed3\u4e86\u4e0d\u540c\u673a\u5668\u4eba\u7c7b\u578b\u548c\u5e94\u7528\u9886\u57df\u7684\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u5f53\u524dSSL\u7684\u6311\u6218\uff08\u5982\u73af\u5883\u9c81\u68d2\u6027\u3001\u591a\u58f0\u6e90\u95ee\u9898\u7b49\uff09\u3002", "conclusion": "\u4e3a\u4e0b\u4e00\u4ee3\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5b9e\u73b0\u9c81\u68d2\u3001\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684DL-based SSL\u7684\u884c\u52a8\u8def\u7ebf\u56fe\u3002"}}
{"id": "2507.01152", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01152", "abs": "https://arxiv.org/abs/2507.01152", "authors": ["Yunke Ao", "Masoud Moghani", "Mayank Mittal", "Manish Prajapat", "Luohong Wu", "Frederic Giraud", "Fabio Carrillo", "Andreas Krause", "Philipp F\u00fcrnstahl"], "title": "SonoGym: High Performance Simulation for Challenging Surgical Tasks with Robotic Ultrasound", "comment": "21 pages, 15 figures", "summary": "Ultrasound (US) is a widely used medical imaging modality due to its\nreal-time capabilities, non-invasive nature, and cost-effectiveness. Robotic\nultrasound can further enhance its utility by reducing operator dependence and\nimproving access to complex anatomical regions. For this, while deep\nreinforcement learning (DRL) and imitation learning (IL) have shown potential\nfor autonomous navigation, their use in complex surgical tasks such as anatomy\nreconstruction and surgical guidance remains limited -- largely due to the lack\nof realistic and efficient simulation environments tailored to these tasks. We\nintroduce SonoGym, a scalable simulation platform for complex robotic\nultrasound tasks that enables parallel simulation across tens to hundreds of\nenvironments. Our framework supports realistic and real-time simulation of US\ndata from CT-derived 3D models of the anatomy through both a physics-based and\na generative modeling approach. Sonogym enables the training of DRL and recent\nIL agents (vision transformers and diffusion policies) for relevant tasks in\nrobotic orthopedic surgery by integrating common robotic platforms and\northopedic end effectors. We further incorporate submodular DRL -- a recent\nmethod that handles history-dependent rewards -- for anatomy reconstruction and\nsafe reinforcement learning for surgery. Our results demonstrate successful\npolicy learning across a range of scenarios, while also highlighting the\nlimitations of current methods in clinically relevant environments. We believe\nour simulation can facilitate research in robot learning approaches for such\nchallenging robotic surgery applications. Dataset, codes, and videos are\npublicly available at https://sonogym.github.io/.", "AI": {"tldr": "SonoGym\u662f\u4e00\u4e2a\u7528\u4e8e\u590d\u6742\u673a\u5668\u4eba\u8d85\u58f0\u4efb\u52a1\u7684\u6a21\u62df\u5e73\u53f0\uff0c\u652f\u6301\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\uff0c\u65e8\u5728\u89e3\u51b3\u7f3a\u4e4f\u771f\u5b9e\u6a21\u62df\u73af\u5883\u7684\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u4eba\u8d85\u58f0\u5728\u590d\u6742\u624b\u672f\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u53d7\u9650\uff0c\u4e3b\u8981\u56e0\u7f3a\u4e4f\u771f\u5b9e\u9ad8\u6548\u7684\u6a21\u62df\u73af\u5883\u3002", "method": "\u5f00\u53d1SonoGym\u5e73\u53f0\uff0c\u7ed3\u5408\u7269\u7406\u548c\u751f\u6210\u5efa\u6a21\u65b9\u6cd5\uff0c\u652f\u6301\u5e76\u884c\u6a21\u62df\uff0c\u5e76\u96c6\u6210\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u6210\u529f\u8bad\u7ec3\u591a\u79cd\u7b56\u7565\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u4ecd\u6709\u5c40\u9650\u3002", "conclusion": "SonoGym\u6709\u671b\u63a8\u52a8\u673a\u5668\u4eba\u624b\u672f\u5b66\u4e60\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.01181", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01181", "abs": "https://arxiv.org/abs/2507.01181", "authors": ["Vinicius M. Gon\u00e7alves", "Shiqing Wei", "Eduardo Malacarne S. de Souza", "Krishnamurthy Prashanth", "Anthony Tzes", "Farshad Khorrami"], "title": "A Differentiable Distance Metric for Robotics Through Generalized Alternating Projection", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In many robotics applications, it is necessary to compute not only the\ndistance between the robot and the environment, but also its derivative - for\nexample, when using control barrier functions. However, since the traditional\nEuclidean distance is not differentiable, there is a need for alternative\ndistance metrics that possess this property. Recently, a metric with guaranteed\ndifferentiability was proposed [1]. This approach has some important drawbacks,\nwhich we address in this paper. We provide much simpler and practical\nexpressions for the smooth projection for general convex polytopes.\nAdditionally, as opposed to [1], we ensure that the distance vanishes as the\nobjects overlap. We show the efficacy of the approach in experimental results.\nOur proposed distance metric is publicly available through the Python-based\nsimulation package UAIBot.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u5fae\u8ddd\u79bb\u5ea6\u91cf\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u51f8\u591a\u9762\u4f53\u4e0a\u7684\u590d\u6742\u6027\u548c\u5b9e\u7528\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u4e0d\u53ef\u5fae\uff0c\u800c\u73b0\u6709\u53ef\u5fae\u8ddd\u79bb\u5ea6\u91cf\u65b9\u6cd5\u5b58\u5728\u590d\u6742\u6027\u548c\u5b9e\u7528\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u66f4\u7b80\u5355\u4e14\u5b9e\u7528\u7684\u5e73\u6ed1\u6295\u5f71\u8868\u8fbe\u5f0f\uff0c\u9002\u7528\u4e8e\u4e00\u822c\u51f8\u591a\u9762\u4f53\uff0c\u5e76\u786e\u4fdd\u8ddd\u79bb\u5728\u7269\u4f53\u91cd\u53e0\u65f6\u6d88\u5931\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u4e14\u5b9e\u7528\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u53ef\u5fae\u8ddd\u79bb\u5ea6\u91cf\u65b9\u6cd5\u7b80\u5316\u4e86\u8ba1\u7b97\uff0c\u63d0\u9ad8\u4e86\u5b9e\u7528\u6027\uff0c\u5e76\u901a\u8fc7\u516c\u5f00\u7684Python\u4eff\u771f\u5305UAIBot\u63d0\u4f9b\u4e86\u5b9e\u73b0\u3002"}}
{"id": "2507.01198", "categories": ["cs.RO", "cs.AI", "cs.CG"], "pdf": "https://arxiv.org/pdf/2507.01198", "abs": "https://arxiv.org/abs/2507.01198", "authors": ["Benjamin Kraljusic", "Zlatan Ajanovic", "Nermin Covic", "Bakir Lacevic"], "title": "Search-Based Robot Motion Planning With Distance-Based Adaptive Motion Primitives", "comment": "6 pages, 3 figures, submitted to a conference", "summary": "This work proposes a motion planning algorithm for robotic manipulators that\ncombines sampling-based and search-based planning methods. The core\ncontribution of the proposed approach is the usage of burs of free\nconfiguration space (C-space) as adaptive motion primitives within the graph\nsearch algorithm. Due to their feature to adaptively expand in free C-space,\nburs enable more efficient exploration of the configuration space compared to\nfixed-sized motion primitives, significantly reducing the time to find a valid\npath and the number of required expansions. The algorithm is implemented within\nthe existing SMPL (Search-Based Motion Planning Library) library and evaluated\nthrough a series of different scenarios involving manipulators with varying\nnumber of degrees-of-freedom (DoF) and environment complexity. Results\ndemonstrate that the bur-based approach outperforms fixed-primitive planning in\ncomplex scenarios, particularly for high DoF manipulators, while achieving\ncomparable performance in simpler scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u91c7\u6837\u548c\u641c\u7d22\u7684\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\uff0c\u5229\u7528\u81ea\u9002\u5e94\u8fd0\u52a8\u57fa\u5143\uff08burs\uff09\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u56fa\u5b9a\u5c3a\u5bf8\u8fd0\u52a8\u57fa\u5143\u5728\u590d\u6742\u573a\u666f\u4e2d\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u9ad8\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u3002", "method": "\u5728SMPL\u5e93\u4e2d\u5b9e\u73b0\uff0c\u5229\u7528burs\u4f5c\u4e3a\u81ea\u9002\u5e94\u8fd0\u52a8\u57fa\u5143\uff0c\u6269\u5c55\u81ea\u7531\u914d\u7f6e\u7a7a\u95f4\u3002", "result": "\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u56fa\u5b9a\u57fa\u5143\u65b9\u6cd5\uff0c\u9ad8\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u81ea\u9002\u5e94\u8fd0\u52a8\u57fa\u5143\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u66f4\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u9ad8\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u3002"}}
{"id": "2507.01206", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.01206", "abs": "https://arxiv.org/abs/2507.01206", "authors": ["Kathy Zhuang", "Zixun Huang", "Yukun Song", "Rui Li", "Yinuo Zhou", "Allen Y. Yang"], "title": "2024 NASA SUITS Report: LLM-Driven Immersive Augmented Reality User Interface for Robotics and Space Exploration", "comment": null, "summary": "As modern computing advances, new interaction paradigms have emerged,\nparticularly in Augmented Reality (AR), which overlays virtual interfaces onto\nphysical objects. This evolution poses challenges in machine perception,\nespecially for tasks like 3D object pose estimation in complex, dynamic\nenvironments. Our project addresses critical issues in human-robot interaction\nwithin mobile AR, focusing on non-intrusive, spatially aware interfaces. We\npresent URSA, an LLM-driven immersive AR system developed for NASA's 2023-2024\nSUITS challenge, targeting future spaceflight needs such as the Artemis\nmissions. URSA integrates three core technologies: a head-mounted AR device\n(e.g., HoloLens) for intuitive visual feedback, voice control powered by large\nlanguage models for hands-free interaction, and robot tracking algorithms that\nenable accurate 3D localization in dynamic settings. To enhance precision, we\nleverage digital twin localization technologies, using datasets like\nDTTD-Mobile and specialized hardware such as the ZED2 camera for real-world\ntracking under noise and occlusion. Our system enables real-time robot control\nand monitoring via an AR interface, even in the absence of ground-truth\nsensors--vital for hazardous or remote operations. Key contributions include:\n(1) a non-intrusive AR interface with LLM-based voice input; (2) a ZED2-based\ndataset tailored for non-rigid robotic bodies; (3) a Local Mission Control\nConsole (LMCC) for mission visualization; (4) a transformer-based 6DoF pose\nestimator (DTTDNet) optimized for depth fusion and real-time tracking; and (5)\nend-to-end integration for astronaut mission support. This work advances\ndigital twin applications in robotics, offering scalable solutions for both\naerospace and industrial domains.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86URSA\u7cfb\u7edf\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684AR\u7cfb\u7edf\uff0c\u7528\u4e8eNASA\u7684SUITS\u6311\u6218\uff0c\u6574\u5408\u4e86AR\u8bbe\u5907\u3001\u8bed\u97f3\u63a7\u5236\u548c\u673a\u5668\u4eba\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u652f\u6301\u5b9e\u65f6\u673a\u5668\u4eba\u63a7\u5236\u548c\u76d1\u63a7\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8AR\u4e2d\u7684\u4eba\u673a\u4ea4\u4e92\u95ee\u9898\uff0c\u7279\u522b\u662f\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u76843D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u4ee5\u6ee1\u8db3\u672a\u6765\u592a\u7a7a\u4efb\u52a1\uff08\u5982Artemis\uff09\u7684\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u4e86URSA\u7cfb\u7edf\uff0c\u7ed3\u5408\u5934\u6234\u5f0fAR\u8bbe\u5907\u3001LLM\u9a71\u52a8\u7684\u8bed\u97f3\u63a7\u5236\u548c\u673a\u5668\u4eba\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u5229\u7528\u6570\u5b57\u5b6a\u751f\u5b9a\u4f4d\u6280\u672f\u548cZED2\u76f8\u673a\u8fdb\u884c\u5b9e\u65f6\u8ddf\u8e2a\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e86\u975e\u4fb5\u5165\u5f0fAR\u754c\u9762\u3001\u4e13\u7528\u6570\u636e\u96c6\u3001\u4efb\u52a1\u53ef\u89c6\u5316\u63a7\u5236\u53f0\u3001\u4f18\u5316\u76846DoF\u59ff\u6001\u4f30\u8ba1\u5668\u548c\u7aef\u5230\u7aef\u96c6\u6210\uff0c\u652f\u6301\u5b87\u822a\u5458\u4efb\u52a1\u3002", "conclusion": "URSA\u7cfb\u7edf\u63a8\u52a8\u4e86\u6570\u5b57\u5b6a\u751f\u5728\u673a\u5668\u4eba\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u822a\u7a7a\u822a\u5929\u548c\u5de5\u4e1a\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01243", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01243", "abs": "https://arxiv.org/abs/2507.01243", "authors": ["Ziang Zheng", "Guojian Zhan", "Shiqi Liu", "Yao Lyu", "Tao Zhang", "Shengbo Eben Li"], "title": "Jump-Start Reinforcement Learning with Self-Evolving Priors for Extreme Monopedal Locomotion", "comment": null, "summary": "Reinforcement learning (RL) has shown great potential in enabling quadruped\nrobots to perform agile locomotion. However, directly training policies to\nsimultaneously handle dual extreme challenges, i.e., extreme underactuation and\nextreme terrains, as in monopedal hopping tasks, remains highly challenging due\nto unstable early-stage interactions and unreliable reward feedback. To address\nthis, we propose JumpER (jump-start reinforcement learning via self-evolving\npriors), an RL training framework that structures policy learning into multiple\nstages of increasing complexity. By dynamically generating self-evolving priors\nthrough iterative bootstrapping of previously learned policies, JumpER\nprogressively refines and enhances guidance, thereby stabilizing exploration\nand policy optimization without relying on external expert priors or\nhandcrafted reward shaping. Specifically, when integrated with a structured\nthree-stage curriculum that incrementally evolves action modality, observation\nspace, and task objective, JumpER enables quadruped robots to achieve robust\nmonopedal hopping on unpredictable terrains for the first time. Remarkably, the\nresulting policy effectively handles challenging scenarios that traditional\nmethods struggle to conquer, including wide gaps up to 60 cm, irregularly\nspaced stairs, and stepping stones with distances varying from 15 cm to 35 cm.\nJumpER thus provides a principled and scalable approach for addressing\nlocomotion tasks under the dual challenges of extreme underactuation and\nextreme terrains.", "AI": {"tldr": "JumpER\u662f\u4e00\u79cd\u901a\u8fc7\u81ea\u8fdb\u5316\u5148\u9a8c\u5206\u9636\u6bb5\u8bad\u7ec3\u7684\u7b56\u7565\uff0c\u5e2e\u52a9\u56db\u8db3\u673a\u5668\u4eba\u5728\u6781\u7aef\u6b20\u9a71\u52a8\u548c\u5730\u5f62\u6761\u4ef6\u4e0b\u5b9e\u73b0\u7a33\u5065\u5355\u8db3\u8df3\u8dc3\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u6781\u7aef\u6b20\u9a71\u52a8\u548c\u5730\u5f62\u6761\u4ef6\u4e0b\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u5956\u52b1\u53cd\u9988\u4e0d\u53ef\u9760\u7684\u95ee\u9898\u3002", "method": "\u5206\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u52a8\u6001\u751f\u6210\u81ea\u8fdb\u5316\u5148\u9a8c\uff0c\u9010\u6b65\u4f18\u5316\u7b56\u7565\uff0c\u65e0\u9700\u5916\u90e8\u4e13\u5bb6\u6216\u624b\u5de5\u5956\u52b1\u8bbe\u8ba1\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u56db\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u53ef\u9884\u6d4b\u5730\u5f62\u4e0a\u7684\u7a33\u5065\u5355\u8db3\u8df3\u8dc3\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u7684\u6311\u6218\u3002", "conclusion": "JumpER\u4e3a\u6781\u7aef\u6761\u4ef6\u4e0b\u7684\u8fd0\u52a8\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01264", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01264", "abs": "https://arxiv.org/abs/2507.01264", "authors": ["Yongjie Fu", "Ruijian Zha", "Pei Tian", "Xuan Di"], "title": "LLM-based Realistic Safety-Critical Driving Video Generation", "comment": null, "summary": "Designing diverse and safety-critical driving scenarios is essential for\nevaluating autonomous driving systems. In this paper, we propose a novel\nframework that leverages Large Language Models (LLMs) for few-shot code\ngeneration to automatically synthesize driving scenarios within the CARLA\nsimulator, which has flexibility in scenario scripting, efficient code-based\ncontrol of traffic participants, and enforcement of realistic physical\ndynamics. Given a few example prompts and code samples, the LLM generates\nsafety-critical scenario scripts that specify the behavior and placement of\ntraffic participants, with a particular focus on collision events. To bridge\nthe gap between simulation and real-world appearance, we integrate a video\ngeneration pipeline using Cosmos-Transfer1 with ControlNet, which converts\nrendered scenes into realistic driving videos. Our approach enables\ncontrollable scenario generation and facilitates the creation of rare but\ncritical edge cases, such as pedestrian crossings under occlusion or sudden\nvehicle cut-ins. Experimental results demonstrate the effectiveness of our\nmethod in generating a wide range of realistic, diverse, and safety-critical\nscenarios, offering a promising tool for simulation-based testing of autonomous\nvehicles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u9a7e\u9a76\u573a\u666f\u4ee3\u7801\u7684\u6846\u67b6\uff0c\u7ed3\u5408CARLA\u6a21\u62df\u5668\u548c\u89c6\u9891\u751f\u6210\u6280\u672f\uff0c\u7528\u4e8e\u81ea\u52a8\u5408\u6210\u591a\u6837\u4e14\u5b89\u5168\u5173\u952e\u7684\u9a7e\u9a76\u573a\u666f\u3002", "motivation": "\u8bbe\u8ba1\u591a\u6837\u4e14\u5b89\u5168\u5173\u952e\u7684\u9a7e\u9a76\u573a\u666f\u5bf9\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u624b\u52a8\u8bbe\u8ba1\u8017\u65f6\u4e14\u96be\u4ee5\u8986\u76d6\u8fb9\u7f18\u60c5\u51b5\u3002", "method": "\u5229\u7528LLMs\u8fdb\u884c\u5c11\u6837\u672c\u4ee3\u7801\u751f\u6210\uff0c\u7ed3\u5408CARLA\u6a21\u62df\u5668\u548cControlNet\u89c6\u9891\u751f\u6210\u6280\u672f\uff0c\u81ea\u52a8\u751f\u6210\u573a\u666f\u811a\u672c\u548c\u903c\u771f\u89c6\u9891\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u751f\u6210\u591a\u6837\u3001\u903c\u771f\u4e14\u5b89\u5168\u5173\u952e\u7684\u9a7e\u9a76\u573a\u666f\uff0c\u5305\u62ec\u7f55\u89c1\u8fb9\u7f18\u60c5\u51b5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u4eff\u771f\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u5de5\u5177\u3002"}}
{"id": "2507.01284", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01284", "abs": "https://arxiv.org/abs/2507.01284", "authors": ["Cristian Gariboldi", "Hayato Tokida", "Ken Kinjo", "Yuki Asada", "Alexander Carballo"], "title": "VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process", "comment": "2025 IEEE 28th International Conference on Intelligent Transportation\n  Systems (ITSC)", "summary": "Recent advancements in open-source Visual Language Models (VLMs) such as\nLLaVA, Qwen-VL, and Llama have catalyzed extensive research on their\nintegration with diverse systems. The internet-scale general knowledge\nencapsulated within these models presents significant opportunities for\nenhancing autonomous driving perception, prediction, and planning capabilities.\nIn this paper we propose VLAD, a vision-language autonomous driving model,\nwhich integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end\nsystem. We implement a specialized fine-tuning approach using custom\nquestion-answer datasets designed specifically to improve the spatial reasoning\ncapabilities of the model. The enhanced VLM generates high-level navigational\ncommands that VAD subsequently processes to guide vehicle operation.\nAdditionally, our system produces interpretable natural language explanations\nof driving decisions, thereby increasing transparency and trustworthiness of\nthe traditionally black-box end-to-end architecture. Comprehensive evaluation\non the real-world nuScenes dataset demonstrates that our integrated system\nreduces average collision rates by 31.82% compared to baseline methodologies,\nestablishing a new benchmark for VLM-augmented autonomous driving systems.", "AI": {"tldr": "VLAD\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e0e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff08VAD\uff09\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u611f\u77e5\u4e0e\u89c4\u5212\u80fd\u529b\uff0c\u5e76\u751f\u6210\u53ef\u89e3\u91ca\u7684\u9a7e\u9a76\u51b3\u7b56\u3002", "motivation": "\u5229\u7528\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982LLaVA\u3001Qwen-VL\uff09\u7684\u901a\u7528\u77e5\u8bc6\uff0c\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6027\u80fd\u4e0e\u900f\u660e\u5ea6\u3002", "method": "\u91c7\u7528\u5b9a\u5236\u95ee\u7b54\u6570\u636e\u96c6\u5bf9VLM\u8fdb\u884c\u5fae\u8c03\uff0c\u63d0\u5347\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u751f\u6210\u9ad8\u7ea7\u5bfc\u822a\u6307\u4ee4\u4f9bVAD\u5904\u7406\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u78b0\u649e\u7387\u964d\u4f4e31.82%\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "VLAD\u4e3aVLM\u589e\u5f3a\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.01308", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01308", "abs": "https://arxiv.org/abs/2507.01308", "authors": ["Muhammad Atta ur Rahman", "Dooseop Choi", "KyoungWook Min"], "title": "LANet: A Lane Boundaries-Aware Approach For Robust Trajectory Prediction", "comment": "Accepted at the 17th IEEE International Conference on Advanced\n  Computational Intelligence (ICACI 2025)", "summary": "Accurate motion forecasting is critical for safe and efficient autonomous\ndriving, enabling vehicles to predict future trajectories and make informed\ndecisions in complex traffic scenarios. Most of the current designs of motion\nprediction models are based on the major representation of lane centerlines,\nwhich limits their capability to capture critical road environments and traffic\nrules and constraints. In this work, we propose an enhanced motion forecasting\nmodel informed by multiple vector map elements, including lane boundaries and\nroad edges, that facilitates a richer and more complete representation of\ndriving environments. An effective feature fusion strategy is developed to\nmerge information in different vector map components, where the model learns\nholistic information on road structures and their interactions with agents.\nSince encoding more information about the road environment increases memory\nusage and is computationally expensive, we developed an effective pruning\nmechanism that filters the most relevant map connections to the target agent,\nensuring computational efficiency while maintaining essential spatial and\nsemantic relationships for accurate trajectory prediction. Overcoming the\nlimitations of lane centerline-based models, our method provides a more\ninformative and efficient representation of the driving environment and\nadvances the state of the art for autonomous vehicle motion forecasting. We\nverify our approach with extensive experiments on the Argoverse 2 motion\nforecasting dataset, where our method maintains competitiveness on AV2 while\nachieving improved performance.\n  Index Terms-Autonomous driving, trajectory prediction, vector map elements,\nroad topology, connection pruning, Argoverse 2.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5411\u91cf\u5730\u56fe\u5143\u7d20\u7684\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u878d\u5408\u8f66\u9053\u8fb9\u754c\u548c\u9053\u8def\u8fb9\u7f18\u7b49\u4fe1\u606f\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u590d\u6742\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u8f68\u8ff9\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u8f66\u9053\u4e2d\u5fc3\u7ebf\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u9053\u8def\u73af\u5883\u548c\u4ea4\u901a\u89c4\u5219\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u5168\u9762\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7279\u5f81\u878d\u5408\u7b56\u7565\uff0c\u7ed3\u5408\u4e0d\u540c\u5411\u91cf\u5730\u56fe\u7ec4\u4ef6\u7684\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u526a\u679d\u673a\u5236\u4ee5\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728Argoverse 2\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u66f4\u5168\u9762\u7684\u9053\u8def\u73af\u5883\u8868\u793a\u548c\u9ad8\u6548\u7684\u8ba1\u7b97\u673a\u5236\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u9884\u6d4b\u7684\u5148\u8fdb\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.01424", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01424", "abs": "https://arxiv.org/abs/2507.01424", "authors": ["Zhenyang Liu", "Yongchong Gu", "Sixiao Zheng", "Xiangyang Xue", "Yanwei Fu"], "title": "TriVLA: A Unified Triple-System-Based Unified Vision-Language-Action Model for General Robot Control", "comment": null, "summary": "Recent advancements in vision-language models (VLMs) for common-sense\nreasoning have led to the development of vision-language-action (VLA) models,\nenabling robots to perform generalized manipulation. Although existing\nautoregressive VLA methods design a specific architecture like dual-system to\nleverage large-scale pretrained knowledge, they tend to capture static\ninformation, often neglecting the dynamic aspects vital for embodied tasks. To\nthis end, we propose TriVLA, a unified Vision-Language-Action model with a\ntriple-system architecture for general robot control. The vision-language\nmodule (System 2) interprets the environment through vision and language\ninstructions. The dynamics perception module (System 3) inherently produces\nvisual representations that encompass both current static information and\npredicted future dynamics, thereby providing valuable guidance for policy\nlearning. TriVLA utilizes pre-trained VLM model and fine-tunes pre-trained\nvideo foundation model on robot datasets along with internet human manipulation\ndata. The subsequent policy learning module (System 1) generates fluid motor\nactions in real time. Experimental evaluation demonstrates that TriVLA operates\nat approximately 36 Hz and surpasses state-of-the-art imitation learning\nbaselines on standard simulation benchmarks as well as challenging real-world\nmanipulation tasks.", "AI": {"tldr": "TriVLA\u662f\u4e00\u79cd\u7edf\u4e00\u7684\u4e09\u7cfb\u7edf\u67b6\u6784\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u7528\u4e8e\u901a\u7528\u673a\u5668\u4eba\u63a7\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u611f\u77e5\u6a21\u5757\u6355\u6349\u9759\u6001\u548c\u52a8\u6001\u4fe1\u606f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u81ea\u56de\u5f52VLA\u65b9\u6cd5\u5e38\u5ffd\u7565\u52a8\u6001\u4fe1\u606f\uff0c\u800c\u52a8\u6001\u4fe1\u606f\u5bf9\u5177\u8eab\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "method": "TriVLA\u91c7\u7528\u4e09\u7cfb\u7edf\u67b6\u6784\uff0c\u5305\u62ec\u89c6\u89c9-\u8bed\u8a00\u6a21\u5757\u3001\u52a8\u6001\u611f\u77e5\u6a21\u5757\u548c\u7b56\u7565\u5b66\u4e60\u6a21\u5757\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u673a\u5668\u4eba\u6570\u636e\u96c6\u3002", "result": "TriVLA\u4ee5\u7ea636 Hz\u8fd0\u884c\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "TriVLA\u901a\u8fc7\u52a8\u6001\u611f\u77e5\u548c\u7edf\u4e00\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u63a7\u5236\u7684\u6027\u80fd\u3002"}}
{"id": "2507.01426", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.01426", "abs": "https://arxiv.org/abs/2507.01426", "authors": ["Ratnangshu Das", "Pushpak Jagtap"], "title": "Approximation-free Control of Unknown Euler-Lagrangian Systems under Input Constraints", "comment": null, "summary": "In this paper, we present a novel funnel-based tracking control algorithm for\nrobotic systems with unknown dynamics and prescribed input constraints. The\nEuler-Lagrange formulation, a common modeling approach for robotic systems, has\nbeen adopted in this study to address the trade-off between performance and\nactuator safety. We establish feasibility conditions that ensure tracking\nerrors evolve within predefined funnel bounds while maintaining bounded control\nefforts, a crucial consideration for robots with limited actuation\ncapabilities. We propose two approximation-free control strategies for\nscenarios where these conditions are violated: one actively corrects the error,\nand the other stops further deviation. Finally, we demonstrate the robust\nperformance and safety of the approach through simulations and experimental\nvalidations. This work represents a significant advancement in funnel-based\ncontrol, enhancing its applicability to real-world robotics systems with input\nconstraints.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6f0f\u6597\u8ddf\u8e2a\u63a7\u5236\u7b97\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u672a\u77e5\u52a8\u6001\u548c\u8f93\u5165\u7ea6\u675f\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e24\u79cd\u65e0\u8fd1\u4f3c\u63a7\u5236\u7b56\u7565\u786e\u4fdd\u8ddf\u8e2a\u8bef\u5dee\u5728\u9884\u8bbe\u8303\u56f4\u5185\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u6027\u80fd\u4e0e\u6267\u884c\u5668\u5b89\u5168\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8f93\u5165\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u91c7\u7528\u6b27\u62c9-\u62c9\u683c\u6717\u65e5\u5efa\u6a21\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e24\u79cd\u65e0\u8fd1\u4f3c\u63a7\u5236\u7b56\u7565\uff1a\u4e00\u79cd\u4e3b\u52a8\u7ea0\u6b63\u8bef\u5dee\uff0c\u53e6\u4e00\u79cd\u963b\u6b62\u8fdb\u4e00\u6b65\u504f\u79bb\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u663e\u8457\u63d0\u5347\u4e86\u6f0f\u6597\u63a7\u5236\u5728\u8f93\u5165\u53d7\u9650\u7684\u5b9e\u9645\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u6027\u3002"}}
{"id": "2507.01462", "categories": ["cs.RO", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.01462", "abs": "https://arxiv.org/abs/2507.01462", "authors": ["Eneko Osaba", "Estibaliz Garrote", "Pablo Miranda-Rodriguez", "Alessia Ciacco", "Itziar Cabanes", "Aitziber Mancisidor"], "title": "Quantum-Assisted Automatic Path-Planning for Robotic Quality Inspection in Industry 4.0", "comment": "2 pages, 1 figure, paper accepted for presentation at the IEEE\n  International Conference on Quantum Computing and Engineering (QCE)", "summary": "This work explores the application of hybrid quantum-classical algorithms to\noptimize robotic inspection trajectories derived from Computer-Aided Design\n(CAD) models in industrial settings. By modeling the task as a 3D variant of\nthe Traveling Salesman Problem, incorporating incomplete graphs and open-route\nconstraints, this study evaluates the performance of two D-Wave-based solvers\nagainst classical methods such as GUROBI and Google OR-Tools. Results across\nfive real-world cases demonstrate competitive solution quality with\nsignificantly reduced computation times, highlighting the potential of quantum\napproaches in automation under Industry 4.0.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u7b97\u6cd5\u5728\u4f18\u5316\u5de5\u4e1a\u73af\u5883\u4e2d\u57fa\u4e8eCAD\u6a21\u578b\u7684\u673a\u5668\u4eba\u68c0\u6d4b\u8f68\u8ff9\u4e2d\u7684\u5e94\u7528\uff0c\u7ed3\u679c\u663e\u793a\u91cf\u5b50\u65b9\u6cd5\u5728\u8ba1\u7b97\u65f6\u95f4\u4e0a\u663e\u8457\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u91cf\u5b50\u8ba1\u7b97\u5728\u5de5\u4e1a\u81ea\u52a8\u5316\uff08\u5982Industry 4.0\uff09\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u4f18\u5316\u673a\u5668\u4eba\u68c0\u6d4b\u8def\u5f84\u65b9\u9762\u3002", "method": "\u5c06\u4efb\u52a1\u5efa\u6a21\u4e3a3D\u65c5\u884c\u5546\u95ee\u9898\u7684\u53d8\u4f53\uff0c\u6bd4\u8f83D-Wave\u91cf\u5b50\u6c42\u89e3\u5668\u4e0e\u7ecf\u5178\u65b9\u6cd5\uff08\u5982GUROBI\u548cGoogle OR-Tools\uff09\u7684\u6027\u80fd\u3002", "result": "\u5728\u4e94\u4e2a\u5b9e\u9645\u6848\u4f8b\u4e2d\uff0c\u91cf\u5b50\u65b9\u6cd5\u5728\u89e3\u8d28\u91cf\u548c\u8ba1\u7b97\u65f6\u95f4\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u91cf\u5b50\u65b9\u6cd5\u5728\u5de5\u4e1a\u81ea\u52a8\u5316\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u4f18\u5316\u590d\u6742\u8def\u5f84\u95ee\u9898\u4e0a\u3002"}}
{"id": "2507.01485", "categories": ["cs.RO", "cs.AI", "cs.MA", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.01485", "abs": "https://arxiv.org/abs/2507.01485", "authors": ["Yibo Qiu", "Zan Huang", "Zhiyu Wang", "Handi Liu", "Yiling Qiao", "Yifeng Hu", "Shu'ang Sun", "Hangke Peng", "Ronald X Xu", "Mingzhai Sun"], "title": "BioMARS: A Multi-Agent Robotic System for Autonomous Biological Experiments", "comment": null, "summary": "Large language models (LLMs) and vision-language models (VLMs) have the\npotential to transform biological research by enabling autonomous\nexperimentation. Yet, their application remains constrained by rigid protocol\ndesign, limited adaptability to dynamic lab conditions, inadequate error\nhandling, and high operational complexity. Here we introduce BioMARS\n(Biological Multi-Agent Robotic System), an intelligent platform that\nintegrates LLMs, VLMs, and modular robotics to autonomously design, plan, and\nexecute biological experiments. BioMARS uses a hierarchical architecture: the\nBiologist Agent synthesizes protocols via retrieval-augmented generation; the\nTechnician Agent translates them into executable robotic pseudo-code; and the\nInspector Agent ensures procedural integrity through multimodal perception and\nanomaly detection. The system autonomously conducts cell passaging and culture\ntasks, matching or exceeding manual performance in viability, consistency, and\nmorphological integrity. It also supports context-aware optimization,\noutperforming conventional strategies in differentiating retinal pigment\nepithelial cells. A web interface enables real-time human-AI collaboration,\nwhile a modular backend allows scalable integration with laboratory hardware.\nThese results highlight the feasibility of generalizable, AI-driven laboratory\nautomation and the transformative role of language-based reasoning in\nbiological research.", "AI": {"tldr": "BioMARS\u662f\u4e00\u4e2a\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c\u6a21\u5757\u5316\u673a\u5668\u4eba\u7684\u667a\u80fd\u5e73\u53f0\uff0c\u7528\u4e8e\u81ea\u4e3b\u8bbe\u8ba1\u3001\u89c4\u5212\u548c\u6267\u884c\u751f\u7269\u5b9e\u9a8c\u3002", "motivation": "\u5f53\u524dLLMs\u548cVLMs\u5728\u751f\u7269\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u50f5\u5316\u7684\u534f\u8bae\u8bbe\u8ba1\u3001\u52a8\u6001\u5b9e\u9a8c\u5ba4\u6761\u4ef6\u7684\u9002\u5e94\u6027\u4e0d\u8db3\u3001\u9519\u8bef\u5904\u7406\u80fd\u529b\u6709\u9650\u4ee5\u53ca\u64cd\u4f5c\u590d\u6742\u6027\u9ad8\u3002", "method": "BioMARS\u91c7\u7528\u5206\u5c42\u67b6\u6784\uff1aBiologist Agent\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5408\u6210\u534f\u8bae\uff1bTechnician Agent\u5c06\u5176\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u4f2a\u4ee3\u7801\uff1bInspector Agent\u901a\u8fc7\u591a\u6a21\u6001\u611f\u77e5\u548c\u5f02\u5e38\u68c0\u6d4b\u786e\u4fdd\u7a0b\u5e8f\u5b8c\u6574\u6027\u3002", "result": "\u7cfb\u7edf\u5728\u7ec6\u80de\u4f20\u4ee3\u548c\u57f9\u517b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u4eba\u5de5\u64cd\u4f5c\u7684\u5b58\u6d3b\u7387\u3001\u4e00\u81f4\u6027\u548c\u5f62\u6001\u5b8c\u6574\u6027\uff0c\u5e76\u5728\u89c6\u7f51\u819c\u8272\u7d20\u4e0a\u76ae\u7ec6\u80de\u5206\u5316\u4e2d\u4f18\u4e8e\u4f20\u7edf\u7b56\u7565\u3002", "conclusion": "BioMARS\u5c55\u793a\u4e86\u901a\u7528\u5316AI\u9a71\u52a8\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u7684\u53ef\u884c\u6027\uff0c\u4ee5\u53ca\u57fa\u4e8e\u8bed\u8a00\u63a8\u7406\u5728\u751f\u7269\u7814\u7a76\u4e2d\u7684\u53d8\u9769\u6027\u4f5c\u7528\u3002"}}
{"id": "2507.01550", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01550", "abs": "https://arxiv.org/abs/2507.01550", "authors": ["Johannes Kohl", "Georg Muck", "Georg J\u00e4ger", "Sebastian Zug"], "title": "Dynamic System Model Generation for Online Fault Detection and Diagnosis of Robotic Systems", "comment": "Accepted for publication in Ada User Journal", "summary": "With the rapid development of more complex robots, Fault Detection and\nDiagnosis (FDD) becomes increasingly harder. Especially the need for\npredetermined models and historic data is problematic because they do not\nencompass the dynamic and fast-changing nature of such systems. To this end, we\npropose a concept that actively generates a dynamic system model at runtime and\nutilizes it to locate root causes. The goal is to be applicable to all kinds of\nrobotic systems that share a similar software design. Additionally, it should\nexhibit minimal overhead and enhance independence from expert attention.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u751f\u6210\u7cfb\u7edf\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6545\u969c\u68c0\u6d4b\u4e0e\u8bca\u65ad\uff0c\u51cf\u5c11\u5bf9\u9884\u5b9a\u4e49\u6a21\u578b\u548c\u5386\u53f2\u6570\u636e\u7684\u4f9d\u8d56\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u7cfb\u7edf\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0c\u4f20\u7edf\u6545\u969c\u68c0\u6d4b\u4e0e\u8bca\u65ad\u65b9\u6cd5\u56e0\u4f9d\u8d56\u9884\u5b9a\u4e49\u6a21\u578b\u548c\u5386\u53f2\u6570\u636e\u800c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u3002", "method": "\u5728\u8fd0\u884c\u65f6\u52a8\u6001\u751f\u6210\u7cfb\u7edf\u6a21\u578b\uff0c\u5e76\u5229\u7528\u8be5\u6a21\u578b\u5b9a\u4f4d\u6545\u969c\u6839\u6e90\uff0c\u9002\u7528\u4e8e\u5177\u6709\u76f8\u4f3c\u8f6f\u4ef6\u8bbe\u8ba1\u7684\u5404\u7c7b\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "result": "\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u5bf9\u4e13\u5bb6\u5e72\u9884\u7684\u9700\u6c42\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u8f83\u4f4e\u7684\u8fd0\u884c\u5f00\u9500\u3002", "conclusion": "\u63d0\u51fa\u7684\u6982\u5ff5\u4e3a\u52a8\u6001\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6545\u969c\u68c0\u6d4b\u4e0e\u8bca\u65ad\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01561", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01561", "abs": "https://arxiv.org/abs/2507.01561", "authors": ["Huijiang Wang", "Holger Kunz", "Timon Adler", "Fumiya Iida"], "title": "Self-Closing Suction Grippers for Industrial Grasping via Form-Flexible Design", "comment": "This manuscript has been submitted for potential consideration at\n  IEEE publication venues", "summary": "Shape-morphing robots have shown benefits in industrial grasping. We propose\nform-flexible grippers for adaptive grasping. The design is based on the hybrid\njamming and suction mechanism, which deforms to handle objects that vary\nsignificantly in size from the aperture, including both larger and smaller\nparts. Compared with traditional grippers, the gripper achieves self-closing to\nform an airtight seal. Under a vacuum, a wide range of grasping is realized\nthrough the passive morphing mechanism at the interface that harmonizes\npressure and flow rate. This hybrid gripper showcases the capability to\nsecurely grasp an egg, as small as 54.5% of its aperture, while achieving a\nmaximum load-to-mass ratio of 94.3.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u5835\u585e\u548c\u5438\u529b\u673a\u5236\u7684\u5f62\u53d8\u67d4\u6027\u5939\u5177\uff0c\u7528\u4e8e\u81ea\u9002\u5e94\u6293\u53d6\uff0c\u80fd\u591f\u5904\u7406\u5c3a\u5bf8\u5dee\u5f02\u5927\u7684\u7269\u4f53\u3002", "motivation": "\u4f20\u7edf\u5939\u5177\u96be\u4ee5\u9002\u5e94\u5c3a\u5bf8\u53d8\u5316\u5927\u7684\u7269\u4f53\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u6df7\u5408\u5835\u585e\u548c\u5438\u529b\u673a\u5236\uff0c\u901a\u8fc7\u88ab\u52a8\u5f62\u53d8\u673a\u5236\u5b9e\u73b0\u81ea\u9002\u5e94\u6293\u53d6\u3002", "result": "\u5939\u5177\u80fd\u591f\u6293\u53d6\u5c0f\u81f354.5%\u5b54\u5f84\u7684\u7269\u4f53\uff0c\u6700\u5927\u8d1f\u8f7d\u8d28\u91cf\u6bd4\u4e3a94.3\u3002", "conclusion": "\u6df7\u5408\u5939\u5177\u5c55\u793a\u4e86\u5728\u5de5\u4e1a\u6293\u53d6\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2507.01697", "categories": ["cs.RO", "math.OC", "00A69, 93C85, 14H55", "I.2.9"], "pdf": "https://arxiv.org/pdf/2507.01697", "abs": "https://arxiv.org/abs/2507.01697", "authors": ["Yu Zhang", "Qi Zhou", "Xiao-Song Yang"], "title": "An RRT* algorithm based on Riemannian metric model for optimal path planning", "comment": "27 pages", "summary": "This paper presents a Riemannian metric-based model to solve the optimal path\nplanning problem on two-dimensional smooth submanifolds in high-dimensional\nspace. Our model is based on constructing a new Riemannian metric on a\ntwo-dimensional projection plane, which is induced by the high-dimensional\nEuclidean metric on two-dimensional smooth submanifold and reflects the\nenvironmental information of the robot. The optimal path planning problem in\nhigh-dimensional space is therefore transformed into a geometric problem on the\ntwo-dimensional plane with new Riemannian metric. Based on the new Riemannian\nmetric, we proposed an incremental algorithm RRT*-R on the projection plane.\nThe experimental results show that the proposed algorithm is suitable for\nscenarios with uneven fields in multiple dimensions. The proposed algorithm can\nhelp the robot to effectively avoid areas with drastic changes in height,\nground resistance and other environmental factors. More importantly, the RRT*-R\nalgorithm shows better smoothness and optimization properties compared with the\noriginal RRT* algorithm using Euclidean distance in high-dimensional workspace.\nThe length of the entire path by RRT*-R is a good approximation of the\ntheoretical minimum geodesic distance on projection plane.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ece\u66fc\u5ea6\u91cf\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u4e8c\u7ef4\u5149\u6ed1\u5b50\u6d41\u5f62\u4e0a\u7684\u6700\u4f18\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u6784\u5efa\u65b0\u7684\u9ece\u66fc\u5ea6\u91cf\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u4e8c\u7ef4\u5e73\u9762\u4e0a\u7684\u51e0\u4f55\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86RRT*-R\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u73af\u5883\u4e0b\u907f\u514d\u9ad8\u5ea6\u53d8\u5316\u3001\u5730\u9762\u963b\u529b\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\u3002", "method": "\u6784\u5efa\u65b0\u7684\u9ece\u66fc\u5ea6\u91cf\uff0c\u5c06\u9ad8\u7ef4\u7a7a\u95f4\u95ee\u9898\u8f6c\u5316\u4e3a\u4e8c\u7ef4\u5e73\u9762\u4e0a\u7684\u51e0\u4f55\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u589e\u91cf\u7b97\u6cd5RRT*-R\u3002", "result": "RRT*-R\u7b97\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u8def\u5f84\u5e73\u6ed1\u4e14\u4f18\u5316\u6548\u679c\u4f18\u4e8e\u539f\u59cbRRT*\u7b97\u6cd5\uff0c\u8def\u5f84\u957f\u5ea6\u63a5\u8fd1\u7406\u8bba\u6700\u5c0f\u6d4b\u5730\u8ddd\u79bb\u3002", "conclusion": "RRT*-R\u7b97\u6cd5\u5728\u9ad8\u7ef4\u7a7a\u95f4\u8def\u5f84\u89c4\u5212\u4e2d\u5177\u6709\u66f4\u597d\u7684\u9002\u5e94\u6027\u548c\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2507.01705", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01705", "abs": "https://arxiv.org/abs/2507.01705", "authors": ["Marc-Philip Ecker", "Bernhard Bischof", "Minh Nhat Vu", "Christoph Fr\u00f6hlich", "Tobias Gl\u00fcck", "Wolfgang Kemmetm\u00fcller"], "title": "Efficient Collision Detection for Long and Slender Robotic Links in Euclidean Distance Fields: Application to a Forestry Crane", "comment": "Accepted at IROS 2025", "summary": "Collision-free motion planning in complex outdoor environments relies heavily\non perceiving the surroundings through exteroceptive sensors. A widely used\napproach represents the environment as a voxelized Euclidean distance field,\nwhere robots are typically approximated by spheres. However, for large-scale\nmanipulators such as forestry cranes, which feature long and slender links,\nthis conventional spherical approximation becomes inefficient and inaccurate.\nThis work presents a novel collision detection algorithm specifically designed\nto exploit the elongated structure of such manipulators, significantly\nenhancing the computational efficiency of motion planning algorithms. Unlike\ntraditional sphere decomposition methods, our approach not only improves\ncomputational efficiency but also naturally eliminates the need to fine-tune\nthe approximation accuracy as an additional parameter. We validate the\nalgorithm's effectiveness using real-world LiDAR data from a forestry crane\napplication, as well as simulated environment data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7ec6\u957f\u673a\u68b0\u81c2\u7684\u65b0\u578b\u78b0\u649e\u68c0\u6d4b\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8fd0\u52a8\u89c4\u5212\u7684\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7403\u5f62\u8fd1\u4f3c\u65b9\u6cd5\u5bf9\u4e8e\u7ec6\u957f\u673a\u68b0\u81c2\u6548\u7387\u4f4e\u4e14\u4e0d\u51c6\u786e\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u5229\u7528\u673a\u68b0\u81c2\u7684\u7ec6\u957f\u7ed3\u6784\u8bbe\u8ba1\u65b0\u578b\u78b0\u649e\u68c0\u6d4b\u7b97\u6cd5\uff0c\u65e0\u9700\u8c03\u6574\u8fd1\u4f3c\u7cbe\u5ea6\u53c2\u6570\u3002", "result": "\u5728\u771f\u5b9eLiDAR\u6570\u636e\u548c\u6a21\u62df\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u7ec6\u957f\u673a\u68b0\u81c2\u7684\u8fd0\u52a8\u89c4\u5212\u3002"}}
{"id": "2507.01723", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01723", "abs": "https://arxiv.org/abs/2507.01723", "authors": ["Xupeng Zhu", "Fan Wang", "Robin Walters", "Jane Shi"], "title": "SE(3)-Equivariant Diffusion Policy in Spherical Fourier Space", "comment": "Accepted at ICML 2025", "summary": "Diffusion Policies are effective at learning closed-loop manipulation\npolicies from human demonstrations but generalize poorly to novel arrangements\nof objects in 3D space, hurting real-world performance. To address this issue,\nwe propose Spherical Diffusion Policy (SDP), an SE(3) equivariant diffusion\npolicy that adapts trajectories according to 3D transformations of the scene.\nSuch equivariance is achieved by embedding the states, actions, and the\ndenoising process in spherical Fourier space. Additionally, we employ novel\nspherical FiLM layers to condition the action denoising process equivariantly\non the scene embeddings. Lastly, we propose a spherical denoising temporal\nU-net that achieves spatiotemporal equivariance with computational efficiency.\nIn the end, SDP is end-to-end SE(3) equivariant, allowing robust generalization\nacross transformed 3D scenes. SDP demonstrates a large performance improvement\nover strong baselines in 20 simulation tasks and 5 physical robot tasks\nincluding single-arm and bi-manual embodiments. Code is available at\nhttps://github.com/amazon-science/Spherical_Diffusion_Policy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdSE(3)\u7b49\u53d8\u7684\u6269\u6563\u7b56\u7565\uff08SDP\uff09\uff0c\u901a\u8fc7\u5c06\u72b6\u6001\u3001\u52a8\u4f5c\u548c\u53bb\u566a\u8fc7\u7a0b\u5d4c\u5165\u7403\u9762\u5085\u91cc\u53f6\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u5bf93D\u573a\u666f\u53d8\u6362\u7684\u9002\u5e94\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u7b56\u7565\u57283D\u7a7a\u95f4\u4e2d\u7269\u4f53\u65b0\u6392\u5217\u65f6\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u5f71\u54cd\u5b9e\u9645\u6027\u80fd\u3002", "method": "\u63d0\u51faSDP\uff0c\u5229\u7528\u7403\u9762\u5085\u91cc\u53f6\u7a7a\u95f4\u5d4c\u5165\u548c\u7403\u9762FiLM\u5c42\uff0c\u5b9e\u73b0SE(3)\u7b49\u53d8\uff1b\u8bbe\u8ba1\u4e86\u7403\u9762\u53bb\u566a\u65f6\u5e8fU-net\uff0c\u9ad8\u6548\u5b9e\u73b0\u65f6\u7a7a\u7b49\u53d8\u3002", "result": "\u572820\u4e2a\u4eff\u771f\u4efb\u52a1\u548c5\u4e2a\u7269\u7406\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0cSDP\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SDP\u901a\u8fc7SE(3)\u7b49\u53d8\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u53d8\u63623D\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.01753", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01753", "abs": "https://arxiv.org/abs/2507.01753", "authors": ["Yash Kulkarni", "Susheela Sharma", "Omid Rezayof", "Siddhartha Kapuria", "Jordan P. Amadio", "Mohsen Khadem", "Maryam Tilton", "Farshid Alambeigi"], "title": "Augmented Bridge Spinal Fixation: A New Concept for Addressing Pedicle Screw Pullout via a Steerable Drilling Robot and Flexible Pedicle Screws", "comment": null, "summary": "To address the screw loosening and pullout limitations of rigid pedicle\nscrews in spinal fixation procedures, and to leverage our recently developed\nConcentric Tube Steerable Drilling Robot (CT-SDR) and Flexible Pedicle Screw\n(FPS), in this paper, we introduce the concept of Augmented Bridge Spinal\nFixation (AB-SF). In this concept, two connecting J-shape tunnels are first\ndrilled through pedicles of vertebra using the CT-SDR. Next, two FPSs are\npassed through this tunnel and bone cement is then injected through the\ncannulated region of the FPS to form an augmented bridge between two pedicles\nand reinforce strength of the fixated spine. To experimentally analyze and\nstudy the feasibility of AB-SF technique, we first used our robotic system\n(i.e., a CT-SDR integrated with a robotic arm) to create two different fixation\nscenarios in which two J-shape tunnels, forming a bridge, were drilled at\ndifferent depth of a vertebral phantom. Next, we implanted two FPSs within the\ndrilled tunnels and then successfully simulated the bone cement augmentation\nprocess.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u810a\u67f1\u56fa\u5b9a\u6280\u672fAB-SF\uff0c\u901a\u8fc7\u673a\u5668\u4eba\u94bb\u5b54\u548c\u67d4\u6027\u87ba\u9489\u7ed3\u5408\u9aa8\u6c34\u6ce5\u589e\u5f3a\u56fa\u5b9a\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u521a\u6027\u690e\u5f13\u6839\u87ba\u9489\u5728\u810a\u67f1\u56fa\u5b9a\u4e2d\u7684\u677e\u52a8\u548c\u62d4\u51fa\u95ee\u9898\u3002", "method": "\u4f7f\u7528CT-SDR\u673a\u5668\u4eba\u94bb\u5b54\u5f62\u6210J\u5f62\u96a7\u9053\uff0c\u690d\u5165FPS\u67d4\u6027\u87ba\u9489\u5e76\u6ce8\u5165\u9aa8\u6c34\u6ce5\u52a0\u56fa\u3002", "result": "\u5b9e\u9a8c\u6210\u529f\u6a21\u62df\u4e86\u4e0d\u540c\u6df1\u5ea6\u7684\u96a7\u9053\u94bb\u5b54\u548c\u9aa8\u6c34\u6ce5\u589e\u5f3a\u8fc7\u7a0b\u3002", "conclusion": "AB-SF\u6280\u672f\u5177\u6709\u53ef\u884c\u6027\uff0c\u80fd\u6709\u6548\u589e\u5f3a\u810a\u67f1\u56fa\u5b9a\u5f3a\u5ea6\u3002"}}
{"id": "2507.01779", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01779", "abs": "https://arxiv.org/abs/2507.01779", "authors": ["Daniyal Maroufi", "Xinyuan Huang", "Yash Kulkarni", "Omid Rezayof", "Susheela Sharma", "Vaibhav Goggela", "Jordan P. Amadio", "Mohsen Khadem", "Farshid Alambeigi"], "title": "S3D: A Spatial Steerable Surgical Drilling Framework for Robotic Spinal Fixation Procedures", "comment": null, "summary": "In this paper, we introduce S3D: A Spatial Steerable Surgical Drilling\nFramework for Robotic Spinal Fixation Procedures. S3D is designed to enable\nrealistic steerable drilling while accounting for the anatomical constraints\nassociated with vertebral access in spinal fixation (SF) procedures. To achieve\nthis, we first enhanced our previously designed concentric tube Steerable\nDrilling Robot (CT-SDR) to facilitate steerable drilling across all vertebral\nlevels of the spinal column. Additionally, we propose a four-Phase calibration,\nregistration, and navigation procedure to perform realistic SF procedures on a\nspine holder phantom by integrating the CT-SDR with a seven-degree-of-freedom\nrobotic manipulator. The functionality of this framework is validated through\nplanar and out-of-plane steerable drilling experiments in vertebral phantoms.", "AI": {"tldr": "S3D\u662f\u4e00\u4e2a\u7a7a\u95f4\u53ef\u64cd\u63a7\u7684\u624b\u672f\u94bb\u5b54\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u810a\u67f1\u56fa\u5b9a\u624b\u672f\uff0c\u901a\u8fc7\u6539\u8fdb\u7684CT-SDR\u548c\u56db\u9636\u6bb5\u6821\u51c6\u3001\u6ce8\u518c\u548c\u5bfc\u822a\u7a0b\u5e8f\u5b9e\u73b0\u3002", "motivation": "\u89e3\u51b3\u810a\u67f1\u56fa\u5b9a\u624b\u672f\u4e2d\u89e3\u5256\u5b66\u9650\u5236\u4e0b\u7684\u53ef\u64cd\u63a7\u94bb\u5b54\u95ee\u9898\u3002", "method": "\u6539\u8fdbCT-SDR\uff0c\u63d0\u51fa\u56db\u9636\u6bb5\u6821\u51c6\u3001\u6ce8\u518c\u548c\u5bfc\u822a\u7a0b\u5e8f\uff0c\u7ed3\u5408\u4e03\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u64cd\u4f5c\u5668\u3002", "result": "\u5728\u810a\u67f1\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u5e73\u9762\u548c\u975e\u5e73\u9762\u53ef\u64cd\u63a7\u94bb\u5b54\u529f\u80fd\u3002", "conclusion": "S3D\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u810a\u67f1\u56fa\u5b9a\u624b\u672f\u4e2d\u7684\u53ef\u64cd\u63a7\u94bb\u5b54\u3002"}}
{"id": "2507.01811", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01811", "abs": "https://arxiv.org/abs/2507.01811", "authors": ["Yash Kulkarni", "Susheela Sharma", "Sarah Go", "Jordan P. Amadio", "Mohsen Khadem", "Farshid Alambeigi"], "title": "Towards Design and Development of a Concentric Tube Steerable Drilling Robot for Creating S-shape Tunnels for Pelvic Fixation Procedures", "comment": null, "summary": "Current pelvic fixation techniques rely on rigid drilling tools, which\ninherently constrain the placement of rigid medical screws in the complex\nanatomy of pelvis. These constraints prevent medical screws from following\nanatomically optimal pathways and force clinicians to fixate screws in linear\ntrajectories. This suboptimal approach, combined with the unnatural placement\nof the excessively long screws, lead to complications such as screw\nmisplacement, extended surgery times, and increased radiation exposure due to\nrepeated X-ray images taken ensure to safety of procedure. To address these\nchallenges, in this paper, we present the design and development of a unique 4\ndegree-of-freedom (DoF) pelvic concentric tube steerable drilling robot (pelvic\nCT-SDR). The pelvic CT-SDR is capable of creating long S-shaped drilling\ntrajectories that follow the natural curvatures of the pelvic anatomy. The\nperformance of the pelvic CT-SDR was thoroughly evaluated through several\nS-shape drilling experiments in simulated bone phantoms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b4\u81ea\u7531\u5ea6\u9aa8\u76c6\u540c\u5fc3\u7ba1\u53ef\u8f6c\u5411\u94bb\u5b54\u673a\u5668\u4eba\uff08pelvic CT-SDR\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u4f20\u7edf\u521a\u6027\u94bb\u5b54\u5de5\u5177\u5728\u9aa8\u76c6\u56fa\u5b9a\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u521a\u6027\u94bb\u5b54\u5de5\u5177\u9650\u5236\u4e86\u87ba\u9489\u7684\u653e\u7f6e\u8def\u5f84\uff0c\u5bfc\u81f4\u624b\u672f\u5e76\u53d1\u75c7\u589e\u52a0\uff0c\u5982\u87ba\u9489\u9519\u4f4d\u3001\u624b\u672f\u65f6\u95f4\u5ef6\u957f\u548c\u8f90\u5c04\u66b4\u9732\u589e\u52a0\u3002", "method": "\u8bbe\u8ba1\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd4\u81ea\u7531\u5ea6\u7684\u9aa8\u76c6\u540c\u5fc3\u7ba1\u53ef\u8f6c\u5411\u94bb\u5b54\u673a\u5668\u4eba\uff0c\u80fd\u591f\u6cbf\u9aa8\u76c6\u81ea\u7136\u66f2\u7387\u8fdb\u884cS\u5f62\u94bb\u5b54\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u9aa8\u6a21\u578b\u7684S\u5f62\u94bb\u5b54\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u673a\u5668\u4eba\u7684\u6027\u80fd\u3002", "conclusion": "\u9aa8\u76c6CT-SDR\u80fd\u591f\u4f18\u5316\u87ba\u9489\u653e\u7f6e\u8def\u5f84\uff0c\u51cf\u5c11\u624b\u672f\u5e76\u53d1\u75c7\u3002"}}
{"id": "2507.01843", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01843", "abs": "https://arxiv.org/abs/2507.01843", "authors": ["Dmytro Kuzmenko", "Nadiya Shvai"], "title": "MoIRA: Modular Instruction Routing Architecture for Multi-Task Robotics", "comment": "Preprint of a manuscript submitted for peer review", "summary": "Mixture-of-Experts (MoE) approaches have recently gained traction in robotics\napplications due to their ability to dynamically allocate computational\nresources and specialize sub-networks for distinct tasks or environmental\ncontexts, enabling more efficient decision-making. Such systems often comprise\nsparsely activated experts combined under a single monolithic architecture and\nrequire a well-configured internal routing mechanism, which does not allow for\nselective low-level expert and router customization and requires additional\ntraining. We propose MoIRA, an architecture-agnostic modular MoE framework\ndesigned to coordinate existing experts with an external text-based router.\nMoIRA incorporates two zero-shot routing options: embedding-based similarity\nand prompt-driven language model inference. In our experiments, we choose large\nVision-Language-Action models, gr00t-N1 and $\\pi_0$, as the underlying experts,\nand train low-rank adapters for low-overhead inference. We evaluate MoIRA on\nvarious GR1 Humanoid tasks and LIBERO Spatial and Goal benchmarks, where it\nconsistently outperforms generalist models and competes with other MoE\npipelines. Additionally, we analyse the robustness of the proposed approach to\nthe variations of the instructions. While relying solely on textual\ndescriptions of tasks and experts, MoIRA demonstrates the practical viability\nof modular deployment with precise, low-effort routing and provides an\nalternative, scalable foundation for future multi-expert robotic systems.", "AI": {"tldr": "MoIRA\u662f\u4e00\u79cd\u6a21\u5757\u5316\u7684Mixture-of-Experts\u6846\u67b6\uff0c\u901a\u8fc7\u5916\u90e8\u6587\u672c\u8def\u7531\u5668\u534f\u8c03\u4e13\u5bb6\uff0c\u63d0\u4f9b\u96f6\u6837\u672c\u8def\u7531\u9009\u9879\uff0c\u5e76\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfMoE\u67b6\u6784\u4e2d\u65e0\u6cd5\u9009\u62e9\u6027\u5b9a\u5236\u4e13\u5bb6\u548c\u8def\u7531\u5668\u7684\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u5c11\u989d\u5916\u8bad\u7ec3\u9700\u6c42\u3002", "method": "\u63d0\u51faMoIRA\u6846\u67b6\uff0c\u91c7\u7528\u5d4c\u5165\u76f8\u4f3c\u6027\u548c\u63d0\u793a\u9a71\u52a8\u7684\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4f5c\u4e3a\u96f6\u6837\u672c\u8def\u7531\u9009\u9879\uff0c\u7ed3\u5408\u4f4e\u79e9\u9002\u914d\u5668\u8fdb\u884c\u4f4e\u5f00\u9500\u63a8\u7406\u3002", "result": "\u5728GR1 Humanoid\u548cLIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u5e76\u4e0e\u5176\u4ed6MoE\u7ba1\u9053\u7ade\u4e89\u3002", "conclusion": "MoIRA\u5c55\u793a\u4e86\u6a21\u5757\u5316\u90e8\u7f72\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u591a\u4e13\u5bb6\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2507.01857", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01857", "abs": "https://arxiv.org/abs/2507.01857", "authors": ["Yuhao Lin", "Yi-Lin Wei", "Haoran Liao", "Mu Lin", "Chengyi Xing", "Hao Li", "Dandan Zhang", "Mark Cutkosky", "Wei-Shi Zheng"], "title": "TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types", "comment": "Project Page: https://isee-laboratory.github.io/TypeTele", "summary": "Dexterous teleoperation plays a crucial role in robotic manipulation for\nreal-world data collection and remote robot control. Previous dexterous\nteleoperation mostly relies on hand retargeting to closely mimic human hand\npostures. However, these approaches may fail to fully leverage the inherent\ndexterity of dexterous hands, which can execute unique actions through their\nstructural advantages compared to human hands. To address this limitation, we\npropose TypeTele, a type-guided dexterous teleoperation system, which enables\ndexterous hands to perform actions that are not constrained by human motion\npatterns. This is achieved by introducing dexterous manipulation types into the\nteleoperation system, allowing operators to employ appropriate types to\ncomplete specific tasks. To support this system, we build an extensible\ndexterous manipulation type library to cover comprehensive dexterous postures\nused in manipulation tasks. During teleoperation, we employ a MLLM\n(Multi-modality Large Language Model)-assisted type retrieval module to\nidentify the most suitable manipulation type based on the specific task and\noperator commands. Extensive experiments of real-world teleoperation and\nimitation learning demonstrate that the incorporation of manipulation types\nsignificantly takes full advantage of the dexterous robot's ability to perform\ndiverse and complex tasks with higher success rates.", "AI": {"tldr": "TypeTele\u662f\u4e00\u79cd\u7c7b\u578b\u5f15\u5bfc\u7684\u7075\u5de7\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f15\u5165\u7075\u5de7\u64cd\u4f5c\u7c7b\u578b\u5e93\u548cMLLM\u8f85\u52a9\u68c0\u7d22\u6a21\u5757\uff0c\u4f7f\u7075\u5de7\u624b\u80fd\u591f\u6267\u884c\u4e0d\u53d7\u4eba\u7c7b\u52a8\u4f5c\u6a21\u5f0f\u9650\u5236\u7684\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7075\u5de7\u9065\u64cd\u4f5c\u4e3b\u8981\u4f9d\u8d56\u624b\u90e8\u91cd\u5b9a\u5411\u6a21\u4eff\u4eba\u7c7b\u624b\u90e8\u59ff\u52bf\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u7075\u5de7\u624b\u7684\u7ed3\u6784\u4f18\u52bf\u3002", "method": "\u63d0\u51faTypeTele\u7cfb\u7edf\uff0c\u6784\u5efa\u7075\u5de7\u64cd\u4f5c\u7c7b\u578b\u5e93\uff0c\u5e76\u91c7\u7528MLLM\u8f85\u52a9\u68c0\u7d22\u6a21\u5757\u5339\u914d\u4efb\u52a1\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u7075\u5de7\u624b\u6267\u884c\u591a\u6837\u590d\u6742\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "conclusion": "TypeTele\u901a\u8fc7\u7c7b\u578b\u5f15\u5bfc\u548cMLLM\u8f85\u52a9\uff0c\u5145\u5206\u53d1\u6325\u4e86\u7075\u5de7\u624b\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.01925", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01925", "abs": "https://arxiv.org/abs/2507.01925", "authors": ["Yifan Zhong", "Fengshuo Bai", "Shaofei Cai", "Xuchuan Huang", "Zhang Chen", "Xiaowei Zhang", "Yuanfei Wang", "Shaoyang Guo", "Tianrui Guan", "Ka Nam Lui", "Zhiquan Qi", "Yitao Liang", "Yuanpei Chen", "Yaodong Yang"], "title": "A Survey on Vision-Language-Action Models: An Action Tokenization Perspective", "comment": "70 pages, 5 figures", "summary": "The remarkable advancements of vision and language foundation models in\nmultimodal understanding, reasoning, and generation has sparked growing efforts\nto extend such intelligence to the physical world, fueling the flourishing of\nvision-language-action (VLA) models. Despite seemingly diverse approaches, we\nobserve that current VLA models can be unified under a single framework: vision\nand language inputs are processed by a series of VLA modules, producing a chain\nof \\textit{action tokens} that progressively encode more grounded and\nactionable information, ultimately generating executable actions. We further\ndetermine that the primary design choice distinguishing VLA models lies in how\naction tokens are formulated, which can be categorized into language\ndescription, code, affordance, trajectory, goal state, latent representation,\nraw action, and reasoning. However, there remains a lack of comprehensive\nunderstanding regarding action tokens, significantly impeding effective VLA\ndevelopment and obscuring future directions. Therefore, this survey aims to\ncategorize and interpret existing VLA research through the lens of action\ntokenization, distill the strengths and limitations of each token type, and\nidentify areas for improvement. Through this systematic review and analysis, we\noffer a synthesized outlook on the broader evolution of VLA models, highlight\nunderexplored yet promising directions, and contribute guidance for future\nresearch, hoping to bring the field closer to general-purpose intelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6846\u67b6\u6765\u7406\u89e3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\uff0c\u91cd\u70b9\u5173\u6ce8\u52a8\u4f5c\u4ee4\u724c\u7684\u5206\u7c7b\u548c\u5206\u6790\uff0c\u65e8\u5728\u63a8\u52a8VLA\u6a21\u578b\u7684\u53d1\u5c55\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u7684\u7814\u7a76\u7f3a\u4e4f\u5bf9\u52a8\u4f5c\u4ee4\u724c\u7684\u5168\u9762\u7406\u89e3\uff0c\u963b\u788d\u4e86\u6709\u6548\u5f00\u53d1\u548c\u672a\u6765\u65b9\u5411\u7684\u660e\u786e\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u548c\u89e3\u91ca\u73b0\u6709VLA\u7814\u7a76\u4e2d\u7684\u52a8\u4f5c\u4ee4\u724c\u7c7b\u578b\uff0c\u5206\u6790\u6bcf\u79cd\u7c7b\u578b\u7684\u4f18\u7f3a\u70b9\u3002", "result": "\u603b\u7ed3\u4e86VLA\u6a21\u578b\u7684\u73b0\u72b6\uff0c\u5e76\u6307\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "\u8bba\u6587\u4e3aVLA\u6a21\u578b\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5e76\u5f3a\u8c03\u4e86\u672a\u63a2\u7d22\u4f46\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2507.01930", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01930", "abs": "https://arxiv.org/abs/2507.01930", "authors": ["Wenhao Wang", "Yanyan Li", "Long Jiao", "Jiawei Yuan"], "title": "Large Language Model-Driven Closed-Loop UAV Operation with Semantic Observations", "comment": "10 pages", "summary": "Large Language Models (LLMs) have revolutionized robotic autonomy, including\nUnmanned Aerial Vehicles (UAVs). Recent studies have demonstrated the potential\nof LLMs for translating human instructions into executable control code for UAV\noperations. However, LLMs still face challenges from logical reasoning and\ncomplex decision-making, leading to concerns about the reliability of\nLLM-driven UAV operations. In this paper, we propose a LLM-driven closed-loop\ncontrol framework that enables reliable UAV operations powered by effective\nfeedback and refinement using two LLM modules, i.e., a Code Generator and an\nEvaluator. Our framework transforms numerical state observations from UAV\noperations into natural language trajectory descriptions to enhance the\nevaluator LLM's understanding of UAV dynamics for precise feedback generation.\nOur framework also enables a simulation-based refinement process, and hence\neliminates the risks to physical UAVs caused by incorrect code execution during\nthe refinement. Extensive experiments on UAV control tasks with different\ncomplexities are conducted. The experimental results show that our framework\ncan achieve reliable UAV operations using LLMs, which significantly outperforms\nbaseline approaches in terms of success rate and completeness with the increase\nof task complexity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u95ed\u73af\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u5668\u548c\u8bc4\u4f30\u5668\u6a21\u5757\u5b9e\u73b0\u53ef\u9760\u7684\u65e0\u4eba\u673a\u64cd\u4f5c\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u8f68\u8ff9\u63cf\u8ff0\u548c\u6a21\u62df\u4f18\u5316\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u65e0\u4eba\u673a\u64cd\u4f5c\u4e2d\u903b\u8f91\u63a8\u7406\u548c\u590d\u6742\u51b3\u7b56\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e24\u4e2aLLM\u6a21\u5757\uff08\u4ee3\u7801\u751f\u6210\u5668\u548c\u8bc4\u4f30\u5668\uff09\uff0c\u5c06\u65e0\u4eba\u673a\u72b6\u6001\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u4f18\u5316\u907f\u514d\u7269\u7406\u98ce\u9669\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u548c\u5b8c\u6574\u6027\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u9a71\u52a8\u7684\u65e0\u4eba\u673a\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01961", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01961", "abs": "https://arxiv.org/abs/2507.01961", "authors": ["Sixiang Chen", "Jiaming Liu", "Siyuan Qian", "Han Jiang", "Lily Li", "Renrui Zhang", "Zhuoyang Liu", "Chenyang Gu", "Chengkai Hou", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang"], "title": "AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation", "comment": null, "summary": "Recently, mobile manipulation has attracted increasing attention for enabling\nlanguage-conditioned robotic control in household tasks. However, existing\nmethods still face challenges in coordinating mobile base and manipulator,\nprimarily due to two limitations. On the one hand, they fail to explicitly\nmodel the influence of the mobile base on manipulator control, which easily\nleads to error accumulation under high degrees of freedom. On the other hand,\nthey treat the entire mobile manipulation process with the same visual\nobservation modality (e.g., either all 2D or all 3D), overlooking the distinct\nmultimodal perception requirements at different stages during mobile\nmanipulation. To address this, we propose the Adaptive Coordination Diffusion\nTransformer (AC-DiT), which enhances mobile base and manipulator coordination\nfor end-to-end mobile manipulation. First, since the motion of the mobile base\ndirectly influences the manipulator's actions, we introduce a mobility-to-body\nconditioning mechanism that guides the model to first extract base motion\nrepresentations, which are then used as context prior for predicting whole-body\nactions. This enables whole-body control that accounts for the potential impact\nof the mobile base's motion. Second, to meet the perception requirements at\ndifferent stages of mobile manipulation, we design a perception-aware\nmultimodal conditioning strategy that dynamically adjusts the fusion weights\nbetween various 2D visual images and 3D point clouds, yielding visual features\ntailored to the current perceptual needs. This allows the model to, for\nexample, adaptively rely more on 2D inputs when semantic information is crucial\nfor action prediction, while placing greater emphasis on 3D geometric\ninformation when precise spatial understanding is required. We validate AC-DiT\nthrough extensive experiments on both simulated and real-world mobile\nmanipulation tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAC-DiT\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u534f\u8c03\u79fb\u52a8\u5e95\u5ea7\u548c\u673a\u68b0\u81c2\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u8a00\u6761\u4ef6\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u534f\u8c03\u548c\u611f\u77e5\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u79fb\u52a8\u5e95\u5ea7\u548c\u673a\u68b0\u81c2\u534f\u8c03\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u672a\u80fd\u533a\u5206\u4e0d\u540c\u9636\u6bb5\u7684\u611f\u77e5\u9700\u6c42\uff0c\u5bfc\u81f4\u8bef\u5dee\u7d2f\u79ef\u548c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faAC-DiT\u6a21\u578b\uff0c\u5305\u542b\u79fb\u52a8\u5e95\u5ea7\u5230\u673a\u68b0\u81c2\u7684\u6761\u4ef6\u673a\u5236\u548c\u611f\u77e5\u611f\u77e5\u7684\u591a\u6a21\u6001\u6761\u4ef6\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u65742D\u548c3D\u89c6\u89c9\u8f93\u5165\u7684\u878d\u5408\u6743\u91cd\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u7684\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86AC-DiT\u7684\u6709\u6548\u6027\u3002", "conclusion": "AC-DiT\u901a\u8fc7\u81ea\u9002\u5e94\u534f\u8c03\u548c\u591a\u6a21\u6001\u611f\u77e5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
