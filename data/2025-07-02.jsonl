{"id": "2507.00166", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.00166", "abs": "https://arxiv.org/abs/2507.00166", "authors": ["Aaron C. Davis", "Siting Zhang", "Adalyn Meeks", "Diya Sakhrani", "Luis Carlos Sanjuan Acosta", "D. Ethan Kelley", "Emma Caldwell", "Luis Solorio", "Craig J. Goergen", "David J. Cappelleri"], "title": "Novel Design of 3D Printed Tumbling Microrobots for in vivo Targeted Drug Delivery", "comment": null, "summary": "This paper presents innovative designs for 3D-printed tumbling microrobots,\nspecifically engineered for targeted in vivo drug delivery applications. The\nmicrorobot designs, created using stereolithography 3D printing technologies,\nincorporate permanent micro-magnets to enable actuation via a rotating magnetic\nfield actuator system. The experimental framework encompasses a series of\nlocomotion characterization tests to evaluate microrobot performance under\nvarious conditions. Testing variables include variations in microrobot\ngeometries, actuation frequencies, and environmental conditions, such as dry\nand wet environments, and temperature changes. The paper outlines designs for\nthree drug loading methods, along with comprehensive assessments thermal drug\nrelease using a focused ultrasound system, as well as biocompatibility tests.\nAnimal model testing involves tissue phantoms and in vivo rat models, ensuring\na thorough evaluation of the microrobots' performance and compatibility. The\nresults highlight the robustness and adaptability of the proposed microrobot\ndesigns, showcasing the potential for efficient and targeted in vivo drug\ndelivery. This novel approach addresses current limitations in existing\ntumbling microrobot designs and paves the way for advancements in targeted drug\ndelivery within the large intestine."}
{"id": "2507.00190", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00190", "abs": "https://arxiv.org/abs/2507.00190", "authors": ["Satoshi Tanaka", "Koji Minoda", "Fumiya Watanabe", "Takamasa Horibe"], "title": "Rethink 3D Object Detection from Physical World", "comment": "15 pages, 10 figures", "summary": "High-accuracy and low-latency 3D object detection is essential for autonomous\ndriving systems. While previous studies on 3D object detection often evaluate\nperformance based on mean average precision (mAP) and latency, they typically\nfail to address the trade-off between speed and accuracy, such as 60.0 mAP at\n100 ms vs 61.0 mAP at 500 ms. A quantitative assessment of the trade-offs\nbetween different hardware devices and accelerators remains unexplored, despite\nbeing critical for real-time applications. Furthermore, they overlook the\nimpact on collision avoidance in motion planning, for example, 60.0 mAP leading\nto safer motion planning or 61.0 mAP leading to high-risk motion planning. In\nthis paper, we introduce latency-aware AP (L-AP) and planning-aware AP (P-AP)\nas new metrics, which consider the physical world such as the concept of time\nand physical constraints, offering a more comprehensive evaluation for\nreal-time 3D object detection. We demonstrate the effectiveness of our metrics\nfor the entire autonomous driving system using nuPlan dataset, and evaluate 3D\nobject detection models accounting for hardware differences and accelerators.\nWe also develop a state-of-the-art performance model for real-time 3D object\ndetection through latency-aware hyperparameter optimization (L-HPO) using our\nmetrics. Additionally, we quantitatively demonstrate that the assumption \"the\nmore point clouds, the better the recognition performance\" is incorrect for\nreal-time applications and optimize both hardware and model selection using our\nmetrics."}
{"id": "2507.00236", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.00236", "abs": "https://arxiv.org/abs/2507.00236", "authors": ["Chinmay Vilas Samak", "Tanmay Vilas Samak", "Bing Li", "Venkat Krovi"], "title": "Sim2Real Diffusion: Learning Cross-Domain Adaptive Representations for Transferable Autonomous Driving", "comment": null, "summary": "Simulation-based design, optimization, and validation of autonomous driving\nalgorithms have proven to be crucial for their iterative improvement over the\nyears. Nevertheless, the ultimate measure of effectiveness is their successful\ntransition from simulation to reality (sim2real). However, existing sim2real\ntransfer methods struggle to comprehensively address the autonomy-oriented\nrequirements of balancing: (i) conditioned domain adaptation, (ii) robust\nperformance with limited examples, (iii) modularity in handling multiple domain\nrepresentations, and (iv) real-time performance. To alleviate these pain\npoints, we present a unified framework for learning cross-domain adaptive\nrepresentations for sim2real transferable autonomous driving algorithms using\nconditional latent diffusion models. Our framework offers options to leverage:\n(i) alternate foundation models, (ii) a few-shot fine-tuning pipeline, and\n(iii) textual as well as image prompts for mapping across given source and\ntarget domains. It is also capable of generating diverse high-quality samples\nwhen diffusing across parameter spaces such as times of day, weather\nconditions, seasons, and operational design domains. We systematically analyze\nthe presented framework and report our findings in the form of critical\nquantitative metrics and ablation studies, as well as insightful qualitative\nexamples and remarks. Additionally, we demonstrate the serviceability of the\nproposed approach in bridging the sim2real gap for end-to-end autonomous\ndriving using a behavioral cloning case study. Our experiments indicate that\nthe proposed framework is capable of bridging the perceptual sim2real gap by\nover 40%. We hope that our approach underscores the potential of generative\ndiffusion models in sim2real transfer, offering a pathway toward more robust\nand adaptive autonomous driving."}
{"id": "2507.00268", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.00268", "abs": "https://arxiv.org/abs/2507.00268", "authors": ["Oren Fivel", "Matan Rudman", "Kobi Cohen"], "title": "Control-Optimized Deep Reinforcement Learning for Artificially Intelligent Autonomous Systems", "comment": "27 pages, 10 figures", "summary": "Deep reinforcement learning (DRL) has become a powerful tool for complex\ndecision-making in machine learning and AI. However, traditional methods often\nassume perfect action execution, overlooking the uncertainties and deviations\nbetween an agent's selected actions and the actual system response. In\nreal-world applications, such as robotics, mechatronics, and communication\nnetworks, execution mismatches arising from system dynamics, hardware\nconstraints, and latency can significantly degrade performance. This work\nadvances AI by developing a novel control-optimized DRL framework that\nexplicitly models and compensates for action execution mismatches, a challenge\nlargely overlooked in existing methods. Our approach establishes a structured\ntwo-stage process: determining the desired action and selecting the appropriate\ncontrol signal to ensure proper execution. It trains the agent while accounting\nfor action mismatches and controller corrections. By incorporating these\nfactors into the training process, the AI agent optimizes the desired action\nwith respect to both the actual control signal and the intended outcome,\nexplicitly considering execution errors. This approach enhances robustness,\nensuring that decision-making remains effective under real-world uncertainties.\nOur approach offers a substantial advancement for engineering practice by\nbridging the gap between idealized learning and real-world implementation. It\nequips intelligent agents operating in engineering environments with the\nability to anticipate and adjust for actuation errors and system disturbances\nduring training. We evaluate the framework in five widely used open-source\nmechanical simulation environments we restructured and developed to reflect\nreal-world operating conditions, showcasing its robustness against\nuncertainties and offering a highly practical and efficient solution for\ncontrol-oriented applications."}
{"id": "2507.00273", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.00273", "abs": "https://arxiv.org/abs/2507.00273", "authors": ["Yusuke Tanaka", "Alvin Zhu", "Quanyou Wang", "Dennis Hong"], "title": "Mechanical Intelligence-Aware Curriculum Reinforcement Learning for Humanoids with Parallel Actuation", "comment": null, "summary": "Reinforcement learning (RL) has enabled significant advances in humanoid\nrobot locomotion, yet most learning frameworks do not account for mechanical\nintelligence embedded in parallel actuation mechanisms due to limitations in\nsimulator support for closed kinematic chains. This omission can lead to\ninaccurate motion modeling and suboptimal policies, particularly for robots\nwith high actuation complexity. This paper presents an end-to-end curriculum RL\nframework for BRUCE, a kid-sized humanoid robot featuring three distinct\nparallel mechanisms in its legs: a differential pulley, a 5-bar linkage, and a\n4-bar linkage. Unlike prior approaches that rely on simplified serial\napproximations, we simulate all closed-chain constraints natively using\nGPU-accelerated MJX (MuJoCo), preserving the hardware's physical properties\nduring training. We benchmark our RL approach against a Model Predictive\nController (MPC), demonstrating better surface generalization and performance\nin real-world zero-shot deployment. This work highlights the computational\napproaches and performance benefits of fully simulating parallel mechanisms in\nend-to-end learning pipelines for legged humanoids."}
{"id": "2507.00319", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.00319", "abs": "https://arxiv.org/abs/2507.00319", "authors": ["Tanmay Vilas Samak", "Chinmay Vilas Samak", "Bing Li", "Venkat Krovi"], "title": "When Digital Twins Meet Large Language Models: Realistic, Interactive, and Editable Simulation for Autonomous Driving", "comment": null, "summary": "Simulation frameworks have been key enablers for the development and\nvalidation of autonomous driving systems. However, existing methods struggle to\ncomprehensively address the autonomy-oriented requirements of balancing: (i)\ndynamical fidelity, (ii) photorealistic rendering, (iii) context-relevant\nscenario orchestration, and (iv) real-time performance. To address these\nlimitations, we present a unified framework for creating and curating\nhigh-fidelity digital twins to accelerate advancements in autonomous driving\nresearch. Our framework leverages a mix of physics-based and data-driven\ntechniques for developing and simulating digital twins of autonomous vehicles\nand their operating environments. It is capable of reconstructing real-world\nscenes and assets (real2sim) with geometric and photorealistic accuracy and\ninfusing them with various physical properties to enable real-time dynamical\nsimulation of the ensuing driving scenarios. Additionally, it also incorporates\na large language model (LLM) interface to flexibly edit the driving scenarios\nonline via natural language prompts. We analyze the presented framework in\nterms of its fidelity, performance, and serviceability. Results indicate that\nour framework can reconstruct 3D scenes and assets with up to 97% structural\nsimilarity, while maintaining frame rates above 60 Hz. We also demonstrate that\nit can handle natural language prompts to generate diverse driving scenarios\nwith up to 95% repeatability and 85% generalizability."}
{"id": "2507.00416", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00416", "abs": "https://arxiv.org/abs/2507.00416", "authors": ["Tao Lin", "Gen Li", "Yilei Zhong", "Yanwen Zou", "Bo Zhao"], "title": "Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding", "comment": null, "summary": "Vision-Language-Action (VLA) models have emerged as a promising framework for\nenabling generalist robots capable of perceiving, reasoning, and acting in the\nreal world. These models usually build upon pretrained Vision-Language Models\n(VLMs), which excel at semantic understanding due to large-scale text\npretraining. However, VLMs typically lack precise spatial understanding\ncapabilities, as they are primarily tuned on 2D image-text pairs without 3D\nsupervision. To address this limitation, recent approaches have incorporated\nexplicit 3D inputs such as point clouds or depth maps, but this necessitates\nadditional depth sensors or defective estimation. In contrast, our work\nintroduces a plug-and-play module that implicitly injects 3D geometry features\ninto VLA models by leveraging an off-the-shelf visual geometry foundation\nmodels. We design five spatially challenging tasks that require precise spatial\nunderstanding ability to validate effectiveness of our method. Extensive\nevaluations show that our method significantly improves the performance of\nstate-of-the-art VLA models across diverse scenarios."}
{"id": "2507.00435", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00435", "abs": "https://arxiv.org/abs/2507.00435", "authors": ["Yi Ru Wang", "Carter Ung", "Grant Tannert", "Jiafei Duan", "Josephine Li", "Amy Le", "Rishabh Oswal", "Markus Grotz", "Wilbert Pumacay", "Yuquan Deng", "Ranjay Krishna", "Dieter Fox", "Siddhartha Srinivasa"], "title": "RoboEval: Where Robotic Manipulation Meets Structured and Scalable Evaluation", "comment": "Project page: https://robo-eval.github.io", "summary": "We present RoboEval, a simulation benchmark and structured evaluation\nframework designed to reveal the limitations of current bimanual manipulation\npolicies. While prior benchmarks report only binary task success, we show that\nsuch metrics often conceal critical weaknesses in policy behavior -- such as\npoor coordination, slipping during grasping, or asymmetric arm usage. RoboEval\nintroduces a suite of tiered, semantically grounded tasks decomposed into\nskill-specific stages, with variations that systematically challenge spatial,\nphysical, and coordination capabilities. Tasks are paired with fine-grained\ndiagnostic metrics and 3000+ human demonstrations to support imitation\nlearning. Our experiments reveal that policies with similar success rates\ndiverge in how tasks are executed -- some struggle with alignment, others with\ntemporally consistent bimanual control. We find that behavioral metrics\ncorrelate with success in over half of task-metric pairs, and remain\ninformative even when binary success saturates. By pinpointing when and how\npolicies fail, RoboEval enables a deeper, more actionable understanding of\nrobotic manipulation -- and highlights the need for evaluation tools that go\nbeyond success alone."}
{"id": "2507.00443", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.00443", "abs": "https://arxiv.org/abs/2507.00443", "authors": ["Reza Ahmadvand", "Sarah Safura Sharif", "Yaser Mike Banad"], "title": "Novel Pigeon-inspired 3D Obstacle Detection and Avoidance Maneuver for Multi-UAV Systems", "comment": "11 Pages, 11 Pictures, 1 Table, 3 Algorithms", "summary": "Recent advances in multi-agent systems manipulation have demonstrated a\nrising demand for the implementation of multi-UAV systems in urban areas, which\nare always subjected to the presence of static and dynamic obstacles. Inspired\nby the collective behavior of tilapia fish and pigeons, the focus of the\npresented research is on the introduction of a nature-inspired collision-free\nformation control for a multi-UAV system, considering the obstacle avoidance\nmaneuvers. The developed framework in this study utilizes a semi-distributed\ncontrol approach, in which, based on a probabilistic Lloyd's algorithm, a\ncentralized guidance algorithm works for optimal positioning of the UAVs, while\na distributed control approach has been used for the intervehicle collision and\nobstacle avoidance. Further, the presented framework has been extended to the\n3D space with a novel definition of 3D maneuvers. Finally, the presented\nframework has been applied to multi-UAV systems in 2D and 3D scenarios, and the\nobtained results demonstrated the validity of the presented method in dynamic\nenvironments with stationary and moving obstacles."}
{"id": "2507.00446", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.00446", "abs": "https://arxiv.org/abs/2507.00446", "authors": ["Yasunori Toshimitsu", "Kento Kawaharazuka", "Akihiro Miki", "Kei Okada", "Masayuki Inaba"], "title": "DIJE: Dense Image Jacobian Estimation for Robust Robotic Self-Recognition and Visual Servoing", "comment": "2022 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)", "summary": "For robots to move in the real world, they must first correctly understand\nthe state of its own body and the tools that it holds. In this research, we\npropose DIJE, an algorithm to estimate the image Jacobian for every pixel. It\nis based on an optical flow calculation and a simplified Kalman Filter that can\nbe efficiently run on the whole image in real time. It does not rely on markers\nnor knowledge of the robotic structure. We use the DIJE in a self-recognition\nprocess which can robustly distinguish between movement by the robot and by\nexternal entities, even when the motion overlaps. We also propose a visual\nservoing controller based on DIJE, which can learn to control the robot's body\nto conduct reaching movements or bimanual tool-tip control. The proposed\nalgorithms were implemented on a physical musculoskeletal robot and its\nperformance was verified. We believe that such global estimation of the\nvisuomotor policy has the potential to be extended into a more general\nframework for manipulation."}
{"id": "2507.00464", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.00464", "abs": "https://arxiv.org/abs/2507.00464", "authors": ["Hyun-Bin Kim", "Kyung-Soo Kim"], "title": "A Miniature High-Resolution Tension Sensor Based on a Photo-Reflector for Robotic Hands and Grippers", "comment": null, "summary": "This paper presents a miniature tension sensor using a photo-reflector,\ndesigned for compact tendon-driven grippers and robotic hands. The proposed\nsensor has a small form factor of 13~mm x 7~mm x 6.5~mm and is capable of\nmeasuring tensile forces up to 200~N. A symmetric elastomer structure\nincorporating fillets and flexure hinges is designed based on Timoshenko beam\ntheory and verified via FEM analysis, enabling improved sensitivity and\nmechanical durability while minimizing torsional deformation. The sensor\nutilizes a compact photo-reflector (VCNT2020) to measure displacement in the\nnear-field region, eliminating the need for light-absorbing materials or\ngeometric modifications required in photo-interrupter-based designs. A 16-bit\nanalog-to-digital converter (ADC) and CAN-FD (Flexible Data-rate) communication\nenable efficient signal acquisition with up to 5~kHz sampling rate. Calibration\nexperiments demonstrate a resolution of 9.9~mN (corresponding to over 14-bit\naccuracy) and a root mean square error (RMSE) of 0.455~N. Force control\nexperiments using a twisted string actuator and PI control yield RMSEs as low\nas 0.073~N. Compared to previous research using photo-interrupter, the proposed\nmethod achieves more than tenfold improvement in resolution while also reducing\nnonlinearity and hysteresis. The design is mechanically simple, lightweight,\neasy to assemble, and suitable for integration into robotic and prosthetic\nsystems requiring high-resolution force feedback."}
{"id": "2507.00523", "categories": ["cs.RO", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.00523", "abs": "https://arxiv.org/abs/2507.00523", "authors": ["Nazish Tahir", "Ramviyas Parasuraman"], "title": "Edge Computing and its Application in Robotics: A Survey", "comment": null, "summary": "The Edge computing paradigm has gained prominence in both academic and\nindustry circles in recent years. By implementing edge computing facilities and\nservices in robotics, it becomes a key enabler in the deployment of artificial\nintelligence applications to robots. Time-sensitive robotics applications\nbenefit from the reduced latency, mobility, and location awareness provided by\nthe edge computing paradigm, which enables real-time data processing and\nintelligence at the network's edge. While the advantages of integrating edge\ncomputing into robotics are numerous, there has been no recent survey that\ncomprehensively examines these benefits. This paper aims to bridge that gap by\nhighlighting important work in the domain of edge robotics, examining recent\nadvancements, and offering deeper insight into the challenges and motivations\nbehind both current and emerging solutions. In particular, this article\nprovides a comprehensive evaluation of recent developments in edge robotics,\nwith an emphasis on fundamental applications, providing in-depth analysis of\nthe key motivations, challenges, and future directions in this rapidly evolving\ndomain. It also explores the importance of edge computing in real-world\nrobotics scenarios where rapid response times are critical. Finally, the paper\noutlines various open research challenges in the field of edge robotics."}
{"id": "2507.00552", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.00552", "abs": "https://arxiv.org/abs/2507.00552", "authors": ["Jiajie Zhang", "Shenrui Wu", "Xu Ma", "Sören Schwertfeger"], "title": "Generation of Indoor Open Street Maps for Robot Navigation from CAD Files", "comment": "8 pages, 8 figures", "summary": "The deployment of autonomous mobile robots is predicated on the availability\nof environmental maps, yet conventional generation via SLAM (Simultaneous\nLocalization and Mapping) suffers from significant limitations in time, labor,\nand robustness, particularly in dynamic, large-scale indoor environments where\nmap obsolescence can lead to critical localization failures. To address these\nchallenges, this paper presents a complete and automated system for converting\narchitectural Computer-Aided Design (CAD) files into a hierarchical topometric\nOpenStreetMap (OSM) representation, tailored for robust life-long robot\nnavigation. Our core methodology involves a multi-stage pipeline that first\nisolates key structural layers from the raw CAD data and then employs an\nAreaGraph-based topological segmentation to partition the building layout into\na hierarchical graph of navigable spaces. This process yields a comprehensive\nand semantically rich map, further enhanced by automatically associating\ntextual labels from the CAD source and cohesively merging multiple building\nfloors into a unified, topologically-correct model. By leveraging the permanent\nstructural information inherent in CAD files, our system circumvents the\ninefficiencies and fragility of SLAM, offering a practical and scalable\nsolution for deploying robots in complex indoor spaces. The software is\nencapsulated within an intuitive Graphical User Interface (GUI) to facilitate\npractical use. The code and dataset are available at\nhttps://github.com/jiajiezhang7/osmAG-from-cad."}
{"id": "2507.00635", "categories": ["cs.RO", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.00635", "abs": "https://arxiv.org/abs/2507.00635", "authors": ["Tinghe Hong", "Shenlin Cai", "Boyang Li", "Kai Huang"], "title": "Stable Tracking of Eye Gaze Direction During Ophthalmic Surgery", "comment": "Accepted by ICRA 2025", "summary": "Ophthalmic surgical robots offer superior stability and precision by reducing\nthe natural hand tremors of human surgeons, enabling delicate operations in\nconfined surgical spaces. Despite the advancements in developing vision- and\nforce-based control methods for surgical robots, preoperative navigation\nremains heavily reliant on manual operation, limiting the consistency and\nincreasing the uncertainty. Existing eye gaze estimation techniques in the\nsurgery, whether traditional or deep learning-based, face challenges including\ndependence on additional sensors, occlusion issues in surgical environments,\nand the requirement for facial detection. To address these limitations, this\nstudy proposes an innovative eye localization and tracking method that combines\nmachine learning with traditional algorithms, eliminating the requirements of\nlandmarks and maintaining stable iris detection and gaze estimation under\nvarying lighting and shadow conditions. Extensive real-world experiment results\nshow that our proposed method has an average estimation error of 0.58 degrees\nfor eye orientation estimation and 2.08-degree average control error for the\nrobotic arm's movement based on the calculated orientation."}
{"id": "2507.00644", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.00644", "abs": "https://arxiv.org/abs/2507.00644", "authors": ["Rohit Kumar", "Melya Boukheddimi", "Dennis Mronga", "Shivesh Kumar", "Frank Kirchner"], "title": "Parallel Transmission Aware Co-Design: Enhancing Manipulator Performance Through Actuation-Space Optimization", "comment": null, "summary": "In robotics, structural design and behavior optimization have long been\nconsidered separate processes, resulting in the development of systems with\nlimited capabilities. Recently, co-design methods have gained popularity, where\nbi-level formulations are used to simultaneously optimize the robot design and\nbehavior for specific tasks. However, most implementations assume a serial or\ntree-type model of the robot, overlooking the fact that many robot platforms\nincorporate parallel mechanisms. In this paper, we present a novel co-design\napproach that explicitly incorporates parallel coupling constraints into the\ndynamic model of the robot. In this framework, an outer optimization loop\nfocuses on the design parameters, in our case the transmission ratios of a\nparallel belt-driven manipulator, which map the desired torques from the joint\nspace to the actuation space. An inner loop performs trajectory optimization in\nthe actuation space, thus exploiting the entire dynamic range of the\nmanipulator. We compare the proposed method with a conventional co-design\napproach based on a simplified tree-type model. By taking advantage of the\nactuation space representation, our approach leads to a significant increase in\ndynamic payload capacity compared to the conventional co-design implementation."}
{"id": "2507.00677", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.00677", "abs": "https://arxiv.org/abs/2507.00677", "authors": ["Dongho Kang", "Jin Cheng", "Fatemeh Zargarbashi", "Taerim Yoon", "Sungjoon Choi", "Stelian Coros"], "title": "Learning Steerable Imitation Controllers from Unstructured Animal Motions", "comment": "The supplementary video is available at https://youtu.be/DukyUGNYf5A", "summary": "This paper presents a control framework for legged robots that leverages\nunstructured real-world animal motion data to generate animal-like and\nuser-steerable behaviors. Our framework learns to follow velocity commands\nwhile reproducing the diverse gait patterns in the original dataset. To begin\nwith, animal motion data is transformed into a robot-compatible database using\nconstrained inverse kinematics and model predictive control, bridging the\nmorphological and physical gap between the animal and the robot. Subsequently,\na variational autoencoder-based motion synthesis module captures the diverse\nlocomotion patterns in the motion database and generates smooth transitions\nbetween them in response to velocity commands. The resulting kinematic motions\nserve as references for a reinforcement learning-based feedback controller\ndeployed on physical robots. We show that this approach enables a quadruped\nrobot to adaptively switch gaits and accurately track user velocity commands\nwhile maintaining the stylistic coherence of the motion data. Additionally, we\nprovide component-wise evaluations to analyze the system's behavior in depth\nand demonstrate the efficacy of our method for more accurate and reliable\nmotion imitation."}
{"id": "2507.00816", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00816", "abs": "https://arxiv.org/abs/2507.00816", "authors": ["Mengyun Wang", "Bo Wang", "Yifeng Niu", "Chang Wang"], "title": "PI-WAN: A Physics-Informed Wind-Adaptive Network for Quadrotor Dynamics Prediction in Unknown Environments", "comment": null, "summary": "Accurate dynamics modeling is essential for quadrotors to achieve precise\ntrajectory tracking in various applications. Traditional physical\nknowledge-driven modeling methods face substantial limitations in unknown\nenvironments characterized by variable payloads, wind disturbances, and\nexternal perturbations. On the other hand, data-driven modeling methods suffer\nfrom poor generalization when handling out-of-distribution (OoD) data,\nrestricting their effectiveness in unknown scenarios. To address these\nchallenges, we introduce the Physics-Informed Wind-Adaptive Network (PI-WAN),\nwhich combines knowledge-driven and data-driven modeling methods by embedding\nphysical constraints directly into the training process for robust quadrotor\ndynamics learning. Specifically, PI-WAN employs a Temporal Convolutional\nNetwork (TCN) architecture that efficiently captures temporal dependencies from\nhistorical flight data, while a physics-informed loss function applies physical\nprinciples to improve model generalization and robustness across previously\nunseen conditions. By incorporating real-time prediction results into a model\npredictive control (MPC) framework, we achieve improvements in closed-loop\ntracking performance. Comprehensive simulations and real-world flight\nexperiments demonstrate that our approach outperforms baseline methods in terms\nof prediction accuracy, tracking precision, and robustness to unknown\nenvironments."}
{"id": "2507.00833", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00833", "abs": "https://arxiv.org/abs/2507.00833", "authors": ["Zhi Jing", "Siyuan Yang", "Jicong Ao", "Ting Xiao", "Yugang Jiang", "Chenjia Bai"], "title": "HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning", "comment": "Project Page: https://openhumanoidgen.github.io", "summary": "For robotic manipulation, existing robotics datasets and simulation\nbenchmarks predominantly cater to robot-arm platforms. However, for humanoid\nrobots equipped with dual arms and dexterous hands, simulation tasks and\nhigh-quality demonstrations are notably lacking. Bimanual dexterous\nmanipulation is inherently more complex, as it requires coordinated arm\nmovements and hand operations, making autonomous data collection challenging.\nThis paper presents HumanoidGen, an automated task creation and demonstration\ncollection framework that leverages atomic dexterous operations and LLM\nreasoning to generate relational constraints. Specifically, we provide spatial\nannotations for both assets and dexterous hands based on the atomic operations,\nand perform an LLM planner to generate a chain of actionable spatial\nconstraints for arm movements based on object affordances and scenes. To\nfurther improve planning ability, we employ a variant of Monte Carlo tree\nsearch to enhance LLM reasoning for long-horizon tasks and insufficient\nannotation. In experiments, we create a novel benchmark with augmented\nscenarios to evaluate the quality of the collected data. The results show that\nthe performance of the 2D and 3D diffusion policies can scale with the\ngenerated dataset. Project page is https://openhumanoidgen.github.io."}
{"id": "2507.00882", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.00882", "abs": "https://arxiv.org/abs/2507.00882", "authors": ["Miguel Ángel de Miguel", "Jorge Beltrán", "Juan S. Cely", "Francisco Martín", "Juan Carlos Manzanares", "Alberto García"], "title": "I Move Therefore I Learn: Experience-Based Traversability in Outdoor Robotics", "comment": null, "summary": "Accurate traversability estimation is essential for safe and effective\nnavigation of outdoor robots operating in complex environments. This paper\nintroduces a novel experience-based method that allows robots to autonomously\nlearn which terrains are traversable based on prior navigation experience,\nwithout relying on extensive pre-labeled datasets. The approach integrates\nelevation and texture data into multi-layered grid maps, which are processed\nusing a variational autoencoder (VAE) trained on a generic texture dataset.\nDuring an initial teleoperated phase, the robot collects sensory data while\nmoving around the environment. These experiences are encoded into compact\nfeature vectors and clustered using the BIRCH algorithm to represent\ntraversable terrain areas efficiently. In deployment, the robot compares new\nterrain patches to its learned feature clusters to assess traversability in\nreal time. The proposed method does not require training with data from the\ntargeted scenarios, generalizes across diverse surfaces and platforms, and\ndynamically adapts as new terrains are encountered. Extensive evaluations on\nboth synthetic benchmarks and real-world scenarios with wheeled and legged\nrobots demonstrate its effectiveness, robustness, and superior adaptability\ncompared to state-of-the-art approaches."}
{"id": "2507.00917", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.00917", "abs": "https://arxiv.org/abs/2507.00917", "authors": ["Xiaoxiao Long", "Qingrui Zhao", "Kaiwen Zhang", "Zihao Zhang", "Dingrui Wang", "Yumeng Liu", "Zhengjie Shu", "Yi Lu", "Shouzheng Wang", "Xinzhe Wei", "Wei Li", "Wei Yin", "Yao Yao", "Jia Pan", "Qiu Shen", "Ruigang Yang", "Xun Cao", "Qionghai Dai"], "title": "A Survey: Learning Embodied Intelligence from Physical Simulators and World Models", "comment": "https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey", "summary": "The pursuit of artificial general intelligence (AGI) has placed embodied\nintelligence at the forefront of robotics research. Embodied intelligence\nfocuses on agents capable of perceiving, reasoning, and acting within the\nphysical world. Achieving robust embodied intelligence requires not only\nadvanced perception and control, but also the ability to ground abstract\ncognition in real-world interactions. Two foundational technologies, physical\nsimulators and world models, have emerged as critical enablers in this quest.\nPhysical simulators provide controlled, high-fidelity environments for training\nand evaluating robotic agents, allowing safe and efficient development of\ncomplex behaviors. In contrast, world models empower robots with internal\nrepresentations of their surroundings, enabling predictive planning and\nadaptive decision-making beyond direct sensory input. This survey\nsystematically reviews recent advances in learning embodied AI through the\nintegration of physical simulators and world models. We analyze their\ncomplementary roles in enhancing autonomy, adaptability, and generalization in\nintelligent robots, and discuss the interplay between external simulation and\ninternal modeling in bridging the gap between simulated training and real-world\ndeployment. By synthesizing current progress and identifying open challenges,\nthis survey aims to provide a comprehensive perspective on the path toward more\ncapable and generalizable embodied AI systems. We also maintain an active\nrepository that contains up-to-date literature and open-source projects at\nhttps://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey."}
{"id": "2507.00937", "categories": ["cs.RO", "cs.AR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00937", "abs": "https://arxiv.org/abs/2507.00937", "authors": ["David Hunt", "Shaocheng Luo", "Spencer Hallyburton", "Shafii Nillongo", "Yi Li", "Tingjun Chen", "Miroslav Pajic"], "title": "RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles", "comment": "8 pages, accepted by IROS 2025", "summary": "Low-cost indoor mobile robots have gained popularity with the increasing\nadoption of automation in homes and commercial spaces. However, existing lidar\nand camera-based solutions have limitations such as poor performance in\nvisually obscured environments, high computational overhead for data\nprocessing, and high costs for lidars. In contrast, mmWave radar sensors offer\na cost-effective and lightweight alternative, providing accurate ranging\nregardless of visibility. However, existing radar-based localization suffers\nfrom sparse point cloud generation, noise, and false detections. Thus, in this\nwork, we introduce RaGNNarok, a real-time, lightweight, and generalizable graph\nneural network (GNN)-based framework to enhance radar point clouds, even in\ncomplex and dynamic environments. With an inference time of just 7.3 ms on the\nlow-cost Raspberry Pi 5, RaGNNarok runs efficiently even on such\nresource-constrained devices, requiring no additional computational resources.\nWe evaluate its performance across key tasks, including localization, SLAM, and\nautonomous navigation, in three different environments. Our results demonstrate\nstrong reliability and generalizability, making RaGNNarok a robust solution for\nlow-cost indoor mobile robots."}
{"id": "2507.00984", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00984", "abs": "https://arxiv.org/abs/2507.00984", "authors": ["Xihang Yu", "Rajat Talak", "Jingnan Shi", "Ulrich Viereck", "Igor Gilitschenski", "Luca Carlone"], "title": "Box Pose and Shape Estimation and Domain Adaptation for Large-Scale Warehouse Automation", "comment": "12 pages, 6 figures. This work will be presented at the 19th\n  International Symposium on Experimental Robotics (ISER2025)", "summary": "Modern warehouse automation systems rely on fleets of intelligent robots that\ngenerate vast amounts of data -- most of which remains unannotated. This paper\ndevelops a self-supervised domain adaptation pipeline that leverages\nreal-world, unlabeled data to improve perception models without requiring\nmanual annotations. Our work focuses specifically on estimating the pose and\nshape of boxes and presents a correct-and-certify pipeline for self-supervised\nbox pose and shape estimation. We extensively evaluate our approach across a\nrange of simulated and real industrial settings, including adaptation to a\nlarge-scale real-world dataset of 50,000 images. The self-supervised model\nsignificantly outperforms models trained solely in simulation and shows\nsubstantial improvements over a zero-shot 3D bounding box estimation baseline."}
{"id": "2507.00990", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00990", "abs": "https://arxiv.org/abs/2507.00990", "authors": ["Shivansh Patel", "Shraddhaa Mohan", "Hanlin Mai", "Unnat Jain", "Svetlana Lazebnik", "Yunzhu Li"], "title": "Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations", "comment": "Project Page: https://rigvid-robot.github.io/", "summary": "This work introduces Robots Imitating Generated Videos (RIGVid), a system\nthat enables robots to perform complex manipulation tasks--such as pouring,\nwiping, and mixing--purely by imitating AI-generated videos, without requiring\nany physical demonstrations or robot-specific training. Given a language\ncommand and an initial scene image, a video diffusion model generates potential\ndemonstration videos, and a vision-language model (VLM) automatically filters\nout results that do not follow the command. A 6D pose tracker then extracts\nobject trajectories from the video, and the trajectories are retargeted to the\nrobot in an embodiment-agnostic fashion. Through extensive real-world\nevaluations, we show that filtered generated videos are as effective as real\ndemonstrations, and that performance improves with generation quality. We also\nshow that relying on generated videos outperforms more compact alternatives\nsuch as keypoint prediction using VLMs, and that strong 6D pose tracking\noutperforms other ways to extract trajectories, such as dense feature point\ntracking. These findings suggest that videos produced by a state-of-the-art\noff-the-shelf model can offer an effective source of supervision for robotic\nmanipulation."}
{"id": "2507.01008", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01008", "abs": "https://arxiv.org/abs/2507.01008", "authors": ["Martin Peticco", "Gabriella Ulloa", "John Marangola", "Pulkit Agrawal"], "title": "DexWrist: A Robotic Wrist for Constrained and Dynamic Manipulation", "comment": "More details about the wrist can be found at: dexwrist.csail.mit.edu", "summary": "We present the DexWrist, a compliant robotic wrist designed to advance\nrobotic manipulation in highly-constrained environments, enable dynamic tasks,\nand speed up data collection. DexWrist is designed to be close to the\nfunctional capabilities of the human wrist and achieves mechanical compliance\nand a greater workspace as compared to existing robotic wrist designs. The\nDexWrist can supercharge policy learning by (i) enabling faster teleoperation\nand therefore making data collection more scalable; (ii) completing tasks in\nfewer steps which reduces trajectory lengths and therefore can ease policy\nlearning; (iii) DexWrist is designed to be torque transparent with easily\nsimulatable kinematics for simulated data collection; and (iv) most importantly\nexpands the workspace of manipulation for approaching highly cluttered scenes\nand tasks. More details about the wrist can be found at:\ndexwrist.csail.mit.edu."}
{"id": "2507.01016", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01016", "abs": "https://arxiv.org/abs/2507.01016", "authors": ["Yating Wang", "Haoyi Zhu", "Mingyu Liu", "Jiange Yang", "Hao-Shu Fang", "Tong He"], "title": "VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers", "comment": "Accepted by ICCV 2025", "summary": "In this paper, we introduce an innovative vector quantization based action\ntokenizer built upon the largest-scale action trajectory dataset to date,\nleveraging over 100 times more data than previous approaches. This extensive\ndataset enables our tokenizer to capture rich spatiotemporal dynamics,\nresulting in a model that not only accelerates inference but also generates\nsmoother and more coherent action outputs. Once trained, the tokenizer can be\nseamlessly adapted to a wide range of downstream tasks in a zero-shot manner,\nfrom short-horizon reactive behaviors to long-horizon planning. A key finding\nof our work is that the domain gap between synthetic and real action\ntrajectories is marginal, allowing us to effectively utilize a vast amount of\nsynthetic data during training without compromising real-world performance. To\nvalidate our approach, we conducted extensive experiments in both simulated\nenvironments and on real robotic platforms. The results demonstrate that as the\nvolume of synthetic trajectory data increases, the performance of our tokenizer\non downstream tasks improves significantly-most notably, achieving up to a 30%\nhigher success rate on two real-world tasks in long-horizon scenarios. These\nfindings highlight the potential of our action tokenizer as a robust and\nscalable solution for real-time embodied intelligence systems, paving the way\nfor more efficient and reliable robotic control in diverse application\ndomains.Project website: https://xiaoxiao0406.github.io/vqvla.github.io"}
