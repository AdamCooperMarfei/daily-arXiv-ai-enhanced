<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 30]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Learning to Grasp Anything by Playing with Random Toys](https://arxiv.org/abs/2510.12866)
*Dantong Niu,Yuvan Sharma,Baifeng Shi,Rachel Ding,Matteo Gioia,Haoru Xue,Henry Tsai,Konstantinos Kallidromitis,Anirudh Pai,Shankar Shastry,Trevor Darrell,Jitendra Malik,Roei Herzig*

Main category: cs.RO

TL;DR: 机器人可以通过训练由四种基本形状（球体、长方体、圆柱体、环体）随机组合的"玩具"来学习可泛化的抓取技能，并在真实世界的YCB数据集上实现67%的零样本抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 受儿童通过掌握少量简单玩具就能发展出可泛化的灵巧操作技能的认知科学启发，研究机器人是否能实现类似的泛化能力。

Method: 使用四种基本形状随机组合的"玩具"进行训练，并提出物体中心视觉表示和检测池化机制。

Result: 在仿真和物理机器人上评估，模型在YCB数据集上达到67%的真实世界抓取成功率，优于依赖更多领域内数据的最先进方法。

Conclusion: 这项工作为机器人操作的可扩展和可泛化学习提供了一条有前景的路径。

Abstract: Robotic manipulation policies often struggle to generalize to novel objects,
limiting their real-world utility. In contrast, cognitive science suggests that
children develop generalizable dexterous manipulation skills by mastering a
small set of simple toys and then applying that knowledge to more complex
items. Inspired by this, we study if similar generalization capabilities can
also be achieved by robots. Our results indicate robots can learn generalizable
grasping using randomly assembled objects that are composed from just four
shape primitives: spheres, cuboids, cylinders, and rings. We show that training
on these "toys" enables robust generalization to real-world objects, yielding
strong zero-shot performance. Crucially, we find the key to this generalization
is an object-centric visual representation induced by our proposed detection
pooling mechanism. Evaluated in both simulation and on physical robots, our
model achieves a 67% real-world grasping success rate on the YCB dataset,
outperforming state-of-the-art approaches that rely on substantially more
in-domain data. We further study how zero-shot generalization performance
scales by varying the number and diversity of training toys and the
demonstrations per toy. We believe this work offers a promising path to
scalable and generalizable learning in robotic manipulation. Demonstration
videos, code, checkpoints and our dataset are available on our project page:
https://lego-grasp.github.io/ .

</details>


### [2] [Gaussian Process Implicit Surfaces as Control Barrier Functions for Safe Robot Navigation](https://arxiv.org/abs/2510.12919)
*Mouhyemen Khan,Tatsuya Ibuki,Abhijit Chatterjee*

Main category: cs.RO

TL;DR: 提出了一种将高斯过程隐式曲面（GPIS）与控制屏障函数（CBF）统一的新框架，使用GPIS表示安全边界，并通过稀疏化解决计算复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 结合水平集方法和控制屏障函数的安全技术，以及通过距离场表示几何形状的隐式曲面方法，创建一个统一的框架，其中隐式曲面本身作为CBF。

Method: 使用高斯过程隐式曲面（GPIS）表示安全边界，利用来自传感器测量的安全样本条件化GP。GP后验均值定义隐式安全曲面（安全信念），后验方差提供鲁棒安全边界。为解决GP计算复杂度问题，开发了稀疏高斯CBF。

Result: 在两种场景中验证方法：模拟7自由度机械臂在斯坦福兔子周围操作，以及四旋翼在3D空间中围绕实体椅子导航。高斯CBF（带和不带稀疏性）都能实现安全交互和无碰撞轨迹执行。

Conclusion: 高斯过程隐式曲面可以成功用于合成控制屏障函数，提供安全边界表示和不确定性估计，稀疏化方法有效解决了计算复杂度问题。

Abstract: Level set methods underpin modern safety techniques such as control barrier
functions (CBFs), while also serving as implicit surface representations for
geometric shapes via distance fields. Inspired by these two paradigms, we
propose a unified framework where the implicit surface itself acts as a CBF. We
leverage Gaussian process (GP) implicit surface (GPIS) to represent the safety
boundaries, using safety samples which are derived from sensor measurements to
condition the GP. The GP posterior mean defines the implicit safety surface
(safety belief), while the posterior variance provides a robust safety margin.
Although GPs have favorable properties such as uncertainty estimation and
analytical tractability, they scale cubically with data. To alleviate this
issue, we develop a sparse solution called sparse Gaussian CBFs. To the best of
our knowledge, GPIS have not been explicitly used to synthesize CBFs. We
validate the approach on collision avoidance tasks in two settings: a simulated
7-DOF manipulator operating around the Stanford bunny, and a quadrotor
navigating in 3D around a physical chair. In both cases, Gaussian CBFs (with
and without sparsity) enable safe interaction and collision-free execution of
trajectories that would otherwise intersect the objects.

</details>


### [3] [Geometric Model Predictive Path Integral for Agile UAV Control with Online Collision Avoidance](https://arxiv.org/abs/2510.12924)
*Pavel Pochobradský,Ondřej Procházka,Robert Pěnička,Vojtěch Vonásek,Martin Saska*

Main category: cs.RO

TL;DR: GMPPI是一种基于采样的控制器，结合几何SE(3)控制和可变参数，能够在敏捷飞行中精确跟踪轨迹并在线避障


<details>
  <summary>Details</summary>
Motivation: 现有MPPI控制器在跟踪平滑低速轨迹时性能不足，且缺乏有效的在线避障能力，限制了无人机在复杂环境中的自主飞行

Method: 提出几何模型预测路径积分控制，使用SE(3)控制生成部分轨迹，引入可变仿真时间步长和动态成本噪声参数，并与立体深度相机集成

Result: 在模拟环境中能以13m/s速度避障，位置误差与几何SE(3)控制器相当；真实实验中能以10m/s速度跟踪敏捷轨迹并避障

Conclusion: GMPPI在保持敏捷轨迹跟踪能力的同时，显著提升了避障性能，是实现复杂环境中自主无人机飞行的关键进展

Abstract: In this letter, we introduce Geometric Model Predictive Path Integral
(GMPPI), a sampling-based controller capable of tracking agile trajectories
while avoiding obstacles. In each iteration, GMPPI generates a large number of
candidate rollout trajectories and then averages them to create a nominal
control to be followed by the Unmanned Aerial Vehicle (UAV). We propose using
geometric SE(3) control to generate part of the rollout trajectories,
significantly increasing precision in agile flight. Furthermore, we introduce
varying rollout simulation time step length and dynamic cost and noise
parameters, vastly improving tracking performance of smooth and low-speed
trajectories over an existing Model Predictive Path Integral (MPPI)
implementation. Finally, we propose an integration of GMPPI with a stereo depth
camera, enabling online obstacle avoidance at high speeds, a crucial step
towards autonomous UAV flights in complex environments. The proposed controller
can track simulated agile reference trajectories with position error similar to
the geometric SE(3) controller. However, the same configuration of the proposed
controller can avoid obstacles in a simulated forest environment at speeds of
up to 13m/s, surpassing the performance of a state-of-the-art obstacle-aware
planner. In real-world experiments, GMPPI retains the capability to track agile
trajectories and avoids obstacles at speeds of up to 10m/s.

</details>


### [4] [Enhancing Sampling-based Planning with a Library of Paths](https://arxiv.org/abs/2510.12962)
*Michal Minařík,Vojtěch Vonásek,Robert Pěnička*

Main category: cs.RO

TL;DR: 提出一种基于经验库的3D物体路径规划方法，通过复用历史路径信息显著提升在狭窄通道场景中的规划效率。


<details>
  <summary>Details</summary>
Motivation: 传统采样规划器在狭窄通道中采样概率低、规划时间长，且每次规划都从零开始，无法利用历史经验。在机器人应用中，不同物体需要在相同环境中规划路径，存在经验复用的机会。

Method: 构建历史路径库，为新物体在库中寻找最相似物体，将其路径作为近似解并进行变换调整，然后沿近似路径在配置空间中进行采样。

Result: 在多种狭窄通道场景中测试，相比OMPL库中的先进方法，规划时间最多减少85%，且在传统规划器失败的场景中也能找到解。

Conclusion: 基于经验复用的路径规划方法能显著提升3D物体在狭窄通道中的规划效率，方法已开源发布。

Abstract: Path planning for 3D solid objects is a challenging problem, requiring a
search in a six-dimensional configuration space, which is, nevertheless,
essential in many robotic applications such as bin-picking and assembly. The
commonly used sampling-based planners, such as Rapidly-exploring Random Trees,
struggle with narrow passages where the sampling probability is low, increasing
the time needed to find a solution. In scenarios like robotic bin-picking,
various objects must be transported through the same environment. However,
traditional planners start from scratch each time, losing valuable information
gained during the planning process. We address this by using a library of past
solutions, allowing the reuse of previous experiences even when planning for a
new, previously unseen object. Paths for a set of objects are stored, and when
planning for a new object, we find the most similar one in the library and use
its paths as approximate solutions, adjusting for possible mutual
transformations. The configuration space is then sampled along the approximate
paths. Our method is tested in various narrow passage scenarios and compared
with state-of-the-art methods from the OMPL library. Results show significant
speed improvements (up to 85% decrease in the required time) of our method,
often finding a solution in cases where the other planners fail. Our
implementation of the proposed method is released as an open-source package.

</details>


### [5] [The Omega Turn: A General Turning Template for Elongate Robots](https://arxiv.org/abs/2510.12970)
*Baxi Chong,Tianyu Wang,Kelimar Diaz,Christopher J. Pierce,Eva Erickson,Julian Whitman,Yuelin Deng,Esteban Flores,Ruijie Fu,Juntao He,Jianfeng Lin,Hang Lu,Guillaume Sartoretti,Howie Choset,Daniel I. Goldman*

Main category: cs.RO

TL;DR: 该论文提出了一种基于秀丽隐杆线虫Ω转弯启发的控制方法，使细长无肢机器人能够在杂乱环境中实现稳健有效的转弯运动。


<details>
  <summary>Details</summary>
Motivation: 细长无肢机器人在搜救和工业检查等应用中具有潜力，但现有研究对其转弯策略关注有限。为实现在杂乱空间中的有效转弯，作者从秀丽隐杆线虫的Ω转弯能力中获得灵感。

Method: 采用比较理论-生物学方法，将Ω转弯描述为两个行波的叠加，并基于此设计控制器，在实验室和杂乱现场环境中进行验证。

Result: 设计的控制器使细长无肢机器人实现了稳健有效的转弯行为，并且该Ω转弯控制器还能推广到细长多足机器人。

Conclusion: 该研究展示了一种基于身体驱动的转弯策略，适用于有肢和无肢的细长机器人，为这类机器人在复杂环境中的机动性提供了新方案。

Abstract: Elongate limbless robots have the potential to locomote through tightly
packed spaces for applications such as search-and-rescue and industrial
inspections. The capability to effectively and robustly maneuver elongate
limbless robots is crucial to realize such potential. However, there has been
limited research on turning strategies for such systems. To achieve effective
and robust turning performance in cluttered spaces, we take inspiration from a
microscopic nematode, C. elegans, which exhibits remarkable maneuverability in
rheologically complex environments partially because of its ability to perform
omega turns. Despite recent efforts to analyze omega turn kinematics, it
remains unknown if there exists a wave equation sufficient to prescribe an
omega turn, let alone its reconstruction on robot platforms. Here, using a
comparative theory-biology approach, we prescribe the omega turn as a
superposition of two traveling waves. With wave equations as a guideline, we
design a controller for limbless robots enabling robust and effective turning
behaviors in lab and cluttered field environments. Finally, we show that such
omega turn controllers can also generalize to elongate multi-legged robots,
demonstrating an alternative effective body-driven turning strategy for
elongate robots, with and without limbs.

</details>


### [6] [Actron3D: Learning Actionable Neural Functions from Videos for Transferable Robotic Manipulation](https://arxiv.org/abs/2510.12971)
*Anran Zhang,Hanzhi Chen,Yannick Burkhardt,Yao Zhong,Johannes Betz,Helen Oleynikova,Stefan Leutenegger*

Main category: cs.RO

TL;DR: Actron3D是一个从少量单目RGB人类视频中学习可迁移6自由度操作技能的机器人框架，通过神经功能表示实现跨任务技能迁移


<details>
  <summary>Details</summary>
Motivation: 解决机器人从少量未标定RGB视频中学习精确6自由度操作技能的挑战，实现高效的知识迁移

Method: 提出神经功能表示法，将几何、视觉和功能信息编码到轻量神经网络中，通过粗到细优化实现6自由度策略迁移

Result: 在仿真和真实环境中显著优于现有方法，13个任务平均成功率提升14.9个百分点，每个任务仅需2-3个演示视频

Conclusion: Actron3D证明了从少量未标定人类视频中学习可迁移6自由度操作技能的可行性，为机器人技能学习提供了新范式

Abstract: We present Actron3D, a framework that enables robots to acquire transferable
6-DoF manipulation skills from just a few monocular, uncalibrated, RGB-only
human videos. At its core lies the Neural Affordance Function, a compact
object-centric representation that distills actionable cues from diverse
uncalibrated videos-geometry, visual appearance, and affordance-into a
lightweight neural network, forming a memory bank of manipulation skills.
During deployment, we adopt a pipeline that retrieves relevant affordance
functions and transfers precise 6-DoF manipulation policies via coarse-to-fine
optimization, enabled by continuous queries to the multimodal features encoded
in the neural functions. Experiments in both simulation and the real world
demonstrate that Actron3D significantly outperforms prior methods, achieving a
14.9 percentage point improvement in average success rate across 13 tasks while
requiring only 2-3 demonstration videos per task.

</details>


### [7] [UNCAP: Uncertainty-Guided Planning Using Natural Language Communication for Cooperative Autonomous Vehicles](https://arxiv.org/abs/2510.12992)
*Neel P. Bhatt,Po-han Li,Kushagra Gupta,Rohan Siva,Daniel Milan,Alexander T. Hogue,Sandeep P. Chinchali,David Fridovich-Keil,Zhangyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: UNCAP提出了一种基于视觉语言模型的多车协同规划方法，通过轻量级自然语言消息进行通信，并显式考虑感知不确定性，显著降低了通信带宽需求并提高了驾驶安全性。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖传输高带宽原始传感器数据，要么忽略共享数据中的感知和规划不确定性，导致系统既不可扩展也不安全。需要一种既能高效通信又能处理不确定性的协同规划方法。

Method: 采用两阶段通信协议：首先识别最相关的车辆进行信息交换，然后选定的CAV传输定量表达感知不确定性的自然语言消息。通过选择性融合最大化互信息的消息，仅将最相关信号整合到决策中。

Result: 实验显示通信带宽减少63%，驾驶安全评分提高31%，决策不确定性降低61%，在接近碰撞事件中碰撞距离裕度增加四倍。

Conclusion: UNCAP通过不确定性引导的自然语言通信实现了高效且安全的协同规划，显著提升了多车系统的可扩展性和可靠性。

Abstract: Safe large-scale coordination of multiple cooperative connected autonomous
vehicles (CAVs) hinges on communication that is both efficient and
interpretable. Existing approaches either rely on transmitting high-bandwidth
raw sensor data streams or neglect perception and planning uncertainties
inherent in shared data, resulting in systems that are neither scalable nor
safe. To address these limitations, we propose Uncertainty-Guided Natural
Language Cooperative Autonomous Planning (UNCAP), a vision-language model-based
planning approach that enables CAVs to communicate via lightweight natural
language messages while explicitly accounting for perception uncertainty in
decision-making. UNCAP features a two-stage communication protocol: (i) an ego
CAV first identifies the subset of vehicles most relevant for information
exchange, and (ii) the selected CAVs then transmit messages that quantitatively
express their perception uncertainty. By selectively fusing messages that
maximize mutual information, this strategy allows the ego vehicle to integrate
only the most relevant signals into its decision-making, improving both the
scalability and reliability of cooperative planning. Experiments across diverse
driving scenarios show a 63% reduction in communication bandwidth with a 31%
increase in driving safety score, a 61% reduction in decision uncertainty, and
a four-fold increase in collision distance margin during near-miss events.
Project website: https://uncap-project.github.io/

</details>


### [8] [Development of a Linear Guide-Rail Testbed for Physically Emulating ISAM Operations](https://arxiv.org/abs/2510.13005)
*Robert Muldrow,Channing Ludden,Christopher Petersen*

Main category: cs.RO

TL;DR: 开发了一个新的硬件在环实验平台，用于模拟在轨服务、组装和制造(ISAM)操作，通过6自由度机械臂和1自由度导轨系统来验证空间运动、机器人操作和接触力学模型。


<details>
  <summary>Details</summary>
Motivation: 在轨服务、组装和制造(ISAM)操作对空间资产的寿命、容量和扩展性至关重要，但机械臂在自由飞行卫星上的运动会产生复杂的扰动，需要实验验证相关动力学模型。

Method: 设计开发了一个硬件在环实验平台，将6自由度UR3e机械臂安装在卫星总线上，该总线又安装在1自由度导轨系统上，使系统能在一个线性方向上自由移动。

Result: 成功构建了能够模拟ISAM操作的实验系统，为空间运动、串行机器人操作和接触力学模型的验证提供了实验平台。

Conclusion: 该实验平台为解决ISAM操作中的复杂控制问题提供了有效的测试和验证手段，有助于推进在轨服务技术的发展。

Abstract: In-Space Servicing, Assembly, and Manufacturing (ISAM) is a set of emerging
operations that provides several benefits to improve the longevity, capacity,
mo- bility, and expandability of existing and future space assets. Serial
robotic ma- nipulators are particularly vital in accomplishing ISAM operations,
however, the complex perturbation forces and motions associated with movement
of a robotic arm on a free-flying satellite presents a complex controls problem
requiring addi- tional study. While many dynamical models are developed,
experimentally test- ing and validating these models is challenging given that
the models operate in space, where satellites have six-degrees-of-freedom
(6-DOF). This paper attempts to resolve those challenges by presenting the
design and development of a new hardware-in-the-loop (HIL) experimental testbed
utilized to emulate ISAM. This emulation will be accomplished by means of a
6-DOF UR3e robotic arm attached to a satellite bus. This satellite bus is
mounted to a 1-DOF guide-rail system, en- abling the satellite bus and robotic
arm to move freely in one linear direction. This experimental ISAM emulation
system will explore and validate models for space motion, serial robot
manipulation, and contact mechanics.

</details>


### [9] [Kinematic Kitbashing for Modeling Functional Articulated Objects](https://arxiv.org/abs/2510.13048)
*Minghao Guo,Victor Zordan,Sheldon Andrews,Wojciech Matusik,Maneesh Agrawala,Hsueh-Ti Derek Liu*

Main category: cs.RO

TL;DR: Kinematic Kitbashing是一个自动框架，通过重用现有模型的部件来合成功能感知的铰接对象。它通过优化部件空间布局，确保几何附着在运动范围内有效，并满足用户指定的功能目标。


<details>
  <summary>Details</summary>
Motivation: 将基于部件的形状建模与功能装配设计相结合，实现快速创建交互式铰接资产。解决现有方法在几何附着和功能目标之间的平衡问题。

Method: 使用运动学感知的附着能量，在多个铰接快照中对齐向量距离函数特征。将该附着项嵌入退火黎曼朗之万动力学采样器中，将功能目标作为额外能量处理。

Result: 框架生成了广泛的铰接形状，从垃圾桶轮子移植到汽车车身到多段灯具、齿轮驱动划桨器和可重构家具。在几何、运动学和功能指标上显著优于现有基准方法。

Conclusion: 通过紧密耦合铰接感知的几何匹配和功能驱动的优化，Kinematic Kitbashing桥接了基于部件的形状建模和功能装配设计，赋能快速创建交互式铰接资产。

Abstract: We introduce Kinematic Kitbashing, an automatic framework that synthesizes
functionality-aware articulated objects by reusing parts from existing models.
Given a kinematic graph with a small collection of articulated parts, our
optimizer jointly solves for the spatial placement of every part so that (i)
attachments remain geometrically sound over the entire range of motion and (ii)
the assembled object satisfies user-specified functional goals such as
collision-free actuation, reachability, or trajectory following. At its core is
a kinematics-aware attachment energy that aligns vector distance function
features sampled across multiple articulation snapshots. We embed this
attachment term within an annealed Riemannian Langevin dynamics sampler that
treats functionality objectives as additional energies, enabling robust global
exploration while accommodating non-differentiable functionality objectives and
constraints. Our framework produces a wide spectrum of assembled articulated
shapes, from trash-can wheels grafted onto car bodies to multi-segment lamps,
gear-driven paddlers, and reconfigurable furniture, and delivers strong
quantitative improvements over state-of-the-art baselines across geometric,
kinematic, and functional metrics. By tightly coupling articulation-aware
geometry matching with functionality-driven optimization, Kinematic Kitbashing
bridges part-based shape modeling and functional assembly design, empowering
rapid creation of interactive articulated assets.

</details>


### [10] [VLA-0: Building State-of-the-Art VLAs with Zero Modification](https://arxiv.org/abs/2510.13054)
*Ankit Goyal,Hugo Hadfield,Xuning Yang,Valts Blukis,Fabio Ramos*

Main category: cs.RO

TL;DR: VLA-0是一个简单但强大的视觉-语言-动作模型，通过将动作表示为文本的方式实现，在多个基准测试中超越了更复杂的模型，包括那些经过大规模机器人数据训练的模型。


<details>
  <summary>Details</summary>
Motivation: 当前构建视觉-语言-动作模型的方法往往过于复杂，比如修改视觉-语言模型的词汇表或引入特殊的动作头。而将动作直接表示为文本这种最简单的方法却很少被探索。

Method: VLA-0采用将动作直接表示为文本的简单策略，通过特定的设计技巧来解锁其高性能潜力。

Result: 在LIBERO基准测试中，VLA-0超越了所有基于相同机器人数据训练的现有方法，包括π0.5-KI、OpenVLA-OFT和SmolVLA。即使没有大规模机器人特定训练，它也超越了经过大规模机器人数据训练的模型。在真实世界测试中，VLA-0也优于经过大规模真实数据预训练的SmolVLA模型。

Conclusion: 将动作表示为文本的简单方法不仅有效，而且出人意料地强大，在适当的设���下能够超越更复杂的模型，为构建通用机器人操作模型提供了新的思路。

Abstract: Vision-Language-Action models (VLAs) hold immense promise for enabling
generalist robot manipulation. However, the best way to build them remains an
open question. Current approaches often add complexity, such as modifying the
existing vocabulary of a Vision-Language Model (VLM) with action tokens or
introducing special action heads. Curiously, the simplest strategy of
representing actions directly as text has remained largely unexplored. This
work introduces VLA-0 to investigate this idea. We find that VLA-0 is not only
effective; it is surprisingly powerful. With the right design, VLA-0
outperforms more involved models. On LIBERO, a popular benchmark for evaluating
VLAs, VLA-0 outperforms all existing methods trained on the same robotic data,
including $\pi_0.5$-KI, OpenVLA-OFT and SmolVLA. Furthermore, without
large-scale robotics-specific training, it outperforms methods trained on
large-scale robotic data, like $\pi_0.5$-KI, $\pi_0$, GR00T-N1 and MolmoAct.
These findings also translate to the real world, where VLA-0 outperforms
SmolVLA, a VLA model pre-trained on large-scale real data. This paper
summarizes our unexpected findings and spells out the specific techniques
required to unlock the high performance of this simple yet potent VLA design.
Visual results, code, and trained models are provided here:
https://vla0.github.io/.

</details>


### [11] [RoboHiMan: A Hierarchical Evaluation Paradigm for Compositional Generalization in Long-Horizon Manipulation](https://arxiv.org/abs/2510.13149)
*Yangtao Chen,Zixuan Chen,Nga Teng Chan,Junting Chen,Junhui Yin,Jieqi Shi,Yang Gao,Yong-Lu Li,Jing Huo*

Main category: cs.RO

TL;DR: 提出了RoboHiMan评估范式，用于系统研究长时程操作中的组合泛化能力，包括HiMan-Bench基准测试和三种评估模式，揭示了现有模型在技能组合和鲁棒性方面的能力差距。


<details>
  <summary>Details</summary>
Motivation: 现有机器人技能调度方法在应对复杂扰动时表现有限，端到端VLA模型泛化能力不足，分层方法在技能组合方面仍有局限，且现有基准主要关注任务完成度，缺乏对组合泛化、鲁棒性及规划与执行交互的系统评估。

Method: 提出RoboHiMan分层评估范式，包括：HiMan-Bench基准（包含原子和组合任务及多样化扰动）、多级训练数据集用于分析渐进数据扩展、三种评估模式（vanilla、decoupled、coupled）来探究技能组合的必要性和分层架构瓶颈。

Result: 实验揭示了代表性模型和架构在组合泛化能力方面存在明显差距，为开发更适合现实世界长时程操作任务的模型指明了方向。

Conclusion: RoboHiMan为系统评估长时程操作中的组合泛化能力提供了有效框架，揭示了现有方法的局限性，并为未来模型发展提供了重要指导。

Abstract: Enabling robots to flexibly schedule and compose learned skills for novel
long-horizon manipulation under diverse perturbations remains a core challenge.
Early explorations with end-to-end VLA models show limited success, as these
models struggle to generalize beyond the training distribution. Hierarchical
approaches, where high-level planners generate subgoals for low-level policies,
bring certain improvements but still suffer under complex perturbations,
revealing limited capability in skill composition. However, existing benchmarks
primarily emphasize task completion in long-horizon settings, offering little
insight into compositional generalization, robustness, and the interplay
between planning and execution. To systematically investigate these gaps, we
propose RoboHiMan, a hierarchical evaluation paradigm for compositional
generalization in long-horizon manipulation. RoboHiMan introduces HiMan-Bench,
a benchmark of atomic and compositional tasks under diverse perturbations,
supported by a multi-level training dataset for analyzing progressive data
scaling, and proposes three evaluation paradigms (vanilla, decoupled, coupled)
that probe the necessity of skill composition and reveal bottlenecks in
hierarchical architectures. Experiments highlight clear capability gaps across
representative models and architectures, pointing to directions for advancing
models better suited to real-world long-horizon manipulation tasks. Videos and
open-source code can be found on our project website:
https://chenyt31.github.io/robo-himan.github.io/.

</details>


### [12] [ALOHA2 Robot Kitchen Application Scenario Reproduction Report](https://arxiv.org/abs/2510.13284)
*Haoyang Wu,Siheng Wu,William X. Liu,Fangui Zeng*

Main category: cs.RO

TL;DR: ALOHA2是双臂遥操作机器人ALOHA的增强版本，具有更高性能和鲁棒性，同时更符合人体工程学设计。


<details>
  <summary>Details</summary>
Motivation: 改进原始ALOHA机器人的性能、鲁棒性和人体工程学设计，提供更好的遥操作体验。

Method: 采用双ViperX 6自由度机械臂和两个小型WidowX机械臂，通过反向驱动控制从动机械臂，配备多视角摄像头收集RGB数据，安装在带铝框架的工作台上。

Result: 开发出性能更高、更鲁棒且更符合人体工程学的双臂遥操作机器人系统。

Conclusion: ALOHA2成功提升了原始设计的性能、鲁棒性和用户体验，为遥操作机器人提供了改进的硬件平台。

Abstract: ALOHA2 is an enhanced version of the dual-arm teleoperated robot ALOHA,
featuring higher performance and robustness compared to the original design,
while also being more ergonomic. Like ALOHA, ALOHA2 consists of two grippers
and two ViperX 6-DoF arms, as well as two smaller WidowX arms. Users control
the follower mechanical arms by operating the leader mechanical arms through
back-driving. The device also includes cameras that generate images from
multiple viewpoints, allowing for RGB data collection during teleoperation. The
robot is mounted on a 48-inch x 30-inch table, equipped with an aluminum frame
that provides additional mounting points for cameras and gravity compensation
systems.

</details>


### [13] [DAMM-LOAM: Degeneracy Aware Multi-Metric LiDAR Odometry and Mapping](https://arxiv.org/abs/2510.13287)
*Nishant Chandna,Akshat Kaushal*

Main category: cs.RO

TL;DR: 提出DAMM-LOAM系统，通过点云分类和退化感知ICP算法，解决LiDAR SLAM在稀疏特征和重复结构环境中的6-DOF姿态估计退化问题。


<details>
  <summary>Details</summary>
Motivation: 当前点对平面ICP算法在结构化环境中表现良好，但在稀疏特征、重复几何结构和高频运动场景中会出现6-DOF姿态估计退化问题，现有方法多依赖多传感器融合，纯LiDAR方案仍面临挑战。

Method: 1. 基于表面法线和邻域分析的点云分类（地面、墙壁、屋顶、边缘、非平面点）；2. 退化感知加权最小二乘ICP算法；3. Scan Context后端支持鲁棒回环检测。

Result: DAMM-LOAM在里程计精度方面显著提升，特别是在长走廊等室内环境中表现优异。

Conclusion: 该方法通过点云分类和退化感知机制，有效提升了纯LiDAR SLAM系统在挑战性环境中的性能。

Abstract: LiDAR Simultaneous Localization and Mapping (SLAM) systems are essential for
enabling precise navigation and environmental reconstruction across various
applications. Although current point-to-plane ICP algorithms perform effec-
tively in structured, feature-rich environments, they struggle in scenarios
with sparse features, repetitive geometric structures, and high-frequency
motion. This leads to degeneracy in 6- DOF pose estimation. Most
state-of-the-art algorithms address these challenges by incorporating
additional sensing modalities, but LiDAR-only solutions continue to face
limitations under such conditions. To address these issues, we propose a novel
Degeneracy-Aware Multi-Metric LiDAR Odometry and Map- ping (DAMM-LOAM) module.
Our system improves mapping accuracy through point cloud classification based
on surface normals and neighborhood analysis. Points are classified into
ground, walls, roof, edges, and non-planar points, enabling accurate
correspondences. A Degeneracy-based weighted least squares-based ICP algorithm
is then applied for accurate odom- etry estimation. Additionally, a Scan
Context based back-end is implemented to support robust loop closures.
DAMM-LOAM demonstrates significant improvements in odometry accuracy,
especially in indoor environments such as long corridors

</details>


### [14] [Tactile-Conditioned Diffusion Policy for Force-Aware Robotic Manipulation](https://arxiv.org/abs/2510.13324)
*Erik Helmut,Niklas Funk,Tim Schneider,Cristiana de Farias,Jan Peters*

Main category: cs.RO

TL;DR: 提出了FARM框架，通过整合高维触觉数据来推断触觉条件力信号，并定义基于力的动作空间，在需要不同力要求的任务中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法通常只将视觉触觉反馈视为额外观察，而将施加的力作为夹持器命令的不可控后果，无法有效处理接触丰富的操作任务。

Method: 使用集成了GelSight Mini视觉触觉传感器的改进版手持UMI夹持器收集人类演示，开发了与手持版本几何匹配的驱动变体。在策略部署时，提出的FARM扩散策略联合预测机器人位姿、夹持宽度和夹持力。

Result: FARM在三个具有不同力要求的任务中（高力、低力和动态力适应）均优于多个基线方法，证明了其两个关键组件的优势。

Conclusion: FARM框架通过利用基于力的高维触觉观测和基于力的控制空间，在接触丰富的操作任务中表现出色，相关代码和设计文件已开源。

Abstract: Contact-rich manipulation depends on applying the correct grasp forces
throughout the manipulation task, especially when handling fragile or
deformable objects. Most existing imitation learning approaches often treat
visuotactile feedback only as an additional observation, leaving applied forces
as an uncontrolled consequence of gripper commands. In this work, we present
Force-Aware Robotic Manipulation (FARM), an imitation learning framework that
integrates high-dimensional tactile data to infer tactile-conditioned force
signals, which in turn define a matching force-based action space. We collect
human demonstrations using a modified version of the handheld Universal
Manipulation Interface (UMI) gripper that integrates a GelSight Mini visual
tactile sensor. For deploying the learned policies, we developed an actuated
variant of the UMI gripper with geometry matching our handheld version. During
policy rollouts, the proposed FARM diffusion policy jointly predicts robot
pose, grip width, and grip force. FARM outperforms several baselines across
three tasks with distinct force requirements -- high-force, low-force, and
dynamic force adaptation -- demonstrating the advantages of its two key
components: leveraging force-grounded, high-dimensional tactile observations
and a force-based control space. The codebase and design files are open-sourced
and available at https://tactile-farm.github.io .

</details>


### [15] [MODUR: A Modular Dual-reconfigurable Robot](https://arxiv.org/abs/2510.13356)
*Jie Gu,Tin Lun Lam,Chunxu Tian,Zhihao Xia,Yongheng Xing,Dan Zhang*

Main category: cs.RO

TL;DR: 提出了一种新型模块化自重构机器人MODUR，具有双层级重构能力，既能实现模块间的高层级重构，又能让单个模块改变形状执行基本运动。


<details>
  <summary>Details</summary>
Motivation: 模块化自重构机器人系统能够通过改变模块间的拓扑关系形成更高级别的机器人系统，在各种环境中提供更强的适应性和鲁棒性。

Method: 设计包括紧凑连接器和剪刀连杆组，形成并联机构，实现连接器运动解耦和相邻位置迁移能力。对相互依赖连接器的工作空间进行全面分析。

Result: 通过一系列实验验证了MODUR的运动能力。

Conclusion: MODUR的双层级重构设计为模块化自重构机器人提供了新的可能性，紧凑连接器和剪刀连杆组的设计实现了有效的运动能力。

Abstract: Modular Self-Reconfigurable Robot (MSRR) systems are a class of robots
capable of forming higher-level robotic systems by altering the topological
relationships between modules, offering enhanced adaptability and robustness in
various environments. This paper presents a novel MSRR called MODUR, featuring
dual-level reconfiguration capabilities designed to integrate reconfigurable
mechanisms into MSRR. Specifically, MODUR can perform high-level
self-reconfiguration among modules to create different configurations, while
each module is also able to change its shape to execute basic motions. The
design of MODUR primarily includes a compact connector and scissor linkage
groups that provide actuation, forming a parallel mechanism capable of
achieving both connector motion decoupling and adjacent position migration
capabilities. Furthermore, the workspace, considering the interdependent
connectors, is comprehensively analyzed, laying a theoretical foundation for
the design of the module's basic motion. Finally, the motion of MODUR is
validated through a series of experiments.

</details>


### [16] [Adversarial Fine-tuning in Offline-to-Online Reinforcement Learning for Robust Robot Control](https://arxiv.org/abs/2510.13358)
*Shingo Ayabe,Hiroshi Kera,Kazuhiko Kawamoto*

Main category: cs.RO

TL;DR: 提出一种离线到在线框架，通过在离线训练后注入动作空间扰动进行对抗性微调，结合性能感知课程来提升策略对执行器故障等扰动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习虽然样本效率高，但在动作空间扰动下策略仍然脆弱。需要一种方法在不牺牲离线效率的前提下提升策略的鲁棒性。

Method: 先在干净数据上离线训练策略，然后进行对抗性微调，注入动作扰动诱导补偿行为。使用基于指数移动平均的性能感知课程动态调整扰动概率。

Result: 在连续控制运动任务中，该方法相比纯离线基线显著提升鲁棒性，比从头训练收敛更快。匹配微调和评估条件时鲁棒性最强，自适应课程策略缓解了线性课程策略的性能下降问题。

Conclusion: 对抗性微调能够在不确定环境中实现自适应和鲁棒控制，弥合离线效率和在线适应性之间的差距。

Abstract: Offline reinforcement learning enables sample-efficient policy acquisition
without risky online interaction, yet policies trained on static datasets
remain brittle under action-space perturbations such as actuator faults. This
study introduces an offline-to-online framework that trains policies on clean
data and then performs adversarial fine-tuning, where perturbations are
injected into executed actions to induce compensatory behavior and improve
resilience. A performance-aware curriculum further adjusts the perturbation
probability during training via an exponential-moving-average signal, balancing
robustness and stability throughout the learning process. Experiments on
continuous-control locomotion tasks demonstrate that the proposed method
consistently improves robustness over offline-only baselines and converges
faster than training from scratch. Matching the fine-tuning and evaluation
conditions yields the strongest robustness to action-space perturbations, while
the adaptive curriculum strategy mitigates the degradation of nominal
performance observed with the linear curriculum strategy. Overall, the results
show that adversarial fine-tuning enables adaptive and robust control under
uncertain environments, bridging the gap between offline efficiency and online
adaptability.

</details>


### [17] [Real-Time Knee Angle Prediction Using EMG and Kinematic Data with an Attention-Based CNN-LSTM Network and Transfer Learning Across Multiple Datasets](https://arxiv.org/abs/2510.13443)
*Mojtaba Mollahossein,Gholamreza Vossoughi,Mohammad Hossein Rohban*

Main category: cs.RO

TL;DR: 提出基于迁移学习的膝关节角度预测框架，仅需少量步态周期数据，在多个数据集上实现高精度预测，适用于康复场景。


<details>
  <summary>Details</summary>
Motivation: 解决传统肌电信号预测方法在实时应用、测试条件代表性和大数据需求方面的挑战。

Method: 开发轻量级注意力CNN-LSTM模型，在Georgia Tech数据集预训练后迁移到UCI和SMLE数据集，结合EMG、运动学和交互力输入。

Result: 仅使用EMG输入时异常受试者NMAE为6.8%(单步)和13.7%(50步)；结合历史角度后正常受试者NMAE降至3.1%和3.5%，异常受试者降至2.8%和7.5%；在SMLE外骨骼上达到1.09%和3.1% NMAE。

Conclusion: 该框架在短期和长期康复场景中表现出鲁棒性能和强泛化能力。

Abstract: Electromyography (EMG) signals are widely used for predicting body joint
angles through machine learning (ML) and deep learning (DL) methods. However,
these approaches often face challenges such as limited real-time applicability,
non-representative test conditions, and the need for large datasets to achieve
optimal performance. This paper presents a transfer-learning framework for knee
joint angle prediction that requires only a few gait cycles from new subjects.
Three datasets - Georgia Tech, the University of California Irvine (UCI), and
the Sharif Mechatronic Lab Exoskeleton (SMLE) - containing four EMG channels
relevant to knee motion were utilized. A lightweight attention-based CNN-LSTM
model was developed and pre-trained on the Georgia Tech dataset, then
transferred to the UCI and SMLE datasets. The proposed model achieved
Normalized Mean Absolute Errors (NMAE) of 6.8 percent and 13.7 percent for
one-step and 50-step predictions on abnormal subjects using EMG inputs alone.
Incorporating historical knee angles reduced the NMAE to 3.1 percent and 3.5
percent for normal subjects, and to 2.8 percent and 7.5 percent for abnormal
subjects. When further adapted to the SMLE exoskeleton with EMG, kinematic, and
interaction force inputs, the model achieved 1.09 percent and 3.1 percent NMAE
for one- and 50-step predictions, respectively. These results demonstrate
robust performance and strong generalization for both short- and long-term
rehabilitation scenarios.

</details>


### [18] [Bridge the Gap: Enhancing Quadruped Locomotion with Vertical Ground Perturbations](https://arxiv.org/abs/2510.13488)
*Maximilian Stasica,Arne Bick,Nico Bohlinger,Omid Mohseni,Max Johannes Alois Fritzsche,Clemens Hübler,Jan Peters,André Seyfarth*

Main category: cs.RO

TL;DR: 该研究通过在振荡桥上训练四足机器人，使用强化学习方法提升了机器人在动态地面扰动下的运动鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 四足机器人虽然在崎岖地形上表现出色，但在垂直地面扰动（如振荡表面）下的性能仍有待探索。

Method: 使用PPO算法在MuJoCo模拟器中训练15种不同的运动策略，结合五种步态和三种训练条件（刚性桥面和两种振荡桥面），并通过领域随机化实现零样本迁移。

Result: 在振荡桥上训练的策略比在刚性表面上训练的策略表现出更好的稳定性和适应性。

Conclusion: 基于模拟的强化学习能够显著提高四足机器人在动态地面扰动下的运动能力，为设计能够穿越振动环境的机器人提供了重要见解。

Abstract: Legged robots, particularly quadrupeds, excel at navigating rough terrains,
yet their performance under vertical ground perturbations, such as those from
oscillating surfaces, remains underexplored. This study introduces a novel
approach to enhance quadruped locomotion robustness by training the Unitree Go2
robot on an oscillating bridge - a 13.24-meter steel-and-concrete structure
with a 2.0 Hz eigenfrequency designed to perturb locomotion. Using
Reinforcement Learning (RL) with the Proximal Policy Optimization (PPO)
algorithm in a MuJoCo simulation, we trained 15 distinct locomotion policies,
combining five gaits (trot, pace, bound, free, default) with three training
conditions: rigid bridge and two oscillating bridge setups with differing
height regulation strategies (relative to bridge surface or ground). Domain
randomization ensured zero-shot transfer to the real-world bridge. Our results
demonstrate that policies trained on the oscillating bridge exhibit superior
stability and adaptability compared to those trained on rigid surfaces. Our
framework enables robust gait patterns even without prior bridge exposure.
These findings highlight the potential of simulation-based RL to improve
quadruped locomotion during dynamic ground perturbations, offering insights for
designing robots capable of traversing vibrating environments.

</details>


### [19] [A Novel Robot Hand with Hoeckens Linkages and Soft Phalanges for Scooping and Self-Adaptive Grasping in Environmental Constraints](https://arxiv.org/abs/2510.13535)
*Wentao Guo,Yizhou Wang,Wenzeng Zhang*

Main category: cs.RO

TL;DR: 提出了一种新型欠驱动自适应机器人手Hockens-A Hand，通过集成Hoeckens机构、双平行四边形连杆和专用四杆机构，仅需单个线性执行器即可实现三种自适应抓取模式：平行捏取、非对称铲取和包络抓取。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够在非结构化环境中实现自适应和顺应性抓取的机器人手，通过被动机械智能减少执行器数量，提高抓取适应性。

Method: 结合Hoeckens机构提供顺应性，双平行四边形连杆确保指尖线接触，四杆放大系统实现抓取模式间自然过渡，并通过运动学分析优化推角和连杆长度设计。

Result: 仿真验证了指尖运动和抓取模式间平滑过渡，实验使用3D打印原型验证了三种抓取模式在不同环境约束下的抓取稳定性和广泛适用性。

Conclusion: Hockens-A Hand成功实现了仅需单个执行器的多模式自适应抓取，在非结构化环境中表现出良好的顺应性和抓取稳定性。

Abstract: This paper presents a novel underactuated adaptive robotic hand, Hockens-A
Hand, which integrates the Hoeckens mechanism, a double-parallelogram linkage,
and a specialized four-bar linkage to achieve three adaptive grasping modes:
parallel pinching, asymmetric scooping, and enveloping grasping. Hockens-A Hand
requires only a single linear actuator, leveraging passive mechanical
intelligence to ensure adaptability and compliance in unstructured
environments. Specifically, the vertical motion of the Hoeckens mechanism
introduces compliance, the double-parallelogram linkage ensures line contact at
the fingertip, and the four-bar amplification system enables natural
transitions between different grasping modes. Additionally, the inclusion of a
mesh-textured silicone phalanx further enhances the ability to envelop objects
of various shapes and sizes. This study employs detailed kinematic analysis to
optimize the push angle and design the linkage lengths for optimal performance.
Simulations validated the design by analyzing the fingertip motion and ensuring
smooth transitions between grasping modes. Furthermore, the grasping force was
analyzed using power equations to enhance the understanding of the system's
performance.Experimental validation using a 3D-printed prototype demonstrates
the three grasping modes of the hand in various scenarios under environmental
constraints, verifying its grasping stability and broad applicability.

</details>


### [20] [Hoecken-D Hand: A Novel Robotic Hand for Linear Parallel Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.13553)
*Wentao Guo,Wenzeng Zhang*

Main category: cs.RO

TL;DR: Hoecken-D Hand是一种欠驱动机器人抓手，结合改进的Hoecken连杆机构和差动弹簧机制，实现线性平行夹持和自适应包络的中间过渡，无需额外执行器即可完成接触触发重构。


<details>
  <summary>Details</summary>
Motivation: 开发一种紧凑、自适应且成本效益高的机器人抓手，能够在非结构化环境中可靠抓取各种几何形状的物体，同时保持简单性和低成本。

Method: 通过重新配置原始Hoecken连杆机构，用差动连杆替换一个构件，保持直线引导同时实现接触触发重构；采用双平行四边形布置保持指尖平行性，差动机制允许遇到障碍时一个手指向内包裹；可由单个线性执行器驱动。

Result: 原型使用PLA 3D打印制造，线性夹持跨度约200mm；初步测试显示在多种物体几何形状下两种模式都能可靠抓取，提高了不规则或薄物体的稳定性。

Conclusion: Hoecken-D Hand是一种紧凑、自适应且成本效益高的解决方案，适用于非结构化环境中的操作任务，展示了可靠的抓取性能。

Abstract: This paper presents the Hoecken-D Hand, an underactuated robotic gripper that
combines a modified Hoecken linkage with a differential spring mechanism to
achieve both linear parallel pinching and a mid-stroke transition to adaptive
envelope. The original Hoecken linkage is reconfigured by replacing one member
with differential links, preserving straight-line guidance while enabling
contact-triggered reconfiguration without additional actuators. A
double-parallelogram arrangement maintains fingertip parallelism during
conventional pinching, whereas the differential mechanism allows one finger to
wrap inward upon encountering an obstacle, improving stability on irregular or
thin objects. The mechanism can be driven by a single linear actuator,
minimizing complexity and cost; in our prototype, each finger is driven by its
own linear actuator for simplicity. We perform kinematic modeling and force
analysis to characterize grasp performance, including simulated grasping forces
and spring-opening behavior under varying geometric parameters. The design was
prototyped using PLA-based 3D printing, achieving a linear pinching span of
approximately 200 mm. Preliminary tests demonstrate reliable grasping in both
modes across a wide range of object geometries, highlighting the Hoecken-D Hand
as a compact, adaptable, and cost-effective solution for manipulation in
unstructured environments.

</details>


### [21] [Development of an Intuitive GUI for Non-Expert Teleoperation of Humanoid Robots](https://arxiv.org/abs/2510.13594)
*Austin Barret,Meng Cheng Lau*

Main category: cs.RO

TL;DR: 开发面向非专家用户的仿人机器人图形用户界面，用于控制机器人通过FIRA障碍赛道


<details>
  <summary>Details</summary>
Motivation: 当前仿人机器人系统缺乏针对非专家用户的直观GUI，限制了系统的可操作性

Method: 结合用户界面开发实践和人机交互概念，开发简单直观的可扩展GUI

Result: 开发出适合非专家用户远程操作的新型界面

Conclusion: 成功创建了简单直观的仿人机器人操作界面，使非专家用户能够有效控制机器人

Abstract: The operation of humanoid robotics is an essential field of research with
many practical and competitive applications. Many of these systems, however, do
not invest heavily in developing a non-expert-centered graphical user interface
(GUI) for operation. The focus of this research is to develop a scalable GUI
that is tailored to be simple and intuitive so non-expert operators can control
the robot through a FIRA-regulated obstacle course. Using common practices from
user interface development (UI) and understanding concepts described in
human-robot interaction (HRI) and other related concepts, we will develop a new
interface with the goal of a non-expert teleoperation system.

</details>


### [22] [Active Tactile Exploration for Rigid Body Pose and Shape Estimation](https://arxiv.org/abs/2510.13595)
*Ethan K. Gordon,Bruke Baraki,Hien Bui,Michael Posa*

Main category: cs.RO

TL;DR: 提出了一种仅使用触觉数据同时确定刚性物体形状和位置的学习与探索框架，通过优化物理约束违反的损失函数来学习立方体和凸多面体几何，在首次接触后仅需不到10秒的随机收集数据。


<details>
  <summary>Details</summary>
Motivation: 通用机器人操作需要处理未见过的物体。在测试时学习物理精确模型可以在数据效率、可预测性和任务间重用方面提供显著优势。触觉感知可以补充视觉感知，但其时间稀疏性需要仔细的在线探索以保持数据效率。

Method: 基于接触丰富系统识别的最新进展，制定了一个惩罚物理约束违反的损失函数，而不引入刚体接触固有的数值刚度。通过优化该损失函数，可以学习几何形状。探索方案旨在最大化期望信息增益。

Result: 能够学习立方体和凸多面体几何，在首次接触后仅需不到10秒的随机收集数据。在模拟和真实机器人实验中，探索方案显著加快了学习速度。

Conclusion: 该框架仅使用触觉数据就能同时确定刚性物体的形状和位置，具有高数据效率，探索方案能显著加速学习过程。

Abstract: General robot manipulation requires the handling of previously unseen
objects. Learning a physically accurate model at test time can provide
significant benefits in data efficiency, predictability, and reuse between
tasks. Tactile sensing can compliment vision with its robustness to occlusion,
but its temporal sparsity necessitates careful online exploration to maintain
data efficiency. Direct contact can also cause an unrestrained object to move,
requiring both shape and location estimation. In this work, we propose a
learning and exploration framework that uses only tactile data to
simultaneously determine the shape and location of rigid objects with minimal
robot motion. We build on recent advances in contact-rich system identification
to formulate a loss function that penalizes physical constraint violation
without introducing the numerical stiffness inherent in rigid-body contact.
Optimizing this loss, we can learn cuboid and convex polyhedral geometries with
less than 10s of randomly collected data after first contact. Our exploration
scheme seeks to maximize Expected Information Gain and results in significantly
faster learning in both simulated and real-robot experiments. More information
can be found at https://dairlab.github.io/activetactile

</details>


### [23] [PlanarMesh: Building Compact 3D Meshes from LiDAR using Incremental Adaptive Resolution Reconstruction](https://arxiv.org/abs/2510.13599)
*Jiahao Wang,Nived Chebrolu,Yifu Tao,Lintong Zhang,Ayoung Kim,Maurice Fallon*

Main category: cs.RO

TL;DR: PlanarMesh是一种新颖的增量式、基于网格的LiDAR重建系统，通过自适应调整网格分辨率实现紧凑、详细的实时重建。


<details>
  <summary>Details</summary>
Motivation: 构建一个在线3D LiDAR建图系统，既能产生详细表面重建，又保持计算效率，是一个具有挑战性的任务。

Method: 引入平面网格表示法，结合平面建模和网格化来捕捉大表面和细节几何；采用多线程架构和BVH进行高效数据存储和快速搜索；增量更新考虑局部表面曲率和传感器测量的自由空间信息。

Result: 重建精度达到或超过最先进技术（包括TSDF、占据映射和体素网格化），同时输出文件大小更小（比原始输入小10倍，比基于网格的方法小5倍以上），保持实时性能（64线传感器约2Hz）。

Conclusion: PlanarMesh系统在保持实时性能的同时，实现了高精度、紧凑的LiDAR重建，优于现有方法。

Abstract: Building an online 3D LiDAR mapping system that produces a detailed surface
reconstruction while remaining computationally efficient is a challenging task.
In this paper, we present PlanarMesh, a novel incremental, mesh-based LiDAR
reconstruction system that adaptively adjusts mesh resolution to achieve
compact, detailed reconstructions in real-time. It introduces a new
representation, planar-mesh, which combines plane modeling and meshing to
capture both large surfaces and detailed geometry. The planar-mesh can be
incrementally updated considering both local surface curvature and free-space
information from sensor measurements. We employ a multi-threaded architecture
with a Bounding Volume Hierarchy (BVH) for efficient data storage and fast
search operations, enabling real-time performance. Experimental results show
that our method achieves reconstruction accuracy on par with, or exceeding,
state-of-the-art techniques-including truncated signed distance functions,
occupancy mapping, and voxel-based meshing-while producing smaller output file
sizes (10 times smaller than raw input and more than 5 times smaller than
mesh-based methods) and maintaining real-time performance (around 2 Hz for a
64-beam sensor).

</details>


### [24] [Efficient Force and Stiffness Prediction in Robotic Produce Handling with a Piezoresistive Pressure Sensor](https://arxiv.org/abs/2510.13616)
*Preston Fairchild,Claudia Chen,Xiaobo Tan*

Main category: cs.RO

TL;DR: 开发了一种低成本柔性压力传感器，集成到机器人夹爪中，用于处理不同形状、大小和硬度的农产品，通过算法实现实时力反馈和产品特性识别。


<details>
  <summary>Details</summary>
Motivation: 农业采摘和加工自动化需要机器人能够正确处理易损农产品，关键在于掌握合适的抓取力度以避免损坏产品。

Method: 将低成本易制造的柔性压力传感器集成到刚性机器人夹爪和气动软指中，提出基于瞬态响应的稳态值快速估计算法，实现实时应用。

Result: 传感器能有效提供反馈以正确抓取未知大小和硬度的物体，同时能估计产品特性如成熟度和损伤程度，并为不同硬度物体提供力反馈。

Conclusion: 该技术不仅可用于农产品识别，还可用于质量控制、基于成熟度的选择性分拣等任务，具有广阔的应用前景。

Abstract: Properly handling delicate produce with robotic manipulators is a major part
of the future role of automation in agricultural harvesting and processing.
Grasping with the correct amount of force is crucial in not only ensuring
proper grip on the object, but also to avoid damaging or bruising the product.
In this work, a flexible pressure sensor that is both low cost and easy to
fabricate is integrated with robotic grippers for working with produce of
varying shapes, sizes, and stiffnesses. The sensor is successfully integrated
with both a rigid robotic gripper, as well as a pneumatically actuated soft
finger. Furthermore, an algorithm is proposed for accelerated estimation of the
steady-state value of the sensor output based on the transient response data,
to enable real-time applications. The sensor is shown to be effective in
incorporating feedback to correctly grasp objects of unknown sizes and
stiffnesses. At the same time, the sensor provides estimates for these values
which can be utilized for identification of qualities such as ripeness levels
and bruising. It is also shown to be able to provide force feedback for objects
of variable stiffnesses. This enables future use not only for produce
identification, but also for tasks such as quality control and selective
distribution based on ripeness levels.

</details>


### [25] [Characterizing Lidar Point-Cloud Adversities Using a Vector Field Visualization](https://arxiv.org/abs/2510.13619)
*Daniel Choate,Jason Rife*

Main category: cs.RO

TL;DR: 提出一种可视化方法，用于帮助分析师识别影响激光雷达扫描匹配的逆境模式，通过生成向量场图来显示配准点云之间的局部差异。


<details>
  <summary>Details</summary>
Motivation: 帮助分析师从原始点云数据中提取难以发现的模式，识别影响扫描匹配质量的逆境机制。

Method: 生成向量场图来表征配准点云之间的局部差异，通过离线分析让分析师能够推理和移除逆境机制。

Result: 在两个概念验证示例（模拟研究和现场实验）中，分析师能够识别一系列逆境机制并迭代地从原始数据中移除这些机制。

Conclusion: 该方法能够有效帮助分析师关注逐渐减小的差异，提升对激光雷达扫描匹配问题的理解。

Abstract: In this paper we introduce a visualization methodology to aid a human analyst
in classifying adversity modes that impact lidar scan matching. Our methodology
is intended for offline rather than real-time analysis. The method generates a
vector-field plot that characterizes local discrepancies between a pair of
registered point clouds. The vector field plot reveals patterns that would be
difficult for the analyst to extract from raw point-cloud data. After
introducing our methodology, we apply the process to two proof-of-concept
examples: one a simulation study and the other a field experiment. For both
data sets, a human analyst was able to reason about a series of adversity
mechanisms and iteratively remove those mechanisms from the raw data, to help
focus attention on progressively smaller discrepancies.

</details>


### [26] [A Modular Object Detection System for Humanoid Robots Using YOLO](https://arxiv.org/abs/2510.13625)
*Nicolas Pottier,Meng Cheng Lau*

Main category: cs.RO

TL;DR: 该研究提出了一种基于YOLOv9的通用视觉模块，用于机器人环境中的计算机视觉任务，并与现有几何框架在静态和动态场景下进行性能比较。


<details>
  <summary>Details</summary>
Motivation: 机器人领域中的计算机视觉仍然是进展的主要障碍，许多任务受到低效视觉系统的阻碍。

Method: 使用YOLOv9框架构建视觉模块，在FIRA机器人Hurocup数据集上训练，并通过ROS1在虚拟环境中实现YOLO兼容性。

Result: YOLO模型在精度上与几何模型相当，但计算成本更高，同时提供了更好的鲁棒性。

Conclusion: YOLO模型在机器人视觉任务中具有更高的鲁棒性，但需要权衡计算成本。

Abstract: Within the field of robotics, computer vision remains a significant barrier
to progress, with many tasks hindered by inefficient vision systems. This
research proposes a generalized vision module leveraging YOLOv9, a
state-of-the-art framework optimized for computationally constrained
environments like robots. The model is trained on a dataset tailored to the
FIRA robotics Hurocup. A new vision module is implemented in ROS1 using a
virtual environment to enable YOLO compatibility. Performance is evaluated
using metrics such as frames per second (FPS) and Mean Average Precision (mAP).
Performance is then compared to the existing geometric framework in static and
dynamic contexts. The YOLO model achieved comparable precision at a higher
computational cost then the geometric model, while providing improved
robustness.

</details>


### [27] [LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models](https://arxiv.org/abs/2510.13626)
*Senyu Fei,Siyin Wang,Junhao Shi,Zihao Dai,Jikun Cai,Pengfang Qian,Li Ji,Xinzhe He,Shiduo Zhang,Zhaoye Fei,Jinlan Fu,Jingjing Gong,Xipeng Qiu*

Main category: cs.RO

TL;DR: 对VLA模型进行系统性脆弱性分析，发现尽管在基准测试中表现优秀，但在七种扰动因素下模型性能大幅下降，暴露出严重的鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: VLA模型在机器人操作基准测试中报告了令人印象深刻的成功率，但这些结果可能掩盖了鲁棒性的根本弱点。

Method: 通过引入七个维度的受控扰动进行系统性脆弱性分析：物体布局、相机视角、机器人初始状态、语言指令、光照条件、背景纹理和传感器噪声。

Result: 模型表现出极端的敏感性，性能从95%降至30%以下；令人惊讶的是，模型对语言变化不敏感，倾向于完全忽略语言指令。

Conclusion: 研究结果挑战了高基准分数等同于真正能力的假设，强调需要评估在真实变化下的可靠性。

Abstract: Visual-Language-Action (VLA) models report impressive success rates on
robotic manipulation benchmarks, yet these results may mask fundamental
weaknesses in robustness. We perform a systematic vulnerability analysis by
introducing controlled perturbations across seven dimensions: objects layout,
camera viewpoints, robot initial states, language instructions, light
conditions, background textures and sensor noise. We comprehensively analyzed
multiple state-of-the-art models and revealed consistent brittleness beneath
apparent competence. Our analysis exposes critical weaknesses: models exhibit
extreme sensitivity to perturbation factors, including camera viewpoints and
robot initial states, with performance dropping from 95% to below 30% under
modest perturbations. Surprisingly, models are largely insensitive to language
variations, with further experiments revealing that models tend to ignore
language instructions completely. Our findings challenge the assumption that
high benchmark scores equate to true competency and highlight the need for
evaluation practices that assess reliability under realistic variation.

</details>


### [28] [On Your Own: Pro-level Autonomous Drone Racing in Uninstrumented Arenas](https://arxiv.org/abs/2510.13644)
*Michael Bosello,Flavio Pinzarrone,Sara Kiade,Davide Aguiari,Yvo Keuter,Aaesha AlShehhi,Gyordan Caminati,Kei Long Wong,Ka Seng Chou,Junaid Halepota,Fares Alneyadi,Jacopo Panerati,Giovanni Pau*

Main category: cs.RO

TL;DR: 该论文提出了一种视觉自主无人机系统，在受控和非受控环境中均能匹敌专业人类飞行员的性能，并公开了相关飞行数据。


<details>
  <summary>Details</summary>
Motivation: 当前自主无人机系统在高度受控环境中表现出色，但在商业和野外作业中的直接应用仍有限，需要验证其在无仪器环境中的实际性能。

Method: 在受控环境中使用外部跟踪进行地面真值比较，同时在具有挑战性的无仪器环境中进行演示，评估系统性能。

Result: 该方法在受控和无仪器环境中均能匹配专业人类飞行员的性能表现。

Conclusion: 该视觉自主无人机系统在实际应用环境中具有可行性，为商业和野外作业提供了有价值的基准数据。

Abstract: Drone technology is proliferating in many industries, including agriculture,
logistics, defense, infrastructure, and environmental monitoring. Vision-based
autonomy is one of its key enablers, particularly for real-world applications.
This is essential for operating in novel, unstructured environments where
traditional navigation methods may be unavailable. Autonomous drone racing has
become the de facto benchmark for such systems. State-of-the-art research has
shown that autonomous systems can surpass human-level performance in racing
arenas. However, direct applicability to commercial and field operations is
still limited as current systems are often trained and evaluated in highly
controlled environments. In our contribution, the system's capabilities are
analyzed within a controlled environment -- where external tracking is
available for ground-truth comparison -- but also demonstrated in a
challenging, uninstrumented environment -- where ground-truth measurements were
never available. We show that our approach can match the performance of
professional human pilots in both scenarios. We also publicly release the data
from the flights carried out by our approach and a world-class human pilot.

</details>


### [29] [Hierarchical Discrete Lattice Assembly: An Approach for the Digital Fabrication of Scalable Macroscale Structures](https://arxiv.org/abs/2510.13686)
*Miana Smith,Paul Arthur Richard,Alexander Htet Kyaw,Neil Gershenfeld*

Main category: cs.RO

TL;DR: 提出了一种使用简单机器人和互锁晶格构建块来制造可扩展宏观结构的方法，通过体素化、分层块分组和移动机器人组装实现米级结构的自动建造。


<details>
  <summary>Details</summary>
Motivation: 当前面向大型结构的数字制造系统通常复杂、昂贵且不可靠，需要开发更简单可靠的大规模制造方法。

Method: 将目标结构体素化并分组为互连块，使用标准数字制造生产小块，然后通过移动机器人在结构上行走并放置新块来组装米级结构，配合数字孪生仿真工具进行控制协调。

Result: 验证了系统能够成功进行体素化、分层块分组、路径规划和机器人制造，展示了米级物体的制造能力。

Conclusion: 该方法通过结合小尺度复杂几何制造能力和简单机器人系统，实现了可靠且可扩展的大型结构制造。

Abstract: Although digital fabrication processes at the desktop scale have become
proficient and prolific, systems aimed at producing larger-scale structures are
still typically complex, expensive, and unreliable. In this work, we present an
approach for the fabrication of scalable macroscale structures using simple
robots and interlocking lattice building blocks. A target structure is first
voxelized so that it can be populated with an architected lattice. These voxels
are then grouped into larger interconnected blocks, which are produced using
standard digital fabrication processes, leveraging their capability to produce
highly complex geometries at a small scale. These blocks, on the size scale of
tens of centimeters, are then fed to mobile relative robots that are able to
traverse over the structure and place new blocks to form structures on the
meter scale. To facilitate the assembly of large structures, we introduce a
live digital twin simulation tool for controlling and coordinating assembly
robots that enables both global planning for a target structure and live user
design, interaction, or intervention. To improve assembly throughput, we
introduce a new modular assembly robot, designed for hierarchical voxel
handling. We validate this system by demonstrating the voxelization,
hierarchical blocking, path planning, and robotic fabrication of a set of
meter-scale objects.

</details>


### [30] [InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy](https://arxiv.org/abs/2510.13778)
*Xinyi Chen,Yilun Chen,Yanwei Fu,Ning Gao,Jiaya Jia,Weiyang Jin,Hao Li,Yao Mu,Jiangmiao Pang,Yu Qiao,Yang Tian,Bin Wang,Bolun Wang,Fangjing Wang,Hanqing Wang,Tai Wang,Ziqin Wang,Xueyuan Wei,Chao Wu,Shuai Yang,Jinhui Ye,Junqiu Yu,Jia Zeng,Jingjing Zhang,Jinyu Zhang,Shi Zhang,Feng Zheng,Bowen Zhou,Yangkun Zhu*

Main category: cs.RO

TL;DR: InternVLA-M1是一个用于空间定位和机器人控制的统一框架，通过空间引导的视觉-语言-动作训练，将空间定位作为指令和机器人动作之间的关键链接。


<details>
  <summary>Details</summary>
Motivation: 推动指令跟随机器人向可扩展、通用智能方向发展，解决当前机器人系统在空间推理和动作执行之间的脱节问题。

Method: 采用两阶段流水线：1) 在230万+空间推理数据上进行空间定位预训练，确定"在哪里行动"；2) 通过即插即用的空间提示进行空间引导的动作后训练，决定"如何行动"。

Result: 在多个基准测试中显著优于无空间引导的变体：SimplerEnv Google Robot提升14.6%，WidowX提升17%，LIBERO Franka提升4.3%。在真实世界集群拾放任务中提升7.3%，在未见物体和新配置上通过合成协同训练实现20.6%提升。

Conclusion: 空间引导训练是构建可扩展和鲁棒通用机器人的统一原则，在长时程推理密集型场景中超越现有工作超过10%。

Abstract: We introduce InternVLA-M1, a unified framework for spatial grounding and
robot control that advances instruction-following robots toward scalable,
general-purpose intelligence. Its core idea is spatially guided
vision-language-action training, where spatial grounding serves as the critical
link between instructions and robot actions. InternVLA-M1 employs a two-stage
pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning
data to determine ``where to act'' by aligning instructions with visual,
embodiment-agnostic positions, and (ii) spatially guided action post-training
to decide ``how to act'' by generating embodiment-aware actions through
plug-and-play spatial prompting. This spatially guided training recipe yields
consistent gains: InternVLA-M1 outperforms its variant without spatial guidance
by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO
Franka, while demonstrating stronger spatial reasoning capability in box,
point, and trace prediction. To further scale instruction following, we built a
simulation engine to collect 244K generalizable pick-and-place episodes,
enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In
real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with
synthetic co-training, achieved +20.6% on unseen objects and novel
configurations. Moreover, in long-horizon reasoning-intensive scenarios, it
surpassed existing works by over 10%. These results highlight spatially guided
training as a unifying principle for scalable and resilient generalist robots.
Code and models are available at
https://github.com/InternRobotics/InternVLA-M1.

</details>
