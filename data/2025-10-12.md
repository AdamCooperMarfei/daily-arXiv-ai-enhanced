<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 35]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [FLEET: Formal Language-Grounded Scheduling for Heterogeneous Robot Teams](https://arxiv.org/abs/2510.07417)
*Corban Rivera,Grayson Byrd,Meghan Booker,Bethany Kemp,Allison Gaines,Emma Holmes,James Uplinger,Celso M de Melo,David Handelman*

Main category: cs.RO

TL;DR: FLEET是一个混合去中心化框架，将自然语言指令转化为优化的多机器人调度方案，结合LLM前端和形式化后端解决异构机器人团队协调问题。


<details>
  <summary>Details</summary>
Motivation: 解决异构机器人团队从自由形式自然语言指令进行协调的挑战，语言规划器难以处理长期协调和幻觉问题，而纯形式化方法需要封闭世界模型。

Method: 使用LLM前端生成任务图和机器人-任务适应度矩阵，形式化后端解决最小化完工时间问题，机器人执行具有自主闭环控制的子任务。

Result: 在多个语言引导的自主协调基准测试中，FLEET在异构任务的两智能体团队上比最先进的生成规划器提高了成功率。消融实验显示MILP主要改进时间结构，LLM衍生的适应度对能力耦合任务至关重要。

Conclusion: FLEET框架成功将自然语言转化为优化的多机器人调度，在真实世界硬件试验中验证了其有效性。

Abstract: Coordinating heterogeneous robot teams from free-form natural-language
instructions is hard. Language-only planners struggle with long-horizon
coordination and hallucination, while purely formal methods require
closed-world models. We present FLEET, a hybrid decentralized framework that
turns language into optimized multi-robot schedules. An LLM front-end produces
(i) a task graph with durations and precedence and (ii) a capability-aware
robot--task fitness matrix; a formal back-end solves a makespan-minimization
problem while the underlying robots execute their free-form subtasks with
agentic closed-loop control. Across multiple free-form language-guided autonomy
coordination benchmarks, FLEET improves success over state of the art
generative planners on two-agent teams across heterogeneous tasks. Ablations
show that mixed integer linear programming (MILP) primarily improves temporal
structure, while LLM-derived fitness is decisive for capability-coupled tasks;
together they deliver the highest overall performance. We demonstrate the
translation to real world challenges with hardware trials using a pair of
quadruped robots with disjoint capabilities.

</details>


### [2] [VeMo: A Lightweight Data-Driven Approach to Model Vehicle Dynamics](https://arxiv.org/abs/2510.07447)
*Girolamo Oddo,Roberto Nuca,Matteo Parsani*

Main category: cs.RO

TL;DR: 提出基于门控循环单元的轻量级编码器-解码器模型，用于在信息稀缺条件下预测高性能车辆的未来状态，仅使用车载测量数据和驾驶员控制动作。


<details>
  <summary>Details</summary>
Motivation: 高性能车辆的动态建模通常需要详细的系统结构信息，但这些信息对非设计者往往不可得，这是自动驾驶应用中的典型问题。

Method: 使用门控循环单元构建轻量级编码器-解码器模型，通过车辆历史状态和驾驶员控制动作来预测未来状态，完全数据驱动且不受物理约束。

Result: 在极端动态条件下，模型的最大平均相对误差低于2.6%，对感兴趣频率成分的噪声输入数据表现出良好鲁棒性。

Conclusion: 该模型在输出信号（如纵向和横向加速度、横摆率和纵向速度）方面表现出物理一致性，为信息稀缺条件下的车辆建模提供了有效解决方案。

Abstract: Developing a dynamic model for a high-performance vehicle is a complex
problem that requires extensive structural information about the system under
analysis. This information is often unavailable to those who did not design the
vehicle and represents a typical issue in autonomous driving applications,
which are frequently developed on top of existing vehicles; therefore, vehicle
models are developed under conditions of information scarcity. This paper
proposes a lightweight encoder-decoder model based on Gate Recurrent Unit
layers to correlate the vehicle's future state with its past states, measured
onboard, and control actions the driver performs. The results demonstrate that
the model achieves a maximum mean relative error below 2.6% in extreme dynamic
conditions. It also shows good robustness when subject to noisy input data
across the interested frequency components. Furthermore, being entirely
data-driven and free from physical constraints, the model exhibits physical
consistency in the output signals, such as longitudinal and lateral
accelerations, yaw rate, and the vehicle's longitudinal velocity.

</details>


### [3] [HJCD-IK: GPU-Accelerated Inverse Kinematics through Batched Hybrid Jacobian Coordinate Descent](https://arxiv.org/abs/2510.07514)
*Cael Yasutake,Zachary Kingston,Brian Plancher*

Main category: cs.RO

TL;DR: HJCD-IK是一种GPU加速的采样式混合逆运动学求解器，结合了方向感知的贪婪坐标下降初始化方案和基于雅可比矩阵的优化程序，在精度和速度方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统解析求解器受限于低自由度和特定拓扑结构，而数值优化方法计算成本高且容易陷入局部极小值。需要结合采样和优化的GPU加速方法来提高IK求解器的准确性和速度。

Method: 提出HJCD-IK方法，使用方向感知的贪婪坐标下降进行初始化，然后通过雅可比矩阵优化程序进行精炼，充分利用GPU并行计算能力。

Result: 与现有最优方法相比，该方法在收敛速度和整体精度方面均有提升，在精度-延迟帕累托边界上始终找到解决方案，通常实现数量级的增益，并产生高质量样本的广泛分布。

Conclusion: HJCD-IK在逆运动学求解方面实现了显著的性能提升，代码已开源供社区使用。

Abstract: Inverse Kinematics (IK) is a core problem in robotics, in which joint
configurations are found to achieve a desired end-effector pose. Although
analytical solvers are fast and efficient, they are limited to systems with low
degrees-of-freedom and specific topological structures. Numerical
optimization-based approaches are more general, but suffer from high
computational costs and frequent convergence to spurious local minima. Recent
efforts have explored the use of GPUs to combine sampling and optimization to
enhance both the accuracy and speed of IK solvers. We build on this recent
literature and introduce HJCD-IK, a GPU-accelerated, sampling-based hybrid
solver that combines an orientation-aware greedy coordinate descent
initialization scheme with a Jacobian-based polishing routine. This design
enables our solver to improve both convergence speed and overall accuracy as
compared to the state-of-the-art, consistently finding solutions along the
accuracy-latency Pareto frontier and often achieving order-of-magnitude gains.
In addition, our method produces a broad distribution of high-quality samples,
yielding the lowest maximum mean discrepancy. We release our code open-source
for the benefit of the community.

</details>


### [4] [AVO: Amortized Value Optimization for Contact Mode Switching in Multi-Finger Manipulation](https://arxiv.org/abs/2510.07548)
*Adam Hung,Fan Yang,Abhinav Kumar,Sergio Aguilera Marinovic,Soshi Iba,Rana Soltani Zarrin,Dmitry Berenson*

Main category: cs.RO

TL;DR: 提出Amortized Value Optimization (AVO)方法，通过引入学习价值函数来预测未来任务性能，指导轨迹优化器选择有利于后续子任务的状态，从而解决灵巧操作中接触模式切换的优化问题。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作任务需要在不同接触模式间切换，传统方法将任务分解为独立子任务分别优化，这限制了性能且计算成本高昂。独立优化可能导致系统进入难以继续后续子任务的状态。

Method: AVO引入学习价值函数预测总未来任务性能，将该价值函数纳入轨迹优化的成本函数中，通过价值函数梯度指导优化器选择最小化未来子任务成本的状态。

Result: 在螺丝刀抓取和转动任务的仿真和真实实验中验证，即使计算预算减少50%，相比没有价值函数的轨迹优化仍能获得性能提升。

Conclusion: AVO有效桥接了独立优化的子任务，通过减少在线计算需求加速了优化过程，在灵巧操作任务中表现出优越性能。

Abstract: Dexterous manipulation tasks often require switching between different
contact modes, such as rolling, sliding, sticking, or non-contact contact
modes. When formulating dexterous manipulation tasks as a trajectory
optimization problem, a common approach is to decompose these tasks into
sub-tasks for each contact mode, which are each solved independently.
Optimizing each sub-task independently can limit performance, as optimizing
contact points, contact forces, or other variables without information about
future sub-tasks can place the system in a state from which it is challenging
to make progress on subsequent sub-tasks. Further, optimizing these sub-tasks
is very computationally expensive. To address these challenges, we propose
Amortized Value Optimization (AVO), which introduces a learned value function
that predicts the total future task performance. By incorporating this value
function into the cost of the trajectory optimization at each planning step,
the value function gradients guide the optimizer toward states that minimize
the cost in future sub-tasks. This effectively bridges separately optimized
sub-tasks, and accelerates the optimization by reducing the amount of online
computation needed. We validate AVO on a screwdriver grasping and turning task
in both simulation and real world experiments, and show improved performance
even with 50% less computational budget compared to trajectory optimization
without the value function.

</details>


### [5] [Inspection Planning Primitives with Implicit Models](https://arxiv.org/abs/2510.07611)
*Jingyang You,Hanna Kurniawati,Lashika Medagoda*

Main category: cs.RO

TL;DR: 提出了基于隐式模型的检查规划原语(IPIM)，使基于采样的检查规划器能够完全使用神经SDF表示，在保持轨迹质量的同时显著减少内存使用


<details>
  <summary>Details</summary>
Motivation: 基础设施老化复杂化使得高效检查规划至关重要。现有基于采样的检查规划器速度快但内存需求大，特别是对于大型复杂结构。虽然隐式模型能高效表示复杂结构，但现有规划原语主要针对显式模型设计

Method: 开发了IPIM原语计算集，使基于采样的检查规划器能够完全使用神经SDF表示进行规划，无需在隐式和显式模型间频繁转换

Result: 在三个场景下评估，包括超过9200万个三角网格面的复杂真实结构，即使使用基本采样规划器配合IPIM，也能生成与最先进规划器质量相当的检查轨迹，同时内存使用减少高达70倍

Conclusion: IPIM方法成功使基于采样的检查规划器能够有效利用隐式模型表示，在保持轨迹质量的同时大幅降低内存需求，为大型复杂基础设施检查提供了更高效的解决方案

Abstract: The aging and increasing complexity of infrastructures make efficient
inspection planning more critical in ensuring safety. Thanks to sampling-based
motion planning, many inspection planners are fast. However, they often require
huge memory. This is particularly true when the structure under inspection is
large and complex, consisting of many struts and pillars of various geometry
and sizes. Such structures can be represented efficiently using implicit
models, such as neural Signed Distance Functions (SDFs). However, most
primitive computations used in sampling-based inspection planner have been
designed to work efficiently with explicit environment models, which in turn
requires the planner to use explicit environment models or performs frequent
transformations between implicit and explicit environment models during
planning. This paper proposes a set of primitive computations, called
Inspection Planning Primitives with Implicit Models (IPIM), that enable
sampling-based inspection planners to entirely use neural SDFs representation
during planning. Evaluation on three scenarios, including inspection of a
complex real-world structure with over 92M triangular mesh faces, indicates
that even a rudimentary sampling-based planner with IPIM can generate
inspection trajectories of similar quality to those generated by the
state-of-the-art planner, while using up to 70x less memory than the
state-of-the-art inspection planner.

</details>


### [6] [GATO: GPU-Accelerated and Batched Trajectory Optimization for Scalable Edge Model Predictive Control](https://arxiv.org/abs/2510.07625)
*Alexander Du,Emre Adabag,Gabriel Bravo,Brian Plancher*

Main category: cs.RO

TL;DR: GATO是一个开源的GPU加速批量轨迹优化求解器，专为中等批量规模（数十到数百个求解）的实时MPC应用设计，在算法、软件和计算硬件层面协同优化，实现了18-21倍于CPU基准和1.4-16倍于GPU基准的加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有GPU加速方法要么并行化单个求解以满足实时截止时间，要么以慢于实时的速率扩展到非常大的批量，或者通过限制模型通用性来获得速度。这导致许多最先进的MPC应用在需要实时批量求解时存在性能差距。

Method: 采用算法、软件和计算硬件协同设计的方法，利用块级、warp级和线程级并行性在求解内部和求解之间实现超高性能。

Result: 模拟基准测试显示，随着批量大小增加，相比CPU基准加速18-21倍，相比GPU基准加速1.4-16倍；案例研究显示改进了扰动抑制和收敛行为；在工业机械臂上进行了硬件验证。

Conclusion: GATO填补了中等批量规模实时MPC应用的求解器性能空白，通过开源支持可复现性和采用。

Abstract: While Model Predictive Control (MPC) delivers strong performance across
robotics applications, solving the underlying (batches of) nonlinear trajectory
optimization (TO) problems online remains computationally demanding. Existing
GPU-accelerated approaches typically (i) parallelize a single solve to meet
real-time deadlines, (ii) scale to very large batches at slower-than-real-time
rates, or (iii) achieve speed by restricting model generality (e.g., point-mass
dynamics or a single linearization). This leaves a large gap in solver
performance for many state-of-the-art MPC applications that require real-time
batches of tens to low-hundreds of solves. As such, we present GATO, an open
source, GPU-accelerated, batched TO solver co-designed across algorithm,
software, and computational hardware to deliver real-time throughput for these
moderate batch size regimes. Our approach leverages a combination of block-,
warp-, and thread-level parallelism within and across solves for ultra-high
performance. We demonstrate the effectiveness of our approach through a
combination of: simulated benchmarks showing speedups of 18-21x over CPU
baselines and 1.4-16x over GPU baselines as batch size increases; case studies
highlighting improved disturbance rejection and convergence behavior; and
finally a validation on hardware using an industrial manipulator. We open
source GATO to support reproducibility and adoption.

</details>


### [7] [Differentiable Particle Optimization for Fast Sequential Manipulation](https://arxiv.org/abs/2510.07674)
*Lucas Chen,Shrutheesh Raman Iyer,Zachary Kingston*

Main category: cs.RO

TL;DR: SPaSM是一个完全GPU并行化的框架，通过编译约束评估、采样和梯度优化到CUDA内核，实现端到端轨迹优化，无需CPU协调，在具有挑战性的基准测试中实现毫秒级求解时间和100%成功率。


<details>
  <summary>Details</summary>
Motivation: 顺序机器人操作任务需要在可能的高维配置空间中找到满足多个对象交互几何约束的无碰撞轨迹。由于计算需求，在实时和大规模下解决这些问题一直难以实现。现有GPU加速方法由于CPU-GPU数据传输开销和复杂逻辑导致性能有限。

Method: SPaSM采用两阶段粒子优化策略：首先通过大规模并行采样解决放置约束，然后将解决方案提升到关节空间中的完整轨迹优化。与分层方法不同，SPaSM联合优化对象放置和机器人轨迹，以处理运动可行性约束放置选项的场景。

Result: 在具有挑战性的基准测试中，SPaSM实现了毫秒级的求解时间和100%的成功率，相比现有方法实现了4000倍的加速。

Conclusion: SPaSM通过完全GPU并行化和优化的CUDA内核实现了高效的顺序操作任务求解，显著提升了计算性能，为实时机器人操作提供了可行的解决方案。

Abstract: Sequential robot manipulation tasks require finding collision-free
trajectories that satisfy geometric constraints across multiple object
interactions in potentially high-dimensional configuration spaces. Solving
these problems in real-time and at large scales has remained out of reach due
to computational requirements. Recently, GPU-based acceleration has shown
promising results, but prior methods achieve limited performance due to CPU-GPU
data transfer overhead and complex logic that prevents full hardware
utilization. To this end, we present SPaSM (Sampling Particle optimization for
Sequential Manipulation), a fully GPU-parallelized framework that compiles
constraint evaluation, sampling, and gradient-based optimization into optimized
CUDA kernels for end-to-end trajectory optimization without CPU coordination.
The method consists of a two-stage particle optimization strategy: first
solving placement constraints through massively parallel sampling, then lifting
solutions to full trajectory optimization in joint space. Unlike hierarchical
approaches, SPaSM jointly optimizes object placements and robot trajectories to
handle scenarios where motion feasibility constrains placement options.
Experimental evaluation on challenging benchmarks demonstrates solution times
in the realm of $\textbf{milliseconds}$ with a 100% success rate; a
$4000\times$ speedup compared to existing approaches.

</details>


### [8] [EB-MBD: Emerging-Barrier Model-Based Diffusion for Safe Trajectory Optimization in Highly Constrained Environments](https://arxiv.org/abs/2510.07700)
*Raghav Mishra,Ian R. Manchester*

Main category: cs.RO

TL;DR: 提出EB-MBD方法，通过引入渐进式障碍函数约束来解决基于模型扩散中的性能退化问题，显著提升解质量并减少计算时间


<details>
  <summary>Details</summary>
Motivation: 基于模型的扩散方法在约束条件下会出现灾难性性能退化，即使在简单的2D系统中，由于蒙特卡洛近似得分函数的样本效率低下

Method: EB-MBD使用渐进引入的障碍约束来避免这些问题，无需计算昂贵的投影操作，并分析每次迭代的采样活跃度来指导障碍参数调度选择

Result: 在2D碰撞避免和3D水下机械臂系统中，该方法比基于模型的扩散获得更低成本的解，比基于投影的方法减少数量级的计算时间

Conclusion: 新兴障碍模型基于扩散方法能有效处理约束问题，在保持解质量的同时大幅降低计算成本

Abstract: We propose enforcing constraints on Model-Based Diffusion by introducing
emerging barrier functions inspired by interior point methods. We show that
constraints on Model-Based Diffusion can lead to catastrophic performance
degradation, even on simple 2D systems due to sample inefficiency in the Monte
Carlo approximation of the score function. We introduce Emerging-Barrier
Model-Based Diffusion (EB-MBD) which uses progressively introduced barrier
constraints to avoid these problems, significantly improving solution quality,
without the need for computationally expensive operations such as projections.
We analyze the sampling liveliness of samples each iteration to inform barrier
parameter scheduling choice. We demonstrate results for 2D collision avoidance
and a 3D underwater manipulator system and show that our method achieves lower
cost solutions than Model-Based Diffusion, and requires orders of magnitude
less computation time than projection based methods.

</details>


### [9] [Probabilistically-Safe Bipedal Navigation over Uncertain Terrain via Conformal Prediction and Contraction Analysis](https://arxiv.org/abs/2510.07725)
*Kasidit Muenprasitivej,Ye Zhao,Glen Chou*

Main category: cs.RO

TL;DR: 提出了一种用于双足机器人在粗糙地形上安全导航的概率安全规划与控制框架，结合高斯过程地形估计和保形预测，确保动态可行性和质心鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决双足机器人在不确定地形上安全导航的挑战，需要确保动态可行性和质心鲁棒性，同时考虑地形高程的不确定性。

Method: 采用高斯过程回归估计地形高程，利用保形预测构建校准的置信区间；提出基于收缩的可达管和飞轮扭矩控制律，确保状态收敛和管不变性。

Result: 通过MuJoCo中Digit双足机器人的物理仿真验证了规划框架的有效性，实现了概率安全性和目标可达性保证。

Conclusion: 该框架为双足机器人在不确定地形上的安全导航提供了概率安全保证和动态稳定性控制。

Abstract: We address the challenge of enabling bipedal robots to traverse rough terrain
by developing probabilistically safe planning and control strategies that
ensure dynamic feasibility and centroidal robustness under terrain uncertainty.
Specifically, we propose a high-level Model Predictive Control (MPC) navigation
framework for a bipedal robot with a specified confidence level of safety that
(i) enables safe traversal toward a desired goal location across a terrain map
with uncertain elevations, and (ii) formally incorporates uncertainty bounds
into the centroidal dynamics of locomotion control. To model the rough terrain,
we employ Gaussian Process (GP) regression to estimate elevation maps and
leverage Conformal Prediction (CP) to construct calibrated confidence intervals
that capture the true terrain elevation. Building on this, we formulate
contraction-based reachable tubes that explicitly account for terrain
uncertainty, ensuring state convergence and tube invariance. In addition, we
introduce a contraction-based flywheel torque control law for the reduced-order
Linear Inverted Pendulum Model (LIPM), which stabilizes the angular momentum
about the center-of-mass (CoM). This formulation provides both probabilistic
safety and goal reachability guarantees. For a given confidence level, we
establish the forward invariance of the proposed torque control law by
demonstrating exponential stabilization of the actual CoM phase-space
trajectory and the desired trajectory prescribed by the high-level planner.
Finally, we evaluate the effectiveness of our planning framework through
physics-based simulations of the Digit bipedal robot in MuJoCo.

</details>


### [10] [Injecting Hallucinations in Autonomous Vehicles: A Component-Agnostic Safety Evaluation Framework](https://arxiv.org/abs/2510.07749)
*Alexandre Moreira Nascimento,Gabriel Kenji Godoy Shimanuki,Lúcio Flavio Vismari,João Batista Camargo Jr,Jorge Rady de Almeida Jr,Paulo Sergio Cugnasca,Anna Carolina Muller Queiroz,Jeremy Noah Bailenson*

Main category: cs.RO

TL;DR: 本文提出了一种可配置、组件无关的幻觉注入框架，通过将感知故障重新定义为幻觉来研究自动驾驶车辆的安全性影响。


<details>
  <summary>Details</summary>
Motivation: 现有的故障注入研究通常针对单一传感器或感知模块，导致难以推广或集成到统一仿真环境中的孤立框架。

Method: 构建了一个可配置的组件无关幻觉注入框架，在开源模拟器中诱导六种合理的幻觉类型，并执行了18,350多次模拟。

Result: 某些幻觉（如感知延迟和漂移）显著增加了测试场景中的碰撞风险，验证了该框架能够有效测试AV系统安全性。

Conclusion: 该框架提供了一个可扩展、统计验证、组件无关且完全互操作的工具集，可简化和加速AV安全验证，并为未来容错和弹性AV设计研究奠定基础。

Abstract: Perception failures in autonomous vehicles (AV) remain a major safety concern
because they are the basis for many accidents. To study how these failures
affect safety, researchers typically inject artificial faults into hardware or
software components and observe the outcomes. However, existing fault injection
studies often target a single sensor or machine perception (MP) module,
resulting in siloed frameworks that are difficult to generalize or integrate
into unified simulation environments. This work addresses that limitation by
reframing perception failures as hallucinations, false perceptions that distort
an AV situational awareness and may trigger unsafe control actions. Since
hallucinations describe only observable effects, this abstraction enables
analysis independent of specific sensors or algorithms, focusing instead on how
their faults manifest along the MP pipeline. Building on this concept, we
propose a configurable, component-agnostic hallucination injection framework
that induces six plausible hallucination types in an iterative open-source
simulator. More than 18,350 simulations were executed in which hallucinations
were injected while AVs crossed an unsignalized transverse street with traffic.
The results statistically validate the framework and quantify the impact of
each hallucination type on collisions and near misses. Certain hallucinations,
such as perceptual latency and drift, significantly increase the risk of
collision in the scenario tested, validating the proposed paradigm can stress
the AV system safety. The framework offers a scalable, statistically validated,
component agnostic, and fully interoperable toolset that simplifies and
accelerates AV safety validations, even those with novel MP architectures and
components. It can potentially reduce the time-to-market of AV and lay the
foundation for future research on fault tolerance, and resilient AV design.

</details>


### [11] [Trajectory Conditioned Cross-embodiment Skill Transfer](https://arxiv.org/abs/2510.07773)
*YuHang Tang,Yixuan Lou,Pengfei Han,Haoming Song,Xinyi Ye,Dong Wang,Bin Zhao*

Main category: cs.RO

TL;DR: TrajSkill是一个从人类演示视频中学习机器人操作技能的框架，通过将人体运动表示为稀疏光流轨迹来实现跨形态技能迁移，无需配对数据集或手动设计奖励。


<details>
  <summary>Details</summary>
Motivation: 解决从人类演示视频学习机器人操作技能时的形态差异问题，现有方法依赖配对数据集或手动设计奖励，限制了可扩展性和泛化能力。

Method: 将人体运动表示为稀疏光流轨迹作为形态无关的运动线索，结合视觉和文本输入，联合合成时间一致的机器人操作视频并转换为可执行动作。

Result: 在MetaWorld模拟数据上，相比最先进方法，FVD降低39.6%，KVD降低36.6%，跨形态成功率提升高达16.7%。真实机器人厨房操作实验验证了方法的有效性。

Conclusion: TrajSkill能够实现从人类演示到机器人的跨形态技能迁移，在模拟和真实环境中都表现出色，为机器人学习人类操作技能提供了实用解决方案。

Abstract: Learning manipulation skills from human demonstration videos presents a
promising yet challenging problem, primarily due to the significant embodiment
gap between human body and robot manipulators. Existing methods rely on paired
datasets or hand-crafted rewards, which limit scalability and generalization.
We propose TrajSkill, a framework for Trajectory Conditioned Cross-embodiment
Skill Transfer, enabling robots to acquire manipulation skills directly from
human demonstration videos. Our key insight is to represent human motions as
sparse optical flow trajectories, which serve as embodiment-agnostic motion
cues by removing morphological variations while preserving essential dynamics.
Conditioned on these trajectories together with visual and textual inputs,
TrajSkill jointly synthesizes temporally consistent robot manipulation videos
and translates them into executable actions, thereby achieving cross-embodiment
skill transfer. Extensive experiments are conducted, and the results on
simulation data (MetaWorld) show that TrajSkill reduces FVD by 39.6\% and KVD
by 36.6\% compared with the state-of-the-art, and improves cross-embodiment
success rate by up to 16.7\%. Real-robot experiments in kitchen manipulation
tasks further validate the effectiveness of our approach, demonstrating
practical human-to-robot skill transfer across embodiments.

</details>


### [12] [IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction](https://arxiv.org/abs/2510.07778)
*Yandu Chen,Kefan Gu,Yuqing Wen,Yucheng Zhao,Tiancai Wang,Liqiang Nie*

Main category: cs.RO

TL;DR: IntentionVLA是一个视觉-语言-动作模型框架，通过课程训练范式赋予模型推理能力，在间接指令下实现快速推理和动作生成，显著提升了人机交互的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的VLA模型主要在有限的多模态任务上预训练，然后微调以将显式指令映射到动作，缺乏推理密集型预训练和推理引导的操作，无法执行复杂真实世界交互所需的隐式人类意图推理。

Method: 首先使用精心设计的推理数据进行训练，结合意图推理、空间定位和紧凑的具身推理，然后在微调阶段使用紧凑推理输出作为动作生成的上下文指导，实现间接指令下的快速推理。

Result: IntentionVLA显著优于π0，在直接指令下成功率提高18%，在意图指令下比ECoT提高28%。在分布外意图任务上，成功率是基线的两倍以上，并实现了40%成功率的零样本人机交互。

Conclusion: IntentionVLA为下一代人机交互系统提供了一个有前景的范式，通过赋予模型推理能力显著提升了复杂交互任务的表现。

Abstract: Vision-Language-Action (VLA) models leverage pretrained vision-language
models (VLMs) to couple perception with robotic control, offering a promising
path toward general-purpose embodied intelligence. However, current SOTA VLAs
are primarily pretrained on multimodal tasks with limited relevance to embodied
scenarios, and then finetuned to map explicit instructions to actions.
Consequently, due to the lack of reasoning-intensive pretraining and
reasoning-guided manipulation, these models are unable to perform implicit
human intention reasoning required for complex, real-world interactions. To
overcome these limitations, we propose \textbf{IntentionVLA}, a VLA framework
with a curriculum training paradigm and an efficient inference mechanism. Our
proposed method first leverages carefully designed reasoning data that combine
intention inference, spatial grounding, and compact embodied reasoning,
endowing the model with both reasoning and perception capabilities. In the
following finetuning stage, IntentionVLA employs the compact reasoning outputs
as contextual guidance for action generation, enabling fast inference under
indirect instructions. Experimental results show that IntentionVLA
substantially outperforms $\pi_0$, achieving 18\% higher success rates with
direct instructions and 28\% higher than ECoT under intention instructions. On
out-of-distribution intention tasks, IntentionVLA achieves over twice the
success rate of all baselines, and further enables zero-shot human-robot
interaction with 40\% success rate. These results highlight IntentionVLA as a
promising paradigm for next-generation human-robot interaction (HRI) systems.

</details>


### [13] [GM3: A General Physical Model for Micro-Mobility Vehicles](https://arxiv.org/abs/2510.07807)
*Grace Cai,Nithin Parepally,Laura Zheng,Ming C. Lin*

Main category: cs.RO

TL;DR: 提出了GM3（广义微移动模型），这是一个基于轮胎刷表示的物理模型，能够统一建模各种微移动车辆的动力学特性，包括轮胎滑移、载荷转移和骑手/车辆倾斜等效应。


<details>
  <summary>Details</summary>
Motivation: 现有的微移动车辆动力学建模工具主要依赖运动学自行车模型或其变体，忽略了轮胎滑移、载荷转移和骑手/车辆倾斜等重要物理效应，且缺乏统一的跨平台模型。

Method: 开发了基于轮胎刷表示的GM3模型，支持任意车轮配置；构建了交互式模型无关仿真框架，包含固定步长RK4积分、人在环控制、实时轨迹跟踪和分析功能。

Result: 在斯坦福无人机数据集的环形交叉路口场景中，对骑行者、滑板者和手推车类别的GM3模型进行了实证验证。

Conclusion: GM3提供了一个统一的物理基础模型，能够准确捕捉各种微移动车辆的动力学特性，弥补了现有模型的不足。

Abstract: Modeling the dynamics of micro-mobility vehicles (MMV) is becoming
increasingly important for training autonomous vehicle systems and building
urban traffic simulations. However, mainstream tools rely on variants of the
Kinematic Bicycle Model (KBM) or mode-specific physics that miss tire slip,
load transfer, and rider/vehicle lean. To our knowledge, no unified,
physics-based model captures these dynamics across the full range of common
MMVs and wheel layouts. We propose the "Generalized Micro-mobility Model"
(GM3), a tire-level formulation based on the tire brush representation that
supports arbitrary wheel configurations, including single/double track and
multi-wheel platforms. We introduce an interactive model-agnostic simulation
framework that decouples vehicle/layout specification from dynamics to compare
the GM3 with the KBM and other models, consisting of fixed step RK4
integration, human-in-the-loop and scripted control, real-time trajectory
traces and logging for analysis. We also empirically validate the GM3 on the
Stanford Drone Dataset's deathCircle (roundabout) scene for biker, skater, and
cart classes.

</details>


### [14] [DM1: MeanFlow with Dispersive Regularization for 1-Step Robotic Manipulation](https://arxiv.org/abs/2510.07865)
*Guowei Zou,Haitao Wang,Hejun Wu,Yukun Qian,Yuhang Wang,Weibing Li*

Main category: cs.RO

TL;DR: DM1提出了一种结合分散正则化的流匹配框架，解决了基于流的策略中的表示崩溃问题，在保持一步生成效率的同时显著提升了机器人操作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 基于流的生成模型在机器人操作中具有采样效率高的优势，但存在表示崩溃问题，无法区分相似的视觉表示，导致精确操作任务失败。

Method: DM1将分散正则化集成到MeanFlow中，在不同中间嵌入层使用多种分散正则化变体，鼓励训练批次中的多样化表示，无需额外网络模块或专门训练过程。

Result: 在RoboMimic基准测试中，DM1实现了20-40倍的推理加速（0.07s vs. 2-3.5s），成功率提高10-20个百分点，Lift任务达到99%成功率（基线为85%）。真实机器人部署验证了从仿真到物理世界的有效迁移。

Conclusion: 这是首个利用表示正则化使基于流的策略在机器人操作中实现强大性能的工作，为高效稳健的操作提供了一种简单而强大的方法。

Abstract: The ability to learn multi-modal action distributions is indispensable for
robotic manipulation policies to perform precise and robust control. Flow-based
generative models have recently emerged as a promising solution to learning
distributions of actions, offering one-step action generation and thus
achieving much higher sampling efficiency compared to diffusion-based methods.
However, existing flow-based policies suffer from representation collapse, the
inability to distinguish similar visual representations, leading to failures in
precise manipulation tasks. We propose DM1 (MeanFlow with Dispersive
Regularization for One-Step Robotic Manipulation), a novel flow matching
framework that integrates dispersive regularization into MeanFlow to prevent
collapse while maintaining one-step efficiency. DM1 employs multiple dispersive
regularization variants across different intermediate embedding layers,
encouraging diverse representations across training batches without introducing
additional network modules or specialized training procedures. Experiments on
RoboMimic benchmarks show that DM1 achieves 20-40 times faster inference (0.07s
vs. 2-3.5s) and improves success rates by 10-20 percentage points, with the
Lift task reaching 99% success over 85% of the baseline. Real-robot deployment
on a Franka Panda further validates that DM1 transfers effectively from
simulation to the physical world. To the best of our knowledge, this is the
first work to leverage representation regularization to enable flow-based
policies to achieve strong performance in robotic manipulation, establishing a
simple yet powerful approach for efficient and robust manipulation.

</details>


### [15] [USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots](https://arxiv.org/abs/2510.07869)
*Junwen Gu,Zhiheng wu,Pengxuan Si,Shuang Qiu,Yukai Feng,Luoyang Sun,Laien Luo,Lianyi Yu,Jian Wang,Zhengxing Wu*

Main category: cs.RO

TL;DR: 提出了USIM模拟数据集和U0模型，用于解决水下机器人多任务自主操作的挑战，通过视觉-语言-动作模型在复杂水下环境中实现多种任务的自主执行。


<details>
  <summary>Details</summary>
Motivation: 水下环境对机器人操作具有独特挑战，如复杂流体动力学、有限能见度和受限通信。现有数据驱动方法缺乏大规模高质量水下数据集，难以开发能够自主执行多任务的水下智能系统。

Method: 构建USIM模拟数据集（包含56.1万帧图像、1852条轨迹），并提出U0视觉-语言-动作模型，集成双目视觉和其他传感器模态，采用卷积注意力感知聚焦增强模块来提升空间理解和移动操作能力。

Result: 在检查、避障、扫描和动态跟踪等任务中达到80%成功率；在移动操作任务中，相比基线方法将目标距离减少21.2%。

Conclusion: VLA模型可有效应用于水下机器人，为可扩展数据集构建、改进任务自主性和实现智能通用水下机器人提供了基础。

Abstract: Underwater environments present unique challenges for robotic operation,
including complex hydrodynamics, limited visibility, and constrained
communication. Although data-driven approaches have advanced embodied
intelligence in terrestrial robots and enabled task-specific autonomous
underwater robots, developing underwater intelligence capable of autonomously
performing multiple tasks remains highly challenging, as large-scale,
high-quality underwater datasets are still scarce. To address these
limitations, we introduce USIM, a simulation-based multi-task
Vision-Language-Action (VLA) dataset for underwater robots. USIM comprises over
561K frames from 1,852 trajectories, totaling approximately 15.6 hours of
BlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging from
visual navigation to mobile manipulation. Building upon this dataset, we
propose U0, a VLA model for general underwater robots, which integrates
binocular vision and other sensor modalities through multimodal fusion, and
further incorporates a convolution-attention-based perception focus enhancement
module (CAP) to improve spatial understanding and mobile manipulation. Across
tasks such as inspection, obstacle avoidance, scanning, and dynamic tracking,
the framework achieves a success rate of 80%, while in challenging mobile
manipulation tasks, it reduces the distance to the target by 21.2% compared
with baseline methods, demonstrating its effectiveness. USIM and U0 show that
VLA models can be effectively applied to underwater robotic applications,
providing a foundation for scalable dataset construction, improved task
autonomy, and the practical realization of intelligent general underwater
robots.

</details>


### [16] [Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception -- Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track](https://arxiv.org/abs/2510.07871)
*Erjia Xiao,Lingfeng Zhang,Yingbo Tang,Hao Cheng,Renjing Xu,Wenbo Ding,Lei Zhou,Long Chen,Hangjun Ye,Xiaoshuai Hao*

Main category: cs.RO

TL;DR: 本文提出了一种基于Falcon模型的主动风险感知模块，用于提升机器人在动态人群环境中的社交导航性能，在IROS 2025 RoboSense挑战赛中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 解决自主机器人在动态人群室内环境中安全、高效且符合社交规范的导航问题，特别是在仅使用机载传感器且无全局地图的情况下。

Method: 在Falcon模型基础上引入主动风险感知模块，通过学习预测周围人类基于距离的碰撞风险分数，增强空间感知和主动避碰能力。

Result: 在Social-HM3D基准测试中，该方法提高了机器人在拥挤室内场景中保持个人空间合规性的能力，在16支参赛队伍中获得第二名。

Conclusion: 主动风险感知模块有效提升了社交导航性能，使机器人能够在动态人类环境中更安全地导航。

Abstract: In this report, we describe the technical details of our submission to the
IROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on
developing RGBD-based perception and navigation systems that enable autonomous
agents to navigate safely, efficiently, and socially compliantly in dynamic
human-populated indoor environments. The challenge requires agents to operate
from an egocentric perspective using only onboard sensors including RGB-D
observations and odometry, without access to global maps or privileged
information, while maintaining social norm compliance such as safe distances
and collision avoidance. Building upon the Falcon model, we introduce a
Proactive Risk Perception Module to enhance social navigation performance. Our
approach augments Falcon with collision risk understanding that learns to
predict distance-based collision risk scores for surrounding humans, which
enables the agent to develop more robust spatial awareness and proactive
collision avoidance behaviors. The evaluation on the Social-HM3D benchmark
demonstrates that our method improves the agent's ability to maintain personal
space compliance while navigating toward goals in crowded indoor scenes with
dynamic human agents, achieving 2nd place among 16 participating teams in the
challenge.

</details>


### [17] [Towards Proprioception-Aware Embodied Planning for Dual-Arm Humanoid Robots](https://arxiv.org/abs/2510.07882)
*Boyu Li,Siyuan He,Hang Xu,Haoqi Yuan,Yu Zang,Liwei Hu,Junpeng Yue,Zhenxiong Jiang,Pengbo Hu,Börje F. Karlsson,Yehui Tang,Zongqing Lu*

Main category: cs.RO

TL;DR: 提出了DualTHOR双臂人形机器人模拟器和Proprio-MLLM模型，通过整合本体感知信息解决MLLM在长视野双臂任务中的局限性，性能提升19.75%。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型作为高级规划器在双臂人形机器人长视野任务中效果有限，主要由于缺乏专用模拟平台和模型缺乏本体感知能力。

Method: 开发DualTHOR双臂人形模拟器，提出Proprio-MLLM模型，整合本体感知信息，采用基于运动的位置嵌入和跨空间编码器。

Result: 在现有MLLM表现不佳的环境中，Proprio-MLLM在规划性能上平均提升19.75%。

Conclusion: 该工作为推进人形机器人的具身智能提供了必要的模拟平台和有效模型。

Abstract: In recent years, Multimodal Large Language Models (MLLMs) have demonstrated
the ability to serve as high-level planners, enabling robots to follow complex
human instructions. However, their effectiveness, especially in long-horizon
tasks involving dual-arm humanoid robots, remains limited. This limitation
arises from two main challenges: (i) the absence of simulation platforms that
systematically support task evaluation and data collection for humanoid robots,
and (ii) the insufficient embodiment awareness of current MLLMs, which hinders
reasoning about dual-arm selection logic and body positions during planning. To
address these issues, we present DualTHOR, a new dual-arm humanoid simulator,
with continuous transition and a contingency mechanism. Building on this
platform, we propose Proprio-MLLM, a model that enhances embodiment awareness
by incorporating proprioceptive information with motion-based position
embedding and a cross-spatial encoder. Experiments show that, while existing
MLLMs struggle in this environment, Proprio-MLLM achieves an average
improvement of 19.75% in planning performance. Our work provides both an
essential simulation platform and an effective model to advance embodied
intelligence in humanoid robotics. The code is available at
https://anonymous.4open.science/r/DualTHOR-5F3B.

</details>


### [18] [Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation](https://arxiv.org/abs/2510.07975)
*Mingyang Sun,Jiude Wei,Qichen He,Donglin Wang,Cewu Lu,Jianhua Sun*

Main category: cs.RO

TL;DR: GRACE框架通过可执行分析概念(EAC)弥合视觉语言模型语义推理与机器人物理执行之间的差距，实现精确和通用的机器人操作。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在语义推理和任务规划方面的能力与机器人实际物理执行之间的"语义到物理"差距。

Method: 引入可执行分析概念(EAC)作为数学定义的蓝图，编码物体可供性、几何约束和操作语义，通过结构化策略脚手架管道将自然语言指令和视觉信息转化为实例化的EAC。

Result: 在模拟和真实环境中对多种铰接物体实现了强大的零样本泛化能力，无需任务特定训练。

Conclusion: GRACE提供了一个统一且可解释的接口，通过语义-物理接地有效实现了精确和可泛化的机器人操作。

Abstract: Enabling robots to perform precise and generalized manipulation in
unstructured environments remains a fundamental challenge in embodied AI. While
Vision-Language Models (VLMs) have demonstrated remarkable capabilities in
semantic reasoning and task planning, a significant gap persists between their
high-level understanding and the precise physical execution required for
real-world manipulation. To bridge this "semantic-to-physical" gap, we
introduce GRACE, a novel framework that grounds VLM-based reasoning through
executable analytic concepts (EAC)-mathematically defined blueprints that
encode object affordances, geometric constraints, and semantics of
manipulation. Our approach integrates a structured policy scaffolding pipeline
that turn natural language instructions and visual information into an
instantiated EAC, from which we derive grasp poses, force directions and plan
physically feasible motion trajectory for robot execution. GRACE thus provides
a unified and interpretable interface between high-level instruction
understanding and low-level robot control, effectively enabling precise and
generalizable manipulation through semantic-physical grounding. Extensive
experiments demonstrate that GRACE achieves strong zero-shot generalization
across a variety of articulated objects in both simulated and real-world
environments, without requiring task-specific training.

</details>


### [19] [Orientation Learning and Adaptation towards Simultaneous Incorporation of Multiple Local Constraints](https://arxiv.org/abs/2510.07986)
*Gaofeng Li,Peisen Xu,Ruize Wang,Qi Ye,Jiming Chen,Dezhen Song,Yanlong Huang*

Main category: cs.RO

TL;DR: 提出基于角度-轴空间的方向表示方法，通过加权平均机制在SO(3)流形上融合多个轨迹，同时处理多个局部约束，解决非欧几何带来的扭曲问题。


<details>
  <summary>Details</summary>
Motivation: 旋转群SO(3)是黎曼流形，其非欧几何特性导致局部约束难以整合，特别是同时处理多个局部约束时存在困难。

Method: 使用角度-轴表示法，在不同基点考虑不同局部约束生成多个轨迹，然后通过提出的加权平均机制融合这些轨迹。

Result: 仿真和实验验证表明，该方法能适应任意期望路径点的方向，处理角加速度约束，同时整合多个局部约束获得额外收益（如更小的加速度成本）。

Conclusion: 所提方法能解决扭曲问题，使现成的欧几里得学习算法在非欧空间中重新适用，有效整合多个局部约束。

Abstract: Orientation learning plays a pivotal role in many tasks. However, the
rotation group SO(3) is a Riemannian manifold. As a result, the distortion
caused by non-Euclidean geometric nature introduces difficulties to the
incorporation of local constraints, especially for the simultaneous
incorporation of multiple local constraints. To address this issue, we propose
the Angle-Axis Space-based orientation representation method to solve several
orientation learning problems, including orientation adaptation and
minimization of angular acceleration. Specifically, we propose a weighted
average mechanism in SO(3) based on the angle-axis representation method. Our
main idea is to generate multiple trajectories by considering different local
constraints at different basepoints. Then these multiple trajectories are fused
to generate a smooth trajectory by our proposed weighted average mechanism,
achieving the goal to incorporate multiple local constraints simultaneously.
Compared with existing solution, ours can address the distortion issue and make
the off-theshelf Euclidean learning algorithm be re-applicable in non-Euclidean
space. Simulation and Experimental evaluations validate that our solution can
not only adapt orientations towards arbitrary desired via-points and cope with
angular acceleration constraints, but also incorporate multiple local
constraints simultaneously to achieve extra benefits, e.g., achieving smaller
acceleration costs.

</details>


### [20] [FastUMI-100K: Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset](https://arxiv.org/abs/2510.08022)
*Kehui Liu,Zhongjie Jia,Yang Li,Zhaxizhuoma,Pengan Chen,Song Liu,Xin Liu,Pingrui Zhang,Haoming Song,Xinyi Ye,Nieqing Cao,Zhigang Wang,Jia Zeng,Dong Wang,Yan Ding,Bin Zhao,Xuelong Li*

Main category: cs.RO

TL;DR: 提出了FastUMI-100K数据集，这是一个大规模UMI风格的多模态演示数据集，包含超过10万条轨迹，涵盖54个任务和数百种物体类型，用于解决机器人操作学习中的数据限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人操作学习数据集主要依赖人类遥操作收集，存在可扩展性差、轨迹不平滑以及在不同机器人实体间适用性有限的问题。需要更高质量、更大规模的数据集来满足复杂现实世界操作任务的需求。

Method: 使用FastUMI机器人系统进行数据收集，该系统采用模块化、硬件解耦的机械设计和集成轻量级跟踪系统。数据集包含多模态数据流，包括末端执行器状态、多视角腕部鱼眼图像和文本注释。

Result: 实验结果表明，FastUMI-100K能够在各种基线算法上实现高策略成功率，验证了其鲁棒性、适应性和在复杂动态操作挑战中的实际应用价值。

Conclusion: FastUMI-100K提供了一个更可扩展、灵活和适应性强的解决方案，能够满足现实世界机器人演示数据的多样化需求，为解决复杂操作问题提供了高质量的数据基础。

Abstract: Data-driven robotic manipulation learning depends on large-scale,
high-quality expert demonstration datasets. However, existing datasets, which
primarily rely on human teleoperated robot collection, are limited in terms of
scalability, trajectory smoothness, and applicability across different robotic
embodiments in real-world environments. In this paper, we present FastUMI-100K,
a large-scale UMI-style multimodal demonstration dataset, designed to overcome
these limitations and meet the growing complexity of real-world manipulation
tasks. Collected by FastUMI, a novel robotic system featuring a modular,
hardware-decoupled mechanical design and an integrated lightweight tracking
system, FastUMI-100K offers a more scalable, flexible, and adaptable solution
to fulfill the diverse requirements of real-world robot demonstration data.
Specifically, FastUMI-100K contains over 100K+ demonstration trajectories
collected across representative household environments, covering 54 tasks and
hundreds of object types. Our dataset integrates multimodal streams, including
end-effector states, multi-view wrist-mounted fisheye images and textual
annotations. Each trajectory has a length ranging from 120 to 500 frames.
Experimental results demonstrate that FastUMI-100K enables high policy success
rates across various baseline algorithms, confirming its robustness,
adaptability, and real-world applicability for solving complex, dynamic
manipulation challenges. The source code and dataset will be released in this
link https://github.com/MrKeee/FastUMI-100K.

</details>


### [21] [Towards Reliable LLM-based Robot Planning via Combined Uncertainty Estimation](https://arxiv.org/abs/2510.08044)
*Shiyuan Yin,Chenjia Bai,Zihao Zhang,Junwei Jin,Xinxin Zhang,Chi Zhang,Xuelong Li*

Main category: cs.RO

TL;DR: 提出了CURE方法，通过分解认知不确定性和内在不确定性来改进LLM在机器人规划中的可靠性，使用随机网络蒸馏和MLP回归头进行不确定性评估。


<details>
  <summary>Details</summary>
Motivation: LLM在机器人规划中存在幻觉问题，导致过度自信但可能不准确或不安全的计划。现有研究未能充分区分认知和内在不确定性，限制了不确定性估计的有效性。

Method: 将不确定性分解为认知不确定性和内在不确定性，其中认知不确定性进一步细分为任务清晰度和任务熟悉度。使用随机网络蒸馏和多层感知机回归头基于LLM特征进行整体不确定性评估。

Result: 在厨房操作和桌面重排实验中验证，相比现有方法，该方法的不确定性估计与实际执行结果更加一致。

Conclusion: CURE方法通过分解不同类型的不确定性，显著提高了LLM在机器人规划中的可靠性，不确定性估计与实际执行结果更加匹配。

Abstract: Large language models (LLMs) demonstrate advanced reasoning abilities,
enabling robots to understand natural language instructions and generate
high-level plans with appropriate grounding. However, LLM hallucinations
present a significant challenge, often leading to overconfident yet potentially
misaligned or unsafe plans. While researchers have explored uncertainty
estimation to improve the reliability of LLM-based planning, existing studies
have not sufficiently differentiated between epistemic and intrinsic
uncertainty, limiting the effectiveness of uncertainty estimation. In this
paper, we present Combined Uncertainty estimation for Reliable Embodied
planning (CURE), which decomposes the uncertainty into epistemic and intrinsic
uncertainty, each estimated separately. Furthermore, epistemic uncertainty is
subdivided into task clarity and task familiarity for more accurate evaluation.
The overall uncertainty assessments are obtained using random network
distillation and multi-layer perceptron regression heads driven by LLM
features. We validated our approach in two distinct experimental settings:
kitchen manipulation and tabletop rearrangement experiments. The results show
that, compared to existing methods, our approach yields uncertainty estimates
that are more closely aligned with the actual execution outcomes.

</details>


### [22] [Beyond hospital reach: Autonomous lightweight ultrasound robot for liver sonography](https://arxiv.org/abs/2510.08106)
*Zihan Li,Yixiao Xu,Lei Zhang,Taiyu Han,Xinshan Yang,Yingni Wang,Mingxuan Liu,Shenghai Xin,Linxun Liu,Hongen Liao,Guochen Ning*

Main category: cs.RO

TL;DR: 开发了一个轻量级自主超声机器人系统，能够在资源有限地区自主获取肝脏超声标准平面并检测病理，包括在高海拔城市和野外环境中有效工作。


<details>
  <summary>Details</summary>
Motivation: 肝脏疾病是全球重大健康负担，超声是首选诊断工具，但肝脏超声需要从目标结构不可见的位置定位多个非连续平面，需要专业知识。然而，在资源有限地区，专业超声医师严重短缺。

Method: 开发了一个自主轻量级超声机器人系统，包含集成多模态感知与记忆注意力的AI代理用于定位未见目标结构，以及一个588克6自由度电缆驱动机器人。通过安装在腹部增强运动鲁棒性。

Result: 机器人能够自主获取专家级标准肝脏超声平面，并在患者中检测病理，包括在海拔2261米的西宁等医疗资源有限地区。系统在快速运动个体和野外环境中表现有效。

Conclusion: 这是首个在多个挑战性场景中展示自主超声的工作，有望改变服务不足地区获得专家级诊断的机会。

Abstract: Liver disease is a major global health burden. While ultrasound is the
first-line diagnostic tool, liver sonography requires locating multiple
non-continuous planes from positions where target structures are often not
visible, for biometric assessment and lesion detection, requiring significant
expertise. However, expert sonographers are severely scarce in resource-limited
regions. Here, we develop an autonomous lightweight ultrasound robot comprising
an AI agent that integrates multi-modal perception with memory attention for
localization of unseen target structures, and a 588-gram 6-degrees-of-freedom
cable-driven robot. By mounting on the abdomen, the system enhances robustness
against motion. Our robot can autonomously acquire expert-level standard liver
ultrasound planes and detect pathology in patients, including two from Xining,
a 2261-meter-altitude city with limited medical resources. Our system performs
effectively on rapid-motion individuals and in wilderness environments. This
work represents the first demonstration of autonomous sonography across
multiple challenging scenarios, potentially transforming access to expert-level
diagnostics in underserved regions.

</details>


### [23] [Accurate and Noise-Tolerant Extraction of Routine Logs in Robotic Process Automation (Extended Version)](https://arxiv.org/abs/2510.08118)
*Massimiliano de Leoni,Faizan Ahmed Khan,Simone Agostinelli*

Main category: cs.RO

TL;DR: 提出基于聚类的技术从UI日志中提取常规日志，用于机器人流程自动化，在噪声环境下表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有工作大多不直接关注模型发现，仅提取常规动作集合，且未在包含噪声（人类执行的自然变异性）的场景中评估

Method: 采用基于聚类的技术从UI日志中提取常规日志，在九个不同噪声水平的UI日志上进行实验

Result: 与现有技术相比，能够提取更准确的常规日志，特别是在存在噪声的情况下表现更优

Conclusion: 该方法在噪声环境下能够有效提取常规日志，为机器人流程自动化提供更好的支持

Abstract: Robotic Process Mining focuses on the identification of the routine types
performed by human resources through a User Interface. The ultimate goal is to
discover routine-type models to enable robotic process automation. The
discovery of routine-type models requires the provision of a routine log.
Unfortunately, the vast majority of existing works do not directly focus on
enabling the model discovery, limiting themselves to extracting the set of
actions that are part of the routines. They were also not evaluated in
scenarios characterized by inconsistent routine execution, hereafter referred
to as noise, which reflects natural variability and occasional errors in human
performance. This paper presents a clustering-based technique that aims to
extract routine logs. Experiments were conducted on nine UI logs from the
literature with different levels of injected noise. Our technique was compared
with existing techniques, most of which are not meant to discover routine logs
but were adapted for the purpose. The results were evaluated through standard
state-of-the-art metrics, showing that we can extract more accurate routine
logs than what the state of the art could, especially in the presence of noise.

</details>


### [24] [NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions](https://arxiv.org/abs/2510.08173)
*Haolin Yang,Yuxing Long,Zhuoyuan Yu,Zihan Yang,Minghan Wang,Jiapeng Xu,Yihan Wang,Ziyan Yu,Wenzhe Cai,Lei Kang,Hao Dong*

Main category: cs.RO

TL;DR: 提出了NavSpace基准测试，包含6个任务类别和1,228个轨迹-指令对，用于评估导航代理的空间智能。评估了22个导航代理，并提出了新的空间智能导航模型SNav，在基准测试和真实机器人测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的指令跟随导航基准主要关注语义理解，但忽视了系统评估导航代理的空间感知和推理能力。

Method: 引入NavSpace基准测试，包含六个任务类别和1,228个轨迹-指令对；提出SNav模型作为新的空间智能导航模型。

Result: 在NavSpace基准上全面评估了22个导航代理，包括最先进的导航模型和多模态大语言模型；SNav模型在NavSpace和真实机器人测试中优于现有导航代理。

Conclusion: NavSpace基准揭示了具身导航中的空间智能问题，SNav模型为未来工作建立了强有力的基线。

Abstract: Instruction-following navigation is a key step toward embodied intelligence.
Prior benchmarks mainly focus on semantic understanding but overlook
systematically evaluating navigation agents' spatial perception and reasoning
capabilities. In this work, we introduce the NavSpace benchmark, which contains
six task categories and 1,228 trajectory-instruction pairs designed to probe
the spatial intelligence of navigation agents. On this benchmark, we
comprehensively evaluate 22 navigation agents, including state-of-the-art
navigation models and multimodal large language models. The evaluation results
lift the veil on spatial intelligence in embodied navigation. Furthermore, we
propose SNav, a new spatially intelligent navigation model. SNav outperforms
existing navigation agents on NavSpace and real robot tests, establishing a
strong baseline for future work.

</details>


### [25] [Evaluation of a Robust Control System in Real-World Cable-Driven Parallel Robots](https://arxiv.org/abs/2510.08270)
*Damir Nurtdinov,Aliaksei Korshuk,Alexei Kornaev,Alexander Maloletov*

Main category: cs.RO

TL;DR: 本文比较了经典PID控制器与现代强化学习算法在欠约束线驱动并联机器人控制中的性能，发现TRPO方法在多种轨迹下均表现最优，具有更好的鲁棒性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 评估经典控制方法和现代强化学习算法在现实世界线驱动并联机器人控制中的性能，特别是在欠约束系统和有限时间离散化条件下的表现。

Method: 对经典PID控制器和现代强化学习算法（包括DDPG、PPO和TRPO）进行对比分析，评估它们在各种轨迹和控制更新间隔下的性能。

Result: TRPO在所有方法中表现最佳，实现了最低的均方根误差，对更大的控制更新间隔具有鲁棒性，能够在噪声环境中实现稳定控制。

Conclusion: TRPO作为复杂机器人控制任务的鲁棒解决方案具有巨大潜力，特别适用于动态环境和未来传感器融合或混合控制策略的应用。

Abstract: This study evaluates the performance of classical and modern control methods
for real-world Cable-Driven Parallel Robots (CDPRs), focusing on
underconstrained systems with limited time discretization. A comparative
analysis is conducted between classical PID controllers and modern
reinforcement learning algorithms, including Deep Deterministic Policy Gradient
(DDPG), Proximal Policy Optimization (PPO), and Trust Region Policy
Optimization (TRPO). The results demonstrate that TRPO outperforms other
methods, achieving the lowest root mean square (RMS) errors across various
trajectories and exhibiting robustness to larger time intervals between control
updates. TRPO's ability to balance exploration and exploitation enables stable
control in noisy, real-world environments, reducing reliance on high-frequency
sensor feedback and computational demands. These findings highlight TRPO's
potential as a robust solution for complex robotic control tasks, with
implications for dynamic environments and future applications in sensor fusion
or hybrid control strategies.

</details>


### [26] [Airy: Reading Robot Intent through Height and Sky](https://arxiv.org/abs/2510.08381)
*Baoyang Chen,Xian Xu,Huamin Qu*

Main category: cs.RO

TL;DR: Airy是一个艺术装置，通过两个强化学习训练的机械臂竞争抖床单，将复杂的多智能体AI决策变得直观可理解。


<details>
  <summary>Details</summary>
Motivation: 工业机器人进入人类共享空间时，其不透明的决策过程威胁安全、信任和公共监督，需要让复杂AI变得可理解。

Method: 采用三个设计原则：竞争作为明确指标（谁举得更高）、具身熟悉性（观众熟悉抖床单动作）、传感器到感知映射（通过森林和天气投影显示机器人合作或竞争）。

Result: 在五个国际展览中的观察表明，观众能实时解读机器人的策略、冲突和合作，情绪反应与系统内部状态一致。

Conclusion: 感官隐喻可以将黑盒AI转变为公共界面，使复杂多智能体AI变得直观可理解。

Abstract: As industrial robots move into shared human spaces, their opaque decision
making threatens safety, trust, and public oversight. This artwork, Airy, asks
whether complex multi agent AI can become intuitively understandable by staging
a competition between two reinforcement trained robot arms that snap a bedsheet
skyward. Building on three design principles, competition as a clear metric
(who lifts higher), embodied familiarity (audiences recognize fabric snapping),
and sensor to sense mapping (robot cooperation or rivalry shown through forest
and weather projections), the installation gives viewers a visceral way to read
machine intent. Observations from five international exhibitions indicate that
audiences consistently read the robots' strategies, conflict, and cooperation
in real time, with emotional reactions that mirror the system's internal state.
The project shows how sensory metaphors can turn a black box into a public
interface.

</details>


### [27] [Reliability of Single-Level Equality-Constrained Inverse Optimal Control](https://arxiv.org/abs/2510.08406)
*Filip Bečanović,Kosta Jovanović,Vincent Bonnet*

Main category: cs.RO

TL;DR: 提出了一种基于单级重构的逆最优控制方法，比传统的双层方法快15倍，同时对噪声具有很好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有逆最优控制方法要么基于缓慢的双层过程，要么基于快速但对噪声敏感的优化条件违反最小化方法，需要一种既快速又鲁棒的解决方案。

Method: 使用单级重构方法替代传统的双层逆最优控制方法，基于等式约束的最优控制模型。

Result: 在平面到达任务中，该方法对非常大的噪声水平表现出韧性，计算时间比经典双层实现减少了15倍。

Conclusion: 单级重构方法在保持结果等效性的同时，显著提高了逆最优控制的效率和鲁棒性。

Abstract: Inverse optimal control (IOC) allows the retrieval of optimal cost function
weights, or behavioral parameters, from human motion. The literature on IOC
uses methods that are either based on a slow bilevel process or a fast but
noise-sensitive minimization of optimality condition violation. Assuming
equality-constrained optimal control models of human motion, this article
presents a faster but robust approach to solving IOC using a single-level
reformulation of the bilevel method and yields equivalent results. Through
numerical experiments in simulation, we analyze the robustness to noise of the
proposed single-level reformulation to the bilevel IOC formulation with a
human-like planar reaching task that is used across recent studies. The
approach shows resilience to very large levels of noise and reduces the
computation time of the IOC on this task by a factor of 15 when compared to a
classical bilevel implementation.

</details>


### [28] [Validation of collision-free spheres of Stewart-Gough platforms for constant orientations using the Application Programming Interface of a CAD software](https://arxiv.org/abs/2510.08408)
*Bibekananda Patra,Rajeevlochana G. Chittawadigi,Sandipan Bandyopadhyay*

Main category: cs.RO

TL;DR: 提出了一种使用CAD软件API验证6-6 Stewart-Gough平台机械臂最大无碰撞球尺寸的方法，通过自动化更新移动平台位置来检测腿部碰撞。


<details>
  <summary>Details</summary>
Motivation: 需要验证平行机械臂在给定移动平台方向下的无碰撞工作空间，确保操作安全性和可靠性。

Method: 利用CAD软件API自动化更新移动平台位置，在无碰撞球壳表面采样点进行检测，检查每对腿部之间的相互碰撞。

Result: 该方法能够验证预计算的无碰撞球安全性，并可估计任何空间平行机械臂的无碰撞工作空间。

Conclusion: 提出的基于CAD API的验证方法有效且通用，适用于各种空间平行机械臂的无碰撞工作空间分析。

Abstract: This paper presents a method of validation of the size of the largest
collision-free sphere (CFS) of a 6-6 Stewart-Gough platform manipulator (SGPM)
for a given orientation of its moving platform (MP) using the Application
Programming Interface (API) of a CAD software. The position of the MP is
updated via the API in an automated manner over a set of samples within a shell
enclosing the surface of the CFS. For each pose of the manipulator, each pair
of legs is investigated for mutual collisions. The CFS is considered safe or
validated iff none of the points falling inside the CFS lead to a collision
between any pair of legs. This approach can not only validate the safety of a
precomputed CFS, but also estimate the same for any spatial parallel
manipulator.

</details>


### [29] [Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered](https://arxiv.org/abs/2510.08464)
*Jason Jabbour,Dong-Ki Kim,Max Smith,Jay Patrikar,Radhika Ghosal,Youhui Wang,Ali Agha,Vijay Janapa Reddi,Shayegan Omidshafiei*

Main category: cs.RO

TL;DR: GLUESTICK是一种后剪枝恢复方法，通过在权重空间对密集模型和剪枝模型进行一次性插值计算修正项，恢复VLA模型剪枝后丢失的功能，同时保持稀疏性优势。


<details>
  <summary>Details</summary>
Motivation: VLA模型在资源受限硬件上部署困难，剪枝虽然能有效压缩大型语言模型，但在机器人领域研究不足，且直接剪枝VLA模型会导致性能急剧下降和安全违规增加。

Method: 在权重空间对密集模型和剪枝模型进行一次性插值计算修正项，推理时每个剪枝层使用该修正项恢复丢失能力，无需额外训练，与剪枝算法无关，仅引入一个控制效率与精度权衡的超参数。

Result: 在多种VLA架构和操作、导航任务中，GLUESTICK实现了有竞争力的内存效率，同时显著恢复成功率并减少安全违规。

Conclusion: GLUESTICK提供了一种无需训练的后剪枝恢复解决方案，有效解决了VLA模型剪枝后的性能退化问题，在保持稀疏性优势的同时恢复了模型功能。

Abstract: Vision-Language-Action (VLA) models have advanced robotic capabilities but
remain challenging to deploy on resource-limited hardware. Pruning has enabled
efficient compression of large language models (LLMs), yet it is largely
understudied in robotics. Surprisingly, we observe that pruning VLA models
leads to drastic degradation and increased safety violations. We introduce
GLUESTICK, a post-pruning recovery method that restores much of the original
model's functionality while retaining sparsity benefits. Our method performs a
one-time interpolation between the dense and pruned models in weight-space to
compute a corrective term. This correction is used during inference by each
pruned layer to recover lost capabilities with minimal overhead. GLUESTICK
requires no additional training, is agnostic to the pruning algorithm, and
introduces a single hyperparameter that controls the tradeoff between
efficiency and accuracy. Across diverse VLA architectures and tasks in
manipulation and navigation, GLUESTICK achieves competitive memory efficiency
while substantially recovering success rates and reducing safety violations.
Additional material can be found at: https://gluestick-vla.github.io/.

</details>


### [30] [DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos](https://arxiv.org/abs/2510.08475)
*Jhen Hsieh,Kuan-Hsun Tu,Kuo-Han Hung,Tsung-Wei Ke*

Main category: cs.RO

TL;DR: DexMan是一个自动化框架，可将人类视觉演示转换为仿真中类人机器人的双手灵巧操作技能，无需相机校准、深度传感器或3D对象资产。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法仅考虑简化浮动手部的问题，直接控制类人机器人，并利用基于接触的奖励从野外视频的噪声手-物体姿态估计中改进策略学习。

Method: 基于第三人称人类操作刚性物体的视频，使用基于接触的奖励进行强化学习策略训练，无需手动数据收集和昂贵的运动捕捉。

Result: 在TACO基准测试中实现最先进的物体姿态估计性能（ADD-S和VSD分别提升0.08和0.12），在OakInk-v2上成功率比先前方法提高19%。

Conclusion: DexMan能够从真实和合成视频生成技能，无需手动数据收集，为训练通用灵巧操作创建大规模多样化数据集提供了可能。

Abstract: We present DexMan, an automated framework that converts human visual
demonstrations into bimanual dexterous manipulation skills for humanoid robots
in simulation. Operating directly on third-person videos of humans manipulating
rigid objects, DexMan eliminates the need for camera calibration, depth
sensors, scanned 3D object assets, or ground-truth hand and object motion
annotations. Unlike prior approaches that consider only simplified floating
hands, it directly controls a humanoid robot and leverages novel contact-based
rewards to improve policy learning from noisy hand-object poses estimated from
in-the-wild videos.
  DexMan achieves state-of-the-art performance in object pose estimation on the
TACO benchmark, with absolute gains of 0.08 and 0.12 in ADD-S and VSD.
Meanwhile, its reinforcement learning policy surpasses previous methods by 19%
in success rate on OakInk-v2. Furthermore, DexMan can generate skills from both
real and synthetic videos, without the need for manual data collection and
costly motion capture, and enabling the creation of large-scale, diverse
datasets for training generalist dexterous manipulation.

</details>


### [31] [R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation](https://arxiv.org/abs/2510.08547)
*Xiuwei Xu,Angyuan Ma,Hankun Li,Bingyao Yu,Zheng Zhu,Jie Zhou,Jiwen Lu*

Main category: cs.RO

TL;DR: 提出R2RGen框架，直接从真实点云观测-动作对生成数据，无需仿真器和渲染，提高机器人操作的空间泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中空间泛化问题，现有方法存在显著的仿真到真实差距，且受限于固定基座场景和预定义相机视角。

Method: 基于单源演示，通过细粒度场景和轨迹解析、分组增强策略处理多对象组合和任务约束，以及相机感知处理对齐真实3D传感器分布。

Result: R2RGen在大量实验中显著提高了数据效率，并展示了在移动操作中的扩展和应用潜力。

Conclusion: R2RGen是一个高效、即插即用的真实到真实3D数据生成框架，有效提升了机器人操作策略的空间泛化性能。

Abstract: Towards the aim of generalized robotic manipulation, spatial generalization
is the most fundamental capability that requires the policy to work robustly
under different spatial distribution of objects, environment and agent itself.
To achieve this, substantial human demonstrations need to be collected to cover
different spatial configurations for training a generalized visuomotor policy
via imitation learning. Prior works explore a promising direction that
leverages data generation to acquire abundant spatially diverse data from
minimal source demonstrations. However, most approaches face significant
sim-to-real gap and are often limited to constrained settings, such as
fixed-base scenarios and predefined camera viewpoints. In this paper, we
propose a real-to-real 3D data generation framework (R2RGen) that directly
augments the pointcloud observation-action pairs to generate real-world data.
R2RGen is simulator- and rendering-free, thus being efficient and
plug-and-play. Specifically, given a single source demonstration, we introduce
an annotation mechanism for fine-grained parsing of scene and trajectory. A
group-wise augmentation strategy is proposed to handle complex multi-object
compositions and diverse task constraints. We further present camera-aware
processing to align the distribution of generated data with real-world 3D
sensor. Empirically, R2RGen substantially enhances data efficiency on extensive
experiments and demonstrates strong potential for scaling and application on
mobile manipulation.

</details>


### [32] [DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model](https://arxiv.org/abs/2510.08556)
*Xueyi Liu,He Wang,Li Yi*

Main category: cs.RO

TL;DR: 提出了一种解决机器人手部物体旋转中模拟到现实转换挑战的新框架，通过联合动力学模型和数据高效的自主动态收集策略，实现了单一策略对多种物体的泛化旋转能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器人手部物体旋转中模拟到现实的转换难题，克服复杂接触动力学造成的"现实差距"，使策略能够泛化到各种物体和条件下。

Method: 使用联合动力学模型学习拟合真实世界数据并调整模拟策略动作，通过关节动态分解、系统影响压缩和个体关节演化学习，配合全自主动态收集策略。

Result: 单一策略成功旋转复杂形状物体、高宽比物体和小尺寸物体，处理多样手腕方向和旋转轴，在真实世界评估和复杂任务遥操作应用中验证有效性。

Conclusion: 该方法在机器人手部物体旋转方面实现了前所未有的泛化能力，有效解决了模拟到现实的转换挑战。

Abstract: Achieving generalized in-hand object rotation remains a significant challenge
in robotics, largely due to the difficulty of transferring policies from
simulation to the real world. The complex, contact-rich dynamics of dexterous
manipulation create a "reality gap" that has limited prior work to constrained
scenarios involving simple geometries, limited object sizes and aspect ratios,
constrained wrist poses, or customized hands. We address this sim-to-real
challenge with a novel framework that enables a single policy, trained in
simulation, to generalize to a wide variety of objects and conditions in the
real world. The core of our method is a joint-wise dynamics model that learns
to bridge the reality gap by effectively fitting limited amount of real-world
collected data and then adapting the sim policy's actions accordingly. The
model is highly data-efficient and generalizable across different whole-hand
interaction distributions by factorizing dynamics across joints, compressing
system-wide influences into low-dimensional variables, and learning each
joint's evolution from its own dynamic profile, implicitly capturing these net
effects. We pair this with a fully autonomous data collection strategy that
gathers diverse, real-world interaction data with minimal human intervention.
Our complete pipeline demonstrates unprecedented generality: a single policy
successfully rotates challenging objects with complex shapes (e.g., animals),
high aspect ratios (up to 5.33), and small sizes, all while handling diverse
wrist orientations and rotation axes. Comprehensive real-world evaluations and
a teleoperation application for complex tasks validate the effectiveness and
robustness of our approach. Website: https://meowuu7.github.io/DexNDM/

</details>


### [33] [NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos](https://arxiv.org/abs/2510.08568)
*Hongyu Li,Lingfeng Sun,Yafei Hu,Duy Ta,Jennifer Barry,George Konidaris,Jiahui Fu*

Main category: cs.RO

TL;DR: NovaFlow是一个零样本自主操作框架，能够将任务描述转换为机器人的可执行计划，无需演示或特定平台训练，支持刚性、关节和可变形物体的操作。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设任务分布内或依赖特定平台数据的微调，限制了跨平台迁移能力。本文旨在实现机器人零样本执行新操作任务，无需演示或特定平台训练。

Method: 使用视频生成模型将任务描述合成为视频，通过现成感知模块提取3D可操作物体流。对刚性物体计算相对位姿，通过抓取提议和轨迹优化实现机器人动作；对可变形物体，将流作为基于粒子动力学模型的跟踪目标。

Result: 在桌面Franka机械臂和Spot四足移动机器人上验证了刚性、关节和可变形物体操作任务，实现了有效的零样本执行，无需演示或特定平台训练。

Conclusion: 通过将任务理解与底层控制解耦，NovaFlow自然支持跨平台迁移，为机器人零样本操作提供了可行解决方案。

Abstract: Enabling robots to execute novel manipulation tasks zero-shot is a central
goal in robotics. Most existing methods assume in-distribution tasks or rely on
fine-tuning with embodiment-matched data, limiting transfer across platforms.
We present NovaFlow, an autonomous manipulation framework that converts a task
description into an actionable plan for a target robot without any
demonstrations. Given a task description, NovaFlow synthesizes a video using a
video generation model and distills it into 3D actionable object flow using
off-the-shelf perception modules. From the object flow, it computes relative
poses for rigid objects and realizes them as robot actions via grasp proposals
and trajectory optimization. For deformable objects, this flow serves as a
tracking objective for model-based planning with a particle-based dynamics
model. By decoupling task understanding from low-level control, NovaFlow
naturally transfers across embodiments. We validate on rigid, articulated, and
deformable object manipulation tasks using a table-top Franka arm and a Spot
quadrupedal mobile robot, and achieve effective zero-shot execution without
demonstrations or embodiment-specific training. Project website:
https://novaflow.lhy.xyz/.

</details>


### [34] [Scalable Offline Metrics for Autonomous Driving](https://arxiv.org/abs/2510.08571)
*Animikh Aich,Adwait Kulkarni,Eshed Ohn-Bar*

Main category: cs.RO

TL;DR: 该论文研究了感知规划模型在机器人系统中的离线与在线评估差异，发现两者相关性比之前研究报道的更差，并提出基于认知不确定性的离线指标来弥合这一差距。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶等机器人系统的感知规划模型主要通过离线评估，但离线性能难以准确预测在线环境下的表现，这种关系在复杂城市机动中研究不足。

Method: 通过大量实验分析离线与在线评估的相关性，提出基于认知不确定性的离线评估指标，并在仿真和真实环境中验证其有效性。

Result: 发现离线与在线评估的相关性比之前研究报道的更差，提出的新离线指标相比之前指标相关性提高了13%以上，在真实环境中效果更显著。

Conclusion: 当前驾驶策略评估实践存在局限性，基于认知不确定性的离线指标能更好地预测在线性能，为更可靠的模型评估提供了新方法。

Abstract: Real-World evaluation of perception-based planning models for robotic
systems, such as autonomous vehicles, can be safely and inexpensively conducted
offline, i.e., by computing model prediction error over a pre-collected
validation dataset with ground-truth annotations. However, extrapolating from
offline model performance to online settings remains a challenge. In these
settings, seemingly minor errors can compound and result in test-time
infractions or collisions. This relationship is understudied, particularly
across diverse closed-loop metrics and complex urban maneuvers. In this work,
we revisit this undervalued question in policy evaluation through an extensive
set of experiments across diverse conditions and metrics. Based on analysis in
simulation, we find an even worse correlation between offline and online
settings than reported by prior studies, casting doubts on the validity of
current evaluation practices and metrics for driving policies. Next, we bridge
the gap between offline and online evaluation. We investigate an offline metric
based on epistemic uncertainty, which aims to capture events that are likely to
cause errors in closed-loop settings. The resulting metric achieves over 13%
improvement in correlation compared to previous offline metrics. We further
validate the generalization of our findings beyond the simulation environment
in real-world settings, where even greater gains are observed.

</details>


### [35] [BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data Generation](https://arxiv.org/abs/2510.08572)
*Rocktim Jyoti Das,Harsh Singh,Diana Turmakhan,Muhammad Abdullah Sohail,Mingfei Han,Preslav Nakov,Fabio Pizzati,Ivan Laptev*

Main category: cs.RO

TL;DR: BLAZER是一个从自动生成数据中学习机器人操作策略的框架，利用LLM规划器的零样本能力在模拟环境中生成多样化操作任务的演示，然后使用成功示例微调LLM以提升规划能力，无需人工监督。


<details>
  <summary>Details</summary>
Motivation: 机器人领域缺乏互联网规模的多任务演示数据，现有数据集受限于人工收集和整理，限制了数据规模和模型泛化能力。

Method: 利用LLM规划器的零样本能力自动在模拟环境中生成多样化操作任务的演示，选择成功示例微调LLM，提升其规划能力，实现从模拟到真实环境的技能迁移。

Result: BLAZER显著提升了模拟和真实环境中的零样本操作能力，在训练任务之外的任务上也有改进，并支持LLM模型的下采样。

Conclusion: BLAZER框架通过自动生成训练数据有效解决了机器人领域数据稀缺问题，实现了从模拟到真实环境的技能迁移，提升了零样本操作性能。

Abstract: Scaling data and models has played a pivotal role in the remarkable progress
of computer vision and language. Inspired by these domains, recent efforts in
robotics have similarly focused on scaling both data and model size to develop
more generalizable and robust policies. However, unlike vision and language,
robotics lacks access to internet-scale demonstrations across diverse robotic
tasks and environments. As a result, the scale of existing datasets typically
suffers from the need for manual data collection and curation. To address this
problem, here we propose BLAZER, a framework that learns manipulation policies
from automatically generated training data. We build on the zero-shot
capabilities of LLM planners and automatically generate demonstrations for
diverse manipulation tasks in simulation. Successful examples are then used to
finetune an LLM and to improve its planning capabilities without human
supervision. Notably, while BLAZER training requires access to the simulator's
state, we demonstrate direct transfer of acquired skills to sensor-based
manipulation. Through extensive experiments, we show BLAZER to significantly
improve zero-shot manipulation in both simulated and real environments.
Moreover, BLAZER improves on tasks outside of its training pool and enables
downscaling of LLM models. Our code and data will be made publicly available on
the project page.

</details>
