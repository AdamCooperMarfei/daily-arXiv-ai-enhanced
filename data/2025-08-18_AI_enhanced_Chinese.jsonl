{"id": "2508.10973", "categories": ["cs.RO", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.10973", "abs": "https://arxiv.org/abs/2508.10973", "authors": ["Hongchen Wang", "Sima Zeinali Danalou", "Jiahao Zhu", "Kenneth Sulimro", "Chaewon Lim", "Smita Basak", "Aimee Tai", "Usan Siriwardana", "Jason Hattrick-Simpers", "Jay Werber"], "title": "Developing and Validating a High-Throughput Robotic System for the Accelerated Development of Porous Membranes", "comment": null, "summary": "The development of porous polymeric membranes remains a labor-intensive\nprocess, often requiring extensive trial and error to identify optimal\nfabrication parameters. In this study, we present a fully automated platform\nfor membrane fabrication and characterization via nonsolvent-induced phase\nseparation (NIPS). The system integrates automated solution preparation, blade\ncasting, controlled immersion, and compression testing, allowing precise\ncontrol over fabrication parameters such as polymer concentration and ambient\nhumidity. The modular design allows parallel processing and reproducible\nhandling of samples, reducing experimental time and increasing consistency.\nCompression testing is introduced as a sensitive mechanical characterization\nmethod for estimating membrane stiffness and as a proxy to infer porosity and\nintra-sample uniformity through automated analysis of stress-strain curves. As\na proof of concept to demonstrate the effectiveness of the system, NIPS was\ncarried out with polysulfone, the green solvent PolarClean, and water as the\npolymer, solvent, and nonsolvent, respectively. Experiments conducted with the\nautomated system reproduced expected effects of polymer concentration and\nambient humidity on membrane properties, namely increased stiffness and\nuniformity with increasing polymer concentration and humidity variations in\npore morphology and mechanical response. The developed automated platform\nsupports high-throughput experimentation and is well-suited for integration\ninto self-driving laboratory workflows, offering a scalable and reproducible\nfoundation for data-driven optimization of porous polymeric membranes through\nNIPS.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5168\u81ea\u52a8\u5e73\u53f0\uff0c\u7528\u4e8e\u901a\u8fc7\u975e\u6eb6\u5242\u8bf1\u5bfc\u76f8\u5206\u79bb\uff08NIPS\uff09\u5236\u5907\u548c\u8868\u5f81\u591a\u5b54\u805a\u5408\u7269\u819c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9e\u9a8c\u6548\u7387\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u591a\u5b54\u805a\u5408\u7269\u819c\u7684\u5f00\u53d1\u8fc7\u7a0b\u8017\u65f6\u4e14\u4f9d\u8d56\u8bd5\u9519\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u5236\u5907\u53c2\u6570\u3002", "method": "\u96c6\u6210\u81ea\u52a8\u6eb6\u6db2\u5236\u5907\u3001\u5200\u7247\u6d82\u5e03\u3001\u63a7\u5236\u6d78\u6ca1\u548c\u538b\u7f29\u6d4b\u8bd5\uff0c\u901a\u8fc7NIPS\u5236\u5907\u819c\uff0c\u5e76\u5206\u6790\u5e94\u529b-\u5e94\u53d8\u66f2\u7ebf\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u805a\u5408\u7269\u6d53\u5ea6\u548c\u73af\u5883\u6e7f\u5ea6\u5bf9\u819c\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u7cfb\u7edf\u652f\u6301\u9ad8\u901a\u91cf\u5b9e\u9a8c\uff0c\u9002\u7528\u4e8e\u81ea\u9a71\u52a8\u5b9e\u9a8c\u5ba4\u5de5\u4f5c\u6d41\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u5e73\u53f0\u4e3aNIPS\u5236\u5907\u591a\u5b54\u805a\u5408\u7269\u819c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u91cd\u590d\u7684\u6570\u636e\u9a71\u52a8\u4f18\u5316\u57fa\u7840\u3002"}}
{"id": "2508.10999", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.10999", "abs": "https://arxiv.org/abs/2508.10999", "authors": ["Yizhi Zhou", "Jie Xu", "Jiawei Xia", "Zechen Hu", "Weizi Li", "Xuan Wang"], "title": "Robust Online Calibration for UWB-Aided Visual-Inertial Navigation with Bias Correction", "comment": null, "summary": "This paper presents a novel robust online calibration framework for\nUltra-Wideband (UWB) anchors in UWB-aided Visual-Inertial Navigation Systems\n(VINS). Accurate anchor positioning, a process known as calibration, is crucial\nfor integrating UWB ranging measurements into state estimation. While several\nprior works have demonstrated satisfactory results by using robot-aided systems\nto autonomously calibrate UWB systems, there are still some limitations: 1)\nthese approaches assume accurate robot localization during the initialization\nstep, ignoring localization errors that can compromise calibration robustness,\nand 2) the calibration results are highly sensitive to the initial guess of the\nUWB anchors' positions, reducing the practical applicability of these methods\nin real-world scenarios. Our approach addresses these challenges by explicitly\nincorporating the impact of robot localization uncertainties into the\ncalibration process, ensuring robust initialization. To further enhance the\nrobustness of the calibration results against initialization errors, we propose\na tightly-coupled Schmidt Kalman Filter (SKF)-based online refinement method,\nmaking the system suitable for practical applications. Simulations and\nreal-world experiments validate the improved accuracy and robustness of our\napproach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9c81\u68d2\u5728\u7ebf\u6821\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u8d85\u5bbd\u5e26\uff08UWB\uff09\u951a\u70b9\u5728UWB\u8f85\u52a9\u7684\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\uff08VINS\uff09\u4e2d\u7684\u6821\u51c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u673a\u5668\u4eba\u5b9a\u4f4d\u8bef\u5dee\u548c\u521d\u59cb\u731c\u6d4b\u654f\u611f\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709UWB\u6821\u51c6\u65b9\u6cd5\u5047\u8bbe\u673a\u5668\u4eba\u5b9a\u4f4d\u51c6\u786e\u4e14\u5bf9\u951a\u70b9\u521d\u59cb\u4f4d\u7f6e\u654f\u611f\uff0c\u5bfc\u81f4\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u663e\u5f0f\u8003\u8651\u673a\u5668\u4eba\u5b9a\u4f4d\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u51fa\u57fa\u4e8eSchmidt Kalman Filter\uff08SKF\uff09\u7684\u7d27\u5bc6\u8026\u5408\u5728\u7ebf\u7ec6\u5316\u65b9\u6cd5\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u7684\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86UWB\u6821\u51c6\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.11002", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11002", "abs": "https://arxiv.org/abs/2508.11002", "authors": ["Nikolaos Gkanatsios", "Jiahe Xu", "Matthew Bronars", "Arsalan Mousavian", "Tsung-Wei Ke", "Katerina Fragkiadaki"], "title": "3D FlowMatch Actor: Unified 3D Policy for Single- and Dual-Arm Manipulation", "comment": null, "summary": "We present 3D FlowMatch Actor (3DFA), a 3D policy architecture for robot\nmanipulation that combines flow matching for trajectory prediction with 3D\npretrained visual scene representations for learning from demonstration. 3DFA\nleverages 3D relative attention between action and visual tokens during action\ndenoising, building on prior work in 3D diffusion-based single-arm policy\nlearning. Through a combination of flow matching and targeted system-level and\narchitectural optimizations, 3DFA achieves over 30x faster training and\ninference than previous 3D diffusion-based policies, without sacrificing\nperformance. On the bimanual PerAct2 benchmark, it establishes a new state of\nthe art, outperforming the next-best method by an absolute margin of 41.4%. In\nextensive real-world evaluations, it surpasses strong baselines with up to\n1000x more parameters and significantly more pretraining. In unimanual\nsettings, it sets a new state of the art on 74 RLBench tasks by directly\npredicting dense end-effector trajectories, eliminating the need for motion\nplanning. Comprehensive ablation studies underscore the importance of our\ndesign choices for both policy effectiveness and efficiency.", "AI": {"tldr": "3DFA\u662f\u4e00\u79cd\u7ed3\u5408\u6d41\u5339\u914d\u548c3D\u9884\u8bad\u7ec3\u89c6\u89c9\u573a\u666f\u8868\u793a\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u67093D\u6269\u6563\u7b56\u7565\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u6d41\u5339\u914d\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\u548c3D\u9884\u8bad\u7ec3\u89c6\u89c9\u8868\u793a\uff0c\u5229\u75283D\u76f8\u5bf9\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u52a8\u4f5c\u53bb\u566a\u3002", "result": "\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u63d0\u534730\u500d\u4ee5\u4e0a\uff0c\u5728PerAct2\u57fa\u51c6\u4e0a\u6027\u80fd\u63d0\u534741.4%\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u548cRLBench\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "3DFA\u901a\u8fc7\u8bbe\u8ba1\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11049", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11049", "abs": "https://arxiv.org/abs/2508.11049", "authors": ["Kelin Yu", "Sheng Zhang", "Harshit Soora", "Furong Huang", "Heng Huang", "Pratap Tokekar", "Ruohan Gao"], "title": "GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning", "comment": "Published at ICCV 2025", "summary": "Recent advances have shown that video generation models can enhance robot\nlearning by deriving effective robot actions through inverse dynamics. However,\nthese methods heavily depend on the quality of generated data and struggle with\nfine-grained manipulation due to the lack of environment feedback. While\nvideo-based reinforcement learning improves policy robustness, it remains\nconstrained by the uncertainty of video generation and the challenges of\ncollecting large-scale robot datasets for training diffusion models. To address\nthese limitations, we propose GenFlowRL, which derives shaped rewards from\ngenerated flow trained from diverse cross-embodiment datasets. This enables\nlearning generalizable and robust policies from diverse demonstrations using\nlow-dimensional, object-centric features. Experiments on 10 manipulation tasks,\nboth in simulation and real-world cross-embodiment evaluations, demonstrate\nthat GenFlowRL effectively leverages manipulation features extracted from\ngenerated object-centric flow, consistently achieving superior performance\nacross diverse and challenging scenarios. Our Project Page:\nhttps://colinyu1.github.io/genflowrl", "AI": {"tldr": "GenFlowRL\u5229\u7528\u751f\u6210\u6d41\u4ece\u591a\u6837\u5316\u7684\u8de8\u4f53\u73b0\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u5f62\u72b6\u5956\u52b1\uff0c\u5b66\u4e60\u901a\u7528\u4e14\u9c81\u68d2\u7684\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u4f9d\u8d56\u751f\u6210\u6570\u636e\u8d28\u91cf\u4e14\u7f3a\u4e4f\u73af\u5883\u53cd\u9988\uff0c\u96be\u4ee5\u5904\u7406\u7cbe\u7ec6\u64cd\u4f5c\uff0c\u540c\u65f6\u89c6\u9891\u751f\u6210\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6570\u636e\u96c6\u6536\u96c6\u7684\u6311\u6218\u9650\u5236\u4e86\u57fa\u4e8e\u89c6\u9891\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "method": "\u63d0\u51faGenFlowRL\uff0c\u901a\u8fc7\u4ece\u591a\u6837\u5316\u7684\u8de8\u4f53\u73b0\u6570\u636e\u96c6\u4e2d\u8bad\u7ec3\u751f\u6210\u6d41\u6765\u63d0\u53d6\u5f62\u72b6\u5956\u52b1\uff0c\u5229\u7528\u4f4e\u7ef4\u3001\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u7279\u5f81\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u572810\u4e2a\u64cd\u4f5c\u4efb\u52a1\u7684\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u8de8\u4f53\u73b0\u8bc4\u4f30\u4e2d\uff0cGenFlowRL\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u751f\u6210\u7684\u5bf9\u8c61\u4e2d\u5fc3\u6d41\u63d0\u53d6\u7684\u64cd\u4f5c\u7279\u5f81\u3002", "conclusion": "GenFlowRL\u901a\u8fc7\u751f\u6210\u6d41\u63d0\u53d6\u5f62\u72b6\u5956\u52b1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u7684\u901a\u7528\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u3002"}}
{"id": "2508.11093", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11093", "abs": "https://arxiv.org/abs/2508.11093", "authors": ["Cesar Alan Contreras", "Manolis Chiou", "Alireza Rastegarpanah", "Michal Szulik", "Rustam Stolkin"], "title": "Utilizing Vision-Language Models as Action Models for Intent Recognition and Assistance", "comment": "Accepted at Human-Centered Robot Autonomy for Human-Robot Teams\n  (HuRoboT) at IEEE RO-MAN 2025, Eindhoven, the Netherlands", "summary": "Human-robot collaboration requires robots to quickly infer user intent,\nprovide transparent reasoning, and assist users in achieving their goals. Our\nrecent work introduced GUIDER, our framework for inferring navigation and\nmanipulation intents. We propose augmenting GUIDER with a vision-language model\n(VLM) and a text-only language model (LLM) to form a semantic prior that\nfilters objects and locations based on the mission prompt. A vision pipeline\n(YOLO for object detection and the Segment Anything Model for instance\nsegmentation) feeds candidate object crops into the VLM, which scores their\nrelevance given an operator prompt; in addition, the list of detected object\nlabels is ranked by a text-only LLM. These scores weight the existing\nnavigation and manipulation layers of GUIDER, selecting context-relevant\ntargets while suppressing unrelated objects. Once the combined belief exceeds a\nthreshold, autonomy changes occur, enabling the robot to navigate to the\ndesired area and retrieve the desired object, while adapting to any changes in\nthe operator's intent. Future work will evaluate the system on Isaac Sim using\na Franka Emika arm on a Ridgeback base, with a focus on real-time assistance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u7eaf\u6587\u672c\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u589e\u5f3aGUIDER\u6846\u67b6\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u63a8\u65ad\u7528\u6237\u610f\u56fe\u5e76\u5b9e\u73b0\u4eba\u673a\u534f\u4f5c\u3002", "motivation": "\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u4e2d\u673a\u5668\u4eba\u5bf9\u7528\u6237\u610f\u56fe\u7684\u5feb\u901f\u63a8\u65ad\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u900f\u660e\u7684\u63a8\u7406\u548c\u8f85\u52a9\u3002", "method": "\u7ed3\u5408VLM\u548cLLM\u5f62\u6210\u8bed\u4e49\u5148\u9a8c\uff0c\u901a\u8fc7\u89c6\u89c9\u7ba1\u9053\uff08YOLO\u548cSegment Anything Model\uff09\u68c0\u6d4b\u5bf9\u8c61\uff0c\u5e76\u7531VLM\u548cLLM\u8bc4\u5206\uff0c\u52a0\u6743GUIDER\u7684\u5bfc\u822a\u548c\u64cd\u4f5c\u5c42\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u9009\u62e9\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u76ee\u6807\uff0c\u6291\u5236\u65e0\u5173\u5bf9\u8c61\uff0c\u5e76\u5728\u610f\u56fe\u53d8\u5316\u65f6\u81ea\u9002\u5e94\u8c03\u6574\u3002", "conclusion": "\u672a\u6765\u5de5\u4f5c\u5c06\u5728Isaac Sim\u4e2d\u8bc4\u4f30\u7cfb\u7edf\uff0c\u91cd\u70b9\u5173\u6ce8\u5b9e\u65f6\u8f85\u52a9\u80fd\u529b\u3002"}}
{"id": "2508.11117", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11117", "abs": "https://arxiv.org/abs/2508.11117", "authors": ["Xuning Yang", "Clemens Eppner", "Jonathan Tremblay", "Dieter Fox", "Stan Birchfield", "Fabio Ramos"], "title": "Robot Policy Evaluation for Sim-to-Real Transfer: A Benchmarking Perspective", "comment": "2025 Robot: Science and Systems (RSS) Workshop on Robot Evaluation\n  for the Real World", "summary": "Current vision-based robotics simulation benchmarks have significantly\nadvanced robotic manipulation research. However, robotics is fundamentally a\nreal-world problem, and evaluation for real-world applications has lagged\nbehind in evaluating generalist policies. In this paper, we discuss challenges\nand desiderata in designing benchmarks for generalist robotic manipulation\npolicies for the goal of sim-to-real policy transfer. We propose 1) utilizing\nhigh visual-fidelity simulation for improved sim-to-real transfer, 2)\nevaluating policies by systematically increasing task complexity and scenario\nperturbation to assess robustness, and 3) quantifying performance alignment\nbetween real-world performance and its simulation counterparts.", "AI": {"tldr": "\u8bba\u6587\u8ba8\u8bba\u4e86\u8bbe\u8ba1\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u57fa\u51c6\u7684\u6311\u6218\u548c\u9700\u6c42\uff0c\u63d0\u51fa\u4e86\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u6a21\u62df\u3001\u4efb\u52a1\u590d\u6742\u6027\u8bc4\u4f30\u548c\u6027\u80fd\u5bf9\u9f50\u91cf\u5316\u7b49\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u4eba\u6a21\u62df\u57fa\u51c6\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u8bc4\u4f30\u901a\u7528\u7b56\u7565\u7684\u8fdb\u5c55\u6ede\u540e\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u652f\u6301\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u7b56\u7565\u8fc1\u79fb\u3002", "method": "1) \u4f7f\u7528\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u6a21\u62df\uff1b2) \u901a\u8fc7\u589e\u52a0\u4efb\u52a1\u590d\u6742\u6027\u548c\u573a\u666f\u6270\u52a8\u8bc4\u4f30\u7b56\u7565\u9c81\u68d2\u6027\uff1b3) \u91cf\u5316\u4eff\u771f\u4e0e\u73b0\u5b9e\u6027\u80fd\u5bf9\u9f50\u3002", "result": "\u63d0\u51fa\u4e86\u6539\u8fdb\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u7684\u57fa\u51c6\u8bbe\u8ba1\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u9ad8\u4fdd\u771f\u6a21\u62df\u548c\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u53ef\u4ee5\u63d0\u5347\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11129", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11129", "abs": "https://arxiv.org/abs/2508.11129", "authors": ["Ryan M. Bena", "Gilbert Bahati", "Blake Werner", "Ryan K. Cosner", "Lizhi Yang", "Aaron D. Ames"], "title": "Geometry-Aware Predictive Safety Filters on Humanoids: From Poisson Safety Functions to CBF Constrained MPC", "comment": "2025 IEEE-RAS 24th International Conference on Humanoid Robots", "summary": "Autonomous navigation through unstructured and dynamically-changing\nenvironments is a complex task that continues to present many challenges for\nmodern roboticists. In particular, legged robots typically possess manipulable\nasymmetric geometries which must be considered during safety-critical\ntrajectory planning. This work proposes a predictive safety filter: a nonlinear\nmodel predictive control (MPC) algorithm for online trajectory generation with\ngeometry-aware safety constraints based on control barrier functions (CBFs).\nCritically, our method leverages Poisson safety functions to numerically\nsynthesize CBF constraints directly from perception data. We extend the\ntheoretical framework for Poisson safety functions to incorporate temporal\nchanges in the domain by reformulating the static Dirichlet problem for\nPoisson's equation as a parameterized moving boundary value problem.\nFurthermore, we employ Minkowski set operations to lift the domain into a\nconfiguration space that accounts for robot geometry. Finally, we implement our\nreal-time predictive safety filter on humanoid and quadruped robots in various\nsafety-critical scenarios. The results highlight the versatility of Poisson\nsafety functions, as well as the benefit of CBF constrained model predictive\nsafety-critical controllers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBFs\uff09\u7684\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u7ebf\u8f68\u8ff9\u751f\u6210\uff0c\u5e76\u7ed3\u5408\u51e0\u4f55\u611f\u77e5\u7684\u5b89\u5168\u7ea6\u675f\u3002", "motivation": "\u89e3\u51b3\u817f\u5f0f\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u548c\u52a8\u6001\u53d8\u5316\u73af\u5883\u4e2d\u5bfc\u822a\u65f6\u7684\u5b89\u5168\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\u3002", "method": "\u5229\u7528\u6cca\u677e\u5b89\u5168\u51fd\u6570\u4ece\u611f\u77e5\u6570\u636e\u4e2d\u6570\u503c\u5408\u6210CBF\u7ea6\u675f\uff0c\u6269\u5c55\u4e86\u6cca\u677e\u5b89\u5168\u51fd\u6570\u7684\u7406\u8bba\u6846\u67b6\u4ee5\u5904\u7406\u52a8\u6001\u8fb9\u754c\u95ee\u9898\uff0c\u5e76\u4f7f\u7528Minkowski\u96c6\u5408\u64cd\u4f5c\u8003\u8651\u673a\u5668\u4eba\u51e0\u4f55\u3002", "result": "\u5728\u591a\u79cd\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u5b9e\u65f6\u9884\u6d4b\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u9a8c\u8bc1\u4e86\u6cca\u677e\u5b89\u5168\u51fd\u6570\u7684\u901a\u7528\u6027\u548cCBF\u7ea6\u675fMPC\u63a7\u5236\u5668\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u817f\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11143", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11143", "abs": "https://arxiv.org/abs/2508.11143", "authors": ["Jiarui Yang", "Bin Zhu", "Jingjing Chen", "Yu-Gang Jiang"], "title": "Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward", "comment": null, "summary": "Existing reinforcement learning (RL) methods struggle with long-horizon\nrobotic manipulation tasks, particularly those involving sparse rewards. While\naction chunking is a promising paradigm for robotic manipulation, using RL to\ndirectly learn continuous action chunks in a stable and data-efficient manner\nremains a critical challenge. This paper introduces AC3 (Actor-Critic for\nContinuous Chunks), a novel RL framework that learns to generate\nhigh-dimensional, continuous action sequences. To make this learning process\nstable and data-efficient, AC3 incorporates targeted stabilization mechanisms\nfor both the actor and the critic. First, to ensure reliable policy\nimprovement, the actor is trained with an asymmetric update rule, learning\nexclusively from successful trajectories. Second, to enable effective value\nlearning despite sparse rewards, the critic's update is stabilized using\nintra-chunk $n$-step returns and further enriched by a self-supervised module\nproviding intrinsic rewards at anchor points aligned with each action chunk. We\nconducted extensive experiments on 25 tasks from the BiGym and RLBench\nbenchmarks. Results show that by using only a few demonstrations and a simple\nmodel architecture, AC3 achieves superior success rates on most tasks,\nvalidating its effective design.", "AI": {"tldr": "AC3\uff08Actor-Critic for Continuous Chunks\uff09\u662f\u4e00\u79cd\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7a33\u5b9a\u673a\u5236\u9ad8\u6548\u5b66\u4e60\u8fde\u7eed\u52a8\u4f5c\u5e8f\u5217\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u7a00\u758f\u5956\u52b1\u7684\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u76f4\u63a5\u5b66\u4e60\u8fde\u7eed\u52a8\u4f5c\u5757\u5b58\u5728\u7a33\u5b9a\u6027\u548c\u6570\u636e\u6548\u7387\u95ee\u9898\u3002", "method": "AC3\u7ed3\u5408\u4e86\u9488\u5bf9\u6027\u7684\u7a33\u5b9a\u673a\u5236\uff1a\u6f14\u5458\u901a\u8fc7\u975e\u5bf9\u79f0\u66f4\u65b0\u89c4\u5219\u4ece\u6210\u529f\u8f68\u8ff9\u5b66\u4e60\uff0c\u8bc4\u8bba\u5bb6\u901a\u8fc7n\u6b65\u56de\u62a5\u548c\u81ea\u6211\u76d1\u7763\u6a21\u5757\u589e\u5f3a\u3002", "result": "\u5728BiGym\u548cRLBench\u768425\u4e2a\u4efb\u52a1\u4e2d\uff0cAC3\u4ec5\u9700\u5c11\u91cf\u6f14\u793a\u5373\u53d6\u5f97\u66f4\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "AC3\u901a\u8fc7\u9ad8\u6548\u8bbe\u8ba1\u9a8c\u8bc1\u4e86\u5176\u5728\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2508.11200", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11200", "abs": "https://arxiv.org/abs/2508.11200", "authors": ["Hongbin Lin", "Bin Li", "Kwok Wai Samuel Au"], "title": "Visuomotor Grasping with World Models for Surgical Robots", "comment": null, "summary": "Grasping is a fundamental task in robot-assisted surgery (RAS), and\nautomating it can reduce surgeon workload while enhancing efficiency, safety,\nand consistency beyond teleoperated systems. Most prior approaches rely on\nexplicit object pose tracking or handcrafted visual features, limiting their\ngeneralization to novel objects, robustness to visual disturbances, and the\nability to handle deformable objects. Visuomotor learning offers a promising\nalternative, but deploying it in RAS presents unique challenges, such as low\nsignal-to-noise ratio in visual observations, demands for high safety and\nmillimeter-level precision, as well as the complex surgical environment. This\npaper addresses three key challenges: (i) sim-to-real transfer of visuomotor\npolicies to ex vivo surgical scenes, (ii) visuomotor learning using only a\nsingle stereo camera pair -- the standard RAS setup, and (iii) object-agnostic\ngrasping with a single policy that generalizes to diverse, unseen surgical\nobjects without retraining or task-specific models. We introduce Grasp Anything\nfor Surgery V2 (GASv2), a visuomotor learning framework for surgical grasping.\nGASv2 leverages a world-model-based architecture and a surgical perception\npipeline for visual observations, combined with a hybrid control system for\nsafe execution. We train the policy in simulation using domain randomization\nfor sim-to-real transfer and deploy it on a real robot in both phantom-based\nand ex vivo surgical settings, using only a single pair of endoscopic cameras.\nExtensive experiments show our policy achieves a 65% success rate in both\nsettings, generalizes to unseen objects and grippers, and adapts to diverse\ndisturbances, demonstrating strong performance, generality, and robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGASv2\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8fd0\u52a8\u5b66\u4e60\u5b9e\u73b0\u624b\u672f\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3001\u5355\u6444\u50cf\u5934\u8f93\u5165\u548c\u5bf9\u8c61\u65e0\u5173\u6027\u4e09\u5927\u6311\u6218\u3002", "motivation": "\u81ea\u52a8\u5316\u624b\u672f\u6293\u53d6\u4efb\u52a1\u53ef\u51cf\u8f7b\u533b\u751f\u8d1f\u62c5\u5e76\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u5bf9\u8c61\u8ddf\u8e2a\u6216\u624b\u5de5\u7279\u5f81\uff0c\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "GASv2\u7ed3\u5408\u4e16\u754c\u6a21\u578b\u67b6\u6784\u548c\u624b\u672f\u611f\u77e5\u7ba1\u9053\uff0c\u4f7f\u7528\u6df7\u5408\u63a7\u5236\u7cfb\u7edf\u5b9e\u73b0\u5b89\u5168\u6267\u884c\uff0c\u5e76\u901a\u8fc7\u57df\u968f\u673a\u5316\u5728\u6a21\u62df\u4e2d\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u7b56\u7565\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u6210\u529f\u738765%\uff0c\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u5bf9\u8c61\u548c\u5939\u5177\uff0c\u9002\u5e94\u591a\u79cd\u5e72\u6270\u3002", "conclusion": "GASv2\u5c55\u793a\u4e86\u9ad8\u6027\u80fd\u3001\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u624b\u672f\u673a\u5668\u4eba\u6293\u53d6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11204", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11204", "abs": "https://arxiv.org/abs/2508.11204", "authors": ["Hongbin Lin", "Juan Rojas", "Kwok Wai Samuel Au"], "title": "Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation", "comment": null, "summary": "Sampling efficiency is critical for deploying visuomotor learning in\nreal-world robotic manipulation. While task symmetry has emerged as a promising\ninductive bias to improve efficiency, most prior work is limited to isometric\nsymmetries -- applying the same group transformation to all task objects across\nall timesteps. In this work, we explore non-isometric symmetries, applying\nmultiple independent group transformations across spatial and temporal\ndimensions to relax these constraints. We introduce a novel formulation of the\npartially observable Markov decision process (POMDP) that incorporates the\nnon-isometric symmetry structures, and propose a simple yet effective data\naugmentation method, Multi-Group Equivariance Augmentation (MEA). We integrate\nMEA with offline reinforcement learning to enhance sampling efficiency, and\nintroduce a voxel-based visual representation that preserves translational\nequivariance. Extensive simulation and real-robot experiments across two\nmanipulation domains demonstrate the effectiveness of our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u7b49\u8ddd\u5bf9\u79f0\u6027\u65b9\u6cd5\uff08MEA\uff09\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u91c7\u6837\u6548\u7387\uff0c\u901a\u8fc7\u591a\u7ec4\u53d8\u6362\u589e\u5f3a\u6570\u636e\uff0c\u7ed3\u5408\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u4f53\u7d20\u89c6\u89c9\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u7b49\u8ddd\u5bf9\u79f0\u6027\uff0c\u9650\u5236\u4e86\u91c7\u6837\u6548\u7387\u7684\u63d0\u5347\u3002", "method": "\u5f15\u5165\u975e\u7b49\u8ddd\u5bf9\u79f0\u6027\u7684POMDP\u6a21\u578b\uff0c\u63d0\u51faMEA\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u7ed3\u5408\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u4f53\u7d20\u89c6\u89c9\u8868\u793a\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u975e\u7b49\u8ddd\u5bf9\u79f0\u6027\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u91c7\u6837\u6548\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u3002"}}
{"id": "2508.11232", "categories": ["cs.RO", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.11232", "abs": "https://arxiv.org/abs/2508.11232", "authors": ["Guoliang Li", "Xibin Jin", "Yujie Wan", "Chenxuan Liu", "Tong Zhang", "Shuai Wang", "Chengzhong Xu"], "title": "Embodied Edge Intelligence Meets Near Field Communication: Concept, Design, and Verification", "comment": "9 pages, 6 figures, to appear in IEEE Network", "summary": "Realizing embodied artificial intelligence is challenging due to the huge\ncomputation demands of large models (LMs). To support LMs while ensuring\nreal-time inference, embodied edge intelligence (EEI) is a promising paradigm,\nwhich leverages an LM edge to provide computing powers in close proximity to\nembodied robots. Due to embodied data exchange, EEI requires higher spectral\nefficiency, enhanced communication security, and reduced inter-user\ninterference. To meet these requirements, near-field communication (NFC), which\nleverages extremely large antenna arrays as its hardware foundation, is an\nideal solution. Therefore, this paper advocates the integration of EEI and NFC,\nresulting in a near-field EEI (NEEI) paradigm. However, NEEI also introduces\nnew challenges that cannot be adequately addressed by isolated EEI or NFC\ndesigns, creating research opportunities for joint optimization of both\nfunctionalities. To this end, we propose radio-friendly embodied planning for\nEEI-assisted NFC scenarios and view-guided beam-focusing for NFC-assisted EEI\nscenarios. We also elaborate how to realize resource-efficient NEEI through\nopportunistic collaborative navigation. Experimental results are provided to\nconfirm the superiority of the proposed techniques compared with various\nbenchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fb9\u7f18\u667a\u80fd\uff08EEI\uff09\u548c\u8fd1\u573a\u901a\u4fe1\uff08NFC\uff09\u7684\u65b0\u8303\u5f0f\uff08NEEI\uff09\uff0c\u4ee5\u89e3\u51b3\u5927\u578b\u6a21\u578b\u5728\u5b9e\u65f6\u63a8\u7406\u4e2d\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u5e76\u901a\u8fc7\u8054\u5408\u4f18\u5316\u89e3\u51b3\u65b0\u6311\u6218\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u6a21\u578b\uff08LMs\uff09\u7684\u9ad8\u8ba1\u7b97\u9700\u6c42\uff0c\u5b9e\u73b0\u5b9e\u65f6\u63a8\u7406\u7684\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u9762\u4e34\u6311\u6218\u3002EEI\u901a\u8fc7\u8fb9\u7f18\u8ba1\u7b97\u63d0\u4f9b\u652f\u6301\uff0c\u4f46\u9700\u8981\u66f4\u9ad8\u7684\u9891\u8c31\u6548\u7387\u3001\u901a\u4fe1\u5b89\u5168\u548c\u51cf\u5c11\u7528\u6237\u95f4\u5e72\u6270\u3002", "method": "\u63d0\u51faNEEI\u8303\u5f0f\uff0c\u7ed3\u5408EEI\u548cNFC\uff0c\u5e76\u8bbe\u8ba1\u65e0\u7ebf\u7535\u53cb\u597d\u7684\u5177\u8eab\u89c4\u5212\u548c\u6ce2\u675f\u805a\u7126\u6280\u672f\uff0c\u901a\u8fc7\u534f\u4f5c\u5bfc\u822a\u5b9e\u73b0\u8d44\u6e90\u9ad8\u6548\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u6280\u672f\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u591a\u79cd\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "NEEI\u4e3a\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u8fdb\u4e00\u6b65\u4f18\u5316\u6269\u5c55\u5176\u5e94\u7528\u3002"}}
{"id": "2508.11261", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11261", "abs": "https://arxiv.org/abs/2508.11261", "authors": ["Shan Luo", "Nathan F. Lepora", "Wenzhen Yuan", "Kaspar Althoefer", "Gordon Cheng", "Ravinder Dahiya"], "title": "Tactile Robotics: An Outlook", "comment": "20 pages, 2 figures, accepted to IEEE Transactions on Robotics", "summary": "Robotics research has long sought to give robots the ability to perceive the\nphysical world through touch in an analogous manner to many biological systems.\nDeveloping such tactile capabilities is important for numerous emerging\napplications that require robots to co-exist and interact closely with humans.\nConsequently, there has been growing interest in tactile sensing, leading to\nthe development of various technologies, including piezoresistive and\npiezoelectric sensors, capacitive sensors, magnetic sensors, and optical\ntactile sensors. These diverse approaches utilise different transduction\nmethods and materials to equip robots with distributed sensing capabilities,\nenabling more effective physical interactions. These advances have been\nsupported in recent years by simulation tools that generate large-scale tactile\ndatasets to support sensor designs and algorithms to interpret and improve the\nutility of tactile data. The integration of tactile sensing with other\nmodalities, such as vision, as well as with action strategies for active\ntactile perception highlights the growing scope of this field. To further the\ntransformative progress in tactile robotics, a holistic approach is essential.\nIn this outlook article, we examine several challenges associated with the\ncurrent state of the art in tactile robotics and explore potential solutions to\ninspire innovations across multiple domains, including manufacturing,\nhealthcare, recycling and agriculture.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u4eba\u89e6\u89c9\u611f\u77e5\u6280\u672f\u7684\u53d1\u5c55\u73b0\u72b6\u3001\u6311\u6218\u53ca\u672a\u6765\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u5176\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u7c7b\u4f3c\u751f\u7269\u7cfb\u7edf\u7684\u89e6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u4ee5\u652f\u6301\u5176\u5728\u4eba\u7c7b\u5171\u5b58\u548c\u7d27\u5bc6\u4ea4\u4e92\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u7efc\u8ff0\u4e86\u591a\u79cd\u89e6\u89c9\u4f20\u611f\u6280\u672f\uff08\u5982\u538b\u963b\u3001\u538b\u7535\u3001\u7535\u5bb9\u3001\u78c1\u6027\u548c\u5149\u5b66\u4f20\u611f\u5668\uff09\u53ca\u5176\u96c6\u6210\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u4eff\u771f\u5de5\u5177\u548c\u591a\u6a21\u6001\u878d\u5408\u7684\u4f5c\u7528\u3002", "result": "\u89e6\u89c9\u611f\u77e5\u6280\u672f\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u5f53\u524d\u6311\u6218\u4ee5\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002", "conclusion": "\u9700\u91c7\u53d6\u6574\u4f53\u65b9\u6cd5\u63a8\u52a8\u89e6\u89c9\u673a\u5668\u4eba\u6280\u672f\u7684\u7a81\u7834\uff0c\u4ee5\u5728\u5236\u9020\u3001\u533b\u7597\u3001\u56de\u6536\u548c\u519c\u4e1a\u7b49\u9886\u57df\u5b9e\u73b0\u521b\u65b0\u3002"}}
{"id": "2508.11275", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11275", "abs": "https://arxiv.org/abs/2508.11275", "authors": ["Masaki Murooka", "Iori Kumagai", "Mitsuharu Morisawa", "Fumio Kanehiro"], "title": "Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation", "comment": null, "summary": "To reduce the computational cost of humanoid motion generation, we introduce\na new approach to representing robot kinematic reachability: the differentiable\nreachability map. This map is a scalar-valued function defined in the task\nspace that takes positive values only in regions reachable by the robot's\nend-effector. A key feature of this representation is that it is continuous and\ndifferentiable with respect to task-space coordinates, enabling its direct use\nas constraints in continuous optimization for humanoid motion planning. We\ndescribe a method to learn such differentiable reachability maps from a set of\nend-effector poses generated using a robot's kinematic model, using either a\nneural network or a support vector machine as the learning model. By\nincorporating the learned reachability map as a constraint, we formulate\nhumanoid motion generation as a continuous optimization problem. We demonstrate\nthat the proposed approach efficiently solves various motion planning problems,\nincluding footstep planning, multi-contact motion planning, and\nloco-manipulation planning for humanoid robots.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5fae\u53ef\u8fbe\u6027\u5730\u56fe\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u964d\u4f4e\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u7684\u7b97\u529b\u6210\u672c\u3002", "motivation": "\u51cf\u5c11\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u7684\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u6216\u652f\u6301\u5411\u91cf\u673a\u5b66\u4e60\u53ef\u5fae\u53ef\u8fbe\u6027\u5730\u56fe\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u7ea6\u675f\u7528\u4e8e\u8fde\u7eed\u4f18\u5316\u8fd0\u52a8\u89c4\u5212\u3002", "result": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u89e3\u51b3\u4e86\u591a\u79cd\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u5982\u6b65\u6001\u89c4\u5212\u3001\u591a\u63a5\u89e6\u8fd0\u52a8\u89c4\u5212\u548c\u64cd\u4f5c\u8fd0\u52a8\u89c4\u5212\u3002", "conclusion": "\u53ef\u5fae\u53ef\u8fbe\u6027\u5730\u56fe\u4e3a\u8fde\u7eed\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u7684\u6548\u7387\u3002"}}
{"id": "2508.11286", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11286", "abs": "https://arxiv.org/abs/2508.11286", "authors": ["Che Rin Yu", "Daewon Chae", "Dabin Seo", "Sangwon Lee", "Hyeongwoo Im", "Jinkyu Kim"], "title": "Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent", "comment": null, "summary": "When humans perform everyday tasks, we naturally adjust our actions based on\nthe current state of the environment. For instance, if we intend to put\nsomething into a drawer but notice it is closed, we open it first. However,\nmany autonomous robots lack this adaptive awareness. They often follow\npre-planned actions that may overlook subtle yet critical changes in the scene,\nwhich can result in actions being executed under outdated assumptions and\neventual failure. While replanning is critical for robust autonomy, most\nexisting methods respond only after failures occur, when recovery may be\ninefficient or infeasible. While proactive replanning holds promise for\npreventing failures in advance, current solutions often rely on manually\ndesigned rules and extensive supervision. In this work, we present a proactive\nreplanning framework that detects and corrects failures at subtask boundaries\nby comparing scene graphs constructed from current RGB-D observations against\nreference graphs extracted from successful demonstrations. When the current\nscene fails to align with reference trajectories, a lightweight reasoning\nmodule is activated to diagnose the mismatch and adjust the plan. Experiments\nin the AI2-THOR simulator demonstrate that our approach detects semantic and\nspatial mismatches before execution failures occur, significantly improving\ntask success and robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u91cd\u65b0\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u6bd4\u8f83\u5f53\u524d\u573a\u666f\u56fe\u4e0e\u6210\u529f\u6f14\u793a\u7684\u53c2\u8003\u56fe\uff0c\u5728\u5b50\u4efb\u52a1\u8fb9\u754c\u68c0\u6d4b\u5e76\u7ea0\u6b63\u6f5c\u5728\u5931\u8d25\uff0c\u63d0\u5347\u673a\u5668\u4eba\u4efb\u52a1\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4eba\u7c7b\u80fd\u6839\u636e\u73af\u5883\u72b6\u6001\u8c03\u6574\u884c\u52a8\uff0c\u800c\u81ea\u4e3b\u673a\u5668\u4eba\u5e38\u56e0\u7f3a\u4e4f\u9002\u5e94\u6027\u5bfc\u81f4\u5931\u8d25\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u88ab\u52a8\u54cd\u5e94\uff0c\u4e3b\u52a8\u89c4\u5212\u4f9d\u8d56\u4eba\u5de5\u89c4\u5219\u548c\u76d1\u7763\u3002", "method": "\u6784\u5efa\u5f53\u524dRGB-D\u89c2\u6d4b\u7684\u573a\u666f\u56fe\u4e0e\u6210\u529f\u6f14\u793a\u7684\u53c2\u8003\u56fe\uff0c\u5728\u5b50\u4efb\u52a1\u8fb9\u754c\u68c0\u6d4b\u4e0d\u5339\u914d\u65f6\u542f\u52a8\u8f7b\u91cf\u63a8\u7406\u6a21\u5757\u8c03\u6574\u8ba1\u5212\u3002", "result": "\u5728AI2-THOR\u6a21\u62df\u5668\u4e2d\u9a8c\u8bc1\uff0c\u80fd\u63d0\u524d\u68c0\u6d4b\u8bed\u4e49\u548c\u7a7a\u95f4\u4e0d\u5339\u914d\uff0c\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u4e3b\u52a8\u68c0\u6d4b\u548c\u8c03\u6574\uff0c\u6709\u6548\u9884\u9632\u6267\u884c\u5931\u8d25\uff0c\u4e3a\u673a\u5668\u4eba\u81ea\u9002\u5e94\u4efb\u52a1\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.11289", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11289", "abs": "https://arxiv.org/abs/2508.11289", "authors": ["Lin Li", "Xueming Liu", "Zhoujingzi Qiu", "Tianjiang Hu", "Qingrui Zhang"], "title": "A Recursive Total Least Squares Solution for Bearing-Only Target Motion Analysis and Circumnavigation", "comment": "Accepted by 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS), 6 Pages", "summary": "Bearing-only Target Motion Analysis (TMA) is a promising technique for\npassive tracking in various applications as a bearing angle is easy to measure.\nDespite its advantages, bearing-only TMA is challenging due to the nonlinearity\nof the bearing measurement model and the lack of range information, which\nimpairs observability and estimator convergence. This paper addresses these\nissues by proposing a Recursive Total Least Squares (RTLS) method for online\ntarget localization and tracking using mobile observers. The RTLS approach,\ninspired by previous results on Total Least Squares (TLS), mitigates biases in\nposition estimation and improves computational efficiency compared to\npseudo-linear Kalman filter (PLKF) methods. Additionally, we propose a\ncircumnavigation controller to enhance system observability and estimator\nconvergence by guiding the mobile observer in orbit around the target.\nExtensive simulations and experiments are performed to demonstrate the\neffectiveness and robustness of the proposed method. The proposed algorithm is\nalso compared with the state-of-the-art approaches, which confirms its superior\nperformance in terms of both accuracy and stability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9012\u5f52\u603b\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff08RTLS\uff09\u7684\u5728\u7ebf\u76ee\u6807\u5b9a\u4f4d\u4e0e\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u79fb\u52a8\u89c2\u6d4b\u5668\u63d0\u5347\u7cfb\u7edf\u53ef\u89c2\u6d4b\u6027\u548c\u4f30\u8ba1\u5668\u6536\u655b\u6027\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4ec5\u65b9\u4f4d\u76ee\u6807\u8fd0\u52a8\u5206\u6790\uff08TMA\uff09\u56e0\u6d4b\u91cf\u6a21\u578b\u975e\u7ebf\u6027\u548c\u7f3a\u4e4f\u8ddd\u79bb\u4fe1\u606f\u5bfc\u81f4\u53ef\u89c2\u6d4b\u6027\u548c\u4f30\u8ba1\u5668\u6536\u655b\u6027\u5dee\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u9012\u5f52\u603b\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff08RTLS\uff09\u548c\u73af\u7ed5\u5bfc\u822a\u63a7\u5236\u5668\uff0c\u4ee5\u51cf\u8f7b\u4f4d\u7f6e\u4f30\u8ba1\u504f\u5dee\u5e76\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "RTLS\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4ec5\u65b9\u4f4dTMA\u7684\u6311\u6218\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.11396", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11396", "abs": "https://arxiv.org/abs/2508.11396", "authors": ["Jingran Zhang", "Zhengzhang Yan", "Yiming Chen", "Zeqiang He", "Jiahao Chen"], "title": "Pedestrian Dead Reckoning using Invariant Extended Kalman Filter", "comment": null, "summary": "This paper presents a cost-effective inertial pedestrian dead reckoning\nmethod for the bipedal robot in the GPS-denied environment. Each time when the\ninertial measurement unit (IMU) is on the stance foot, a stationary\npseudo-measurement can be executed to provide innovation to the IMU measurement\nbased prediction. The matrix Lie group based theoretical development of the\nadopted invariant extended Kalman filter (InEKF) is set forth for tutorial\npurpose. Three experiments are conducted to compare between InEKF and standard\nEKF, including motion capture benchmark experiment, large-scale multi-floor\nwalking experiment, and bipedal robot experiment, as an effort to show our\nmethod's feasibility in real-world robot system. In addition, a sensitivity\nanalysis is included to show that InEKF is much easier to tune than EKF.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728GPS\u7f3a\u5931\u73af\u5883\u4e0b\u4e3a\u53cc\u8db3\u673a\u5668\u4eba\u8bbe\u8ba1\u7684\u4f4e\u6210\u672c\u60ef\u6027\u884c\u4eba\u822a\u4f4d\u63a8\u7b97\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f2a\u6d4b\u91cf\u6539\u8fdbIMU\u9884\u6d4b\uff0c\u5e76\u5c55\u793a\u4e86InEKF\u4f18\u4e8e\u6807\u51c6EKF\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002", "motivation": "\u5728GPS\u7f3a\u5931\u73af\u5883\u4e2d\uff0c\u53cc\u8db3\u673a\u5668\u4eba\u9700\u8981\u53ef\u9760\u7684\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u6210\u672c\u9ad8\u6216\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u5229\u7528IMU\u5728\u652f\u6491\u811a\u65f6\u7684\u4f2a\u6d4b\u91cf\u6539\u8fdb\u9884\u6d4b\uff0c\u91c7\u7528\u57fa\u4e8e\u77e9\u9635\u674e\u7fa4\u7684InEKF\u7406\u8bba\u3002", "result": "\u5b9e\u9a8c\u8868\u660eInEKF\u5728\u8fd0\u52a8\u6355\u6349\u3001\u591a\u697c\u5c42\u884c\u8d70\u548c\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u6807\u51c6EKF\uff0c\u4e14\u53c2\u6570\u8c03\u6574\u66f4\u7b80\u5355\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u53ef\u884c\uff0c\u4e14InEKF\u6bd4EKF\u66f4\u6613\u8c03\u53c2\u3002"}}
{"id": "2508.11404", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11404", "abs": "https://arxiv.org/abs/2508.11404", "authors": ["Junyeon Kim", "Tianshu Ruan", "Cesar Alan Contreras", "Manolis Chiou"], "title": "An Exploratory Study on Crack Detection in Concrete through Human-Robot Collaboration", "comment": null, "summary": "Structural inspection in nuclear facilities is vital for maintaining\noperational safety and integrity. Traditional methods of manual inspection pose\nsignificant challenges, including safety risks, high cognitive demands, and\npotential inaccuracies due to human limitations. Recent advancements in\nArtificial Intelligence (AI) and robotic technologies have opened new\npossibilities for safer, more efficient, and accurate inspection methodologies.\nSpecifically, Human-Robot Collaboration (HRC), leveraging robotic platforms\nequipped with advanced detection algorithms, promises significant improvements\nin inspection outcomes and reductions in human workload. This study explores\nthe effectiveness of AI-assisted visual crack detection integrated into a\nmobile Jackal robot platform. The experiment results indicate that HRC enhances\ninspection accuracy and reduces operator workload, resulting in potential\nsuperior performance outcomes compared to traditional manual methods.", "AI": {"tldr": "AI\u4e0e\u673a\u5668\u4eba\u6280\u672f\u7ed3\u5408\u7684\u4eba\u673a\u534f\u4f5c\uff08HRC\uff09\u5728\u6838\u8bbe\u65bd\u7ed3\u6784\u68c0\u6d4b\u4e2d\u5c55\u73b0\u51fa\u6bd4\u4f20\u7edf\u4eba\u5de5\u65b9\u6cd5\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u4f4e\u7684\u5de5\u4f5c\u8d1f\u62c5\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u5b89\u5168\u98ce\u9669\u3001\u8ba4\u77e5\u8d1f\u62c5\u9ad8\u548c\u4eba\u4e3a\u8bef\u5dee\u7b49\u95ee\u9898\uff0cAI\u548c\u673a\u5668\u4eba\u6280\u672f\u7684\u8fdb\u6b65\u4e3a\u66f4\u5b89\u5168\u3001\u9ad8\u6548\u3001\u51c6\u786e\u7684\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u7814\u7a76\u5c06AI\u8f85\u52a9\u7684\u89c6\u89c9\u88c2\u7eb9\u68c0\u6d4b\u96c6\u6210\u5230\u79fb\u52a8Jackal\u673a\u5668\u4eba\u5e73\u53f0\u4e2d\uff0c\u63a2\u7d22HRC\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHRC\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u4e86\u64cd\u4f5c\u5458\u7684\u5de5\u4f5c\u8d1f\u62c5\u3002", "conclusion": "HRC\u5728\u6838\u8bbe\u65bd\u7ed3\u6784\u68c0\u6d4b\u4e2d\u5177\u6709\u6f5c\u5728\u4f18\u8d8a\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u4eba\u5de5\u65b9\u6cd5\u3002"}}
{"id": "2508.11406", "categories": ["cs.RO", "cs.AI", "68T40", "I.2.9"], "pdf": "https://arxiv.org/pdf/2508.11406", "abs": "https://arxiv.org/abs/2508.11406", "authors": ["Benjamin Alt", "Mareike Picklum", "Sorin Arion", "Franklin Kenghagho Kenfack", "Michael Beetz"], "title": "Open, Reproducible and Trustworthy Robot-Based Experiments with Virtual Labs and Digital-Twin-Based Execution Tracing", "comment": "8 pages, 6 figures, submitted to the 1st IROS Workshop on Embodied AI\n  and Robotics for Future Scientific Discovery", "summary": "We envision a future in which autonomous robots conduct scientific\nexperiments in ways that are not only precise and repeatable, but also open,\ntrustworthy, and transparent. To realize this vision, we present two key\ncontributions: a semantic execution tracing framework that logs sensor data\ntogether with semantically annotated robot belief states, ensuring that\nautomated experimentation is transparent and replicable; and the AICOR Virtual\nResearch Building (VRB), a cloud-based platform for sharing, replicating, and\nvalidating robot task executions at scale. Together, these tools enable\nreproducible, robot-driven science by integrating deterministic execution,\nsemantic memory, and open knowledge representation, laying the foundation for\nautonomous systems to participate in scientific discovery.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u6267\u884c\u8ffd\u8e2a\u6846\u67b6\u548c\u4e91\u5e73\u53f0AICOR VRB\uff0c\u4ee5\u5b9e\u73b0\u900f\u660e\u3001\u53ef\u590d\u73b0\u7684\u673a\u5668\u4eba\u81ea\u4e3b\u79d1\u5b66\u5b9e\u9a8c\u3002", "motivation": "\u5b9e\u73b0\u673a\u5668\u4eba\u79d1\u5b66\u5b9e\u9a8c\u7684\u900f\u660e\u6027\u3001\u53ef\u4fe1\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u63a8\u52a8\u81ea\u4e3b\u7cfb\u7edf\u53c2\u4e0e\u79d1\u5b66\u53d1\u73b0\u3002", "method": "\u5f00\u53d1\u8bed\u4e49\u6267\u884c\u8ffd\u8e2a\u6846\u67b6\u8bb0\u5f55\u4f20\u611f\u5668\u6570\u636e\u548c\u673a\u5668\u4eba\u4fe1\u5ff5\u72b6\u6001\uff0c\u5e76\u6784\u5efa\u4e91\u5e73\u53f0AICOR VRB\u7528\u4e8e\u5171\u4eab\u548c\u9a8c\u8bc1\u4efb\u52a1\u6267\u884c\u3002", "result": "\u5de5\u5177\u6574\u5408\u4e86\u786e\u5b9a\u6027\u6267\u884c\u3001\u8bed\u4e49\u8bb0\u5fc6\u548c\u5f00\u653e\u77e5\u8bc6\u8868\u793a\uff0c\u652f\u6301\u53ef\u590d\u73b0\u7684\u673a\u5668\u4eba\u9a71\u52a8\u79d1\u5b66\u3002", "conclusion": "\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u53c2\u4e0e\u79d1\u5b66\u53d1\u73b0\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u900f\u660e\u548c\u53ef\u590d\u73b0\u7684\u5b9e\u9a8c\u65b9\u6cd5\u3002"}}
{"id": "2508.11453", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11453", "abs": "https://arxiv.org/abs/2508.11453", "authors": ["Jiayue Jin", "Lang Qian", "Jingyu Zhang", "Chuanyu Ju", "Liang Song"], "title": "EvoPSF: Online Evolution of Autonomous Driving Models via Planning-State Feedback", "comment": null, "summary": "Recent years have witnessed remarkable progress in autonomous driving, with\nsystems evolving from modular pipelines to end-to-end architectures. However,\nmost existing methods are trained offline and lack mechanisms to adapt to new\nenvironments during deployment. As a result, their generalization ability\ndiminishes when faced with unseen variations in real-world driving scenarios.\nIn this paper, we break away from the conventional \"train once, deploy forever\"\nparadigm and propose EvoPSF, a novel online Evolution framework for autonomous\ndriving based on Planning-State Feedback. We argue that planning failures are\nprimarily caused by inaccurate object-level motion predictions, and such\nfailures are often reflected in the form of increased planner uncertainty. To\naddress this, we treat planner uncertainty as a trigger for online evolution,\nusing it as a diagnostic signal to initiate targeted model updates. Rather than\nperforming blind updates, we leverage the planner's agent-agent attention to\nidentify the specific objects that the ego vehicle attends to most, which are\nprimarily responsible for the planning failures. For these critical objects, we\ncompute a targeted self-supervised loss by comparing their predicted waypoints\nfrom the prediction module with their actual future positions, selected from\nthe perception module's outputs with high confidence scores. This loss is then\nbackpropagated to adapt the model online. As a result, our method improves the\nmodel's robustness to environmental changes, leads to more precise motion\npredictions, and therefore enables more accurate and stable planning behaviors.\nExperiments on both cross-region and corrupted variants of the nuScenes dataset\ndemonstrate that EvoPSF consistently improves planning performance under\nchallenging conditions.", "AI": {"tldr": "EvoPSF\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5212\u72b6\u6001\u53cd\u9988\u7684\u5728\u7ebf\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u89c4\u5212\u5668\u4e0d\u786e\u5b9a\u6027\u89e6\u53d1\u6a21\u578b\u66f4\u65b0\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u591a\u4e3a\u79bb\u7ebf\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u5bf9\u65b0\u73af\u5883\u7684\u9002\u5e94\u673a\u5236\uff0c\u5bfc\u81f4\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u89c4\u5212\u5668\u4e0d\u786e\u5b9a\u6027\u8bca\u65ad\u5931\u8d25\u539f\u56e0\uff0c\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u8bc6\u522b\u5173\u952e\u5bf9\u8c61\uff0c\u8ba1\u7b97\u81ea\u76d1\u7763\u635f\u5931\u8fdb\u884c\u5728\u7ebf\u6a21\u578b\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEvoPSF\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u6027\u80fd\uff0c\u5c24\u5176\u5728\u8de8\u533a\u57df\u548c\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "EvoPSF\u901a\u8fc7\u5728\u7ebf\u8fdb\u5316\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u89c4\u5212\u51c6\u786e\u6027\u3002"}}
{"id": "2508.11479", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11479", "abs": "https://arxiv.org/abs/2508.11479", "authors": ["Tatiana Zemskova", "Aleksei Staroverov", "Dmitry Yudin", "Aleksandr Panov"], "title": "OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation", "comment": null, "summary": "Open-vocabulary Object Goal Navigation requires an embodied agent to reach\nobjects described by free-form language, including categories never seen during\ntraining. Existing end-to-end policies overfit small simulator datasets,\nachieving high success on training scenes but failing to generalize and\nexhibiting unsafe behaviour (frequent collisions). We introduce OVSegDT, a\nlightweight transformer policy that tackles these issues with two synergistic\ncomponents. The first component is the semantic branch, which includes an\nencoder for the target binary mask and an auxiliary segmentation loss function,\ngrounding the textual goal and providing precise spatial cues. The second\ncomponent consists of a proposed Entropy-Adaptive Loss Modulation, a per-sample\nscheduler that continuously balances imitation and reinforcement signals\naccording to the policy entropy, eliminating brittle manual phase switches.\nThese additions cut the sample complexity of training by 33%, and reduce\ncollision count in two times while keeping inference cost low (130M parameters,\nRGB-only input). On HM3D-OVON, our model matches the performance on unseen\ncategories to that on seen ones and establishes state-of-the-art results (40.1%\nSR, 20.9% SPL on val unseen) without depth, odometry, or large vision-language\nmodels. Code is available at https://github.com/CognitiveAISystems/OVSegDT.", "AI": {"tldr": "OVSegDT\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7Transformer\u7b56\u7565\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u652f\u548c\u71b5\u81ea\u9002\u5e94\u635f\u5931\u8c03\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u5bfc\u822a\u7684\u6027\u80fd\uff0c\u51cf\u5c11\u4e86\u78b0\u649e\u5e76\u964d\u4f4e\u4e86\u8bad\u7ec3\u590d\u6742\u5ea6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7aef\u5230\u7aef\u7b56\u7565\u5728\u5c0f\u89c4\u6a21\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u8fc7\u62df\u5408\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u53ca\u4e0d\u5b89\u5168\u884c\u4e3a\uff08\u9891\u7e41\u78b0\u649e\uff09\u7684\u95ee\u9898\u3002", "method": "1. \u8bed\u4e49\u5206\u652f\uff1a\u5305\u542b\u76ee\u6807\u4e8c\u8fdb\u5236\u63a9\u7801\u7f16\u7801\u5668\u548c\u8f85\u52a9\u5206\u5272\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u6587\u672c\u76ee\u6807\u7684\u7a7a\u95f4\u5b9a\u4f4d\u30022. \u71b5\u81ea\u9002\u5e94\u635f\u5931\u8c03\u5236\uff1a\u52a8\u6001\u5e73\u8861\u6a21\u4eff\u548c\u5f3a\u5316\u4fe1\u53f7\uff0c\u907f\u514d\u624b\u52a8\u5207\u6362\u3002", "result": "\u8bad\u7ec3\u6837\u672c\u590d\u6742\u5ea6\u964d\u4f4e33%\uff0c\u78b0\u649e\u6b21\u6570\u51cf\u5c11\u4e00\u534a\uff0c\u63a8\u7406\u6210\u672c\u4f4e\uff08130M\u53c2\u6570\uff0c\u4ec5RGB\u8f93\u5165\uff09\u3002\u5728HM3D-OVON\u4e0a\uff0c\u672a\u89c1\u7c7b\u522b\u6027\u80fd\u4e0e\u5df2\u89c1\u7c7b\u522b\u76f8\u5f53\uff0840.1% SR\uff0c20.9% SPL\uff09\u3002", "conclusion": "OVSegDT\u5728\u65e0\u9700\u6df1\u5ea6\u3001\u91cc\u7a0b\u8ba1\u6216\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u5bfc\u822a\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002"}}
{"id": "2508.11485", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11485", "abs": "https://arxiv.org/abs/2508.11485", "authors": ["Hailiang Tang", "Tisheng Zhang", "Liqiang Wang", "Xin Ding", "Man Yuan", "Zhiyu Xiang", "Jujin Chen", "Yuhan Bian", "Shuangyan Liu", "Yuqing Wang", "Guan Wang", "Xiaoji Niu"], "title": "i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping", "comment": "10 pages, 12 figures", "summary": "Accurate and reliable navigation is crucial for autonomous unmanned ground\nvehicle (UGV). However, current UGV datasets fall short in meeting the demands\nfor advancing navigation and mapping techniques due to limitations in sensor\nconfiguration, time synchronization, ground truth, and scenario diversity. To\naddress these challenges, we present i2Nav-Robot, a large-scale dataset\ndesigned for multi-sensor fusion navigation and mapping in indoor-outdoor\nenvironments. We integrate multi-modal sensors, including the newest front-view\nand 360-degree solid-state LiDARs, 4-dimensional (4D) radar, stereo cameras,\nodometer, global navigation satellite system (GNSS) receiver, and inertial\nmeasurement units (IMU) on an omnidirectional wheeled robot. Accurate\ntimestamps are obtained through both online hardware synchronization and\noffline calibration for all sensors. The dataset comprises ten larger-scale\nsequences covering diverse UGV operating scenarios, such as outdoor streets,\nand indoor parking lots, with a total length of about 17060 meters.\nHigh-frequency ground truth, with centimeter-level accuracy for position, is\nderived from post-processing integrated navigation methods using a\nnavigation-grade IMU. The proposed i2Nav-Robot dataset is evaluated by more\nthan ten open-sourced multi-sensor fusion systems, and it has proven to have\nsuperior data quality.", "AI": {"tldr": "i2Nav-Robot\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u4f20\u611f\u5668\u878d\u5408\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709UGV\u6570\u636e\u96c6\u5728\u4f20\u611f\u5668\u914d\u7f6e\u3001\u65f6\u95f4\u540c\u6b65\u3001\u5730\u9762\u771f\u5b9e\u6027\u548c\u573a\u666f\u591a\u6837\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4ee5\u63a8\u52a8\u5bfc\u822a\u548c\u5730\u56fe\u6280\u672f\u7684\u8fdb\u6b65\u3002", "motivation": "\u73b0\u6709UGV\u6570\u636e\u96c6\u5728\u4f20\u611f\u5668\u914d\u7f6e\u3001\u65f6\u95f4\u540c\u6b65\u3001\u5730\u9762\u771f\u5b9e\u6027\u548c\u573a\u666f\u591a\u6837\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5bfc\u822a\u548c\u5730\u56fe\u6280\u672f\u53d1\u5c55\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u591a\u6a21\u6001\u4f20\u611f\u5668\uff08\u5982LiDAR\u3001\u96f7\u8fbe\u3001\u76f8\u673a\u7b49\uff09\u5e76\u91c7\u7528\u786c\u4ef6\u540c\u6b65\u548c\u79bb\u7ebf\u6821\u51c6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u8986\u76d6\u5ba4\u5185\u5916\u591a\u6837\u5316\u573a\u666f\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b10\u4e2a\u5927\u89c4\u6a21\u5e8f\u5217\uff0c\u603b\u957f\u5ea6\u7ea617060\u7c73\uff0c\u5730\u9762\u771f\u5b9e\u6570\u636e\u5398\u7c73\u7ea7\u7cbe\u5ea6\uff0c\u5e76\u901a\u8fc7\u591a\u4f20\u611f\u5668\u878d\u5408\u7cfb\u7edf\u9a8c\u8bc1\u4e86\u5176\u6570\u636e\u8d28\u91cf\u4f18\u8d8a\u3002", "conclusion": "i2Nav-Robot\u6570\u636e\u96c6\u4e3a\u591a\u4f20\u611f\u5668\u878d\u5408\u5bfc\u822a\u548c\u5730\u56fe\u6280\u672f\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2508.11492", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11492", "abs": "https://arxiv.org/abs/2508.11492", "authors": ["Bozhou Zhang", "Nan Song", "Bingzhao Gao", "Li Zhang"], "title": "Relative Position Matters: Trajectory Prediction and Planning with Polar Representation", "comment": null, "summary": "Trajectory prediction and planning in autonomous driving are highly\nchallenging due to the complexity of predicting surrounding agents' movements\nand planning the ego agent's actions in dynamic environments. Existing methods\nencode map and agent positions and decode future trajectories in Cartesian\ncoordinates. However, modeling the relationships between the ego vehicle and\nsurrounding traffic elements in Cartesian space can be suboptimal, as it does\nnot naturally capture the varying influence of different elements based on\ntheir relative distances and directions. To address this limitation, we adopt\nthe Polar coordinate system, where positions are represented by radius and\nangle. This representation provides a more intuitive and effective way to model\nspatial changes and relative relationships, especially in terms of distance and\ndirectional influence. Based on this insight, we propose Polaris, a novel\nmethod that operates entirely in Polar coordinates, distinguishing itself from\nconventional Cartesian-based approaches. By leveraging the Polar\nrepresentation, this method explicitly models distance and direction variations\nand captures relative relationships through dedicated encoding and refinement\nmodules, enabling more structured and spatially aware trajectory prediction and\nplanning. Extensive experiments on the challenging prediction (Argoverse 2) and\nplanning benchmarks (nuPlan) demonstrate that Polaris achieves state-of-the-art\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6781\u5750\u6807\u7684\u65b0\u65b9\u6cd5Polaris\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8f68\u8ff9\u9884\u6d4b\u548c\u89c4\u5212\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u7b1b\u5361\u5c14\u5750\u6807\u7cfb\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7b1b\u5361\u5c14\u5750\u6807\u7cfb\u4e2d\u5efa\u6a21\u8f66\u8f86\u4e0e\u5468\u56f4\u4ea4\u901a\u5143\u7d20\u7684\u5173\u7cfb\u6548\u679c\u4e0d\u4f73\uff0c\u65e0\u6cd5\u81ea\u7136\u6355\u6349\u8ddd\u79bb\u548c\u65b9\u5411\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u6781\u5750\u6807\u7cfb\u8868\u793a\u4f4d\u7f6e\uff0c\u901a\u8fc7\u4e13\u7528\u7f16\u7801\u548c\u7ec6\u5316\u6a21\u5757\u5efa\u6a21\u8ddd\u79bb\u548c\u65b9\u5411\u53d8\u5316\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u9884\u6d4b\u548c\u89c4\u5212\u3002", "result": "\u5728Argoverse 2\u548cnuPlan\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPolaris\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u6781\u5750\u6807\u8868\u793a\u66f4\u76f4\u89c2\u6709\u6548\uff0cPolaris\u65b9\u6cd5\u5728\u8f68\u8ff9\u9884\u6d4b\u548c\u89c4\u5212\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.11498", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11498", "abs": "https://arxiv.org/abs/2508.11498", "authors": ["Agnes Bressan de Almeida", "Joao Aires Correa Fernandes Marsicano"], "title": "Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based Language", "comment": null, "summary": "Swarm in Blocks, originally developed for CopterHack 2022, is a high-level\ninterface that simplifies drone swarm programming using a block-based language.\nBuilding on the Clover platform, this tool enables users to create\nfunctionalities like loops and conditional structures by assembling code\nblocks. In 2023, we introduced Swarm in Blocks 2.0, further refining the\nplatform to address the complexities of swarm management in a user-friendly\nway. As drone swarm applications grow in areas like delivery, agriculture, and\nsurveillance, the challenge of managing them, especially for beginners, has\nalso increased. The Atena team developed this interface to make swarm handling\naccessible without requiring extensive knowledge of ROS or programming. The\nblock-based approach not only simplifies swarm control but also expands\neducational opportunities in programming.", "AI": {"tldr": "Swarm in Blocks 2.0\u662f\u4e00\u4e2a\u57fa\u4e8e\u5757\u7f16\u7a0b\u7684\u9ad8\u5c42\u63a5\u53e3\uff0c\u7b80\u5316\u4e86\u65e0\u4eba\u673a\u7fa4\u7f16\u7a0b\uff0c\u9002\u7528\u4e8e\u6559\u80b2\u548c\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u7fa4\u5728\u914d\u9001\u3001\u519c\u4e1a\u548c\u76d1\u63a7\u7b49\u9886\u57df\u7684\u5e94\u7528\u589e\u52a0\uff0c\u7ba1\u7406\u590d\u6742\u6027\u4e5f\u968f\u4e4b\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5bf9\u521d\u5b66\u8005\u3002Atena\u56e2\u961f\u5f00\u53d1\u6b64\u5de5\u5177\u4ee5\u964d\u4f4eROS\u548c\u7f16\u7a0b\u77e5\u8bc6\u95e8\u69db\u3002", "method": "\u57fa\u4e8eClover\u5e73\u53f0\uff0c\u4f7f\u7528\u5757\u7f16\u7a0b\u8bed\u8a00\u5b9e\u73b0\u529f\u80fd\uff08\u5982\u5faa\u73af\u548c\u6761\u4ef6\u7ed3\u6784\uff09\uff0c\u5e76\u901a\u8fc72.0\u7248\u672c\u8fdb\u4e00\u6b65\u4f18\u5316\u7fa4\u7ba1\u7406\u3002", "result": "\u8be5\u5de5\u5177\u7b80\u5316\u4e86\u7fa4\u63a7\u5236\uff0c\u6269\u5c55\u4e86\u7f16\u7a0b\u6559\u80b2\u673a\u4f1a\u3002", "conclusion": "Swarm in Blocks 2.0\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u53cb\u597d\u7684\u7fa4\u7ba1\u7406\u754c\u9762\uff0c\u964d\u4f4e\u4e86\u6280\u672f\u95e8\u69db\u3002"}}
{"id": "2508.11503", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11503", "abs": "https://arxiv.org/abs/2508.11503", "authors": ["Andrej Orsula", "Matthieu Geist", "Miguel Olivares-Mendez", "Carol Martinez"], "title": "Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media", "comment": "The source code is available at\n  https://github.com/AndrejOrsula/space_robotics_bench", "summary": "Reliable autonomous navigation across the unstructured terrains of distant\nplanetary surfaces is a critical enabler for future space exploration. However,\nthe deployment of learning-based controllers is hindered by the inherent\nsim-to-real gap, particularly for the complex dynamics of wheel interactions\nwith granular media. This work presents a complete sim-to-real framework for\ndeveloping and validating robust control policies for dynamic waypoint tracking\non such challenging surfaces. We leverage massively parallel simulation to\ntrain reinforcement learning agents across a vast distribution of procedurally\ngenerated environments with randomized physics. These policies are then\ntransferred zero-shot to a physical wheeled rover operating in a lunar-analogue\nfacility. Our experiments systematically compare multiple reinforcement\nlearning algorithms and action smoothing filters to identify the most effective\ncombinations for real-world deployment. Crucially, we provide strong empirical\nevidence that agents trained with procedural diversity achieve superior\nzero-shot performance compared to those trained on static scenarios. We also\nanalyze the trade-offs of fine-tuning with high-fidelity particle physics,\nwhich offers minor gains in low-speed precision at a significant computational\ncost. Together, these contributions establish a validated workflow for creating\nreliable learning-based navigation systems, marking a critical step towards\ndeploying autonomous robots in the final frontier.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u53d1\u53ef\u9760\u7684\u81ea\u4e3b\u5bfc\u822a\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5e76\u884c\u4eff\u771f\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u96f6\u6837\u672c\u90e8\u7f72\u3002", "motivation": "\u89e3\u51b3\u5728\u9065\u8fdc\u884c\u661f\u8868\u9762\u975e\u7ed3\u6784\u5316\u5730\u5f62\u4e0a\u53ef\u9760\u81ea\u4e3b\u5bfc\u822a\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u514b\u670d\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002", "method": "\u5229\u7528\u5927\u89c4\u6a21\u5e76\u884c\u4eff\u771f\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u901a\u8fc7\u7a0b\u5e8f\u751f\u6210\u591a\u6837\u5316\u73af\u5883\uff0c\u5e76\u5728\u771f\u5b9e\u8f6e\u5f0f\u6f2b\u6e38\u8f66\u4e0a\u96f6\u6837\u672c\u90e8\u7f72\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7a0b\u5e8f\u591a\u6837\u6027\u8bad\u7ec3\u7684\u4ee3\u7406\u5728\u96f6\u6837\u672c\u6027\u80fd\u4e0a\u4f18\u4e8e\u9759\u6001\u573a\u666f\u8bad\u7ec3\u7684\u4ee3\u7406\uff0c\u9ad8\u4fdd\u771f\u7c92\u5b50\u7269\u7406\u5fae\u8c03\u7684\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u521b\u5efa\u53ef\u9760\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u5bfc\u822a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9a8c\u8bc1\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u662f\u90e8\u7f72\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2508.11520", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.11520", "abs": "https://arxiv.org/abs/2508.11520", "authors": ["Evangelos Tsiatsianas", "Chairi Kiourt", "Konstantinos Chatzilygeroudis"], "title": "A Comparative Study of Floating-Base Space Parameterizations for Agile Whole-Body Motion Planning", "comment": "8 pages, 2 figures, 4 tables, Accepted at Humanoids 2025", "summary": "Automatically generating agile whole-body motions for legged and humanoid\nrobots remains a fundamental challenge in robotics. While numerous trajectory\noptimization approaches have been proposed, there is no clear guideline on how\nthe choice of floating-base space parameterization affects performance,\nespecially for agile behaviors involving complex contact dynamics. In this\npaper, we present a comparative study of different parameterizations for direct\ntranscription-based trajectory optimization of agile motions in legged systems.\nWe systematically evaluate several common choices under identical optimization\nsettings to ensure a fair comparison. Furthermore, we introduce a novel\nformulation based on the tangent space of SE(3) for representing the robot's\nfloating-base pose, which, to our knowledge, has not received attention from\nthe literature. This approach enables the use of mature off-the-shelf numerical\nsolvers without requiring specialized manifold optimization techniques. We hope\nthat our experiments and analysis will provide meaningful insights for\nselecting the appropriate floating-based representation for agile whole-body\nmotion generation.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e0d\u540c\u53c2\u6570\u5316\u65b9\u6cd5\u5728\u817f\u5f0f\u673a\u5668\u4eba\u654f\u6377\u8fd0\u52a8\u8f68\u8ff9\u4f18\u5316\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSE(3)\u5207\u7a7a\u95f4\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u817f\u5f0f\u548c\u4eba\u5f62\u673a\u5668\u4eba\u751f\u6210\u654f\u6377\u5168\u8eab\u8fd0\u52a8\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u6d6e\u52a8\u57fa\u5ea7\u53c2\u6570\u5316\u9009\u62e9\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u76f4\u63a5\u8f6c\u5f55\u6cd5\u6bd4\u8f83\u4e0d\u540c\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8eSE(3)\u5207\u7a7a\u95f4\u7684\u65b0\u53c2\u6570\u5316\u65b9\u6cd5\u3002", "result": "\u65b0\u65b9\u6cd5\u65e0\u9700\u4e13\u7528\u6d41\u5f62\u4f18\u5316\u6280\u672f\uff0c\u53ef\u76f4\u63a5\u4f7f\u7528\u6210\u719f\u6570\u503c\u6c42\u89e3\u5668\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9009\u62e9\u9002\u5408\u7684\u6d6e\u52a8\u57fa\u5ea7\u8868\u793a\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.11537", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11537", "abs": "https://arxiv.org/abs/2508.11537", "authors": ["Han Zheng", "Zikang Zhou", "Guli Zhang", "Zhepei Wang", "Kaixuan Wang", "Peiliang Li", "Shaojie Shen", "Ming Yang", "Tong Qin"], "title": "MultiPark: Multimodal Parking Transformer with Next-Segment Prediction", "comment": null, "summary": "Parking accurately and safely in highly constrained spaces remains a critical\nchallenge. Unlike structured driving environments, parking requires executing\ncomplex maneuvers such as frequent gear shifts and steering saturation. Recent\nattempts to employ imitation learning (IL) for parking have achieved promising\nresults. However, existing works ignore the multimodal nature of parking\nbehavior in lane-free open space, failing to derive multiple plausible\nsolutions under the same situation. Notably, IL-based methods encompass\ninherent causal confusion, so enabling a neural network to generalize across\ndiverse parking scenarios is particularly difficult. To address these\nchallenges, we propose MultiPark, an autoregressive transformer for multimodal\nparking. To handle paths filled with abrupt turning points, we introduce a\ndata-efficient next-segment prediction paradigm, enabling spatial\ngeneralization and temporal extrapolation. Furthermore, we design learnable\nparking queries factorized into gear, longitudinal, and lateral components,\nparallelly decoding diverse parking behaviors. To mitigate causal confusion in\nIL, our method employs target-centric pose and ego-centric collision as\noutcome-oriented loss across all modalities beyond pure imitation loss.\nEvaluations on real-world datasets demonstrate that MultiPark achieves\nstate-of-the-art performance across various scenarios. We deploy MultiPark on a\nproduction vehicle, further confirming our approach's robustness in real-world\nparking environments.", "AI": {"tldr": "MultiPark\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u56de\u5f52Transformer\u7684\u591a\u6a21\u6001\u505c\u8f66\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6bb5\u9884\u6d4b\u548c\u5206\u89e3\u67e5\u8be2\u89e3\u51b3\u505c\u8f66\u884c\u4e3a\u7684\u591a\u6837\u6027\u548c\u56e0\u679c\u6df7\u6dc6\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u9ad8\u6027\u80fd\u3002", "motivation": "\u505c\u8f66\u5728\u9ad8\u5ea6\u53d7\u9650\u7a7a\u95f4\u4e2d\u7684\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5ffd\u7565\u4e86\u505c\u8f66\u884c\u4e3a\u7684\u591a\u6a21\u6001\u6027\u4e14\u5b58\u5728\u56e0\u679c\u6df7\u6dc6\u95ee\u9898\u3002", "method": "\u63d0\u51faMultiPark\uff0c\u91c7\u7528\u81ea\u56de\u5f52Transformer\u548c\u5206\u6bb5\u9884\u6d4b\u8303\u5f0f\uff0c\u5206\u89e3\u505c\u8f66\u67e5\u8be2\u4e3a\u6863\u4f4d\u3001\u7eb5\u5411\u548c\u6a2a\u5411\u7ec4\u4ef6\uff0c\u5e76\u4f7f\u7528\u76ee\u6807\u4e2d\u5fc3\u4f4d\u59ff\u548c\u81ea\u6211\u4e2d\u5fc3\u78b0\u649e\u4f5c\u4e3a\u635f\u5931\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u8868\u660e\uff0cMultiPark\u5728\u5404\u79cd\u573a\u666f\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9645\u8f66\u8f86\u4e0a\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u3002", "conclusion": "MultiPark\u901a\u8fc7\u591a\u6a21\u6001\u5efa\u6a21\u548c\u56e0\u679c\u6df7\u6dc6\u7f13\u89e3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u505c\u8f66\u884c\u4e3a\u7684\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.11547", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11547", "abs": "https://arxiv.org/abs/2508.11547", "authors": ["Martin Jirou\u0161ek", "Tom\u00e1\u0161 B\u00e1\u010da", "Martin Saska"], "title": "Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads", "comment": null, "summary": "This paper addresses the problem of tracking the position of a\ncable-suspended payload carried by an unmanned aerial vehicle, with a focus on\nreal-world deployment and minimal hardware requirements. In contrast to many\nexisting approaches that rely on motion-capture systems, additional onboard\ncameras, or instrumented payloads, we propose a framework that uses only\nstandard onboard sensors--specifically, real-time kinematic global navigation\nsatellite system measurements and data from the onboard inertial measurement\nunit--to estimate and control the payload's position. The system models the\nfull coupled dynamics of the aerial vehicle and payload, and integrates a\nlinear Kalman filter for state estimation, a model predictive contouring\ncontrol planner, and an incremental model predictive controller. The control\narchitecture is designed to remain effective despite sensing limitations and\nestimation uncertainty. Extensive simulations demonstrate that the proposed\nsystem achieves performance comparable to control based on ground-truth\nmeasurements, with only minor degradation (< 6%). The system also shows strong\nrobustness to variations in payload parameters. Field experiments further\nvalidate the framework, confirming its practical applicability and reliable\nperformance in outdoor environments using only off-the-shelf aerial vehicle\nhardware.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ec5\u4f7f\u7528\u6807\u51c6\u673a\u8f7d\u4f20\u611f\u5668\uff08RTK-GNSS\u548cIMU\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f30\u8ba1\u548c\u63a7\u5236\u65e0\u4eba\u673a\u60ac\u6302\u8d1f\u8f7d\u7684\u4f4d\u7f6e\uff0c\u6027\u80fd\u63a5\u8fd1\u771f\u5b9e\u6d4b\u91cf\uff0c\u4e14\u786c\u4ef6\u9700\u6c42\u4f4e\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u786c\u4ef6\uff08\u5982\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u6216\u989d\u5916\u6444\u50cf\u5934\uff09\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u7ebf\u6027\u5361\u5c14\u66fc\u6ee4\u6ce2\u3001\u6a21\u578b\u9884\u6d4b\u8f6e\u5ed3\u63a7\u5236\u89c4\u5212\u5668\u548c\u589e\u91cf\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff0c\u5efa\u6a21\u65e0\u4eba\u673a\u4e0e\u8d1f\u8f7d\u7684\u8026\u5408\u52a8\u529b\u5b66\u3002", "result": "\u4eff\u771f\u663e\u793a\u6027\u80fd\u63a5\u8fd1\u771f\u5b9e\u6d4b\u91cf\uff08\u8bef\u5dee<6%\uff09\uff0c\u4e14\u5bf9\u8d1f\u8f7d\u53c2\u6570\u53d8\u5316\u5177\u6709\u5f3a\u9c81\u68d2\u6027\uff1b\u91ce\u5916\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u786c\u4ef6\u9700\u6c42\u4f4e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u6237\u5916\u90e8\u7f72\u3002"}}
{"id": "2508.11573", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11573", "abs": "https://arxiv.org/abs/2508.11573", "authors": ["Mogens Plessen"], "title": "Nominal Evaluation Of Automatic Multi-Sections Control Potential In Comparison To A Simpler One- Or Two-Sections Alternative With Predictive Spray Switching", "comment": "14 pages plus 7 pages appendix with additional figures, 18 main\n  figures, 3 tables", "summary": "Automatic Section Control (ASC) is a long-standing trend for spraying in\nagriculture. It promises to minimise spray overlap areas. The core idea is to\n(i) switch off spray nozzles on areas that have already been sprayed, and (ii)\nto dynamically adjust nozzle flow rates along the boom bar that holds the spray\nnozzles when velocities of boom sections vary during turn maneuvers. ASC is not\npossible without sensors, in particular for accurate positioning data. Spraying\nand the movement of modern wide boom bars are highly dynamic processes. In\naddition, many uncertainty factors have an effect such as cross wind drift,\nboom height, nozzle clogging in open-field conditions, and so forth. In view of\nthis complexity, the natural question arises if a simpler alternative exist.\nTherefore, an Automatic Multi-Sections Control method is compared to a proposed\nsimpler one- or two-sections alternative that uses predictive spray switching.\nThe comparison is provided under nominal conditions. Agricultural spraying is\nintrinsically linked to area coverage path planning and spray switching logic.\nCombinations of two area coverage path planning and switching logics as well as\nthree sections-setups are compared. The three sections-setups differ by\ncontrolling 48 sections, 2 sections or controlling all nozzles uniformly with\nthe same control signal as one single section. Methods are evaluated on 10\ndiverse real-world field examples, including non-convex field contours,\nfreeform mainfield lanes and multiple obstacle areas. A preferred method is\nsuggested that (i) minimises area coverage pathlength, (ii) offers intermediate\noverlap, (iii) is suitable for manual driving by following a pre-planned\npredictive spray switching logic for an area coverage path plan, and (iv) and\nin contrast to ASC can be implemented sensor-free and therefore at low cost.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u81ea\u52a8\u591a\u6bb5\u63a7\u5236\uff08ASC\uff09\u4e0e\u7b80\u5316\u7684\u4e00\u6216\u4e24\u6bb5\u9884\u6d4b\u55b7\u96fe\u5207\u6362\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u65e0\u4f20\u611f\u5668\u7684\u4f18\u9009\u65b9\u6848\u3002", "motivation": "\u63a2\u8ba8\u5728\u590d\u6742\u52a8\u6001\u7684\u519c\u4e1a\u55b7\u96fe\u8fc7\u7a0b\u4e2d\uff0c\u662f\u5426\u5b58\u5728\u6bd4\u4f20\u7edfASC\u66f4\u7b80\u5355\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u6bd4\u8f83\u4e09\u79cd\u5206\u6bb5\u8bbe\u7f6e\uff0848\u6bb5\u30012\u6bb5\u3001\u5355\u6bb5\uff09\u548c\u4e24\u79cd\u8def\u5f84\u89c4\u5212\u4e0e\u5207\u6362\u903b\u8f91\uff0c\u572810\u4e2a\u5b9e\u9645\u7530\u95f4\u6848\u4f8b\u4e2d\u8bc4\u4f30\u3002", "result": "\u4f18\u9009\u65b9\u6cd5\u5728\u8def\u5f84\u957f\u5ea6\u3001\u91cd\u53e0\u63a7\u5236\u548c\u6210\u672c\u6548\u76ca\u4e0a\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u7b80\u5316\u7684\u4e00\u6216\u4e24\u6bb5\u9884\u6d4b\u55b7\u96fe\u5207\u6362\u65b9\u6cd5\u5728\u4f4e\u6210\u672c\u548c\u65e0\u4f20\u611f\u5668\u9700\u6c42\u4e0b\u66f4\u5177\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.11584", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11584", "abs": "https://arxiv.org/abs/2508.11584", "authors": ["Jakub \u0141ucki", "Jonathan Becktor", "Georgios Georgakis", "Robert Royce", "Shehryar Khattak"], "title": "Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks", "comment": "6 pages, 6 figures, 2 tables", "summary": "Deploying multiple machine learning models on resource-constrained robotic\nplatforms for different perception tasks often results in redundant\ncomputations, large memory footprints, and complex integration challenges. In\nresponse, this work presents Visual Perception Engine (VPEngine), a modular\nframework designed to enable efficient GPU usage for visual multitasking while\nmaintaining extensibility and developer accessibility. Our framework\narchitecture leverages a shared foundation model backbone that extracts image\nrepresentations, which are efficiently shared, without any unnecessary GPU-CPU\nmemory transfers, across multiple specialized task-specific model heads running\nin parallel. This design eliminates the computational redundancy inherent in\nfeature extraction component when deploying traditional sequential models while\nenabling dynamic task prioritization based on application demands. We\ndemonstrate our framework's capabilities through an example implementation\nusing DINOv2 as the foundation model with multiple task (depth, object\ndetection and semantic segmentation) heads, achieving up to 3x speedup compared\nto sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine\noffers efficient GPU utilization and maintains a constant memory footprint\nwhile allowing per-task inference frequencies to be adjusted dynamically during\nruntime. The framework is written in Python and is open source with ROS2 C++\n(Humble) bindings for ease of use by the robotics community across diverse\nrobotic platforms. Our example implementation demonstrates end-to-end real-time\nperformance at $\\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized\nmodels.", "AI": {"tldr": "VPEngine\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u7684\u56fe\u50cf\u8868\u793a\uff0c\u5b9e\u73b0\u591a\u4efb\u52a1\u89c6\u89c9\u5904\u7406\u7684\u9ad8\u6548GPU\u5229\u7528\uff0c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u8fbe\u52303\u500d\u52a0\u901f\u3002", "motivation": "\u89e3\u51b3\u5728\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u90e8\u7f72\u591a\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u65f6\u51fa\u73b0\u7684\u8ba1\u7b97\u5197\u4f59\u3001\u5185\u5b58\u5360\u7528\u5927\u548c\u96c6\u6210\u590d\u6742\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5171\u4eab\u57fa\u7840\u6a21\u578b\uff08\u5982DINOv2\uff09\u63d0\u53d6\u56fe\u50cf\u8868\u793a\uff0c\u5e76\u884c\u8fd0\u884c\u591a\u4e2a\u4efb\u52a1\u4e13\u7528\u6a21\u578b\u5934\uff0c\u907f\u514dGPU-CPU\u5185\u5b58\u4f20\u8f93\uff0c\u652f\u6301\u52a8\u6001\u4efb\u52a1\u4f18\u5148\u7ea7\u8c03\u6574\u3002", "result": "\u5728NVIDIA Jetson Orin AGX\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\uff08\u226550 Hz\uff09\uff0c\u76f8\u6bd4\u987a\u5e8f\u6267\u884c\u901f\u5ea6\u63d0\u53473\u500d\u3002", "conclusion": "VPEngine\u901a\u8fc7\u9ad8\u6548GPU\u5229\u7528\u548c\u52a8\u6001\u4efb\u52a1\u7ba1\u7406\uff0c\u4e3a\u673a\u5668\u4eba\u89c6\u89c9\u591a\u4efb\u52a1\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6613\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11588", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11588", "abs": "https://arxiv.org/abs/2508.11588", "authors": ["Benjamin Walt", "Jordan Westphal", "Girish Krishnan"], "title": "Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation", "comment": null, "summary": "Effective and efficient agricultural manipulation and harvesting depend on\naccurately understanding the current state of the grasp. The agricultural\nenvironment presents unique challenges due to its complexity, clutter, and\nocclusion. Additionally, fruit is physically attached to the plant, requiring\nprecise separation during harvesting. Selecting appropriate sensors and\nmodeling techniques is critical for obtaining reliable feedback and correctly\nidentifying grasp states. This work investigates a set of key sensors, namely\ninertial measurement units (IMUs), infrared (IR) reflectance, tension, tactile\nsensors, and RGB cameras, integrated into a compliant gripper to classify grasp\nstates. We evaluate the individual contribution of each sensor and compare the\nperformance of two widely used classification models: Random Forest and Long\nShort-Term Memory (LSTM) networks. Our results demonstrate that a Random Forest\nclassifier, trained in a controlled lab environment and tested on real cherry\ntomato plants, achieved 100% accuracy in identifying slip, grasp failure, and\nsuccessful picks, marking a substantial improvement over baseline performance.\nFurthermore, we identify a minimal viable sensor combination, namely IMU and\ntension sensors that effectively classifies grasp states. This classifier\nenables the planning of corrective actions based on real-time feedback, thereby\nenhancing the efficiency and reliability of fruit harvesting operations.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u519c\u4e1a\u91c7\u6458\u4e2d\u6293\u53d6\u72b6\u6001\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u4f20\u611f\u5668\u548c\u4e24\u79cd\u5206\u7c7b\u6a21\u578b\uff0c\u53d1\u73b0\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u5728\u5b9e\u9a8c\u5ba4\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8fbe100%\u3002", "motivation": "\u519c\u4e1a\u73af\u5883\u7684\u590d\u6742\u6027\u3001\u906e\u6321\u548c\u679c\u5b9e\u9644\u7740\u7279\u6027\u8981\u6c42\u7cbe\u786e\u7684\u6293\u53d6\u72b6\u6001\u8bc6\u522b\uff0c\u4ee5\u63d0\u9ad8\u91c7\u6458\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "method": "\u7814\u7a76\u96c6\u6210\u4e86IMU\u3001\u7ea2\u5916\u53cd\u5c04\u3001\u5f20\u529b\u3001\u89e6\u89c9\u4f20\u611f\u5668\u548cRGB\u76f8\u673a\uff0c\u5e76\u4f7f\u7528\u968f\u673a\u68ee\u6797\u548cLSTM\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u5728\u5b9e\u9a8c\u5ba4\u548c\u5b9e\u9645\u6a31\u6843\u756a\u8304\u690d\u682a\u6d4b\u8bd5\u4e2d\u8fbe\u5230100%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6027\u80fd\u3002IMU\u548c\u5f20\u529b\u4f20\u611f\u5668\u7ec4\u5408\u88ab\u786e\u5b9a\u4e3a\u6700\u5c0f\u53ef\u884c\u65b9\u6848\u3002", "conclusion": "\u8be5\u5206\u7c7b\u5668\u901a\u8fc7\u5b9e\u65f6\u53cd\u9988\u652f\u6301\u7ea0\u6b63\u52a8\u4f5c\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u679c\u91c7\u6458\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002"}}
