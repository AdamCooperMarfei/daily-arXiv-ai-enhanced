{"id": "2506.18960", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18960", "abs": "https://arxiv.org/abs/2506.18960", "authors": ["Siqi Shang", "Mingyo Seo", "Yuke Zhu", "Lilly Chin"], "title": "FORTE: Tactile Force and Slip Sensing on Compliant Fingers for Delicate Manipulation", "comment": null, "summary": "Handling delicate and fragile objects remains a major challenge for robotic\nmanipulation, especially for rigid parallel grippers. While the simplicity and\nversatility of parallel grippers have led to widespread adoption, these\ngrippers are limited by their heavy reliance on visual feedback. Tactile\nsensing and soft robotics can add responsiveness and compliance. However,\nexisting methods typically involve high integration complexity or suffer from\nslow response times. In this work, we introduce FORTE, a tactile sensing system\nembedded in compliant gripper fingers. FORTE uses 3D-printed fin-ray grippers\nwith internal air channels to provide low-latency force and slip feedback.\nFORTE applies just enough force to grasp objects without damaging them, while\nremaining easy to fabricate and integrate. We find that FORTE can accurately\nestimate grasping forces from 0-8 N with an average error of 0.2 N, and detect\nslip events within 100 ms of occurring. We demonstrate FORTE's ability to grasp\na wide range of slippery, fragile, and deformable objects. In particular, FORTE\ngrasps fragile objects like raspberries and potato chips with a 98.6% success\nrate, and achieves 93% accuracy in detecting slip events. These results\nhighlight FORTE's potential as a robust and practical solution for enabling\ndelicate robotic manipulation. Project page: https://merge-lab.github.io/FORTE", "AI": {"tldr": "FORTE\u662f\u4e00\u79cd\u5d4c\u5165\u67d4\u6027\u5939\u722a\u7684\u89e6\u89c9\u4f20\u611f\u7cfb\u7edf\uff0c\u901a\u8fc73D\u6253\u5370\u7684\u9ccd\u5c04\u7ebf\u5939\u722a\u548c\u5185\u90e8\u6c14\u901a\u9053\u63d0\u4f9b\u4f4e\u5ef6\u8fdf\u7684\u529b\u548c\u6ed1\u52a8\u53cd\u9988\uff0c\u9002\u7528\u4e8e\u6293\u53d6\u6613\u788e\u7269\u4f53\u3002", "motivation": "\u89e3\u51b3\u521a\u6027\u5e73\u884c\u5939\u722a\u5728\u6293\u53d6\u6613\u788e\u7269\u4f53\u65f6\u8fc7\u5ea6\u4f9d\u8d56\u89c6\u89c9\u53cd\u9988\u7684\u95ee\u9898\uff0c\u7ed3\u5408\u89e6\u89c9\u4f20\u611f\u548c\u8f6f\u673a\u5668\u4eba\u6280\u672f\u63d0\u9ad8\u54cd\u5e94\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u4f7f\u75283D\u6253\u5370\u7684\u9ccd\u5c04\u7ebf\u5939\u722a\u548c\u5185\u90e8\u6c14\u901a\u9053\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u7684\u529b\u548c\u6ed1\u52a8\u53cd\u9988\uff0c\u786e\u4fdd\u6293\u53d6\u65f6\u65bd\u52a0\u7684\u529b\u9002\u4e2d\u4e14\u4e0d\u635f\u574f\u7269\u4f53\u3002", "result": "FORTE\u80fd\u51c6\u786e\u4f30\u8ba10-8N\u7684\u6293\u53d6\u529b\uff08\u5e73\u5747\u8bef\u5dee0.2N\uff09\uff0c\u5e76\u5728100ms\u5185\u68c0\u6d4b\u6ed1\u52a8\u4e8b\u4ef6\uff0c\u6293\u53d6\u6613\u788e\u7269\u4f53\u6210\u529f\u7387\u8fbe98.6%\uff0c\u6ed1\u52a8\u68c0\u6d4b\u51c6\u786e\u7387\u4e3a93%\u3002", "conclusion": "FORTE\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u6613\u788e\u7269\u4f53\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002"}}
{"id": "2506.19016", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19016", "abs": "https://arxiv.org/abs/2506.19016", "authors": ["Nancy Amato", "Stav Ashur", "Sariel Har-Peled%"], "title": "Faster Motion Planning via Restarts", "comment": "arXiv admin note: text overlap with arXiv:2503.04633", "summary": "Randomized methods such as PRM and RRT are widely used in motion planning.\nHowever, in some cases, their running-time suffers from inherent instability,\nleading to ``catastrophic'' performance even for relatively simple instances.\nWe apply stochastic restart techniques, some of them new, for speeding up Las\nVegas algorithms, that provide dramatic speedups in practice (a factor of $3$\n[or larger] in many cases).\n  Our experiments demonstrate that the new algorithms have faster runtimes,\nshorter paths, and greater gains from multi-threading (when compared with\nstraightforward parallel implementation). We prove the optimality of the new\nvariants. Our implementation is open source, available on github, and is easy\nto deploy and use.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u968f\u673a\u91cd\u542f\u6280\u672f\u6539\u8fdbPRM\u548cRRT\u7b49\u968f\u673a\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u8fd0\u884c\u901f\u5ea6\u3001\u8def\u5f84\u8d28\u91cf\u548c\u591a\u7ebf\u7a0b\u6027\u80fd\u3002", "motivation": "\u968f\u673a\u65b9\u6cd5\u5982PRM\u548cRRT\u5728\u8fd0\u52a8\u89c4\u5212\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u8fd0\u884c\u65f6\u95f4\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5e94\u7528\u968f\u673a\u91cd\u542f\u6280\u672f\uff08\u5305\u62ec\u65b0\u65b9\u6cd5\uff09\u52a0\u901fLas Vegas\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u65b0\u7b97\u6cd5\u8fd0\u884c\u66f4\u5feb\u3001\u8def\u5f84\u66f4\u77ed\uff0c\u591a\u7ebf\u7a0b\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u6613\u4e8e\u90e8\u7f72\u548c\u4f7f\u7528\u3002"}}
{"id": "2506.19077", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19077", "abs": "https://arxiv.org/abs/2506.19077", "authors": ["Christoph Willibald", "Daniel Sliwowski", "Dongheui Lee"], "title": "Multimodal Anomaly Detection with a Mixture-of-Experts", "comment": "8 pages, 5 figures, 1 table, the paper has been accepted for\n  publication in the Proceedings of the 2025 IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS 2025)", "summary": "With a growing number of robots being deployed across diverse applications,\nrobust multimodal anomaly detection becomes increasingly important. In robotic\nmanipulation, failures typically arise from (1) robot-driven anomalies due to\nan insufficient task model or hardware limitations, and (2) environment-driven\nanomalies caused by dynamic environmental changes or external interferences.\nConventional anomaly detection methods focus either on the first by low-level\nstatistical modeling of proprioceptive signals or the second by deep\nlearning-based visual environment observation, each with different\ncomputational and training data requirements. To effectively capture anomalies\nfrom both sources, we propose a mixture-of-experts framework that integrates\nthe complementary detection mechanisms with a visual-language model for\nenvironment monitoring and a Gaussian-mixture regression-based detector for\ntracking deviations in interaction forces and robot motions. We introduce a\nconfidence-based fusion mechanism that dynamically selects the most reliable\ndetector for each situation. We evaluate our approach on both household and\nindustrial tasks using two robotic systems, demonstrating a 60% reduction in\ndetection delay while improving frame-wise anomaly detection performance\ncompared to individual detectors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u9ad8\u65af\u6df7\u5408\u56de\u5f52\u68c0\u6d4b\u5668\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u591a\u6a21\u6001\u5f02\u5e38\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u5728\u591a\u6837\u5316\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u591a\u6a21\u6001\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u540c\u65f6\u5e94\u5bf9\u673a\u5668\u4eba\u9a71\u52a8\u548c\u73af\u5883\u9a71\u52a8\u7684\u5f02\u5e38\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u9ad8\u65af\u6df7\u5408\u56de\u5f52\u68c0\u6d4b\u5668\uff0c\u5e76\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u878d\u5408\u673a\u5236\u52a8\u6001\u9009\u62e9\u6700\u53ef\u9760\u7684\u68c0\u6d4b\u5668\u3002", "result": "\u5728\u5bb6\u5ead\u548c\u5de5\u4e1a\u4efb\u52a1\u4e2d\u6d4b\u8bd5\uff0c\u68c0\u6d4b\u5ef6\u8fdf\u51cf\u5c1160%\uff0c\u5e27\u7ea7\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u4f18\u4e8e\u5355\u4e00\u68c0\u6d4b\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u6574\u5408\u4e86\u4e92\u8865\u68c0\u6d4b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2506.19112", "categories": ["cs.RO", "math.DS"], "pdf": "https://arxiv.org/pdf/2506.19112", "abs": "https://arxiv.org/abs/2506.19112", "authors": ["Rom Levy", "Ari Dantus", "Zitao Yu", "Yizhar Or"], "title": "Analysis and experiments of the dissipative Twistcar: direction reversal and asymptotic approximations", "comment": null, "summary": "Underactuated wheeled vehicles are commonly studied as nonholonomic systems\nwith periodic actuation. Twistcar is a classical example inspired by a riding\ntoy, which has been analyzed using a planar model of a dynamical system with\nnonholonomic constraints. Most of the previous analyses did not account for\nenergy dissipation due to friction. In this work, we study a theoretical\ntwo-link model of the Twistcar while incorporating dissipation due to rolling\nresistance. We obtain asymptotic expressions for the system's small-amplitude\nsteady-state periodic dynamics, which reveals the possibility of reversing the\ndirection of motion upon varying the geometric and mass properties of the\nvehicle. Next, we design and construct a robotic prototype of the Twistcar\nwhose center-of-mass position can be shifted by adding and removing a massive\nblock, enabling demonstration of the Twistcar's direction reversal phenomenon.\nWe also conduct parameter fitting for the frictional resistance in order to\nimprove agreement with experiments.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e00\u79cd\u8003\u8651\u6eda\u52a8\u6469\u64e6\u7684Twistcar\u4e24\u8fde\u6746\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u901a\u8fc7\u6539\u53d8\u51e0\u4f55\u548c\u8d28\u91cf\u5c5e\u6027\u53ef\u5b9e\u73b0\u8fd0\u52a8\u65b9\u5411\u53cd\u8f6c\uff0c\u5e76\u6784\u5efa\u4e86\u673a\u5668\u4eba\u539f\u578b\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u8003\u8651\u6469\u64e6\u80fd\u91cf\u8017\u6563\uff0c\u672c\u7814\u7a76\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22\u4e86Twistcar\u5728\u6469\u64e6\u4f5c\u7528\u4e0b\u7684\u52a8\u529b\u5b66\u884c\u4e3a\u3002", "method": "\u5efa\u7acb\u7406\u8bba\u4e24\u8fde\u6746\u6a21\u578b\uff0c\u7ed3\u5408\u6eda\u52a8\u6469\u64e6\uff0c\u5206\u6790\u5c0f\u632f\u5e45\u7a33\u6001\u5468\u671f\u52a8\u529b\u5b66\uff0c\u5e76\u901a\u8fc7\u673a\u5668\u4eba\u539f\u578b\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0\u8fd0\u52a8\u65b9\u5411\u53cd\u8f6c\u73b0\u8c61\uff0c\u5e76\u901a\u8fc7\u53c2\u6570\u62df\u5408\u63d0\u9ad8\u4e86\u7406\u8bba\u4e0e\u5b9e\u9a8c\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u6469\u64e6\u5bf9Twistcar\u52a8\u529b\u5b66\u6709\u663e\u8457\u5f71\u54cd\uff0c\u65b9\u5411\u53cd\u8f6c\u73b0\u8c61\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.19121", "categories": ["cs.RO", "cs.AI", "cs.LG", "I.2.6; I.2.9"], "pdf": "https://arxiv.org/pdf/2506.19121", "abs": "https://arxiv.org/abs/2506.19121", "authors": ["Christopher Agia", "Rohan Sinha", "Jingyun Yang", "Rika Antonova", "Marco Pavone", "Haruki Nishimura", "Masha Itkina", "Jeannette Bohg"], "title": "CUPID: Curating Data your Robot Loves with Influence Functions", "comment": "Project page: https://cupid-curation.github.io. 28 pages, 15 figures", "summary": "In robot imitation learning, policy performance is tightly coupled with the\nquality and composition of the demonstration data. Yet, developing a precise\nunderstanding of how individual demonstrations contribute to downstream\noutcomes - such as closed-loop task success or failure - remains a persistent\nchallenge. We propose CUPID, a robot data curation method based on a novel\ninfluence function-theoretic formulation for imitation learning policies. Given\na set of evaluation rollouts, CUPID estimates the influence of each training\ndemonstration on the policy's expected return. This enables ranking and\nselection of demonstrations according to their impact on the policy's\nclosed-loop performance. We use CUPID to curate data by 1) filtering out\ntraining demonstrations that harm policy performance and 2) subselecting newly\ncollected trajectories that will most improve the policy. Extensive simulated\nand hardware experiments show that our approach consistently identifies which\ndata drives test-time performance. For example, training with less than 33% of\ncurated data can yield state-of-the-art diffusion policies on the simulated\nRoboMimic benchmark, with similar gains observed in hardware. Furthermore,\nhardware experiments show that our method can identify robust strategies under\ndistribution shift, isolate spurious correlations, and even enhance the\npost-training of generalist robot policies. Additional materials are made\navailable at: https://cupid-curation.github.io.", "AI": {"tldr": "CUPID\u662f\u4e00\u79cd\u57fa\u4e8e\u5f71\u54cd\u51fd\u6570\u7406\u8bba\u7684\u673a\u5668\u4eba\u6570\u636e\u7b5b\u9009\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u7684\u6027\u80fd\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u4e2d\u7b56\u7565\u6027\u80fd\u4e0e\u6f14\u793a\u6570\u636e\u8d28\u91cf\u5bc6\u5207\u76f8\u5173\uff0c\u4f46\u7406\u89e3\u5355\u4e2a\u6f14\u793a\u5bf9\u4efb\u52a1\u6210\u529f\u7684\u5f71\u54cd\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faCUPID\uff0c\u901a\u8fc7\u5f71\u54cd\u51fd\u6570\u7406\u8bba\u4f30\u8ba1\u6bcf\u4e2a\u8bad\u7ec3\u6f14\u793a\u5bf9\u7b56\u7565\u9884\u671f\u56de\u62a5\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u7b5b\u9009\u548c\u4f18\u5316\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCUPID\u80fd\u663e\u8457\u63d0\u5347\u7b56\u7565\u6027\u80fd\uff0c\u4ec5\u752833%\u7684\u7b5b\u9009\u6570\u636e\u5373\u53ef\u8fbe\u5230\u6700\u4f73\u6548\u679c\uff0c\u5e76\u5728\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u3002", "conclusion": "CUPID\u4e3a\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u7b5b\u9009\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.19179", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.19179", "abs": "https://arxiv.org/abs/2506.19179", "authors": ["Qiaoqiao Ren", "Tony Belpaeme"], "title": "Situated Haptic Interaction: Exploring the Role of Context in Affective Perception of Robotic Touch", "comment": null, "summary": "Affective interaction is not merely about recognizing emotions; it is an\nembodied, situated process shaped by context and co-created through\ninteraction. In affective computing, the role of haptic feedback within dynamic\nemotional exchanges remains underexplored. This study investigates how\nsituational emotional cues influence the perception and interpretation of\nhaptic signals given by a robot. In a controlled experiment, 32 participants\nwatched video scenarios in which a robot experienced either positive actions\n(such as being kissed), negative actions (such as being slapped) or neutral\nactions. After each video, the robot conveyed its emotional response through\nhaptic communication, delivered via a wearable vibration sleeve worn by the\nparticipant. Participants rated the robot's emotional state-its valence\n(positive or negative) and arousal (intensity)-based on the video, the haptic\nfeedback, and the combination of the two. The study reveals a dynamic interplay\nbetween visual context and touch. Participants' interpretation of haptic\nfeedback was strongly shaped by the emotional context of the video, with visual\ncontext often overriding the perceived valence of the haptic signal. Negative\nhaptic cues amplified the perceived valence of the interaction, while positive\ncues softened it. Furthermore, haptics override the participants' perception of\narousal of the video. Together, these results offer insights into how situated\nhaptic feedback can enrich affective human-robot interaction, pointing toward\nmore nuanced and embodied approaches to emotional communication with machines.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u60c5\u5883\u60c5\u7eea\u7ebf\u7d22\u5982\u4f55\u5f71\u54cd\u673a\u5668\u4eba\u89e6\u89c9\u4fe1\u53f7\u7684\u611f\u77e5\uff0c\u53d1\u73b0\u89c6\u89c9\u60c5\u5883\u5e38\u4e3b\u5bfc\u89e6\u89c9\u4fe1\u53f7\u7684\u6548\u4ef7\u611f\u77e5\uff0c\u89e6\u89c9\u53cd\u9988\u80fd\u589e\u5f3a\u6216\u51cf\u5f31\u4ea4\u4e92\u7684\u60c5\u611f\u5f3a\u5ea6\u3002", "motivation": "\u60c5\u611f\u8ba1\u7b97\u4e2d\u89e6\u89c9\u53cd\u9988\u5728\u52a8\u6001\u60c5\u611f\u4ea4\u6d41\u4e2d\u7684\u4f5c\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "32\u540d\u53c2\u4e0e\u8005\u89c2\u770b\u673a\u5668\u4eba\u7ecf\u5386\u4e0d\u540c\u60c5\u611f\u52a8\u4f5c\u7684\u89c6\u9891\uff0c\u5e76\u901a\u8fc7\u7a7f\u6234\u632f\u52a8\u8896\u63a5\u6536\u89e6\u89c9\u53cd\u9988\uff0c\u8bc4\u4f30\u673a\u5668\u4eba\u7684\u60c5\u611f\u72b6\u6001\u3002", "result": "\u89c6\u89c9\u60c5\u5883\u4e3b\u5bfc\u89e6\u89c9\u4fe1\u53f7\u7684\u6548\u4ef7\u611f\u77e5\uff0c\u8d1f\u9762\u89e6\u89c9\u7ebf\u7d22\u589e\u5f3a\u4ea4\u4e92\u6548\u4ef7\uff0c\u6b63\u9762\u7ebf\u7d22\u51cf\u5f31\u6548\u4ef7\uff0c\u89e6\u89c9\u53cd\u9988\u8fd8\u5f71\u54cd\u5524\u9192\u5ea6\u611f\u77e5\u3002", "conclusion": "\u60c5\u5883\u89e6\u89c9\u53cd\u9988\u80fd\u4e30\u5bcc\u4eba\u673a\u60c5\u611f\u4ea4\u4e92\uff0c\u4e3a\u673a\u5668\u60c5\u611f\u4ea4\u6d41\u63d0\u4f9b\u66f4\u7ec6\u81f4\u548c\u5177\u8eab\u5316\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.19201", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19201", "abs": "https://arxiv.org/abs/2506.19201", "authors": ["Hanyang Zhou", "Haozhe Lou", "Wenhao Liu", "Enyu Zhao", "Yue Wang", "Daniel Seita"], "title": "The MOTIF Hand: A Robotic Hand for Multimodal Observations with Thermal, Inertial, and Force Sensors", "comment": null, "summary": "Advancing dexterous manipulation with multi-fingered robotic hands requires\nrich sensory capabilities, while existing designs lack onboard thermal and\ntorque sensing. In this work, we propose the MOTIF hand, a novel multimodal and\nversatile robotic hand that extends the LEAP hand by integrating: (i) dense\ntactile information across the fingers, (ii) a depth sensor, (iii) a thermal\ncamera, (iv), IMU sensors, and (v) a visual sensor. The MOTIF hand is designed\nto be relatively low-cost (under 4000 USD) and easily reproducible. We validate\nour hand design through experiments that leverage its multimodal sensing for\ntwo representative tasks. First, we integrate thermal sensing into 3D\nreconstruction to guide temperature-aware, safe grasping. Second, we show how\nour hand can distinguish objects with identical appearance but different masses\n- a capability beyond methods that use vision only.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMOTIF\u7684\u591a\u6a21\u6001\u673a\u5668\u4eba\u624b\uff0c\u6269\u5c55\u4e86LEAP\u624b\u7684\u529f\u80fd\uff0c\u96c6\u6210\u4e86\u591a\u79cd\u4f20\u611f\u5668\uff0c\u7528\u4e8e\u589e\u5f3a\u7075\u5de7\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u624b\u7f3a\u4e4f\u70ed\u611f\u548c\u626d\u77e9\u611f\u6d4b\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86MOTIF\u624b\uff0c\u96c6\u6210\u4e86\u5bc6\u96c6\u89e6\u89c9\u4fe1\u606f\u3001\u6df1\u5ea6\u4f20\u611f\u5668\u3001\u70ed\u50cf\u4eea\u3001IMU\u4f20\u611f\u5668\u548c\u89c6\u89c9\u4f20\u611f\u5668\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MOTIF\u624b\u5728\u6e29\u5ea6\u611f\u77e5\u6293\u53d6\u548c\u533a\u5206\u5916\u89c2\u76f8\u540c\u4f46\u8d28\u91cf\u4e0d\u540c\u7684\u7269\u4f53\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "MOTIF\u624b\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u6613\u590d\u5236\u7684\u591a\u6a21\u6001\u673a\u5668\u4eba\u624b\uff0c\u6269\u5c55\u4e86\u673a\u5668\u4eba\u624b\u7684\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2506.19202", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.19202", "abs": "https://arxiv.org/abs/2506.19202", "authors": ["Claire Yang", "Heer Patel", "Max Kleiman-Weiner", "Maya Cakmak"], "title": "Preserving Sense of Agency: User Preferences for Robot Autonomy and User Control across Household Tasks", "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN)", "summary": "Roboticists often design with the assumption that assistive robots should be\nfully autonomous. However, it remains unclear whether users prefer highly\nautonomous robots, as prior work in assistive robotics suggests otherwise. High\nrobot autonomy can reduce the user's sense of agency, which represents feeling\nin control of one's environment. How much control do users, in fact, want over\nthe actions of robots used for in-home assistance? We investigate how robot\nautonomy levels affect users' sense of agency and the autonomy level they\nprefer in contexts with varying risks. Our study asked participants to rate\ntheir sense of agency as robot users across four distinct autonomy levels and\nranked their robot preferences with respect to various household tasks. Our\nfindings revealed that participants' sense of agency was primarily influenced\nby two factors: (1) whether the robot acts autonomously, and (2) whether a\nthird party is involved in the robot's programming or operation. Notably, an\nend-user programmed robot highly preserved users' sense of agency, even though\nit acts autonomously. However, in high-risk settings, e.g., preparing a snack\nfor a child with allergies, they preferred robots that prioritized their\ncontrol significantly more. Additional contextual factors, such as trust in a\nthird party operator, also shaped their preferences.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u7528\u6237\u5bf9\u8f85\u52a9\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u7684\u504f\u597d\u53ca\u5176\u5bf9\u7528\u6237\u63a7\u5236\u611f\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u81ea\u4e3b\u6027\u548c\u7b2c\u4e09\u65b9\u53c2\u4e0e\u662f\u5173\u952e\u56e0\u7d20\uff0c\u9ad8\u98ce\u9669\u4efb\u52a1\u4e2d\u7528\u6237\u66f4\u503e\u5411\u63a7\u5236\u3002", "motivation": "\u63a2\u8ba8\u7528\u6237\u662f\u5426\u504f\u597d\u9ad8\u5ea6\u81ea\u4e3b\u7684\u8f85\u52a9\u673a\u5668\u4eba\uff0c\u4ee5\u53ca\u81ea\u4e3b\u6027\u5982\u4f55\u5f71\u54cd\u7528\u6237\u7684\u63a7\u5236\u611f\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u8ba9\u53c2\u4e0e\u8005\u8bc4\u4f30\u4e0d\u540c\u81ea\u4e3b\u6027\u6c34\u5e73\u4e0b\u5bf9\u673a\u5668\u4eba\u7684\u63a7\u5236\u611f\uff0c\u5e76\u9488\u5bf9\u4e0d\u540c\u5bb6\u5ead\u4efb\u52a1\u6392\u540d\u504f\u597d\u3002", "result": "\u7528\u6237\u63a7\u5236\u611f\u53d7\u5230\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u548c\u7b2c\u4e09\u65b9\u53c2\u4e0e\u7684\u5f71\u54cd\uff1b\u9ad8\u98ce\u9669\u4efb\u52a1\u4e2d\u7528\u6237\u66f4\u503e\u5411\u63a7\u5236\u3002", "conclusion": "\u8f85\u52a9\u673a\u5668\u4eba\u8bbe\u8ba1\u9700\u5e73\u8861\u81ea\u4e3b\u6027\u4e0e\u7528\u6237\u63a7\u5236\u611f\uff0c\u5c24\u5176\u5728\u9ad8\u98ce\u9669\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2506.19212", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19212", "abs": "https://arxiv.org/abs/2506.19212", "authors": ["Vincent de Bakker", "Joey Hejna", "Tyler Ga Wei Lum", "Onur Celik", "Aleksandar Taranovic", "Denis Blessing", "Gerhard Neumann", "Jeannette Bohg", "Dorsa Sadigh"], "title": "Scaffolding Dexterous Manipulation with Vision-Language Models", "comment": null, "summary": "Dexterous robotic hands are essential for performing complex manipulation\ntasks, yet remain difficult to train due to the challenges of demonstration\ncollection and high-dimensional control. While reinforcement learning (RL) can\nalleviate the data bottleneck by generating experience in simulation, it\ntypically relies on carefully designed, task-specific reward functions, which\nhinder scalability and generalization. Thus, contemporary works in dexterous\nmanipulation have often bootstrapped from reference trajectories. These\ntrajectories specify target hand poses that guide the exploration of RL\npolicies and object poses that enable dense, task-agnostic rewards. However,\nsourcing suitable trajectories - particularly for dexterous hands - remains a\nsignificant challenge. Yet, the precise details in explicit reference\ntrajectories are often unnecessary, as RL ultimately refines the motion. Our\nkey insight is that modern vision-language models (VLMs) already encode the\ncommonsense spatial and semantic knowledge needed to specify tasks and guide\nexploration effectively. Given a task description (e.g., \"open the cabinet\")\nand a visual scene, our method uses an off-the-shelf VLM to first identify\ntask-relevant keypoints (e.g., handles, buttons) and then synthesize 3D\ntrajectories for hand motion and object motion. Subsequently, we train a\nlow-level residual RL policy in simulation to track these coarse trajectories\nor \"scaffolds\" with high fidelity. Across a number of simulated tasks involving\narticulated objects and semantic understanding, we demonstrate that our method\nis able to learn robust dexterous manipulation policies. Moreover, we showcase\nthat our method transfers to real-world robotic hands without any human\ndemonstrations or handcrafted rewards.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u751f\u6210\u4efb\u52a1\u76f8\u5173\u8f68\u8ff9\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u7075\u5de7\u673a\u5668\u4eba\u624b\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u65e0\u9700\u4eba\u5de5\u6f14\u793a\u6216\u624b\u5de5\u8bbe\u8ba1\u5956\u52b1\u3002", "motivation": "\u7075\u5de7\u673a\u5668\u4eba\u624b\u7684\u8bad\u7ec3\u9762\u4e34\u6570\u636e\u6536\u96c6\u548c\u9ad8\u7ef4\u63a7\u5236\u7684\u6311\u6218\uff0c\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u7684\u5956\u52b1\u51fd\u6570\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u73b0\u6210\u7684VLM\u8bc6\u522b\u4efb\u52a1\u5173\u952e\u70b9\u5e76\u5408\u62103D\u8f68\u8ff9\uff0c\u8bad\u7ec3\u4f4e\u5c42\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8ddf\u8e2a\u8fd9\u4e9b\u8f68\u8ff9\u3002", "result": "\u5728\u6a21\u62df\u4efb\u52a1\u4e2d\u6210\u529f\u5b66\u4e60\u5230\u9c81\u68d2\u7684\u7075\u5de7\u64cd\u4f5c\u7b56\u7565\uff0c\u5e76\u80fd\u8fc1\u79fb\u5230\u771f\u5b9e\u673a\u5668\u4eba\u624b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7VLM\u751f\u6210\u7684\u8f68\u8ff9\u89e3\u51b3\u4e86\u6570\u636e\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u7075\u5de7\u64cd\u4f5c\u8bad\u7ec3\u3002"}}
{"id": "2506.19269", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19269", "abs": "https://arxiv.org/abs/2506.19269", "authors": ["Ziyan Zhao", "Ke Fan", "He-Yang Xu", "Ning Qiao", "Bo Peng", "Wenlong Gao", "Dongjiang Li", "Hui Shen"], "title": "AnchorDP3: 3D Affordance Guided Sparse Diffusion Policy for Robotic Manipulation", "comment": null, "summary": "We present AnchorDP3, a diffusion policy framework for dual-arm robotic\nmanipulation that achieves state-of-the-art performance in highly randomized\nenvironments. AnchorDP3 integrates three key innovations: (1)\nSimulator-Supervised Semantic Segmentation, using rendered ground truth to\nexplicitly segment task-critical objects within the point cloud, which provides\nstrong affordance priors; (2) Task-Conditioned Feature Encoders, lightweight\nmodules processing augmented point clouds per task, enabling efficient\nmulti-task learning through a shared diffusion-based action expert; (3)\nAffordance-Anchored Keypose Diffusion with Full State Supervision, replacing\ndense trajectory prediction with sparse, geometrically meaningful action\nanchors, i.e., keyposes such as pre-grasp pose, grasp pose directly anchored to\naffordances, drastically simplifying the prediction space; the action expert is\nforced to predict both robot joint angles and end-effector poses\nsimultaneously, which exploits geometric consistency to accelerate convergence\nand boost accuracy. Trained on large-scale, procedurally generated simulation\ndata, AnchorDP3 achieves a 98.7% average success rate in the RoboTwin benchmark\nacross diverse tasks under extreme randomization of objects, clutter, table\nheight, lighting, and backgrounds. This framework, when integrated with the\nRoboTwin real-to-sim pipeline, has the potential to enable fully autonomous\ngeneration of deployable visuomotor policies from only scene and instruction,\ntotally eliminating human demonstrations from learning manipulation skills.", "AI": {"tldr": "AnchorDP3\u662f\u4e00\u79cd\u7528\u4e8e\u53cc\u81c2\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6269\u6563\u7b56\u7565\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9879\u521b\u65b0\u6280\u672f\u5728\u9ad8\u5ea6\u968f\u673a\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5728\u9ad8\u5ea6\u968f\u673a\u5316\u73af\u5883\u4e2d\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6311\u6218\uff0c\u51cf\u5c11\u5bf9\u4eba\u7c7b\u793a\u8303\u7684\u4f9d\u8d56\u3002", "method": "\u7ed3\u5408\u6a21\u62df\u5668\u76d1\u7763\u8bed\u4e49\u5206\u5272\u3001\u4efb\u52a1\u6761\u4ef6\u7279\u5f81\u7f16\u7801\u5668\u548c\u57fa\u4e8e\u5173\u952e\u59ff\u52bf\u7684\u6269\u6563\u7b56\u7565\uff0c\u7b80\u5316\u9884\u6d4b\u7a7a\u95f4\u5e76\u63d0\u5347\u6548\u7387\u3002", "result": "\u5728RoboTwin\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523098.7%\u7684\u5e73\u5747\u6210\u529f\u7387\u3002", "conclusion": "AnchorDP3\u6846\u67b6\u6709\u671b\u5b9e\u73b0\u5b8c\u5168\u81ea\u4e3b\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u751f\u6210\uff0c\u65e0\u9700\u4eba\u7c7b\u793a\u8303\u3002"}}
{"id": "2506.19277", "categories": ["cs.RO", "cs.SY", "eess.SY", "68T40, 93C41", "I.2.9; I.2.8; F.2.2"], "pdf": "https://arxiv.org/pdf/2506.19277", "abs": "https://arxiv.org/abs/2506.19277", "authors": ["Jaehong Oh"], "title": "Ontology Neural Network and ORTSF: A Framework for Topological Reasoning and Delay-Robust Control", "comment": "12 pages, 5 figures, includes theoretical proofs and simulation\n  results", "summary": "The advancement of autonomous robotic systems has led to impressive\ncapabilities in perception, localization, mapping, and control. Yet, a\nfundamental gap remains: existing frameworks excel at geometric reasoning and\ndynamic stability but fall short in representing and preserving relational\nsemantics, contextual reasoning, and cognitive transparency essential for\ncollaboration in dynamic, human-centric environments. This paper introduces a\nunified architecture comprising the Ontology Neural Network (ONN) and the\nOntological Real-Time Semantic Fabric (ORTSF) to address this gap. The ONN\nformalizes relational semantic reasoning as a dynamic topological process. By\nembedding Forman-Ricci curvature, persistent homology, and semantic tensor\nstructures within a unified loss formulation, ONN ensures that relational\nintegrity and topological coherence are preserved as scenes evolve over time.\nThe ORTSF transforms reasoning traces into actionable control commands while\ncompensating for system delays. It integrates predictive and delay-aware\noperators that ensure phase margin preservation and continuity of control\nsignals, even under significant latency conditions. Empirical studies\ndemonstrate the ONN + ORTSF framework's ability to unify semantic cognition and\nrobust control, providing a mathematically principled and practically viable\nsolution for cognitive robotics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u672c\u4f53\u795e\u7ecf\u7f51\u7edc\uff08ONN\uff09\u548c\u672c\u4f53\u5b9e\u65f6\u8bed\u4e49\u6846\u67b6\uff08ORTSF\uff09\u7684\u7edf\u4e00\u67b6\u6784\uff0c\u4ee5\u89e3\u51b3\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5173\u7cfb\u8bed\u4e49\u548c\u8ba4\u77e5\u900f\u660e\u5ea6\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u51e0\u4f55\u63a8\u7406\u548c\u52a8\u6001\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5173\u7cfb\u8bed\u4e49\u8868\u793a\u3001\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u8ba4\u77e5\u900f\u660e\u5ea6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u963b\u788d\u4e86\u5176\u5728\u52a8\u6001\u3001\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u80fd\u529b\u3002", "method": "ONN\u901a\u8fc7\u52a8\u6001\u62d3\u6251\u8fc7\u7a0b\u5f62\u5f0f\u5316\u5173\u7cfb\u8bed\u4e49\u63a8\u7406\uff0c\u5d4c\u5165Forman-Ricci\u66f2\u7387\u3001\u6301\u4e45\u540c\u8c03\u548c\u8bed\u4e49\u5f20\u91cf\u7ed3\u6784\uff0c\u786e\u4fdd\u5173\u7cfb\u5b8c\u6574\u6027\u548c\u62d3\u6251\u4e00\u81f4\u6027\u3002ORTSF\u5c06\u63a8\u7406\u75d5\u8ff9\u8f6c\u5316\u4e3a\u63a7\u5236\u547d\u4ee4\uff0c\u5e76\u8865\u507f\u7cfb\u7edf\u5ef6\u8fdf\uff0c\u786e\u4fdd\u63a7\u5236\u4fe1\u53f7\u7684\u8fde\u7eed\u6027\u548c\u76f8\u4f4d\u88d5\u5ea6\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0cONN + ORTSF\u6846\u67b6\u80fd\u591f\u7edf\u4e00\u8bed\u4e49\u8ba4\u77e5\u548c\u9c81\u68d2\u63a7\u5236\uff0c\u4e3a\u8ba4\u77e5\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6570\u5b66\u4e0a\u4e25\u8c28\u4e14\u5b9e\u9645\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u6846\u67b6\u586b\u8865\u4e86\u73b0\u6709\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u8bed\u4e49\u548c\u8ba4\u77e5\u80fd\u529b\u4e0a\u7684\u7a7a\u767d\uff0c\u4e3a\u52a8\u6001\u3001\u4eba\u673a\u534f\u4f5c\u73af\u5883\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.19303", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19303", "abs": "https://arxiv.org/abs/2506.19303", "authors": ["Zexiang Guo", "Hengxiang Chen", "Xinheng Mai", "Qiusang Qiu", "Gan Ma", "Zhanat Kappassov", "Qiang Li", "Nutan Chen"], "title": "Robotic Perception with a Large Tactile-Vision-Language Model for Physical Property Inference", "comment": "This paper has been accepted by the 2025 International Conference on\n  Climbing and Walking Robots (CLAWAR). These authors contributed equally to\n  this work: Zexiang Guo, Hengxiang Chen, Xinheng Mai", "summary": "Inferring physical properties can significantly enhance robotic manipulation\nby enabling robots to handle objects safely and efficiently through adaptive\ngrasping strategies. Previous approaches have typically relied on either\ntactile or visual data, limiting their ability to fully capture properties. We\nintroduce a novel cross-modal perception framework that integrates visual\nobservations with tactile representations within a multimodal vision-language\nmodel. Our physical reasoning framework, which employs a hierarchical feature\nalignment mechanism and a refined prompting strategy, enables our model to make\nproperty-specific predictions that strongly correlate with ground-truth\nmeasurements. Evaluated on 35 diverse objects, our approach outperforms\nexisting baselines and demonstrates strong zero-shot generalization. Keywords:\ntactile perception, visual-tactile fusion, physical property inference,\nmultimodal integration, robot perception", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u6a21\u6001\u611f\u77e5\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u6570\u636e\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u673a\u5668\u4eba\u5bf9\u7269\u4f53\u7269\u7406\u5c5e\u6027\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u901a\u8fc7\u66f4\u5168\u9762\u5730\u6355\u6349\u7269\u4f53\u7269\u7406\u5c5e\u6027\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5b89\u5168\u9ad8\u6548\u64cd\u4f5c\u7269\u4f53\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7279\u5f81\u5bf9\u9f50\u673a\u5236\u548c\u6539\u8fdb\u7684\u63d0\u793a\u7b56\u7565\uff0c\u6574\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u6570\u636e\u3002", "result": "\u572835\u79cd\u4e0d\u540c\u7269\u4f53\u4e0a\u8bc4\u4f30\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8de8\u6a21\u6001\u611f\u77e5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5bf9\u7269\u4f53\u7269\u7406\u5c5e\u6027\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.19350", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.19350", "abs": "https://arxiv.org/abs/2506.19350", "authors": ["Carsten Reiners", "Minh Trinh", "Lukas Gr\u00fcndel", "Sven Tauchmann", "David Bitterolf", "Oliver Petrovic", "Christian Brecher"], "title": "Zero-Shot Parameter Learning of Robot Dynamics Using Bayesian Statistics and Prior Knowledge", "comment": "Carsten Reiners and Minh Trinh contributed equally to this work", "summary": "Inertial parameter identification of industrial robots is an established\nprocess, but standard methods using Least Squares or Machine Learning do not\nconsider prior information about the robot and require extensive measurements.\nInspired by Bayesian statistics, this paper presents an identification method\nwith improved generalization that incorporates prior knowledge and is able to\nlearn with only a few or without additional measurements (Zero-Shot Learning).\nFurthermore, our method is able to correctly learn not only the inertial but\nalso the mechanical and base parameters of the MABI Max 100 robot while\nensuring physical feasibility and specifying the confidence intervals of the\nresults. We also provide different types of priors for serial robots with 6\ndegrees of freedom, where datasheets or CAD models are not available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u7edf\u8ba1\u7684\u60ef\u6027\u53c2\u6570\u8bc6\u522b\u65b9\u6cd5\uff0c\u7ed3\u5408\u5148\u9a8c\u77e5\u8bc6\uff0c\u51cf\u5c11\u6d4b\u91cf\u9700\u6c42\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u5b66\u4e60\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u6700\u5c0f\u4e8c\u4e58\u6cd5\u6216\u673a\u5668\u5b66\u4e60\uff09\u672a\u5145\u5206\u5229\u7528\u673a\u5668\u4eba\u5148\u9a8c\u4fe1\u606f\u4e14\u9700\u5927\u91cf\u6d4b\u91cf\uff0c\u9650\u5236\u4e86\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u7edf\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7\u5c11\u91cf\u6216\u65e0\u989d\u5916\u6d4b\u91cf\uff08\u96f6\u6837\u672c\u5b66\u4e60\uff09\u8bc6\u522b\u60ef\u6027\u3001\u673a\u68b0\u548c\u57fa\u7840\u53c2\u6570\u3002", "result": "\u6210\u529f\u8bc6\u522bMABI Max 100\u673a\u5668\u4eba\u7684\u53c2\u6570\uff0c\u786e\u4fdd\u7269\u7406\u53ef\u884c\u6027\u5e76\u63d0\u4f9b\u7f6e\u4fe1\u533a\u95f4\uff1b\u8fd8\u4e3a6\u81ea\u7531\u5ea6\u4e32\u8054\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5148\u9a8c\u7c7b\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u53c2\u6570\u8bc6\u522b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5148\u9a8c\u4fe1\u606f\u6709\u9650\u6216\u65e0CAD\u6a21\u578b\u7684\u573a\u666f\u3002"}}
{"id": "2506.19397", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19397", "abs": "https://arxiv.org/abs/2506.19397", "authors": ["Zixi Chen", "Di Wu", "Qinghua Guan", "David Hardman", "Federico Renda", "Josie Hughes", "Thomas George Thuruthel", "Cosimo Della Santina", "Barbara Mazzolai", "Huichan Zhao", "Cesare Stefanini"], "title": "A Survey on Soft Robot Adaptability: Implementations, Applications, and Prospects", "comment": "12 pages, 4 figures, accepted by IEEE Robotics & Automation Magazine", "summary": "Soft robots, compared to rigid robots, possess inherent advantages, including\nhigher degrees of freedom, compliance, and enhanced safety, which have\ncontributed to their increasing application across various fields. Among these\nbenefits, adaptability is particularly noteworthy. In this paper, adaptability\nin soft robots is categorized into external and internal adaptability. External\nadaptability refers to the robot's ability to adjust, either passively or\nactively, to variations in environments, object properties, geometries, and\ntask dynamics. Internal adaptability refers to the robot's ability to cope with\ninternal variations, such as manufacturing tolerances or material aging, and to\ngeneralize control strategies across different robots. As the field of soft\nrobotics continues to evolve, the significance of adaptability has become\nincreasingly pronounced. In this review, we summarize various approaches to\nenhancing the adaptability of soft robots, including design, sensing, and\ncontrol strategies. Additionally, we assess the impact of adaptability on\napplications such as surgery, wearable devices, locomotion, and manipulation.\nWe also discuss the limitations of soft robotics adaptability and prospective\ndirections for future research. By analyzing adaptability through the lenses of\nimplementation, application, and challenges, this paper aims to provide a\ncomprehensive understanding of this essential characteristic in soft robotics\nand its implications for diverse applications.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u9002\u5e94\u6027\u7684\u5206\u7c7b\u3001\u5b9e\u73b0\u65b9\u6cd5\u53ca\u5176\u5e94\u7528\uff0c\u5e76\u63a2\u8ba8\u4e86\u5f53\u524d\u5c40\u9650\u6027\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u56e0\u5176\u9ad8\u81ea\u7531\u5ea6\u3001\u67d4\u987a\u6027\u548c\u5b89\u5168\u6027\u5728\u591a\u4e2a\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u9002\u5e94\u6027\u662f\u5176\u5173\u952e\u7279\u6027\u4e4b\u4e00\u3002\u672c\u6587\u65e8\u5728\u5168\u9762\u5206\u6790\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u9002\u5e94\u6027\uff0c\u4ee5\u63a8\u52a8\u5176\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "method": "\u5c06\u9002\u5e94\u6027\u5206\u4e3a\u5916\u90e8\u548c\u5185\u90e8\u4e24\u7c7b\uff0c\u603b\u7ed3\u4e86\u901a\u8fc7\u8bbe\u8ba1\u3001\u4f20\u611f\u548c\u63a7\u5236\u7b56\u7565\u63d0\u5347\u9002\u5e94\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u5728\u624b\u672f\u3001\u53ef\u7a7f\u6234\u8bbe\u5907\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002", "result": "\u9002\u5e94\u6027\u5728\u8f6f\u4f53\u673a\u5668\u4eba\u4e2d\u5177\u6709\u663e\u8457\u91cd\u8981\u6027\uff0c\u4f46\u5176\u5b9e\u73b0\u4ecd\u9762\u4e34\u5c40\u9650\uff0c\u5982\u6750\u6599\u8001\u5316\u548c\u63a7\u5236\u7b56\u7565\u7684\u901a\u7528\u6027\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u514b\u670d\u73b0\u6709\u5c40\u9650\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u9002\u5e94\u6027\uff0c\u4ee5\u62d3\u5c55\u5176\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2506.19424", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19424", "abs": "https://arxiv.org/abs/2506.19424", "authors": ["Tiankai Yang", "Kaixin Chai", "Jialin Ji", "Yuze Wu", "Chao Xu", "Fei Gao"], "title": "Ground-Effect-Aware Modeling and Control for Multicopters", "comment": null, "summary": "The ground effect on multicopters introduces several challenges, such as\ncontrol errors caused by additional lift, oscillations that may occur during\nnear-ground flight due to external torques, and the influence of ground airflow\non models such as the rotor drag and the mixing matrix. This article collects\nand analyzes the dynamics data of near-ground multicopter flight through\nvarious methods, including force measurement platforms and real-world flights.\nFor the first time, we summarize the mathematical model of the external torque\nof multicopters under ground effect. The influence of ground airflow on rotor\ndrag and the mixing matrix is also verified through adequate experimentation\nand analysis. Through simplification and derivation, the differential flatness\nof the multicopter's dynamic model under ground effect is confirmed. To\nmitigate the influence of these disturbance models on control, we propose a\ncontrol method that combines dynamic inverse and disturbance models, ensuring\nconsistent control effectiveness at both high and low altitudes. In this\nmethod, the additional thrust and variations in rotor drag under ground effect\nare both considered and compensated through feedforward models. The leveling\ntorque of ground effect can be equivalently represented as variations in the\ncenter of gravity and the moment of inertia. In this way, the leveling torque\ndoes not explicitly appear in the dynamic model. The final experimental results\nshow that the method proposed in this paper reduces the control error (RMSE) by\n\\textbf{45.3\\%}. Please check the supplementary material at:\nhttps://github.com/ZJU-FAST-Lab/Ground-effect-controller.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u65cb\u7ffc\u98de\u884c\u5668\u5728\u8fd1\u5730\u98de\u884c\u65f6\u7684\u5730\u9762\u6548\u5e94\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u52a8\u6001\u9006\u548c\u6270\u52a8\u6a21\u578b\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a7\u5236\u8bef\u5dee\u3002", "motivation": "\u5730\u9762\u6548\u5e94\u5bf9\u591a\u65cb\u7ffc\u98de\u884c\u5668\u5e26\u6765\u989d\u5916\u5347\u529b\u3001\u5916\u90e8\u626d\u77e9\u5f15\u8d77\u7684\u632f\u8361\u4ee5\u53ca\u5730\u9762\u6c14\u6d41\u5bf9\u6a21\u578b\u7684\u5f71\u54cd\u7b49\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u3002", "method": "\u901a\u8fc7\u529b\u6d4b\u91cf\u5e73\u53f0\u548c\u5b9e\u9645\u98de\u884c\u6536\u96c6\u6570\u636e\uff0c\u603b\u7ed3\u5730\u9762\u6548\u5e94\u4e0b\u7684\u5916\u90e8\u626d\u77e9\u6570\u5b66\u6a21\u578b\uff0c\u9a8c\u8bc1\u5730\u9762\u6c14\u6d41\u5bf9\u8f6c\u5b50\u963b\u529b\u548c\u6df7\u5408\u77e9\u9635\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u52a8\u6001\u9006\u4e0e\u6270\u52a8\u6a21\u578b\u7ed3\u5408\u7684\u63a7\u5236\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u63a7\u5236\u8bef\u5dee\uff08RMSE\uff09\u964d\u4f4e\u4e8645.3%\u3002", "conclusion": "\u63d0\u51fa\u7684\u63a7\u5236\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5730\u9762\u6548\u5e94\u5bf9\u591a\u65cb\u7ffc\u98de\u884c\u5668\u63a7\u5236\u7684\u5f71\u54cd\uff0c\u786e\u4fdd\u4e86\u9ad8\u4f4e\u7a7a\u98de\u884c\u65f6\u7684\u4e00\u81f4\u6027\u63a7\u5236\u6548\u679c\u3002"}}
{"id": "2506.19498", "categories": ["cs.RO", "cs.AI", "I.2.9; I.2.10; I.4.8; H.5.2"], "pdf": "https://arxiv.org/pdf/2506.19498", "abs": "https://arxiv.org/abs/2506.19498", "authors": ["Yiteng Chen", "Wenbo Li", "Shiyi Wang", "Huiping Zhuang", "Qingyao Wu"], "title": "T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models", "comment": "submitted to NeurIPS 2025", "summary": "Building a general robotic manipulation system capable of performing a wide\nvariety of tasks in real-world settings is a challenging task. Vision-Language\nModels (VLMs) have demonstrated remarkable potential in robotic manipulation\ntasks, primarily due to the extensive world knowledge they gain from\nlarge-scale datasets. In this process, Spatial Representations (such as points\nrepresenting object positions or vectors representing object orientations) act\nas a bridge between VLMs and real-world scene, effectively grounding the\nreasoning abilities of VLMs and applying them to specific task scenarios.\nHowever, existing VLM-based robotic approaches often adopt a fixed spatial\nrepresentation extraction scheme for various tasks, resulting in insufficient\nrepresentational capability or excessive extraction time. In this work, we\nintroduce T-Rex, a Task-Adaptive Framework for Spatial Representation\nExtraction, which dynamically selects the most appropriate spatial\nrepresentation extraction scheme for each entity based on specific task\nrequirements. Our key insight is that task complexity determines the types and\ngranularity of spatial representations, and Stronger representational\ncapabilities are typically associated with Higher overall system operation\ncosts. Through comprehensive experiments in real-world robotic environments, we\nshow that our approach delivers significant advantages in spatial\nunderstanding, efficiency, and stability without additional training.", "AI": {"tldr": "T-Rex\u662f\u4e00\u4e2a\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u7a7a\u95f4\u8868\u793a\u63d0\u53d6\u6846\u67b6\uff0c\u52a8\u6001\u9009\u62e9\u6700\u9002\u5408\u4efb\u52a1\u9700\u6c42\u7684\u8868\u793a\u65b9\u6848\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6548\u7387\u4e0e\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u673a\u5668\u4eba\u65b9\u6cd5\u91c7\u7528\u56fa\u5b9a\u7a7a\u95f4\u8868\u793a\u65b9\u6848\uff0c\u5bfc\u81f4\u8868\u793a\u80fd\u529b\u4e0d\u8db3\u6216\u63d0\u53d6\u65f6\u95f4\u8fc7\u957f\u3002", "method": "\u63d0\u51faT-Rex\u6846\u67b6\uff0c\u6839\u636e\u4efb\u52a1\u590d\u6742\u5ea6\u52a8\u6001\u9009\u62e9\u7a7a\u95f4\u8868\u793a\u7c7b\u578b\u548c\u7c92\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cT-Rex\u5728\u7a7a\u95f4\u7406\u89e3\u3001\u6548\u7387\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "T-Rex\u901a\u8fc7\u4efb\u52a1\u81ea\u9002\u5e94\u8868\u793a\u63d0\u53d6\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.19579", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19579", "abs": "https://arxiv.org/abs/2506.19579", "authors": ["Federico Tavella", "Kathryn Mearns", "Angelo Cangelosi"], "title": "Fake or Real, Can Robots Tell? Evaluating Embodied Vision-Language Models on Real and 3D-Printed Objects", "comment": null, "summary": "Robotic scene understanding increasingly relies on vision-language models\n(VLMs) to generate natural language descriptions of the environment. In this\nwork, we present a comparative study of captioning strategies for tabletop\nscenes captured by a robotic arm equipped with an RGB camera. The robot\ncollects images of objects from multiple viewpoints, and we evaluate several\nmodels that generate scene descriptions. We compare the performance of various\ncaptioning models, like BLIP and VLMs. Our experiments examine the trade-offs\nbetween single-view and multi-view captioning, and difference between\nrecognising real-world and 3D printed objects. We quantitatively evaluate\nobject identification accuracy, completeness, and naturalness of the generated\ncaptions. Results show that VLMs can be used in robotic settings where common\nobjects need to be recognised, but fail to generalise to novel representations.\nOur findings provide practical insights into deploying foundation models for\nembodied agents in real-world settings.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u673a\u5668\u4eba\u573a\u666f\u7406\u89e3\u4e2d\u4e0d\u540c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u6807\u9898\u751f\u6210\u7b56\u7565\uff0c\u8bc4\u4f30\u4e86\u5355\u89c6\u89d2\u4e0e\u591a\u89c6\u89d2\u3001\u771f\u5b9e\u7269\u4f53\u4e0e3D\u6253\u5370\u7269\u4f53\u7684\u8bc6\u522b\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u673a\u5668\u4eba\u5982\u4f55\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u751f\u6210\u73af\u5883\u63cf\u8ff0\uff0c\u4ee5\u63d0\u5347\u573a\u666f\u7406\u89e3\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u673a\u5668\u4eba\u624b\u81c2\u91c7\u96c6\u591a\u89c6\u89d2\u56fe\u50cf\uff0c\u6bd4\u8f83BLIP\u548cVLMs\u7b49\u6a21\u578b\u7684\u6807\u9898\u751f\u6210\u6027\u80fd\uff0c\u5206\u6790\u5355\u89c6\u89d2\u4e0e\u591a\u89c6\u89d2\u3001\u771f\u5b9e\u7269\u4f53\u4e0e3D\u6253\u5370\u7269\u4f53\u7684\u5dee\u5f02\u3002", "result": "VLMs\u5728\u5e38\u89c1\u7269\u4f53\u8bc6\u522b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u65b0\u9896\u8868\u5f81\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5b9e\u9645\u90e8\u7f72\u57fa\u7840\u6a21\u578b\u4e8e\u673a\u5668\u4eba\u573a\u666f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2506.19597", "categories": ["cs.RO", "cs.AI", "cs.AR", "cs.ET", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.19597", "abs": "https://arxiv.org/abs/2506.19597", "authors": ["Haruki Uchiito", "Akhilesh Bhat", "Koji Kusaka", "Xiaoya Zhang", "Hiraku Kinjo", "Honoka Uehara", "Motoki Koyama", "Shinji Natsume"], "title": "Robotics Under Construction: Challenges on Job Sites", "comment": "Workshop on Field Robotics, ICRA", "summary": "As labor shortages and productivity stagnation increasingly challenge the\nconstruction industry, automation has become essential for sustainable\ninfrastructure development. This paper presents an autonomous payload\ntransportation system as an initial step toward fully unmanned construction\nsites. Our system, based on the CD110R-3 crawler carrier, integrates autonomous\nnavigation, fleet management, and GNSS-based localization to facilitate\nmaterial transport in construction site environments. While the current system\ndoes not yet incorporate dynamic environment adaptation algorithms, we have\nbegun fundamental investigations into external-sensor based perception and\nmapping system. Preliminary results highlight the potential challenges,\nincluding navigation in evolving terrain, environmental perception under\nconstruction-specific conditions, and sensor placement optimization for\nimproving autonomy and efficiency. Looking forward, we envision a construction\necosystem where collaborative autonomous agents dynamically adapt to site\nconditions, optimizing workflow and reducing human intervention. This paper\nprovides foundational insights into the future of robotics-driven construction\nautomation and identifies critical areas for further technological development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCD110R-3\u5c65\u5e26\u5f0f\u8fd0\u8f93\u8f66\u7684\u81ea\u4e3b\u8f7d\u8377\u8fd0\u8f93\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u5efa\u7b51\u884c\u4e1a\u52b3\u52a8\u529b\u77ed\u7f3a\u548c\u751f\u4ea7\u529b\u505c\u6ede\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u65e0\u4eba\u5efa\u7b51\u5de5\u5730\u7684\u53ef\u80fd\u6027\u3002", "motivation": "\u5efa\u7b51\u884c\u4e1a\u9762\u4e34\u52b3\u52a8\u529b\u77ed\u7f3a\u548c\u751f\u4ea7\u529b\u505c\u6ede\u7684\u6311\u6218\uff0c\u81ea\u52a8\u5316\u6210\u4e3a\u53ef\u6301\u7eed\u57fa\u7840\u8bbe\u65bd\u53d1\u5c55\u7684\u5173\u952e\u3002", "method": "\u7cfb\u7edf\u96c6\u6210\u4e86\u81ea\u4e3b\u5bfc\u822a\u3001\u8f66\u961f\u7ba1\u7406\u548c\u57fa\u4e8eGNSS\u7684\u5b9a\u4f4d\u6280\u672f\uff0c\u7528\u4e8e\u5efa\u7b51\u5de5\u5730\u73af\u5883\u4e2d\u7684\u6750\u6599\u8fd0\u8f93\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u63ed\u793a\u4e86\u5bfc\u822a\u3001\u73af\u5883\u611f\u77e5\u548c\u4f20\u611f\u5668\u4f18\u5316\u7b49\u6f5c\u5728\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u4e3a\u673a\u5668\u4eba\u9a71\u52a8\u7684\u5efa\u7b51\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u57fa\u7840\u6027\u89c1\u89e3\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u6280\u672f\u53d1\u5c55\u7684\u5173\u952e\u65b9\u5411\u3002"}}
{"id": "2506.19602", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19602", "abs": "https://arxiv.org/abs/2506.19602", "authors": ["Leonardo Zamora Yanez", "Jacob Rogatinsky", "Dominic Recco", "Sang-Yoep Lee", "Grace Matthews", "Andrew P. Sabelhaus", "Tommaso Ranzani"], "title": "Soft Robotic Delivery of Coiled Anchors for Cardiac Interventions", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Trans-catheter cardiac intervention has become an increasingly available\noption for high-risk patients without the complications of open heart surgery.\nHowever, current catheterbased platforms suffer from a lack of dexterity, force\napplication, and compliance required to perform complex intracardiac\nprocedures. An exemplary task that would significantly ease minimally invasive\nintracardiac procedures is the implantation of anchor coils, which can be used\nto fix and implant various devices in the beating heart. We introduce a robotic\nplatform capable of delivering anchor coils. We develop a kineto-statics model\nof the robotic platform and demonstrate low positional error. We leverage the\npassive compliance and high force output of the actuator in a multi-anchor\ndelivery procedure against a motile in-vitro simulator with millimeter level\naccuracy.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u5fc3\u810f\u5185\u951a\u5b9a\u7ebf\u5708\u690d\u5165\u7684\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5bfc\u7ba1\u5e73\u53f0\u7075\u6d3b\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5bfc\u7ba1\u5e73\u53f0\u5728\u590d\u6742\u5fc3\u810f\u5185\u624b\u672f\u4e2d\u7f3a\u4e4f\u7075\u6d3b\u6027\u3001\u65bd\u529b\u548c\u987a\u5e94\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u9ad8\u98ce\u9669\u60a3\u8005\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u5efa\u7acb\u4e86\u5176\u8fd0\u52a8\u9759\u529b\u5b66\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u4f4e\u4f4d\u7f6e\u8bef\u5dee\u3002", "result": "\u5728\u591a\u951a\u5b9a\u690d\u5165\u8fc7\u7a0b\u4e2d\uff0c\u673a\u5668\u4eba\u5e73\u53f0\u8868\u73b0\u51fa\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u673a\u5668\u4eba\u5e73\u53f0\u4e3a\u590d\u6742\u5fc3\u810f\u5185\u624b\u672f\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.19620", "categories": ["cs.RO", "cs.FL", "cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.19620", "abs": "https://arxiv.org/abs/2506.19620", "authors": ["Mustafa Adam", "Kangfeng Ye", "David A. Anisi", "Ana Cavalcanti", "Jim Woodcock", "Robert Morris"], "title": "Probabilistic modelling and safety assurance of an agriculture robot providing light-treatment", "comment": null, "summary": "Continued adoption of agricultural robots postulates the farmer's trust in\nthe reliability, robustness and safety of the new technology. This motivates\nour work on safety assurance of agricultural robots, particularly their ability\nto detect, track and avoid obstacles and humans. This paper considers a\nprobabilistic modelling and risk analysis framework for use in the early\ndevelopment phases. Starting off with hazard identification and a risk\nassessment matrix, the behaviour of the mobile robot platform, sensor and\nperception system, and any humans present are captured using three state\nmachines. An auto-generated probabilistic model is then solved and analysed\nusing the probabilistic model checker PRISM. The result provides unique insight\ninto fundamental development and engineering aspects by quantifying the effect\nof the risk mitigation actions and risk reduction associated with distinct\ndesign concepts. These include implications of adopting a higher performance\nand more expensive Object Detection System or opting for a more elaborate\nwarning system to increase human awareness. Although this paper mainly focuses\non the initial concept-development phase, the proposed safety assurance\nframework can also be used during implementation, and subsequent deployment and\noperation phases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u519c\u4e1a\u673a\u5668\u4eba\u5b89\u5168\u4fdd\u8bc1\u7684\u6982\u7387\u5efa\u6a21\u548c\u98ce\u9669\u5206\u6790\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u68c0\u6d4b\u3001\u8ddf\u8e2a\u548c\u907f\u969c\u80fd\u529b\u3002", "motivation": "\u519c\u6c11\u5bf9\u519c\u4e1a\u673a\u5668\u4eba\u53ef\u9760\u6027\u3001\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u7684\u4fe1\u4efb\u662f\u5176\u6301\u7eed\u91c7\u7528\u7684\u5173\u952e\uff0c\u56e0\u6b64\u9700\u8981\u786e\u4fdd\u5176\u5b89\u5168\u6027\u3002", "method": "\u901a\u8fc7\u5371\u9669\u8bc6\u522b\u548c\u98ce\u9669\u8bc4\u4f30\u77e9\u9635\uff0c\u4f7f\u7528\u4e09\u4e2a\u72b6\u6001\u673a\u5efa\u6a21\u673a\u5668\u4eba\u5e73\u53f0\u3001\u4f20\u611f\u5668\u7cfb\u7edf\u53ca\u4eba\u7c7b\u884c\u4e3a\uff0c\u5e76\u5229\u7528PRISM\u6982\u7387\u6a21\u578b\u68c0\u67e5\u5668\u5206\u6790\u3002", "result": "\u91cf\u5316\u4e86\u98ce\u9669\u7f13\u89e3\u63aa\u65bd\u7684\u6548\u679c\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u8bbe\u8ba1\u6982\u5ff5\uff08\u5982\u9ad8\u6027\u80fd\u7269\u4f53\u68c0\u6d4b\u7cfb\u7edf\u6216\u66f4\u590d\u6742\u7684\u8b66\u544a\u7cfb\u7edf\uff09\u7684\u98ce\u9669\u964d\u4f4e\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u9002\u7528\u4e8e\u6982\u5ff5\u5f00\u53d1\u9636\u6bb5\uff0c\u8fd8\u53ef\u7528\u4e8e\u5b9e\u65bd\u3001\u90e8\u7f72\u548c\u64cd\u4f5c\u9636\u6bb5\u3002"}}
{"id": "2506.19622", "categories": ["cs.RO", "cs.FL", "cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.19622", "abs": "https://arxiv.org/abs/2506.19622", "authors": ["Mustafa Adam", "David A. Anisi", "Pedro Ribeiro"], "title": "A Verification Methodology for Safety Assurance of Robotic Autonomous Systems", "comment": "In Proc. of the 26th TAROS (Towards Autonomous Robotic Systems)\n  Conference, York, UK, August, 2025", "summary": "Autonomous robots deployed in shared human environments, such as agricultural\nsettings, require rigorous safety assurance to meet both functional reliability\nand regulatory compliance. These systems must operate in dynamic, unstructured\nenvironments, interact safely with humans, and respond effectively to a wide\nrange of potential hazards. This paper presents a verification workflow for the\nsafety assurance of an autonomous agricultural robot, covering the entire\ndevelopment life-cycle, from concept study and design to runtime verification.\nThe outlined methodology begins with a systematic hazard analysis and risk\nassessment to identify potential risks and derive corresponding safety\nrequirements. A formal model of the safety controller is then developed to\ncapture its behaviour and verify that the controller satisfies the specified\nsafety properties with respect to these requirements. The proposed approach is\ndemonstrated on a field robot operating in an agricultural setting. The results\nshow that the methodology can be effectively used to verify safety-critical\nproperties and facilitate the early identification of design issues,\ncontributing to the development of safer robots and autonomous systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u519c\u4e1a\u81ea\u4e3b\u673a\u5668\u4eba\u5b89\u5168\u9a8c\u8bc1\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6db5\u76d6\u4ece\u6982\u5ff5\u8bbe\u8ba1\u5230\u8fd0\u884c\u65f6\u9a8c\u8bc1\u7684\u6574\u4e2a\u5f00\u53d1\u751f\u547d\u5468\u671f\u3002", "motivation": "\u5728\u52a8\u6001\u3001\u975e\u7ed3\u6784\u5316\u7684\u519c\u4e1a\u73af\u5883\u4e2d\uff0c\u786e\u4fdd\u81ea\u4e3b\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u5b89\u5168\u4ea4\u4e92\u5e76\u5e94\u5bf9\u6f5c\u5728\u5371\u9669\uff0c\u9700\u8981\u4e25\u683c\u7684\u5b89\u5168\u9a8c\u8bc1\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u5371\u9669\u5206\u6790\u548c\u98ce\u9669\u8bc4\u4f30\u786e\u5b9a\u5b89\u5168\u9700\u6c42\uff0c\u5f00\u53d1\u5b89\u5168\u63a7\u5236\u5668\u7684\u5f62\u5f0f\u5316\u6a21\u578b\u4ee5\u9a8c\u8bc1\u5176\u6ee1\u8db3\u5b89\u5168\u5c5e\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u5e94\u7528\u4e8e\u519c\u4e1a\u73b0\u573a\u673a\u5668\u4eba\uff0c\u9a8c\u8bc1\u4e86\u5b89\u5168\u5173\u952e\u5c5e\u6027\u5e76\u65e9\u671f\u8bc6\u522b\u8bbe\u8ba1\u95ee\u9898\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6d41\u7a0b\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u5b89\u5168\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2506.19699", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19699", "abs": "https://arxiv.org/abs/2506.19699", "authors": ["Jian Hou", "Xin Zhou", "Qihan Yang", "Adam J. Spiers"], "title": "UniTac-NV: A Unified Tactile Representation For Non-Vision-Based Tactile Sensors", "comment": "7 pages, 8 figures. Accepted version to appear in: 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS)", "summary": "Generalizable algorithms for tactile sensing remain underexplored, primarily\ndue to the diversity of sensor modalities. Recently, many methods for\ncross-sensor transfer between optical (vision-based) tactile sensors have been\ninvestigated, yet little work focus on non-optical tactile sensors. To address\nthis gap, we propose an encoder-decoder architecture to unify tactile data\nacross non-vision-based sensors. By leveraging sensor-specific encoders, the\nframework creates a latent space that is sensor-agnostic, enabling cross-sensor\ndata transfer with low errors and direct use in downstream applications. We\nleverage this network to unify tactile data from two commercial tactile\nsensors: the Xela uSkin uSPa 46 and the Contactile PapillArray. Both were\nmounted on a UR5e robotic arm, performing force-controlled pressing sequences\nagainst distinct object shapes (circular, square, and hexagonal prisms) and two\nmaterials (rigid PLA and flexible TPU). Another more complex unseen object was\nalso included to investigate the model's generalization capabilities. We show\nthat alignment in latent space can be implicitly learned from joint autoencoder\ntraining with matching contacts collected via different sensors. We further\ndemonstrate the practical utility of our approach through contact geometry\nestimation, where downstream models trained on one sensor's latent\nrepresentation can be directly applied to another without retraining.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u7528\u4e8e\u7edf\u4e00\u975e\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\u7684\u6570\u636e\uff0c\u5b9e\u73b0\u8de8\u4f20\u611f\u5668\u6570\u636e\u8f6c\u6362\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u76f4\u63a5\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5149\u5b66\u89e6\u89c9\u4f20\u611f\u5668\u7684\u8de8\u4f20\u611f\u5668\u8f6c\u6362\uff0c\u800c\u5ffd\u7565\u4e86\u975e\u5149\u5b66\u4f20\u611f\u5668\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u4f20\u611f\u5668\u7279\u5b9a\u7f16\u7801\u5668\u6784\u5efa\u4f20\u611f\u5668\u65e0\u5173\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u8054\u5408\u81ea\u7f16\u7801\u5668\u8bad\u7ec3\u5b9e\u73b0\u6570\u636e\u5bf9\u9f50\u3002\u5b9e\u9a8c\u4f7f\u7528\u4e24\u79cd\u5546\u4e1a\u89e6\u89c9\u4f20\u611f\u5668\uff08Xela uSkin uSPa 46\u548cContactile PapillArray\uff09\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u4f4e\u8bef\u5dee\u7684\u8de8\u4f20\u611f\u5668\u8f6c\u6362\uff0c\u5e76\u5728\u51e0\u4f55\u4f30\u8ba1\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u76f4\u63a5\u5e94\u7528\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u975e\u5149\u5b66\u89e6\u89c9\u4f20\u611f\u5668\u7684\u901a\u7528\u5316\u7b97\u6cd5\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.19712", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.19712", "abs": "https://arxiv.org/abs/2506.19712", "authors": ["Praneeth Somisetty", "Robert Griffin", "Victor M. Baez", "Miguel F. Arevalo-Castiblanco", "Aaron T. Becker", "Jason M. O'Kane"], "title": "Estimating Spatially-Dependent GPS Errors Using a Swarm of Robots", "comment": "6 pages, 7 figures, 2025 IEEE 21st International Conference on\n  Automation Science and Engineering", "summary": "External factors, including urban canyons and adversarial interference, can\nlead to Global Positioning System (GPS) inaccuracies that vary as a function of\nthe position in the environment. This study addresses the challenge of\nestimating a static, spatially-varying error function using a team of robots.\nWe introduce a State Bias Estimation Algorithm (SBE) whose purpose is to\nestimate the GPS biases. The central idea is to use sensed estimates of the\nrange and bearing to the other robots in the team to estimate changes in bias\nacross the environment. A set of drones moves in a 2D environment, each\nsampling data from GPS, range, and bearing sensors. The biases calculated by\nthe SBE at estimated positions are used to train a Gaussian Process Regression\n(GPR) model. We use a Sparse Gaussian process-based Informative Path Planning\n(IPP) algorithm that identifies high-value regions of the environment for data\ncollection. The swarm plans paths that maximize information gain in each\niteration, further refining their understanding of the environment's positional\nbias landscape. We evaluated SBE and IPP in simulation and compared the IPP\nmethodology to an open-loop strategy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4f30\u8ba1GPS\u504f\u5dee\u7684\u72b6\u6001\u504f\u5dee\u4f30\u8ba1\u7b97\u6cd5\uff08SBE\uff09\uff0c\u5e76\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff08GPR\uff09\u548c\u4fe1\u606f\u8def\u5f84\u89c4\u5212\uff08IPP\uff09\u4f18\u5316\u6570\u636e\u6536\u96c6\u3002", "motivation": "\u89e3\u51b3\u56e0\u57ce\u5e02\u5ce1\u8c37\u548c\u5e72\u6270\u5bfc\u81f4\u7684GPS\u4e0d\u51c6\u786e\u95ee\u9898\uff0c\u901a\u8fc7\u673a\u5668\u4eba\u56e2\u961f\u4f30\u8ba1\u9759\u6001\u7a7a\u95f4\u53d8\u5316\u7684\u8bef\u5dee\u51fd\u6570\u3002", "method": "\u4f7f\u7528SBE\u7b97\u6cd5\u4f30\u8ba1GPS\u504f\u5dee\uff0c\u7ed3\u5408GPR\u6a21\u578b\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7IPP\u7b97\u6cd5\u89c4\u5212\u8def\u5f84\u4ee5\u6700\u5927\u5316\u4fe1\u606f\u589e\u76ca\u3002", "result": "\u5728\u6a21\u62df\u4e2d\u8bc4\u4f30\u4e86SBE\u548cIPP\uff0c\u5e76\u4e0e\u5f00\u73af\u7b56\u7565\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "SBE\u548cIPP\u65b9\u6cd5\u80fd\u6709\u6548\u4f30\u8ba1\u548c\u4f18\u5316GPS\u504f\u5dee\uff0c\u63d0\u5347\u73af\u5883\u4f4d\u7f6e\u504f\u5dee\u7684\u7406\u89e3\u3002"}}
{"id": "2506.19781", "categories": ["cs.RO", "cs.NI"], "pdf": "https://arxiv.org/pdf/2506.19781", "abs": "https://arxiv.org/abs/2506.19781", "authors": ["Boyi Liu", "Qianyi Zhang", "Qiang Yang", "Jianhao Jiao", "Jagmohan Chauhan", "Dimitrios Kanoulas"], "title": "The Starlink Robot: A Platform and Dataset for Mobile Satellite Communication", "comment": null, "summary": "The integration of satellite communication into mobile devices represents a\nparadigm shift in connectivity, yet the performance characteristics under\nmotion and environmental occlusion remain poorly understood. We present the\nStarlink Robot, the first mobile robotic platform equipped with Starlink\nsatellite internet, comprehensive sensor suite including upward-facing camera,\nLiDAR, and IMU, designed to systematically study satellite communication\nperformance during movement. Our multi-modal dataset captures synchronized\ncommunication metrics, motion dynamics, sky visibility, and 3D environmental\ncontext across diverse scenarios including steady-state motion, variable\nspeeds, and different occlusion conditions. This platform and dataset enable\nresearchers to develop motion-aware communication protocols, predict\nconnectivity disruptions, and optimize satellite communication for emerging\nmobile applications from smartphones to autonomous vehicles. The project is\navailable at https://github.com/StarlinkRobot.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u914d\u5907Starlink\u536b\u661f\u4e92\u8054\u7f51\u7684\u79fb\u52a8\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u7528\u4e8e\u7814\u7a76\u8fd0\u52a8\u548c\u73af\u5883\u906e\u6321\u4e0b\u7684\u536b\u661f\u901a\u4fe1\u6027\u80fd\u3002", "motivation": "\u536b\u661f\u901a\u4fe1\u5728\u79fb\u52a8\u8bbe\u5907\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5176\u5728\u8fd0\u52a8\u548c\u906e\u6321\u73af\u5883\u4e0b\u7684\u6027\u80fd\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86Starlink Robot\u5e73\u53f0\uff0c\u914d\u5907\u591a\u79cd\u4f20\u611f\u5668\uff0c\u6536\u96c6\u540c\u6b65\u7684\u901a\u4fe1\u6307\u6807\u3001\u8fd0\u52a8\u52a8\u6001\u3001\u5929\u7a7a\u53ef\u89c1\u6027\u548c3D\u73af\u5883\u6570\u636e\u3002", "result": "\u751f\u6210\u4e86\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e0d\u540c\u8fd0\u52a8\u72b6\u6001\u548c\u906e\u6321\u6761\u4ef6\u4e0b\u7684\u901a\u4fe1\u6027\u80fd\u3002", "conclusion": "\u8be5\u5e73\u53f0\u548c\u6570\u636e\u96c6\u4e3a\u5f00\u53d1\u8fd0\u52a8\u611f\u77e5\u901a\u4fe1\u534f\u8bae\u548c\u4f18\u5316\u536b\u661f\u901a\u4fe1\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.19815", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19815", "abs": "https://arxiv.org/abs/2506.19815", "authors": ["Runsheng Wang", "Xinyue Zhu", "Ava Chen", "Jingxi Xu", "Lauren Winterbottom", "Dawn M. Nilsen", "Joel Stein", "Matei Ciocarlie"], "title": "ReactEMG: Zero-Shot, Low-Latency Intent Detection via sEMG", "comment": null, "summary": "Surface electromyography (sEMG) signals show promise for effective\nhuman-computer interfaces, particularly in rehabilitation and prosthetics.\nHowever, challenges remain in developing systems that respond quickly and\nreliably to user intent, across different subjects and without requiring\ntime-consuming calibration. In this work, we propose a framework for EMG-based\nintent detection that addresses these challenges. Unlike traditional gesture\nrecognition models that wait until a gesture is completed before classifying\nit, our approach uses a segmentation strategy to assign intent labels at every\ntimestep as the gesture unfolds. We introduce a novel masked modeling strategy\nthat aligns muscle activations with their corresponding user intents, enabling\nrapid onset detection and stable tracking of ongoing gestures. In evaluations\nagainst baseline methods, considering both accuracy and stability for device\ncontrol, our approach surpasses state-of-the-art performance in zero-shot\ntransfer conditions, demonstrating its potential for wearable robotics and\nnext-generation prosthetic systems. Our project page is available at:\nhttps://reactemg.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8esEMG\u4fe1\u53f7\u7684\u610f\u56fe\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6bb5\u7b56\u7565\u548c\u63a9\u7801\u5efa\u6a21\u5b9e\u73b0\u5feb\u901f\u610f\u56fe\u8bc6\u522b\uff0c\u9002\u7528\u4e8e\u5eb7\u590d\u548c\u5047\u80a2\u9886\u57df\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709sEMG\u7cfb\u7edf\u5728\u4e0d\u540c\u7528\u6237\u95f4\u5feb\u901f\u3001\u53ef\u9760\u8bc6\u522b\u610f\u56fe\u4e14\u65e0\u9700\u8017\u65f6\u6821\u51c6\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u6bb5\u7b56\u7565\u5b9e\u65f6\u6807\u8bb0\u610f\u56fe\uff0c\u5e76\u5f15\u5165\u63a9\u7801\u5efa\u6a21\u5bf9\u9f50\u808c\u8089\u6fc0\u6d3b\u4e0e\u7528\u6237\u610f\u56fe\u3002", "result": "\u5728\u96f6\u6837\u672c\u8fc1\u79fb\u6761\u4ef6\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u548c\u4e0b\u4e00\u4ee3\u5047\u80a2\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5feb\u901f\u3001\u7a33\u5b9a\u7684\u610f\u56fe\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.19816", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19816", "abs": "https://arxiv.org/abs/2506.19816", "authors": ["Hao Li", "Shuai Yang", "Yilun Chen", "Yang Tian", "Xiaoda Yang", "Xinyi Chen", "Hanqing Wang", "Tai Wang", "Feng Zhao", "Dahua Lin", "Jiangmiao Pang"], "title": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation", "comment": "36 pages, 21 figures", "summary": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness.", "AI": {"tldr": "CronusVLA\u6269\u5c55\u5355\u5e27VLA\u6a21\u578b\u5230\u591a\u5e27\u8303\u5f0f\uff0c\u901a\u8fc7\u9ad8\u6548\u540e\u8bad\u7ec3\u9636\u6bb5\u63d0\u5347\u6027\u80fd\uff0c\u51cf\u5c11\u8ba1\u7b97\u5197\u4f59\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u53d7\u9650\u4e8e\u5355\u5e27\u89c2\u5bdf\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u5e27\u8fd0\u52a8\u4fe1\u606f\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u5305\u62ec\u5355\u5e27\u9884\u8bad\u7ec3\u3001\u591a\u5e27\u7f16\u7801\u548c\u8de8\u5e27\u89e3\u7801\uff0c\u901a\u8fc7\u7279\u5f81\u5206\u5757\u548c\u52a8\u4f5c\u9002\u5e94\u673a\u5236\u4f18\u5316\u3002", "result": "\u5728SimplerEnv\u4e0a\u6210\u529f\u738770.9%\uff0cLIBERO\u4e0a\u63d0\u534712.7%\uff0c\u5b9e\u9645\u673a\u5668\u4eba\u5b9e\u9a8c\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CronusVLA\u901a\u8fc7\u591a\u5e27\u5904\u7406\u548c\u9ad8\u6548\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347VLA\u6a21\u578b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.19827", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19827", "abs": "https://arxiv.org/abs/2506.19827", "authors": ["Ola Elmaghraby", "Eslam Mounier", "Paulo Ricardo Marques de Araujo", "Aboelmagd Noureldin"], "title": "Look to Locate: Vision-Based Multisensory Navigation with 3-D Digital Maps for GNSS-Challenged Environments", "comment": null, "summary": "In Global Navigation Satellite System (GNSS)-denied environments such as\nindoor parking structures or dense urban canyons, achieving accurate and robust\nvehicle positioning remains a significant challenge. This paper proposes a\ncost-effective, vision-based multi-sensor navigation system that integrates\nmonocular depth estimation, semantic filtering, and visual map registration\n(VMR) with 3-D digital maps. Extensive testing in real-world indoor and outdoor\ndriving scenarios demonstrates the effectiveness of the proposed system,\nachieving sub-meter accuracy of 92% indoors and more than 80% outdoors, with\nconsistent horizontal positioning and heading average root mean-square errors\nof approximately 0.98 m and 1.25 {\\deg}, respectively. Compared to the\nbaselines examined, the proposed solution significantly reduced drift and\nimproved robustness under various conditions, achieving positioning accuracy\nimprovements of approximately 88% on average. This work highlights the\npotential of cost-effective monocular vision systems combined with 3D maps for\nscalable, GNSS-independent navigation in land vehicles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u57fa\u4e8e\u89c6\u89c9\u7684\u591a\u4f20\u611f\u5668\u5bfc\u822a\u7cfb\u7edf\uff0c\u7ed3\u5408\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3001\u8bed\u4e49\u8fc7\u6ee4\u548c\u89c6\u89c9\u5730\u56fe\u6ce8\u518c\u6280\u672f\uff0c\u5728GNSS\u4fe1\u53f7\u7f3a\u5931\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8f66\u8f86\u5b9a\u4f4d\u3002", "motivation": "\u5728GNSS\u4fe1\u53f7\u7f3a\u5931\u7684\u73af\u5883\uff08\u5982\u5ba4\u5185\u505c\u8f66\u573a\u6216\u5bc6\u96c6\u57ce\u5e02\u5ce1\u8c37\uff09\u4e2d\uff0c\u5b9e\u73b0\u51c6\u786e\u4e14\u9c81\u68d2\u7684\u8f66\u8f86\u5b9a\u4f4d\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u6574\u5408\u4e86\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3001\u8bed\u4e49\u8fc7\u6ee4\u548c\u89c6\u89c9\u5730\u56fe\u6ce8\u518c\uff08VMR\uff09\u6280\u672f\uff0c\u5e76\u4e0e3D\u6570\u5b57\u5730\u56fe\u7ed3\u5408\u3002", "result": "\u5728\u771f\u5b9e\u5ba4\u5185\u5916\u9a7e\u9a76\u573a\u666f\u4e2d\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u5b9e\u73b0\u4e86\u5ba4\u518592%\u3001\u5ba4\u591680%\u4ee5\u4e0a\u7684\u4e9a\u7c73\u7ea7\u7cbe\u5ea6\uff0c\u6c34\u5e73\u5b9a\u4f4d\u548c\u822a\u5411\u7684\u5e73\u5747\u5747\u65b9\u6839\u8bef\u5dee\u5206\u522b\u4e3a0.98\u7c73\u548c1.25\u5ea6\u3002\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9a\u4f4d\u7cbe\u5ea6\u5e73\u5747\u63d0\u5347\u7ea688%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4f4e\u6210\u672c\u5355\u76ee\u89c6\u89c9\u7cfb\u7edf\u7ed3\u54083D\u5730\u56fe\u5728\u9646\u5730\u8f66\u8f86\u5bfc\u822a\u4e2d\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u72ec\u7acb\u6027\uff0c\u65e0\u9700\u4f9d\u8d56GNSS\u3002"}}
{"id": "2506.19842", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19842", "abs": "https://arxiv.org/abs/2506.19842", "authors": ["Tengbo Yu", "Guanxing Lu", "Zaijia Yang", "Haoyuan Deng", "Season Si Chen", "Jiwen Lu", "Wenbo Ding", "Guoqiang Hu", "Yansong Tang", "Ziwei Wang"], "title": "ManiGaussian++: General Robotic Bimanual Manipulation with Hierarchical Gaussian World Model", "comment": null, "summary": "Multi-task robotic bimanual manipulation is becoming increasingly popular as\nit enables sophisticated tasks that require diverse dual-arm collaboration\npatterns. Compared to unimanual manipulation, bimanual tasks pose challenges to\nunderstanding the multi-body spatiotemporal dynamics. An existing method\nManiGaussian pioneers encoding the spatiotemporal dynamics into the visual\nrepresentation via Gaussian world model for single-arm settings, which ignores\nthe interaction of multiple embodiments for dual-arm systems with significant\nperformance drop. In this paper, we propose ManiGaussian++, an extension of\nManiGaussian framework that improves multi-task bimanual manipulation by\ndigesting multi-body scene dynamics through a hierarchical Gaussian world\nmodel. To be specific, we first generate task-oriented Gaussian Splatting from\nintermediate visual features, which aims to differentiate acting and\nstabilizing arms for multi-body spatiotemporal dynamics modeling. We then build\na hierarchical Gaussian world model with the leader-follower architecture,\nwhere the multi-body spatiotemporal dynamics is mined for intermediate visual\nrepresentation via future scene prediction. The leader predicts Gaussian\nSplatting deformation caused by motions of the stabilizing arm, through which\nthe follower generates the physical consequences resulted from the movement of\nthe acting arm. As a result, our method significantly outperforms the current\nstate-of-the-art bimanual manipulation techniques by an improvement of 20.2% in\n10 simulated tasks, and achieves 60% success rate on average in 9 challenging\nreal-world tasks. Our code is available at\nhttps://github.com/April-Yz/ManiGaussian_Bimanual.", "AI": {"tldr": "ManiGaussian++\u6269\u5c55\u4e86ManiGaussian\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u9ad8\u65af\u4e16\u754c\u6a21\u578b\u6539\u8fdb\u591a\u4efb\u52a1\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u590d\u6742\uff0c\u73b0\u6709\u65b9\u6cd5ManiGaussian\u5ffd\u7565\u4e86\u591a\u4f53\u4ea4\u4e92\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u9ad8\u65af\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u4efb\u52a1\u5bfc\u5411\u7684\u9ad8\u65af\u70b9\u4e91\u548c\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u67b6\u6784\u5efa\u6a21\u591a\u4f53\u65f6\u7a7a\u52a8\u6001\u3002", "result": "\u572810\u4e2a\u6a21\u62df\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u534720.2%\uff0c\u57289\u4e2a\u771f\u5b9e\u4efb\u52a1\u4e2d\u5e73\u5747\u6210\u529f\u7387\u8fbe60%\u3002", "conclusion": "ManiGaussian++\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u6280\u672f\uff0c\u9002\u7528\u4e8e\u590d\u6742\u4efb\u52a1\u3002"}}
