<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 35]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [A Novel Theoretical Approach on Micro-Nano Robotic Networks Based on Density Matrices and Swarm Quantum Mechanics](https://arxiv.org/abs/2509.08002)
*Maria Mannone,Mahathi Anand,Peppino Fazio,Abdalla Swikir*

Main category: cs.RO

TL;DR: 本文提出将机器人群体建模为混合量子态，使用密度矩阵表示，其大小不随机器人数量变化


<details>
  <summary>Details</summary>
Motivation: 传统群体机器人参数描述存在局限性，需要更高效的数学表示方法来处理大规模群体系统

Method: 将群体定义为混合量子态，使用密度矩阵进行描述，建立块矩阵表示方法

Result: 提出了群体密度矩阵表示框架，其矩阵尺寸固定，不随机器人数量增加而增大

Conclusion: 量子方法为群体机器人建模提供了新思路，密度矩阵表示具有可扩展性优势，为未来研究指明方向

Abstract: In a robotic swarm, parameters such as position and proximity to the target
can be described in terms of probability amplitudes. This idea led to recent
studies on a quantum approach to the definition of the swarm, including a
block-matrix representation. Here, we propose an advancement of the idea,
defining a swarm as a mixed quantum state, to be described with a density
matrix, whose size does not change with the number of robots. We end the
article with some directions for future research.

</details>


### [2] [PySensors 2.0: A Python Package for Sparse Sensor Placement](https://arxiv.org/abs/2509.08017)
*Niharika Karnik,Yash Bhangale,Mohammad G. Abdo,Andrei A. Klishin,Joshua J. Cogliati,Bingni W. Brunton,J. Nathan Kutz,Steven L. Brunton,Krithika Manohar*

Main category: cs.RO

TL;DR: PySensors主要更新：引入空间约束传感器放置功能，支持自定义基输入，提出热力学方法映射传感器交互全景，包含噪声不确定性量化，并提供可视化工具和代码示例。


<details>
  <summary>Details</summary>
Motivation: 为了增强传感器选择和放置的灵活性，允许用户在不同区域施加传感器数量约束，整合预定传感器位置，并支持任意数据驱动或谱基输入。

Method: 采用空间约束传感器放置算法，热力学方法映射传感器交互景观，正则化最小二乘方法处理过采样和欠采样，以及噪声诱导的不确定性量化技术。

Result: 实现了更灵活的传感器配置，能够处理复杂空间约束，提供完整的传感器交互视图，并具备鲁棒的重建能力和不确定性可视化功能。

Conclusion: PySensors的重大更新显著扩展了其在传感器选择和放置方面的能力，为各种应用领域提供了更强大的工具，并为进一步的功能扩展奠定了基础。

Abstract: PySensors is a Python package for selecting and placing a sparse set of
sensors for reconstruction and classification tasks. In this major update to
\texttt{PySensors}, we introduce spatially constrained sensor placement
capabilities, allowing users to enforce constraints such as maximum or exact
sensor counts in specific regions, incorporate predetermined sensor locations,
and maintain minimum distances between sensors. We extend functionality to
support custom basis inputs, enabling integration of any data-driven or
spectral basis. We also propose a thermodynamic approach that goes beyond a
single ``optimal'' sensor configuration and maps the complete landscape of
sensor interactions induced by the training data. This comprehensive view
facilitates integration with external selection criteria and enables assessment
of sensor replacement impacts. The new optimization technique also accounts for
over- and under-sampling of sensors, utilizing a regularized least squares
approach for robust reconstruction. Additionally, we incorporate noise-induced
uncertainty quantification of the estimation error and provide visual
uncertainty heat maps to guide deployment decisions. To highlight these
additions, we provide a brief description of the mathematical algorithms and
theory underlying these new capabilities. We demonstrate the usage of new
features with illustrative code examples and include practical advice for
implementation across various application domains. Finally, we outline a
roadmap of potential extensions to further enhance the package's functionality
and applicability to emerging sensing challenges.

</details>


### [3] [SVN-ICP: Uncertainty Estimation of ICP-based LiDAR Odometry using Stein Variational Newton](https://arxiv.org/abs/2509.08069)
*Shiping Ma,Haoming Zhang,Marc Toussaint*

Main category: cs.RO

TL;DR: SVN-ICP是一种基于Stein变分牛顿方法的ICP算法，具有不确定性估计能力，专为多传感器系统中的LiDAR里程计融合设计，无需显式噪声建模或手动参数调整。


<details>
  <summary>Details</summary>
Motivation: 传统ICP算法在多传感器融合中缺乏可靠的不确定性估计，特别是在LiDAR性能下降的环境中，需要一种能够自动推断噪声参数并提供准确位姿估计的方法。

Method: 采用Stein变分推理框架，在流形上使用粒子近似后验分布，结合Stein变分牛顿方法进行位姿估计和噪声参数推断，并集成到误差状态卡尔曼滤波器中与IMU融合。

Result: 在多个数据集和不同机器人平台上测试表明，该方法在挑战性场景中优于最先进的方法，并提供可靠的不确定性估计。

Conclusion: SVN-ICP成功解决了LiDAR里程计融合中的不确定性估计问题，在LiDAR性能下降的环境中仍能保持准确的位姿估计和一致的噪声参数推断。

Abstract: This letter introduces SVN-ICP, a novel Iterative Closest Point (ICP)
algorithm with uncertainty estimation that leverages Stein Variational Newton
(SVN) on manifold. Designed specifically for fusing LiDAR odometry in
multisensor systems, the proposed method ensures accurate pose estimation and
consistent noise parameter inference, even in LiDAR-degraded environments. By
approximating the posterior distribution using particles within the Stein
Variational Inference framework, SVN-ICP eliminates the need for explicit noise
modeling or manual parameter tuning. To evaluate its effectiveness, we
integrate SVN-ICP into a simple error-state Kalman filter alongside an IMU and
test it across multiple datasets spanning diverse environments and robot types.
Extensive experimental results demonstrate that our approach outperforms
best-in-class methods on challenging scenarios while providing reliable
uncertainty estimates.

</details>


### [4] [Real-Time Obstacle Avoidance for a Mobile Robot Using CNN-Based Sensor Fusion](https://arxiv.org/abs/2509.08095)
*Lamiaa H. Zain,Raafat E. Shalaby*

Main category: cs.RO

TL;DR: 这篇论文训练了三种卷积神经网络模型用于移动机器人实时障碍避免，其中NetConEmb模型表现最优，在知和未知环境中都达到100%成功率。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在复杂未知环境中需要有效的障碍避免能力，这是导航栈的关键组件。

Method: 使用Intel RealSense D415 RGB-D相机获取颜色和深度图像，训练了三种绘制式卷积神经网络(CNN)模型，包括NetConEmb、NetEmb和NetGated。

Result: NetConEmb模型在离线评估中达到最优性能(MedAE 0.58×10^{-3} rad/s)，实时导航中在所有环境中达到100%成功率。NetEmb模型较轻量(参数减少25%)但性能相似(RMSE 21.68×10^{-3} rad/s)。

Conclusion: NetConEmb模型在障碍避免任务中表现最为稳健，适合复杂环境；NetEmb模型在保持性能的同时更轻量化，适合资源受限的场景。

Abstract: Obstacle avoidance is a critical component of the navigation stack required
for mobile robots to operate effectively in complex and unknown environments.
In this research, three end-to-end Convolutional Neural Networks (CNNs) were
trained and evaluated offline and deployed on a differential-drive mobile robot
for real-time obstacle avoidance to generate low-level steering commands from
synchronized color and depth images acquired by an Intel RealSense D415 RGB-D
camera in diverse environments. Offline evaluation showed that the NetConEmb
model achieved the best performance with a notably low MedAE of $0.58 \times
10^{-3}$ rad/s. In comparison, the lighter NetEmb architecture adopted in this
study, which reduces the number of trainable parameters by approximately 25\%
and converges faster, produced comparable results with an RMSE of $21.68 \times
10^{-3}$ rad/s, close to the $21.42 \times 10^{-3}$ rad/s obtained by
NetConEmb. Real-time navigation further confirmed NetConEmb's robustness,
achieving a 100\% success rate in both known and unknown environments, while
NetEmb and NetGated succeeded only in navigating the known environment.

</details>


### [5] [Online Learning and Coverage of Unknown Fields Using Random-Feature Gaussian Processes](https://arxiv.org/abs/2509.08117)
*Ruijie Du,Ruoyu Lin,Yanning Shen,Magnus Egerstedt*

Main category: cs.RO

TL;DR: 提出了一个多机器人系统框架，用于同时学习和覆盖具有未知且可能时变密度函数的感兴趣区域，结合随机特征高斯过程、Voronoi覆盖控制和UCB采样策略


<details>
  <summary>Details</summary>
Motivation: 解决多机器人系统在未知和时变环境中的同时学习和覆盖问题，克服传统高斯过程回归的局限性

Method: 采用随机特征高斯过程(RFGP)及其在线变体(O-RFGP)进行在线增量推理，结合Voronoi覆盖控制和上置信界(UCB)采样策略

Result: 在时不变场景下提供理论保证并通过仿真验证，在时变设置下通过额外仿真和物理实验证明有效性

Conclusion: 该框架使机器人团队能够自适应地关注重要区域，同时细化学习到的空间场以实现高效覆盖

Abstract: This paper proposes a framework for multi-robot systems to perform
simultaneous learning and coverage of the domain of interest characterized by
an unknown and potentially time-varying density function. To overcome the
limitations of Gaussian Process (GP) regression, we employ Random Feature GP
(RFGP) and its online variant (O-RFGP) that enables online and incremental
inference. By integrating these with Voronoi-based coverage control and Upper
Confidence Bound (UCB) sampling strategy, a team of robots can adaptively focus
on important regions while refining the learned spatial field for efficient
coverage. Under mild assumptions, we provide theoretical guarantees and
evaluate the framework through simulations in time-invariant scenarios.
Furthermore, its effectiveness in time-varying settings is demonstrated through
additional simulations and a physical experiment.

</details>


### [6] [Attribute-based Object Grounding and Robot Grasp Detection with Spatial Reasoning](https://arxiv.org/abs/2509.08126)
*Houjian Yu,Zheming Zhou,Min Sun,Omid Ghasemalizadeh,Yuyin Sun,Cheng-Hao Kuo,Arnie Sen,Changhyun Choi*

Main category: cs.RO

TL;DR: OGRG是一个新颖的框架，通过自然语言指令实现机器人对目标物体的定位和抓取，即使在有重复物体的场景中也能工作，支持全监督和弱监督两种学习设置。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在处理开放形式语言表达、重复物体场景时的局限性，以及减少对昂贵密集像素标注的依赖。

Method: 提出基于属性的物体定位和抓取框架，包含双向视觉语言融合模块和深度信息整合，支持全监督(RGS)和弱监督(RGA)两种学习方式。

Result: 在桌面场景中优于现有基线方法，RGS模式下在RTX 2080 Ti上达到17.59 FPS，在弱监督RGA设置下在仿真和真实机器人试验中都取得了更高的抓取成功率。

Conclusion: OGRG框架通过有效的空间推理设计，显著提升了基于自然语言的机器人抓取性能，特别是在处理重复物体和减少标注需求方面表现出色。

Abstract: Enabling robots to grasp objects specified through natural language is
essential for effective human-robot interaction, yet it remains a significant
challenge. Existing approaches often struggle with open-form language
expressions and typically assume unambiguous target objects without duplicates.
Moreover, they frequently rely on costly, dense pixel-wise annotations for both
object grounding and grasp configuration. We present Attribute-based Object
Grounding and Robotic Grasping (OGRG), a novel framework that interprets
open-form language expressions and performs spatial reasoning to ground target
objects and predict planar grasp poses, even in scenes containing duplicated
object instances. We investigate OGRG in two settings: (1) Referring Grasp
Synthesis (RGS) under pixel-wise full supervision, and (2) Referring Grasp
Affordance (RGA) using weakly supervised learning with only single-pixel grasp
annotations. Key contributions include a bi-directional vision-language fusion
module and the integration of depth information to enhance geometric reasoning,
improving both grounding and grasping performance. Experiment results show that
OGRG outperforms strong baselines in tabletop scenes with diverse spatial
language instructions. In RGS, it operates at 17.59 FPS on a single NVIDIA RTX
2080 Ti GPU, enabling potential use in closed-loop or multi-object sequential
grasping, while delivering superior grounding and grasp prediction accuracy
compared to all the baselines considered. Under the weakly supervised RGA
setting, OGRG also surpasses baseline grasp-success rates in both simulation
and real-robot trials, underscoring the effectiveness of its spatial reasoning
design. Project page: https://z.umn.edu/ogrg

</details>


### [7] [Mean Field Game-Based Interactive Trajectory Planning Using Physics-Inspired Unified Potential Fields](https://arxiv.org/abs/2509.08147)
*Zhen Tian,Fujiang Yuan,Chunhong Yuan,Yanhong Peng*

Main category: cs.RO

TL;DR: 提出了Interaction-Enriched Unified Potential Field (IUPF)框架，通过物理启发的变分模型融合风格相关的效益和风险场，在自动驾驶中实现安全、高效且可扩展的交互轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中交互轨迹规划需要平衡安全性、效率和可扩展性的挑战，现有方法存在计算成本高或依赖外部安全评估模块的问题。

Method: 基于平均场博弈理论，构建统一的势场框架，融合风格相关的效益和风险场，使用随机微分方程保证纳什均衡和指数收敛性，无需额外安全模块。

Result: 在换道和超车场景的仿真中，IUPF确保了安全距离，生成平滑高效的轨迹，在适应性和计算效率方面优于传统优化和博弈论基线方法。

Conclusion: IUPF框架能够有效处理异构驾驶行为，提供安全可靠的交互轨迹规划解决方案，具有实际应用价值。

Abstract: Interactive trajectory planning in autonomous driving must balance safety,
efficiency, and scalability under heterogeneous driving behaviors. Existing
methods often face high computational cost or rely on external safety critics.
To address this, we propose an Interaction-Enriched Unified Potential Field
(IUPF) framework that fuses style-dependent benefit and risk fields through a
physics-inspired variational model, grounded in mean field game theory. The
approach captures conservative, aggressive, and cooperative behaviors without
additional safety modules, and employs stochastic differential equations to
guarantee Nash equilibrium with exponential convergence. Simulations on lane
changing and overtaking scenarios show that IUPF ensures safe distances,
generates smooth and efficient trajectories, and outperforms traditional
optimization and game-theoretic baselines in both adaptability and
computational efficiency.

</details>


### [8] [Risk-Bounded Multi-Agent Visual Navigation via Dynamic Budget Allocation](https://arxiv.org/abs/2509.08157)
*Viraj Parimi,Brian C. Williams*

Main category: cs.RO

TL;DR: 基于风险预算分配的多机器人安全导航规划方法RB-CBS，充分利用风险空间提高导航效率


<details>
  <summary>Details</summary>
Motivation: 解决传统图剪枝方法在多机器人安全导航中过于保守的问题，允许机器人在控制风险的前提下穿过高风险区域

Method: 提出RB-CBS方法，在冲突基于搜索的规划器中动态分配和调整风险预算，为每个机器人设置局部风险预算

Result: 实验结果表明RB-CBS在复杂环境中展现出更优的性能，能够在指定风险约束内找到无碰撞路径

Conclusion: 迭代风险分配框架有效提升了多机器人安全导航的效率和灵活性

Abstract: Safe navigation is essential for autonomous systems operating in hazardous
environments, especially when multiple agents must coordinate using just visual
inputs over extended time horizons. Traditional planning methods excel at
solving long-horizon tasks but rely on predefined distance metrics, while safe
Reinforcement Learning (RL) can learn complex behaviors using high-dimensional
inputs yet struggles with multi-agent, goal-conditioned scenarios. Recent work
combined these paradigms by leveraging goal-conditioned RL (GCRL) to build an
intermediate graph from replay buffer states, pruning unsafe edges, and using
Conflict-Based Search (CBS) for multi-agent path planning. Although effective,
this graph-pruning approach can be overly conservative, limiting mission
efficiency by precluding missions that must traverse high-risk regions. To
address this limitation, we propose RB-CBS, a novel extension to CBS that
dynamically allocates and adjusts user-specified risk bound ($\Delta$) across
agents to flexibly trade off safety and speed. Our improved planner ensures
that each agent receives a local risk budget ($\delta$) enabling more efficient
navigation while still respecting overall safety constraints. Experimental
results demonstrate that this iterative risk-allocation framework yields
superior performance in complex environments, allowing multiple agents to find
collision-free paths within the user-specified $\Delta$.

</details>


### [9] [Zero-Shot Metric Depth Estimation via Monocular Visual-Inertial Rescaling for Autonomous Aerial Navigation](https://arxiv.org/abs/2509.08159)
*Steven Yang,Xiaoyu Tian,Kshitij Goel,Wennie Tabib*

Main category: cs.RO

TL;DR: 提出一种从单目RGB图像和IMU预测度量深度的轻量级零样本重缩放方法，使用单调样条拟合实现实时碰撞避免


<details>
  <summary>Details</summary>
Motivation: 现有方法需要重型传感器或数据密集型微调，本文旨在开发轻量级零样本方法用于计算受限的无人机自主飞行碰撞避免

Method: 利用视觉惯性导航系统创建的稀疏3D特征图，通过几种轻量级零样本重缩放策略从相对深度估计获得度量深度，最佳方法是单调样条拟合

Result: 在多样化仿真环境中比较了不同策略的准确性，最佳方法在真实世界计算受限四旋翼上以15Hz频率获得度量深度估计，成功实现碰撞避免

Conclusion: 提出的轻量级零样本重缩放方法能够有效实现实时度量深度估计和碰撞避免，适用于资源受限的自主飞行平台

Abstract: This paper presents a methodology to predict metric depth from monocular RGB
images and an inertial measurement unit (IMU). To enable collision avoidance
during autonomous flight, prior works either leverage heavy sensors (e.g.,
LiDARs or stereo cameras) or data-intensive and domain-specific fine-tuning of
monocular metric depth estimation methods. In contrast, we propose several
lightweight zero-shot rescaling strategies to obtain metric depth from relative
depth estimates via the sparse 3D feature map created using a visual-inertial
navigation system. These strategies are compared for their accuracy in diverse
simulation environments. The best performing approach, which leverages
monotonic spline fitting, is deployed in the real-world on a
compute-constrained quadrotor. We obtain on-board metric depth estimates at 15
Hz and demonstrate successful collision avoidance after integrating the
proposed method with a motion primitives-based planner.

</details>


### [10] [Diffusion-Guided Multi-Arm Motion Planning](https://arxiv.org/abs/2509.08160)
*Viraj Parimi,Brian C. Williams*

Main category: cs.RO

TL;DR: 通过结合单臂轨迹生成模型和双臂碰撞解决模型的滴涼导向多臂规划方法，解决多臂运动规划的可扩展性问题


<details>
  <summary>Details</summary>
Motivation: 当前多臂运动规划方法遇到状态空间指数增长和依赖大量训练数据集的可扩展性问题

Method: 受MAPF启发，训练两个条件滴涼模型：一个生成可行单臂轨迹，另一个模型双臂动态以解决碰撞，通过结构化分解实现高效扩展

Result: 在不同团队规模下评估，证明方法的有效性和实际应用能力

Conclusion: DG-MAP方法通过专门化生成模型与MAPF结构化分解的结合，显著提升了学习基方法的可扩展性并减少对大规模多臂数据集的依赖

Abstract: Multi-arm motion planning is fundamental for enabling arms to complete
complex long-horizon tasks in shared spaces efficiently but current methods
struggle with scalability due to exponential state-space growth and reliance on
large training datasets for learned models. Inspired by Multi-Agent Path
Finding (MAPF), which decomposes planning into single-agent problems coupled
with collision resolution, we propose a novel diffusion-guided multi-arm
planner (DG-MAP) that enhances scalability of learning-based models while
reducing their reliance on massive multi-arm datasets. Recognizing that
collisions are primarily pairwise, we train two conditional diffusion models,
one to generate feasible single-arm trajectories, and a second, to model the
dual-arm dynamics required for effective pairwise collision resolution. By
integrating these specialized generative models within a MAPF-inspired
structured decomposition, our planner efficiently scales to larger number of
arms. Evaluations against alternative learning-based methods across various
team sizes demonstrate our method's effectiveness and practical applicability.
Project website can be found at https://diff-mapf-mers.csail.mit.edu

</details>


### [11] [Quadrotor Navigation using Reinforcement Learning with Privileged Information](https://arxiv.org/abs/2509.08177)
*Jonathan Lee,Abhishek Rathod,Kshitij Goel,John Stecklein,Wennie Tabib*

Main category: cs.RO

TL;DR: 基于强化学习的四旋翼导航方法，利用可微分仿真、新型损失函数和特权信息在大障碍物环境中导航，成功率86%，比基线高34%，并在真实室外环境中验证。


<details>
  <summary>Details</summary>
Motivation: 现有的学习型导航方法在狭窄障碍物场景表现良好，但在目标位置被大型墙壁或地形阻挡时表现不佳，需要解决大型障碍物环境下的导航问题。

Method: 使用时达（ToA）地图作为特权信息，结合偏航对齐损失函数来引导机器人绕过大型障碍物，采用可微分仿真进行强化学习训练。

Result: 在包含大型障碍物、急转弯和死角的逼真仿真环境中，该方法达到86%的成功率，比基线策略高出34%。在真实室外环境中进行了20次飞行测试，覆盖589米无碰撞，速度达4m/s。

Conclusion: 该方法有效解决了大型障碍物环境下的四旋翼导航问题，通过特权信息和新型损失函数的结合，在仿真和真实环境中都表现出优异的性能。

Abstract: This paper presents a reinforcement learning-based quadrotor navigation
method that leverages efficient differentiable simulation, novel loss
functions, and privileged information to navigate around large obstacles. Prior
learning-based methods perform well in scenes that exhibit narrow obstacles,
but struggle when the goal location is blocked by large walls or terrain. In
contrast, the proposed method utilizes time-of-arrival (ToA) maps as privileged
information and a yaw alignment loss to guide the robot around large obstacles.
The policy is evaluated in photo-realistic simulation environments containing
large obstacles, sharp corners, and dead-ends. Our approach achieves an 86%
success rate and outperforms baseline strategies by 34%. We deploy the policy
onboard a custom quadrotor in outdoor cluttered environments both during the
day and night. The policy is validated across 20 flights, covering 589 meters
without collisions at speeds up to 4 m/s.

</details>


### [12] [Online Dynamic SLAM with Incremental Smoothing and Mapping](https://arxiv.org/abs/2509.08197)
*Jesse Morris,Yiduo Wang,Viorela Ila*

Main category: cs.RO

TL;DR: 首次将增量优化技术应用于动态SLAM，通过新的因子图构造和系统架构实现了更高效的在线估计，运算速度提升5倍


<details>
  <summary>Details</summary>
Motivation: 现有动态SLAM方法虽然准确但计算成本高，不适合在线应用，需要一种更高效的在线解决方案

Method: 提出新的因子图构造和系统架构，利用现有增量优化方法支持在线估算

Result: 在多个数据集上达到或超过最高水平的相机位姿和物体运动精度，运算速度提升5倍

Conclusion: 该方法构造了适合增量求解器的问题结构，系统架构进一步提升了性能，为动态SLAM的在线应用提供了可行解决方案

Abstract: Dynamic SLAM methods jointly estimate for the static and dynamic scene
components, however existing approaches, while accurate, are computationally
expensive and unsuitable for online applications. In this work, we present the
first application of incremental optimisation techniques to Dynamic SLAM. We
introduce a novel factor-graph formulation and system architecture designed to
take advantage of existing incremental optimisation methods and support online
estimation. On multiple datasets, we demonstrate that our method achieves equal
to or better than state-of-the-art in camera pose and object motion accuracy.
We further analyse the structural properties of our approach to demonstrate its
scalability and provide insight regarding the challenges of solving Dynamic
SLAM incrementally. Finally, we show that our formulation results in problem
structure well-suited to incremental solvers, while our system architecture
further enhances performance, achieving a 5x speed-up over existing methods.

</details>


### [13] [A Comprehensive Review of Reinforcement Learning for Autonomous Driving in the CARLA Simulator](https://arxiv.org/abs/2509.08221)
*Elahe Delavari,Feeza Khan Khanzada,Jaerock Kwon*

Main category: cs.RO

TL;DR: 这篇论文系统分析了约100篇使用CARLA模拟器的深度强化学习自动驾驶研究，总结了主要算法分类、状态-动作-奖励设计、评估指标，并指出了当前挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶研究广泛采用深度强化学习，但缺乏对这些算法如何被使用、基准测试和评估的系统性分析，需要填补这一空白。

Method: 通过系统分析约100篇同行评审论文，对文献进行分类（模型无关、模型相关、分层、混合），量化流行度，分析状态、动作、奖励设计，并整理评估指标和基准配置。

Result: 发现80%以上研究仍依赖模型无关方法（如DQN、PPO、SAC）；总结了传感器模态、控制抽象和奖励塑造的多样性；整理了常用评估指标和CARLA基准配置。

Conclusion: 识别了稀疏奖励、仿真到现实迁移、安全保证和行为多样性有限等持续挑战，提出了模型相关RL、元学习和更丰富的多智能体模拟等有前景的方向，为RL自动驾驶研究提供统一分类和路线图。

Abstract: Autonomous-driving research has recently embraced deep Reinforcement Learning
(RL) as a promising framework for data-driven decision making, yet a clear
picture of how these algorithms are currently employed, benchmarked and
evaluated is still missing. This survey fills that gap by systematically
analysing around 100 peer-reviewed papers that train, test or validate RL
policies inside the open-source CARLA simulator. We first categorize the
literature by algorithmic family model-free, model-based, hierarchical, and
hybrid and quantify their prevalence, highlighting that more than 80% of
existing studies still rely on model-free methods such as DQN, PPO and SAC.
Next, we explain the diverse state, action and reward formulations adopted
across works, illustrating how choices of sensor modality (RGB, LiDAR, BEV,
semantic maps, and carla kinematics states), control abstraction (discrete vs.
continuous) and reward shaping are used across various literature. We also
consolidate the evaluation landscape by listing the most common metrics
(success rate, collision rate, lane deviation, driving score) and the towns,
scenarios and traffic configurations used in CARLA benchmarks. Persistent
challenges including sparse rewards, sim-to-real transfer, safety guarantees
and limited behaviour diversity are distilled into a set of open research
questions, and promising directions such as model-based RL, meta-learning and
richer multi-agent simulations are outlined. By providing a unified taxonomy,
quantitative statistics and a critical discussion of limitations, this review
aims to serve both as a reference for newcomers and as a roadmap for advancing
RL-based autonomous driving toward real-world deployment.

</details>


### [14] [Input-gated Bilateral Teleoperation: An Easy-to-implement Force Feedback Teleoperation Method for Low-cost Hardware](https://arxiv.org/abs/2509.08226)
*Yoshiki Kanai,Akira Kanazawa,Hideyuki Ichiwara,Hiroshi Ito,Naoaki Noguchi,Tetsuya Ogata*

Main category: cs.RO

TL;DR: 一种无需力传感器的双向遥控方法，通过简单反馈控制器实现力反馈，适用于低成本硬件，具有高操作性、接触稳定性和强健性


<details>
  <summary>Details</summary>
Motivation: 解决双向遥控系统复杂性高、实现困难的问题，提供一种简单易用的力反馈遥控方案

Method: 仅使用简单反馈控制器，无需力传感器，适用于主从设置的低成本硬件

Result: 方法参数调整最小化，但达到了高操作性和接触稳定性，超过传统方法；在低通信频率下性能衰减最小；在两种商用低成本硬件上无需参数调整即可实现

Conclusion: 该方法将扩大力反馈遥控系统在低成本硬件上的应用，推进任务自主性在仿真学习中的发展

Abstract: Effective data collection in contact-rich manipulation requires force
feedback during teleoperation, as accurate perception of contact is crucial for
stable control. However, such technology remains uncommon, largely because
bilateral teleoperation systems are complex and difficult to implement. To
overcome this, we propose a bilateral teleoperation method that relies only on
a simple feedback controller and does not require force sensors. The approach
is designed for leader-follower setups using low-cost hardware, making it
broadly applicable. Through numerical simulations and real-world experiments,
we demonstrate that the method requires minimal parameter tuning, yet achieves
both high operability and contact stability, outperforming conventional
approaches. Furthermore, we show its high robustness: even at low communication
cycle rates between leader and follower, control performance degradation is
minimal compared to high-speed operation. We also prove our method can be
implemented on two types of commercially available low-cost hardware with zero
parameter adjustments. This highlights its high ease of implementation and
versatility. We expect this method will expand the use of force feedback
teleoperation systems on low-cost hardware. This will contribute to advancing
contact-rich task autonomy in imitation learning.

</details>


### [15] [Deep Visual Odometry for Stereo Event Cameras](https://arxiv.org/abs/2509.08235)
*Sheng Zhong,Junkai Niu,Yi Zhou*

Main category: cs.RO

TL;DR: 提出了一种基于学习的立体事件视觉里程计系统Stereo-DEVO，通过新颖的静态立体关联策略和紧密耦合的束调整优化，实现了在HDR和低光条件下的实时高精度姿态估计。


<details>
  <summary>Details</summary>
Motivation: 传统基于手工数据关联的事件视觉里程计在低光高动态范围条件下不可靠，深度学习为解决这些挑战提供了新的可能性。

Method: 在DEVO基础上引入静态立体关联策略进行稀疏深度估计，结合紧密耦合的束调整优化和基于体素的事件表示进行光流估计。

Result: 系统能够实时处理VGA分辨率事件数据，在多个真实世界数据集上表现优于最先进的事件VO方法，特别在夜间HDR场景中保持稳定姿态估计。

Conclusion: Stereo-DEVO系统通过深度学习方法和新颖的立体关联策略，成功解决了事件视觉里程计在挑战性光照条件下的可靠性问题，实现了实时高性能的姿态估计。

Abstract: Event-based cameras are bio-inspired sensors with pixels that independently
and asynchronously respond to brightness changes at microsecond resolution,
offering the potential to handle state estimation tasks involving motion blur
and high dynamic range (HDR) illumination conditions. However, the versatility
of event-based visual odometry (VO) relying on handcrafted data association
(either direct or indirect methods) is still unreliable, especially in field
robot applications under low-light HDR conditions, where the dynamic range can
be enormous and the signal-to-noise ratio is spatially-and-temporally varying.
Leveraging deep neural networks offers new possibilities for overcoming these
challenges. In this paper, we propose a learning-based stereo event visual
odometry. Building upon Deep Event Visual Odometry (DEVO), our system (called
Stereo-DEVO) introduces a novel and efficient static-stereo association
strategy for sparse depth estimation with almost no additional computational
burden. By integrating it into a tightly coupled bundle adjustment (BA)
optimization scheme, and benefiting from the recurrent network's ability to
perform accurate optical flow estimation through voxel-based event
representations to establish reliable patch associations, our system achieves
high-precision pose estimation in metric scale. In contrast to the offline
performance of DEVO, our system can process event data of \zs{Video Graphics
Array} (VGA) resolution in real time. Extensive evaluations on multiple public
real-world datasets and self-collected data justify our system's versatility,
demonstrating superior performance compared to state-of-the-art event-based VO
methods. More importantly, our system achieves stable pose estimation even in
large-scale nighttime HDR scenarios.

</details>


### [16] [Sample-Efficient Online Control Policy Learning with Real-Time Recursive Model Updates](https://arxiv.org/abs/2509.08241)
*Zixin Zhang,James Avtges,Todd D. Murphey*

Main category: cs.RO

TL;DR: 提出递归Koopman学习(RKL)方法，通过Koopman理论将非线性系统表示为可观测量的线性模型，实现轻量级、快速且样本高效的数据驱动控制，在硬件系统上仅需基准方法10%的数据量。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法需要大数据集且难以实时更新模型，限制了在动态环境中的性能。特别是在硬件学习和数据获取、计算资源有限的情况下，需要样本高效且轻量级的控制方法。

Method: 基于Koopman理论，将非线性系统表示为可观测量的线性模型，通过递归学习实现快速模型更新。算法复杂度与数据集大小无关，支持实时更新。

Result: 在模拟平面二连杆臂和具有软执行器的混合非线性硬件系统上验证，实时递归Koopman模型更新显著提高了样本效率和控制器合成的稳定性，仅需基准方法<10%的数据量。

Conclusion: RKL方法提供了高度样本高效的Koopman学习管道，具有模型收敛的充分条件，复杂度与数据集大小无关，适用于资源受限的硬件学习场景，代码已开源。

Abstract: Data-driven control methods need to be sample-efficient and lightweight,
especially when data acquisition and computational resources are limited --
such as during learning on hardware. Most modern data-driven methods require
large datasets and struggle with real-time updates of models, limiting their
performance in dynamic environments. Koopman theory formally represents
nonlinear systems as linear models over observables, and Koopman
representations can be determined from data in an optimization-friendly setting
with potentially rapid model updates. In this paper, we present a highly
sample-efficient, Koopman-based learning pipeline: Recursive Koopman Learning
(RKL). We identify sufficient conditions for model convergence and provide
formal algorithmic analysis supporting our claim that RKL is lightweight and
fast, with complexity independent of dataset size. We validate our method on a
simulated planar two-link arm and a hybrid nonlinear hardware system with soft
actuators, showing that real-time recursive Koopman model updates improve the
sample efficiency and stability of data-driven controller synthesis --
requiring only <10% of the data compared to benchmarks. The high-performance
C++ codebase is open-sourced. Website:
https://www.zixinatom990.com/home/robotics/corl-2025-recursive-koopman-learning.

</details>


### [17] [Behaviorally Heterogeneous Multi-Agent Exploration Using Distributed Task Allocation](https://arxiv.org/abs/2509.08242)
*Nirabhra Mandal,Aamodh Suresh,Carlos Nieto-Granda,Sonia Martínez*

Main category: cs.RO

TL;DR: 提出了一种多智能体探索方法，使用行为熵和分布式博弈论算法进行任务分配，证明异构行为团队比同构团队更有效


<details>
  <summary>Details</summary>
Motivation: 研究多智能体探索中行为异构机器人的协同问题，旨在通过分布式算法优化探索效率和任务分配

Method: 使用SLAM构建地图并识别兴趣区域，通过行为熵评估前沿效用，将任务分配转化为非合作博弈问题，采用d-PBRAG分布式算法收敛至纳什均衡

Result: 算法通信成本低、收敛快，仿真实验表明异构行为团队在探索完成时间和路径长度方面表现更优

Conclusion: 行为异构的多智能体团队在探索任务中具有优势，分布式博弈论方法能有效解决任务分配问题

Abstract: We study a problem of multi-agent exploration with behaviorally heterogeneous
robots. Each robot maps its surroundings using SLAM and identifies a set of
areas of interest (AoIs) or frontiers that are the most informative to explore
next. The robots assess the utility of going to a frontier using Behavioral
Entropy (BE) and then determine which frontier to go to via a distributed task
assignment scheme. We convert the task assignment problem into a
non-cooperative game and use a distributed algorithm (d-PBRAG) to converge to
the Nash equilibrium (which we show is the optimal task allocation solution).
For unknown utility cases, we provide robust bounds using approximate rewards.
We test our algorithm (which has less communication cost and fast convergence)
in simulation, where we explore the effect of sensing radii, sensing accuracy,
and heterogeneity among robotic teams with respect to the time taken to
complete exploration and path traveled. We observe that having a team of agents
with heterogeneous behaviors is beneficial.

</details>


### [18] [Symmetry-Guided Multi-Agent Inverse Reinforcement Learnin](https://arxiv.org/abs/2509.08257)
*Yongkai Tian,Yirong Qi,Xin Yu,Wenjun Wu,Jie Luo*

Main category: cs.RO

TL;DR: 提出一个利用多智能体系统对称性的通用框架，显著提高多智能体逆强化学习的样本效率，减少对专家演示数据的依赖


<details>
  <summary>Details</summary>
Motivation: 在机器人系统中，手动设计的奖励函数往往不准确导致策略失败，而传统逆强化学习需要大量专家演示数据，收集成本高昂，特别是在多机器人系统中部署困难

Method: 基于多智能体系统的对称性理论，提出将对称性集成到现有多智能体对抗逆强化学习算法中的通用框架

Result: 在多个挑战性任务中的实验结果表明该框架有效，在物理多机器人系统中的验证证明了其实用性

Conclusion: 利用对称性可以恢复更准确的奖励函数，显著提高样本效率，解决了多智能体逆强化学习中的关键挑战

Abstract: In robotic systems, the performance of reinforcement learning depends on the
rationality of predefined reward functions. However, manually designed reward
functions often lead to policy failures due to inaccuracies. Inverse
Reinforcement Learning (IRL) addresses this problem by inferring implicit
reward functions from expert demonstrations. Nevertheless, existing methods
rely heavily on large amounts of expert demonstrations to accurately recover
the reward function. The high cost of collecting expert demonstrations in
robotic applications, particularly in multi-robot systems, severely hinders the
practical deployment of IRL. Consequently, improving sample efficiency has
emerged as a critical challenge in multi-agent inverse reinforcement learning
(MIRL). Inspired by the symmetry inherent in multi-agent systems, this work
theoretically demonstrates that leveraging symmetry enables the recovery of
more accurate reward functions. Building upon this insight, we propose a
universal framework that integrates symmetry into existing multi-agent
adversarial IRL algorithms, thereby significantly enhancing sample efficiency.
Experimental results from multiple challenging tasks have demonstrated the
effectiveness of this framework. Further validation in physical multi-robot
systems has shown the practicality of our method.

</details>


### [19] [Foundation Models for Autonomous Driving Perception: A Survey Through Core Capabilities](https://arxiv.org/abs/2509.08302)
*Rajendramayavan Sathyam,Yueqi Li*

Main category: cs.RO

TL;DR: 基础模型正在革命自动驾驶感知领域，本调查提出了一种新的分类法，重点关注四大关键能力：广泛知识、空间理解、多传感器稳健性和时间推理，以解决自动驾驶中的性能挑战。


<details>
  <summary>Details</summary>
Motivation: 解决传统任务特定深度学习模型在自动驾驶感知中的局限性，包括泛化能力不足、扩展性差和对分布偏移的稳健性问题。基础模型通过培养在广泛多样数据集上，提供了更灵活和通用的解决方案。

Method: 调查提出了一种新的分类法，以四大关键能力为核心结构：广泛知识、空间理解、多传感器稳健性和时间推理。不同于传统的方法个体调查，这个框架更加重视概念设计原则，为模型开发提供能力驱动的指南。

Result: 调查全面评估了各种先进方法在每个关键能力方面的表现，并强调了将这些能力集成到实时、可扩展系统中的挑战。同时识别了计算需求、模型可靠性（如幻觉和分布外失效）等更广泛的部署挑战。

Conclusion: 基础模型有望根本改变自动驾驶感知领域，但仍面临重大挑战。未来研究应重点关注如何安全有效地部署这些模型，确保其在实际应用中的可靠性和稳定性。这份调查为领域发展提供了概念性指南和研究方向。

Abstract: Foundation models are revolutionizing autonomous driving perception,
transitioning the field from narrow, task-specific deep learning models to
versatile, general-purpose architectures trained on vast, diverse datasets.
This survey examines how these models address critical challenges in autonomous
perception, including limitations in generalization, scalability, and
robustness to distributional shifts. The survey introduces a novel taxonomy
structured around four essential capabilities for robust performance in dynamic
driving environments: generalized knowledge, spatial understanding,
multi-sensor robustness, and temporal reasoning. For each capability, the
survey elucidates its significance and comprehensively reviews cutting-edge
approaches. Diverging from traditional method-centric surveys, our unique
framework prioritizes conceptual design principles, providing a
capability-driven guide for model development and clearer insights into
foundational aspects. We conclude by discussing key challenges, particularly
those associated with the integration of these capabilities into real-time,
scalable systems, and broader deployment challenges related to computational
demands and ensuring model reliability against issues like hallucinations and
out-of-distribution failures. The survey also outlines crucial future research
directions to enable the safe and effective deployment of foundation models in
autonomous driving systems.

</details>


### [20] [Good Deep Features to Track: Self-Supervised Feature Extraction and Tracking in Visual Odometry](https://arxiv.org/abs/2509.08333)
*Sai Puneeth Reddy Gottam,Haoming Zhang,Eivydas Keras*

Main category: cs.RO

TL;DR: 通过自监督学习与任务特定反馈增强深度特征提取和跟踪，提高大规模户外环境下视觉定位的稳定性和可靠性


<details>
  <summary>Details</summary>
Motivation: 解决大规模、户外、长期环境下视觉定位性能下降问题，包括光照变化、动态场景、低纹理区域等挑战导致的特征提取和跟踪困难

Method: 采用自监督学习方法，给予任务特定反馈来增强深度特征提取和跟踪能力

Result: 提升了特征的稳定性和信息密度，在具有挑战性的环境中改善了方法的沿用性和可靠性

Conclusion: 自监督学习结合任务特定反馈能够有效提升深度特征提取和跟踪的表现，为具有挑战性环境下的视觉定位提供了更好的解决方案

Abstract: Visual-based localization has made significant progress, yet its performance
often drops in large-scale, outdoor, and long-term settings due to factors like
lighting changes, dynamic scenes, and low-texture areas. These challenges
degrade feature extraction and tracking, which are critical for accurate motion
estimation. While learning-based methods such as SuperPoint and SuperGlue show
improved feature coverage and robustness, they still face generalization issues
with out-of-distribution data. We address this by enhancing deep feature
extraction and tracking through self-supervised learning with task specific
feedback. Our method promotes stable and informative features, improving
generalization and reliability in challenging environments.

</details>


### [21] [Grasp Like Humans: Learning Generalizable Multi-Fingered Grasping from Human Proprioceptive Sensorimotor Integration](https://arxiv.org/abs/2509.08354)
*Ce Guo,Xieyuanli Chen,Zhiwen Zeng,Zirui Guo,Yihong Li,Haoran Xiao,Dewen Hu,Huimin Lu*

Main category: cs.RO

TL;DR: 通过手套中介的触觉-动力觉知预测框架，将人类直觉性技能转移到机器手，支持包括可变形物体的普适性抓取任务


<details>
  <summary>Details</summary>
Motivation: 人类通过触觉和动力觉实现灵敏操控，但机器手将这种感知反馈映射到动作仍面临挑战，需要一种有效的技能转移方法

Method: 整合数据手套获取关节级触觉动力觉数据，构建基于极坐标图结构的统一多模态表示，开发TK-STGN网络通过多维子图卷积和关注LSTM提取时空特征，最终通过力-位置混合映射生成控制命令

Result: 框架能够有效处理不同场景的自然手势示范，确保原始数据格式一致性，并在包括可变形物体的普适性抓取任务中验证了有效性

Conclusion: 该方法为人类直觉性操作技能向机器执行的转移提供了一种有效的体验学习方案，通过图结构表征和时空特征提取实现了跨演示者和机器手的兼容性

Abstract: Tactile and kinesthetic perceptions are crucial for human dexterous
manipulation, enabling reliable grasping of objects via proprioceptive
sensorimotor integration. For robotic hands, even though acquiring such tactile
and kinesthetic feedback is feasible, establishing a direct mapping from this
sensory feedback to motor actions remains challenging. In this paper, we
propose a novel glove-mediated tactile-kinematic perception-prediction
framework for grasp skill transfer from human intuitive and natural operation
to robotic execution based on imitation learning, and its effectiveness is
validated through generalized grasping tasks, including those involving
deformable objects. Firstly, we integrate a data glove to capture tactile and
kinesthetic data at the joint level. The glove is adaptable for both human and
robotic hands, allowing data collection from natural human hand demonstrations
across different scenarios. It ensures consistency in the raw data format,
enabling evaluation of grasping for both human and robotic hands. Secondly, we
establish a unified representation of multi-modal inputs based on graph
structures with polar coordinates. We explicitly integrate the morphological
differences into the designed representation, enhancing the compatibility
across different demonstrators and robotic hands. Furthermore, we introduce the
Tactile-Kinesthetic Spatio-Temporal Graph Networks (TK-STGN), which leverage
multidimensional subgraph convolutions and attention-based LSTM layers to
extract spatio-temporal features from graph inputs to predict node-based states
for each hand joint. These predictions are then mapped to final commands
through a force-position hybrid mapping.

</details>


### [22] [PegasusFlow: Parallel Rolling-Denoising Score Sampling for Robot Diffusion Planner Flow Matching](https://arxiv.org/abs/2509.08435)
*Lei Ye,Haibo Gao,Peng Xu,Zhelin Zhang,Junqi Shan,Ao Zhang,Wei Zhang,Ruyi Zhou,Zongquan Deng,Liang Ding*

Main category: cs.RO

TL;DR: PegasusFlow是一个无需专家数据的机器人轨迹规划框架，通过分层滚动去噪和WBFO算法实现高效并行采样，在复杂地形导航任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在机器人轨迹规划中依赖专家演示数据的问题，特别是在专业机器人数据稀缺场景下的实际部署瓶颈

Method: 提出分层滚动去噪框架PegasusFlow，核心是加权基函数优化(WBFO)算法，利用样条基表示实现高效采样，结合异步并行仿真架构

Result: 在轨迹优化和机器人导航任务中显著优于基线方法，在障碍跨越任务中达到100%成功率和18%的速度提升

Conclusion: 该方法完全绕过专家数据需求，为复杂地形运动规划提供了高效实用的解决方案

Abstract: Diffusion models offer powerful generative capabilities for robot trajectory
planning, yet their practical deployment on robots is hindered by a critical
bottleneck: a reliance on imitation learning from expert demonstrations. This
paradigm is often impractical for specialized robots where data is scarce and
creates an inefficient, theoretically suboptimal training pipeline. To overcome
this, we introduce PegasusFlow, a hierarchical rolling-denoising framework that
enables direct and parallel sampling of trajectory score gradients from
environmental interaction, completely bypassing the need for expert data. Our
core innovation is a novel sampling algorithm, Weighted Basis Function
Optimization (WBFO), which leverages spline basis representations to achieve
superior sample efficiency and faster convergence compared to traditional
methods like MPPI. The framework is embedded within a scalable, asynchronous
parallel simulation architecture that supports massively parallel rollouts for
efficient data collection. Extensive experiments on trajectory optimization and
robotic navigation tasks demonstrate that our approach, particularly
Action-Value WBFO (AVWBFO) combined with a reinforcement learning warm-start,
significantly outperforms baselines. In a challenging barrier-crossing task,
our method achieved a 100% success rate and was 18% faster than the next-best
method, validating its effectiveness for complex terrain locomotion planning.
https://masteryip.github.io/pegasusflow.github.io/

</details>


### [23] [Augmenting Neural Networks-based Model Approximators in Robotic Force-tracking Tasks](https://arxiv.org/abs/2509.08440)
*Kevin Saad,Vincenzo Petrone,Enrico Ferrentino,Pasquale Chiacchio,Francesco Braghin,Loris Roveda*

Main category: cs.RO

TL;DR: 通过神经网络预测接触力并考虑切向速度，VAICAM控制器在力跟踪任务中表现优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统交互控制器需要大量调参或环境专家知识，在实际应用中不实用，特别是在高速运动时力跟踪性能不佳

Method: 使用神经网络集成预测接触力，考虑操作手的切向速度，通过优化问题生成最优剩余动作，加到直接力控制器输出中

Result: 在Gazebo模拟器中使用Franka Emika Panda机器人验证，在大量轨迹下VAICAM表现超过两种基线控制器

Conclusion: VAICAM提供了一种无需深度调参或环境知识的高效力跟踪控制方案，特别在快速运动场景中显示优势

Abstract: As robotics gains popularity, interaction control becomes crucial for
ensuring force tracking in manipulator-based tasks. Typically, traditional
interaction controllers either require extensive tuning, or demand expert
knowledge of the environment, which is often impractical in real-world
applications. This work proposes a novel control strategy leveraging Neural
Networks (NNs) to enhance the force-tracking behavior of a Direct Force
Controller (DFC). Unlike similar previous approaches, it accounts for the
manipulator's tangential velocity, a critical factor in force exertion,
especially during fast motions. The method employs an ensemble of feedforward
NNs to predict contact forces, then exploits the prediction to solve an
optimization problem and generate an optimal residual action, which is added to
the DFC output and applied to an impedance controller. The proposed
Velocity-augmented Artificial intelligence Interaction Controller for Ambiguous
Models (VAICAM) is validated in the Gazebo simulator on a Franka Emika Panda
robot. Against a vast set of trajectories, VAICAM achieves superior performance
compared to two baseline controllers.

</details>


### [24] [Dual-Stage Safe Herding Framework for Adversarial Attacker in Dynamic Environment](https://arxiv.org/abs/2509.08460)
*Wenqing Wang,Ye Zhang,Haoyu Li,Jingyu Wang*

Main category: cs.RO

TL;DR: 这篇论文提出了一种基于达到-避免游戏理论的层次混合框架，用于在复杂环境中安全导航对抗性代理到安全区域的方法。


<details>
  <summary>Details</summary>
Motivation: 传统的围攻方法在城市和障碍物丰富的场景中效果不佳或风险较大，特别是面对具有未知和适应性行为的对抗性代理时。

Method: 采用层次混合框架，结合达到-避免游戏理论和局部运动规划，包含虚拟容纳边界和事件触发追踪机制，支持可扩展的多代理协调。

Result: 模拟结果表明所提方法能够安全高效地将对抗性代理导航到指定区域。

Conclusion: 该研究为复杂环境中的对抗性代理导航问题提供了一种可扩展和稳健的解决方案。

Abstract: Recent advances in robotics have enabled the widespread deployment of
autonomous robotic systems in complex operational environments, presenting both
unprecedented opportunities and significant security problems. Traditional
shepherding approaches based on fixed formations are often ineffective or risky
in urban and obstacle-rich scenarios, especially when facing adversarial agents
with unknown and adaptive behaviors. This paper addresses this challenge as an
extended herding problem, where defensive robotic systems must safely guide
adversarial agents with unknown strategies away from protected areas and into
predetermined safe regions, while maintaining collision-free navigation in
dynamic environments. We propose a hierarchical hybrid framework based on
reach-avoid game theory and local motion planning, incorporating a virtual
containment boundary and event-triggered pursuit mechanisms to enable scalable
and robust multi-agent coordination. Simulation results demonstrate that the
proposed approach achieves safe and efficient guidance of adversarial agents to
designated regions.

</details>


### [25] [CLAP: Clustering to Localize Across n Possibilities, A Simple, Robust Geometric Approach in the Presence of Symmetries](https://arxiv.org/abs/2509.08495)
*Gabriel I. Fernandez,Ruochen Hou,Alex Xu,Colin Togashi,Dennis W. Hong*

Main category: cs.RO

TL;DR: CLAP是一种基于聚类的定位方法，通过将场特征对估计的状态进行聚类来实现全局定位，在RoboCup 2024人形足球机器人比赛中表现出优异的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于比赛规则限制传感器只能使用立体视觉和惯性传感器，且需要应对变化的光照条件、动态特征遮挡、高冲击步态噪声以及误检特征等挑战，需要开发准确且鲁棒的定位算法作为路径规划和比赛策略的基础。

Method: CLAP通过聚类从场特征对估计的机器人状态来实现定位，正确的状态估计自然聚集在一起，而错误估计则分散开，使其对噪声和错误输入具有弹性。该方法与粒子滤波和扩展卡尔曼滤波结合使用以提高一致性和平滑性。

Result: 测试表明CLAP与其他基于地标的定位方法具有相似的准确性，但在增加误检特征的情况下，CLAP在鲁棒性方面表现更优，几乎没有发散和速度跳跃。在实际比赛中表现良好，使机器人能够远距离射门并严密防守。

Conclusion: CLAP是一种有效的定位方法，特别适用于传感器受限且环境嘈杂的机器人足球比赛场景，其聚类方法提供了出色的鲁棒性和抗干扰能力。

Abstract: In this paper, we present our localization method called CLAP, Clustering to
Localize Across $n$ Possibilities, which helped us win the RoboCup 2024
adult-sized autonomous humanoid soccer competition. Competition rules limited
our sensor suite to stereo vision and an inertial sensor, similar to humans. In
addition, our robot had to deal with varying lighting conditions, dynamic
feature occlusions, noise from high-impact stepping, and mistaken features from
bystanders and neighboring fields. Therefore, we needed an accurate, and most
importantly robust localization algorithm that would be the foundation for our
path-planning and game-strategy algorithms. CLAP achieves these requirements by
clustering estimated states of our robot from pairs of field features to
localize its global position and orientation. Correct state estimates naturally
cluster together, while incorrect estimates spread apart, making CLAP resilient
to noise and incorrect inputs. CLAP is paired with a particle filter and an
extended Kalman filter to improve consistency and smoothness. Tests of CLAP
with other landmark-based localization methods showed similar accuracy.
However, tests with increased false positive feature detection showed that CLAP
outperformed other methods in terms of robustness with very little divergence
and velocity jumps. Our localization performed well in competition, allowing
our robot to shoot faraway goals and narrowly defend our goal.

</details>


### [26] [Facilitating the Emergence of Assistive Robots to Support Frailty: Psychosocial and Environmental Realities](https://arxiv.org/abs/2509.08510)
*Angela Higgins,Stephen Potter,Mauro Dragone,Mark Hawley,Farshid Amirabdollahian,Alessandro Di Nuovo,Praminda Caleb-Solly*

Main category: cs.RO

TL;DR: 通过共同设计研讨会发现，辅助机器人要真正应用于现实世界，必须考虑情感、社会和心理因素等复杂心理社会和环境因素的相互作用。


<details>
  <summary>Details</summary>
Motivation: 辅助机器人虽然有很大潜力帮助虚弱老年人，但实际应用很少，实验室开发与真实世界需求之间存在差距。

Method: 通过7场共61人参与的共同设计研讨会，包括有虚弱经历者、护理人员和医疗专业人员，采用基于角色的方法探讨情感、社会和心理问题。

Result: 研究发现任何辅助解决方案都必须在复杂的心理社会和环境因素相互作用背景下开发，提出了与虚弱直接相关的设计需求。

Conclusion: 研究结果可以帮助促进更务实的设计思维，使辅助机器人更接近现实世界应用。

Abstract: While assistive robots have much potential to help older people with
frailty-related needs, there are few in use. There is a gap between what is
developed in laboratories and what would be viable in real-world contexts.
Through a series of co-design workshops (61 participants across 7 sessions)
including those with lived experience of frailty, their carers, and healthcare
professionals, we gained a deeper understanding of everyday issues concerning
the place of new technologies in their lives. A persona-based approach surfaced
emotional, social, and psychological issues. Any assistive solution must be
developed in the context of this complex interplay of psychosocial and
environmental factors. Our findings, presented as design requirements in direct
relation to frailty, can help promote design thinking that addresses people's
needs in a more pragmatic way to move assistive robotics closer to real-world
use.

</details>


### [27] [FMT$^{x}$: An Efficient and Asymptotically Optimal Extension of the Fast Marching Tree for Dynamic Replanning](https://arxiv.org/abs/2509.08521)
*Soheil Espahbodini Nia*

Main category: cs.RO

TL;DR: FMT^x是Fast Marching Tree算法的扩展版本，通过在动态环境中实现高效重规划来解决传统FMT*算法无法实时适应环境变化的问题。


<details>
  <summary>Details</summary>
Motivation: 动态环境中的路径规划是机器人技术的核心挑战，传统FMT*算法在静态环境中提供渐近最优解，但其单次遍历设计无法进行路径修订，而完全重规划计算成本过高。

Method: 重新审视FMT*的邻居选择规则，通过最小化修改克服单次遍历限制，维护成本排序的优先队列，应用选择性更新条件，使用扩展邻居识别和触发潜在次优路径节点的重新评估。

Result: FMT^x在环境变化后能够恢复渐近最优解，实验结果表明其性能优于RRT^x，对动态事件反应更快，计算开销更低。

Conclusion: FMT^x保持了FMT*的内在效率，同时实现了对障碍物配置变化的鲁棒适应，为不可预测世界中的实时机器人导航提供了更有效的解决方案。

Abstract: Path planning in dynamic environments remains a core challenge in robotics,
especially as autonomous systems are deployed in unpredictable spaces such as
warehouses and public roads. While algorithms like Fast Marching Tree
(FMT$^{*}$) offer asymptotically optimal solutions in static settings, their
single-pass design prevents path revisions which are essential for real-time
adaptation. On the other hand, full replanning is often too computationally
expensive. This paper introduces FMT$^{x}$, an extension of the Fast Marching
Tree algorithm that enables efficient and consistent replanning in dynamic
environments. We revisit the neighbor selection rule of FMT$^{*}$ and
demonstrate that a minimal change overcomes its single-pass limitation,
enabling the algorithm to update cost-to-come values upon discovering better
connections without sacrificing asymptotic optimality or computational
efficiency. By maintaining a cost-ordered priority queue and applying a
selective update condition that uses an expanding neighbor to identify and
trigger the re-evaluation of any node with a potentially suboptimal path,
FMT$^{x}$ ensures that suboptimal routes are efficiently repaired as the
environment evolves. This targeted strategy preserves the inherent efficiency
of FMT$^{*}$ while enabling robust adaptation to changes in obstacle
configuration. FMT$^{x}$ is proven to recover an asymptotically optimal
solution after environmental changes. Experimental results demonstrate that
FMT$^{x}$ outperforms the influential replanner RRT$^{x}$, reacting more
swiftly to dynamic events with lower computational overhead and thus offering a
more effective solution for real-time robotic navigation in unpredictable
worlds.

</details>


### [28] [RoboMatch: A Mobile-Manipulation Teleoperation Platform with Auto-Matching Network Architecture for Long-Horizon Manipulation](https://arxiv.org/abs/2509.08522)
*Hanyu Liu,Yunsheng Ma,Jiaxin Huang,Keqiang Ren,Jiayi Wen,Yilin Zheng,Baishu Wan,Pan Li,Jiejun Hou,Haoru Luan,Zhihua Wang,Zhigong Song*

Main category: cs.RO

TL;DR: RoboMatch是一个统一的移动操作遥操作平台，采用自动匹配网络架构，通过驾驶舱式控制界面、PVE-DP策略和AMN架构，显著提升远程操作性能和数据收集效率。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中长时程移动操作任务的挑战，提高遥操作性能、数据收集效率和任务准确性。

Method: 1) 驾驶舱式控制界面实现移动底座和双臂同步操作；2) PVE-DP策略使用离散小波变换进行多尺度视觉特征提取，并集成高精度IMU传感器；3) AMN架构将长时程任务分解为逻辑序列，动态分配轻量级预训练模型进行分布式推理。

Result: 数据收集效率提升20%以上，PVE-DP使任务成功率提高20-30%，AMN使长时程推理性能提升约40%。

Conclusion: RoboMatch为复杂操作任务提供了强大的解决方案，在遥操作性能、数据效率和任务成功率方面均有显著改进。

Abstract: This paper presents RoboMatch, a novel unified teleoperation platform for
mobile manipulation with an auto-matching network architecture, designed to
tackle long-horizon tasks in dynamic environments. Our system enhances
teleoperation performance, data collection efficiency, task accuracy, and
operational stability. The core of RoboMatch is a cockpit-style control
interface that enables synchronous operation of the mobile base and dual arms,
significantly improving control precision and data collection. Moreover, we
introduce the Proprioceptive-Visual Enhanced Diffusion Policy (PVE-DP), which
leverages Discrete Wavelet Transform (DWT) for multi-scale visual feature
extraction and integrates high-precision IMUs at the end-effector to enrich
proprioceptive feedback, substantially boosting fine manipulation performance.
Furthermore, we propose an Auto-Matching Network (AMN) architecture that
decomposes long-horizon tasks into logical sequences and dynamically assigns
lightweight pre-trained models for distributed inference. Experimental results
demonstrate that our approach improves data collection efficiency by over 20%,
increases task success rates by 20-30% with PVE-DP, and enhances long-horizon
inference performance by approximately 40% with AMN, offering a robust solution
for complex manipulation tasks.

</details>


### [29] [AutoODD: Agentic Audits via Bayesian Red Teaming in Black-Box Models](https://arxiv.org/abs/2509.08638)
*Rebecca Martin,Jay Patrikar,Sebastian Scherer*

Main category: cs.RO

TL;DR: 提出了一个基于LLM-Agent的自动化测试框架，用于发现专业黑盒模型的故障模式，通过将高维输入空间映射到低维文本嵌入空间来构建故障分布模型


<details>
  <summary>Details</summary>
Motivation: 专业机器学习模型在部署中容易失败，需要确定其操作设计域来确保安全性和合规性，但传统方法需要大量人力和领域专业知识

Method: 使用LLM-Agent作为工具协调器，将高维输入空间投影到低维文本嵌入潜在空间，构建不确定性感知的故障分布模型，通过迭代生成测试用例来探测被测模型

Result: 在MNIST数据集缺失数字的模型和无人机视觉入侵检测的真实场景中验证了该方法的有效性

Conclusion: 该框架能够自动化地发现专业模型的故障模式，减少对人力和领域专业知识的需求，提高模型审计的效率和可靠性

Abstract: Specialized machine learning models, regardless of architecture and training,
are susceptible to failures in deployment. With their increasing use in high
risk situations, the ability to audit these models by determining their
operational design domain (ODD) is crucial in ensuring safety and compliance.
However, given the high-dimensional input spaces, this process often requires
significant human resources and domain expertise. To alleviate this, we
introduce \coolname, an LLM-Agent centric framework for automated generation of
semantically relevant test cases to search for failure modes in specialized
black-box models. By leveraging LLM-Agents as tool orchestrators, we aim to fit
a uncertainty-aware failure distribution model on a learned text-embedding
manifold by projecting the high-dimension input space to low-dimension
text-embedding latent space. The LLM-Agent is tasked with iteratively building
the failure landscape by leveraging tools for generating test-cases to probe
the model-under-test (MUT) and recording the response. The agent also guides
the search using tools to probe uncertainty estimate on the low dimensional
manifold. We demonstrate this process in a simple case using models trained
with missing digits on the MNIST dataset and in the real world setting of
vision-based intruder detection for aerial vehicles.

</details>


### [30] [TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals](https://arxiv.org/abs/2509.08699)
*Stefan Podgorski,Sourav Garg,Mehdi Hosseinzadeh,Lachlan Mares,Feras Dayoub,Ian Reid*

Main category: cs.RO

TL;DR: 一种新的RGB单目标导航管道，通过对象级判断和局部路径规划，实现了无需全局3D地图或领域特定训练的零样本导航能力


<details>
  <summary>Details</summary>
Motivation: 解决传统视觉导航方法依赖全局3D地图或领域特定训练的控制器，这些方法计算成本高且维护困难，难以在多样化环境中普遍推广

Method: 结合全局拓扑路径规划与局部路径控制，使用单目深度估计和可通行性预测连续预测局部路径，并包含自动切换机制在必要时调用基准控制器

Result: 在模拟环境和真实世界测试中表现出艰固性和可部署性，性能超过现有最先进方法

Conclusion: 该方法为开放集环境中的视觉导航提供了更适应性强、效果更好的解决方案，且代码开源可用

Abstract: Visual navigation in robotics traditionally relies on globally-consistent 3D
maps or learned controllers, which can be computationally expensive and
difficult to generalize across diverse environments. In this work, we present a
novel RGB-only, object-level topometric navigation pipeline that enables
zero-shot, long-horizon robot navigation without requiring 3D maps or
pre-trained controllers. Our approach integrates global topological path
planning with local metric trajectory control, allowing the robot to navigate
towards object-level sub-goals while avoiding obstacles. We address key
limitations of previous methods by continuously predicting local trajectory
using monocular depth and traversability estimation, and incorporating an
auto-switching mechanism that falls back to a baseline controller when
necessary. The system operates using foundational models, ensuring open-set
applicability without the need for domain-specific fine-tuning. We demonstrate
the effectiveness of our method in both simulated environments and real-world
tests, highlighting its robustness and deployability. Our approach outperforms
existing state-of-the-art methods, offering a more adaptable and effective
solution for visual navigation in open-set environments. The source code is
made publicly available: https://github.com/podgorki/TANGO.

</details>


### [31] [Parallel, Asymptotically Optimal Algorithms for Moving Target Traveling Salesman Problems](https://arxiv.org/abs/2509.08743)
*Anoop Bhat,Geordan Gutow,Bhaskar Vundurthy,Zhongqiang Ren,Sivakumar Rathinam,Howie Choset*

Main category: cs.RO

TL;DR: 提出了IRG框架解决移动目标旅行商问题，通过交替随机采样目标拦截点和求解广义TSP来实现渐进最优收敛，并开发了两个并行算法IRG-PGLNS和PCG。


<details>
  <summary>Details</summary>
Motivation: 移动目标旅行商问题在存在非线性目标轨迹或智能体运动学约束时，现有算法无法保证收敛到最优解，因此需要新的求解框架。

Method: IRG框架：交替随机采样智能体配置-时间点（对应目标拦截）和求解广义TSP序列。开发了两个并行算法：IRG-PGLNS（使用并行化PGLNS求解器）和PCG（同时求解多组点的GTSP）。

Result: 在三种MT-TSP变体上的数值结果表明，IRG-PGLNS和PCG都比基于先前工作的基线方法收敛更快。

Conclusion: IRG框架能够渐进收敛到最优解，提出的两个并行算法在多个MT-TSP变体上都表现出优于基线方法的性能。

Abstract: The Moving Target Traveling Salesman Problem (MT-TSP) seeks an agent
trajectory that intercepts several moving targets, within a particular time
window for each target. In the presence of generic nonlinear target
trajectories or kinematic constraints on the agent, no prior algorithm
guarantees convergence to an optimal MT-TSP solution. Therefore, we introduce
the Iterated Random Generalized (IRG) TSP framework. The key idea behind IRG is
to alternate between randomly sampling a set of agent configuration-time
points, corresponding to interceptions of targets, and finding a sequence of
interception points by solving a generalized TSP (GTSP). This alternation
enables asymptotic convergence to the optimum. We introduce two parallel
algorithms within the IRG framework. The first algorithm, IRG-PGLNS, solves
GTSPs using PGLNS, our parallelized extension of the state-of-the-art solver
GLNS. The second algorithm, Parallel Communicating GTSPs (PCG), solves GTSPs
corresponding to several sets of points simultaneously. We present numerical
results for three variants of the MT-TSP: one where intercepting a target only
requires coming within a particular distance, another where the agent is a
variable-speed Dubins car, and a third where the agent is a redundant robot
arm. We show that IRG-PGLNS and PCG both converge faster than a baseline based
on prior work.

</details>


### [32] [SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation](https://arxiv.org/abs/2509.08757)
*Michael J. Munje,Chen Tang,Shuijing Liu,Zichao Hu,Yifeng Zhu,Jiaxun Cui,Garrett Warnell,Joydeep Biswas,Peter Stone*

Main category: cs.RO

TL;DR: 这篇论文提出了社交导航场景理解测试集SocialNav-SUB，用于评估视觉-语言模型在社交机器人导航场景中的理解能力，发现当前模型仍落后于规则方法和人类水平。


<details>
  <summary>Details</summary>
Motivation: 社交机器人导航需要深度场景理解能力，而当前视觉-语言模型的社交场景理解能力缺乏系统性评估。

Method: 构建SocialNav-SUB数据集，包含需要空间、时空和社交推理的视觉问答任务，并与人类和规则基准进行比较。

Result: 最佳VLM模型与人类答案的一致性较高，但仍落后于规则方法和人类水平，显示当前模型在社交场景理解上存在显著空白。

Conclusion: 该测试集为社交机器人导航基础模型的研究提供了框架，显示了当前VLM模型在社交理解能力上的不足。

Abstract: Robot navigation in dynamic, human-centered environments requires
socially-compliant decisions grounded in robust scene understanding. Recent
Vision-Language Models (VLMs) exhibit promising capabilities such as object
recognition, common-sense reasoning, and contextual understanding-capabilities
that align with the nuanced requirements of social robot navigation. However,
it remains unclear whether VLMs can accurately understand complex social
navigation scenes (e.g., inferring the spatial-temporal relations among agents
and human intentions), which is essential for safe and socially compliant robot
navigation. While some recent works have explored the use of VLMs in social
robot navigation, no existing work systematically evaluates their ability to
meet these necessary conditions. In this paper, we introduce the Social
Navigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question
Answering (VQA) dataset and benchmark designed to evaluate VLMs for scene
understanding in real-world social robot navigation scenarios. SocialNav-SUB
provides a unified framework for evaluating VLMs against human and rule-based
baselines across VQA tasks requiring spatial, spatiotemporal, and social
reasoning in social robot navigation. Through experiments with state-of-the-art
VLMs, we find that while the best-performing VLM achieves an encouraging
probability of agreeing with human answers, it still underperforms simpler
rule-based approach and human consensus baselines, indicating critical gaps in
social scene understanding of current VLMs. Our benchmark sets the stage for
further research on foundation models for social robot navigation, offering a
framework to explore how VLMs can be tailored to meet real-world social robot
navigation needs. An overview of this paper along with the code and data can be
found at https://larg.github.io/socialnav-sub .

</details>


### [33] [Joint Model-based Model-free Diffusion for Planning with Constraints](https://arxiv.org/abs/2509.08775)
*Wonsuhk Jung,Utkarsh A. Mishra,Nadun Ranawaka Arachchige,Yongxin Chen,Danfei Xu,Shreyas Kousik*

Main category: cs.RO

TL;DR: JM2D是一个新颖的生成建模框架，通过联合采样方法解决模型自由扩散规划器与基于模型的优化模块的兼容性问题，无需额外训练即可提高任务性能同时保持安全性。


<details>
  <summary>Details</summary>
Motivation: 实际机器人系统中，模型自由扩散规划器需要与基于模型的优化模块（如安全约束）结合使用，但简单的集成会导致多模态输出与优化模块之间的兼容性挑战。

Method: JM2D将模块集成建模为联合采样问题，通过重要性采样和交互势函数引导模块输出，处理非可微目标和非凸优化模块。

Result: 在离线强化学习和机器人操作任务中，JM2D相比传统安全过滤器显著提高了任务性能，同时不牺牲安全性。

Conclusion: JM2D提供了一个有效的框架来解决扩散规划器与优化模块的集成问题，条件生成是其特例，为相关领域提供了重要设计指导。

Abstract: Model-free diffusion planners have shown great promise for robot motion
planning, but practical robotic systems often require combining them with
model-based optimization modules to enforce constraints, such as safety.
Naively integrating these modules presents compatibility challenges when
diffusion's multi-modal outputs behave adversarially to optimization-based
modules. To address this, we introduce Joint Model-based Model-free Diffusion
(JM2D), a novel generative modeling framework. JM2D formulates module
integration as a joint sampling problem to maximize compatibility via an
interaction potential, without additional training. Using importance sampling,
JM2D guides modules outputs based only on evaluations of the interaction
potential, thus handling non-differentiable objectives commonly arising from
non-convex optimization modules. We evaluate JM2D via application to aligning
diffusion planners with safety modules on offline RL and robot manipulation.
JM2D significantly improves task performance compared to conventional safety
filters without sacrificing safety. Further, we show that conditional
generation is a special case of JM2D and elucidate key design choices by
comparing with SOTA gradient-based and projection-based diffusion planners.
More details at: https://jm2d-corl25.github.io/.

</details>


### [34] [Calib3R: A 3D Foundation Model for Multi-Camera to Robot Calibration and 3D Metric-Scaled Scene Reconstruction](https://arxiv.org/abs/2509.08813)
*Davide Allegro,Matteo Terreran,Stefano Ghidoni*

Main category: cs.RO

TL;DR: Calib3R是一种无需标定板的联合标定方法，通过统一优化同时实现相机-机器人标定和度量尺度3D重建，支持单/多相机配置，仅需不到10张图像即可达到高精度。


<details>
  <summary>Details</summary>
Motivation: 机器人依赖RGB图像进行交互，但需要度量尺度的3D场景表示。传统方法将相机标定和3D重建分开处理，标定需要标定板，RGB重建则存在尺度不确定和坐标系不对齐的问题。

Method: 基于3D基础模型MASt3R从RGB图像提取点云图，结合机器人位姿通过统一优化重建与机器人坐标系对齐的度量尺度3D场景，支持机器人手臂和移动机器人的单/多相机配置。

Result: 在多样化数据集上的实验表明，Calib3R使用少于10张图像就能实现精确标定，性能优于无目标方法和基于标记的方法。

Conclusion: Calib3R提供了一种无需标定板的联合优化解决方案，有效解决了相机-机器人标定和度量尺度3D重建的统一处理问题，为机器人视觉系统提供了更简洁高效的标定方法。

Abstract: Robots often rely on RGB images for tasks like manipulation and navigation.
However, reliable interaction typically requires a 3D scene representation that
is metric-scaled and aligned with the robot reference frame. This depends on
accurate camera-to-robot calibration and dense 3D reconstruction, tasks usually
treated separately, despite both relying on geometric correspondences from RGB
data. Traditional calibration needs patterns, while RGB-based reconstruction
yields geometry with an unknown scale in an arbitrary frame. Multi-camera
setups add further complexity, as data must be expressed in a shared reference
frame. We present Calib3R, a patternless method that jointly performs
camera-to-robot calibration and metric-scaled 3D reconstruction via unified
optimization. Calib3R handles single- and multi-camera setups on robot arms or
mobile robots. It builds on the 3D foundation model MASt3R to extract pointmaps
from RGB images, which are combined with robot poses to reconstruct a scaled 3D
scene aligned with the robot. Experiments on diverse datasets show that Calib3R
achieves accurate calibration with less than 10 images, outperforming
target-less and marker-based methods.

</details>


### [35] [RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation](https://arxiv.org/abs/2509.08820)
*Zongzheng Zhang,Chenghao Yue,Haobo Xu,Minwen Liao,Xianglin Qi,Huan-ang Gao,Ziwei Wang,Hao Zhao*

Main category: cs.RO

TL;DR: RoboChemist是一个双循环框架，集成视觉语言模型(VLM)和视觉语言动作(VLA)模型，用于机器人化学实验，在成功率和合规性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决化学实验中机器人执行长时程程序、处理危险和可变形物质时的挑战，需要同时保证任务完成和严格遵守实验规范。

Method: 使用VLM作为规划器分解任务、生成视觉提示指导VLA模型、监控任务成功和合规性；引入基于图像的视觉目标VLA接口实现精确的目标条件控制。

Result: 系统成功执行原始动作和完整多步化学协议，平均成功率提高23.57%，合规率平均提升0.298，在物体和任务上表现出强泛化能力。

Conclusion: RoboChemist框架有效解决了透明实验器皿处理和语义级反馈等现有方法的局限性，为机器人化学家的发展提供了可行方案。

Abstract: Robotic chemists promise to both liberate human experts from repetitive tasks
and accelerate scientific discovery, yet remain in their infancy. Chemical
experiments involve long-horizon procedures over hazardous and deformable
substances, where success requires not only task completion but also strict
compliance with experimental norms. To address these challenges, we propose
\textit{RoboChemist}, a dual-loop framework that integrates Vision-Language
Models (VLMs) with Vision-Language-Action (VLA) models. Unlike prior VLM-based
systems (e.g., VoxPoser, ReKep) that rely on depth perception and struggle with
transparent labware, and existing VLA systems (e.g., RDT, pi0) that lack
semantic-level feedback for complex tasks, our method leverages a VLM to serve
as (1) a planner to decompose tasks into primitive actions, (2) a visual prompt
generator to guide VLA models, and (3) a monitor to assess task success and
regulatory compliance. Notably, we introduce a VLA interface that accepts
image-based visual targets from the VLM, enabling precise, goal-conditioned
control. Our system successfully executes both primitive actions and complete
multi-step chemistry protocols. Results show 23.57% higher average success rate
and a 0.298 average increase in compliance rate over state-of-the-art VLA
baselines, while also demonstrating strong generalization to objects and tasks.

</details>
