{"id": "2511.19528", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19528", "abs": "https://arxiv.org/abs/2511.19528", "authors": ["Rushuai Yang", "Zhiyuan Feng", "Tianxiang Zhang", "Kaixin Wang", "Chuheng Zhang", "Li Zhao", "Xiu Su", "Yi Chen", "Jiang Bian"], "title": "Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories", "comment": null, "summary": "Scaling vision-language-action (VLA) model pre-training requires large volumes of diverse, high-quality manipulation trajectories. Most current data is obtained via human teleoperation, which is expensive and difficult to scale. Reinforcement learning (RL) methods learn useful skills through autonomous exploration, making them a viable approach for generating data. However, standard RL training collapses to a narrow execution pattern, limiting its utility for large-scale pre-training. We propose Discover, Lea rn and Reinforce (DLR), an information-theoretic pattern discovery framework that generates multiple distinct, high-success behavioral patterns for VLA pretraining. Empirically, DLR generates a markedly more diverse trajectory corpus on LIBERO. Specifically, it learns multiple distinct, high-success strategies for the same task where standard RL discovers only one, and hence it covers substantially broader regions of the state-action space. When adapted to unseen downstream task suites, VLA models pretrained on our diverse RL data surpass counterparts trained on equal-sized standard RL datasets. Moreover, DLR exhibits positive data-scaling behavior that single-pattern RL lacks. These results position multi-pattern RL as a practical, scalable data engine for embodied foundation models.", "AI": {"tldr": "\u63d0\u51faDLR\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u8bba\u6a21\u5f0f\u53d1\u73b0\u751f\u6210\u591a\u79cd\u4e0d\u540c\u7684\u9ad8\u6210\u529f\u7387\u884c\u4e3a\u6a21\u5f0f\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u9884\u8bad\u7ec3\u63d0\u4f9b\u66f4\u591a\u6837\u5316\u7684\u8f68\u8ff9\u6570\u636e\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u9884\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u7684\u64cd\u63a7\u8f68\u8ff9\u6570\u636e\uff0c\u4f46\u4eba\u7c7b\u8fdc\u7a0b\u64cd\u4f5c\u83b7\u53d6\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u80fd\u901a\u8fc7\u81ea\u4e3b\u63a2\u7d22\u5b66\u4e60\u6709\u7528\u6280\u80fd\uff0c\u4f46\u6807\u51c6RL\u8bad\u7ec3\u4f1a\u6536\u655b\u5230\u5355\u4e00\u6267\u884c\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faDiscover\u3001Learn\u548cReinforce\uff08DLR\uff09\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u6a21\u5f0f\u53d1\u73b0\u6846\u67b6\uff0c\u80fd\u591f\u4e3aVLA\u9884\u8bad\u7ec3\u751f\u6210\u591a\u79cd\u4e0d\u540c\u7684\u9ad8\u6210\u529f\u7387\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDLR\u751f\u6210\u4e86\u660e\u663e\u66f4\u591a\u6837\u5316\u7684\u8f68\u8ff9\u8bed\u6599\u5e93\uff0c\u5b66\u4f1a\u4e86\u540c\u4e00\u4efb\u52a1\u7684\u591a\u79cd\u4e0d\u540c\u9ad8\u6210\u529f\u7387\u7b56\u7565\uff0c\u800c\u6807\u51c6RL\u53ea\u53d1\u73b0\u4e00\u79cd\u7b56\u7565\u3002\u5f53\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u4e0b\u6e38\u4efb\u52a1\u5957\u4ef6\u65f6\uff0c\u5728\u591a\u6837\u5316RL\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684VLA\u6a21\u578b\u4f18\u4e8e\u5728\u540c\u7b49\u89c4\u6a21\u6807\u51c6RL\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u5bf9\u5e94\u6a21\u578b\u3002", "conclusion": "DLR\u5c55\u793a\u4e86\u6807\u51c6\u5355\u6a21\u5f0fRL\u6240\u7f3a\u4e4f\u7684\u79ef\u6781\u6570\u636e\u6269\u5c55\u884c\u4e3a\uff0c\u5c06\u591a\u6a21\u5f0fRL\u5b9a\u4f4d\u4e3a\u5177\u8eab\u57fa\u7840\u6a21\u578b\u7684\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u6570\u636e\u5f15\u64ce\u3002"}}
{"id": "2511.19543", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19543", "abs": "https://arxiv.org/abs/2511.19543", "authors": ["Omar Faris", "S\u0142awomir Tadeja", "Fulvio Forni"], "title": "A Virtual Mechanical Interaction Layer Enables Resilient Human-to-Robot Object Handovers", "comment": null, "summary": "Object handover is a common form of interaction that is widely present in collaborative tasks. However, achieving it efficiently remains a challenge. We address the problem of ensuring resilient robotic actions that can adapt to complex changes in object pose during human-to-robot object handovers. We propose the use of Virtual Model Control to create an interaction layer that controls the robot and adapts to the dynamic changes in the handover process. Additionally, we propose the use of augmented reality to facilitate bidirectional communication between humans and robots during handovers. We assess the performance of our controller in a set of experiments that demonstrate its resilience to various sources of uncertainties, including complex changes to the object's pose during the handover. Finally, we performed a user study with 16 participants to understand human preferences for different robot control profiles and augmented reality visuals in object handovers. Our results showed a general preference for the proposed approach and revealed insights that can guide further development in adapting the interaction with the user.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u865a\u62df\u6a21\u578b\u63a7\u5236\u548c\u589e\u5f3a\u73b0\u5b9e\u7684\u4eba\u673a\u7269\u4f53\u4ea4\u63a5\u65b9\u6cd5\uff0c\u80fd\u591f\u9002\u5e94\u4ea4\u63a5\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u53d8\u5316\u548c\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u7269\u4f53\u4ea4\u63a5\u662f\u534f\u4f5c\u4efb\u52a1\u4e2d\u7684\u5e38\u89c1\u4ea4\u4e92\u5f62\u5f0f\uff0c\u4f46\u5b9e\u73b0\u9ad8\u6548\u4ea4\u63a5\u4ecd\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u673a\u5668\u4eba\u80fd\u591f\u9002\u5e94\u4ea4\u63a5\u8fc7\u7a0b\u4e2d\u7269\u4f53\u59ff\u6001\u7684\u590d\u6742\u53d8\u5316\u3002", "method": "\u4f7f\u7528\u865a\u62df\u6a21\u578b\u63a7\u5236\u521b\u5efa\u4ea4\u4e92\u5c42\u6765\u63a7\u5236\u673a\u5668\u4eba\u5e76\u9002\u5e94\u4ea4\u63a5\u8fc7\u7a0b\u7684\u52a8\u6001\u53d8\u5316\uff0c\u540c\u65f6\u5229\u7528\u589e\u5f3a\u73b0\u5b9e\u4fc3\u8fdb\u4eba\u673a\u53cc\u5411\u901a\u4fe1\u3002", "result": "\u63a7\u5236\u5668\u5728\u5404\u79cd\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u7684\u97e7\u6027\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u53c2\u4e0e\u8005\u666e\u904d\u504f\u597d\u8be5\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u6307\u5bfc\u8fdb\u4e00\u6b65\u53d1\u5c55\u7684\u89c1\u89e3\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4eba\u673a\u7269\u4f53\u4ea4\u63a5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4e3a\u9002\u5e94\u7528\u6237\u4ea4\u4e92\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2511.19647", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19647", "abs": "https://arxiv.org/abs/2511.19647", "authors": ["Jennifer Grannen", "Michelle Pan", "Kenneth Llontop", "Cherie Ho", "Mark Zolotas", "Jeannette Bohg", "Dorsa Sadigh"], "title": "Robot-Powered Data Flywheels: Deploying Robots in the Wild for Continual Data Collection and Foundation Model Adaptation", "comment": null, "summary": "Foundation models (FM) have unlocked powerful zero-shot capabilities in vision and language, yet their reliance on internet pretraining data leaves them brittle in unstructured, real-world settings. The messy, real-world data encountered during deployment (e.g. occluded or multilingual text) remains massively underrepresented in existing corpora. Robots, as embodied agents, are uniquely positioned to close this gap: they can act in physical environments to collect large-scale, real-world data that enriches FM training with precisely the examples current models lack. We introduce the Robot-Powered Data Flywheel, a framework that transforms robots from FM consumers into data generators. By deploying robots equipped with FMs in the wild, we enable a virtuous cycle: robots perform useful tasks while collecting real-world data that improves both domain-specific adaptation and domain-adjacent generalization. We instantiate this framework with Scanford, a mobile manipulator deployed in the East Asia Library for 2 weeks. Scanford autonomously scans shelves, identifies books using a vision-language model (VLM), and leverages the library catalog to label images without human annotation. This deployment both aids librarians and produces a dataset to finetune the underlying VLM, improving performance on the domain-specific in-the-wild library setting and on domain-adjacent multilingual OCR benchmarks. Using data collected from 2103 shelves, Scanford improves VLM performance on book identification from 32.0% to 71.8% and boosts domain-adjacent multilingual OCR from 24.8% to 46.6% (English) and 30.8% to 38.0% (Chinese), while saving an ~18.7 hrs of human time. These results highlight how robot-powered data flywheels can both reduce human effort in real deployments and unlock new pathways for continually adapting FMs to the messiness of reality. More details are at: https://scanford-robot.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86\u673a\u5668\u4eba\u9a71\u52a8\u7684\u6570\u636e\u98de\u8f6e\u6846\u67b6\uff0c\u5c06\u673a\u5668\u4eba\u4ece\u57fa\u7840\u6a21\u578b\u6d88\u8d39\u8005\u8f6c\u53d8\u4e3a\u6570\u636e\u751f\u6210\u5668\u3002\u901a\u8fc7\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u90e8\u7f72\u914d\u5907\u57fa\u7840\u6a21\u578b\u7684\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u826f\u6027\u5faa\u73af\uff1a\u673a\u5668\u4eba\u6267\u884c\u6709\u7528\u4efb\u52a1\u7684\u540c\u65f6\u6536\u96c6\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u7528\u4e8e\u6539\u8fdb\u9886\u57df\u7279\u5b9a\u9002\u5e94\u548c\u9886\u57df\u76f8\u90bb\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u4f46\u4f9d\u8d56\u4e92\u8054\u7f51\u9884\u8bad\u7ec3\u6570\u636e\u4f7f\u5176\u5728\u975e\u7ed3\u6784\u5316\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u8106\u5f31\u3002\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u6742\u4e71\u6570\u636e\uff08\u5982\u906e\u6321\u6216\u591a\u8bed\u8a00\u6587\u672c\uff09\u5728\u73b0\u6709\u8bed\u6599\u5e93\u4e2d\u4e25\u91cd\u4e0d\u8db3\u3002\u673a\u5668\u4eba\u4f5c\u4e3a\u5177\u8eab\u4ee3\u7406\uff0c\u80fd\u591f\u901a\u8fc7\u7269\u7406\u73af\u5883\u4ea4\u4e92\u6536\u96c6\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6765\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u5f00\u53d1\u4e86Scanford\u79fb\u52a8\u673a\u68b0\u81c2\u7cfb\u7edf\uff0c\u5728\u4e1c\u4e9a\u56fe\u4e66\u9986\u90e8\u7f722\u5468\u3002\u7cfb\u7edf\u81ea\u4e3b\u626b\u63cf\u4e66\u67b6\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u4e66\u7c4d\uff0c\u5e76\u5229\u7528\u56fe\u4e66\u9986\u76ee\u5f55\u8fdb\u884c\u65e0\u4eba\u5de5\u6807\u6ce8\u7684\u56fe\u50cf\u6807\u8bb0\u3002\u901a\u8fc72103\u4e2a\u4e66\u67b6\u7684\u6570\u636e\u6536\u96c6\uff0c\u5b9e\u73b0\u6570\u636e\u98de\u8f6e\u5faa\u73af\u3002", "result": "Scanford\u5c06\u4e66\u7c4d\u8bc6\u522b\u51c6\u786e\u7387\u4ece32.0%\u63d0\u5347\u81f371.8%\uff0c\u9886\u57df\u76f8\u90bb\u7684\u591a\u8bed\u8a00OCR\u6027\u80fd\u4ece24.8%\u63d0\u5347\u81f346.6%\uff08\u82f1\u6587\uff09\u548c30.8%\u63d0\u5347\u81f338.0%\uff08\u4e2d\u6587\uff09\uff0c\u540c\u65f6\u8282\u7701\u7ea618.7\u5c0f\u65f6\u4eba\u5de5\u65f6\u95f4\u3002", "conclusion": "\u673a\u5668\u4eba\u9a71\u52a8\u7684\u6570\u636e\u98de\u8f6e\u65e2\u80fd\u51cf\u5c11\u771f\u5b9e\u90e8\u7f72\u4e2d\u7684\u4eba\u5de5\u52aa\u529b\uff0c\u53c8\u80fd\u4e3a\u57fa\u7840\u6a21\u578b\u6301\u7eed\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\u5f00\u8f9f\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.19651", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19651", "abs": "https://arxiv.org/abs/2511.19651", "authors": ["Lishuo Pan", "Mattia Catellani", "Thales C. Silva", "Lorenzo Sabattini", "Nora Ayanian"], "title": "Online Learning-Enhanced High Order Adaptive Safety Control", "comment": "8 pages, 7 figures, submitted to RA-L", "summary": "Control barrier functions (CBFs) are an effective model-based tool to formally certify the safety of a system. With the growing complexity of modern control problems, CBFs have received increasing attention in both optimization-based and learning-based control communities as a safety filter, owing to their provable guarantees. However, success in transferring these guarantees to real-world systems is critically tied to model accuracy. For example, payloads or wind disturbances can significantly influence the dynamics of an aerial vehicle and invalidate the safety guarantee. In this work, we propose an efficient yet flexible online learning-enhanced high-order adaptive control barrier function using Neural ODEs. Our approach improves the safety of a CBF-certified system on the fly, even under complex time-varying model perturbations. In particular, we deploy our hybrid adaptive CBF controller on a 38g nano quadrotor, keeping a safe distance from the obstacle, against 18km/h wind.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecfODE\u7684\u5728\u7ebf\u5b66\u4e60\u589e\u5f3a\u9ad8\u9636\u81ea\u9002\u5e94\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u590d\u6742\u65f6\u53d8\u6a21\u578b\u6270\u52a8\u4e0b\u5b9e\u65f6\u63d0\u9ad8CBF\u8ba4\u8bc1\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u5e76\u572838g\u7eb3\u7c73\u56db\u65cb\u7ffc\u4e0a\u6210\u529f\u9a8c\u8bc1\uff0c\u572818km/h\u98ce\u901f\u4e0b\u4fdd\u6301\u4e0e\u969c\u788d\u7269\u7684\u5b89\u5168\u8ddd\u79bb\u3002", "motivation": "\u63a7\u5236\u5c4f\u969c\u51fd\u6570(CBFs)\u662f\u4fdd\u8bc1\u7cfb\u7edf\u5b89\u5168\u6027\u7684\u6709\u6548\u5de5\u5177\uff0c\u4f46\u5176\u5b89\u5168\u4fdd\u8bc1\u7684\u6210\u529f\u8f6c\u79fb\u5230\u73b0\u5b9e\u7cfb\u7edf\u4e25\u91cd\u4f9d\u8d56\u4e8e\u6a21\u578b\u51c6\u786e\u6027\u3002\u6709\u6548\u8f7d\u8377\u6216\u98ce\u6270\u52a8\u7b49\u4f1a\u663e\u8457\u5f71\u54cd\u98de\u884c\u5668\u52a8\u529b\u5b66\u5e76\u7834\u574f\u5b89\u5168\u4fdd\u8bc1\u3002", "method": "\u4f7f\u7528\u795e\u7ecfODE\u5f00\u53d1\u9ad8\u6548\u7075\u6d3b\u7684\u9ad8\u9636\u81ea\u9002\u5e94\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff0c\u7ed3\u5408\u5728\u7ebf\u5b66\u4e60\u589e\u5f3a\uff0c\u5728\u590d\u6742\u65f6\u53d8\u6a21\u578b\u6270\u52a8\u4e0b\u5b9e\u65f6\u6539\u8fdbCBF\u8ba4\u8bc1\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002", "result": "\u572838g\u7eb3\u7c73\u56db\u65cb\u7ffc\u4e0a\u6210\u529f\u90e8\u7f72\u6df7\u5408\u81ea\u9002\u5e94CBF\u63a7\u5236\u5668\uff0c\u572818km/h\u98ce\u901f\u4e0b\u80fd\u591f\u4fdd\u6301\u4e0e\u969c\u788d\u7269\u7684\u5b89\u5168\u8ddd\u79bb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u590d\u6742\u65f6\u53d8\u6a21\u578b\u6270\u52a8\u4e0b\u6709\u6548\u63d0\u9ad8CBF\u8ba4\u8bc1\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u7cfb\u7edf\u7684\u5b89\u5168\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.19653", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19653", "abs": "https://arxiv.org/abs/2511.19653", "authors": ["Mahmud Suhaimi Ibrahim", "Shantanu Rahman", "Muhammad Samin Hasan", "Minhaj Uddin Ahmad", "Abdullah Abrar"], "title": "Flow-Based Path Planning for Multiple Homogenous UAVs for Outdoor Formation-Flying", "comment": "9 pages, 15 figures, conference", "summary": "Collision-free path planning is the most crucial component in multi-UAV formation-flying (MFF). We use unlabeled homogenous quadcopters (UAVs) to demonstrate the use of a flow network to create complete (inter-UAV) collision-free paths. This procedure has three main parts: 1) Creating a flow network graph from physical GPS coordinates, 2) Finding a path of minimum cost (least distance) using any graph-based path-finding algorithm, and 3) Implementing the Ford-Fulkerson Method to find the paths with the maximum flow (no collision). Simulations of up to 64 UAVs were conducted for various formations, followed by a practical experiment with 3 quadcopters for testing physical plausibility and feasibility. The results of these tests show the efficacy of this method's ability to produce safe, collision-free paths.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u7f51\u7edc\u7684\u591a\u65e0\u4eba\u673a\u7f16\u961f\u98de\u884c\u65e0\u78b0\u649e\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d41\u7f51\u7edc\u56fe\u3001\u6700\u5c0f\u6210\u672c\u8def\u5f84\u7b97\u6cd5\u548cFord-Fulkerson\u6700\u5927\u6d41\u65b9\u6cd5\u5b9e\u73b0\u5b89\u5168\u8def\u5f84\u89c4\u5212\u3002", "motivation": "\u591a\u65e0\u4eba\u673a\u7f16\u961f\u98de\u884c\u4e2d\u7684\u65e0\u78b0\u649e\u8def\u5f84\u89c4\u5212\u662f\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u65e0\u4eba\u673a\u95f4\u7684\u78b0\u649e\u95ee\u9898\u3002", "method": "1) \u4eceGPS\u5750\u6807\u6784\u5efa\u6d41\u7f51\u7edc\u56fe\uff1b2) \u4f7f\u7528\u56fe\u8def\u5f84\u7b97\u6cd5\u5bfb\u627e\u6700\u5c0f\u6210\u672c\u8def\u5f84\uff1b3) \u5e94\u7528Ford-Fulkerson\u65b9\u6cd5\u5bfb\u627e\u6700\u5927\u6d41\u65e0\u78b0\u649e\u8def\u5f84\u3002", "result": "\u8fdb\u884c\u4e86\u6700\u591a64\u67b6\u65e0\u4eba\u673a\u7684\u4eff\u771f\u548c3\u67b6\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7684\u5b9e\u9645\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5b89\u5168\u7684\u65e0\u78b0\u649e\u8def\u5f84\uff0c\u5728\u591a\u65e0\u4eba\u673a\u7f16\u961f\u98de\u884c\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.19655", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19655", "abs": "https://arxiv.org/abs/2511.19655", "authors": ["Shantanu Rahman", "Nayeb Hasin", "Mainul Islam"], "title": "Development of a Testbed for Autonomous Vehicles: Integrating MPC Control with Monocular Camera Lane Detection", "comment": "49 pages, 23 figures", "summary": "Autonomous vehicles are becoming popular day by day not only for autonomous road traversal but also for industrial automation, farming and military. Most of the standard vehicles follow the Ackermann style steering mechanism. This has become to de facto standard for large and long faring vehicles. The local planner of an autonomous vehicle controls the low-level vehicle movement upon which the vehicle will perform its motor actuation. In our work, we focus on autonomous vehicles in road and perform experiments to analyze the effect of low-level controllers in the simulation and a real environment. To increase the precision and stability of trajectory tracking in autonomous cars, a novel method that combines lane identification with Model Predictive Control (MPC) is presented. The research focuses on camera-equipped autonomous vehicles and uses methods like edge recognition, sliding window-based straight-line identification for lane line extraction, and dynamic region of interest (ROI) extraction. Next, to follow the identified lane line, an MPC built on a bicycle vehicle dynamics model is created. A single-lane road simulation model is built using ROS Gazebo and tested in order to verify the controller's performance. The root mean square error between the optimal tracking trajectory and the target trajectory was reduced by 27.65% in the simulation results, demonstrating the high robustness and flexibility of the developed controller.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8f66\u9053\u8bc6\u522b\u4e0e\u6a21\u578b\u9884\u6d4b\u63a7\u5236(MPC)\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u8f68\u8ff9\u8ddf\u8e2a\u7684\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u5728\u4eff\u771f\u4e2d\u8f68\u8ff9\u8ddf\u8e2a\u8bef\u5dee\u964d\u4f4e\u4e8627.65%\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u9053\u8def\u5e94\u7528\u4e2d\u9700\u8981\u63d0\u9ad8\u8f68\u8ff9\u8ddf\u8e2a\u7684\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u91c7\u7528\u963f\u514b\u66fc\u8f6c\u5411\u673a\u5236\u7684\u6807\u51c6\u8f66\u8f86\u3002", "method": "\u4f7f\u7528\u8fb9\u7f18\u8bc6\u522b\u3001\u6ed1\u52a8\u7a97\u53e3\u76f4\u7ebf\u8bc6\u522b\u8fdb\u884c\u8f66\u9053\u7ebf\u63d0\u53d6\uff0c\u7ed3\u5408\u57fa\u4e8e\u81ea\u884c\u8f66\u8f66\u8f86\u52a8\u529b\u5b66\u6a21\u578b\u7684MPC\u63a7\u5236\u5668\uff0c\u5e76\u5728ROS Gazebo\u4e2d\u6784\u5efa\u5355\u8f66\u9053\u9053\u8def\u4eff\u771f\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u6700\u4f18\u8ddf\u8e2a\u8f68\u8ff9\u4e0e\u76ee\u6807\u8f68\u8ff9\u4e4b\u95f4\u7684\u5747\u65b9\u6839\u8bef\u5dee\u964d\u4f4e\u4e8627.65%\uff0c\u8bc1\u660e\u4e86\u6240\u5f00\u53d1\u63a7\u5236\u5668\u7684\u9ad8\u9c81\u68d2\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "\u7ed3\u5408\u8f66\u9053\u8bc6\u522b\u4e0eMPC\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u8f68\u8ff9\u8ddf\u8e2a\u6027\u80fd\uff0c\u5728\u4eff\u771f\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u63a7\u5236\u6548\u679c\u3002"}}
{"id": "2511.19691", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19691", "abs": "https://arxiv.org/abs/2511.19691", "authors": ["Thomas Marshall Vielmetti", "Devansh R Agrawal", "Dimitra Panagou"], "title": "Multi-Agent gatekeeper: Safe Flight Planning and Formation Control for Urban Air Mobility", "comment": "13 pages, 4 figures, to appear AIAA SciTech 2026", "summary": "We present Multi-Agent gatekeeper, a framework that provides provable safety guarantees for leader-follower formation control in cluttered 3D environments. Existing methods face a trad-off: online planners and controllers lack formal safety guarantees, while offline planners lack adaptability to changes in the number of agents or desired formation. To address this gap, we propose a hybrid architecture where a single leader tracks a pre-computed, safe trajectory, which serves as a shared trajectory backup set for all follower agents. Followers execute a nominal formation-keeping tracking controller, and are guaranteed to remain safe by always possessing a known-safe backup maneuver along the leader's path. We formally prove this method ensures collision avoidance with both static obstacles and other agents. The primary contributions are: (1) the multi-agent gatekeeper algorithm, which extends our single-agent gatekeeper framework to multi-agent systems; (2) the trajectory backup set for provably safe inter-agent coordination for leader-follower formation control; and (3) the first application of the gatekeeper framework in a 3D environment. We demonstrate our approach in a simulated 3D urban environment, where it achieved a 100% collision-avoidance success rate across 100 randomized trials, significantly outperforming baseline CBF and NMPC methods. Finally, we demonstrate the physical feasibility of the resulting trajectories on a team of quadcopters.", "AI": {"tldr": "\u63d0\u51fa\u4e86Multi-Agent gatekeeper\u6846\u67b6\uff0c\u4e3a3D\u6742\u4e71\u73af\u5883\u4e2d\u7684\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u7f16\u961f\u63a7\u5236\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u5b89\u5168\u4fdd\u8bc1\uff0c\u901a\u8fc7\u9886\u5bfc\u8005\u5b89\u5168\u8f68\u8ff9\u4f5c\u4e3a\u5171\u4eab\u5907\u4efd\u96c6\uff0c\u786e\u4fdd\u8ddf\u968f\u8005\u59cb\u7ec8\u6709\u5b89\u5168\u5907\u4efd\u673a\u52a8\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u6743\u8861\uff1a\u5728\u7ebf\u89c4\u5212\u5668\u548c\u63a7\u5236\u5668\u7f3a\u4e4f\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u8bc1\uff0c\u800c\u79bb\u7ebf\u89c4\u5212\u5668\u7f3a\u4e4f\u5bf9\u4ee3\u7406\u6570\u91cf\u6216\u671f\u671b\u7f16\u961f\u53d8\u5316\u7684\u9002\u5e94\u6027\u3002", "method": "\u91c7\u7528\u6df7\u5408\u67b6\u6784\uff0c\u5355\u4e2a\u9886\u5bfc\u8005\u8ddf\u8e2a\u9884\u8ba1\u7b97\u7684\u5b89\u5168\u8f68\u8ff9\u4f5c\u4e3a\u5171\u4eab\u8f68\u8ff9\u5907\u4efd\u96c6\uff0c\u8ddf\u968f\u8005\u6267\u884c\u540d\u4e49\u7f16\u961f\u4fdd\u6301\u8ddf\u8e2a\u63a7\u5236\u5668\uff0c\u5e76\u59cb\u7ec8\u62e5\u6709\u6cbf\u9886\u5bfc\u8005\u8def\u5f84\u7684\u5df2\u77e5\u5b89\u5168\u5907\u4efd\u673a\u52a8\u3002", "result": "\u5728\u6a21\u62df3D\u57ce\u5e02\u73af\u5883\u4e2d\uff0c100\u6b21\u968f\u673a\u8bd5\u9a8c\u4e2d\u5b9e\u73b0\u4e86100%\u7684\u78b0\u649e\u907f\u514d\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfCBF\u548cNMPC\u65b9\u6cd5\uff0c\u5e76\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u56e2\u961f\u4e0a\u9a8c\u8bc1\u4e86\u8f68\u8ff9\u7684\u7269\u7406\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u8bc1\u660e\u7684\u5b89\u5168\u4fdd\u8bc1\uff0c\u662f\u95e8\u536b\u6846\u67b6\u57283D\u73af\u5883\u4e2d\u7684\u9996\u6b21\u5e94\u7528\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u7684\u591a\u667a\u80fd\u4f53\u534f\u8c03\u3002"}}
{"id": "2511.19709", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19709", "abs": "https://arxiv.org/abs/2511.19709", "authors": ["Lukas Molnar", "Jin Cheng", "Gabriele Fadini", "Dongho Kang", "Fatemeh Zargarbashi", "Stelian Coros"], "title": "Whole-Body Inverse Dynamics MPC for Legged Loco-Manipulation", "comment": "9 pages, 6 figures, to be published in IEEE Robotics and Automation Letters (Special Issue: Advancements in MPC and Learning Algorithms for Legged Robots)", "summary": "Loco-manipulation demands coordinated whole-body motion to manipulate objects effectively while maintaining locomotion stability, presenting significant challenges for both planning and control. In this work, we propose a whole-body model predictive control (MPC) framework that directly optimizes joint torques through full-order inverse dynamics, enabling unified motion and force planning and execution within a single predictive layer. This approach allows emergent, physically consistent whole-body behaviors that account for the system's dynamics and physical constraints. We implement our MPC formulation using open software frameworks (Pinocchio and CasADi), along with the state-of-the-art interior-point solver Fatrop. In real-world experiments on a Unitree B2 quadruped equipped with a Unitree Z1 manipulator arm, our MPC formulation achieves real-time performance at 80 Hz. We demonstrate loco-manipulation tasks that demand fine control over the end-effector's position and force to perform real-world interactions like pulling heavy loads, pushing boxes, and wiping whiteboards.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u8eab\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u9636\u9006\u52a8\u529b\u5b66\u76f4\u63a5\u4f18\u5316\u5173\u8282\u626d\u77e9\uff0c\u5728\u5355\u4e2a\u9884\u6d4b\u5c42\u5185\u5b9e\u73b0\u7edf\u4e00\u7684\u8fd0\u52a8\u548c\u529b\u89c4\u5212\u4e0e\u6267\u884c\uff0c\u5728\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e8680Hz\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u5168\u8eab\u8fd0\u52a8\u64cd\u4f5c\u9700\u8981\u534f\u8c03\u7684\u5168\u8eab\u8fd0\u52a8\u6765\u6709\u6548\u64cd\u7eb5\u7269\u4f53\uff0c\u540c\u65f6\u4fdd\u6301\u8fd0\u52a8\u7a33\u5b9a\u6027\uff0c\u8fd9\u5bf9\u89c4\u5212\u548c\u63a7\u5236\u90fd\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u4f7f\u7528\u5168\u8eab\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u9636\u9006\u52a8\u529b\u5b66\u76f4\u63a5\u4f18\u5316\u5173\u8282\u626d\u77e9\uff0c\u91c7\u7528Pinocchio\u3001CasADi\u8f6f\u4ef6\u6846\u67b6\u548cFatrop\u6c42\u89e3\u5668\u5b9e\u73b0\u3002", "result": "\u5728\u914d\u5907\u673a\u68b0\u81c2\u7684Unitree B2\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e8680Hz\u7684\u5b9e\u65f6\u6027\u80fd\uff0c\u6210\u529f\u5b8c\u6210\u4e86\u62c9\u52a8\u91cd\u7269\u3001\u63a8\u7bb1\u5b50\u548c\u64e6\u62ed\u767d\u677f\u7b49\u73b0\u5b9e\u4e16\u754c\u4ea4\u4e92\u4efb\u52a1\u3002", "conclusion": "\u8be5MPC\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u7269\u7406\u4e00\u81f4\u7684\u5168\u8eab\u884c\u4e3a\uff0c\u6709\u6548\u5904\u7406\u7cfb\u7edf\u52a8\u529b\u5b66\u548c\u7269\u7406\u7ea6\u675f\uff0c\u5728\u5b9e\u65f6\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.19859", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19859", "abs": "https://arxiv.org/abs/2511.19859", "authors": ["Xiangkai Ma", "Lekai Xing", "Han Zhang", "Wenzhong Li", "Sanglu Lu"], "title": "Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation", "comment": null, "summary": "Vision-Language-Action (VLA) models built upon Chain-of-Thought (CoT) have achieved remarkable success in advancing general-purpose robotic agents, owing to its significant perceptual comprehension. Recently, since text-only CoT struggles to adequately capture scene details in complex spatial environments, a highly promising strategy involves leveraging visual priors to guide robotic action generation. Nevertheless, these strategies face two inherent challenges: (i) a modality gap between visual observations and low-level actions, and (ii) unstable training due to competing objectives between visual prediction and action generation. To address these challenges, we propose a Vision-Integrated Trajectory Alignment (VITA) framework that learns a shared discrete latent space for vision and action, enabling joint modeling of perception and motor control. VITA introduces a implicit visual CoT: autoregressively generated tokens is simultaneously decoded into future frames predictions and robot actions, thereby internalizing visual dynamics as an inductive bias for motion planning. Extensive experiments on simulated and real-world environments demonstrate state-of-the-art performance. VITA improves 14.5\\%, 9.6\\% and 12.1\\% over existing baselines on CALVIN, LIBERO and SimplerEnv. Furthermore, VITA attains an average success rate of 80.5\\% across six real-world tasks, demonstrating its potential as a generalist robotic manipulation model.", "AI": {"tldr": "\u63d0\u51faVITA\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u79bb\u6563\u6f5c\u7a7a\u95f4\u8054\u5408\u5efa\u6a21\u89c6\u89c9\u548c\u52a8\u4f5c\uff0c\u89e3\u51b3\u89c6\u89c9\u89c2\u5bdf\u4e0e\u4f4e\u7ea7\u52a8\u4f5c\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u8ddd\u4ee5\u53ca\u89c6\u89c9\u9884\u6d4b\u4e0e\u52a8\u4f5c\u751f\u6210\u7684\u7ade\u4e89\u76ee\u6807\u95ee\u9898\u3002", "motivation": "\u6587\u672cCoT\u96be\u4ee5\u5728\u590d\u6742\u7a7a\u95f4\u73af\u5883\u4e2d\u5145\u5206\u6355\u6349\u573a\u666f\u7ec6\u8282\uff0c\u800c\u73b0\u6709\u5229\u7528\u89c6\u89c9\u5148\u9a8c\u7684\u65b9\u6cd5\u9762\u4e34\u89c6\u89c9\u89c2\u5bdf\u4e0e\u4f4e\u7ea7\u52a8\u4f5c\u7684\u6a21\u6001\u5dee\u8ddd\uff0c\u4ee5\u53ca\u89c6\u89c9\u9884\u6d4b\u4e0e\u52a8\u4f5c\u751f\u6210\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "method": "VITA\u6846\u67b6\u5b66\u4e60\u89c6\u89c9\u548c\u52a8\u4f5c\u7684\u5171\u4eab\u79bb\u6563\u6f5c\u7a7a\u95f4\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u751f\u6210\u7684token\u540c\u65f6\u89e3\u7801\u4e3a\u672a\u6765\u5e27\u9884\u6d4b\u548c\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u5c06\u89c6\u89c9\u52a8\u6001\u5185\u5316\u4e3a\u8fd0\u52a8\u89c4\u5212\u7684\u5f52\u7eb3\u504f\u7f6e\u3002", "result": "\u5728CALVIN\u3001LIBERO\u548cSimplerEnv\u4e0a\u5206\u522b\u6bd4\u73b0\u6709\u57fa\u7ebf\u63d0\u534714.5%\u30019.6%\u548c12.1%\uff0c\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u5e73\u5747\u6210\u529f\u7387\u8fbe\u523080.5%\u3002", "conclusion": "VITA\u5c55\u793a\u4e86\u4f5c\u4e3a\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u611f\u77e5\u548c\u8fd0\u52a8\u63a7\u5236\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2511.19869", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19869", "abs": "https://arxiv.org/abs/2511.19869", "authors": ["Eito Sato", "Takahiro Wada"], "title": "Human-Centered Cooperative Control Coupling Autonomous and Haptic Shared Control via Control Barrier Function", "comment": null, "summary": "Haptic shared control (HSC) is effective in teleoperation when full autonomy is limited by uncertainty or sensing constraints. However, autonomous control performance achieved by maximizing HSC strength is limited because the dynamics of the joystick and human arm affect the robot's behavior. We propose a cooperative framework coupling a joystick-independent autonomous controller with HSC. A control barrier function ignores joystick inputs within a safe region determined by the human operator in real-time, while HSC is engaged otherwise. A pilot experiment on simulated tasks with tele-operated underwater robot in virtual environment demonstrated improved accuracy and reduced required time over conventional HSC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u72ec\u7acb\u64cd\u7eb5\u6746\u81ea\u4e3b\u63a7\u5236\u5668\u4e0e\u89e6\u89c9\u5171\u4eab\u63a7\u5236\u7684\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u5728\u5b89\u5168\u533a\u57df\u5185\u5ffd\u7565\u64cd\u7eb5\u6746\u8f93\u5165\uff0c\u5728\u865a\u62df\u73af\u5883\u5b9e\u9a8c\u4e2d\u76f8\u6bd4\u4f20\u7edf\u89e6\u89c9\u5171\u4eab\u63a7\u5236\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u5e76\u51cf\u5c11\u4e86\u6240\u9700\u65f6\u95f4\u3002", "motivation": "\u5f53\u5b8c\u5168\u81ea\u4e3b\u6027\u53d7\u5230\u4e0d\u786e\u5b9a\u6027\u6216\u611f\u77e5\u7ea6\u675f\u9650\u5236\u65f6\uff0c\u89e6\u89c9\u5171\u4eab\u63a7\u5236(HSC)\u5728\u9065\u64cd\u4f5c\u4e2d\u5f88\u6709\u6548\u3002\u4f46\u7531\u4e8e\u64cd\u7eb5\u6746\u548c\u4eba\u7c7b\u624b\u81c2\u7684\u52a8\u529b\u5b66\u4f1a\u5f71\u54cd\u673a\u5668\u4eba\u884c\u4e3a\uff0c\u4ec5\u901a\u8fc7\u6700\u5927\u5316HSC\u5f3a\u5ea6\u83b7\u5f97\u7684\u81ea\u4e3b\u63a7\u5236\u6027\u80fd\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u534f\u4f5c\u6846\u67b6\uff0c\u5c06\u72ec\u7acb\u4e8e\u64cd\u7eb5\u6746\u7684\u81ea\u4e3b\u63a7\u5236\u5668\u4e0eHSC\u8026\u5408\u3002\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u5728\u7531\u4eba\u7c7b\u64cd\u4f5c\u5458\u5b9e\u65f6\u786e\u5b9a\u7684\u5b89\u5168\u533a\u57df\u5185\u5ffd\u7565\u64cd\u7eb5\u6746\u8f93\u5165\uff0c\u800c\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\u542f\u7528HSC\u3002", "result": "\u5728\u865a\u62df\u73af\u5883\u4e2d\u5bf9\u9065\u64cd\u4f5c\u6c34\u4e0b\u673a\u5668\u4eba\u8fdb\u884c\u7684\u6a21\u62df\u4efb\u52a1\u8bd5\u70b9\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u4f20\u7edfHSC\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u5e76\u51cf\u5c11\u4e86\u6240\u9700\u65f6\u95f4\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u534f\u4f5c\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u81ea\u4e3b\u63a7\u5236\u5668\u548c\u89e6\u89c9\u5171\u4eab\u63a7\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfHSC\u7684\u6027\u80fd\u9650\u5236\u95ee\u9898\uff0c\u5728\u9065\u64cd\u4f5c\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2511.19914", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19914", "abs": "https://arxiv.org/abs/2511.19914", "authors": ["Dapeng Zhang", "Fei Shen", "Rui Zhao", "Yinda Chen", "Peng Zhi", "Chenyang Li", "Rui Zhou", "Qingguo Zhou"], "title": "CoC-VLA: Delving into Adversarial Domain Transfer for Explainable Autonomous Driving via Chain-of-Causality Visual-Language-Action Model", "comment": null, "summary": "Autonomous driving represents a prominent application of artificial intelligence. Recent approaches have shifted from focusing solely on common scenarios to addressing complex, long-tail situations such as subtle human behaviors, traffic accidents, and non-compliant driving patterns. Given the demonstrated capabilities of large language models (LLMs) in understanding visual and natural language inputs and following instructions, recent methods have integrated LLMs into autonomous driving systems to enhance reasoning, interpretability, and performance across diverse scenarios. However, existing methods typically rely either on real-world data, which is suitable for industrial deployment, or on simulation data tailored to rare or hard case scenarios. Few approaches effectively integrate the complementary advantages of both data sources. To address this limitation, we propose a novel VLM-guided, end-to-end adversarial transfer framework for autonomous driving that transfers long-tail handling capabilities from simulation to real-world deployment, named CoC-VLA. The framework comprises a teacher VLM model, a student VLM model, and a discriminator. Both the teacher and student VLM models utilize a shared base architecture, termed the Chain-of-Causality Visual-Language Model (CoC VLM), which integrates temporal information via an end-to-end text adapter. This architecture supports chain-of-thought reasoning to infer complex driving logic. The teacher and student VLM models are pre-trained separately on simulated and real-world datasets. The discriminator is trained adversarially to facilitate the transfer of long-tail handling capabilities from simulated to real-world environments by the student VLM model, using a novel backpropagation strategy.", "AI": {"tldr": "\u63d0\u51fa\u4e86CoC-VLA\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u8fc1\u79fb\u5b66\u4e60\u5c06\u4eff\u771f\u73af\u5883\u4e2d\u7684\u957f\u5c3e\u573a\u666f\u5904\u7406\u80fd\u529b\u8fc1\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\uff0c\u7ed3\u5408\u6559\u5e08-\u5b66\u751fVLM\u6a21\u578b\u548c\u5224\u522b\u5668\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u77e5\u8bc6\u8f6c\u79fb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff08\u9002\u5408\u5de5\u4e1a\u90e8\u7f72\uff09\uff0c\u8981\u4e48\u4f7f\u7528\u9488\u5bf9\u7f55\u89c1\u573a\u666f\u7684\u4eff\u771f\u6570\u636e\uff0c\u4f46\u5f88\u5c11\u80fd\u6709\u6548\u6574\u5408\u4e24\u8005\u7684\u4e92\u8865\u4f18\u52bf\u3002\u9700\u8981\u89e3\u51b3\u4eff\u771f\u5230\u771f\u5b9e\u4e16\u754c\u7684\u957f\u5c3e\u573a\u666f\u5904\u7406\u80fd\u529b\u8fc1\u79fb\u95ee\u9898\u3002", "method": "\u63d0\u51faCoC-VLA\u6846\u67b6\uff0c\u5305\u542b\u6559\u5e08VLM\u3001\u5b66\u751fVLM\u548c\u5224\u522b\u5668\u3002\u91c7\u7528\u5171\u4eab\u7684\u56e0\u679c\u94fe\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(CoC VLM)\u67b6\u6784\uff0c\u96c6\u6210\u65f6\u95f4\u4fe1\u606f\u5e76\u901a\u8fc7\u7aef\u5230\u7aef\u6587\u672c\u9002\u914d\u5668\u652f\u6301\u94fe\u5f0f\u63a8\u7406\u3002\u6559\u5e08\u548c\u5b66\u751fVLM\u5206\u522b\u5728\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\uff0c\u5224\u522b\u5668\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u548c\u65b0\u578b\u53cd\u5411\u4f20\u64ad\u7b56\u7565\u4fc3\u8fdb\u80fd\u529b\u8fc1\u79fb\u3002", "result": "\u6846\u67b6\u80fd\u591f\u6709\u6548\u5c06\u4eff\u771f\u73af\u5883\u4e2d\u7684\u957f\u5c3e\u573a\u666f\u5904\u7406\u80fd\u529b\u8fc1\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u60c5\u51b5\u4e0b\u7684\u63a8\u7406\u548c\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "CoC-VLA\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4eff\u771f\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6e90\u7684\u6709\u6548\u6574\u5408\u95ee\u9898\uff0c\u901a\u8fc7\u5bf9\u6297\u8fc1\u79fb\u5b66\u4e60\u5b9e\u73b0\u4e86\u957f\u5c3e\u573a\u666f\u5904\u7406\u80fd\u529b\u7684\u8de8\u57df\u8f6c\u79fb\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u573a\u666f\u8986\u76d6\u80fd\u529b\u3002"}}
{"id": "2511.19932", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19932", "abs": "https://arxiv.org/abs/2511.19932", "authors": ["Lidi Zhang", "Han Wu", "Liyu Zhang", "Ruofeng Liu", "Haotian Wang", "Chao Li", "Desheng Zhang", "Yunhuai Liu", "Tian He"], "title": "Collaborate sim and real: Robot Bin Packing Learning in Real-world and Physical Engine", "comment": null, "summary": "The 3D bin packing problem, with its diverse industrial applications, has garnered significant research attention in recent years. Existing approaches typically model it as a discrete and static process, while real-world applications involve continuous gravity-driven interactions. This idealized simplification leads to infeasible deployments (e.g., unstable packing) in practice. Simulations with physical engine offer an opportunity to emulate continuous gravity effects, enabling the training of reinforcement learning (RL) agents to address such limitations and improve packing stability. However, a simulation-to-reality gap persists due to dynamic variations in physical properties of real-world objects, such as various friction coefficients, elasticity, and non-uniform weight distributions. To bridge this gap, we propose a hybrid RL framework that collaborates with physical simulation with real-world data feedback. Firstly, domain randomization is applied during simulation to expose agents to a spectrum of physical parameters, enhancing their generalization capability. Secondly, the RL agent is fine-tuned with real-world deployment feedback, further reducing collapse rates. Extensive experiments demonstrate that our method achieves lower collapse rates in both simulated and real-world scenarios. Large-scale deployments in logistics systems validate the practical effectiveness, with a 35\\% reduction in packing collapse compared to baseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u53cd\u9988\u7684\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b33D\u88c5\u7bb1\u95ee\u9898\u4e2d\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u5dee\u8ddd\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5305\u88c5\u574d\u584c\u7387\u3002", "motivation": "\u73b0\u6709\u76843D\u88c5\u7bb1\u65b9\u6cd5\u901a\u5e38\u5c06\u5176\u5efa\u6a21\u4e3a\u79bb\u6563\u548c\u9759\u6001\u8fc7\u7a0b\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u6d89\u53ca\u8fde\u7eed\u7684\u91cd\u529b\u9a71\u52a8\u4ea4\u4e92\uff0c\u8fd9\u79cd\u7406\u60f3\u5316\u7b80\u5316\u5bfc\u81f4\u5b9e\u9645\u90e8\u7f72\u4e0d\u53ef\u884c\uff08\u5982\u4e0d\u7a33\u5b9a\u5305\u88c5\uff09\u3002\u7269\u7406\u4eff\u771f\u4e0e\u771f\u5b9e\u4e16\u754c\u7269\u7406\u5c5e\u6027\uff08\u5982\u6469\u64e6\u7cfb\u6570\u3001\u5f39\u6027\u3001\u91cd\u91cf\u5206\u5e03\uff09\u7684\u52a8\u6001\u53d8\u5316\u4e4b\u95f4\u5b58\u5728\u4eff\u771f\u5230\u73b0\u5b9e\u5dee\u8ddd\u3002", "method": "1. \u5728\u4eff\u771f\u4e2d\u5e94\u7528\u9886\u57df\u968f\u673a\u5316\uff0c\u8ba9\u667a\u80fd\u4f53\u63a5\u89e6\u5404\u79cd\u7269\u7406\u53c2\u6570\u4ee5\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff1b2. \u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u53cd\u9988\u5bf9RL\u667a\u80fd\u4f53\u8fdb\u884c\u5fae\u8c03\uff0c\u8fdb\u4e00\u6b65\u964d\u4f4e\u574d\u584c\u7387\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4eff\u771f\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u90fd\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u574d\u584c\u7387\u3002\u5728\u7269\u6d41\u7cfb\u7edf\u4e2d\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u9a8c\u8bc1\u4e86\u5b9e\u9645\u6709\u6548\u6027\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\u5305\u88c5\u574d\u584c\u51cf\u5c11\u4e8635%\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408RL\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u53cd\u9988\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u88c5\u7bb1\u95ee\u9898\u4e2d\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u5dee\u8ddd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5305\u88c5\u7a33\u5b9a\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.19955", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19955", "abs": "https://arxiv.org/abs/2511.19955", "authors": ["Jinxuan Zhu", "Zihao Yan", "Yangyu Xiao", "Jingxiang Guo", "Chenrui Tie", "Xinyi Cao", "Yuhang Zheng", "Lin Shao"], "title": "ShapeForce: Low-Cost Soft Robotic Wrist for Contact-Rich Manipulation", "comment": null, "summary": "Contact feedback is essential for contact-rich robotic manipulation, as it allows the robot to detect subtle interaction changes and adjust its actions accordingly. Six-axis force-torque sensors are commonly used to obtain contact feedback, but their high cost and fragility have discouraged many researchers from adopting them in contact-rich tasks. To offer a more cost-efficient and easy-accessible source of contact feedback, we present ShapeForce, a low-cost, plug-and-play soft wrist that provides force-like signals for contact-rich robotic manipulation. Inspired by how humans rely on relative force changes in contact rather than precise force magnitudes, ShapeForce converts external force and torque into measurable deformations of its compliant core, which are then estimated via marker-based pose tracking and converted into force-like signals. Our design eliminates the need for calibration or specialized electronics to obtain exact values, and instead focuses on capturing force and torque changes sufficient for enabling contact-rich manipulation. Extensive experiments across diverse contact-rich tasks and manipulation policies demonstrate that ShapeForce delivers performance comparable to six-axis force-torque sensors at an extremely low cost.", "AI": {"tldr": "ShapeForce\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u5373\u63d2\u5373\u7528\u7684\u8f6f\u624b\u8155\uff0c\u901a\u8fc7\u5c06\u5916\u529b\u626d\u77e9\u8f6c\u6362\u4e3a\u53ef\u6d4b\u91cf\u7684\u5f62\u53d8\u6765\u63d0\u4f9b\u7c7b\u4f3c\u529b\u7684\u4fe1\u53f7\uff0c\u7528\u4e8e\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u516d\u7ef4\u529b\u626d\u77e9\u4f20\u611f\u5668\u6210\u672c\u9ad8\u4e14\u6613\u788e\uff0c\u9650\u5236\u4e86\u5728\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002\u9700\u8981\u63d0\u4f9b\u66f4\u7ecf\u6d4e\u6613\u7528\u7684\u63a5\u89e6\u53cd\u9988\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u67d4\u6027\u6838\u5fc3\u5c06\u5916\u529b\u626d\u77e9\u8f6c\u6362\u4e3a\u5f62\u53d8\uff0c\u4f7f\u7528\u57fa\u4e8e\u6807\u8bb0\u7684\u59ff\u6001\u8ddf\u8e2a\u4f30\u8ba1\u5f62\u53d8\uff0c\u7136\u540e\u8f6c\u6362\u4e3a\u7c7b\u4f3c\u529b\u7684\u4fe1\u53f7\u3002\u65e0\u9700\u6821\u51c6\u6216\u4e13\u7528\u7535\u5b50\u8bbe\u5907\u3002", "result": "\u5728\u591a\u79cd\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u548c\u64cd\u4f5c\u7b56\u7565\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cShapeForce\u4ee5\u6781\u4f4e\u6210\u672c\u5b9e\u73b0\u4e86\u4e0e\u516d\u7ef4\u529b\u626d\u77e9\u4f20\u611f\u5668\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "ShapeForce\u4e3a\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u63a5\u89e6\u53cd\u9988\u89e3\u51b3\u65b9\u6848\uff0c\u6027\u80fd\u53ef\u4e0e\u6602\u8d35\u4f20\u611f\u5668\u76f8\u5ab2\u7f8e\u3002"}}
{"id": "2511.20050", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20050", "abs": "https://arxiv.org/abs/2511.20050", "authors": ["Yan Li", "Yingzhao Li", "Gim Hee Lee"], "title": "Active3D: Active High-Fidelity 3D Reconstruction via Hierarchical Uncertainty Quantification", "comment": null, "summary": "In this paper, we present an active exploration framework for high-fidelity 3D reconstruction that incrementally builds a multi-level uncertainty space and selects next-best-views through an uncertainty-driven motion planner. We introduce a hybrid implicit-explicit representation that fuses neural fields with Gaussian primitives to jointly capture global structural priors and locally observed details. Based on this hybrid state, we derive a hierarchical uncertainty volume that quantifies both implicit global structure quality and explicit local surface confidence. To focus optimization on the most informative regions, we propose an uncertainty-driven keyframe selection strategy that anchors high-entropy viewpoints as sparse attention nodes, coupled with a viewpoint-space sliding window for uncertainty-aware local refinement. The planning module formulates next-best-view selection as an Expected Hybrid Information Gain problem and incorporates a risk-sensitive path planner to ensure efficient and safe exploration. Extensive experiments on challenging benchmarks demonstrate that our approach consistently achieves state-of-the-art accuracy, completeness, and rendering quality, highlighting its effectiveness for real-world active reconstruction and robotic perception tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9ad8\u4fdd\u771f3D\u91cd\u5efa\u7684\u4e3b\u52a8\u63a2\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u7ea7\u4e0d\u786e\u5b9a\u6027\u7a7a\u95f4\u548c\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u8fd0\u52a8\u89c4\u5212\u6765\u9009\u62e9\u6700\u4f73\u89c6\u89d2\u3002", "motivation": "\u89e3\u51b3\u73b0\u67093D\u91cd\u5efa\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u96be\u4ee5\u5e73\u8861\u5168\u5c40\u7ed3\u6784\u5148\u9a8c\u548c\u5c40\u90e8\u7ec6\u8282\u6355\u6349\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u6709\u6548\u7684\u4e3b\u52a8\u63a2\u7d22\u7b56\u7565\u6765\u4f18\u5316\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u6df7\u5408\u9690\u5f0f-\u663e\u5f0f\u8868\u793a\u878d\u5408\u795e\u7ecf\u573a\u548c\u9ad8\u65af\u57fa\u5143\uff1b\u6784\u5efa\u5206\u5c42\u4e0d\u786e\u5b9a\u6027\u4f53\u79ef\u91cf\u5316\u5168\u5c40\u7ed3\u6784\u8d28\u91cf\u548c\u5c40\u90e8\u8868\u9762\u7f6e\u4fe1\u5ea6\uff1b\u63d0\u51fa\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u5173\u952e\u5e27\u9009\u62e9\u7b56\u7565\u548c\u89c6\u89d2\u7a7a\u95f4\u6ed1\u52a8\u7a97\u53e3\uff1b\u5c06\u6700\u4f73\u89c6\u89d2\u9009\u62e9\u5efa\u6a21\u4e3a\u671f\u671b\u6df7\u5408\u4fe1\u606f\u589e\u76ca\u95ee\u9898\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u3001\u5b8c\u6574\u6027\u548c\u6e32\u67d3\u8d28\u91cf\u65b9\u9762\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u4e3b\u52a8\u91cd\u5efa\u548c\u673a\u5668\u4eba\u611f\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2511.20180", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20180", "abs": "https://arxiv.org/abs/2511.20180", "authors": ["Ryohei Kobayashi", "Kosei Isomoto", "Kosei Yamao", "Soma Fumoto", "Koshun Arimura", "Naoki Yamaguchi", "Akinobu Mizutani", "Tomoya Shiba", "Kouki Kimizuka", "Yuta Ohno", "Ryo Terashima", "Hiromasa Yamaguchi", "Tomoaki Fujino", "Ryoga Maruno", "Wataru Yoshimura", "Kazuhito Mine", "Tang Phu Thien Nhan", "Yuga Yano", "Yuichiro Tanaka", "Takeshi Nishida", "Takashi Morie", "Hakaru Tamukoh"], "title": "Hibikino-Musashi@Home 2025 Team Description Paper", "comment": null, "summary": "This paper provides an overview of the techniques employed by Hibikino-Musashi@Home, which intends to participate in the domestic standard platform league. The team developed a dataset generator for training a robot vision system and an open-source development environment running on a Human Support Robot simulator. The large-language-model-powered task planner selects appropriate primitive skills to perform the task requested by the user. Moreover, the team has focused on research involving brain-inspired memory models for adaptation to individual home environments. This approach aims to provide intuitive and personalized assistance. Additionally, the team contributed to the reusability of the navigation system developed by Pumas in RoboCup2024. The team aimed to design a home service robot to assist humans in their homes and continuously attend competitions to evaluate and improve the developed system.", "AI": {"tldr": "Hibikino-Musashi@Home\u56e2\u961f\u5f00\u53d1\u4e86\u7528\u4e8e\u5bb6\u5ead\u670d\u52a1\u673a\u5668\u4eba\u7684\u6570\u636e\u96c6\u751f\u6210\u5668\u3001\u5f00\u6e90\u5f00\u53d1\u73af\u5883\u3001LLM\u4efb\u52a1\u89c4\u5212\u5668\u548c\u8111\u542f\u53d1\u8bb0\u5fc6\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u4f9b\u76f4\u89c2\u4e2a\u6027\u5316\u5bb6\u5ead\u8f85\u52a9\u670d\u52a1\u3002", "motivation": "\u8bbe\u8ba1\u80fd\u591f\u534f\u52a9\u4eba\u7c7b\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u5b8c\u6210\u4efb\u52a1\u7684\u673a\u5668\u4eba\uff0c\u5e76\u901a\u8fc7\u6301\u7eed\u53c2\u4e0e\u7ade\u8d5b\u6765\u8bc4\u4f30\u548c\u6539\u8fdb\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u6570\u636e\u96c6\u751f\u6210\u5668\u8bad\u7ec3\u673a\u5668\u4eba\u89c6\u89c9\u7cfb\u7edf\uff0c\u521b\u5efa\u5f00\u6e90\u5f00\u53d1\u73af\u5883\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4efb\u52a1\u89c4\u5212\uff0c\u7814\u7a76\u8111\u542f\u53d1\u8bb0\u5fc6\u6a21\u578b\u4ee5\u9002\u5e94\u4e2a\u4f53\u5bb6\u5ead\u73af\u5883\u3002", "result": "\u56e2\u961f\u5f00\u53d1\u4e86\u5b8c\u6574\u7684\u6280\u672f\u6808\uff0c\u5305\u62ec\u89c6\u89c9\u7cfb\u7edf\u8bad\u7ec3\u5de5\u5177\u3001\u6a21\u62df\u5668\u73af\u5883\u3001\u667a\u80fd\u4efb\u52a1\u89c4\u5212\u5668\u548c\u4e2a\u6027\u5316\u9002\u5e94\u673a\u5236\uff0c\u5e76\u8d21\u732e\u4e86\u5bfc\u822a\u7cfb\u7edf\u7684\u53ef\u91cd\u7528\u6027\u3002", "conclusion": "\u8be5\u56e2\u961f\u901a\u8fc7\u7efc\u5408\u6280\u672f\u65b9\u6cd5\u6784\u5efa\u4e86\u5bb6\u5ead\u670d\u52a1\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u7ade\u8d5b\u9a8c\u8bc1\u7cfb\u7edf\u6027\u80fd\uff0c\u4e3a\u4e2a\u6027\u5316\u5bb6\u5ead\u8f85\u52a9\u670d\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.20226", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20226", "abs": "https://arxiv.org/abs/2511.20226", "authors": ["Yu Sun", "Yaosheng Deng", "Wenjie Mei", "Xiaogang Xiong", "Yang Bai", "Masaki Ogura", "Zeyu Zhou", "Mir Feroskhan", "Michael Yu Wang", "Qiyang Zuo", "Yao Li", "Yunjiang Lou"], "title": "Toward generic control for soft robotic systems", "comment": null, "summary": "Soft robotics has advanced rapidly, yet its control methods remain fragmented: different morphologies and actuation schemes still require task-specific controllers, hindering theoretical integration and large-scale deployment. A generic control framework is therefore essential, and a key obstacle lies in the persistent use of rigid-body control logic, which relies on precise models and strict low-level execution. Such a paradigm is effective for rigid robots but fails for soft robots, where the ability to tolerate and exploit approximate action representations, i.e., control compliance, is the basis of robustness and adaptability rather than a disturbance to be eliminated. Control should thus shift from suppressing compliance to explicitly exploiting it. Human motor control exemplifies this principle: instead of computing exact dynamics or issuing detailed muscle-level commands, it expresses intention through high-level movement tendencies, while reflexes and biomechanical mechanisms autonomously resolve local details. This architecture enables robustness, flexibility, and cross-task generalization. Motivated by this insight, we propose a generic soft-robot control framework grounded in control compliance and validate it across robots with diverse morphologies and actuation mechanisms. The results demonstrate stable, safe, and cross-platform transferable behavior, indicating that embracing control compliance, rather than resisting it, may provide a widely applicable foundation for unified soft-robot control.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a7\u5236\u67d4\u987a\u6027\u7684\u901a\u7528\u8f6f\u4f53\u673a\u5668\u4eba\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u800c\u975e\u6291\u5236\u8fd1\u4f3c\u52a8\u4f5c\u8868\u793a\u6765\u5b9e\u73b0\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u9a8c\u8bc1\u4e86\u8de8\u4e0d\u540c\u5f62\u6001\u548c\u9a71\u52a8\u673a\u5236\u7684\u7a33\u5b9a\u3001\u5b89\u5168\u3001\u53ef\u8fc1\u79fb\u63a7\u5236\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u63a7\u5236\u65b9\u6cd5\u4ecd\u5904\u4e8e\u788e\u7247\u5316\u72b6\u6001\uff0c\u4e0d\u540c\u5f62\u6001\u548c\u9a71\u52a8\u65b9\u6848\u9700\u8981\u7279\u5b9a\u63a7\u5236\u5668\uff0c\u963b\u788d\u4e86\u7406\u8bba\u6574\u5408\u548c\u5927\u89c4\u6a21\u90e8\u7f72\u3002\u521a\u6027\u673a\u5668\u4eba\u63a7\u5236\u903b\u8f91\u4f9d\u8d56\u7cbe\u786e\u6a21\u578b\u548c\u4e25\u683c\u4f4e\u7ea7\u6267\u884c\uff0c\u4e0d\u9002\u7528\u4e8e\u8f6f\u4f53\u673a\u5668\u4eba\uff0c\u800c\u63a7\u5236\u67d4\u987a\u6027\uff08\u5bb9\u5fcd\u548c\u5229\u7528\u8fd1\u4f3c\u52a8\u4f5c\u8868\u793a\u7684\u80fd\u529b\uff09\u662f\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u7684\u57fa\u7840\u3002", "method": "\u53d7\u4eba\u7c7b\u8fd0\u52a8\u63a7\u5236\u542f\u53d1\uff0c\u63d0\u51fa\u57fa\u4e8e\u63a7\u5236\u67d4\u987a\u6027\u7684\u901a\u7528\u8f6f\u4f53\u673a\u5668\u4eba\u63a7\u5236\u6846\u67b6\uff1a\u901a\u8fc7\u9ad8\u5c42\u8fd0\u52a8\u8d8b\u52bf\u8868\u8fbe\u610f\u56fe\uff0c\u800c\u53cd\u5c04\u548c\u751f\u7269\u529b\u5b66\u673a\u5236\u81ea\u4e3b\u89e3\u51b3\u5c40\u90e8\u7ec6\u8282\uff0c\u4e0d\u8ba1\u7b97\u7cbe\u786e\u52a8\u529b\u5b66\u6216\u53d1\u51fa\u8be6\u7ec6\u808c\u8089\u7ea7\u547d\u4ee4\u3002", "result": "\u5728\u5177\u6709\u4e0d\u540c\u5f62\u6001\u548c\u9a71\u52a8\u673a\u5236\u7684\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u7a33\u5b9a\u3001\u5b89\u5168\u4e14\u8de8\u5e73\u53f0\u53ef\u8fc1\u79fb\u7684\u884c\u4e3a\u8868\u73b0\u3002", "conclusion": "\u62e5\u62b1\u800c\u975e\u62b5\u6297\u63a7\u5236\u67d4\u987a\u6027\u53ef\u80fd\u4e3a\u7edf\u4e00\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u5e7f\u6cdb\u5e94\u7528\u57fa\u7840\uff0c\u5b9e\u73b0\u9c81\u68d2\u6027\u3001\u7075\u6d3b\u6027\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.20275", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20275", "abs": "https://arxiv.org/abs/2511.20275", "authors": ["Chenhui Dong", "Haozhe Xu", "Wenhao Feng", "Zhipeng Wang", "Yanmin Zhou", "Yifei Zhao", "Bin He"], "title": "HAFO: Humanoid Force-Adaptive Control for Intense External Force Interaction Environments", "comment": null, "summary": "Reinforcement learning controllers have made impressive progress in humanoid locomotion and light load manipulation. However, achieving robust and precise motion with strong force interaction remains a significant challenge. Based on the above limitations, this paper proposes HAFO, a dual-agent reinforcement learning control framework that simultaneously optimizes both a robust locomotion strategy and a precise upper-body manipulation strategy through coupled training under external force interaction environments. Simultaneously, we explicitly model the external pulling disturbances through a spring-damper system and achieve fine-grained force control by manipulating the virtual spring. During this process, the reinforcement-learning policy spontaneously generates disturbance-rejection response by exploiting environmental feedback. Moreover, HAFO employs an asymmetric Actor-Critic framework in which the Critic-network access to privileged spring-damping forces guides the actor-network to learn a generalizable, robust policy for resisting external disturbances. The experimental results demonstrate that HAFO achieves stable control of humanoid robot under various strong force interactions, showing remarkable performance in load tasks and ensuring stable robot operation under rope tension disturbances. Project website: hafo-robot.github.io.", "AI": {"tldr": "HAFO\u662f\u4e00\u4e2a\u53cc\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u8026\u5408\u8bad\u7ec3\u540c\u65f6\u4f18\u5316\u7a33\u5065\u7684\u6b65\u6001\u7b56\u7565\u548c\u7cbe\u786e\u7684\u4e0a\u534a\u8eab\u64cd\u4f5c\u7b56\u7565\uff0c\u5728\u5f3a\u5916\u529b\u4ea4\u4e92\u73af\u5883\u4e0b\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u7684\u7a33\u5b9a\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\u5728\u4eba\u5f62\u673a\u5668\u4eba\u6b65\u6001\u548c\u8f7b\u8d1f\u8f7d\u64cd\u4f5c\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u5f3a\u5916\u529b\u4ea4\u4e92\u4e0b\u5b9e\u73b0\u7a33\u5065\u7cbe\u786e\u8fd0\u52a8\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u53cc\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5f39\u7c27-\u963b\u5c3c\u7cfb\u7edf\u663e\u5f0f\u5efa\u6a21\u5916\u90e8\u62c9\u529b\u5e72\u6270\uff0c\u5229\u7528\u975e\u5bf9\u79f0Actor-Critic\u6846\u67b6\uff0cCritic\u7f51\u7edc\u8bbf\u95ee\u7279\u6743\u5f39\u7c27\u963b\u5c3c\u529b\u4fe1\u606f\u6307\u5bfcActor\u7f51\u7edc\u5b66\u4e60\u901a\u7528\u7a33\u5065\u7b56\u7565\u3002", "result": "HAFO\u5728\u5404\u79cd\u5f3a\u5916\u529b\u4ea4\u4e92\u4e0b\u5b9e\u73b0\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7684\u7a33\u5b9a\u63a7\u5236\uff0c\u5728\u8d1f\u8f7d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u7ef3\u7d22\u62c9\u529b\u5e72\u6270\u4e0b\u786e\u4fdd\u673a\u5668\u4eba\u7a33\u5b9a\u8fd0\u884c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u5f3a\u5916\u529b\u4ea4\u4e92\u73af\u5883\u4e0b\u7684\u4eba\u5f62\u673a\u5668\u4eba\u7a33\u5065\u63a7\u5236\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u4f18\u5f02\u6027\u80fd\u3002"}}
{"id": "2511.20292", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20292", "abs": "https://arxiv.org/abs/2511.20292", "authors": ["Dong Wang", "Daniel Casado Herraez", "Stefan May", "Andreas N\u00fcchter"], "title": "Dynamic-ICP: Doppler-Aware Iterative Closest Point Registration for Dynamic Scenes", "comment": "8 pages, 5 figures", "summary": "Reliable odometry in highly dynamic environments remains challenging when it relies on ICP-based registration: ICP assumes near-static scenes and degrades in repetitive or low-texture geometry. We introduce Dynamic-ICP, a Doppler-aware registration framework. The method (i) estimates ego motion from per-point Doppler velocity via robust regression and builds a velocity filter, (ii) clusters dynamic objects and reconstructs object-wise translational velocities from ego-compensated radial measurements, (iii) predicts dynamic points with a constant-velocity model, and (iv) aligns scans using a compact objective that combines point-to-plane geometry residual with a translation-invariant, rotation-only Doppler residual. The approach requires no external sensors or sensor-vehicle calibration and operates directly on FMCW LiDAR range and Doppler velocities. We evaluate Dynamic-ICP on three datasets-HeRCULES, HeLiPR, AevaScenes-focusing on highly dynamic scenes. Dynamic-ICP consistently improves rotational stability and translation accuracy over the state-of-the-art methods. Our approach is also simple to integrate into existing pipelines, runs in real time, and provides a lightweight solution for robust registration in dynamic environments. To encourage further research, the code is available at: https://github.com/JMUWRobotics/Dynamic-ICP.", "AI": {"tldr": "Dynamic-ICP\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u666e\u52d2\u611f\u77e5\u7684\u6fc0\u5149\u96f7\u8fbe\u91cc\u7a0b\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4f7f\u7528\u591a\u666e\u52d2\u901f\u5ea6\u4fe1\u606f\u6765\u6539\u8fdb\u70b9\u4e91\u914d\u51c6\u7cbe\u5ea6\u3002", "motivation": "\u5728\u9ad8\u5ea6\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u7684ICP\u914d\u51c6\u65b9\u6cd5\u5047\u8bbe\u573a\u666f\u8fd1\u4f3c\u9759\u6001\uff0c\u5728\u91cd\u590d\u6216\u4f4e\u7eb9\u7406\u51e0\u4f55\u4e2d\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u52a8\u6001\u7269\u4f53\u7684\u53ef\u9760\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\u56db\u4e2a\u6b65\u9aa4\uff1a(i)\u901a\u8fc7\u7a33\u5065\u56de\u5f52\u4ece\u70b9\u7ea7\u591a\u666e\u52d2\u901f\u5ea6\u4f30\u8ba1\u81ea\u6211\u8fd0\u52a8\u5e76\u6784\u5efa\u901f\u5ea6\u6ee4\u6ce2\u5668\uff1b(ii)\u805a\u7c7b\u52a8\u6001\u7269\u4f53\u5e76\u4ece\u81ea\u6211\u8865\u507f\u7684\u5f84\u5411\u6d4b\u91cf\u91cd\u5efa\u7269\u4f53\u5e73\u79fb\u901f\u5ea6\uff1b(iii)\u4f7f\u7528\u6052\u5b9a\u901f\u5ea6\u6a21\u578b\u9884\u6d4b\u52a8\u6001\u70b9\uff1b(iv)\u4f7f\u7528\u7ed3\u5408\u70b9\u9762\u51e0\u4f55\u6b8b\u5dee\u548c\u65cb\u8f6c\u4e0d\u53d8\u591a\u666e\u52d2\u6b8b\u5dee\u7684\u7d27\u51d1\u76ee\u6807\u51fd\u6570\u5bf9\u9f50\u626b\u63cf\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\uff08HeRCULES\u3001HeLiPR\u3001AevaScenes\uff09\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cDynamic-ICP\u5728\u65cb\u8f6c\u7a33\u5b9a\u6027\u548c\u5e73\u79fb\u7cbe\u5ea6\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "Dynamic-ICP\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u4f20\u611f\u5668\u6216\u4f20\u611f\u5668-\u8f66\u8f86\u6807\u5b9a\u7684\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u9c81\u68d2\u914d\u51c6\uff0c\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u6d41\u7a0b\u4e2d\u3002"}}
{"id": "2511.20299", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.20299", "abs": "https://arxiv.org/abs/2511.20299", "authors": ["R\u00f3is\u00edn Keenan", "Joost C. Dessing"], "title": "How Robot Kinematics Influence Human Performance in Virtual Robot-to-Human Handover Tasks", "comment": null, "summary": "Recent advancements in robotics have increased the possibilities for integrating robotic systems into human-involved workplaces, highlighting the need to examine and optimize human-robot coordination in collaborative settings. This study explores human-robot interactions during handover tasks using Virtual Reality (VR) to investigate differences in human motor performance across various task dynamics and robot kinematics. A VR-based robot handover simulation afforded safe and controlled assessments of human-robot interactions. In separate experiments, four potential influences on human performance were examined (1) control over task initiation and robot movement synchrony (temporal and spatiotemporal); (2) partner appearance (human versus robotic); (3) robot velocity profiles (minimum jerk, constant velocity, constant acceleration, and biphasic); and (4) the timing of rotational object motion. Findings across experiments emphasize humans benefit from robots providing early and salient visual information about task-relevant object motion, and advantages of human-like smooth robot trajectories. To varying degrees, these manipulations improved predictive accuracy and synchronization during interaction. This suggests that human-robot interactions should be designed to allow humans to leverage their natural capabilities for detecting biological motion, which conversely may reduce the need for costly robotic computations or added cognitive adaptation on the human side.", "AI": {"tldr": "\u7814\u7a76\u4f7f\u7528VR\u63a2\u7d22\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u7269\u4f53\u4f20\u9012\u4efb\u52a1\uff0c\u53d1\u73b0\u4eba\u7c7b\u53d7\u76ca\u4e8e\u673a\u5668\u4eba\u63d0\u4f9b\u65e9\u671f\u89c6\u89c9\u4fe1\u606f\u548c\u5e73\u6ed1\u8f68\u8ff9\uff0c\u8fd9\u80fd\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u540c\u6b65\u6027\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u878d\u5165\u4eba\u7c7b\u5de5\u4f5c\u573a\u6240\uff0c\u9700\u8981\u4f18\u5316\u4eba\u673a\u534f\u4f5c\u534f\u8c03\u6027\uff0c\u7279\u522b\u662f\u5728\u7269\u4f53\u4f20\u9012\u4efb\u52a1\u4e2d\u3002", "method": "\u901a\u8fc7VR\u6a21\u62df\u673a\u5668\u4eba\u4f20\u9012\u4efb\u52a1\uff0c\u5206\u522b\u6d4b\u8bd5\u4e86\u56db\u4e2a\u5f71\u54cd\u56e0\u7d20\uff1a\u4efb\u52a1\u542f\u52a8\u63a7\u5236\u548c\u673a\u5668\u4eba\u8fd0\u52a8\u540c\u6b65\u6027\u3001\u4f19\u4f34\u5916\u89c2\u3001\u673a\u5668\u4eba\u901f\u5ea6\u66f2\u7ebf\u3001\u7269\u4f53\u65cb\u8f6c\u8fd0\u52a8\u65f6\u673a\u3002", "result": "\u4eba\u7c7b\u5728\u673a\u5668\u4eba\u63d0\u4f9b\u65e9\u671f\u89c6\u89c9\u4fe1\u606f\u548c\u5e73\u6ed1\u8f68\u8ff9\u65f6\u8868\u73b0\u66f4\u597d\uff0c\u8fd9\u4e9b\u56e0\u7d20\u4e0d\u540c\u7a0b\u5ea6\u5730\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u4ea4\u4e92\u540c\u6b65\u6027\u3002", "conclusion": "\u4eba\u673a\u4ea4\u4e92\u8bbe\u8ba1\u5e94\u8ba9\u4eba\u7c7b\u80fd\u591f\u5229\u7528\u5176\u68c0\u6d4b\u751f\u7269\u8fd0\u52a8\u7684\u81ea\u7136\u80fd\u529b\uff0c\u4ece\u800c\u51cf\u5c11\u673a\u5668\u4eba\u8ba1\u7b97\u6210\u672c\u6216\u4eba\u7c7b\u8ba4\u77e5\u9002\u5e94\u9700\u6c42\u3002"}}
{"id": "2511.20330", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.20330", "abs": "https://arxiv.org/abs/2511.20330", "authors": ["Yuhan Wu", "Tiantian Wei", "Shuo Wang", "ZhiChao Wang", "Yanyong Zhang", "Daniel Cremers", "Yan Xia"], "title": "ArtiBench and ArtiBrain: Benchmarking Generalizable Vision-Language Articulated Object Manipulation", "comment": null, "summary": "Interactive articulated manipulation requires long-horizon, multi-step interactions with appliances while maintaining physical consistency. Existing vision-language and diffusion-based policies struggle to generalize across parts, instances, and categories. We first introduce ArtiBench, a five-level benchmark covering kitchen, storage, office, and tool environments. ArtiBench enables structured evaluation from cross-part and cross-instance variation to long-horizon multi-object tasks, revealing the core generalization challenges of articulated object manipulation. Building on this benchmark, we propose ArtiBrain, a modular framework that unifies high-level reasoning with adaptive low-level control. ArtiBrain uses a VLM-based Task Reasoner (GPT-4.1) to decompose and validate subgoals, and employs a Hybrid Controller that combines geometry-aware keyframe execution with affordance-guided diffusion for precise and interpretable manipulation. An Affordance Memory Bank continually accumulates successful execution episodes and propagates part-level actionable affordances to unseen articulated parts and configurations. Extensive experiments on ArtiBench show that our ArtiBrain significantly outperforms state-of-the-art multimodal and diffusion-based methods in robustness and generalization. Code and dataset will be released upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86ArtiBench\u57fa\u51c6\u6d4b\u8bd5\u548cArtiBrain\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5173\u8282\u7269\u4f53\u64cd\u4f5c\u4e2d\u7684\u6cdb\u5316\u6311\u6218\uff0c\u7ed3\u5408\u9ad8\u5c42\u63a8\u7406\u548c\u81ea\u9002\u5e94\u5e95\u5c42\u63a7\u5236\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u548c\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u5728\u8de8\u90e8\u4ef6\u3001\u5b9e\u4f8b\u548c\u7c7b\u522b\u7684\u5173\u8282\u64cd\u4f5c\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u89e3\u51b3\u957f\u65f6\u7a0b\u3001\u591a\u6b65\u9aa4\u4ea4\u4e92\u7684\u7269\u7406\u4e00\u81f4\u6027\u6311\u6218\u3002", "method": "ArtiBrain\u91c7\u7528\u6a21\u5757\u5316\u6846\u67b6\uff1a\u4f7f\u7528VLM\u4efb\u52a1\u63a8\u7406\u5668\u5206\u89e3\u548c\u9a8c\u8bc1\u5b50\u76ee\u6807\uff0c\u6df7\u5408\u63a7\u5236\u5668\u7ed3\u5408\u51e0\u4f55\u611f\u77e5\u5173\u952e\u5e27\u6267\u884c\u548c\u53ef\u4f9b\u6027\u5f15\u5bfc\u6269\u6563\uff0c\u901a\u8fc7\u53ef\u4f9b\u6027\u8bb0\u5fc6\u5e93\u79ef\u7d2f\u548c\u4f20\u64ad\u6210\u529f\u7ecf\u9a8c\u3002", "result": "\u5728ArtiBench\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cArtiBrain\u5728\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u548c\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u3002", "conclusion": "ArtiBrain\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u9ad8\u5c42\u63a8\u7406\u548c\u81ea\u9002\u5e94\u63a7\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5173\u8282\u7269\u4f53\u64cd\u4f5c\u7684\u6cdb\u5316\u6311\u6218\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u5173\u8282\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20353", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20353", "abs": "https://arxiv.org/abs/2511.20353", "authors": ["Benjamin Sportich", "Kenza Boubakri", "Olivier Simonin", "Alessandro Renzaglia"], "title": "Quality-guided UAV Surface Exploration for 3D Reconstruction", "comment": null, "summary": "Reasons for mapping an unknown environment with autonomous robots are wide-ranging, but in practice, they are often overlooked when developing planning strategies. Rapid information gathering and comprehensive structural assessment of buildings have different requirements and therefore necessitate distinct methodologies. In this paper, we propose a novel modular Next-Best-View (NBV) planning framework for aerial robots that explicitly uses a reconstruction quality objective to guide the exploration planning. In particular, our approach introduces new and efficient methods for view generation and selection of viewpoint candidates that are adaptive to the user-defined quality requirements, fully exploiting the uncertainty encoded in a Truncated Signed Distance field (TSDF) representation of the environment. This results in informed and efficient exploration decisions tailored towards the predetermined objective. Finally, we validate our method via extensive simulations in realistic environments. We demonstrate that it successfully adjusts its behavior to the user goal while consistently outperforming conventional NBV strategies in terms of coverage, quality of the final 3D map and path efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7a7a\u4e2d\u673a\u5668\u4eba\u7684\u6a21\u5757\u5316Next-Best-View\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u5efa\u8d28\u91cf\u76ee\u6807\u6307\u5bfc\u63a2\u7d22\u89c4\u5212\uff0c\u5728\u8986\u76d6\u8303\u56f4\u30013D\u5730\u56fe\u8d28\u91cf\u548c\u8def\u5f84\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u4f20\u7edfNBV\u7b56\u7565\u3002", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5efa\u56fe\u9700\u6c42\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u89c4\u5212\u7b56\u7565\u5f80\u5f80\u5ffd\u89c6\u4e86\u4e0d\u540c\u5e94\u7528\u573a\u666f\uff08\u5982\u5feb\u901f\u4fe1\u606f\u6536\u96c6\u548c\u5efa\u7b51\u7ed3\u6784\u8bc4\u4f30\uff09\u5bf9\u65b9\u6cd5\u7684\u4e0d\u540c\u8981\u6c42\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u622a\u65ad\u7b26\u53f7\u8ddd\u79bb\u573a(TSDF)\u7684\u6a21\u5757\u5316NBV\u89c4\u5212\u6846\u67b6\uff0c\u5305\u542b\u81ea\u9002\u5e94\u4e8e\u7528\u6237\u5b9a\u4e49\u8d28\u91cf\u8981\u6c42\u7684\u89c6\u56fe\u751f\u6210\u548c\u89c6\u70b9\u5019\u9009\u9009\u62e9\u65b9\u6cd5\uff0c\u5145\u5206\u5229\u7528\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\u3002", "result": "\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u4eff\u771f\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6210\u529f\u6839\u636e\u7528\u6237\u76ee\u6807\u8c03\u6574\u884c\u4e3a\uff0c\u5728\u8986\u76d6\u8303\u56f4\u3001\u6700\u7ec83D\u5730\u56fe\u8d28\u91cf\u548c\u8def\u5f84\u6548\u7387\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u4f20\u7edfNBV\u7b56\u7565\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6839\u636e\u9884\u5b9a\u76ee\u6807\u8fdb\u884c\u4fe1\u606f\u5316\u548c\u9ad8\u6548\u7684\u63a2\u7d22\u51b3\u7b56\uff0c\u4e3a\u4e0d\u540c\u8d28\u91cf\u8981\u6c42\u7684\u5efa\u56fe\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20394", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20394", "abs": "https://arxiv.org/abs/2511.20394", "authors": ["Shiqian Liu", "Azlan Mohd Zain", "Le-le Mao"], "title": "Improved adaptive wind driven optimization algorithm for real-time path planning", "comment": "23 pages, 4 figures", "summary": "Recently, path planning has achieved remarkable progress in enhancing global search capability and convergence accuracy through heuristic and learning-inspired optimization frameworks. However, real-time adaptability in dynamic environments remains a critical challenge for autonomous navigation, particularly when robots must generate collision-free, smooth, and efficient trajectories under complex constraints. By analyzing the difficulties in dynamic path planning, the Wind Driven Optimization (WDO) algorithm emerges as a promising framework owing to its physically interpretable search dynamics. Motivated by these observations, this work revisits the WDO principle and proposes a variant formulation, Multi-hierarchical adaptive wind driven optimization(MAWDO), that improves adaptability and robustness in time-varying environments. To mitigate instability and premature convergence, a hierarchical-guidance mechanism divides the population into multiple groups guided by individual, regional, and global leaders to balance exploration and exploitation. Extensive evaluations on sixteen benchmark functions show that MAWDO achieves superior optimization accuracy, convergence stability, and adaptability over state-of-the art metaheuristics. In dynamic path planning, MAWDO shortens the path length to 469.28 pixels, improving over Multi-strategy ensemble wind driven optimization(MEWDO), Adaptive wind driven optimization(AWDO) and WDO by 3.51\\%, 11.63\\% and 14.93\\%, and achieves the smallest optimality gap (1.01) with smoothness 0.71 versus 13.50 and 15.67 for AWDO and WDO, leading to smoother, shorter, and collision-free trajectories that confirm its effectiveness for real-time path planning in complex environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u98ce\u9a71\u52a8\u4f18\u5316\u7b97\u6cd5MAWDO\uff0c\u901a\u8fc7\u5206\u5c42\u5f15\u5bfc\u673a\u5236\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u8def\u5f84\u89c4\u5212\u6027\u80fd\uff0c\u7f29\u77ed\u8def\u5f84\u957f\u5ea6\u5e76\u63d0\u9ad8\u5e73\u6ed1\u5ea6\u3002", "motivation": "\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u9002\u5e94\u6027\u662f\u81ea\u4e3b\u5bfc\u822a\u7684\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u751f\u6210\u65e0\u78b0\u649e\u3001\u5e73\u6ed1\u4e14\u9ad8\u6548\u7684\u8f68\u8ff9\u3002\u98ce\u9a71\u52a8\u4f18\u5316\u7b97\u6cd5\u56e0\u5176\u7269\u7406\u53ef\u89e3\u91ca\u7684\u641c\u7d22\u52a8\u6001\u800c\u5177\u6709\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u5c42\u7ea7\u81ea\u9002\u5e94\u98ce\u9a71\u52a8\u4f18\u5316(MAWDO)\uff0c\u91c7\u7528\u5206\u5c42\u5f15\u5bfc\u673a\u5236\u5c06\u79cd\u7fa4\u5212\u5206\u4e3a\u591a\u4e2a\u7ec4\uff0c\u7531\u4e2a\u4f53\u3001\u533a\u57df\u548c\u5168\u5c40\u9886\u5bfc\u8005\u5f15\u5bfc\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u572816\u4e2a\u57fa\u51c6\u51fd\u6570\u4e0a\u8bc4\u4f30\u663e\u793aMAWDO\u5177\u6709\u4f18\u8d8a\u7684\u4f18\u5316\u7cbe\u5ea6\u3001\u6536\u655b\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\u3002\u5728\u52a8\u6001\u8def\u5f84\u89c4\u5212\u4e2d\uff0c\u8def\u5f84\u957f\u5ea6\u7f29\u77ed\u81f3469.28\u50cf\u7d20\uff0c\u6bd4MEWDO\u3001AWDO\u548cWDO\u5206\u522b\u63d0\u53473.51%\u300111.63%\u548c14.93%\uff0c\u6700\u4f18\u6027\u5dee\u8ddd\u6700\u5c0f(1.01)\uff0c\u5e73\u6ed1\u5ea6\u4e3a0.71\u3002", "conclusion": "MAWDO\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u751f\u6210\u66f4\u5e73\u6ed1\u3001\u66f4\u77ed\u4e14\u65e0\u78b0\u649e\u7684\u8f68\u8ff9\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.20467", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.20467", "abs": "https://arxiv.org/abs/2511.20467", "authors": ["Liangkai Liu", "Weisong Shi", "Kang G. Shin"], "title": "Power-Efficient Autonomous Mobile Robots", "comment": "13 pages, 16 figures", "summary": "This paper presents pNav, a novel power-management system that significantly enhances the power/energy-efficiency of Autonomous Mobile Robots (AMRs) by jointly optimizing their physical/mechanical and cyber subsystems. By profiling AMRs' power consumption, we identify three challenges in achieving CPS (cyber-physical system) power-efficiency that involve both cyber (C) and physical (P) subsystems: (1) variabilities of system power consumption breakdown, (2) environment-aware navigation locality, and (3) coordination of C and P subsystems. pNav takes a multi-faceted approach to achieve power-efficiency of AMRs. First, it integrates millisecond-level power consumption prediction for both C and P subsystems. Second, it includes novel real-time modeling and monitoring of spatial and temporal navigation localities for AMRs. Third, it supports dynamic coordination of AMR software (navigation, detection) and hardware (motors, DVFS driver) configurations. pNav is prototyped using the Robot Operating System (ROS) Navigation Stack, 2D LiDAR, and camera. Our in-depth evaluation with a real robot and Gazebo environments demonstrates a >96% accuracy in predicting power consumption and a 38.1% reduction in power consumption without compromising navigation accuracy and safety.", "AI": {"tldr": "pNav\u662f\u4e00\u4e2a\u65b0\u578b\u7684\u529f\u7387\u7ba1\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7269\u7406/\u673a\u68b0\u548c\u7f51\u7edc\u5b50\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba(AMR)\u7684\u529f\u7387/\u80fd\u6548\u3002", "motivation": "\u901a\u8fc7\u5206\u6790AMR\u7684\u529f\u8017\u7279\u6027\uff0c\u53d1\u73b0\u5b9e\u73b0CPS(\u7f51\u7edc\u7269\u7406\u7cfb\u7edf)\u529f\u7387\u6548\u7387\u9762\u4e34\u4e09\u4e2a\u6311\u6218\uff1a\u7cfb\u7edf\u529f\u8017\u5206\u89e3\u7684\u53ef\u53d8\u6027\u3001\u73af\u5883\u611f\u77e5\u5bfc\u822a\u7684\u5c40\u90e8\u6027\u3001\u4ee5\u53ca\u7f51\u7edc\u548c\u7269\u7406\u5b50\u7cfb\u7edf\u7684\u534f\u8c03\u3002", "method": "\u91c7\u7528\u591a\u5c42\u9762\u65b9\u6cd5\uff1a\u96c6\u6210\u6beb\u79d2\u7ea7\u529f\u8017\u9884\u6d4b\u3001\u5b9e\u65f6\u5efa\u6a21\u548c\u76d1\u63a7\u5bfc\u822a\u65f6\u7a7a\u5c40\u90e8\u6027\u3001\u52a8\u6001\u534f\u8c03\u8f6f\u4ef6\u548c\u786c\u4ef6\u914d\u7f6e\u3002\u57fa\u4e8eROS\u5bfc\u822a\u6808\u30012D LiDAR\u548c\u6444\u50cf\u5934\u6784\u5efa\u539f\u578b\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u548cGazebo\u73af\u5883\u4e2d\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u529f\u8017\u9884\u6d4b\u51c6\u786e\u7387>96%\uff0c\u529f\u8017\u964d\u4f4e38.1%\uff0c\u4e14\u4e0d\u5f71\u54cd\u5bfc\u822a\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002", "conclusion": "pNav\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86AMR\u529f\u7387\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u529f\u8017\u964d\u4f4e\u548c\u51c6\u786e\u7684\u529f\u8017\u9884\u6d4b\u3002"}}
{"id": "2511.20492", "categories": ["cs.RO", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.20492", "abs": "https://arxiv.org/abs/2511.20492", "authors": ["Cyrill P\u00fcntener", "Johann Schwabe", "Dominique Garmier", "Jonas Frey", "Marco Hutter"], "title": "Kleinkram: Open Robotic Data Management", "comment": "for associated source code, see https://github.com/leggedrobotics/kleinkram", "summary": "We introduce Kleinkram, a free and open-source system designed to solve the challenge of managing massive, unstructured robotic datasets. Designed as a modular, on-premises cloud solution, Kleinkram enables scalable storage, indexing, and sharing of datasets, ranging from individual experiments to large-scale research collections. Kleinkram natively integrates with standard formats such as ROS bags and MCAP and utilises S3-compatible storage for flexibility. Beyond storage, Kleinkram features an integrated \"Action Runner\" that executes customizable Docker-based workflows for data validation, curation, and benchmarking. Kleinkram has successfully managed over 30 TB of data from diverse robotic systems, streamlining the research lifecycle through a modern web interface and a robust Command Line Interface (CLI).", "AI": {"tldr": "Kleinkram\u662f\u4e00\u4e2a\u5f00\u6e90\u7cfb\u7edf\uff0c\u7528\u4e8e\u7ba1\u7406\u5927\u89c4\u6a21\u975e\u7ed3\u6784\u5316\u673a\u5668\u4eba\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u5b58\u50a8\u3001\u7d22\u5f15\u548c\u5171\u4eab\u529f\u80fd\uff0c\u652f\u6301ROS bags\u548cMCAP\u7b49\u6807\u51c6\u683c\u5f0f\uff0c\u5e76\u5305\u542b\u57fa\u4e8eDocker\u7684\u5de5\u4f5c\u6d41\u6267\u884c\u5668\u3002", "motivation": "\u89e3\u51b3\u7ba1\u7406\u5927\u89c4\u6a21\u3001\u975e\u7ed3\u6784\u5316\u673a\u5668\u4eba\u6570\u636e\u96c6\u7684\u6311\u6218\uff0c\u652f\u6301\u4ece\u4e2a\u4f53\u5b9e\u9a8c\u5230\u5927\u89c4\u6a21\u7814\u7a76\u96c6\u5408\u7684\u6570\u636e\u7ba1\u7406\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u3001\u672c\u5730\u4e91\u89e3\u51b3\u65b9\u6848\uff0c\u96c6\u6210S3\u517c\u5bb9\u5b58\u50a8\uff0c\u539f\u751f\u652f\u6301ROS bags\u548cMCAP\u683c\u5f0f\uff0c\u5e76\u5305\u542b\u53ef\u5b9a\u5236\u7684Docker\u5de5\u4f5c\u6d41\u6267\u884c\u5668\uff08Action Runner\uff09\u3002", "result": "\u5df2\u6210\u529f\u7ba1\u7406\u8d85\u8fc730TB\u6765\u81ea\u4e0d\u540c\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6570\u636e\uff0c\u901a\u8fc7\u73b0\u4ee3Web\u754c\u9762\u548c\u5f3a\u5927CLI\u7b80\u5316\u7814\u7a76\u751f\u547d\u5468\u671f\u3002", "conclusion": "Kleinkram\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6ee1\u8db3\u673a\u5668\u4eba\u7814\u7a76\u4e2d\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7ba1\u7406\u7684\u9700\u6c42\uff0c\u5177\u6709\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.20496", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20496", "abs": "https://arxiv.org/abs/2511.20496", "authors": ["Jiaxin Liu", "Min Li", "Wanting Xu", "Liang Li", "Jiaqi Yang", "Laurent Kneip"], "title": "Metric, inertially aligned monocular state estimation via kinetodynamic priors", "comment": null, "summary": "Accurate state estimation for flexible robotic systems poses significant challenges, particular for platforms with dynamically deforming structures that invalidate rigid-body assumptions. This paper tackles this problem and allows to extend existing rigid-body pose estimation methods to non-rigid systems. Our approach hinges on two core assumptions: first, the elastic properties are captured by an injective deformation-force model, efficiently learned via a Multi-Layer Perceptron; second, we solve the platform's inherently smooth motion using continuous-time B-spline kinematic models. By continuously applying Newton's Second Law, our method establishes a physical link between visually-derived trajectory acceleration and predicted deformation-induced acceleration. We demonstrate that our approach not only enables robust and accurate pose estimation on non-rigid platforms, but that the properly modeled platform physics instigate inertial sensing properties. We demonstrate this feasibility on a simple spring-camera system, and show how it robustly resolves the typically ill-posed problem of metric scale and gravity recovery in monocular visual odometry.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u73b0\u6709\u521a\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u6269\u5c55\u5230\u975e\u521a\u6027\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5b66\u4e60\u7684\u53d8\u5f62-\u529b\u6a21\u578b\u548c\u8fde\u7eed\u65f6\u95f4B\u6837\u6761\u8fd0\u52a8\u6a21\u578b\uff0c\u5efa\u7acb\u89c6\u89c9\u8f68\u8ff9\u52a0\u901f\u5ea6\u4e0e\u53d8\u5f62\u8bf1\u5bfc\u52a0\u901f\u5ea6\u4e4b\u95f4\u7684\u7269\u7406\u8054\u7cfb\u3002", "motivation": "\u67d4\u6027\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7cbe\u786e\u72b6\u6001\u4f30\u8ba1\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5177\u6709\u52a8\u6001\u53d8\u5f62\u7ed3\u6784\u7684\u5e73\u53f0\uff0c\u8fd9\u4e9b\u7ed3\u6784\u4f7f\u521a\u4f53\u5047\u8bbe\u5931\u6548\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5c06\u73b0\u6709\u7684\u521a\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u6269\u5c55\u5230\u975e\u521a\u6027\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u591a\u5c42\u611f\u77e5\u5668\u5b66\u4e60\u6ce8\u5165\u5f0f\u53d8\u5f62-\u529b\u6a21\u578b\u6765\u6355\u6349\u5f39\u6027\u7279\u6027\uff1b\u91c7\u7528\u8fde\u7eed\u65f6\u95f4B\u6837\u6761\u8fd0\u52a8\u6a21\u578b\u6c42\u89e3\u5e73\u53f0\u7684\u5e73\u6ed1\u8fd0\u52a8\uff1b\u901a\u8fc7\u8fde\u7eed\u5e94\u7528\u725b\u987f\u7b2c\u4e8c\u5b9a\u5f8b\uff0c\u5efa\u7acb\u89c6\u89c9\u8f68\u8ff9\u52a0\u901f\u5ea6\u4e0e\u9884\u6d4b\u53d8\u5f62\u8bf1\u5bfc\u52a0\u901f\u5ea6\u4e4b\u95f4\u7684\u7269\u7406\u8054\u7cfb\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u5728\u975e\u521a\u6027\u5e73\u53f0\u4e0a\u5b9e\u73b0\u7a33\u5065\u51c6\u786e\u7684\u59ff\u6001\u4f30\u8ba1\uff0c\u800c\u4e14\u6b63\u786e\u5efa\u6a21\u7684\u5e73\u53f0\u7269\u7406\u7279\u6027\u6fc0\u53d1\u4e86\u60ef\u6027\u4f20\u611f\u7279\u6027\u3002\u5728\u7b80\u5355\u7684\u5f39\u7c27\u76f8\u673a\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u7a33\u5065\u89e3\u51b3\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\u4e2d\u5ea6\u91cf\u5c3a\u5ea6\u548c\u91cd\u529b\u6062\u590d\u8fd9\u4e00\u901a\u5e38\u4e0d\u9002\u5b9a\u95ee\u9898\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u521a\u4f53\u59ff\u6001\u4f30\u8ba1\u6269\u5c55\u5230\u975e\u521a\u6027\u7cfb\u7edf\uff0c\u901a\u8fc7\u7269\u7406\u5efa\u6a21\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u59ff\u6001\u4f30\u8ba1\uff0c\u5e76\u89e3\u51b3\u4e86\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\u4e2d\u7684\u5c3a\u5ea6\u548c\u91cd\u529b\u6062\u590d\u95ee\u9898\u3002"}}
{"id": "2511.20570", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20570", "abs": "https://arxiv.org/abs/2511.20570", "authors": ["Tasha Kim", "Oiwi Parker Jones"], "title": "Gated Uncertainty-Aware Runtime Dual Invariants for Neural Signal-Controlled Robotics", "comment": "Embodied and Safe-Assured Robotic Systems workshop at NeurIPS 2025", "summary": "Safety-critical assistive systems that directly decode user intent from neural signals require rigorous guarantees of reliability and trust. We present GUARDIAN (Gated Uncertainty-Aware Runtime Dual Invariants), a framework for real-time neuro-symbolic verification for neural signal-controlled robotics. GUARDIAN enforces both logical safety and physiological trust by coupling confidence-calibrated brain signal decoding with symbolic goal grounding and dual-layer runtime monitoring. On the BNCI2014 motor imagery electroencephalogram (EEG) dataset with 9 subjects and 5,184 trials, the system performs at a high safety rate of 94-97% even with lightweight decoder architectures with low test accuracies (27-46%) and high ECE confidence miscalibration (0.22-0.41). We demonstrate 1.7x correct interventions in simulated noise testing versus at baseline. The monitor operates at 100Hz and sub-millisecond decision latency, making it practically viable for closed-loop neural signal-based systems. Across 21 ablation results, GUARDIAN exhibits a graduated response to signal degradation, and produces auditable traces from intent, plan to action, helping to link neural evidence to verifiable robot action.", "AI": {"tldr": "GUARDIAN\u662f\u4e00\u4e2a\u7528\u4e8e\u795e\u7ecf\u4fe1\u53f7\u63a7\u5236\u673a\u5668\u4eba\u7684\u5b9e\u65f6\u795e\u7ecf\u7b26\u53f7\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u8111\u4fe1\u53f7\u89e3\u7801\u4e0e\u7b26\u53f7\u76ee\u6807\u63a5\u5730\u548c\u53cc\u5c42\u7ea7\u8fd0\u884c\u65f6\u76d1\u63a7\uff0c\u786e\u4fdd\u903b\u8f91\u5b89\u5168\u548c\u751f\u7406\u4fe1\u4efb\u3002", "motivation": "\u5b89\u5168\u5173\u952e\u7684\u8f85\u52a9\u7cfb\u7edf\u9700\u8981\u76f4\u63a5\u4ece\u795e\u7ecf\u4fe1\u53f7\u89e3\u7801\u7528\u6237\u610f\u56fe\uff0c\u8fd9\u8981\u6c42\u4e25\u683c\u7684\u53ef\u9760\u6027\u548c\u4fe1\u4efb\u4fdd\u8bc1\u3002", "method": "\u91c7\u7528\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u8111\u4fe1\u53f7\u89e3\u7801\u3001\u7b26\u53f7\u76ee\u6807\u63a5\u5730\u548c\u53cc\u5c42\u7ea7\u8fd0\u884c\u65f6\u76d1\u63a7\uff0c\u5728BNCI2014\u8fd0\u52a8\u60f3\u8c61EEG\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7cfb\u7edf\u5728\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u67b6\u6784\u4e0b\u5b9e\u73b0\u4e8694-97%\u7684\u9ad8\u5b89\u5168\u7387\uff0c\u5728\u6a21\u62df\u566a\u58f0\u6d4b\u8bd5\u4e2d\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e861.7\u500d\u7684\u6b63\u786e\u5e72\u9884\u7387\uff0c\u76d1\u63a7\u5668\u4ee5100Hz\u9891\u7387\u548c\u4e9a\u6beb\u79d2\u5ef6\u8fdf\u8fd0\u884c\u3002", "conclusion": "GUARDIAN\u6846\u67b6\u80fd\u591f\u63d0\u4f9b\u4ece\u610f\u56fe\u5230\u884c\u52a8\u7684\u53ef\u5ba1\u8ba1\u8ffd\u8e2a\uff0c\u5728\u4fe1\u53f7\u9000\u5316\u65f6\u8868\u73b0\u51fa\u6e10\u8fdb\u54cd\u5e94\uff0c\u9002\u7528\u4e8e\u95ed\u73af\u795e\u7ecf\u4fe1\u53f7\u7cfb\u7edf\u3002"}}
{"id": "2511.20593", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.20593", "abs": "https://arxiv.org/abs/2511.20593", "authors": ["Allen Emmanuel Binny", "Mahathi Anand", "Hugo T. M. Kussaba", "Lingyun Chen", "Shreenabh Agrawal", "Fares J. Abu-Dakka", "Abdalla Swikir"], "title": "Safe and Stable Neural Network Dynamical Systems for Robot Motion Planning", "comment": null, "summary": "Learning safe and stable robot motions from demonstrations remains a challenge, especially in complex, nonlinear tasks involving dynamic, obstacle-rich environments. In this paper, we propose Safe and Stable Neural Network Dynamical Systems S$^2$-NNDS, a learning-from-demonstration framework that simultaneously learns expressive neural dynamical systems alongside neural Lyapunov stability and barrier safety certificates. Unlike traditional approaches with restrictive polynomial parameterizations, S$^2$-NNDS leverages neural networks to capture complex robot motions providing probabilistic guarantees through split conformal prediction in learned certificates. Experimental results on various 2D and 3D datasets -- including LASA handwriting and demonstrations recorded kinesthetically from the Franka Emika Panda robot -- validate S$^2$-NNDS effectiveness in learning robust, safe, and stable motions from potentially unsafe demonstrations.", "AI": {"tldr": "S\u00b2-NNDS\u662f\u4e00\u4e2a\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u6846\u67b6\uff0c\u540c\u65f6\u5b66\u4e60\u795e\u7ecf\u52a8\u529b\u5b66\u7cfb\u7edf\u4ee5\u53ca\u795e\u7ecfLyapunov\u7a33\u5b9a\u6027\u548c\u5c4f\u969c\u5b89\u5168\u8bc1\u4e66\uff0c\u63d0\u4f9b\u6982\u7387\u5b89\u5168\u4fdd\u8bc1\u3002", "motivation": "\u5728\u590d\u6742\u975e\u7ebf\u6027\u4efb\u52a1\u4e2d\uff0c\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u5b89\u5168\u7a33\u5b9a\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u3001\u969c\u788d\u7269\u4e30\u5bcc\u7684\u73af\u5883\u4e2d\u3002", "method": "\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u590d\u6742\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u901a\u8fc7\u5206\u5272\u4fdd\u5f62\u9884\u6d4b\u5728\u5b66\u4e60\u8bc1\u4e66\u4e2d\u63d0\u4f9b\u6982\u7387\u4fdd\u8bc1\uff0c\u4e0d\u540c\u4e8e\u4f20\u7edf\u9650\u5236\u6027\u591a\u9879\u5f0f\u53c2\u6570\u5316\u65b9\u6cd5\u3002", "result": "\u57282D\u548c3D\u6570\u636e\u96c6\uff08\u5305\u62ecLASA\u624b\u5199\u548cFranka Emika Panda\u673a\u5668\u4eba\u6f14\u793a\uff09\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86S\u00b2-NNDS\u4ece\u6f5c\u5728\u4e0d\u5b89\u5168\u6f14\u793a\u4e2d\u5b66\u4e60\u9c81\u68d2\u3001\u5b89\u5168\u3001\u7a33\u5b9a\u8fd0\u52a8\u7684\u6709\u6548\u6027\u3002", "conclusion": "S\u00b2-NNDS\u80fd\u591f\u540c\u65f6\u5b66\u4e60\u8868\u8fbe\u6027\u795e\u7ecf\u52a8\u529b\u5b66\u7cfb\u7edf\u4ee5\u53ca\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u8bc1\u4e66\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u4e60\u63d0\u4f9b\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20633", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20633", "abs": "https://arxiv.org/abs/2511.20633", "authors": ["Jiahui Zhang", "Ze Huang", "Chun Gu", "Zipei Ma", "Li Zhang"], "title": "Reinforcing Action Policies by Prophesying", "comment": "https://LogosRoboticsGroup.github.io/ProphRL", "summary": "Vision-Language-Action (VLA) policies excel in aligning language, perception, and robot control. However, most VLAs are trained purely by imitation, which overfits to demonstrations, and is brittle under distribution shift. Reinforcement learning (RL) directly optimizes task reward and thus addresses this misalignment, but real-robot interaction is expensive and conventional simulators are hard to engineer and transfer. We address both data efficiency and optimization stability in VLA post-training via a learned world model and an RL procedure tailored to flow-based action heads. Specifically, we introduce Prophet, a unified action-to-video robot actuation pretrained across large-scale, heterogeneous robot data to learn reusable action-outcome dynamics. It is able to few-shot adapt to new robots, objects, and environments, yielding a rollout-ready simulator. Upon Prophet, we reinforce action policies with Flow-action-GRPO (FA-GRPO), which adapts Flow-GRPO to operate on VLA actions, and with FlowScale, a stepwise reweighting that rescales per-step gradients in the flow head. Together, Prophet, FA-GRPO, and FlowScale constitute ProphRL, a practical, data- and compute-efficient path to VLA post-training. Experiments show 5-17% success gains on public benchmarks and 24-30% gains on real robots across different VLA variants.", "AI": {"tldr": "ProphRL\u901a\u8fc7\u5148\u77e5\u4e16\u754c\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u7b56\u7565\uff0c\u5728\u516c\u5f00\u57fa\u51c6\u4e0a\u63d0\u53475-17%\u6210\u529f\u7387\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u63d0\u534724-30%\u6210\u529f\u7387", "motivation": "\u89e3\u51b3VLA\u7b56\u7565\u4ec5\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u548c\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u771f\u5b9e\u673a\u5668\u4eba\u4ea4\u4e92\u7684\u9ad8\u6210\u672c\u548c\u4f20\u7edf\u4eff\u771f\u5668\u7684\u5de5\u7a0b\u96be\u5ea6", "method": "\u63d0\u51faProphRL\u6846\u67b6\uff1a1) Prophet - \u8de8\u5927\u89c4\u6a21\u5f02\u6784\u673a\u5668\u4eba\u6570\u636e\u9884\u8bad\u7ec3\u7684\u52a8\u4f5c\u5230\u89c6\u9891\u4e16\u754c\u6a21\u578b\uff1b2) FA-GRPO - \u9002\u914dVLA\u52a8\u4f5c\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff1b3) FlowScale - \u6d41\u52a8\u4f5c\u5934\u4e2d\u7684\u68af\u5ea6\u91cd\u7f29\u653e\u65b9\u6cd5", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u4e0a\u83b7\u5f975-17%\u6210\u529f\u7387\u63d0\u5347\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u83b7\u5f9724-30%\u6210\u529f\u7387\u63d0\u5347\uff0c\u80fd\u591f\u5c11\u6837\u672c\u9002\u5e94\u65b0\u673a\u5668\u4eba\u3001\u7269\u4f53\u548c\u73af\u5883", "conclusion": "ProphRL\u4e3aVLA\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u6570\u636e\u548c\u8ba1\u7b97\u9ad8\u6548\u7684\u8def\u5f84\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd"}}
