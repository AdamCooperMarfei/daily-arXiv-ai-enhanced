<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 17]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation](https://arxiv.org/abs/2508.00097)
*Zhigen Zhao,Liuchuan Yu,Ke Jing,Ning Yang*

Main category: cs.RO

TL;DR: XRoboToolkit是一个基于OpenXR标准的跨平台扩展现实机器人遥操作框架，旨在解决现有数据收集方法的可扩展性和质量问题。


<details>
  <summary>Details</summary>
Motivation: 当前机器人演示数据收集方法存在可扩展性差、设置复杂和数据质量低的问题，需要一种更高效的解决方案。

Method: 提出XRoboToolkit框架，支持低延迟立体视觉反馈、优化逆运动学和多种跟踪模式，模块化设计便于跨平台集成。

Result: 通过精确操作任务验证了框架的有效性，并训练出具有鲁棒自主性能的VLA模型。

Conclusion: XRoboToolkit为高质量机器人演示数据收集提供了一种高效、可扩展的解决方案。

Abstract: The rapid advancement of Vision-Language-Action models has created an urgent
need for large-scale, high-quality robot demonstration datasets. Although
teleoperation is the predominant method for data collection, current approaches
suffer from limited scalability, complex setup procedures, and suboptimal data
quality. This paper presents XRoboToolkit, a cross-platform framework for
extended reality based robot teleoperation built on the OpenXR standard. The
system features low-latency stereoscopic visual feedback, optimization-based
inverse kinematics, and support for diverse tracking modalities including head,
controller, hand, and auxiliary motion trackers. XRoboToolkit's modular
architecture enables seamless integration across robotic platforms and
simulation environments, spanning precision manipulators, mobile robots, and
dexterous hands. We demonstrate the framework's effectiveness through precision
manipulation tasks and validate data quality by training VLA models that
exhibit robust autonomous performance.

</details>


### [2] [CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System](https://arxiv.org/abs/2508.00162)
*Noboru Myers,Obin Kwon,Sankalp Yamsani,Joohyung Kim*

Main category: cs.RO

TL;DR: CHILD是一种紧凑可重构的遥操作系统，支持人形机器人的关节级控制，适用于全身控制和移动操作。


<details>
  <summary>Details</summary>
Motivation: 现有遥操作技术很少支持人形机器人的全身关节级控制，限制了任务的多样性。

Method: CHILD系统设计为可穿戴设备，支持直接关节映射和自适应力反馈，确保操作安全和体验。

Result: 实验验证了CHILD在人形机器人和双臂系统上的移动操作和全身控制能力。

Conclusion: CHILD系统成功实现了人形机器人的关节级遥操作，并开源硬件设计以促进可访问性和可重复性。

Abstract: Recent advances in teleoperation have demonstrated robots performing complex
manipulation tasks. However, existing works rarely support whole-body
joint-level teleoperation for humanoid robots, limiting the diversity of tasks
that can be accomplished. This work presents Controller for Humanoid Imitation
and Live Demonstration (CHILD), a compact reconfigurable teleoperation system
that enables joint level control over humanoid robots. CHILD fits within a
standard baby carrier, allowing the operator control over all four limbs, and
supports both direct joint mapping for full-body control and loco-manipulation.
Adaptive force feedback is incorporated to enhance operator experience and
prevent unsafe joint movements. We validate the capabilities of this system by
conducting loco-manipulation and full-body control examples on a humanoid robot
and multiple dual-arm systems. Lastly, we open-source the design of the
hardware promoting accessibility and reproducibility. Additional details and
open-source information are available at our project website:
https://uiuckimlab.github.io/CHILD-pages.

</details>


### [3] [Topology-Inspired Morphological Descriptor for Soft Continuum Robots](https://arxiv.org/abs/2508.00258)
*Zhiwei Wu,Siyi Wei,Jiahao Luo,Jinhui Zhang*

Main category: cs.RO

TL;DR: 提出了一种基于拓扑的形态描述符，结合伪刚体模型和莫尔斯理论，用于软连续机器人的形态定量表征与控制。


<details>
  <summary>Details</summary>
Motivation: 提升软连续机器人在医疗应用（如微创手术和血管内介入）中的精确性和适应性。

Method: 结合伪刚体模型和莫尔斯理论，通过计算方向投影的临界点实现形态分类与控制。

Result: 实现了软连续机器人形态的定量描述、分类和控制，为优化目标形态提供方法。

Conclusion: 该框架为软连续机器人的形态描述与控制提供了统一方法，具有医疗应用的潜力。

Abstract: This paper presents a topology-inspired morphological descriptor for soft
continuum robots by combining a pseudo-rigid-body (PRB) model with Morse theory
to achieve a quantitative characterization of robot morphologies. By counting
critical points of directional projections, the proposed descriptor enables a
discrete representation of multimodal configurations and facilitates
morphological classification. Furthermore, we apply the descriptor to
morphology control by formulating the target configuration as an optimization
problem to compute actuation parameters that generate equilibrium shapes with
desired topological features. The proposed framework provides a unified
methodology for quantitative morphology description, classification, and
control of soft continuum robots, with the potential to enhance their precision
and adaptability in medical applications such as minimally invasive surgery and
endovascular interventions.

</details>


### [4] [UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents](https://arxiv.org/abs/2508.00288)
*Jianqiang Xiao,Yuexuan Sun,Yixin Shao,Boxi Gan,Rongqiang Liu,Yanjing Wu,Weili Gua,Xiang Deng*

Main category: cs.RO

TL;DR: 论文提出了UAV-ON基准，用于评估无人机在开放环境中基于语义目标的导航能力，解决了传统依赖语言指令的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多依赖语言指令（如VLN），限制了无人机导航的扩展性和自主性。UAV-ON旨在通过语义目标导航填补这一空白。

Method: 构建了包含14个高保真环境的UAV-ON基准，定义了1270个目标对象，并提出了模块化策略AOA，结合语义与观测进行导航。

Result: 实验显示基线方法表现不佳，突显了语义目标导航与空中导航的双重挑战。

Conclusion: UAV-ON为复杂环境中基于语义的无人机自主导航研究提供了新方向。

Abstract: Aerial navigation is a fundamental yet underexplored capability in embodied
intelligence, enabling agents to operate in large-scale, unstructured
environments where traditional navigation paradigms fall short. However, most
existing research follows the Vision-and-Language Navigation (VLN) paradigm,
which heavily depends on sequential linguistic instructions, limiting its
scalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark
for large-scale Object Goal Navigation (ObjectNav) by aerial agents in
open-world environments, where agents operate based on high-level semantic
goals without relying on detailed instructional guidance as in VLN. UAV-ON
comprises 14 high-fidelity Unreal Engine environments with diverse semantic
regions and complex spatial layouts, covering urban, natural, and mixed-use
settings. It defines 1270 annotated target objects, each characterized by an
instance-level instruction that encodes category, physical footprint, and
visual descriptors, allowing grounded reasoning. These instructions serve as
semantic goals, introducing realistic ambiguity and complex reasoning
challenges for aerial agents. To evaluate the benchmark, we implement several
baseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that
integrates instruction semantics with egocentric observations for long-horizon,
goal-directed exploration. Empirical results show that all baselines struggle
in this setting, highlighting the compounded challenges of aerial navigation
and semantic goal grounding. UAV-ON aims to advance research on scalable UAV
autonomy driven by semantic goal descriptions in complex real-world
environments.

</details>


### [5] [TopoDiffuser: A Diffusion-Based Multimodal Trajectory Prediction Model with Topometric Maps](https://arxiv.org/abs/2508.00303)
*Zehui Xu,Junhui Wang,Yongliang Shi,Chao Gao,Guyue Zhou*

Main category: cs.RO

TL;DR: TopoDiffuser是一种基于扩散模型的多模态轨迹预测框架，结合拓扑地图生成准确、多样且符合道路的未来运动预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成符合道路几何的轨迹时依赖显式约束，TopoDiffuser旨在通过嵌入拓扑地图的结构信息，无需显式约束即可实现自然符合道路几何的轨迹生成。

Method: 通过条件扩散模型，将拓扑地图的结构信息融入去噪过程，并使用多模态编码器融合LiDAR观测、历史运动和路径信息为统一的鸟瞰图表示。

Result: 在KITTI基准测试中，TopoDiffuser优于现有方法，并保持强几何一致性。消融实验验证了各输入模态的贡献以及去噪步骤和轨迹样本数量的影响。

Conclusion: TopoDiffuser通过结合拓扑地图和扩散模型，实现了无需显式约束的高质量轨迹预测，为未来研究提供了开源代码支持。

Abstract: This paper introduces TopoDiffuser, a diffusion-based framework for
multimodal trajectory prediction that incorporates topometric maps to generate
accurate, diverse, and road-compliant future motion forecasts. By embedding
structural cues from topometric maps into the denoising process of a
conditional diffusion model, the proposed approach enables trajectory
generation that naturally adheres to road geometry without relying on explicit
constraints. A multimodal conditioning encoder fuses LiDAR observations,
historical motion, and route information into a unified bird's-eye-view (BEV)
representation. Extensive experiments on the KITTI benchmark demonstrate that
TopoDiffuser outperforms state-of-the-art methods, while maintaining strong
geometric consistency. Ablation studies further validate the contribution of
each input modality, as well as the impact of denoising steps and the number of
trajectory samples. To support future research, we publicly release our code at
https://github.com/EI-Nav/TopoDiffuser.

</details>


### [6] [Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging](https://arxiv.org/abs/2508.00354)
*Tianshuang Qiu,Zehan Ma,Karim El-Refai,Hiya Shah,Chung Min Kim,Justin Kerr,Ken Goldberg*

Main category: cs.RO

TL;DR: Omni-Scan是一种利用双手机器人抓取和旋转物体以生成高质量3D高斯溅射模型的管道，适用于零件缺陷检测。


<details>
  <summary>Details</summary>
Motivation: 现有3D物体扫描方法受限于工作空间和设备，需要更灵活的解决方案。

Method: 使用双手机器人抓取物体并旋转，结合DepthAnything、Segment Anything和RAFT光流模型去除背景和夹持器遮挡，改进3DGS训练管道。

Result: 在12种工业和家用物体上检测视觉或几何缺陷的平均准确率为83%。

Conclusion: Omni-Scan提供了一种高效且灵活的3D物体建模方法，适用于多种应用场景。

Abstract: 3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view
images. Such "digital twins" are useful for simulations, virtual reality,
marketing, robot policy fine-tuning, and part inspection. 3D object scanning
usually requires multi-camera arrays, precise laser scanners, or robot
wrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,
a pipeline for producing high-quality 3D Gaussian Splat models using a
bi-manual robot that grasps an object with one gripper and rotates the object
with respect to a stationary camera. The object is then re-grasped by a second
gripper to expose surfaces that were occluded by the first gripper. We present
the Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as
RAFT optical flow models to identify and isolate objects held by a robot
gripper while removing the gripper and the background. We then modify the 3DGS
training pipeline to support concatenated datasets with gripper occlusion,
producing an omni-directional (360 degree view) model of the object. We apply
Omni-Scan to part defect inspection, finding that it can identify visual or
geometric defects in 12 different industrial and household objects with an
average accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be
found at https://berkeleyautomation.github.io/omni-scan/

</details>


### [7] [TOP: Time Optimization Policy for Stable and Accurate Standing Manipulation with Humanoid Robots](https://arxiv.org/abs/2508.00355)
*Zhenghan Chen,Haocheng Xu,Haodong Zhang,Liang Zhang,He Li,Dongqi Wang,Jiyu Yu,Yifei Yang,Zhongxiang Zhou,Rong Xiong*

Main category: cs.RO

TL;DR: 提出了一种新颖的时间优化策略（TOP），用于训练人形机器人的站立操控控制模型，确保平衡、精确和时间效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时满足高维上半身关节的精确控制和整体稳定性，尤其是在上半身快速运动时。

Method: 结合变分自编码器（VAE）表示运动先验，将全身控制解耦为上半身的PD控制器和下半身的强化学习控制器，并训练TOP策略以优化时间轨迹。

Result: 仿真和实际实验验证了该方法在稳定且精确完成站立操控任务上的优越性。

Conclusion: TOP方法有效解决了上半身快速运动对平衡的干扰问题，提升了人形机器人的操控能力。

Abstract: Humanoid robots have the potential capability to perform a diverse range of
manipulation tasks, but this is based on a robust and precise standing
controller. Existing methods are either ill-suited to precisely control
high-dimensional upper-body joints, or difficult to ensure both robustness and
accuracy, especially when upper-body motions are fast. This paper proposes a
novel time optimization policy (TOP), to train a standing manipulation control
model that ensures balance, precision, and time efficiency simultaneously, with
the idea of adjusting the time trajectory of upper-body motions but not only
strengthening the disturbance resistance of the lower-body. Our approach
consists of three parts. Firstly, we utilize motion prior to represent
upper-body motions to enhance the coordination ability between the upper and
lower-body by training a variational autoencoder (VAE). Then we decouple the
whole-body control into an upper-body PD controller for precision and a
lower-body RL controller to enhance robust stability. Finally, we train TOP
method in conjunction with the decoupled controller and VAE to reduce the
balance burden resulting from fast upper-body motions that would destabilize
the robot and exceed the capabilities of the lower-body RL policy. The
effectiveness of the proposed approach is evaluated via both simulation and
real world experiments, which demonstrate the superiority on standing
manipulation tasks stably and accurately. The project page can be found at
https://anonymous.4open.science/w/top-258F/.

</details>


### [8] [A Whole-Body Motion Imitation Framework from Human Data for Full-Size Humanoid Robot](https://arxiv.org/abs/2508.00362)
*Zhenghan Chen,Haodong Zhang,Dongqi Wang,Jiyu Yu,Haocheng Xu,Yue Wang,Rong Xiong*

Main category: cs.RO

TL;DR: 提出了一种新颖的全身运动模仿框架，用于全尺寸人形机器人，通过接触感知的全身运动重定向和非线性质心模型预测控制器，实现了高精度运动模仿与实时平衡控制。


<details>
  <summary>Details</summary>
Motivation: 人形机器人模仿人类运动是实现多样化复杂动作的关键，但由于机器人与人类在运动学和动力学上的差异，准确模仿并保持平衡具有挑战性。

Method: 采用接触感知的全身运动重定向方法模仿人类运动，并结合非线性质心模型预测控制器实时确保运动精度与平衡。

Result: 实验表明，该方法在仿真和真实人形机器人中均能准确模仿多种人类动作，并具备适应性和平衡能力。

Conclusion: 所提出的框架有效解决了人形机器人运动模仿的挑战，验证了其在实际应用中的可行性与有效性。

Abstract: Motion imitation is a pivotal and effective approach for humanoid robots to
achieve a more diverse range of complex and expressive movements, making their
performances more human-like. However, the significant differences in
kinematics and dynamics between humanoid robots and humans present a major
challenge in accurately imitating motion while maintaining balance. In this
paper, we propose a novel whole-body motion imitation framework for a full-size
humanoid robot. The proposed method employs contact-aware whole-body motion
retargeting to mimic human motion and provide initial values for reference
trajectories, and the non-linear centroidal model predictive controller ensures
the motion accuracy while maintaining balance and overcoming external
disturbances in real time. The assistance of the whole-body controller allows
for more precise torque control. Experiments have been conducted to imitate a
variety of human motions both in simulation and in a real-world humanoid robot.
These experiments demonstrate the capability of performing with accuracy and
adaptability, which validates the effectiveness of our approach.

</details>


### [9] [On Learning Closed-Loop Probabilistic Multi-Agent Simulator](https://arxiv.org/abs/2508.00384)
*Juanwu Lu,Rohit Gupta,Ahmadreza Moradipari,Kyungtae Han,Ruqi Zhang,Ziran Wang*

Main category: cs.RO

TL;DR: NIVA是一个基于分层贝叶斯模型的概率框架，用于多智能体交通模拟，通过自回归采样实现闭环模拟，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆的快速部署，需要构建真实且可扩展的多智能体交通模拟器以高效评估。

Method: NIVA采用分层贝叶斯模型，通过自回归采样从高斯分布的潜在有限混合中生成闭环、观察条件下的模拟。

Result: 在Waymo Open Motion数据集上的实验表明，NIVA性能优于现有方法，并能灵活控制意图和驾驶风格。

Conclusion: NIVA为多智能体模拟提供了一种统一且高效的方法，具有实际应用潜力。

Abstract: The rapid iteration of autonomous vehicle (AV) deployments leads to
increasing needs for building realistic and scalable multi-agent traffic
simulators for efficient evaluation. Recent advances in this area focus on
closed-loop simulators that enable generating diverse and interactive
scenarios. This paper introduces Neural Interactive Agents (NIVA), a
probabilistic framework for multi-agent simulation driven by a hierarchical
Bayesian model that enables closed-loop, observation-conditioned simulation
through autoregressive sampling from a latent, finite mixture of Gaussian
distributions. We demonstrate how NIVA unifies preexisting sequence-to-sequence
trajectory prediction models and emerging closed-loop simulation models trained
on Next-token Prediction (NTP) from a Bayesian inference perspective.
Experiments on the Waymo Open Motion Dataset demonstrate that NIVA attains
competitive performance compared to the existing method while providing
embellishing control over intentions and driving styles.

</details>


### [10] [SubCDM: Collective Decision-Making with a Swarm Subset](https://arxiv.org/abs/2508.00467)
*Samratul Fuady,Danesh Tarapore,Mohammad D. Soorati*

Main category: cs.RO

TL;DR: 提出了一种基于子集的集体决策方法（SubCDM），通过动态构建子集减少资源消耗，同时保持决策准确性。


<details>
  <summary>Details</summary>
Motivation: 现有集体决策方法需要所有机器人参与，资源消耗大且无法分配机器人执行其他任务。

Method: 动态构建子集，仅依赖局部信息，自适应确定子集规模以达成共识。

Result: 仿真实验表明，SubCDM在减少机器人参与数量的同时，保持了与全群决策相当的准确性。

Conclusion: SubCDM是一种资源高效的集体决策方法，适用于群体机器人系统。

Abstract: Collective decision-making is a key function of autonomous robot swarms,
enabling them to reach a consensus on actions based on environmental features.
Existing strategies require the participation of all robots in the
decision-making process, which is resource-intensive and prevents the swarm
from allocating the robots to any other tasks. We propose Subset-Based
Collective Decision-Making (SubCDM), which enables decisions using only a swarm
subset. The construction of the subset is dynamic and decentralized, relying
solely on local information. Our method allows the swarm to adaptively
determine the size of the subset for accurate decision-making, depending on the
difficulty of reaching a consensus. Simulation results using one hundred robots
show that our approach achieves accuracy comparable to using the entire swarm
while reducing the number of robots required to perform collective
decision-making, making it a resource-efficient solution for collective
decision-making in swarm robotics.

</details>


### [11] [HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning](https://arxiv.org/abs/2508.00491)
*Carlo Alessi,Federico Vasile,Federico Ceola,Giulia Pasquale,Nicolò Boccardo,Lorenzo Natale*

Main category: cs.RO

TL;DR: 论文提出了一种基于模仿学习的假手控制方法HannesImitationPolicy，通过扩散策略预测手腕方向和手部闭合动作，成功实现了在非结构化环境中的物体抓取。


<details>
  <summary>Details</summary>
Motivation: 减少用户认知负担，提升假手在非结构化环境中的操作能力，探索模仿学习在假手控制中的应用。

Method: 使用HannesImitationDataset中的抓取演示数据，训练单一扩散策略，预测手腕方向和手部闭合动作。

Result: 实验表明，该方法在多种物体和条件下成功抓取，且优于基于分割的视觉伺服控制器。

Conclusion: 模仿学习方法可有效提升假手的灵活性和适应性，适用于非结构化环境。

Abstract: Recent advancements in control of prosthetic hands have focused on increasing
autonomy through the use of cameras and other sensory inputs. These systems aim
to reduce the cognitive load on the user by automatically controlling certain
degrees of freedom. In robotics, imitation learning has emerged as a promising
approach for learning grasping and complex manipulation tasks while simplifying
data collection. Its application to the control of prosthetic hands remains,
however, largely unexplored. Bridging this gap could enhance dexterity
restoration and enable prosthetic devices to operate in more unconstrained
scenarios, where tasks are learned from demonstrations rather than relying on
manually annotated sequences. To this end, we present HannesImitationPolicy, an
imitation learning-based method to control the Hannes prosthetic hand, enabling
object grasping in unstructured environments. Moreover, we introduce the
HannesImitationDataset comprising grasping demonstrations in table, shelf, and
human-to-prosthesis handover scenarios. We leverage such data to train a single
diffusion policy and deploy it on the prosthetic hand to predict the wrist
orientation and hand closure for grasping. Experimental evaluation demonstrates
successful grasps across diverse objects and conditions. Finally, we show that
the policy outperforms a segmentation-based visual servo controller in
unstructured scenarios. Additional material is provided on our project page:
https://hsp-iit.github.io/HannesImitation

</details>


### [12] [OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery](https://arxiv.org/abs/2508.00580)
*Raul Castilla-Arquillo,Carlos Perez-del-Pulgar,Levin Gerdes,Alfonso Garcia-Cerezo,Miguel A. Olivares-Mendez*

Main category: cs.RO

TL;DR: OmniUnet是一种基于Transformer的神经网络架构，用于RGB-D-T图像语义分割，在火星探测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，机器人导航需要多模态感知系统以确保安全，而火星探测中热成像对地形安全评估尤为重要。

Method: 开发了OmniUnet架构，结合RGB、深度和热成像数据，并使用3D打印传感器外壳在火星模拟环境中收集数据集。

Result: 模型像素准确率达80.37%，在复杂地形分割中表现优异，推理时间673ms，适合机器人部署。

Conclusion: OmniUnet在多模态地形感知中表现高效，公开了软件和数据集以支持未来研究。

Abstract: Robot navigation in unstructured environments requires multimodal perception
systems that can support safe navigation. Multimodality enables the integration
of complementary information collected by different sensors. However, this
information must be processed by machine learning algorithms specifically
designed to leverage heterogeneous data. Furthermore, it is necessary to
identify which sensor modalities are most informative for navigation in the
target environment. In Martian exploration, thermal imagery has proven valuable
for assessing terrain safety due to differences in thermal behaviour between
soil types. This work presents OmniUnet, a transformer-based neural network
architecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)
imagery. A custom multimodal sensor housing was developed using 3D printing and
mounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a
multimodal dataset in the Bardenas semi-desert in northern Spain. This location
serves as a representative environment of the Martian surface, featuring
terrain types such as sand, bedrock, and compact soil. A subset of this dataset
was manually labeled to support supervised training of the network. The model
was evaluated both quantitatively and qualitatively, achieving a pixel accuracy
of 80.37% and demonstrating strong performance in segmenting complex
unstructured terrain. Inference tests yielded an average prediction time of 673
ms on a resource-constrained computer (Jetson Orin Nano), confirming its
suitability for on-robot deployment. The software implementation of the network
and the labeled dataset have been made publicly available to support future
research in multimodal terrain perception for planetary robotics.

</details>


### [13] [A control scheme for collaborative object transportation between a human and a quadruped robot using the MIGHTY suction cup](https://arxiv.org/abs/2508.00584)
*Konstantinos Plotas,Emmanouil Papadakis,Drosakis Drosakis,Panos Trahanias,Dimitrios Papageorgiou*

Main category: cs.RO

TL;DR: 提出了一种基于导纳控制的四足机器人与人类协作运输物体的控制方案，结合可变阻尼和屏障人工势能确保安全性和可控性。


<details>
  <summary>Details</summary>
Motivation: 解决人机协作运输中的人类控制性和努力减少问题，同时确保物体不会从吸盘上脱落。

Method: 采用导纳控制框架，引入可变阻尼和屏障人工势能作为额外控制信号。

Result: 实验验证了控制方案的被动性和有效性，使用Unitree Go1机器人和MIGHTY吸盘进行测试。

Conclusion: 提出的控制方案在提高人机协作效率和安全性方面表现良好。

Abstract: In this work, a control scheme for human-robot collaborative object
transportation is proposed, considering a quadruped robot equipped with the
MIGHTY suction cup that serves both as a gripper for holding the object and a
force/torque sensor. The proposed control scheme is based on the notion of
admittance control, and incorporates a variable damping term aiming towards
increasing the controllability of the human and, at the same time, decreasing
her/his effort. Furthermore, to ensure that the object is not detached from the
suction cup during the collaboration, an additional control signal is proposed,
which is based on a barrier artificial potential. The proposed control scheme
is proven to be passive and its performance is demonstrated through
experimental evaluations conducted using the Unitree Go1 robot equipped with
the MIGHTY suction cup.

</details>


### [14] [OpenScout v1.1 mobile robot: a case study on open hardware continuation](https://arxiv.org/abs/2508.00625)
*Bartosz Krawczyk,Ahmed Elbary,Robbie Cato,Jagdish Patil,Kaung Myat,Anyeh Ndi-Tah,Nivetha Sakthivel,Mark Crampton,Gautham Das,Charles Fox*

Main category: cs.RO

TL;DR: OpenScout v1.1是一款开源的移动机器人，用于研究和工业领域，具有更简化、更便宜且更强大的计算硬件，支持ROS2接口和Gazebo模拟。


<details>
  <summary>Details</summary>
Motivation: 为研究和工业提供一个低成本、高性能的开源硬件移动机器人平台。

Method: 通过简化硬件设计、优化计算能力，并集成ROS2和Gazebo模拟功能。

Result: 成功开发了OpenScout v1.1，提供了更高效、更经济的解决方案。

Conclusion: OpenScout v1.1是一个成功的开源硬件案例，展示了其在研究和工业中的潜力。

Abstract: OpenScout is an Open Source Hardware (OSH) mobile robot for research and
industry. It is extended to v1.1 which includes simplified, cheaper and more
powerful onboard compute hardware; a simulated ROS2 interface; and a Gazebo
simulation. Changes, their rationale, project methodology, and results are
reported as an OSH case study.

</details>


### [15] [Towards Data-Driven Adaptive Exoskeleton Assistance for Post-stroke Gait](https://arxiv.org/abs/2508.00691)
*Fabian C. Weigend,Dabin K. Choe,Santiago Canete,Conor J. Walsh*

Main category: cs.RO

TL;DR: 数据驱动方法用于外骨骼控制，首次在卒中后步态中实现自适应踝关节力矩估计，验证了实时控制的可行性。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动方法在卒中后步态辅助中的挑战，如人群异质性和数据缺乏，以实现安全有效的社区应用。

Method: 使用多任务时间卷积网络（TCN），结合卒中后和健康步态数据，通过IMU数据进行踝关节力矩估计。

Result: 模型在卒中后步态数据上表现良好（R²=0.74±0.13），并在原型中验证了实时控制的可行性。

Conclusion: 数据驱动方法为卒中后外骨骼控制提供了可行路径，未来需进一步优化和扩展。

Abstract: Recent work has shown that exoskeletons controlled through data-driven
methods can dynamically adapt assistance to various tasks for healthy young
adults. However, applying these methods to populations with neuromotor gait
deficits, such as post-stroke hemiparesis, is challenging. This is due not only
to high population heterogeneity and gait variability but also to a lack of
post-stroke gait datasets to train accurate models. Despite these challenges,
data-driven methods offer a promising avenue for control, potentially allowing
exoskeletons to function safely and effectively in unstructured community
settings. This work presents a first step towards enabling adaptive
plantarflexion and dorsiflexion assistance from data-driven torque estimation
during post-stroke walking. We trained a multi-task Temporal Convolutional
Network (TCN) using collected data from four post-stroke participants walking
on a treadmill ($R^2$ of $0.74 \pm 0.13$). The model uses data from three
inertial measurement units (IMU) and was pretrained on healthy walking data
from 6 participants. We implemented a wearable prototype for our ankle torque
estimation approach for exoskeleton control and demonstrated the viability of
real-time sensing, estimation, and actuation with one post-stroke participant.

</details>


### [16] [On-Device Diffusion Transformer Policy for Efficient Robot Manipulation](https://arxiv.org/abs/2508.00697)
*Yiming Wu,Huan Wang,Zhenghao Chen,Jianxin Pang,Dong Xu*

Main category: cs.RO

TL;DR: LightDP 是一个专为移动设备设计的框架，通过压缩去噪模块和减少采样步骤，加速 Diffusion Policies 的实时部署。


<details>
  <summary>Details</summary>
Motivation: Diffusion Policies 在资源受限的移动平台上应用时存在计算效率低和内存占用大的问题。

Method: LightDP 采用网络压缩和采样步骤减少策略，并通过统一的剪枝和再训练流程优化模型恢复能力。

Result: 在多个标准数据集上，LightDP 实现了实时动作预测，性能接近最先进的 Diffusion Policies。

Conclusion: LightDP 是资源受限环境中部署基于扩散的策略的重要进展。

Abstract: Diffusion Policies have significantly advanced robotic manipulation tasks via
imitation learning, but their application on resource-constrained mobile
platforms remains challenging due to computational inefficiency and extensive
memory footprint. In this paper, we propose LightDP, a novel framework
specifically designed to accelerate Diffusion Policies for real-time deployment
on mobile devices. LightDP addresses the computational bottleneck through two
core strategies: network compression of the denoising modules and reduction of
the required sampling steps. We first conduct an extensive computational
analysis on existing Diffusion Policy architectures, identifying the denoising
network as the primary contributor to latency. To overcome performance
degradation typically associated with conventional pruning methods, we
introduce a unified pruning and retraining pipeline, optimizing the model's
post-pruning recoverability explicitly. Furthermore, we combine pruning
techniques with consistency distillation to effectively reduce sampling steps
while maintaining action prediction accuracy. Experimental evaluations on the
standard datasets, \ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that
LightDP achieves real-time action prediction on mobile devices with competitive
performance, marking an important step toward practical deployment of
diffusion-based policies in resource-limited environments. Extensive real-world
experiments also show the proposed LightDP can achieve performance comparable
to state-of-the-art Diffusion Policies.

</details>


### [17] [Video Generators are Robot Policies](https://arxiv.org/abs/2508.00795)
*Junbang Liang,Pavel Tokmakov,Ruoshi Liu,Sruthi Sudhakar,Paarth Shah,Rares Ambrus,Carl Vondrick*

Main category: cs.RO

TL;DR: 论文提出Video Policy框架，通过视频生成作为机器人策略学习的代理，解决泛化性和数据效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前视觉运动策略在感知或行为分布变化下泛化能力有限，且依赖大量人类演示数据。

Method: 结合视频和动作生成的模块化框架，端到端训练。

Result: 生成机器人行为视频可提取高效策略，显著提升鲁棒性和样本效率，泛化能力强。

Conclusion: 视频生成模型为机器人策略学习提供了更高效和可扩展的途径。

Abstract: Despite tremendous progress in dexterous manipulation, current visuomotor
policies remain fundamentally limited by two challenges: they struggle to
generalize under perceptual or behavioral distribution shifts, and their
performance is constrained by the size of human demonstration data. In this
paper, we use video generation as a proxy for robot policy learning to address
both limitations simultaneously. We propose Video Policy, a modular framework
that combines video and action generation that can be trained end-to-end. Our
results demonstrate that learning to generate videos of robot behavior allows
for the extraction of policies with minimal demonstration data, significantly
improving robustness and sample efficiency. Our method shows strong
generalization to unseen objects, backgrounds, and tasks, both in simulation
and the real world. We further highlight that task success is closely tied to
the generated video, with action-free video data providing critical benefits
for generalizing to novel tasks. By leveraging large-scale video generative
models, we achieve superior performance compared to traditional behavior
cloning, paving the way for more scalable and data-efficient robot policy
learning.

</details>
