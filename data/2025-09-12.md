<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 19]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Multi Robot Coordination in Highly Dynamic Environments: Tackling Asymmetric Obstacles and Limited Communication](https://arxiv.org/abs/2509.08859)
*Vincenzo Suriani,Daniele Affinita,Domenico D. Bloisi,Daniele Nardi*

Main category: cs.RO

TL;DR: 提出一种分布式多智能体协调方法，用于通信受限、环境部分可观测且存在非对称障碍物的场景，显著减少任务重叠52%


<details>
  <summary>Details</summary>
Motivation: 解决通信能力有限（发送速率和包载荷受限）的多智能体系统在高度部分可观测环境中协调任务的挑战，特别是处理需要频繁重新分配任务的动态场景和非对称障碍物

Method: 受市场式任务分配启发，提出新颖的分布式协调算法，专门处理非对称障碍物和低通信场景，通过仿真和NAO机器人实际比赛验证

Result: 实验结果显示在有限通信设置下任务重叠显著减少，最频繁重新分配的任务减少了52%

Conclusion: 该方法能有效处理主动非对称障碍物、通信质量差和部分可观测环境下的多智能体协调问题，具有实际应用价值

Abstract: Coordinating a fully distributed multi-agent system (MAS) can be challenging
when the communication channel has very limited capabilities in terms of
sending rate and packet payload. When the MAS has to deal with active obstacles
in a highly partially observable environment, the communication channel
acquires considerable relevance. In this paper, we present an approach to deal
with task assignments in extremely active scenarios, where tasks need to be
frequently reallocated among the agents participating in the coordination
process. Inspired by market-based task assignments, we introduce a novel
distributed coordination method to orchestrate autonomous agents' actions
efficiently in low communication scenarios. In particular, our algorithm takes
into account asymmetric obstacles. While in the real world, the majority of
obstacles are asymmetric, they are usually treated as symmetric ones, thus
limiting the applicability of existing methods. To summarize, the presented
architecture is designed to tackle scenarios where the obstacles are active and
asymmetric, the communication channel is poor and the environment is partially
observable. Our approach has been validated in simulation and in the real
world, using a team of NAO robots during official RoboCup competitions.
Experimental results show a notable reduction in task overlaps in limited
communication settings, with a decrease of 52% in the most frequent reallocated
task.

</details>


### [2] [Rapid Manufacturing of Lightweight Drone Frames Using Single-Tow Architected Composites](https://arxiv.org/abs/2509.09024)
*Md Habib Ullah Khan,Kaiyue Deng,Ismail Mujtaba Khan,Kelvin Fu*

Main category: cs.RO

TL;DR: 通过3D纤维缆结构技术（3DFiT）制造连续单纽纤维增强的面心立方格子无人机架体，实现轻量化和高强度，性能超过金属和热塑性材料


<details>
  <summary>Details</summary>
Motivation: 传统复合材料制造方法难以实现复杂三维结构，并需组装多个部件导致缺陷，连续纤维增强遇到挑战

Method: 使用3D纤维缆结构技术（3DFiT），采用连续单纽纤维构建面心立方格子结构，确保精确的纤维对齐

Result: 无人机架体仅重260克，比DJI F450轻10%，比强度是金属和热塑性材料的4-8倍，飞行时间延长3分钟

Conclusion: 单纽格子架构无人机架体具有很大潜力，3DFiT技术是一种可扩展的高效制造方法

Abstract: The demand for lightweight and high-strength composite structures is rapidly
growing in aerospace and robotics, particularly for optimized drone frames.
However, conventional composite manufacturing methods struggle to achieve
complex 3D architectures for weight savings and rely on assembling separate
components, which introduce weak points at the joints. Additionally,
maintaining continuous fiber reinforcement remains challenging, limiting
structural efficiency. In this study, we demonstrate the lightweight Face
Centered Cubic (FFC) lattice structured conceptualization of drone frames for
weight reduction and complex topology fabrication through 3D Fiber Tethering
(3DFiT) using continuous single tow fiber ensuring precise fiber alignment,
eliminating weak points associated with traditional composite assembly.
Mechanical testing demonstrates that the fabricated drone frame exhibits a high
specific strength of around four to eight times the metal and thermoplastic,
outperforming other conventional 3D printing methods. The drone frame weighs
only 260 g, making it 10% lighter than the commercial DJI F450 frame, enhancing
structural integrity and contributing to an extended flight time of three
minutes, while flight testing confirms its stability and durability under
operational conditions. The findings demonstrate the potential of single tow
lattice truss-based drone frames, with 3DFiT serving as a scalable and
efficient manufacturing method.

</details>


### [3] [KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning](https://arxiv.org/abs/2509.09074)
*Alice Kate Li,Thales C Silva,Victoria Edwards,Vijay Kumar,M. Ani Hsieh*

Main category: cs.RO

TL;DR: KoopMotion是一种基于流场的运动规划方法，利用Koopman算子理论学习期望轨迹的动力学系统，能够从任意初始状态收敛到目标轨迹终点，具有高样本效率。


<details>
  <summary>Details</summary>
Motivation: 虽然Koopman算子理论在动力学系统建模中表现出色，但它本身不能保证收敛到期望轨迹或指定目标，这在从演示学习(LfD)中是必需的要求。

Method: 提出KoopMotion方法，将运动流场表示为参数化Koopman算子的动力学系统，利用学习流场的发散特性获得平滑运动场，使机器人能够收敛到期望参考轨迹并跟踪至终点。

Result: 在LASA手写数据集和3D机械臂末端轨迹数据集上评估显示优异性能，物理机器人实验验证了在非静态流体环境中的有效性，仅需3%的LASA数据即可生成密集运动规划，在时空动力学建模指标上显著优于基线方法。

Conclusion: KoopMotion提供了一种样本高效的流场运动规划方法，能够有效解决从任意初始状态收敛到目标轨迹的问题，在复杂环境中表现出良好的性能。

Abstract: In this work, we propose a novel flow field-based motion planning method that
drives a robot from any initial state to a desired reference trajectory such
that it converges to the trajectory's end point. Despite demonstrated efficacy
in using Koopman operator theory for modeling dynamical systems, Koopman does
not inherently enforce convergence to desired trajectories nor to specified
goals -- a requirement when learning from demonstrations (LfD). We present
KoopMotion which represents motion flow fields as dynamical systems,
parameterized by Koopman Operators to mimic desired trajectories, and leverages
the divergence properties of the learnt flow fields to obtain smooth motion
fields that converge to a desired reference trajectory when a robot is placed
away from the desired trajectory, and tracks the trajectory until the end
point. To demonstrate the effectiveness of our approach, we show evaluations of
KoopMotion on the LASA human handwriting dataset and a 3D manipulator
end-effector trajectory dataset, including spectral analysis. We also perform
experiments on a physical robot, verifying KoopMotion on a miniature autonomous
surface vehicle operating in a non-static fluid flow environment. Our approach
is highly sample efficient in both space and time, requiring only 3\% of the
LASA dataset to generate dense motion plans. Additionally, KoopMotion provides
a significant improvement over baselines when comparing metrics that measure
spatial and temporal dynamics modeling efficacy.

</details>


### [4] [Kinetostatics and Particle-Swarm Optimization of Vehicle-Mounted Underactuated Metamorphic Loading Manipulators](https://arxiv.org/abs/2509.09093)
*Nan Mao,Guanglu Jia,Junpeng Chen,Emmanouil Spyrakos-Papastavridis,Jian S. Dai*

Main category: cs.RO

TL;DR: 提出了一种欠驱动变结构加载机械手(UMLM)，将变结构臂与被动自适应夹爪集成，通过几何约束实现拓扑重构和灵活运动轨迹，无需额外驱动器。


<details>
  <summary>Details</summary>
Motivation: 解决传统固定自由度加载机构存在驱动器过多、控制复杂、对动态任务适应性有限的问题，开发高效、适应性强的加载解决方案。

Method: 集成变结构臂和被动自适应夹爪，利用几何约束实现拓扑重构，进行运动静力学分析，采用粒子群优化算法优化夹爪尺寸参数。

Result: 仿真验证了UMLM控制策略易于实现、操作灵活，在动态环境中能有效抓取各种物体。

Conclusion: 欠驱动变结构机构在需要高效和适应性加载的应用中具有实际潜力，所提出的建模和优化框架可扩展到更广泛的机械手类别。

Abstract: Fixed degree-of-freedom (DoF) loading mechanisms often suffer from excessive
actuators, complex control, and limited adaptability to dynamic tasks. This
study proposes an innovative mechanism of underactuated metamorphic loading
manipulators (UMLM), integrating a metamorphic arm with a passively adaptive
gripper. The metamorphic arm exploits geometric constraints, enabling the
topology reconfiguration and flexible motion trajectories without additional
actuators. The adaptive gripper, driven entirely by the arm, conforms to
diverse objects through passive compliance. A structural model is developed,
and a kinetostatics analysis is conducted to investigate isomorphic grasping
configurations. To optimize performance, Particle-Swarm Optimization (PSO) is
utilized to refine the gripper's dimensional parameters, ensuring robust
adaptability across various applications. Simulation results validate the
UMLM's easily implemented control strategy, operational versatility, and
effectiveness in grasping diverse objects in dynamic environments. This work
underscores the practical potential of underactuated metamorphic mechanisms in
applications requiring efficient and adaptable loading solutions. Beyond the
specific design, this generalized modeling and optimization framework extends
to a broader class of manipulators, offering a scalable approach to the
development of robotic systems that require efficiency, flexibility, and robust
performance.

</details>


### [5] [LIPM-Guided Reinforcement Learning for Stable and Perceptive Locomotion in Bipedal Robots](https://arxiv.org/abs/2509.09106)
*Haokai Su,Haoxiang Luo,Shunpeng Yang,Kaiwen Jiang,Wei Zhang,Hua Chen*

Main category: cs.RO

TL;DR: 提出基于线性倒立摆模型(LIPM)的奖励函数设计，实现双足机器人在非结构化户外环境中的感知和稳定运动


<details>
  <summary>Details</summary>
Motivation: 解决双足机器人在复杂地形几何和外部干扰下的稳定感知运动挑战，需要理论指导来确保动态平衡和稳定的摄像头视角

Method: 设计基于LIPM理论的奖励函数，调节质心高度和躯干方向；采用奖励融合模块(RFM)自适应平衡速度跟踪和稳定性；使用双评论家架构分别评估稳定性和运动目标

Result: 在仿真和真实户外环境中验证，表现出优异的地形适应性、干扰抑制能力，以及在各种速度和感知条件下的一致性能

Conclusion: 基于LIPM的奖励设计和双评论家架构有效提升了双足机器人在非结构化环境中的感知运动稳定性和鲁棒性

Abstract: Achieving stable and robust perceptive locomotion for bipedal robots in
unstructured outdoor environments remains a critical challenge due to complex
terrain geometry and susceptibility to external disturbances. In this work, we
propose a novel reward design inspired by the Linear Inverted Pendulum Model
(LIPM) to enable perceptive and stable locomotion in the wild. The LIPM
provides theoretical guidance for dynamic balance by regulating the center of
mass (CoM) height and the torso orientation. These are key factors for
terrain-aware locomotion, as they help ensure a stable viewpoint for the
robot's camera. Building on this insight, we design a reward function that
promotes balance and dynamic stability while encouraging accurate CoM
trajectory tracking. To adaptively trade off between velocity tracking and
stability, we leverage the Reward Fusion Module (RFM) approach that prioritizes
stability when needed. A double-critic architecture is adopted to separately
evaluate stability and locomotion objectives, improving training efficiency and
robustness. We validate our approach through extensive experiments on a bipedal
robot in both simulation and real-world outdoor environments. The results
demonstrate superior terrain adaptability, disturbance rejection, and
consistent performance across a wide range of speeds and perceptual conditions.

</details>


### [6] [AEOS: Active Environment-aware Optimal Scanning Control for UAV LiDAR-Inertial Odometry in Complex Scenes](https://arxiv.org/abs/2509.09141)
*Jianping Li,Xinhang Xu,Zhongyuan Liu,Shenghai Yuan,Muqing Cao,Lihua Xie*

Main category: cs.RO

TL;DR: AEOS是一个受猫头鹰主动感知启发的自适应LiDAR控制框架，结合MPC和RL，通过混合架构提升无人机LiDAR惯性里程计在复杂环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决无人机LiDAR感知的局限性：紧凑LiDAR的窄视场角限制、多传感器配置的载荷约束，以及传统固定转速扫描系统缺乏场景感知和任务适应性，导致在复杂遮挡环境中里程计和建图性能下降。

Method: 提出AEOS混合架构：结合模型预测控制(MPC)和强化学习(RL)，使用分析不确定性模型预测未来位姿可观测性进行利用，轻量神经网络从全景深度表示学习隐式代价图指导探索。开发点云仿真环境支持可扩展训练和泛化。

Result: 在仿真和真实环境中的大量实验表明，AEOS相比固定速率、仅优化和完全学习基线方法显著提高了里程计精度，同时在机载计算约束下保持实时性能。

Conclusion: AEOS框架通过生物启发的主动感知方法，有效解决了无人机LiDAR感知的固有局限性，为复杂环境中的自适应LiDAR控制提供了计算高效的解决方案。

Abstract: LiDAR-based 3D perception and localization on unmanned aerial vehicles (UAVs)
are fundamentally limited by the narrow field of view (FoV) of compact LiDAR
sensors and the payload constraints that preclude multi-sensor configurations.
Traditional motorized scanning systems with fixed-speed rotations lack scene
awareness and task-level adaptability, leading to degraded odometry and mapping
performance in complex, occluded environments. Inspired by the active sensing
behavior of owls, we propose AEOS (Active Environment-aware Optimal Scanning),
a biologically inspired and computationally efficient framework for adaptive
LiDAR control in UAV-based LiDAR-Inertial Odometry (LIO). AEOS combines model
predictive control (MPC) and reinforcement learning (RL) in a hybrid
architecture: an analytical uncertainty model predicts future pose
observability for exploitation, while a lightweight neural network learns an
implicit cost map from panoramic depth representations to guide exploration. To
support scalable training and generalization, we develop a point cloud-based
simulation environment with real-world LiDAR maps across diverse scenes,
enabling sim-to-real transfer. Extensive experiments in both simulation and
real-world environments demonstrate that AEOS significantly improves odometry
accuracy compared to fixed-rate, optimization-only, and fully learned
baselines, while maintaining real-time performance under onboard computational
constraints. The project page can be found at
https://kafeiyin00.github.io/AEOS/.

</details>


### [7] [Occupancy-aware Trajectory Planning for Autonomous Valet Parking in Uncertain Dynamic Environments](https://arxiv.org/abs/2509.09206)
*Farhad Nawaz,Faizan M. Tariq,Sangjae Bae,David Isele,Avinash Singh,Nadia Figueroa,Nikolai Matni,Jovin D'sa*

Main category: cs.RO

TL;DR: 基于动态预测和信息收益的自主代客停车案例，通过区分空闲/占用停车位以及预测动态物体运动，提高停车效率和安全性


<details>
  <summary>Details</summary>
Motivation: 解决现有自主代客停车方法仅依赖即时观测或静态假设的问题，需要在动态不确定环境中进行更准确的停车位可用性预测和综合规划

Method: 提出概率性停车位占用估计器，结合有限视野模型中的部分和噪声观测，考虑未观测区域的不确定性；设计策略规划器，基于信息收益动态平衡目标导向的停车操作和探索导航，并在有前景停车位采用等待-前进行为

Result: 通过模拟大型停车场环境进行随机模拟，证明该框架在停车效率、安全距离和轨迹平滑性方面显著优于现有方法

Conclusion: 该方法通过系统性的预测、不确定性管理和自适应性规划，为自主代客停车提供了更加安全高效的解决方案

Abstract: Accurately reasoning about future parking spot availability and integrated
planning is critical for enabling safe and efficient autonomous valet parking
in dynamic, uncertain environments. Unlike existing methods that rely solely on
instantaneous observations or static assumptions, we present an approach that
predicts future parking spot occupancy by explicitly distinguishing between
initially vacant and occupied spots, and by leveraging the predicted motion of
dynamic agents. We introduce a probabilistic spot occupancy estimator that
incorporates partial and noisy observations within a limited Field-of-View
(FoV) model and accounts for the evolving uncertainty of unobserved regions.
Coupled with this, we design a strategy planner that adaptively balances
goal-directed parking maneuvers with exploratory navigation based on
information gain, and intelligently incorporates wait-and-go behaviors at
promising spots. Through randomized simulations emulating large parking lots,
we demonstrate that our framework significantly improves parking efficiency,
safety margins, and trajectory smoothness compared to existing approaches.

</details>


### [8] [RENet: Fault-Tolerant Motion Control for Quadruped Robots via Redundant Estimator Networks under Visual Collapse](https://arxiv.org/abs/2509.09283)
*Yueqi Zhang,Quancheng Qian,Taixian Hou,Peng Zhai,Xiaoyi Wei,Kangmai Hu,Jiafu Yi,Lihua Zhang*

Main category: cs.RO

TL;DR: 提出RENet框架，通过双估计器架构解决四足机器人在户外环境中视觉定位的挑战，特别是在视觉感知退化时保持稳定运动性能


<details>
  <summary>Details</summary>
Motivation: 户外环境中基于视觉的 locomotion 面临环境预测不准确和深度传感器噪声处理困难的问题，严重限制了算法在户外的实际应用

Method: 采用冗余估计器网络(RENet)框架，包含双估计器架构，通过在线估计器自适应实现在视觉感知不确定性时的无缝切换

Result: 在真实机器人上的实验验证表明，该框架在复杂户外环境中有效，特别是在视觉感知退化场景中表现出优势

Conclusion: 该框架展示了在挑战性野外条件下实现可靠机器人部署的潜力，是解决视觉运动控制部署挑战的实用方案

Abstract: Vision-based locomotion in outdoor environments presents significant
challenges for quadruped robots. Accurate environmental prediction and
effective handling of depth sensor noise during real-world deployment remain
difficult, severely restricting the outdoor applications of such algorithms. To
address these deployment challenges in vision-based motion control, this letter
proposes the Redundant Estimator Network (RENet) framework. The framework
employs a dual-estimator architecture that ensures robust motion performance
while maintaining deployment stability during onboard vision failures. Through
an online estimator adaptation, our method enables seamless transitions between
estimation modules when handling visual perception uncertainties. Experimental
validation on a real-world robot demonstrates the framework's effectiveness in
complex outdoor environments, showing particular advantages in scenarios with
degraded visual perception. This framework demonstrates its potential as a
practical solution for reliable robotic deployment in challenging field
conditions. Project website: https://RENet-Loco.github.io/

</details>


### [9] [OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning](https://arxiv.org/abs/2509.09332)
*Yuecheng Liu,Dafeng Chi,Shiguang Wu,Zhanguang Zhang,Yuzheng Zhuang,Bowen Yang,He Zhu,Lingfeng Zhang,Pengwei Xie,David Gamaliel Arcos Bravo,Yingxue Zhang,Jianye Hao,Xingyue Quan*

Main category: cs.RO

TL;DR: OmniEVA是一个多模态大语言模型驱动的具身智能规划器，通过任务自适应3D接地机制和具身感知推理框架，解决了现有方法在几何适应性和物理约束方面的局限性，实现了先进的具身推理和任务规划。


<details>
  <summary>Details</summary>
Motivation: 当前基于MLLM的具身系统面临两个关键限制：几何适应性差距（2D输入或硬编码3D几何注入导致空间信息不足或泛化受限）和具身约束差距（忽视真实机器人的物理约束，导致计划理论上有效但实际不可行）。

Method: 提出两个关键创新：(1)任务自适应3D接地机制：使用门控路由器根据上下文需求进行选择性3D融合调节，实现上下文感知的3D接地；(2)具身感知推理框架：将任务目标和具身约束共同纳入推理循环，产生既目标导向又可执行的规划决策。

Result: 广泛的实验结果表明，OmniEVA不仅实现了最先进的通用具身推理性能，还在广泛的下游场景中表现出强大能力。在一系列提出的具身基准测试（包括原始和复合任务）中验证了其稳健且多功能的规划能力。

Conclusion: OmniEVA通过创新的3D接地和具身感知推理方法，有效解决了现有MLLM具身系统的局限性，为具身智能提供了更强大和实用的规划解决方案。

Abstract: Recent advances in multimodal large language models (MLLMs) have opened new
opportunities for embodied intelligence, enabling multimodal understanding,
reasoning, and interaction, as well as continuous spatial decision-making.
Nevertheless, current MLLM-based embodied systems face two critical
limitations. First, Geometric Adaptability Gap: models trained solely on 2D
inputs or with hard-coded 3D geometry injection suffer from either insufficient
spatial information or restricted 2D generalization, leading to poor
adaptability across tasks with diverse spatial demands. Second, Embodiment
Constraint Gap: prior work often neglects the physical constraints and
capacities of real robots, resulting in task plans that are theoretically valid
but practically infeasible.To address these gaps, we introduce OmniEVA -- an
embodied versatile planner that enables advanced embodied reasoning and task
planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding
mechanism, which introduces a gated router to perform explicit selective
regulation of 3D fusion based on contextual requirements, enabling
context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware
Reasoning framework that jointly incorporates task goals and embodiment
constraints into the reasoning loop, resulting in planning decisions that are
both goal-directed and executable. Extensive experimental results demonstrate
that OmniEVA not only achieves state-of-the-art general embodied reasoning
performance, but also exhibits a strong ability across a wide range of
downstream scenarios. Evaluations of a suite of proposed embodied benchmarks,
including both primitive and composite tasks, confirm its robust and versatile
planning capabilities. Project page: https://omnieva.github.io

</details>


### [10] [AGILOped: Agile Open-Source Humanoid Robot for Research](https://arxiv.org/abs/2509.09364)
*Grzegorz Ficht,Luis Denninger,Sven Behnke*

Main category: cs.RO

TL;DR: AGILOped是一个开源人形机器人，填补了高性能与可访问性之间的空白，使用现成的驱动器和标准电子元件，单人即可操作，在行走、跳跃、冲击缓解和起身等实验中表现出色


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人平台多为闭源或成本高昂，需要开发一个既高性能又易于获取的开源机器人平台

Method: 使用现成的可反向驱动驱动器和高功率密度标准电子元件，设计高度110cm、重量14.5kg的轻量化结构，无需龙门架单人即可操作

Result: 实验证明AGILOped在行走、跳跃、冲击缓解和起身等任务中表现良好，适合研究使用

Conclusion: AGILOped成功实现了高性能与可访问性的平衡，为研究社区提供了一个实用的开源人形机器人平台

Abstract: With academic and commercial interest for humanoid robots peaking, multiple
platforms are being developed. Through a high level of customization, they
showcase impressive performance. Most of these systems remain closed-source or
have high acquisition and maintenance costs, however. In this work, we present
AGILOped - an open-source humanoid robot that closes the gap between high
performance and accessibility. Our robot is driven by off-the-shelf
backdrivable actuators with high power density and uses standard electronic
components. With a height of 110 cm and weighing only 14.5 kg, AGILOped can be
operated without a gantry by a single person. Experiments in walking, jumping,
impact mitigation and getting-up demonstrate its viability for use in research.

</details>


### [11] [VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model](https://arxiv.org/abs/2509.09372)
*Yihao Wang,Pengxiang Ding,Lingxiao Li,Can Cui,Zirui Ge,Xinyang Tong,Wenxuan Song,Han Zhao,Wei Zhao,Pengxu Hou,Siteng Huang,Yifan Tang,Wenhui Wang,Ru Zhang,Jianyi Liu,Donglin Wang*

Main category: cs.RO

TL;DR: VLA-Adapter是一个轻量级适配器，通过桥接注意力机制将视觉语言表示连接到动作空间，无需大规模VLM预训练和机器人数据，仅用0.5B参数即可达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 传统VLA模型依赖大规模视觉语言模型预训练，训练成本高昂。本文旨在降低VLA模型对大规模VLM和预训练的依赖，寻找更高效的视觉语言-动作桥接方法

Method: 系统分析不同VL条件有效性，提出轻量级策略模块和桥接注意力机制，自动将最优条件注入动作空间。无需机器人数据预训练，仅使用0.5B参数骨干网络

Result: 在仿真和真实机器人基准测试中达到最先进性能，推理速度最快，单消费级GPU仅需8小时即可训练强大VLA模型

Conclusion: VLA-Adapter提供了一种高效桥接范式，大幅降低了VLA模型的部署门槛，为机器人视觉语言动作建模提供了更实用的解决方案

Abstract: Vision-Language-Action (VLA) models typically bridge the gap between
perceptual and action spaces by pre-training a large-scale Vision-Language
Model (VLM) on robotic data. While this approach greatly enhances performance,
it also incurs significant training costs. In this paper, we investigate how to
effectively bridge vision-language (VL) representations to action (A). We
introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA
models on large-scale VLMs and extensive pre-training. To this end, we first
systematically analyze the effectiveness of various VL conditions and present
key findings on which conditions are essential for bridging perception and
action spaces. Based on these insights, we propose a lightweight Policy module
with Bridge Attention, which autonomously injects the optimal condition into
the action space. In this way, our method achieves high performance using only
a 0.5B-parameter backbone, without any robotic data pre-training. Extensive
experiments on both simulated and real-world robotic benchmarks demonstrate
that VLA-Adapter not only achieves state-of-the-art level performance, but also
offers the fast inference speed reported to date. Furthermore, thanks to the
proposed advanced bridging paradigm, VLA-Adapter enables the training of a
powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly
lowering the barrier to deploying the VLA model. Project page:
https://vla-adapter.github.io/.

</details>


### [12] [A Hybrid Hinge-Beam Continuum Robot with Passive Safety Capping for Real-Time Fatigue Awareness](https://arxiv.org/abs/2509.09404)
*Tongshun Chen,Zezhou Sun,Yanhan Sun,Yuhao Wang,Dezhen Song,Ke Wu*

Main category: cs.RO

TL;DR: 这篇论文提出了一种疲劳感知连续体机器人新架构，通过混合轴提梁结构、被动限位器和实时疲劳估计方法，将疲劳积累减少49%，实现了安全可靠的长期操作。


<details>
  <summary>Details</summary>
Motivation: 线驱动连续体机器人在长期使用中容易发生机械疲劳和材料降级，影响性能并带来结构失效风险，而现有技术在疲劳估计方面研究不涵。

Method: 提出三项核心创新：(1)混合轴提梁结构，通过TwistBeam和BendBeam解耦扭曲和弯曲；(2)被动限位器，通过机械约束安全限制运动；(3)实时疲劳感知方法，从极限位置的电机扭矩估计刚度。

Result: 实验结果显示，该设计与传统设计相比将疲劳积累减少49%，被动机械限制结合电机侦测允许准确估计结构疲劳和损坏。

Conclusion: 证实了所提出架构在实现安全可靠的长期操作方面的有效性。

Abstract: Cable-driven continuum robots offer high flexibility and lightweight design,
making them well-suited for tasks in constrained and unstructured environments.
However, prolonged use can induce mechanical fatigue from plastic deformation
and material degradation, compromising performance and risking structural
failure. In the state of the art, fatigue estimation of continuum robots
remains underexplored, limiting long-term operation. To address this, we
propose a fatigue-aware continuum robot with three key innovations: (1) a
Hybrid Hinge-Beam structure where TwistBeam and BendBeam decouple torsion and
bending: passive revolute joints in the BendBeam mitigate stress concentration,
while TwistBeam's limited torsional deformation reduces BendBeam stress
magnitude, enhancing durability; (2) a Passive Stopper that safely constrains
motion via mechanical constraints and employs motor torque sensing to detect
corresponding limit torque, ensuring safety and enabling data collection; and
(3) a real-time fatigue-awareness method that estimates stiffness from motor
torque at the limit pose, enabling online fatigue estimation without additional
sensors. Experiments show that the proposed design reduces fatigue accumulation
by about 49% compared with a conventional design, while passive mechanical
limiting combined with motor-side sensing allows accurate estimation of
structural fatigue and damage. These results confirm the effectiveness of the
proposed architecture for safe and reliable long-term operation.

</details>


### [13] [BagIt! An Adaptive Dual-Arm Manipulation of Fabric Bags for Object Bagging](https://arxiv.org/abs/2509.09484)
*Peng Zhou,Jiaming Qi,Hongmin Wu,Chen Wang,Yizhou Chen,Zeqing Zhang*

Main category: cs.RO

TL;DR: 基于双机器臂的自适应结构关注点操纵策略的自动装袋系统，通过实时视觉反馈动态调整动作，无需预先知道袋子属性


<details>
  <summary>Details</summary>
Motivation: 解决工业场景中变形袋子装袋任务的挑战，因为可变形袋子具有复杂和不可预测的特性

Method: 使用高斯混合模型(GMM)估计结构关注点状态，优化技术生成SOI，通过约束双向快速随机树(CBiRRT)进行运动规划，采用模型预测控制(MPC)实现双臂协调

Result: 经过大量实验验证，系统能够在各种物体上执行精确和稳健的装袋任务，显示了其适应能力

Conclusion: 这项工作为机器人可变形物体操纵特别是自动装袋任务提供了新的解决方案

Abstract: Bagging tasks, commonly found in industrial scenarios, are challenging
considering deformable bags' complicated and unpredictable nature. This paper
presents an automated bagging system from the proposed adaptive
Structure-of-Interest (SOI) manipulation strategy for dual robot arms. The
system dynamically adjusts its actions based on real-time visual feedback,
removing the need for pre-existing knowledge of bag properties. Our framework
incorporates Gaussian Mixture Models (GMM) for estimating SOI states,
optimization techniques for SOI generation, motion planning via Constrained
Bidirectional Rapidly-exploring Random Tree (CBiRRT), and dual-arm coordination
using Model Predictive Control (MPC). Extensive experiments validate the
capability of our system to perform precise and robust bagging across various
objects, showcasing its adaptability. This work offers a new solution for
robotic deformable object manipulation (DOM), particularly in automated bagging
tasks. Video of this work is available at https://youtu.be/6JWjCOeTGiQ.

</details>


### [14] [SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking](https://arxiv.org/abs/2509.09509)
*Pedro Miguel Bastos Soares,Ali Tourani,Miguel Fernandez-Cortizas,Asier Bikandi Noya,Jose Luis Sanchez-Lopez,Holger Voos*

Main category: cs.RO

TL;DR: SMapper是一个开源的硬件平台，专为SLAM研究设计，集成了LiDAR、多摄像头和惯性传感器，提供精确的时空同步。同时发布了SMapper-light数据集，包含室内外序列和亚厘米级精度的地面真实轨迹，用于SLAM算法的基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前SLAM和自主导航研究缺乏可靠、可复现的多模态数据集，现有数据集在传感模态、环境多样性和硬件设置可复现性方面存在局限。

Method: 开发了开源的SMapper硬件平台，集成同步的LiDAR、多摄像头和惯性传感，配备强大的校准和同步流水线。发布了SMapper-light数据集，包含同步多模态数据和基于LiDAR SLAM的高精度地面真实轨迹。

Result: 建立了完整的开源硬件平台和数据集，提供了精确的时空对齐多模态数据，支持手持和机器人安装场景的实验复现。

Conclusion: SMapper通过开源硬件设计、可复现数据收集和全面基准测试，为SLAM算法开发、评估和可复现性奠定了坚实基础。

Abstract: Advancing research in fields like Simultaneous Localization and Mapping
(SLAM) and autonomous navigation critically depends on reliable and
reproducible multimodal datasets. While several influential datasets have
driven progress in these domains, they often suffer from limitations in sensing
modalities, environmental diversity, and the reproducibility of the underlying
hardware setups. To address these challenges, this paper introduces SMapper, a
novel open-hardware, multi-sensor platform designed explicitly for, though not
limited to, SLAM research. The device integrates synchronized LiDAR,
multi-camera, and inertial sensing, supported by a robust calibration and
synchronization pipeline that ensures precise spatio-temporal alignment across
modalities. Its open and replicable design allows researchers to extend its
capabilities and reproduce experiments across both handheld and robot-mounted
scenarios. To demonstrate its practicality, we additionally release
SMapper-light, a publicly available SLAM dataset containing representative
indoor and outdoor sequences. The dataset includes tightly synchronized
multimodal data and ground-truth trajectories derived from offline LiDAR-based
SLAM with sub-centimeter accuracy, alongside dense 3D reconstructions.
Furthermore, the paper contains benchmarking results on state-of-the-art LiDAR
and visual SLAM frameworks using the SMapper-light dataset. By combining
open-hardware design, reproducible data collection, and comprehensive
benchmarking, SMapper establishes a robust foundation for advancing SLAM
algorithm development, evaluation, and reproducibility.

</details>


### [15] [A Neuromorphic Incipient Slip Detection System using Papillae Morphology](https://arxiv.org/abs/2509.09546)
*Yanhui Lu,Zeyu Deng,Stephen J. Redmond,Efi Psomopoulou,Benjamin Ward-Cherrier*

Main category: cs.RO

TL;DR: 这篇论文提出了一种基于神经元感知的触觉感知系统，通过刷新卷积神经网络实现了高精度的滑动状态分类，能够在360毫秒前预测偏移滑动，提高机器人操作安全性。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署滑动检测系统遇到能源约束挑战，需要一种能够及早检测偏移滑动的高效解决方案来预阻物体滑落和提升机器人操作安全性。

Method: 采用NeuroTac感器配备突出乳头皮肤结构，构建基于刷新卷积神经网络(SCNN)的神经元触觉感知系统，用于滑动状态分类。通过对SCNN最后层刷新计数进行时间平滑处理，实现偏移滑动检测。

Result: SCNN模型在三种滑动状态(无滑动、偏移滑动、完全滑动)分类中达刱94.33%的准确率。在动态重力引起的滑动验证条件下，系统能够在所有试验中至少360毫秒前检测到偏移滑动，一致性地在完全滑动发生前识别偏移滑动。

Conclusion: 这种神经元感知系统具有稳定和响应快速的偏移滑动检测能力，为机器人安全操作提供了有效的解决方案。

Abstract: Detecting incipient slip enables early intervention to prevent object
slippage and enhance robotic manipulation safety. However, deploying such
systems on edge platforms remains challenging, particularly due to energy
constraints. This work presents a neuromorphic tactile sensing system based on
the NeuroTac sensor with an extruding papillae-based skin and a spiking
convolutional neural network (SCNN) for slip-state classification. The SCNN
model achieves 94.33% classification accuracy across three classes (no slip,
incipient slip, and gross slip) in slip conditions induced by sensor motion.
Under the dynamic gravity-induced slip validation conditions, after temporal
smoothing of the SCNN's final-layer spike counts, the system detects incipient
slip at least 360 ms prior to gross slip across all trials, consistently
identifying incipient slip before gross slip occurs. These results demonstrate
that this neuromorphic system has stable and responsive incipient slip
detection capability.

</details>


### [16] [ObjectReact: Learning Object-Relative Control for Visual Navigation](https://arxiv.org/abs/2509.09594)
*Sourav Garg,Dustin Craggs,Vineeth Bhat,Lachlan Mares,Stefan Podgorski,Madhava Krishna,Feras Dayoub,Ian Reid*

Main category: cs.RO

TL;DR: 这篇论文提出了一种基于对象相对控制的新视觉导航方法ObjectReact，通过构建相对3D场景图来实现更好的导航性能和跨体验逆向性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于图像相对控制的方法存在限制，图像表征与代理体姿态绑定，而对象表征具有更好的体验和轨迹不变性。

Method: 提出使用相对3D场景图的顶拓扑地图表征，基于高级别的"WayObject Costmap"表征训练本地控制器ObjectReact，避免显式RGB输入。

Result: 在传感器高度变化和多种导航任务中，对象相对控制方法比图像相对方法表现更好，能够在反向导航等挑战性任务中表现优异。仅使用模拟训练的策略能够艾好地渐迁到真实室内环境。

Conclusion: 对象相对控制为单相机导航提供了一种更加稳健和可渐迁的方案，具有更好的跨体验逆向性和空间理解能力。

Abstract: Visual navigation using only a single camera and a topological map has
recently become an appealing alternative to methods that require additional
sensors and 3D maps. This is typically achieved through an "image-relative"
approach to estimating control from a given pair of current observation and
subgoal image. However, image-level representations of the world have
limitations because images are strictly tied to the agent's pose and
embodiment. In contrast, objects, being a property of the map, offer an
embodiment- and trajectory-invariant world representation. In this work, we
present a new paradigm of learning "object-relative" control that exhibits
several desirable characteristics: a) new routes can be traversed without
strictly requiring to imitate prior experience, b) the control prediction
problem can be decoupled from solving the image matching problem, and c) high
invariance can be achieved in cross-embodiment deployment for variations across
both training-testing and mapping-execution settings. We propose a topometric
map representation in the form of a "relative" 3D scene graph, which is used to
obtain more informative object-level global path planning costs. We train a
local controller, dubbed "ObjectReact", conditioned directly on a high-level
"WayObject Costmap" representation that eliminates the need for an explicit RGB
input. We demonstrate the advantages of learning object-relative control over
its image-relative counterpart across sensor height variations and multiple
navigation tasks that challenge the underlying spatial understanding
capability, e.g., navigating a map trajectory in the reverse direction. We
further show that our sim-only policy is able to generalize well to real-world
indoor environments. Code and supplementary material are accessible via project
page: https://object-react.github.io/

</details>


### [17] [MOFU: Development of a MOrphing Fluffy Unit with Expansion and Contraction Capabilities and Evaluation of the Animacy of Its Movements](https://arxiv.org/abs/2509.09613)
*Taisei Mogi,Mari Saito,Yoshihiro Nakata*

Main category: cs.RO

TL;DR: 开发了能够全身伸缩的毛绒机器人MOFU，研究发现伸缩运动能显著提升机器人感知活力，是未来机器人设计的重要元素


<details>
  <summary>Details</summary>
Motivation: 现有机器人主要模仿外观和关节运动，忽视了生物体中观察到的全身伸缩体积变化运动及其对活力感知的影响

Method: 开发MOFU机器人（单电机驱动Jitterbug结构实现210-280mm直径变化），通过在线视频调查使用Godspeed问卷评估伸缩运动对活力感知的影响

Result: 1) 伸缩运动显著增加感知活力；2) 双机器人未比单机器人显著提升活力；3) 伸缩+运动比单纯运动获得更高活力评分

Conclusion: 体积变化运动如伸缩能增强机器人感知活力，应作为塑造人类印象的重要设计元素

Abstract: Robots for therapy and social interaction are often intended to evoke
"animacy" in humans. While many robots imitate appearance and joint movements,
little attention has been given to whole-body expansion-contraction,
volume-changing movements observed in living organisms, and their effect on
animacy perception. We developed a mobile robot called "MOFU (Morphing Fluffy
Unit)," capable of whole-body expansion-contraction with a single motor and
covered with a fluffy exterior. MOFU employs a "Jitterbug" structure, a
geometric transformation mechanism that enables smooth volume change in
diameter from 210 to 280 mm using one actuator. It is also equipped with a
differential two-wheel drive mechanism for locomotion. To evaluate the effect
of expansion-contraction movements, we conducted an online survey using videos
of MOFU's behavior. Participants rated impressions with the Godspeed
Questionnaire Series. First, we compared videos of MOFU in a stationary state
with and without expansion-contraction and turning, finding that
expansion-contraction significantly increased perceived animacy. Second, we
hypothesized that presenting two MOFUs would increase animacy compared with a
single robot; however, this was not supported, as no significant difference
emerged. Exploratory analyses further compared four dual-robot motion
conditions. Third, when expansion-contraction was combined with locomotion,
animacy ratings were higher than locomotion alone. These results suggest that
volume-changing movements such as expansion and contraction enhance perceived
animacy in robots and should be considered an important design element in
future robot development aimed at shaping human impressions.

</details>


### [18] [Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference-Scoped Exploration](https://arxiv.org/abs/2509.09671)
*Sirui Xu,Yu-Wei Chao,Liuyu Bian,Arsalan Mousavian,Yu-Xiong Wang,Liang-Yan Gui,Wei Yang*

Main category: cs.RO

TL;DR: Dexplore是一个统一的单循环优化框架，直接从运动捕捉数据中学习机器人控制策略，避免了传统三阶段方法中的误差累积问题


<details>
  <summary>Details</summary>
Motivation: 现有的手-物体运动捕捉数据存在不准确性和人机手之间的本体差异，传统三阶段方法（重定向、跟踪、残差校正）导致演示数据利用不足和误差累积

Method: 采用统一的单循环优化，联合执行重定向和跟踪，将演示作为软指导而非绝对真值，从原始轨迹推导自适应空间范围，使用强化学习训练策略保持在范围内同时最小化控制努力并完成任务

Result: 该统一框架保留了演示意图，使机器人特定策略得以出现，提高了对噪声的鲁棒性，并能扩展到大型演示语料库

Conclusion: Dexplore作为一个原则性桥梁，将不完美的演示转化为灵巧操作的有效训练信号，并通过蒸馏得到支持跨对象泛化和真实世界部署的视觉生成控制器

Abstract: Hand-object motion-capture (MoCap) repositories offer large-scale,
contact-rich demonstrations and hold promise for scaling dexterous robotic
manipulation. Yet demonstration inaccuracies and embodiment gaps between human
and robot hands limit the straightforward use of these data. Existing methods
adopt a three-stage workflow, including retargeting, tracking, and residual
correction, which often leaves demonstrations underused and compound errors
across stages. We introduce Dexplore, a unified single-loop optimization that
jointly performs retargeting and tracking to learn robot control policies
directly from MoCap at scale. Rather than treating demonstrations as ground
truth, we use them as soft guidance. From raw trajectories, we derive adaptive
spatial scopes, and train with reinforcement learning to keep the policy
in-scope while minimizing control effort and accomplishing the task. This
unified formulation preserves demonstration intent, enables robot-specific
strategies to emerge, improves robustness to noise, and scales to large
demonstration corpora. We distill the scaled tracking policy into a
vision-based, skill-conditioned generative controller that encodes diverse
manipulation skills in a rich latent representation, supporting generalization
across objects and real-world deployment. Taken together, these contributions
position Dexplore as a principled bridge that transforms imperfect
demonstrations into effective training signals for dexterous manipulation.

</details>


### [19] [SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning](https://arxiv.org/abs/2509.09674)
*Haozhan Li,Yuxin Zuo,Jiale Yu,Yuhao Zhang,Zhaohui Yang,Kaiyan Zhang,Xuekai Zhu,Yuchen Zhang,Tianxing Chen,Ganqu Cui,Dehui Wang,Dingxiang Luo,Yuchen Fan,Youbang Sun,Jia Zeng,Jiangmiao Pang,Shanghang Zhang,Yu Wang,Yao Mu,Bowen Zhou,Ning Ding*

Main category: cs.RO

TL;DR: SimpleVLA-RL是一个针对视觉-语言-动作模型的强化学习框架，通过改进采样、并行化和损失计算，显著提升机器人操作任务的性能，减少对大规模监督数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型面临的两个核心挑战：大规模人类操作轨迹数据稀缺昂贵，以及在分布偏移任务中泛化能力有限的问题。受大型推理模型RL成功启发，探索RL是否能提升VLA的长时程动作规划能力。

Method: 基于veRL框架，引入VLA特定的轨迹采样、可扩展并行化、多环境渲染和优化损失计算。应用于OpenVLA-OFT模型，并引入探索增强策略。

Result: 在LIBERO基准上达到SOTA性能，在RoboTwin 1.0&2.0上甚至超越π_0。减少了对大规模数据的依赖，实现了鲁棒泛化，在真实任务中显著超越监督微调。

Conclusion: SimpleVLA-RL不仅证明了RL可以有效提升VLA模型的性能，还发现了训练过程中的"pushcut"现象，表明策略能够发现训练过程中未见的新模式。

Abstract: Vision-Language-Action (VLA) models have recently emerged as a powerful
paradigm for robotic manipulation. Despite substantial progress enabled by
large-scale pretraining and supervised fine-tuning (SFT), these models face two
fundamental challenges: (i) the scarcity and high cost of large-scale
human-operated robotic trajectories required for SFT scaling, and (ii) limited
generalization to tasks involving distribution shift. Recent breakthroughs in
Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can
dramatically enhance step-by-step reasoning capabilities, raising a natural
question: Can RL similarly improve the long-horizon step-by-step action
planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL
framework tailored for VLA models. Building upon veRL, we introduce
VLA-specific trajectory sampling, scalable parallelization, multi-environment
rendering, and optimized loss computation. When applied to OpenVLA-OFT,
SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\pi_0$
on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce.
SimpleVLA-RL not only reduces dependence on large-scale data and enables robust
generalization, but also remarkably surpasses SFT in real-world tasks.
Moreover, we identify a novel phenomenon ``pushcut'' during RL training,
wherein the policy discovers previously unseen patterns beyond those seen in
the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL

</details>
