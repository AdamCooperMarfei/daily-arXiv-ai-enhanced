<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 28]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Practical and Performant Enhancements for Maximization of Algebraic Connectivity](https://arxiv.org/abs/2511.08694)
*Leonard Jung,Alan Papalia,Kevin Doherty,Michael Everett*

Main category: cs.RO

TL;DR: 本文改进了图稀疏化算法MAC，通过开发专用求解器、优化步长策略和自动连通性保证方案，使其更适合实时估计应用。


<details>
  <summary>Details</summary>
Motivation: 当前图估计方法在大规模长期图上扩展性差，MAC算法虽然能保持估计性能但计算成本高且需要手动指定边集。

Method: 开发代数连通性专用求解器（2倍加速）、研究MAC优化步长策略、提出自动连通性保证方案。

Result: 使MAC算法更可扩展、可靠且适合实时应用。

Conclusion: 这些改进使MAC算法在大规模图估计中更具实用性。

Abstract: Long-term state estimation over graphs remains challenging as current graph estimation methods scale poorly on large, long-term graphs. To address this, our work advances a current state-of-the-art graph sparsification algorithm, maximizing algebraic connectivity (MAC). MAC is a sparsification method that preserves estimation performance by maximizing the algebraic connectivity, a spectral graph property that is directly connected to the estimation error. Unfortunately, MAC remains computationally prohibitive for online use and requires users to manually pre-specify a connectivity-preserving edge set. Our contributions close these gaps along three complementary fronts: we develop a specialized solver for algebraic connectivity that yields an average 2x runtime speedup; we investigate advanced step size strategies for MAC's optimization procedure to enhance both convergence speed and solution quality; and we propose automatic schemes that guarantee graph connectivity without requiring manual specification of edges. Together, these contributions make MAC more scalable, reliable, and suitable for real-time estimation applications.

</details>


### [2] [Intuitive Programming, Adaptive Task Planning, and Dynamic Role Allocation in Human-Robot Collaboration](https://arxiv.org/abs/2511.08732)
*Marta Lagomarsino,Elena Merlo,Andrea Pupa,Timo Birr,Franziska Krebs,Cristian Secchi,Tamim Asfour,Arash Ajoudani*

Main category: cs.RO

TL;DR: 这篇论文综述了人机协作中实现直观信息交换和技能转移的关键组件，包括从多模态输入到机器人理解、自适应规划、角色分配到控制层和反馈机制的完整交互流程。


<details>
  <summary>Details</summary>
Motivation: 当前机器人和AI虽然能处理复杂任务，但人类往往只是被动观察者，而机器人在人类环境中无法充分发挥潜力，需要有效建模人类状态和意图并调整行为。

Method: 通过建立人机双向信息流：人类直观传达指令、分享专业知识和表达需求，同时机器人清晰传达内部状态和即将执行的动作。

Result: 识别并连接了实现人机直观信息交换和技能转移的关键组件，构建了完整的交互流程框架。

Conclusion: 提出了更自适应、可访问的人机协作的发展趋势和前景方向。

Abstract: Remarkable capabilities have been achieved by robotics and AI, mastering complex tasks and environments. Yet, humans often remain passive observers, fascinated but uncertain how to engage. Robots, in turn, cannot reach their full potential in human-populated environments without effectively modeling human states and intentions and adapting their behavior. To achieve a synergistic human-robot collaboration (HRC), a continuous information flow should be established: humans must intuitively communicate instructions, share expertise, and express needs. In parallel, robots must clearly convey their internal state and forthcoming actions to keep users informed, comfortable, and in control. This review identifies and connects key components enabling intuitive information exchange and skill transfer between humans and robots. We examine the full interaction pipeline: from the human-to-robot communication bridge translating multimodal inputs into robot-understandable representations, through adaptive planning and role allocation, to the control layer and feedback mechanisms to close the loop. Finally, we highlight trends and promising directions toward more adaptive, accessible HRC.

</details>


### [3] [ATOM-CBF: Adaptive Safe Perception-Based Control under Out-of-Distribution Measurements](https://arxiv.org/abs/2511.08741)
*Kai S. Yun,Navid Azizan*

Main category: cs.RO

TL;DR: ATOM-CBF是一个新颖的安全控制框架，通过显式计算和适应分布外测量中的认知不确定性来确保系统安全，无需真实标签或分布偏移信息。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统依赖学习感知模块从高维传感器数据推断系统状态，但这些模块容易受到认知不确定性的影响，在遇到训练时未见过的分布外测量时会失效，需要确保系统安全。

Method: 提出ATOM-CBF框架，包含两个关键组件：(1) 分布外感知的自适应感知误差边界；(2) 集成该自适应误差边界的安全滤波器，能够实时调整其保守性。

Result: 在仿真中进行了实证验证，证明ATOM-CBF能够为配备LiDAR扫描的F1Tenth车辆和使用RGB图像的四足机器人维持安全。

Conclusion: ATOM-CBF框架通过显式处理分布外测量中的认知不确定性，为依赖学习感知模块的系统提供了一种有效的安全保障方法。

Abstract: Ensuring the safety of real-world systems is challenging, especially when they rely on learned perception modules to infer the system state from high-dimensional sensor data. These perception modules are vulnerable to epistemic uncertainty, often failing when encountering out-of-distribution (OoD) measurements not seen during training. To address this gap, we introduce ATOM-CBF (Adaptive-To-OoD-Measurement Control Barrier Function), a novel safe control framework that explicitly computes and adapts to the epistemic uncertainty from OoD measurements, without the need for ground-truth labels or information on distribution shifts. Our approach features two key components: (1) an OoD-aware adaptive perception error margin and (2) a safety filter that integrates this adaptive error margin, enabling the filter to adjust its conservatism in real-time. We provide empirical validation in simulations, demonstrating that ATOM-CBF maintains safety for an F1Tenth vehicle with LiDAR scans and a quadruped robot with RGB images.

</details>


### [4] [CENIC: Convex Error-controlled Numerical Integration for Contact](https://arxiv.org/abs/2511.08771)
*Vince Kurtz,Alejandro Castro*

Main category: cs.RO

TL;DR: CENIC是一个新的连续时间积分器，结合了凸时间步进和误差控制积分的最新进展，解决了离散时间模拟器需要选择时间步长的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有机器人模拟器在离散时间下运行，需要选择时间步长：大步长会产生非物理伪影，小步长则运行缓慢。误差控制积分可以自动调整时间步长，但现有方法难以处理接触的刚性动力学，无法满足现代机器人工作流程的速度和可扩展性要求。

Method: CENIC结合了凸时间步进和误差控制积分的最新进展，继承了连续积分和离散时间步进的优点。

Result: CENIC以与MuJoCo、Drake和Isaac Sim等离散时间机器人模拟器相当的快速实时速率运行，同时提供精度和收敛性保证。

Conclusion: CENIC成功解决了离散时间模拟器的时间步长选择问题，在保持高速运行的同时提供了连续时间积分器的精度优势。

Abstract: State-of-the-art robotics simulators operate in discrete time. This requires users to choose a time step, which is both critical and challenging: large steps can produce non-physical artifacts, while small steps force the simulation to run slowly. Continuous-time error-controlled integration avoids such issues by automatically adjusting the time step to achieve a desired accuracy. But existing error-controlled integrators struggle with the stiff dynamics of contact, and cannot meet the speed and scalability requirements of modern robotics workflows. We introduce CENIC, a new continuous-time integrator that brings together recent advances in convex time-stepping and error-controlled integration, inheriting benefits from both continuous integration and discrete time-stepping. CENIC runs at fast real-time rates comparable to discrete-time robotics simulators like MuJoCo, Drake and Isaac Sim, while also providing guarantees on accuracy and convergence.

</details>


### [5] [Dual-Arm Whole-Body Motion Planning: Leveraging Overlapping Kinematic Chains](https://arxiv.org/abs/2511.08778)
*Richard Cheng,Peter Werner,Carolyn Matl*

Main category: cs.RO

TL;DR: 提出了一种针对高自由度双臂机器人的实时运动规划方法，通过利用共享关节结构构建动态路图，有效缓解维度灾难问题。


<details>
  <summary>Details</summary>
Motivation: 高自由度双臂机器人在未知动态环境中的实时运动规划面临维度灾难和复杂避障约束的挑战。

Method: 为每个运动链（左臂+躯干、右臂+躯干）构建具有共享关节结构的动态路图，并利用这种结构高效搜索两个路图的组合。

Result: 在真实超市环境中，19自由度移动操作机器人执行杂货配送任务，平均规划时间0.4秒，成功率99.9%，超过2000次运动规划。

Conclusion: 该方法通过利用共享关节结构有效缓解了高维运动规划的维度灾难，实现了双臂机器人在动态环境中的实时可靠规划。

Abstract: High degree-of-freedom dual-arm robots are becoming increasingly common due to their morphology enabling them to operate effectively in human environments. However, motion planning in real-time within unknown, changing environments remains a challenge for such robots due to the high dimensionality of the configuration space and the complex collision-avoidance constraints that must be obeyed. In this work, we propose a novel way to alleviate the curse of dimensionality by leveraging the structure imposed by shared joints (e.g. torso joints) in a dual-arm robot. First, we build two dynamic roadmaps (DRM) for each kinematic chain (i.e. left arm + torso, right arm + torso) with specific structure induced by the shared joints. Then, we show that we can leverage this structure to efficiently search through the composition of the two roadmaps and largely sidestep the curse of dimensionality. Finally, we run several experiments in a real-world grocery store with this motion planner on a 19 DoF mobile manipulation robot executing a grocery fulfillment task, achieving 0.4s average planning times with 99.9% success rate across more than 2000 motion plans.

</details>


### [6] [Low-cost Multi-agent Fleet for Acoustic Cooperative Localization Research](https://arxiv.org/abs/2511.08822)
*Nelson Durrant,Braden Meyers,Matthew McMurray,Clayton Smith,Brighton Anderson,Tristan Hodgins,Kalliyan Velasco,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: 开发低成本、可配置的水下自主机器人平台CoUGARs，用于多智能体自主性研究，成本低于3000美元，支持声学协作定位研究。


<details>
  <summary>Details</summary>
Motivation: 真实水下多智能体自主性测试面临高昂成本和工程挑战，需要开发低成本、可配置的解决方案。

Method: 基于商用和3D打印部件构建低成本AUV平台，配备DVL和USBL声学阵列，开发容器化软件栈并与HoloOcean模拟器集成。

Result: 成功开发出成本低于3000美元的CoUGARs平台，在模拟和犹他州湖泊实地测试中验证了系统性能。

Conclusion: CoUGARs为水下多智能体自主性研究提供了经济高效且可配置的平台解决方案。

Abstract: Real-world underwater testing for multi-agent autonomy presents substantial financial and engineering challenges. In this work, we introduce the Configurable Underwater Group of Autonomous Robots (CoUGARs) as a low-cost, configurable autonomous-underwater-vehicle (AUV) platform for multi-agent autonomy research. The base design costs less than $3,000 USD (as of May 2025) and is based on commercially-available and 3D-printed parts, enabling quick customization for various sensor payloads and configurations. Our current expanded model is equipped with a doppler velocity log (DVL) and ultra-short-baseline (USBL) acoustic array/transducer to support research on acoustic-based cooperative localization. State estimation, navigation, and acoustic communications software has been developed and deployed using a containerized software stack and is tightly integrated with the HoloOcean simulator. The system was tested both in simulation and via in-situ field trials in Utah lakes and reservoirs.

</details>


### [7] [XPRESS: X-Band Radar Place Recognition via Elliptical Scan Shaping](https://arxiv.org/abs/2511.08863)
*Hyesu Jang,Wooseong Yang,Ayoung Kim,Dongje Lee,Hanguen Kim*

Main category: cs.RO

TL;DR: 提出了一种专门针对X波段雷达的场所识别算法，用于实现海上环境中的X波段雷达自主导航


<details>
  <summary>Details</summary>
Motivation: X波段雷达作为船舶主要传感器，在自主导航中应用受限，主要原因是传感器分辨率低和信息内容不足

Method: 采用基于物体密度的规则进行高效候选选择，并通过有意降低雷达检测质量来实现鲁棒的检索性能

Result: 在公共海事雷达数据集和自收集数据集上进行了评估，并与最先进的雷达场所识别方法进行了性能比较

Conclusion: 进行了消融研究以评估算法对关键参数的敏感性，证明了所提算法的有效性

Abstract: X-band radar serves as the primary sensor on maritime vessels, however, its application in autonomous navigation has been limited due to low sensor resolution and insufficient information content. To enable X-band radar-only autonomous navigation in maritime environments, this paper proposes a place recognition algorithm specifically tailored for X-band radar, incorporating an object density-based rule for efficient candidate selection and intentional degradation of radar detections to achieve robust retrieval performance. The proposed algorithm was evaluated on both public maritime radar datasets and our own collected dataset, and its performance was compared against state-of-the-art radar place recognition methods. An ablation study was conducted to assess the algorithm's performance sensitivity with respect to key parameters.

</details>


### [8] [MirrorLimb: Implementing hand pose acquisition and robot teleoperation based on RealMirror](https://arxiv.org/abs/2511.08865)
*Cong Tai,Hansheng Wu,Haixu Long,Zhengbin Long,Zhaoyu Zheng,Haodong Xiang,Tao Shen*

Main category: cs.RO

TL;DR: 提出了一种基于PICO的机器人远程操作框架，能够低成本实时获取手部运动和姿态数据，在成本效益上优于主流视觉跟踪和运动捕捉解决方案。


<details>
  <summary>Details</summary>
Motivation: 降低上肢机器人操作研究的技术门槛，加速视觉-语言-动作（VLA）相关研究的进展。

Method: 开发了与RealMirror生态系统原生兼容的PICO机器人远程操作框架，在Isaac仿真环境中提供稳定精确的机器人轨迹记录功能。

Result: 实现了低成本实时手部运动数据采集，支持多种末端执行器（灵巧手和机器人夹爪）的实时遥操作。

Conclusion: 该框架为构建VLA数据集提供了即用型解决方案，在成本效益方面优于现有主流方案。

Abstract: In this work, we present a PICO-based robot remote operating framework that enables low-cost, real-time acquisition of hand motion and pose data, outperforming mainstream visual tracking and motion capture solutions in terms of cost-effectiveness. The framework is natively compatible with the RealMirror ecosystem, offering ready-to-use functionality for stable and precise robotic trajectory recording within the Isaac simulation environment, thereby facilitating the construction of Vision-Language-Action (VLA) datasets. Additionally, the system supports real-time teleoperation of a variety of end-effector-equipped robots, including dexterous hands and robotic grippers. This work aims to lower the technical barriers in the study of upper-limb robotic manipulation, thereby accelerating advancements in VLA-related research.

</details>


### [9] [A Shared Control Framework for Mobile Robots with Planning-Level Intention Prediction](https://arxiv.org/abs/2511.08912)
*Jinyu Zhang,Lijun Han,Feng Jian,Lingxi Zhang,Hesheng Wang*

Main category: cs.RO

TL;DR: 提出了一种基于规划级意图预测的移动机器人共享控制框架，通过深度强化学习联合解决意图域预测和路径重规划问题，显著降低操作员工作负荷并提高安全性。


<details>
  <summary>Details</summary>
Motivation: 在移动机器人共享控制中，有效理解人类运动意图对于实现无缝人机协作至关重要。现有方法在意图理解和轨迹调整方面存在局限性。

Method: 引入意图域概念表示未来运动意图，将意图域预测和路径重规划问题建模为马尔可夫决策过程，通过深度强化学习求解；开发基于Voronoi图的人类轨迹生成算法，实现完全仿真训练。

Result: 大量仿真和真实用户研究表明，该方法相比现有辅助遥操作方法显著降低操作员工作负荷、提高安全性，同时不牺牲任务效率。

Conclusion: 所提出的共享控制框架通过规划级意图预测实现了更有效的人机协作，为移动机器人共享控制提供了新的解决方案。

Abstract: In mobile robot shared control, effectively understanding human motion intention is critical for seamless human-robot collaboration. This paper presents a novel shared control framework featuring planning-level intention prediction. A path replanning algorithm is designed to adjust the robot's desired trajectory according to inferred human intentions. To represent future motion intentions, we introduce the concept of an intention domain, which serves as a constraint for path replanning. The intention-domain prediction and path replanning problems are jointly formulated as a Markov Decision Process and solved through deep reinforcement learning. In addition, a Voronoi-based human trajectory generation algorithm is developed, allowing the model to be trained entirely in simulation without human participation or demonstration data. Extensive simulations and real-world user studies demonstrate that the proposed method significantly reduces operator workload and enhances safety, without compromising task efficiency compared with existing assistive teleoperation approaches.

</details>


### [10] [Expand Your SCOPE: Semantic Cognition over Potential-Based Exploration for Embodied Visual Navigation](https://arxiv.org/abs/2511.08935)
*Ningnan Wang,Weihuang Chen,Liming Chen,Haoxuan Ji,Zhongyu Guo,Xuchong Zhang,Hongbin Sun*

Main category: cs.RO

TL;DR: SCOPE是一个零样本框架，利用边界信息驱动基于潜力的探索，通过视觉语言模型估计探索潜力并构建时空潜力图，结合自我重新考虑机制来改进决策质量。


<details>
  <summary>Details</summary>
Motivation: 现有零样本研究虽然通过记忆机制改进了长期规划性能，但忽视了视觉边界边界对轨迹和观测的影响，且未能推断部分视觉观测与导航目标之间的关系。

Method: 提出SCOPE框架，使用视觉语言模型估计探索潜力并构建时空潜力图，捕获边界动态以支持长期规划，同时引入自我重新考虑机制来修正先前决策。

Result: 在两个不同的具身导航任务上，SCOPE在准确率上比最先进基线方法高出4.6%，其核心组件带来了更好的校准、更强的泛化能力和更高的决策质量。

Conclusion: SCOPE通过显式利用边界信息和自我重新考虑机制，在零样本具身视觉导航中实现了更明智和与目标相关的决策，显著提升了导航性能。

Abstract: Embodied visual navigation remains a challenging task, as agents must explore unknown environments with limited knowledge. Existing zero-shot studies have shown that incorporating memory mechanisms to support goal-directed behavior can improve long-horizon planning performance. However, they overlook visual frontier boundaries, which fundamentally dictate future trajectories and observations, and fall short of inferring the relationship between partial visual observations and navigation goals. In this paper, we propose Semantic Cognition Over Potential-based Exploration (SCOPE), a zero-shot framework that explicitly leverages frontier information to drive potential-based exploration, enabling more informed and goal-relevant decisions. SCOPE estimates exploration potential with a Vision-Language Model and organizes it into a spatio-temporal potential graph, capturing boundary dynamics to support long-horizon planning. In addition, SCOPE incorporates a self-reconsideration mechanism that revisits and refines prior decisions, enhancing reliability and reducing overconfident errors. Experimental results on two diverse embodied navigation tasks show that SCOPE outperforms state-of-the-art baselines by 4.6\% in accuracy. Further analysis demonstrates that its core components lead to improved calibration, stronger generalization, and higher decision quality.

</details>


### [11] [Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning](https://arxiv.org/abs/2511.08942)
*Mobin Habibpour,Fatemeh Afghah*

Main category: cs.RO

TL;DR: 该论文提出了一种新框架，将视觉语言模型从被动观察者转变为主动策略制定者，通过结构化思维链提示、动态动作历史集成和障碍物地图解读来增强机器人导航能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用视觉语言模型的推理能力，需要将其角色从被动观察转变为主动策略制定，以释放其在机器人导航中的全部潜力。

Method: 采用结构化思维链提示激发逻辑推理，集成智能体的近期动作历史防止陷入循环，并让VLM能够解读俯视障碍物地图与第一人称视图以增强空间感知。

Result: 在HM3D、Gibson和MP3D等挑战性基准测试中，该方法产生了极其直接和逻辑的轨迹，导航效率相比现有方法有显著提升。

Conclusion: 该方法为开发更具能力的具身智能体开辟了新路径，显著提升了机器人导航性能。

Abstract: While Vision-Language Models (VLMs) are set to transform robotic navigation, existing methods often underutilize their reasoning capabilities. To unlock the full potential of VLMs in robotics, we shift their role from passive observers to active strategists in the navigation process. Our framework outsources high-level planning to a VLM, which leverages its contextual understanding to guide a frontier-based exploration agent. This intelligent guidance is achieved through a trio of techniques: structured chain-of-thought prompting that elicits logical, step-by-step reasoning; dynamic inclusion of the agent's recent action history to prevent getting stuck in loops; and a novel capability that enables the VLM to interpret top-down obstacle maps alongside first-person views, thereby enhancing spatial awareness. When tested on challenging benchmarks like HM3D, Gibson, and MP3D, this method produces exceptionally direct and logical trajectories, marking a substantial improvement in navigation efficiency over existing approaches and charting a path toward more capable embodied agents.

</details>


### [12] [UniMM-V2X: MoE-Enhanced Multi-Level Fusion for End-to-End Cooperative Autonomous Driving](https://arxiv.org/abs/2511.09013)
*Ziyi Song,Chen Xia,Chenbing Wang,Haibao Yu,Sheng Zhou,Zhisheng Niu*

Main category: cs.RO

TL;DR: UniMM-V2X是一个端到端多智能体框架，通过多层次融合策略实现感知、预测和规划的层次化协作，结合MoE架构动态增强BEV表示，在DAIR-V2X数据集上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统存在感知有限和决策孤立的问题，现有多智能体方法主要关注感知级任务，忽略了与下游规划控制的协调，未能充分利用端到端自动驾驶的潜力。

Method: 提出多层次融合策略统一感知和预测协作，让智能体共享查询并协同推理；引入MoE架构动态增强BEV表示，并将MoE扩展到解码器以更好地捕捉多样化运动模式。

Result: 在DAIR-V2X数据集上，相比UniV2X，感知精度提升39.7%，预测误差降低7.2%，规划性能提升33.2%，达到SOTA水平。

Conclusion: MoE增强的多层次协作范式展示了强大的性能，为端到端自动驾驶提供了有效的多智能体协作解决方案。

Abstract: Autonomous driving holds transformative potential but remains fundamentally constrained by the limited perception and isolated decision-making with standalone intelligence. While recent multi-agent approaches introduce cooperation, they often focus merely on perception-level tasks, overlooking the alignment with downstream planning and control, or fall short in leveraging the full capacity of the recent emerging end-to-end autonomous driving. In this paper, we present UniMM-V2X, a novel end-to-end multi-agent framework that enables hierarchical cooperation across perception, prediction, and planning. At the core of our framework is a multi-level fusion strategy that unifies perception and prediction cooperation, allowing agents to share queries and reason cooperatively for consistent and safe decision-making. To adapt to diverse downstream tasks and further enhance the quality of multi-level fusion, we incorporate a Mixture-of-Experts (MoE) architecture to dynamically enhance the BEV representations. We further extend MoE into the decoder to better capture diverse motion patterns. Extensive experiments on the DAIR-V2X dataset demonstrate our approach achieves state-of-the-art (SOTA) performance with a 39.7% improvement in perception accuracy, a 7.2% reduction in prediction error, and a 33.2% improvement in planning performance compared with UniV2X, showcasing the strength of our MoE-enhanced multi-level cooperative paradigm.

</details>


### [13] [A Quantum Tunneling and Bio-Phototactic Driven Enhanced Dwarf Mongoose Optimizer for UAV Trajectory Planning and Engineering Problem](https://arxiv.org/abs/2511.09020)
*Mingyang Yu,Haorui Yang,Kangning An,Xinjian Wei,Xiaoxuan Xu,Jing Xu*

Main category: cs.RO

TL;DR: 提出增强型多策略矮獴优化算法(EDMO)，用于解决无人机在动态障碍环境中的三维路径规划问题，通过三种新策略提升算法性能。


<details>
  <summary>Details</summary>
Motivation: 传统搜索方法在无人机路径规划中存在局限性，元启发式算法虽流行但仍面临早熟收敛和缺乏解多样性等问题，特别是在复杂动态环境中。

Method: 集成三种新策略：动态量子隧道优化策略(DQTOS)用于跳出局部最优；生物趋光动态聚焦搜索策略(BDFSS)进行自适应局部优化；正交透镜对立学习策略(OLOBL)增强全局探索。

Result: 在CEC2017和CEC2020的39个标准测试函数上表现优异，超越14种先进算法，收敛速度、鲁棒性和优化精度均更优。在无人机三维路径规划和工程设计中验证了实际有效性。

Conclusion: EDMO算法在复杂动态环境中表现出强大的优化能力，为无人机智能路径规划提供了高效实用的解决方案。

Abstract: With the widespread adoption of unmanned aerial vehicles (UAV), effective path planning has become increasingly important. Although traditional search methods have been extensively applied, metaheuristic algorithms have gained popularity due to their efficiency and problem-specific heuristics. However, challenges such as premature convergence and lack of solution diversity still hinder their performance in complex scenarios. To address these issues, this paper proposes an Enhanced Multi-Strategy Dwarf Mongoose Optimization (EDMO) algorithm, tailored for three-dimensional UAV trajectory planning in dynamic and obstacle-rich environments. EDMO integrates three novel strategies: (1) a Dynamic Quantum Tunneling Optimization Strategy (DQTOS) to enable particles to probabilistically escape local optima; (2) a Bio-phototactic Dynamic Focusing Search Strategy (BDFSS) inspired by microbial phototaxis for adaptive local refinement; and (3) an Orthogonal Lens Opposition-Based Learning (OLOBL) strategy to enhance global exploration through structured dimensional recombination. EDMO is benchmarked on 39 standard test functions from CEC2017 and CEC2020, outperforming 14 advanced algorithms in convergence speed, robustness, and optimization accuracy. Furthermore, real-world validations on UAV three-dimensional path planning and three engineering design tasks confirm its practical applicability and effectiveness in field robotics missions requiring intelligent, adaptive, and time-efficient planning.

</details>


### [14] [SMF-VO: Direct Ego-Motion Estimation via Sparse Motion Fields](https://arxiv.org/abs/2511.09072)
*Sangheon Yang,Yeongin Yoon,Hong Mo Jung,Jongwoo Lim*

Main category: cs.RO

TL;DR: SMF-VO是一种轻量级的视觉里程计方法，通过直接估计瞬时线速度和角速度来替代传统的位姿估计，在树莓派5上仅使用CPU即可实现超过100 FPS的性能。


<details>
  <summary>Details</summary>
Motivation: 传统VO和VIO方法采用'位姿中心'范式，需要维护大规模地标和持续地图优化，计算成本高，限制了在资源受限设备上的实时性能。

Method: 提出稀疏运动场视觉里程计(SMF-VO)，直接从稀疏光流估计瞬时线速度和角速度，绕开显式位姿估计和昂贵的地标跟踪，采用通用的3D射线运动场公式，适用于各种相机模型。

Result: 在基准数据集上展示了优越的效率和竞争性的精度，在树莓派5上仅使用CPU即可实现超过100 FPS的性能。

Conclusion: 为传统方法建立了可扩展且高效的替代方案，非常适合移动机器人和可穿戴设备应用。

Abstract: Traditional Visual Odometry (VO) and Visual Inertial Odometry (VIO) methods rely on a 'pose-centric' paradigm, which computes absolute camera poses from the local map thus requires large-scale landmark maintenance and continuous map optimization. This approach is computationally expensive, limiting their real-time performance on resource-constrained devices. To overcome these limitations, we introduce Sparse Motion Field Visual Odometry (SMF-VO), a lightweight, 'motion-centric' framework. Our approach directly estimates instantaneous linear and angular velocity from sparse optical flow, bypassing the need for explicit pose estimation or expensive landmark tracking. We also employed a generalized 3D ray-based motion field formulation that works accurately with various camera models, including wide-field-of-view lenses. SMF-VO demonstrates superior efficiency and competitive accuracy on benchmark datasets, achieving over 100 FPS on a Raspberry Pi 5 using only a CPU. Our work establishes a scalable and efficient alternative to conventional methods, making it highly suitable for mobile robotics and wearable devices.

</details>


### [15] [D-AWSIM: Distributed Autonomous Driving Simulator for Dynamic Map Generation Framework](https://arxiv.org/abs/2511.09080)
*Shunsuke Ito,Chaoran Zhao,Ryo Okamura,Takuya Azumi*

Main category: cs.RO

TL;DR: 提出了D-AWSIM分布式仿真器，通过多机负载分配支持大规模传感器部署和密集交通环境仿真，显著提升了车辆数量和LiDAR传感器处理的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现实世界实验成本高且面临监管挑战，传统单机仿真器无法处理大规模城市交通场景，需要分布式解决方案来支持自动驾驶研究中信息共享策略的探索。

Method: 开发了D-AWSIM分布式仿真器，采用多机工作负载分配机制，并构建了动态地图生成框架，与Autoware集成验证实用性。

Result: 与单机设置相比，D-AWSIM显著提高了车辆数量和LiDAR传感器处理的吞吐量，能够支持大规模传感器部署和密集交通环境仿真。

Conclusion: D-AWSIM为自动驾驶研究提供了有效的分布式仿真平台，使研究人员能够在无需物理测试平台的情况下探索信息共享策略，具有实际应用价值。

Abstract: Autonomous driving systems have achieved significant advances, and full autonomy within defined operational design domains near practical deployment. Expanding these domains requires addressing safety assurance under diverse conditions. Information sharing through vehicle-to-vehicle and vehicle-to-infrastructure communication, enabled by a Dynamic Map platform built from vehicle and roadside sensor data, offers a promising solution. Real-world experiments with numerous infrastructure sensors incur high costs and regulatory challenges. Conventional single-host simulators lack the capacity for large-scale urban traffic scenarios. This paper proposes D-AWSIM, a distributed simulator that partitions its workload across multiple machines to support the simulation of extensive sensor deployment and dense traffic environments. A Dynamic Map generation framework on D-AWSIM enables researchers to explore information-sharing strategies without relying on physical testbeds. The evaluation shows that D-AWSIM increases throughput for vehicle count and LiDAR sensor processing substantially compared to a single-machine setup. Integration with Autoware demonstrates applicability for autonomous driving research.

</details>


### [16] [APEX: Action Priors Enable Efficient Exploration for Robust Motion Tracking on Legged Robots](https://arxiv.org/abs/2511.09091)
*Shivam Sood,Laukik Nakhwa,Sun Ge,Yuhong Cao,Jin Cheng,Fatemah Zargarbashi,Taerim Yoon,Sungjoon Choi,Stelian Coros,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: APEX是一种即插即用的强化学习扩展方法，通过整合动作先验来消除对参考数据的依赖，提高样本效率并减少参数调优工作。


<details>
  <summary>Details</summary>
Motivation: 现有运动跟踪方法需要大量调优并在部署时依赖参考数据，限制了适应性。需要一种能够独立学习自然步态的方法。

Method: 结合衰减动作先验和多评价器框架，初始阶段偏向专家演示但逐渐允许独立探索，平衡任务性能与运动风格。

Result: 在仿真和Unitree Go2机器人上的实验验证了方法的有效性，能够学习多样化运动并在不同地形和速度间迁移风格。

Conclusion: APEX通过演示引导探索而不强加显式偏置，使腿式机器人能够以更高稳定性、效率和泛化能力学习，为从运动到操作的广泛机器人任务中的自然技能获取铺平道路。

Abstract: Learning natural, animal-like locomotion from demonstrations has become a core paradigm in legged robotics. Despite the recent advancements in motion tracking, most existing methods demand extensive tuning and rely on reference data during deployment, limiting adaptability. We present APEX (Action Priors enable Efficient Exploration), a plug-and-play extension to state-of-the-art motion tracking algorithms that eliminates any dependence on reference data during deployment, improves sample efficiency, and reduces parameter tuning effort. APEX integrates expert demonstrations directly into reinforcement learning (RL) by incorporating decaying action priors, which initially bias exploration toward expert demonstrations but gradually allow the policy to explore independently. This is combined with a multi-critic framework that balances task performance with motion style. Moreover, APEX enables a single policy to learn diverse motions and transfer reference-like styles across different terrains and velocities, while remaining robust to variations in reward design. We validate the effectiveness of our method through extensive experiments in both simulation and on a Unitree Go2 robot. By leveraging demonstrations to guide exploration during RL training, without imposing explicit bias toward them, APEX enables legged robots to learn with greater stability, efficiency, and generalization. We believe this approach paves the way for guidance-driven RL to boost natural skill acquisition in a wide array of robotic tasks, from locomotion to manipulation. Website and code: https://marmotlab.github.io/APEX/.

</details>


### [17] [Decoupling Torque and Stiffness: A Unified Modeling and Control Framework for Antagonistic Artificial Muscles](https://arxiv.org/abs/2511.09104)
*Amirhossein Kazemipour,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 提出了一个统一框架，用于实时独立控制软体拮抗执行器的扭矩和刚度，通过级联控制器和分析逆动力学实现解耦控制，在接触实验中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的软体肌肉控制器难以在动态接触瞬变中维持扭矩和刚度的独立控制，限制了拮抗软体执行器在安全人机交互中的应用。

Method: 使用统一力定律捕捉多种软体肌肉物理特性，采用级联控制器和分析逆动力学，通过共收缩/偏置坐标实现扭矩和刚度的独立调制。

Result: 在接触实验中保持独立控制：软表面200倍更快稳定，刚性表面81%力减少，稳定交互对比固定策略的22-54%稳定性。

Conclusion: 该框架为肌肉骨骼拮抗系统执行自适应阻抗控制提供了基础，支持安全的人机交互。

Abstract: Antagonistic soft actuators built from artificial muscles (PAMs, HASELs, DEAs) promise plant-level torque-stiffness decoupling, yet existing controllers for soft muscles struggle to maintain independent control through dynamic contact transients. We present a unified framework enabling independent torque and stiffness commands in real-time for diverse soft actuator types. Our unified force law captures diverse soft muscle physics in a single model with sub-ms computation, while our cascaded controller with analytical inverse dynamics maintains decoupling despite model errors and disturbances. Using co-contraction/bias coordinates, the controller independently modulates torque via bias and stiffness via co-contraction-replicating biological impedance strategies. Simulation-based validation through contact experiments demonstrates maintained independence: 200x faster settling on soft surfaces, 81% force reduction on rigid surfaces, and stable interaction vs 22-54% stability for fixed policies. This framework provides a foundation for enabling musculoskeletal antagonistic systems to execute adaptive impedance control for safe human-robot interaction.

</details>


### [18] [Data Assessment for Embodied Intelligence](https://arxiv.org/abs/2511.09119)
*Jiahao Xiao,Bowen Yan,Jianbo Zhang,Jia Wang,Chunyi Li,Zhengxue Cheng,Guangtao Zhai*

Main category: cs.RO

TL;DR: 本文提出了两种数据驱动工具来评估具身智能数据集：多样性熵和可学习性量化算法，旨在解决数据集信息量和学习难易程度的评估难题。


<details>
  <summary>Details</summary>
Motivation: 当前具身智能数据集评估主要关注多样性但方法不全面，而可学习性评估通常需要昂贵的模型训练且缺乏可解释性，亟需更高效、可解释的评估方法。

Method: 1) 构建统一的多模态数据表示并提出多样性熵来量化数据集信息量；2) 开发首个无需训练的可解释算法来量化数据集可学习性。

Result: 在模拟和真实具身数据集上的验证表明，该算法能提供忠实、可操作的见解，帮助研究人员同时改进数据集的多样性和可学习性。

Conclusion: 这项工作为设计更高质量的具身智能数据集奠定了基础，有望推动具身智能的发展。

Abstract: In embodied intelligence, datasets play a pivotal role, serving as both a knowledge repository and a conduit for information transfer. The two most critical attributes of a dataset are the amount of information it provides and how easily this information can be learned by models. However, the multimodal nature of embodied data makes evaluating these properties particularly challenging. Prior work has largely focused on diversity, typically counting tasks and scenes or evaluating isolated modalities, which fails to provide a comprehensive picture of dataset diversity. On the other hand, the learnability of datasets has received little attention and is usually assessed post-hoc through model training, an expensive, time-consuming process that also lacks interpretability, offering little guidance on how to improve a dataset. In this work, we address both challenges by introducing two principled, data-driven tools. First, we construct a unified multimodal representation for each data sample and, based on it, propose diversity entropy, a continuous measure that characterizes the amount of information contained in a dataset. Second, we introduce the first interpretable, data-driven algorithm to efficiently quantify dataset learnability without training, enabling researchers to assess a dataset's learnability immediately upon its release. We validate our algorithm on both simulated and real-world embodied datasets, demonstrating that it yields faithful, actionable insights that enable researchers to jointly improve diversity and learnability. We hope this work provides a foundation for designing higher-quality datasets that advance the development of embodied intelligence.

</details>


### [19] [RGMP: Recurrent Geometric-prior Multimodal Policy for Generalizable Humanoid Robot Manipulation](https://arxiv.org/abs/2511.09141)
*Xuetao Li,Wenke Huang,Nengyuan Pan,Kaiyan Zhao,Songhua Yang,Yiming Wang,Mengde Li,Mang Ye,Jifeng Xuan,Miao Li*

Main category: cs.RO

TL;DR: RGMP是一个端到端框架，通过几何先验技能选择器和自适应递归高斯网络，将几何语义技能推理与数据高效的视觉运动控制相结合，在未见场景中实现87%的任务成功率，数据效率比现有最佳模型提高5倍。


<details>
  <summary>Details</summary>
Motivation: 当前基于数据驱动的方法需要大量训练数据，忽视了未见场景中的几何推理，且对机器人-目标关系的建模效率低下，造成训练资源浪费。

Method: 提出RGMP框架：1) 几何先验技能选择器，将几何归纳偏置注入视觉语言模型，为未见场景生成自适应技能序列；2) 自适应递归高斯网络，将机器人-物体交互参数化为高斯过程层次结构，递归编码多尺度空间关系。

Result: 在人形机器人和桌面双臂机器人上的评估显示，RGMP在泛化测试中达到87%的任务成功率，数据效率比最先进模型提高5倍。

Conclusion: RGMP通过几何语义推理和递归高斯自适应，实现了卓越的跨领域泛化能力，为数据高效的人形机器人技能学习提供了有效解决方案。

Abstract: Humanoid robots exhibit significant potential in executing diverse human-level skills. However, current research predominantly relies on data-driven approaches that necessitate extensive training datasets to achieve robust multimodal decision-making capabilities and generalizable visuomotor control. These methods raise concerns due to the neglect of geometric reasoning in unseen scenarios and the inefficient modeling of robot-target relationships within the training data, resulting in significant waste of training resources. To address these limitations, we present the Recurrent Geometric-prior Multimodal Policy (RGMP), an end-to-end framework that unifies geometric-semantic skill reasoning with data-efficient visuomotor control. For perception capabilities, we propose the Geometric-prior Skill Selector, which infuses geometric inductive biases into a vision language model, producing adaptive skill sequences for unseen scenes with minimal spatial common sense tuning. To achieve data-efficient robotic motion synthesis, we introduce the Adaptive Recursive Gaussian Network, which parameterizes robot-object interactions as a compact hierarchy of Gaussian processes that recursively encode multi-scale spatial relationships, yielding dexterous, data-efficient motion synthesis even from sparse demonstrations. Evaluated on both our humanoid robot and desktop dual-arm robot, the RGMP framework achieves 87% task success in generalization tests and exhibits 5x greater data efficiency than the state-of-the-art model. This performance underscores its superior cross-domain generalization, enabled by geometric-semantic reasoning and recursive-Gaussion adaptation.

</details>


### [20] [LODESTAR: Degeneracy-Aware LiDAR-Inertial Odometry with Adaptive Schmidt-Kalman Filter and Data Exploitation](https://arxiv.org/abs/2511.09142)
*Eungchang Mason Lee,Kevin Christiansen Marsim,Hyun Myung*

Main category: cs.RO

TL;DR: LODESTAR是一种新颖的LiDAR-惯性里程计方法，通过退化感知自适应施密特-卡尔曼滤波和退化感知数据利用两个关键模块，解决了在退化环境中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 传统LiDAR-惯性里程计在退化环境（如长走廊、高空飞行）中性能下降，因为LiDAR测量不平衡或稀疏导致状态估计病态。

Method: 使用DA-ASKF模块通过滑动窗口利用过去状态和测量作为额外约束，根据退化水平自适应分类状态为活动或固定；使用DA-DE模块基于局部化贡献和雅可比矩阵条件数，修剪活动状态中信息量较少的测量并选择性利用固定状态的测量。

Result: 实验结果表明LODESTAR在各种退化条件下，在精度和鲁棒性方面优于现有的基于LiDAR的里程计方法和退化感知模块。

Conclusion: LODESTAR通过退化感知约束优化和测量不平衡处理，有效解决了LiDAR-惯性里程计在退化环境中的性能问题。

Abstract: LiDAR-inertial odometry (LIO) has been widely used in robotics due to its high accuracy. However, its performance degrades in degenerate environments, such as long corridors and high-altitude flights, where LiDAR measurements are imbalanced or sparse, leading to ill-posed state estimation. In this letter, we present LODESTAR, a novel LIO method that addresses these degeneracies through two key modules: degeneracy-aware adaptive Schmidt-Kalman filter (DA-ASKF) and degeneracy-aware data exploitation (DA-DE). DA-ASKF employs a sliding window to utilize past states and measurements as additional constraints. Specifically, it introduces degeneracy-aware sliding modes that adaptively classify states as active or fixed based on their degeneracy level. Using Schmidt-Kalman update, it partially optimizes active states while preserving fixed states. These fixed states influence the update of active states via their covariances, serving as reference anchors--akin to a lodestar. Additionally, DA-DE prunes less-informative measurements from active states and selectively exploits measurements from fixed states, based on their localizability contribution and the condition number of the Jacobian matrix. Consequently, DA-ASKF enables degeneracy-aware constrained optimization and mitigates measurement sparsity, while DA-DE addresses measurement imbalance. Experimental results show that LODESTAR outperforms existing LiDAR-based odometry methods and degeneracy-aware modules in terms of accuracy and robustness under various degenerate conditions.

</details>


### [21] [Unveiling the Impact of Data and Model Scaling on High-Level Control for Humanoid Robots](https://arxiv.org/abs/2511.09241)
*Yuxi Wei,Zirui Wang,Kangning Yin,Yue Hu,Jingbo Wang,Siheng Chen*

Main category: cs.RO

TL;DR: Humanoid-Union是一个通过自主流程生成的大规模人形机器人运动数据集，包含260多小时多样化高质量运动数据，并基于此提出SCHUR框架，通过数据扩展实现高质量运动生成和文本-运动对齐。


<details>
  <summary>Details</summary>
Motivation: 解决机器人学习中数据扩展的关键瓶颈，利用丰富的人类视频和运动数据作为免费的大规模数据源，探索如何从原始视频中提取机器人可学习的表示并用于可扩展学习。

Method: 提出Humanoid-Union数据集生成管道，自动从人类运动视频中提取语义标注的运动数据；基于此构建SCHUR可扩展学习框架，研究大规模数据对人形机器人高级控制的影响。

Result: SCHUR在数据和模型扩展下实现了高质量机器人运动生成和强文本-运动对齐，相比先前方法在MPJPE上重建改进37%，在FID上对齐改进25%，并在真实人形机器人上验证了有效性。

Conclusion: 通过Humanoid-Union数据集和SCHUR框架，证明了大规模数据对人形机器人高级控制学习的重要作用，为机器人学习提供了可扩展的数据资源和方法。

Abstract: Data scaling has long remained a critical bottleneck in robot learning. For humanoid robots, human videos and motion data are abundant and widely available, offering a free and large-scale data source. Besides, the semantics related to the motions enable modality alignment and high-level robot control learning. However, how to effectively mine raw video, extract robot-learnable representations, and leverage them for scalable learning remains an open problem. To address this, we introduce Humanoid-Union, a large-scale dataset generated through an autonomous pipeline, comprising over 260 hours of diverse, high-quality humanoid robot motion data with semantic annotations derived from human motion videos. The dataset can be further expanded via the same pipeline. Building on this data resource, we propose SCHUR, a scalable learning framework designed to explore the impact of large-scale data on high-level control in humanoid robots. Experimental results demonstrate that SCHUR achieves high robot motion generation quality and strong text-motion alignment under data and model scaling, with 37\% reconstruction improvement under MPJPE and 25\% alignment improvement under FID comparing with previous methods. Its effectiveness is further validated through deployment in real-world humanoid robot.

</details>


### [22] [UMIGen: A Unified Framework for Egocentric Point Cloud Generation and Cross-Embodiment Robotic Imitation Learning](https://arxiv.org/abs/2511.09302)
*Yan Huang,Shoujie Li,Xingting Li,Wenbo Ding*

Main category: cs.RO

TL;DR: UMIGen是一个统一框架，通过手持设备Cloud-UMI收集点云观测-动作对数据，并采用可见性感知优化机制，实现高效数据生成和跨机器人实施例的泛化。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的机器人学习面临数据收集成本高、依赖专用硬件以及空间泛化能力有限的问题。UMI方法虽然降低了硬件要求，但仅能捕获RGB图像而缺少3D几何信息。

Method: UMIGen包含两个关键组件：(1) Cloud-UMI手持数据收集设备，无需视觉SLAM，同时记录点云观测-动作对；(2) 可见性感知优化机制，扩展DemoGen流水线以处理自我中心3D观测，仅生成相机视野内的点。

Result: 在仿真和真实环境中的实验表明，UMIGen支持强大的跨实施例泛化能力，并在多样化操作任务中加速了数据收集过程。

Conclusion: UMIGen框架能够实现高效的数据生成，与真实自我中心观测对齐，并且可以直接在不同机器人实施例间迁移，无需任何后处理。

Abstract: Data-driven robotic learning faces an obvious dilemma: robust policies demand large-scale, high-quality demonstration data, yet collecting such data remains a major challenge owing to high operational costs, dependence on specialized hardware, and the limited spatial generalization capability of current methods. The Universal Manipulation Interface (UMI) relaxes the strict hardware requirements for data collection, but it is restricted to capturing only RGB images of a scene and omits the 3D geometric information on which many tasks rely. Inspired by DemoGen, we propose UMIGen, a unified framework that consists of two key components: (1) Cloud-UMI, a handheld data collection device that requires no visual SLAM and simultaneously records point cloud observation-action pairs; and (2) a visibility-aware optimization mechanism that extends the DemoGen pipeline to egocentric 3D observations by generating only points within the camera's field of view. These two components enable efficient data generation that aligns with real egocentric observations and can be directly transferred across different robot embodiments without any post-processing. Experiments in both simulated and real-world settings demonstrate that UMIGen supports strong cross-embodiment generalization and accelerates data collection in diverse manipulation tasks.

</details>


### [23] [CoRL-MPPI: Enhancing MPPI With Learnable Behaviours For Efficient And Provably-Safe Multi-Robot Collision Avoidance](https://arxiv.org/abs/2511.09331)
*Stepan Dergachev,Artem Pshenitsyn,Aleksandr Panov,Alexey Skrynnik,Konstantin Yakovlev*

Main category: cs.RO

TL;DR: CoRL-MPPI结合了合作强化学习和MPPI，通过训练深度神经网络学习局部合作避障行为，并将其嵌入MPPI框架来引导采样分布，从而在多机器人系统中实现更智能高效的导航。


<details>
  <summary>Details</summary>
Motivation: 传统MPPI控制器在多机器人避障中可能产生次优轨迹，因为其性能依赖于无信息的随机采样。需要一种方法来改进MPPI的采样策略，使其更智能和合作。

Method: 在仿真中训练深度神经网络学习局部合作避障行为，然后将学习到的策略嵌入MPPI框架，引导其采样分布偏向更智能和合作的动作。

Result: 在密集动态仿真环境中，CoRL-MPPI相比ORCA、BVC和多智能体MPPI等基线方法，显著提高了导航效率（成功率、完成时间）和安全性。

Conclusion: CoRL-MPPI保持了常规MPPI的所有理论保证，同时实现了更敏捷和鲁棒的多机器人导航。

Abstract: Decentralized collision avoidance remains a core challenge for scalable multi-robot systems. One of the promising approaches to tackle this problem is Model Predictive Path Integral (MPPI) -- a framework that is naturally suited to handle any robot motion model and provides strong theoretical guarantees. Still, in practice MPPI-based controller may provide suboptimal trajectories as its performance relies heavily on uninformed random sampling. In this work, we introduce CoRL-MPPI, a novel fusion of Cooperative Reinforcement Learning and MPPI to address this limitation. We train an action policy (approximated as deep neural network) in simulation that learns local cooperative collision avoidance behaviors. This learned policy is then embedded into the MPPI framework to guide its sampling distribution, biasing it towards more intelligent and cooperative actions. Notably, CoRL-MPPI preserves all the theoretical guarantees of regular MPPI. We evaluate our approach in dense, dynamic simulation environments against state-of-the-art baselines, including ORCA, BVC, and a multi-agent MPPI implementation. Our results demonstrate that CoRL-MPPI significantly improves navigation efficiency (measured by success rate and makespan) and safety, enabling agile and robust multi-robot navigation.

</details>


### [24] [SPIDER: Scalable Physics-Informed Dexterous Retargeting](https://arxiv.org/abs/2511.09484)
*Chaoyi Pan,Changhao Wang,Haozhi Qi,Zixi Liu,Homanga Bharadhwaj,Akash Sharma,Tingfan Wu,Guanya Shi,Jitendra Malik,Francois Hogan*

Main category: cs.RO

TL;DR: SPIDER是一个基于物理的重新定向框架，可将人类运动数据转换为机器人可执行的动态可行轨迹，解决了人机数据差距问题。


<details>
  <summary>Details</summary>
Motivation: 机器人控制需要大量演示数据，但收集机器人特定数据成本高昂。相比之下，人类运动数据丰富易得，但由于体现差距和缺乏动态信息，无法直接在机器人上执行。

Method: 提出SPIDER框架，利用人类演示提供全局任务结构和目标，通过大规模基于物理的采样和课程式虚拟接触指导来精炼轨迹，确保动态可行性和正确接触序列。

Result: SPIDER在9种人形/灵巧手实体和6个数据集上扩展，相比标准采样成功率提高18%，比强化学习基线快10倍，生成了240万帧动态可行机器人数据集。

Conclusion: SPIDER作为通用基于物理的重新定向方法，可处理多样化质量数据并生成高质量数据，支持高效策略学习。

Abstract: Learning dexterous and agile policy for humanoid and dexterous hand control requires large-scale demonstrations, but collecting robot-specific data is prohibitively expensive. In contrast, abundant human motion data is readily available from motion capture, videos, and virtual reality, which could help address the data scarcity problem. However, due to the embodiment gap and missing dynamic information like force and torque, these demonstrations cannot be directly executed on robots. To bridge this gap, we propose Scalable Physics-Informed DExterous Retargeting (SPIDER), a physics-based retargeting framework to transform and augment kinematic-only human demonstrations to dynamically feasible robot trajectories at scale. Our key insight is that human demonstrations should provide global task structure and objective, while large-scale physics-based sampling with curriculum-style virtual contact guidance should refine trajectories to ensure dynamical feasibility and correct contact sequences. SPIDER scales across diverse 9 humanoid/dexterous hand embodiments and 6 datasets, improving success rates by 18% compared to standard sampling, while being 10X faster than reinforcement learning (RL) baselines, and enabling the generation of a 2.4M frames dynamic-feasible robot dataset for policy learning. As a universal physics-based retargeting method, SPIDER can work with diverse quality data and generate diverse and high-quality data to enable efficient policy learning with methods like RL.

</details>


### [25] [WMPO: World Model-based Policy Optimization for Vision-Language-Action Models](https://arxiv.org/abs/2511.09515)
*Fangqi Zhu,Zhengyang Yan,Zicong Hong,Quanxin Shou,Xiao Ma,Song Guo*

Main category: cs.RO

TL;DR: 提出了WMPO框架，通过基于像素预测的世界模型在VLA模型中实现无需真实环境交互的在线强化学习，显著提升样本效率并实现自我纠正等涌现行为。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型依赖专家演示无法从失败中学习的问题，同时避免强化学习在真实机器人上的高样本复杂度。

Method: 基于像素预测的世界模型框架WMPO，将"想象"轨迹与预训练的VLA特征对齐，实现在线GRPO策略优化。

Result: 在仿真和真实机器人实验中，WMPO显著提升样本效率、获得更强性能，并展现出自我纠正、泛化能力和持续学习能力。

Conclusion: WMPO为VLA模型提供了一种无需真实环境交互的在线强化学习方法，解决了现有方法的局限性。

Abstract: Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the "imagined" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.

</details>


### [26] [MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation](https://arxiv.org/abs/2511.09516)
*Runhao Li,Wenkai Guo,Zhenyu Wu,Changyuan Wang,Haoyuan Deng,Zhenyu Weng,Yap-Peng Tan,Ziwei Wang*

Main category: cs.RO

TL;DR: 提出MAP-VLA框架，通过演示衍生的记忆提示增强预训练VLA模型，解决长时程机器人操作任务中的记忆缺失问题


<details>
  <summary>Details</summary>
Motivation: 预训练的视觉-语言-动作模型在端到端机器人操作中表现出色，但缺乏记忆能力，仅依赖即时感官输入，难以处理长时程任务

Method: 构建基于历史演示的记忆库，将记忆单元实现为可学习的软提示；通过轨迹相似性匹配检索相关记忆，动态集成到VLA模型中增强动作生成

Result: 在仿真基准测试中实现7.0%的绝对性能提升，在真实机器人评估中实现25.0%的性能提升，超越当前最先进方法

Conclusion: MAP-VLA作为即插即用模块，为冻结的VLA模型提供轻量灵活的解决方案，显著提升长时程任务性能

Abstract: Pre-trained Vision-Language-Action (VLA) models have achieved remarkable success in improving robustness and generalization for end-to-end robotic manipulation. However, these models struggle with long-horizon tasks due to their lack of memory and reliance solely on immediate sensory inputs. To address this limitation, we propose Memory-Augmented Prompting for Vision-Language-Action model (MAP-VLA), a novel framework that empowers pre-trained VLA models with demonstration-derived memory prompts to augment action generation for long-horizon robotic manipulation tasks. To achieve this, MAP-VLA first constructs a memory library from historical demonstrations, where each memory unit captures information about a specific stage of a task. These memory units are implemented as learnable soft prompts optimized through prompt tuning. Then, during real-time task execution, MAP-VLA retrieves relevant memory through trajectory similarity matching and dynamically integrates it into the VLA model for augmented action generation. Importantly, this prompt tuning and retrieval augmentation approach operates as a plug-and-play module for a frozen VLA model, offering a lightweight and flexible solution to improve task performance. Experimental results show that MAP-VLA delivers up to 7.0% absolute performance gains in the simulation benchmark and 25.0% on real robot evaluations for long-horizon tasks, surpassing the current state-of-the-art methods.

</details>


### [27] [SpatialActor: Exploring Disentangled Spatial Representations for Robust Robotic Manipulation](https://arxiv.org/abs/2511.09555)
*Hao Shi,Bin Xie,Yingfei Liu,Yang Yue,Tiancai Wang,Haoqiang Fan,Xiangyu Zhang,Gao Huang*

Main category: cs.RO

TL;DR: SpatialActor是一个用于机器人操作的解耦框架，通过分离语义和几何信息来解决现有方法在噪声深度数据和空间理解方面的不足，在多个任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作方法存在两个主要问题：基于点的方法采样稀疏导致细粒度语义丢失；基于图像的方法将RGB和深度数据输入预训练的2D骨干网络，但语义和几何的纠缠使其对现实世界中的深度噪声敏感，同时忽略了精确交互所需的低层空间线索。

Method: 提出SpatialActor解耦框架，包含语义引导的几何模块和空间变换器。语义引导的几何模块自适应融合来自噪声深度和语义引导专家先验的两种互补几何信息；空间变换器利用低层空间线索进行准确的2D-3D映射，并实现空间特征之间的交互。

Result: 在多个仿真和现实世界场景的50多个任务中评估，在RLBench上达到87.4%的最先进性能，在不同噪声条件下性能提升13.9%到19.4%，显示出强大的鲁棒性。同时显著提高了对新任务的少样本泛化能力，并在各种空间扰动下保持鲁棒性。

Conclusion: SpatialActor通过明确解耦语义和几何信息，有效解决了现有机器人操作方法在噪声鲁棒性和空间理解方面的局限性，为精确的机器人操作提供了更可靠的解决方案。

Abstract: Robotic manipulation requires precise spatial understanding to interact with objects in the real world. Point-based methods suffer from sparse sampling, leading to the loss of fine-grained semantics. Image-based methods typically feed RGB and depth into 2D backbones pre-trained on 3D auxiliary tasks, but their entangled semantics and geometry are sensitive to inherent depth noise in real-world that disrupts semantic understanding. Moreover, these methods focus on high-level geometry while overlooking low-level spatial cues essential for precise interaction. We propose SpatialActor, a disentangled framework for robust robotic manipulation that explicitly decouples semantics and geometry. The Semantic-guided Geometric Module adaptively fuses two complementary geometry from noisy depth and semantic-guided expert priors. Also, a Spatial Transformer leverages low-level spatial cues for accurate 2D-3D mapping and enables interaction among spatial features. We evaluate SpatialActor on multiple simulation and real-world scenarios across 50+ tasks. It achieves state-of-the-art performance with 87.4% on RLBench and improves by 13.9% to 19.4% under varying noisy conditions, showing strong robustness. Moreover, it significantly enhances few-shot generalization to new tasks and maintains robustness under various spatial perturbations. Project Page: https://shihao1895.github.io/SpatialActor

</details>


### [28] [IFG: Internet-Scale Guidance for Functional Grasping Generation](https://arxiv.org/abs/2511.09558)
*Ray Muxin Liu,Mingxuan Li,Kenneth Shaw,Deepak Pathak*

Main category: cs.RO

TL;DR: 提出了一种结合互联网规模视觉模型的语义理解与基于仿真的局部几何感知力闭合的语义抓取方法，无需手动收集训练数据即可实现高性能3D抓取。


<details>
  <summary>Details</summary>
Motivation: 大型视觉模型虽然能在杂乱场景中分割和理解物体部件，但缺乏几何理解能力，无法精确控制灵巧机械手进行3D抓取。需要结合几何精度来实现精确的语义抓取。

Method: 利用仿真环境构建力闭合抓取生成流程，理解手和物体的局部几何关系；将慢速仿真数据蒸馏到扩散模型中，使其能在相机点云上实时运行；结合互联网规模模型的全局语义理解与仿真基础的局部几何感知。

Result: 实现了无需手动训练数据的高性能语义抓取，结合了语义理解和几何精度。

Conclusion: 通过将互联网规模模型的语义能力与仿真基础的几何理解相结合，可以构建出能够精确控制灵巧机械手进行3D抓取的语义抓取系统。

Abstract: Large Vision Models trained on internet-scale data have demonstrated strong capabilities in segmenting and semantically understanding object parts, even in cluttered, crowded scenes. However, while these models can direct a robot toward the general region of an object, they lack the geometric understanding required to precisely control dexterous robotic hands for 3D grasping. To overcome this, our key insight is to leverage simulation with a force-closure grasping generation pipeline that understands local geometries of the hand and object in the scene. Because this pipeline is slow and requires ground-truth observations, the resulting data is distilled into a diffusion model that operates in real-time on camera point clouds. By combining the global semantic understanding of internet-scale models with the geometric precision of a simulation-based locally-aware force-closure, \our achieves high-performance semantic grasping without any manually collected training data. For visualizations of this please visit our website at https://ifgrasping.github.io/

</details>
