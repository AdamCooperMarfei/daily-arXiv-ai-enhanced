<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 28]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Environment-Aware and Human-Cooperative Swing Control for Lower-Limb Prostheses in Diverse Obstacle Scenarios](https://arxiv.org/abs/2507.01111)
*Haosen Xing,Haoran Ma,Sijin Zhang,Hartmut Geyer*

Main category: cs.RO

TL;DR: 提出一种新型控制策略，结合环境感知与用户意图，优化下肢假肢在复杂地形中的障碍物跨越能力。


<details>
  <summary>Details</summary>
Motivation: 现有下肢假肢控制策略缺乏对环境与用户意图的感知，尤其在复杂地形中表现不足。

Method: 使用机载深度相机实时检测障碍物，结合用户生物力学信号动态调整摆动轨迹。

Result: 实验显示，在150多次跨越和30多次踏上障碍物的测试中，成功率100%。

Conclusion: 该系统成功解决了障碍物导航问题，展现了适应环境与用户意图的潜力，适用于多种运动场景。

Abstract: Current control strategies for powered lower limb prostheses often lack
awareness of the environment and the user's intended interactions with it. This
limitation becomes particularly apparent in complex terrains. Obstacle
negotiation, a critical scenario exemplifying such challenges, requires both
real-time perception of obstacle geometry and responsiveness to user intention
about when and where to step over or onto, to dynamically adjust swing
trajectories. We propose a novel control strategy that fuses environmental
awareness and human cooperativeness: an on-board depth camera detects obstacles
ahead of swing phase, prompting an elevated early-swing trajectory to ensure
clearance, while late-swing control defers to natural biomechanical cues from
the user. This approach enables intuitive stepping strategies without requiring
unnatural movement patterns. Experiments with three non-amputee participants
demonstrated 100 percent success across more than 150 step-overs and 30
step-ons with randomly placed obstacles of varying heights (4-16 cm) and
distances (15-70 cm). By effectively addressing obstacle navigation -- a
gateway challenge for complex terrain mobility -- our system demonstrates
adaptability to both environmental constraints and user intentions, with
promising applications across diverse locomotion scenarios.

</details>


### [2] [VISTA: Open-Vocabulary, Task-Relevant Robot Exploration with Online Semantic Gaussian Splatting](https://arxiv.org/abs/2507.01125)
*Keiko Nagami,Timothy Chen,Javier Yu,Ola Shorinwa,Maximilian Adang,Carlyn Dougherty,Eric Cristofalo,Mac Schwager*

Main category: cs.RO

TL;DR: VISTA是一种机器人主动探索方法，通过语义任务感知规划轨迹，提升3D地图质量，专注于任务相关区域。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在开放词汇指令下高效探索环境并构建语义3D地图的问题。

Method: VISTA结合语义相似性和未探索区域优先级，规划轨迹，并引入高效的视角-语义覆盖度量。

Result: 在静态数据集上，VISTA在计算速度和重建质量上优于现有方法；硬件实验中，成功率显著提升。

Conclusion: VISTA平台无关，适用于多种机器人，性能优越且开源。

Abstract: We present VISTA (Viewpoint-based Image selection with Semantic Task
Awareness), an active exploration method for robots to plan informative
trajectories that improve 3D map quality in areas most relevant for task
completion. Given an open-vocabulary search instruction (e.g., "find a
person"), VISTA enables a robot to explore its environment to search for the
object of interest, while simultaneously building a real-time semantic 3D
Gaussian Splatting reconstruction of the scene. The robot navigates its
environment by planning receding-horizon trajectories that prioritize semantic
similarity to the query and exploration of unseen regions of the environment.
To evaluate trajectories, VISTA introduces a novel, efficient
viewpoint-semantic coverage metric that quantifies both the geometric view
diversity and task relevance in the 3D scene. On static datasets, our coverage
metric outperforms state-of-the-art baselines, FisherRF and Bayes' Rays, in
computation speed and reconstruction quality. In quadrotor hardware
experiments, VISTA achieves 6x higher success rates in challenging maps,
compared to baseline methods, while matching baseline performance in less
challenging maps. Lastly, we show that VISTA is platform-agnostic by deploying
it on a quadrotor drone and a Spot quadruped robot. Open-source code will be
released upon acceptance of the paper.

</details>


### [3] [A Review on Sound Source Localization in Robotics: Focusing on Deep Learning Methods](https://arxiv.org/abs/2507.01143)
*Reza Jalayer,Masoud Jalayer,Amirali Baniasadi*

Main category: cs.RO

TL;DR: 本文综述了机器人领域中的声源定位（SSL）技术，重点介绍了深度学习方法的最新进展，并探讨了当前挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有综述多关注通用音频应用，未充分考虑机器人领域的限制或深度学习的最新进展，本文旨在填补这一空白。

Method: 回顾了经典方法（如TDOA、波束成形等）和现代深度学习方法（如CNN、CRNN等），并分析了数据与训练策略。

Result: 总结了不同机器人类型和应用领域的研究，提出了当前SSL的挑战（如环境鲁棒性、多声源问题等）。

Conclusion: 为下一代机器人提供了实现鲁棒、高效、可解释的DL-based SSL的行动路线图。

Abstract: Sound source localization (SSL) adds a spatial dimension to auditory
perception, allowing a system to pinpoint the origin of speech, machinery
noise, warning tones, or other acoustic events, capabilities that facilitate
robot navigation, human-machine dialogue, and condition monitoring. While
existing surveys provide valuable historical context, they typically address
general audio applications and do not fully account for robotic constraints or
the latest advancements in deep learning. This review addresses these gaps by
offering a robotics-focused synthesis, emphasizing recent progress in deep
learning methodologies. We start by reviewing classical methods such as Time
Difference of Arrival (TDOA), beamforming, Steered-Response Power (SRP), and
subspace analysis. Subsequently, we delve into modern machine learning (ML) and
deep learning (DL) approaches, discussing traditional ML and neural networks
(NNs), convolutional neural networks (CNNs), convolutional recurrent neural
networks (CRNNs), and emerging attention-based architectures. The data and
training strategy that are the two cornerstones of DL-based SSL are explored.
Studies are further categorized by robot types and application domains to
facilitate researchers in identifying relevant work for their specific
contexts. Finally, we highlight the current challenges in SSL works in general,
regarding environmental robustness, sound source multiplicity, and specific
implementation constraints in robotics, as well as data and learning strategies
in DL-based SSL. Also, we sketch promising directions to offer an actionable
roadmap toward robust, adaptable, efficient, and explainable DL-based SSL for
next-generation robots.

</details>


### [4] [SonoGym: High Performance Simulation for Challenging Surgical Tasks with Robotic Ultrasound](https://arxiv.org/abs/2507.01152)
*Yunke Ao,Masoud Moghani,Mayank Mittal,Manish Prajapat,Luohong Wu,Frederic Giraud,Fabio Carrillo,Andreas Krause,Philipp Fürnstahl*

Main category: cs.RO

TL;DR: SonoGym是一个用于复杂机器人超声任务的模拟平台，支持深度强化学习和模仿学习，旨在解决缺乏真实模拟环境的问题。


<details>
  <summary>Details</summary>
Motivation: 机器人超声在复杂手术任务中的应用受限，主要因缺乏真实高效的模拟环境。

Method: 开发SonoGym平台，结合物理和生成建模方法，支持并行模拟，并集成深度强化学习和模仿学习算法。

Result: 成功训练多种策略，但现有方法在临床环境中仍有局限。

Conclusion: SonoGym有望推动机器人手术学习研究的发展。

Abstract: Ultrasound (US) is a widely used medical imaging modality due to its
real-time capabilities, non-invasive nature, and cost-effectiveness. Robotic
ultrasound can further enhance its utility by reducing operator dependence and
improving access to complex anatomical regions. For this, while deep
reinforcement learning (DRL) and imitation learning (IL) have shown potential
for autonomous navigation, their use in complex surgical tasks such as anatomy
reconstruction and surgical guidance remains limited -- largely due to the lack
of realistic and efficient simulation environments tailored to these tasks. We
introduce SonoGym, a scalable simulation platform for complex robotic
ultrasound tasks that enables parallel simulation across tens to hundreds of
environments. Our framework supports realistic and real-time simulation of US
data from CT-derived 3D models of the anatomy through both a physics-based and
a generative modeling approach. Sonogym enables the training of DRL and recent
IL agents (vision transformers and diffusion policies) for relevant tasks in
robotic orthopedic surgery by integrating common robotic platforms and
orthopedic end effectors. We further incorporate submodular DRL -- a recent
method that handles history-dependent rewards -- for anatomy reconstruction and
safe reinforcement learning for surgery. Our results demonstrate successful
policy learning across a range of scenarios, while also highlighting the
limitations of current methods in clinically relevant environments. We believe
our simulation can facilitate research in robot learning approaches for such
challenging robotic surgery applications. Dataset, codes, and videos are
publicly available at https://sonogym.github.io/.

</details>


### [5] [A Differentiable Distance Metric for Robotics Through Generalized Alternating Projection](https://arxiv.org/abs/2507.01181)
*Vinicius M. Gonçalves,Shiqing Wei,Eduardo Malacarne S. de Souza,Krishnamurthy Prashanth,Anthony Tzes,Farshad Khorrami*

Main category: cs.RO

TL;DR: 本文提出了一种新的可微距离度量方法，解决了现有方法在凸多面体上的复杂性和实用性不足的问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统欧几里得距离不可微，而现有可微距离度量方法存在复杂性和实用性不足的问题，本文旨在解决这些问题。

Method: 提出了更简单且实用的平滑投影表达式，适用于一般凸多面体，并确保距离在物体重叠时消失。

Result: 实验结果表明，该方法有效且实用。

Conclusion: 本文提出的可微距离度量方法简化了计算，提高了实用性，并通过公开的Python仿真包UAIBot提供了实现。

Abstract: In many robotics applications, it is necessary to compute not only the
distance between the robot and the environment, but also its derivative - for
example, when using control barrier functions. However, since the traditional
Euclidean distance is not differentiable, there is a need for alternative
distance metrics that possess this property. Recently, a metric with guaranteed
differentiability was proposed [1]. This approach has some important drawbacks,
which we address in this paper. We provide much simpler and practical
expressions for the smooth projection for general convex polytopes.
Additionally, as opposed to [1], we ensure that the distance vanishes as the
objects overlap. We show the efficacy of the approach in experimental results.
Our proposed distance metric is publicly available through the Python-based
simulation package UAIBot.

</details>


### [6] [Search-Based Robot Motion Planning With Distance-Based Adaptive Motion Primitives](https://arxiv.org/abs/2507.01198)
*Benjamin Kraljusic,Zlatan Ajanovic,Nermin Covic,Bakir Lacevic*

Main category: cs.RO

TL;DR: 提出了一种结合采样和搜索的运动规划算法，利用自适应运动基元（burs）提高效率。


<details>
  <summary>Details</summary>
Motivation: 解决固定尺寸运动基元在复杂场景中效率低下的问题，特别是高自由度机械臂。

Method: 在SMPL库中实现，利用burs作为自适应运动基元，扩展自由配置空间。

Result: 在复杂场景中表现优于固定基元方法，高自由度机械臂效果显著。

Conclusion: 自适应运动基元方法在复杂环境中更高效，适用于高自由度机械臂。

Abstract: This work proposes a motion planning algorithm for robotic manipulators that
combines sampling-based and search-based planning methods. The core
contribution of the proposed approach is the usage of burs of free
configuration space (C-space) as adaptive motion primitives within the graph
search algorithm. Due to their feature to adaptively expand in free C-space,
burs enable more efficient exploration of the configuration space compared to
fixed-sized motion primitives, significantly reducing the time to find a valid
path and the number of required expansions. The algorithm is implemented within
the existing SMPL (Search-Based Motion Planning Library) library and evaluated
through a series of different scenarios involving manipulators with varying
number of degrees-of-freedom (DoF) and environment complexity. Results
demonstrate that the bur-based approach outperforms fixed-primitive planning in
complex scenarios, particularly for high DoF manipulators, while achieving
comparable performance in simpler scenarios.

</details>


### [7] [2024 NASA SUITS Report: LLM-Driven Immersive Augmented Reality User Interface for Robotics and Space Exploration](https://arxiv.org/abs/2507.01206)
*Kathy Zhuang,Zixun Huang,Yukun Song,Rui Li,Yinuo Zhou,Allen Y. Yang*

Main category: cs.RO

TL;DR: 论文介绍了URSA系统，一个基于LLM的AR系统，用于NASA的SUITS挑战，整合了AR设备、语音控制和机器人跟踪算法，支持实时机器人控制和监控。


<details>
  <summary>Details</summary>
Motivation: 解决移动AR中的人机交互问题，特别是复杂动态环境中的3D物体姿态估计，以满足未来太空任务（如Artemis）的需求。

Method: 开发了URSA系统，结合头戴式AR设备、LLM驱动的语音控制和机器人跟踪算法，利用数字孪生定位技术和ZED2相机进行实时跟踪。

Result: 系统实现了非侵入式AR界面、专用数据集、任务可视化控制台、优化的6DoF姿态估计器和端到端集成，支持宇航员任务。

Conclusion: URSA系统推动了数字孪生在机器人中的应用，为航空航天和工业领域提供了可扩展的解决方案。

Abstract: As modern computing advances, new interaction paradigms have emerged,
particularly in Augmented Reality (AR), which overlays virtual interfaces onto
physical objects. This evolution poses challenges in machine perception,
especially for tasks like 3D object pose estimation in complex, dynamic
environments. Our project addresses critical issues in human-robot interaction
within mobile AR, focusing on non-intrusive, spatially aware interfaces. We
present URSA, an LLM-driven immersive AR system developed for NASA's 2023-2024
SUITS challenge, targeting future spaceflight needs such as the Artemis
missions. URSA integrates three core technologies: a head-mounted AR device
(e.g., HoloLens) for intuitive visual feedback, voice control powered by large
language models for hands-free interaction, and robot tracking algorithms that
enable accurate 3D localization in dynamic settings. To enhance precision, we
leverage digital twin localization technologies, using datasets like
DTTD-Mobile and specialized hardware such as the ZED2 camera for real-world
tracking under noise and occlusion. Our system enables real-time robot control
and monitoring via an AR interface, even in the absence of ground-truth
sensors--vital for hazardous or remote operations. Key contributions include:
(1) a non-intrusive AR interface with LLM-based voice input; (2) a ZED2-based
dataset tailored for non-rigid robotic bodies; (3) a Local Mission Control
Console (LMCC) for mission visualization; (4) a transformer-based 6DoF pose
estimator (DTTDNet) optimized for depth fusion and real-time tracking; and (5)
end-to-end integration for astronaut mission support. This work advances
digital twin applications in robotics, offering scalable solutions for both
aerospace and industrial domains.

</details>


### [8] [Jump-Start Reinforcement Learning with Self-Evolving Priors for Extreme Monopedal Locomotion](https://arxiv.org/abs/2507.01243)
*Ziang Zheng,Guojian Zhan,Shiqi Liu,Yao Lyu,Tao Zhang,Shengbo Eben Li*

Main category: cs.RO

TL;DR: JumpER是一种通过自进化先验分阶段训练的策略，帮助四足机器人在极端欠驱动和地形条件下实现稳健单足跳跃。


<details>
  <summary>Details</summary>
Motivation: 解决传统强化学习在极端欠驱动和地形条件下训练不稳定和奖励反馈不可靠的问题。

Method: 分阶段训练策略，动态生成自进化先验，逐步优化策略，无需外部专家或手工奖励设计。

Result: 成功实现四足机器人在不可预测地形上的稳健单足跳跃，克服了传统方法难以应对的挑战。

Conclusion: JumpER为极端条件下的运动任务提供了可扩展的解决方案。

Abstract: Reinforcement learning (RL) has shown great potential in enabling quadruped
robots to perform agile locomotion. However, directly training policies to
simultaneously handle dual extreme challenges, i.e., extreme underactuation and
extreme terrains, as in monopedal hopping tasks, remains highly challenging due
to unstable early-stage interactions and unreliable reward feedback. To address
this, we propose JumpER (jump-start reinforcement learning via self-evolving
priors), an RL training framework that structures policy learning into multiple
stages of increasing complexity. By dynamically generating self-evolving priors
through iterative bootstrapping of previously learned policies, JumpER
progressively refines and enhances guidance, thereby stabilizing exploration
and policy optimization without relying on external expert priors or
handcrafted reward shaping. Specifically, when integrated with a structured
three-stage curriculum that incrementally evolves action modality, observation
space, and task objective, JumpER enables quadruped robots to achieve robust
monopedal hopping on unpredictable terrains for the first time. Remarkably, the
resulting policy effectively handles challenging scenarios that traditional
methods struggle to conquer, including wide gaps up to 60 cm, irregularly
spaced stairs, and stepping stones with distances varying from 15 cm to 35 cm.
JumpER thus provides a principled and scalable approach for addressing
locomotion tasks under the dual challenges of extreme underactuation and
extreme terrains.

</details>


### [9] [LLM-based Realistic Safety-Critical Driving Video Generation](https://arxiv.org/abs/2507.01264)
*Yongjie Fu,Ruijian Zha,Pei Tian,Xuan Di*

Main category: cs.RO

TL;DR: 提出了一种利用大型语言模型（LLMs）生成驾驶场景代码的框架，结合CARLA模拟器和视频生成技术，用于自动合成多样且安全关键的驾驶场景。


<details>
  <summary>Details</summary>
Motivation: 设计多样且安全关键的驾驶场景对评估自动驾驶系统至关重要，但手动设计耗时且难以覆盖边缘情况。

Method: 利用LLMs进行少样本代码生成，结合CARLA模拟器和ControlNet视频生成技术，自动生成场景脚本和逼真视频。

Result: 实验表明，该方法能高效生成多样、逼真且安全关键的驾驶场景，包括罕见边缘情况。

Conclusion: 该框架为自动驾驶系统的仿真测试提供了高效且可控的工具。

Abstract: Designing diverse and safety-critical driving scenarios is essential for
evaluating autonomous driving systems. In this paper, we propose a novel
framework that leverages Large Language Models (LLMs) for few-shot code
generation to automatically synthesize driving scenarios within the CARLA
simulator, which has flexibility in scenario scripting, efficient code-based
control of traffic participants, and enforcement of realistic physical
dynamics. Given a few example prompts and code samples, the LLM generates
safety-critical scenario scripts that specify the behavior and placement of
traffic participants, with a particular focus on collision events. To bridge
the gap between simulation and real-world appearance, we integrate a video
generation pipeline using Cosmos-Transfer1 with ControlNet, which converts
rendered scenes into realistic driving videos. Our approach enables
controllable scenario generation and facilitates the creation of rare but
critical edge cases, such as pedestrian crossings under occlusion or sudden
vehicle cut-ins. Experimental results demonstrate the effectiveness of our
method in generating a wide range of realistic, diverse, and safety-critical
scenarios, offering a promising tool for simulation-based testing of autonomous
vehicles.

</details>


### [10] [VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process](https://arxiv.org/abs/2507.01284)
*Cristian Gariboldi,Hayato Tokida,Ken Kinjo,Yuki Asada,Alexander Carballo*

Main category: cs.RO

TL;DR: VLAD模型通过结合视觉语言模型（VLM）与自动驾驶系统（VAD），提升了自动驾驶的感知与规划能力，并生成可解释的驾驶决策。


<details>
  <summary>Details</summary>
Motivation: 利用开源视觉语言模型（如LLaVA、Qwen-VL）的通用知识，增强自动驾驶系统的性能与透明度。

Method: 采用定制问答数据集对VLM进行微调，提升空间推理能力，生成高级导航指令供VAD处理。

Result: 在nuScenes数据集上，碰撞率降低31.82%，性能优于基线方法。

Conclusion: VLAD为VLM增强的自动驾驶系统设定了新基准，同时提高了系统的可解释性。

Abstract: Recent advancements in open-source Visual Language Models (VLMs) such as
LLaVA, Qwen-VL, and Llama have catalyzed extensive research on their
integration with diverse systems. The internet-scale general knowledge
encapsulated within these models presents significant opportunities for
enhancing autonomous driving perception, prediction, and planning capabilities.
In this paper we propose VLAD, a vision-language autonomous driving model,
which integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end
system. We implement a specialized fine-tuning approach using custom
question-answer datasets designed specifically to improve the spatial reasoning
capabilities of the model. The enhanced VLM generates high-level navigational
commands that VAD subsequently processes to guide vehicle operation.
Additionally, our system produces interpretable natural language explanations
of driving decisions, thereby increasing transparency and trustworthiness of
the traditionally black-box end-to-end architecture. Comprehensive evaluation
on the real-world nuScenes dataset demonstrates that our integrated system
reduces average collision rates by 31.82% compared to baseline methodologies,
establishing a new benchmark for VLM-augmented autonomous driving systems.

</details>


### [11] [LANet: A Lane Boundaries-Aware Approach For Robust Trajectory Prediction](https://arxiv.org/abs/2507.01308)
*Muhammad Atta ur Rahman,Dooseop Choi,KyoungWook Min*

Main category: cs.RO

TL;DR: 提出了一种基于多向量地图元素的运动预测模型，通过融合车道边界和道路边缘等信息，提升自动驾驶车辆在复杂交通场景中的轨迹预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有运动预测模型主要依赖车道中心线，无法充分捕捉道路环境和交通规则，限制了预测的准确性和全面性。

Method: 开发了一种特征融合策略，结合不同向量地图组件的信息，并引入剪枝机制以优化计算效率。

Result: 在Argoverse 2数据集上验证了方法的有效性，性能优于现有模型。

Conclusion: 该方法通过更全面的道路环境表示和高效的计算机制，提升了自动驾驶运动预测的先进性和实用性。

Abstract: Accurate motion forecasting is critical for safe and efficient autonomous
driving, enabling vehicles to predict future trajectories and make informed
decisions in complex traffic scenarios. Most of the current designs of motion
prediction models are based on the major representation of lane centerlines,
which limits their capability to capture critical road environments and traffic
rules and constraints. In this work, we propose an enhanced motion forecasting
model informed by multiple vector map elements, including lane boundaries and
road edges, that facilitates a richer and more complete representation of
driving environments. An effective feature fusion strategy is developed to
merge information in different vector map components, where the model learns
holistic information on road structures and their interactions with agents.
Since encoding more information about the road environment increases memory
usage and is computationally expensive, we developed an effective pruning
mechanism that filters the most relevant map connections to the target agent,
ensuring computational efficiency while maintaining essential spatial and
semantic relationships for accurate trajectory prediction. Overcoming the
limitations of lane centerline-based models, our method provides a more
informative and efficient representation of the driving environment and
advances the state of the art for autonomous vehicle motion forecasting. We
verify our approach with extensive experiments on the Argoverse 2 motion
forecasting dataset, where our method maintains competitiveness on AV2 while
achieving improved performance.
  Index Terms-Autonomous driving, trajectory prediction, vector map elements,
road topology, connection pruning, Argoverse 2.

</details>


### [12] [TriVLA: A Unified Triple-System-Based Unified Vision-Language-Action Model for General Robot Control](https://arxiv.org/abs/2507.01424)
*Zhenyang Liu,Yongchong Gu,Sixiao Zheng,Xiangyang Xue,Yanwei Fu*

Main category: cs.RO

TL;DR: TriVLA是一种统一的三系统架构视觉-语言-动作模型，用于通用机器人控制，通过动态感知模块捕捉静态和动态信息，显著优于现有模仿学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有自回归VLA方法常忽略动态信息，而动态信息对具身任务至关重要。

Method: TriVLA采用三系统架构，包括视觉-语言模块、动态感知模块和策略学习模块，结合预训练模型和机器人数据集。

Result: TriVLA以约36 Hz运行，在仿真和真实世界任务中超越现有模仿学习方法。

Conclusion: TriVLA通过动态感知和统一架构，显著提升了机器人控制的性能。

Abstract: Recent advancements in vision-language models (VLMs) for common-sense
reasoning have led to the development of vision-language-action (VLA) models,
enabling robots to perform generalized manipulation. Although existing
autoregressive VLA methods design a specific architecture like dual-system to
leverage large-scale pretrained knowledge, they tend to capture static
information, often neglecting the dynamic aspects vital for embodied tasks. To
this end, we propose TriVLA, a unified Vision-Language-Action model with a
triple-system architecture for general robot control. The vision-language
module (System 2) interprets the environment through vision and language
instructions. The dynamics perception module (System 3) inherently produces
visual representations that encompass both current static information and
predicted future dynamics, thereby providing valuable guidance for policy
learning. TriVLA utilizes pre-trained VLM model and fine-tunes pre-trained
video foundation model on robot datasets along with internet human manipulation
data. The subsequent policy learning module (System 1) generates fluid motor
actions in real time. Experimental evaluation demonstrates that TriVLA operates
at approximately 36 Hz and surpasses state-of-the-art imitation learning
baselines on standard simulation benchmarks as well as challenging real-world
manipulation tasks.

</details>


### [13] [Approximation-free Control of Unknown Euler-Lagrangian Systems under Input Constraints](https://arxiv.org/abs/2507.01426)
*Ratnangshu Das,Pushpak Jagtap*

Main category: cs.RO

TL;DR: 提出了一种新型漏斗跟踪控制算法，用于处理未知动态和输入约束的机器人系统，通过两种无近似控制策略确保跟踪误差在预设范围内。


<details>
  <summary>Details</summary>
Motivation: 解决机器人系统中性能与执行器安全性之间的权衡问题，特别是在输入受限的情况下。

Method: 采用欧拉-拉格朗日建模方法，提出两种无近似控制策略：一种主动纠正误差，另一种阻止进一步偏离。

Result: 通过仿真和实验验证了算法的鲁棒性能和安全性。

Conclusion: 该研究显著提升了漏斗控制在输入受限的实际机器人系统中的应用性。

Abstract: In this paper, we present a novel funnel-based tracking control algorithm for
robotic systems with unknown dynamics and prescribed input constraints. The
Euler-Lagrange formulation, a common modeling approach for robotic systems, has
been adopted in this study to address the trade-off between performance and
actuator safety. We establish feasibility conditions that ensure tracking
errors evolve within predefined funnel bounds while maintaining bounded control
efforts, a crucial consideration for robots with limited actuation
capabilities. We propose two approximation-free control strategies for
scenarios where these conditions are violated: one actively corrects the error,
and the other stops further deviation. Finally, we demonstrate the robust
performance and safety of the approach through simulations and experimental
validations. This work represents a significant advancement in funnel-based
control, enhancing its applicability to real-world robotics systems with input
constraints.

</details>


### [14] [Quantum-Assisted Automatic Path-Planning for Robotic Quality Inspection in Industry 4.0](https://arxiv.org/abs/2507.01462)
*Eneko Osaba,Estibaliz Garrote,Pablo Miranda-Rodriguez,Alessia Ciacco,Itziar Cabanes,Aitziber Mancisidor*

Main category: cs.RO

TL;DR: 研究探讨了混合量子-经典算法在优化工业环境中基于CAD模型的机器人检测轨迹中的应用，结果显示量子方法在计算时间上显著优于经典方法。


<details>
  <summary>Details</summary>
Motivation: 探索量子计算在工业自动化（如Industry 4.0）中的潜力，特别是在优化机器人检测路径方面。

Method: 将任务建模为3D旅行商问题的变体，比较D-Wave量子求解器与经典方法（如GUROBI和Google OR-Tools）的性能。

Result: 在五个实际案例中，量子方法在解质量和计算时间上表现优异。

Conclusion: 量子方法在工业自动化中具有显著潜力，尤其是在优化复杂路径问题上。

Abstract: This work explores the application of hybrid quantum-classical algorithms to
optimize robotic inspection trajectories derived from Computer-Aided Design
(CAD) models in industrial settings. By modeling the task as a 3D variant of
the Traveling Salesman Problem, incorporating incomplete graphs and open-route
constraints, this study evaluates the performance of two D-Wave-based solvers
against classical methods such as GUROBI and Google OR-Tools. Results across
five real-world cases demonstrate competitive solution quality with
significantly reduced computation times, highlighting the potential of quantum
approaches in automation under Industry 4.0.

</details>


### [15] [BioMARS: A Multi-Agent Robotic System for Autonomous Biological Experiments](https://arxiv.org/abs/2507.01485)
*Yibo Qiu,Zan Huang,Zhiyu Wang,Handi Liu,Yiling Qiao,Yifeng Hu,Shu'ang Sun,Hangke Peng,Ronald X Xu,Mingzhai Sun*

Main category: cs.RO

TL;DR: BioMARS是一个结合大型语言模型（LLMs）、视觉语言模型（VLMs）和模块化机器人的智能平台，用于自主设计、规划和执行生物实验。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs和VLMs在生物研究中的应用受限于僵化的协议设计、动态实验室条件的适应性不足、错误处理能力有限以及操作复杂性高。

Method: BioMARS采用分层架构：Biologist Agent通过检索增强生成合成协议；Technician Agent将其转化为可执行的机器人伪代码；Inspector Agent通过多模态感知和异常检测确保程序完整性。

Result: 系统在细胞传代和培养任务中表现优异，匹配或超越人工操作的存活率、一致性和形态完整性，并在视网膜色素上皮细胞分化中优于传统策略。

Conclusion: BioMARS展示了通用化AI驱动实验室自动化的可行性，以及基于语言推理在生物研究中的变革性作用。

Abstract: Large language models (LLMs) and vision-language models (VLMs) have the
potential to transform biological research by enabling autonomous
experimentation. Yet, their application remains constrained by rigid protocol
design, limited adaptability to dynamic lab conditions, inadequate error
handling, and high operational complexity. Here we introduce BioMARS
(Biological Multi-Agent Robotic System), an intelligent platform that
integrates LLMs, VLMs, and modular robotics to autonomously design, plan, and
execute biological experiments. BioMARS uses a hierarchical architecture: the
Biologist Agent synthesizes protocols via retrieval-augmented generation; the
Technician Agent translates them into executable robotic pseudo-code; and the
Inspector Agent ensures procedural integrity through multimodal perception and
anomaly detection. The system autonomously conducts cell passaging and culture
tasks, matching or exceeding manual performance in viability, consistency, and
morphological integrity. It also supports context-aware optimization,
outperforming conventional strategies in differentiating retinal pigment
epithelial cells. A web interface enables real-time human-AI collaboration,
while a modular backend allows scalable integration with laboratory hardware.
These results highlight the feasibility of generalizable, AI-driven laboratory
automation and the transformative role of language-based reasoning in
biological research.

</details>


### [16] [Dynamic System Model Generation for Online Fault Detection and Diagnosis of Robotic Systems](https://arxiv.org/abs/2507.01550)
*Johannes Kohl,Georg Muck,Georg Jäger,Sebastian Zug*

Main category: cs.RO

TL;DR: 提出了一种动态生成系统模型的方法，用于机器人系统的故障检测与诊断，减少对预定义模型和历史数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 随着机器人系统复杂度的增加，传统故障检测与诊断方法因依赖预定义模型和历史数据而难以适应动态变化。

Method: 在运行时动态生成系统模型，并利用该模型定位故障根源，适用于具有相似软件设计的各类机器人系统。

Result: 该方法减少了对专家干预的需求，同时实现了较低的运行开销。

Conclusion: 提出的概念为动态机器人系统提供了一种高效的故障检测与诊断解决方案。

Abstract: With the rapid development of more complex robots, Fault Detection and
Diagnosis (FDD) becomes increasingly harder. Especially the need for
predetermined models and historic data is problematic because they do not
encompass the dynamic and fast-changing nature of such systems. To this end, we
propose a concept that actively generates a dynamic system model at runtime and
utilizes it to locate root causes. The goal is to be applicable to all kinds of
robotic systems that share a similar software design. Additionally, it should
exhibit minimal overhead and enhance independence from expert attention.

</details>


### [17] [Self-Closing Suction Grippers for Industrial Grasping via Form-Flexible Design](https://arxiv.org/abs/2507.01561)
*Huijiang Wang,Holger Kunz,Timon Adler,Fumiya Iida*

Main category: cs.RO

TL;DR: 提出了一种基于混合堵塞和吸力机制的形变柔性夹具，用于自适应抓取，能够处理尺寸差异大的物体。


<details>
  <summary>Details</summary>
Motivation: 传统夹具难以适应尺寸变化大的物体，需要一种更灵活的设计。

Method: 采用混合堵塞和吸力机制，通过被动形变机制实现自适应抓取。

Result: 夹具能够抓取小至54.5%孔径的物体，最大负载质量比为94.3。

Conclusion: 混合夹具展示了在工业抓取中的高效性和适应性。

Abstract: Shape-morphing robots have shown benefits in industrial grasping. We propose
form-flexible grippers for adaptive grasping. The design is based on the hybrid
jamming and suction mechanism, which deforms to handle objects that vary
significantly in size from the aperture, including both larger and smaller
parts. Compared with traditional grippers, the gripper achieves self-closing to
form an airtight seal. Under a vacuum, a wide range of grasping is realized
through the passive morphing mechanism at the interface that harmonizes
pressure and flow rate. This hybrid gripper showcases the capability to
securely grasp an egg, as small as 54.5% of its aperture, while achieving a
maximum load-to-mass ratio of 94.3.

</details>


### [18] [An RRT* algorithm based on Riemannian metric model for optimal path planning](https://arxiv.org/abs/2507.01697)
*Yu Zhang,Qi Zhou,Xiao-Song Yang*

Main category: cs.RO

TL;DR: 提出了一种基于黎曼度量的模型，用于解决高维空间中二维光滑子流形上的最优路径规划问题，通过构建新的黎曼度量将问题转化为二维平面上的几何问题，并提出了RRT*-R算法。


<details>
  <summary>Details</summary>
Motivation: 解决高维空间中机器人路径规划问题，尤其是在复杂环境下避免高度变化、地面阻力等因素的影响。

Method: 构建新的黎曼度量，将高维空间问题转化为二维平面上的几何问题，并提出增量算法RRT*-R。

Result: RRT*-R算法在复杂环境下表现优异，路径平滑且优化效果优于原始RRT*算法，路径长度接近理论最小测地距离。

Conclusion: RRT*-R算法在高维空间路径规划中具有更好的适应性和优化性能。

Abstract: This paper presents a Riemannian metric-based model to solve the optimal path
planning problem on two-dimensional smooth submanifolds in high-dimensional
space. Our model is based on constructing a new Riemannian metric on a
two-dimensional projection plane, which is induced by the high-dimensional
Euclidean metric on two-dimensional smooth submanifold and reflects the
environmental information of the robot. The optimal path planning problem in
high-dimensional space is therefore transformed into a geometric problem on the
two-dimensional plane with new Riemannian metric. Based on the new Riemannian
metric, we proposed an incremental algorithm RRT*-R on the projection plane.
The experimental results show that the proposed algorithm is suitable for
scenarios with uneven fields in multiple dimensions. The proposed algorithm can
help the robot to effectively avoid areas with drastic changes in height,
ground resistance and other environmental factors. More importantly, the RRT*-R
algorithm shows better smoothness and optimization properties compared with the
original RRT* algorithm using Euclidean distance in high-dimensional workspace.
The length of the entire path by RRT*-R is a good approximation of the
theoretical minimum geodesic distance on projection plane.

</details>


### [19] [Efficient Collision Detection for Long and Slender Robotic Links in Euclidean Distance Fields: Application to a Forestry Crane](https://arxiv.org/abs/2507.01705)
*Marc-Philip Ecker,Bernhard Bischof,Minh Nhat Vu,Christoph Fröhlich,Tobias Glück,Wolfgang Kemmetmüller*

Main category: cs.RO

TL;DR: 提出了一种针对细长机械臂的新型碰撞检测算法，显著提高了运动规划的计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统球形近似方法对于细长机械臂效率低且不准确，需要改进。

Method: 利用机械臂的细长结构设计新型碰撞检测算法，无需调整近似精度参数。

Result: 在真实LiDAR数据和模拟环境中验证了算法的有效性。

Conclusion: 新算法显著提升了计算效率，适用于大规模细长机械臂的运动规划。

Abstract: Collision-free motion planning in complex outdoor environments relies heavily
on perceiving the surroundings through exteroceptive sensors. A widely used
approach represents the environment as a voxelized Euclidean distance field,
where robots are typically approximated by spheres. However, for large-scale
manipulators such as forestry cranes, which feature long and slender links,
this conventional spherical approximation becomes inefficient and inaccurate.
This work presents a novel collision detection algorithm specifically designed
to exploit the elongated structure of such manipulators, significantly
enhancing the computational efficiency of motion planning algorithms. Unlike
traditional sphere decomposition methods, our approach not only improves
computational efficiency but also naturally eliminates the need to fine-tune
the approximation accuracy as an additional parameter. We validate the
algorithm's effectiveness using real-world LiDAR data from a forestry crane
application, as well as simulated environment data.

</details>


### [20] [SE(3)-Equivariant Diffusion Policy in Spherical Fourier Space](https://arxiv.org/abs/2507.01723)
*Xupeng Zhu,Fan Wang,Robin Walters,Jane Shi*

Main category: cs.RO

TL;DR: 论文提出了一种SE(3)等变的扩散策略（SDP），通过将状态、动作和去噪过程嵌入球面傅里叶空间，实现了对3D场景变换的适应性，显著提升了泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散策略在3D空间中物体新排列时泛化能力差，影响实际性能。

Method: 提出SDP，利用球面傅里叶空间嵌入和球面FiLM层，实现SE(3)等变；设计了球面去噪时序U-net，高效实现时空等变。

Result: 在20个仿真任务和5个物理机器人任务中，SDP表现优于基线方法。

Conclusion: SDP通过SE(3)等变性，显著提升了在变换3D场景中的鲁棒性和泛化能力。

Abstract: Diffusion Policies are effective at learning closed-loop manipulation
policies from human demonstrations but generalize poorly to novel arrangements
of objects in 3D space, hurting real-world performance. To address this issue,
we propose Spherical Diffusion Policy (SDP), an SE(3) equivariant diffusion
policy that adapts trajectories according to 3D transformations of the scene.
Such equivariance is achieved by embedding the states, actions, and the
denoising process in spherical Fourier space. Additionally, we employ novel
spherical FiLM layers to condition the action denoising process equivariantly
on the scene embeddings. Lastly, we propose a spherical denoising temporal
U-net that achieves spatiotemporal equivariance with computational efficiency.
In the end, SDP is end-to-end SE(3) equivariant, allowing robust generalization
across transformed 3D scenes. SDP demonstrates a large performance improvement
over strong baselines in 20 simulation tasks and 5 physical robot tasks
including single-arm and bi-manual embodiments. Code is available at
https://github.com/amazon-science/Spherical_Diffusion_Policy.

</details>


### [21] [Augmented Bridge Spinal Fixation: A New Concept for Addressing Pedicle Screw Pullout via a Steerable Drilling Robot and Flexible Pedicle Screws](https://arxiv.org/abs/2507.01753)
*Yash Kulkarni,Susheela Sharma,Omid Rezayof,Siddhartha Kapuria,Jordan P. Amadio,Mohsen Khadem,Maryam Tilton,Farshid Alambeigi*

Main category: cs.RO

TL;DR: 提出了一种新型脊柱固定技术AB-SF，通过机器人钻孔和柔性螺钉结合骨水泥增强固定效果。


<details>
  <summary>Details</summary>
Motivation: 解决传统刚性椎弓根螺钉在脊柱固定中的松动和拔出问题。

Method: 使用CT-SDR机器人钻孔形成J形隧道，植入FPS柔性螺钉并注入骨水泥加固。

Result: 实验成功模拟了不同深度的隧道钻孔和骨水泥增强过程。

Conclusion: AB-SF技术具有可行性，能有效增强脊柱固定强度。

Abstract: To address the screw loosening and pullout limitations of rigid pedicle
screws in spinal fixation procedures, and to leverage our recently developed
Concentric Tube Steerable Drilling Robot (CT-SDR) and Flexible Pedicle Screw
(FPS), in this paper, we introduce the concept of Augmented Bridge Spinal
Fixation (AB-SF). In this concept, two connecting J-shape tunnels are first
drilled through pedicles of vertebra using the CT-SDR. Next, two FPSs are
passed through this tunnel and bone cement is then injected through the
cannulated region of the FPS to form an augmented bridge between two pedicles
and reinforce strength of the fixated spine. To experimentally analyze and
study the feasibility of AB-SF technique, we first used our robotic system
(i.e., a CT-SDR integrated with a robotic arm) to create two different fixation
scenarios in which two J-shape tunnels, forming a bridge, were drilled at
different depth of a vertebral phantom. Next, we implanted two FPSs within the
drilled tunnels and then successfully simulated the bone cement augmentation
process.

</details>


### [22] [S3D: A Spatial Steerable Surgical Drilling Framework for Robotic Spinal Fixation Procedures](https://arxiv.org/abs/2507.01779)
*Daniyal Maroufi,Xinyuan Huang,Yash Kulkarni,Omid Rezayof,Susheela Sharma,Vaibhav Goggela,Jordan P. Amadio,Mohsen Khadem,Farshid Alambeigi*

Main category: cs.RO

TL;DR: S3D是一个空间可操控的手术钻孔框架，用于机器人脊柱固定手术，通过改进的CT-SDR和四阶段校准、注册和导航程序实现。


<details>
  <summary>Details</summary>
Motivation: 解决脊柱固定手术中解剖学限制下的可操控钻孔问题。

Method: 改进CT-SDR，提出四阶段校准、注册和导航程序，结合七自由度机器人操作器。

Result: 在脊柱模型上验证了平面和非平面可操控钻孔功能。

Conclusion: S3D框架成功实现了脊柱固定手术中的可操控钻孔。

Abstract: In this paper, we introduce S3D: A Spatial Steerable Surgical Drilling
Framework for Robotic Spinal Fixation Procedures. S3D is designed to enable
realistic steerable drilling while accounting for the anatomical constraints
associated with vertebral access in spinal fixation (SF) procedures. To achieve
this, we first enhanced our previously designed concentric tube Steerable
Drilling Robot (CT-SDR) to facilitate steerable drilling across all vertebral
levels of the spinal column. Additionally, we propose a four-Phase calibration,
registration, and navigation procedure to perform realistic SF procedures on a
spine holder phantom by integrating the CT-SDR with a seven-degree-of-freedom
robotic manipulator. The functionality of this framework is validated through
planar and out-of-plane steerable drilling experiments in vertebral phantoms.

</details>


### [23] [Towards Design and Development of a Concentric Tube Steerable Drilling Robot for Creating S-shape Tunnels for Pelvic Fixation Procedures](https://arxiv.org/abs/2507.01811)
*Yash Kulkarni,Susheela Sharma,Sarah Go,Jordan P. Amadio,Mohsen Khadem,Farshid Alambeigi*

Main category: cs.RO

TL;DR: 提出了一种新型4自由度骨盆同心管可转向钻孔机器人（pelvic CT-SDR），用于解决传统刚性钻孔工具在骨盆固定中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统刚性钻孔工具限制了螺钉的放置路径，导致手术并发症增加，如螺钉错位、手术时间延长和辐射暴露增加。

Method: 设计并开发了一种4自由度的骨盆同心管可转向钻孔机器人，能够沿骨盆自然曲率进行S形钻孔。

Result: 通过模拟骨模型的S形钻孔实验验证了该机器人的性能。

Conclusion: 骨盆CT-SDR能够优化螺钉放置路径，减少手术并发症。

Abstract: Current pelvic fixation techniques rely on rigid drilling tools, which
inherently constrain the placement of rigid medical screws in the complex
anatomy of pelvis. These constraints prevent medical screws from following
anatomically optimal pathways and force clinicians to fixate screws in linear
trajectories. This suboptimal approach, combined with the unnatural placement
of the excessively long screws, lead to complications such as screw
misplacement, extended surgery times, and increased radiation exposure due to
repeated X-ray images taken ensure to safety of procedure. To address these
challenges, in this paper, we present the design and development of a unique 4
degree-of-freedom (DoF) pelvic concentric tube steerable drilling robot (pelvic
CT-SDR). The pelvic CT-SDR is capable of creating long S-shaped drilling
trajectories that follow the natural curvatures of the pelvic anatomy. The
performance of the pelvic CT-SDR was thoroughly evaluated through several
S-shape drilling experiments in simulated bone phantoms.

</details>


### [24] [MoIRA: Modular Instruction Routing Architecture for Multi-Task Robotics](https://arxiv.org/abs/2507.01843)
*Dmytro Kuzmenko,Nadiya Shvai*

Main category: cs.RO

TL;DR: MoIRA是一种模块化的Mixture-of-Experts框架，通过外部文本路由器协调专家，提供零样本路由选项，并在机器人任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决传统MoE架构中无法选择性定制专家和路由器的问题，同时减少额外训练需求。

Method: 提出MoIRA框架，采用嵌入相似性和提示驱动的语言模型推理作为零样本路由选项，结合低秩适配器进行低开销推理。

Result: 在GR1 Humanoid和LIBERO基准测试中表现优于通用模型，并与其他MoE管道竞争。

Conclusion: MoIRA展示了模块化部署的可行性，为未来多专家机器人系统提供了可扩展的基础。

Abstract: Mixture-of-Experts (MoE) approaches have recently gained traction in robotics
applications due to their ability to dynamically allocate computational
resources and specialize sub-networks for distinct tasks or environmental
contexts, enabling more efficient decision-making. Such systems often comprise
sparsely activated experts combined under a single monolithic architecture and
require a well-configured internal routing mechanism, which does not allow for
selective low-level expert and router customization and requires additional
training. We propose MoIRA, an architecture-agnostic modular MoE framework
designed to coordinate existing experts with an external text-based router.
MoIRA incorporates two zero-shot routing options: embedding-based similarity
and prompt-driven language model inference. In our experiments, we choose large
Vision-Language-Action models, gr00t-N1 and $\pi_0$, as the underlying experts,
and train low-rank adapters for low-overhead inference. We evaluate MoIRA on
various GR1 Humanoid tasks and LIBERO Spatial and Goal benchmarks, where it
consistently outperforms generalist models and competes with other MoE
pipelines. Additionally, we analyse the robustness of the proposed approach to
the variations of the instructions. While relying solely on textual
descriptions of tasks and experts, MoIRA demonstrates the practical viability
of modular deployment with precise, low-effort routing and provides an
alternative, scalable foundation for future multi-expert robotic systems.

</details>


### [25] [TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types](https://arxiv.org/abs/2507.01857)
*Yuhao Lin,Yi-Lin Wei,Haoran Liao,Mu Lin,Chengyi Xing,Hao Li,Dandan Zhang,Mark Cutkosky,Wei-Shi Zheng*

Main category: cs.RO

TL;DR: TypeTele是一种类型引导的灵巧遥操作系统，通过引入灵巧操作类型库和MLLM辅助检索模块，使灵巧手能够执行不受人类动作模式限制的任务。


<details>
  <summary>Details</summary>
Motivation: 现有灵巧遥操作主要依赖手部重定向模仿人类手部姿势，未能充分利用灵巧手的结构优势。

Method: 提出TypeTele系统，构建灵巧操作类型库，并采用MLLM辅助检索模块匹配任务需求。

Result: 实验表明，该系统显著提升了灵巧手执行多样复杂任务的成功率。

Conclusion: TypeTele通过类型引导和MLLM辅助，充分发挥了灵巧手的潜力。

Abstract: Dexterous teleoperation plays a crucial role in robotic manipulation for
real-world data collection and remote robot control. Previous dexterous
teleoperation mostly relies on hand retargeting to closely mimic human hand
postures. However, these approaches may fail to fully leverage the inherent
dexterity of dexterous hands, which can execute unique actions through their
structural advantages compared to human hands. To address this limitation, we
propose TypeTele, a type-guided dexterous teleoperation system, which enables
dexterous hands to perform actions that are not constrained by human motion
patterns. This is achieved by introducing dexterous manipulation types into the
teleoperation system, allowing operators to employ appropriate types to
complete specific tasks. To support this system, we build an extensible
dexterous manipulation type library to cover comprehensive dexterous postures
used in manipulation tasks. During teleoperation, we employ a MLLM
(Multi-modality Large Language Model)-assisted type retrieval module to
identify the most suitable manipulation type based on the specific task and
operator commands. Extensive experiments of real-world teleoperation and
imitation learning demonstrate that the incorporation of manipulation types
significantly takes full advantage of the dexterous robot's ability to perform
diverse and complex tasks with higher success rates.

</details>


### [26] [A Survey on Vision-Language-Action Models: An Action Tokenization Perspective](https://arxiv.org/abs/2507.01925)
*Yifan Zhong,Fengshuo Bai,Shaofei Cai,Xuchuan Huang,Zhang Chen,Xiaowei Zhang,Yuanfei Wang,Shaoyang Guo,Tianrui Guan,Ka Nam Lui,Zhiquan Qi,Yitao Liang,Yuanpei Chen,Yaodong Yang*

Main category: cs.RO

TL;DR: 该论文提出了一种统一的框架来理解视觉-语言-动作（VLA）模型，重点关注动作令牌的分类和分析，旨在推动VLA模型的发展。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型的研究缺乏对动作令牌的全面理解，阻碍了有效开发和未来方向的明确。

Method: 通过分类和解释现有VLA研究中的动作令牌类型，分析每种类型的优缺点。

Result: 总结了VLA模型的现状，并指出了改进方向。

Conclusion: 论文为VLA模型的未来发展提供了指导，并强调了未探索但有前景的方向。

Abstract: The remarkable advancements of vision and language foundation models in
multimodal understanding, reasoning, and generation has sparked growing efforts
to extend such intelligence to the physical world, fueling the flourishing of
vision-language-action (VLA) models. Despite seemingly diverse approaches, we
observe that current VLA models can be unified under a single framework: vision
and language inputs are processed by a series of VLA modules, producing a chain
of \textit{action tokens} that progressively encode more grounded and
actionable information, ultimately generating executable actions. We further
determine that the primary design choice distinguishing VLA models lies in how
action tokens are formulated, which can be categorized into language
description, code, affordance, trajectory, goal state, latent representation,
raw action, and reasoning. However, there remains a lack of comprehensive
understanding regarding action tokens, significantly impeding effective VLA
development and obscuring future directions. Therefore, this survey aims to
categorize and interpret existing VLA research through the lens of action
tokenization, distill the strengths and limitations of each token type, and
identify areas for improvement. Through this systematic review and analysis, we
offer a synthesized outlook on the broader evolution of VLA models, highlight
underexplored yet promising directions, and contribute guidance for future
research, hoping to bring the field closer to general-purpose intelligence.

</details>


### [27] [Large Language Model-Driven Closed-Loop UAV Operation with Semantic Observations](https://arxiv.org/abs/2507.01930)
*Wenhao Wang,Yanyan Li,Long Jiao,Jiawei Yuan*

Main category: cs.RO

TL;DR: 提出了一种基于LLM的闭环控制框架，通过代码生成器和评估器模块实现可靠的无人机操作，利用自然语言轨迹描述和模拟优化提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在无人机操作中逻辑推理和复杂决策的可靠性问题。

Method: 使用两个LLM模块（代码生成器和评估器），将无人机状态转化为自然语言描述，并通过模拟优化避免物理风险。

Result: 实验表明，该框架在任务复杂度增加时显著优于基线方法，成功率和完整性更高。

Conclusion: 该框架为LLM驱动的无人机操作提供了可靠解决方案。

Abstract: Large Language Models (LLMs) have revolutionized robotic autonomy, including
Unmanned Aerial Vehicles (UAVs). Recent studies have demonstrated the potential
of LLMs for translating human instructions into executable control code for UAV
operations. However, LLMs still face challenges from logical reasoning and
complex decision-making, leading to concerns about the reliability of
LLM-driven UAV operations. In this paper, we propose a LLM-driven closed-loop
control framework that enables reliable UAV operations powered by effective
feedback and refinement using two LLM modules, i.e., a Code Generator and an
Evaluator. Our framework transforms numerical state observations from UAV
operations into natural language trajectory descriptions to enhance the
evaluator LLM's understanding of UAV dynamics for precise feedback generation.
Our framework also enables a simulation-based refinement process, and hence
eliminates the risks to physical UAVs caused by incorrect code execution during
the refinement. Extensive experiments on UAV control tasks with different
complexities are conducted. The experimental results show that our framework
can achieve reliable UAV operations using LLMs, which significantly outperforms
baseline approaches in terms of success rate and completeness with the increase
of task complexity.

</details>


### [28] [AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation](https://arxiv.org/abs/2507.01961)
*Sixiang Chen,Jiaming Liu,Siyuan Qian,Han Jiang,Lily Li,Renrui Zhang,Zhuoyang Liu,Chenyang Gu,Chengkai Hou,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 论文提出AC-DiT模型，通过自适应协调移动底座和机械臂，解决了现有方法在语言条件机器人控制中的协调和感知问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在移动底座和机械臂协调上存在不足，且未能区分不同阶段的感知需求，导致误差累积和效率低下。

Method: 提出AC-DiT模型，包含移动底座到机械臂的条件机制和感知感知的多模态条件策略，动态调整2D和3D视觉输入的融合权重。

Result: 在仿真和真实世界的移动操作任务中验证了AC-DiT的有效性。

Conclusion: AC-DiT通过自适应协调和多模态感知，显著提升了移动操作任务的性能。

Abstract: Recently, mobile manipulation has attracted increasing attention for enabling
language-conditioned robotic control in household tasks. However, existing
methods still face challenges in coordinating mobile base and manipulator,
primarily due to two limitations. On the one hand, they fail to explicitly
model the influence of the mobile base on manipulator control, which easily
leads to error accumulation under high degrees of freedom. On the other hand,
they treat the entire mobile manipulation process with the same visual
observation modality (e.g., either all 2D or all 3D), overlooking the distinct
multimodal perception requirements at different stages during mobile
manipulation. To address this, we propose the Adaptive Coordination Diffusion
Transformer (AC-DiT), which enhances mobile base and manipulator coordination
for end-to-end mobile manipulation. First, since the motion of the mobile base
directly influences the manipulator's actions, we introduce a mobility-to-body
conditioning mechanism that guides the model to first extract base motion
representations, which are then used as context prior for predicting whole-body
actions. This enables whole-body control that accounts for the potential impact
of the mobile base's motion. Second, to meet the perception requirements at
different stages of mobile manipulation, we design a perception-aware
multimodal conditioning strategy that dynamically adjusts the fusion weights
between various 2D visual images and 3D point clouds, yielding visual features
tailored to the current perceptual needs. This allows the model to, for
example, adaptively rely more on 2D inputs when semantic information is crucial
for action prediction, while placing greater emphasis on 3D geometric
information when precise spatial understanding is required. We validate AC-DiT
through extensive experiments on both simulated and real-world mobile
manipulation tasks.

</details>
