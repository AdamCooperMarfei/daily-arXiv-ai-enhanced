{"id": "2508.13303", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13303", "abs": "https://arxiv.org/abs/2508.13303", "authors": ["Yingfan Zhou", "Philip Sanderink", "Sigurd Jager Lemming", "Cheng Fang"], "title": "Diff-MSM: Differentiable MusculoSkeletal Model for Simultaneous Identification of Human Muscle and Bone Parameters", "comment": "8 pages, 7 figures", "summary": "High-fidelity personalized human musculoskeletal models are crucial for\nsimulating realistic behavior of physically coupled human-robot interactive\nsystems and verifying their safety-critical applications in simulations before\nactual deployment, such as human-robot co-transportation and rehabilitation\nthrough robotic exoskeletons. Identifying subject-specific Hill-type muscle\nmodel parameters and bone dynamic parameters is essential for a personalized\nmusculoskeletal model, but very challenging due to the difficulty of measuring\nthe internal biomechanical variables in vivo directly, especially the joint\ntorques. In this paper, we propose using Differentiable MusculoSkeletal Model\n(Diff-MSM) to simultaneously identify its muscle and bone parameters with an\nend-to-end automatic differentiation technique differentiating from the\nmeasurable muscle activation, through the joint torque, to the resulting\nobservable motion without the need to measure the internal joint torques.\nThrough extensive comparative simulations, the results manifested that our\nproposed method significantly outperformed the state-of-the-art baseline\nmethods, especially in terms of accurate estimation of the muscle parameters\n(i.e., initial guess sampled from a normal distribution with the mean being the\nground truth and the standard deviation being 10% of the ground truth could end\nup with an average of the percentage errors of the estimated values as low as\n0.05%). In addition to human musculoskeletal modeling and simulation, the new\nparameter identification technique with the Diff-MSM has great potential to\nenable new applications in muscle health monitoring, rehabilitation, and sports\nscience.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u5fae\u5206\u808c\u8089\u9aa8\u9abc\u6a21\u578b(Diff-MSM)\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u81ea\u52a8\u5fae\u5206\u6280\u672f\u540c\u65f6\u8bc6\u522b\u808c\u8089\u548c\u9aa8\u9abc\u53c2\u6570\uff0c\u65e0\u9700\u6d4b\u91cf\u5185\u90e8\u5173\u8282\u626d\u77e9\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9ad8\u4fdd\u771f\u4e2a\u6027\u5316\u4eba\u4f53\u808c\u8089\u9aa8\u9abc\u6a21\u578b\u5bf9\u4e8e\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u4eff\u771f\u548c\u5b89\u5168\u5173\u952e\u5e94\u7528\u9a8c\u8bc1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u6d4b\u91cf\u5185\u90e8\u751f\u7269\u529b\u5b66\u53d8\u91cf\u7279\u522b\u662f\u5173\u8282\u626d\u77e9\u3002", "method": "\u4f7f\u7528\u53ef\u5fae\u5206\u808c\u8089\u9aa8\u9abc\u6a21\u578b(Diff-MSM)\uff0c\u901a\u8fc7\u81ea\u52a8\u5fae\u5206\u6280\u672f\u4ece\u53ef\u6d4b\u91cf\u7684\u808c\u8089\u6fc0\u6d3b\u5230\u53ef\u89c2\u5bdf\u7684\u8fd0\u52a8\u8fdb\u884c\u7aef\u5230\u7aef\u53c2\u6570\u8bc6\u522b\uff0c\u65e0\u9700\u6d4b\u91cf\u5185\u90e8\u5173\u8282\u626d\u77e9\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u808c\u8089\u53c2\u6570\u4f30\u8ba1\u8bef\u5dee\u53ef\u4f4e\u81f30.05%\uff0c\u521d\u59cb\u731c\u6d4b\u4ece\u5747\u503c\u4e3a\u771f\u5b9e\u503c\u3001\u6807\u51c6\u5dee\u4e3a10%\u7684\u6b63\u6001\u5206\u5e03\u91c7\u6837\u3002", "conclusion": "Diff-MSM\u53c2\u6570\u8bc6\u522b\u6280\u672f\u5728\u808c\u8089\u9aa8\u9abc\u5efa\u6a21\u548c\u4eff\u771f\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u5e94\u7528\u4e8e\u808c\u8089\u5065\u5eb7\u76d1\u6d4b\u3001\u5eb7\u590d\u548c\u8fd0\u52a8\u79d1\u5b66\u7b49\u9886\u57df\u3002"}}
{"id": "2508.13319", "categories": ["cs.RO", "cs.AI", "cs.CV", "I.2.9; I.2.10; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.13319", "abs": "https://arxiv.org/abs/2508.13319", "authors": ["Kshitij Kavimandan", "Pooja Mangal", "Devanshi Mehta"], "title": "A Surveillance Based Interactive Robot", "comment": "4 pages, 5 figures", "summary": "We build a mobile surveillance robot that streams video in real time and\nresponds to speech so a user can monitor and steer it from a phone or browser.\nThe system uses two Raspberry Pi 4 units: a front unit on a differential drive\nbase with camera, mic, and speaker, and a central unit that serves the live\nfeed and runs perception. Video is sent with FFmpeg. Objects in the scene are\ndetected using YOLOv3 to support navigation and event awareness. For voice\ninteraction, we use Python libraries for speech recognition, multilingual\ntranslation, and text-to-speech, so the robot can take spoken commands and read\nback responses in the requested language. A Kinect RGB-D sensor provides visual\ninput and obstacle cues. In indoor tests the robot detects common objects at\ninteractive frame rates on CPU, recognises commands reliably, and translates\nthem to actions without manual control. The design relies on off-the-shelf\nhardware and open software, making it easy to reproduce. We discuss limits and\npractical extensions, including sensor fusion with ultrasonic range data, GPU\nacceleration, and adding face and text recognition.", "AI": {"tldr": "\u57fa\u4e8e\u6811\u8393\u6d3e4\u7684\u79fb\u52a8\u76d1\u63a7\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u652f\u6301\u5b9e\u65f6\u89c6\u9891\u6d41\u3001\u8bed\u97f3\u63a7\u5236\u548c\u591a\u8bed\u8a00\u4ea4\u4e92\uff0c\u901a\u8fc7YOLOv3\u8fdb\u884c\u7269\u4f53\u68c0\u6d4b\u548c\u81ea\u4e3b\u5bfc\u822a", "motivation": "\u6784\u5efa\u4e00\u4e2a\u4f7f\u7528\u666e\u901a\u786c\u4ef6\u548c\u5f00\u6e90\u8f6f\u4ef6\u7684\u53ef\u590d\u73b0\u79fb\u52a8\u76d1\u63a7\u673a\u5668\u4eba\uff0c\u652f\u6301\u8fdc\u7a0b\u76d1\u63a7\u548c\u8bed\u97f3\u63a7\u5236\uff0c\u63d0\u4f9b\u81ea\u52a8\u5316\u76d1\u89c6\u529f\u80fd", "method": "\u4f7f\u75282\u53f0Raspberry Pi 4\uff1a\u524d\u7aef\u5355\u5143\u8d1f\u8d23\u79fb\u52a8\u3001\u6444\u50cf\u5934\u548c\u97f3\u9891\uff0c\u4e2d\u592e\u5355\u5143\u8d1f\u8d23\u89c6\u9891\u6d41\u548c\u89c6\u89c9\u5904\u7406\u3002\u91c7\u7528FFmpeg\u4f20\u8f93\u89c6\u9891\uff0cYOLOv3\u8fdb\u884c\u7269\u4f53\u68c0\u6d4b\uff0cPython\u8bed\u97f3\u5e93\u5b9e\u73b0\u8bed\u97f3\u8bc6\u522b\u548c\u7ffb\u8bd1\uff0cKinect RGB-D\u4f20\u611f\u5668\u63d0\u4f9b\u6df1\u5ea6\u4fe1\u606f", "result": "\u5ba4\u5185\u6d4b\u8bd5\u663e\u793a\u673a\u5668\u4eba\u80fd\u5728CPU\u4e0a\u4ee5\u4ea4\u4e92\u5e27\u7387\u68c0\u6d4b\u5e38\u89c1\u7269\u4f53\uff0c\u53ef\u9760\u8bc6\u522b\u547d\u4ee4\u5e76\u81ea\u52a8\u6267\u884c\u52a8\u4f5c\uff0c\u65e0\u9700\u624b\u52a8\u63a7\u5236", "conclusion": "\u8bbe\u8ba1\u4f9d\u9760\u5546\u7528\u786c\u4ef6\u548c\u5f00\u6e90\u8f6f\u4ef6\uff0c\u6613\u4e8e\u590d\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u4f20\u611f\u5668\u878d\u5408\u3001GPU\u52a0\u901f\u3001\u4eba\u8138\u548c\u6587\u672c\u8bc6\u522b\u7b49\u672a\u6765\u6269\u5c55\u65b9\u5411"}}
{"id": "2508.13392", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13392", "abs": "https://arxiv.org/abs/2508.13392", "authors": ["Sidharth Talia", "Oren Salzman", "Siddhartha Srinivasa"], "title": "Incremental Generalized Hybrid A*", "comment": "8 pages, 7 figures", "summary": "We address the problem of efficiently organizing search over very large\ntrees, which arises in many applications ranging from autonomous driving to\naerial vehicles. Here, we are motivated by off-road autonomy, where real-time\nplanning is essential. Classical approaches use graphs of motion primitives and\nexploit dominance to mitigate the curse of dimensionality and prune expansions\nefficiently. However, for complex dynamics, repeatedly solving two-point\nboundary-value problems makes graph construction too slow for fast kinodynamic\nplanning. Hybrid A* (HA*) addressed this challenge by searching over a tree of\nmotion primitives and introducing approximate pruning using a grid-based\ndominance check. However, choosing the grid resolution is difficult: too coarse\nrisks failure, while too fine leads to excessive expansions and slow planning.\nWe propose Incremental Generalized Hybrid A* (IGHA*), an anytime tree-search\nframework that dynamically organizes vertex expansions without rigid pruning.\nIGHA* provably matches or outperforms HA*. For both on-road kinematic and\noff-road kinodynamic planning queries for a car-like robot, variants of IGHA*\nuse 6x fewer expansions to the best solution compared to an optimized version\nof HA*. In simulated off-road experiments in a high fidelity simulator, IGHA*\noutperforms HA*M when both are used in the loop with a model predictive\ncontroller. We demonstrate real-time performance both in simulation and on a\nsmall-scale off-road vehicle, enabling fast, robust planning under complex\ndynamics. Code: https://github.com/personalrobotics/IGHAStar", "AI": {"tldr": "\u63d0\u51fa\u4e86Incremental Generalized Hybrid A* (IGHA*)\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u7ec4\u7ec7\u9876\u70b9\u6269\u5c55\u800c\u975e\u521a\u6027\u526a\u679d\uff0c\u5728\u8fd0\u52a8\u89c4\u5212\u4e2d\u6bd4\u4f20\u7edfHybrid A*\u51cf\u5c116\u500d\u6269\u5c55\u6b21\u6570\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd", "motivation": "\u89e3\u51b3\u590d\u6742\u52a8\u529b\u5b66\u4e0b\u8fd0\u52a8\u89c4\u5212\u7684\u9ad8\u6548\u641c\u7d22\u95ee\u9898\uff0c\u4f20\u7edfHybrid A*\u7684\u7f51\u683c\u5206\u8fa8\u7387\u9009\u62e9\u56f0\u96be\uff1a\u592a\u7c97\u53ef\u80fd\u5bfc\u81f4\u5931\u8d25\uff0c\u592a\u7ec6\u5219\u5bfc\u81f4\u8fc7\u5ea6\u6269\u5c55\u548c\u89c4\u5212\u7f13\u6162", "method": "\u5f00\u53d1\u4e86IGHA*\u7b97\u6cd5\uff0c\u8fd9\u662f\u4e00\u4e2a\u968f\u65f6\u6811\u641c\u7d22\u6846\u67b6\uff0c\u52a8\u6001\u7ec4\u7ec7\u9876\u70b9\u6269\u5c55\u800c\u4e0d\u4f7f\u7528\u521a\u6027\u526a\u679d\uff0c\u53ef\u8bc1\u660e\u5339\u914d\u6216\u4f18\u4e8eHA*", "result": "\u5728\u6c7d\u8f66\u7c7b\u673a\u5668\u4eba\u7684\u9053\u8def\u548c\u975e\u9053\u8def\u89c4\u5212\u67e5\u8be2\u4e2d\uff0cIGHA*\u53d8\u4f53\u6bd4\u4f18\u5316\u7248HA*\u51cf\u5c116\u500d\u6269\u5c55\u5230\u6700\u4f73\u89e3\u7684\u6269\u5c55\u6b21\u6570\uff1b\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u4e2d\uff0cIGHA*\u5728\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5faa\u73af\u4e2d\u4f18\u4e8eHA*M", "conclusion": "IGHA*\u5728\u4eff\u771f\u548c\u5c0f\u578b\u8d8a\u91ce\u8f66\u8f86\u4e0a\u5c55\u793a\u4e86\u5b9e\u65f6\u6027\u80fd\uff0c\u80fd\u591f\u5728\u590d\u6742\u52a8\u529b\u5b66\u4e0b\u5b9e\u73b0\u5feb\u901f\u3001\u9c81\u68d2\u7684\u89c4\u5212"}}
{"id": "2508.13407", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13407", "abs": "https://arxiv.org/abs/2508.13407", "authors": ["Jiming Ren", "Xuan Lin", "Roman Mineyev", "Karen M. Feigh", "Samuel Coogan", "Ye Zhao"], "title": "Accelerating Signal-Temporal-Logic-Based Task and Motion Planning of Bipedal Navigation using Benders Decomposition", "comment": "16 pages, 7 figures, 6 tables", "summary": "Task and motion planning under Signal Temporal Logic constraints is known to\nbe NP-hard. A common class of approaches formulates these hybrid problems,\nwhich involve discrete task scheduling and continuous motion planning, as\nmixed-integer programs (MIP). However, in applications for bipedal locomotion,\nintroduction of non-convex constraints such as kinematic reachability and\nfootstep rotation exacerbates the computational complexity of MIPs. In this\nwork, we present a method based on Benders Decomposition to address scenarios\nwhere solving the entire monolithic optimization problem is prohibitively\nintractable. Benders Decomposition proposes an iterative cutting-plane\ntechnique that partitions the problem into a master problem to prototype a plan\nthat meets the task specification, and a series of subproblems for kinematics\nand dynamics feasibility checks. Our experiments demonstrate that this method\nachieves faster planning compared to alternative algorithms for solving the\nresulting optimization program with nonlinear constraints.", "AI": {"tldr": "\u57fa\u4e8eBenders\u5206\u89e3\u7684\u65b9\u6cd5\uff0c\u9010\u6b65\u89e3\u51b3\u53cc\u8db3\u884c\u8d70\u4e2d\u7684\u4efb\u52a1\u4e0e\u52a8\u4f5c\u89c4\u5212\u95ee\u9898\uff0c\u907f\u514d\u6574\u4f53\u6700\u4f18\u5316\u7684\u8ba1\u7b97\u590d\u6742\u6027", "motivation": "\u53cc\u8db3\u884c\u8d70\u4e2d\u7684\u4efb\u52a1\u4e0e\u52a8\u4f5c\u89c4\u5212\u95ee\u9898\u5177\u6709\u9ad8\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u7279\u522b\u662f\u5f15\u5165\u975e\u51f8\u7ea6\u675f\u540e\u7684\u6df7\u5408\u6574\u6570\u89c4\u5212\u95ee\u9898\u66f4\u52a0\u96be\u4ee5\u89e3\u51b3", "method": "\u91c7\u7528Benders\u5206\u89e3\u6280\u672f\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u4e3b\u95ee\u9898\uff08\u8ba1\u5212\u539f\u578b\uff09\u548c\u5b50\u95ee\u9898\uff08\u52a8\u529b\u5b66\u548c\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u68c0\u67e5\uff09\uff0c\u9010\u6b65\u8fed\u4ee3\u6dfb\u52a0\u5207\u5272\u5e73\u9762", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u975e\u7ebf\u6027\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\u65f6\uff0c\u89c4\u5212\u901f\u5ea6\u66f4\u5feb\u4e8e\u5176\u4ed6\u7b97\u6cd5", "conclusion": "Benders\u5206\u89e3\u6280\u672f\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u53cc\u8db3\u884c\u8d70\u4e2d\u7684\u590d\u6742\u89c4\u5212\u95ee\u9898\uff0c\u63d0\u9ad8\u89e3\u51b3\u6548\u7387\u800c\u4e0d\u5f71\u54cd\u89c4\u5212\u8d28\u91cf"}}
{"id": "2508.13444", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13444", "abs": "https://arxiv.org/abs/2508.13444", "authors": ["Tianyu Li", "Jeonghwan Kim", "Wontaek Kim", "Donghoon Baek", "Seungeun Rho", "Sehoon Ha"], "title": "Switch4EAI: Leveraging Console Game Platform for Benchmarking Robotic Athletics", "comment": "Workshop Submission", "summary": "Recent advances in whole-body robot control have enabled humanoid and legged\nrobots to execute increasingly agile and coordinated movements. However,\nstandardized benchmarks for evaluating robotic athletic performance in\nreal-world settings and in direct comparison to humans remain scarce. We\npresent Switch4EAI(Switch-for-Embodied-AI), a low-cost and easily deployable\npipeline that leverages motion-sensing console games to evaluate whole-body\nrobot control policies. Using Just Dance on the Nintendo Switch as a\nrepresentative example, our system captures, reconstructs, and retargets\nin-game choreography for robotic execution. We validate the system on a Unitree\nG1 humanoid with an open-source whole-body controller, establishing a\nquantitative baseline for the robot's performance against a human player. In\nthe paper, we discuss these results, which demonstrate the feasibility of using\ncommercial games platform as physically grounded benchmarks and motivate future\nwork to for benchmarking embodied AI.", "AI": {"tldr": "Switch4EAI\u662f\u4e00\u4e2a\u5229\u7528\u4f53\u611f\u6e38\u620f\uff08\u5982Just Dance\uff09\u6765\u8bc4\u4f30\u5168\u8eab\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\u7684\u4f4e\u6210\u672c\u7cfb\u7edf\uff0c\u901a\u8fc7\u6355\u6349\u3001\u91cd\u5efa\u548c\u91cd\u5b9a\u5411\u6e38\u620f\u4e2d\u7684\u821e\u8e48\u52a8\u4f5c\u7ed9\u673a\u5668\u4eba\u6267\u884c\uff0c\u4e3a\u673a\u5668\u4eba\u6027\u80fd\u63d0\u4f9b\u91cf\u5316\u57fa\u51c6\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8bc4\u4f30\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u6027\u80fd\u5e76\u4e0e\u4eba\u7c7b\u8fdb\u884c\u76f4\u63a5\u6bd4\u8f83\u7684\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Nintendo Switch\u7684Just Dance\u6e38\u620f\u6355\u6349\u821e\u8e48\u52a8\u4f5c\uff0c\u901a\u8fc7\u8fd0\u52a8\u91cd\u5efa\u548c\u91cd\u5b9a\u5411\u6280\u672f\u8ba9Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u6267\u884c\u8fd9\u4e9b\u52a8\u4f5c\uff0c\u5e76\u5efa\u7acb\u91cf\u5316\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "\u7cfb\u7edf\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u6210\u529f\u9a8c\u8bc1\uff0c\u5efa\u7acb\u4e86\u673a\u5668\u4eba\u76f8\u5bf9\u4e8e\u4eba\u7c7b\u73a9\u5bb6\u7684\u6027\u80fd\u91cf\u5316\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86\u4f7f\u7528\u5546\u4e1a\u6e38\u620f\u5e73\u53f0\u4f5c\u4e3a\u7269\u7406\u57fa\u51c6\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u5546\u4e1a\u6e38\u620f\u5e73\u53f0\u53ef\u4ee5\u4f5c\u4e3a\u7269\u7406\u57fa\u7840\u7684\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u4e3a\u5177\u8eabAI\u7684\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u548c\u65b9\u6cd5\u3002"}}
{"id": "2508.13446", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13446", "abs": "https://arxiv.org/abs/2508.13446", "authors": ["Catherine Glossop", "William Chen", "Arjun Bhorkar", "Dhruv Shah", "Sergey Levine"], "title": "CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models", "comment": null, "summary": "Generalist robots should be able to understand and follow user instructions,\nbut current vision-language-action (VLA) models struggle with following\nfine-grained commands despite providing a powerful architecture for mapping\nopen-vocabulary natural language instructions to robot actions. One cause for\nthis is a lack of semantic diversity and language grounding in existing robot\ndatasets and, specifically, a lack of fine-grained task diversity for similar\nobservations. To address this, we present a novel method to augment existing\nrobot datasets by leveraging vision language models to create counterfactual\nlabels. Our method improves the language-following capabilities of VLAs by\nincreasing the diversity and granularity of language grounding for robot\ndatasets by generating counterfactual language and actions. We evaluate the\nresulting model's ability to follow language instructions, ranging from simple\nobject-centric commands to complex referential tasks, by conducting visual\nlanguage navigation experiments in 3 different indoor and outdoor environments.\nOur experiments demonstrate that counterfactual relabeling, without any\nadditional data collection, significantly improves instruction-following in VLA\npolicies, making them competitive with state-of-the-art methods and increasing\nsuccess rate by 27% on navigation tasks.", "AI": {"tldr": "\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5047\u60f3\u53cd\u4e8b\u5b9e\u6807\u7b7e\uff0c\u63d0\u5347\u673a\u5668\u4eba\u6570\u636e\u96c6\u8bed\u8a00\u57fa\u7840\u7684\u7ec6\u7c92\u5ea6\u548c\u591a\u6837\u6027\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u9075\u5faa\u7ec6\u7c92\u5ea6\u6307\u4ee4\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u73b0\u6709\u673a\u5668\u4eba\u6570\u636e\u96c6\u7f3a\u4e4f\u8bed\u4e49\u591a\u6837\u6027\u548c\u8bed\u8a00\u57fa\u7840\uff0c\u5c24\u5176\u662f\u7c7b\u4f3c\u89c2\u6d4b\u60c5\u5883\u4e0b\u7684\u7ec6\u7c92\u5ea6\u4efb\u52a1\u591a\u6837\u6027\u4e0d\u8db3", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6570\u636e\u589e\u5e3d\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3a\u73b0\u6709\u673a\u5668\u4eba\u6570\u636e\u96c6\u521b\u5efa\u5047\u60f3\u53cd\u4e8b\u5b9e\u6807\u7b7e\uff0c\u901a\u8fc7\u751f\u6210\u5047\u60f3\u53cd\u4e8b\u5b9e\u7684\u8bed\u8a00\u548c\u52a8\u4f5c\u6765\u63d0\u9ad8\u8bed\u8a00\u57fa\u7840\u7684\u591a\u6837\u6027\u548c\u7ec6\u7c92\u5ea6", "result": "\u57283\u79cd\u4e0d\u540c\u5ba4\u5185\u5916\u73af\u5883\u4e2d\u8fdb\u884c\u89c6\u89c6\u8bed\u8a00\u5bfc\u822a\u5b9e\u9a8c\uff0c\u8bc1\u660e\u5047\u60f3\u53cd\u4e8b\u5b9e\u91cd\u6807\u6ce8\u65b9\u6cd5\u5728\u4e0d\u9700\u989d\u5916\u6570\u636e\u6536\u96c6\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86VLA\u7b56\u7565\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u5728\u5bfc\u822a\u4efb\u52a1\u4e0a\u6210\u529f\u7387\u63d0\u9ad827%\uff0c\u8fbe\u5230\u4e86\u9886\u5148\u65b9\u6cd5\u7684\u7ade\u4e89\u529b", "conclusion": "\u5047\u60f3\u53cd\u4e8b\u5b9e\u6807\u7b7e\u662f\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u589e\u5e3d\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u8bed\u8a00\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u66f4\u597d\u7684\u901a\u7528\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u6280\u672f\u652f\u6491"}}
{"id": "2508.13457", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.13457", "abs": "https://arxiv.org/abs/2508.13457", "authors": ["Xu Yang", "Jun Ni", "Hengyang Feng", "Feiyu Wang", "Tiezhen Wang"], "title": "Modeling and Control of AWOISV: A Filtered Tube-Based MPC Approach for Simultaneous Tracking of Lateral Position and Heading Angle", "comment": null, "summary": "An all-wheel omni-directional independent steering vehicle (AWOISV) is a\nspecialized all-wheel independent steering vehicle with each wheel capable of\nsteering up to 90{\\deg}, enabling unique maneuvers like yaw and diagonal\nmovement. This paper introduces a theoretical steering radius angle and\nsideslip angle (\\( \\theta_R \\)-\\(\\beta_R \\)) representation, based on the\nposition of the instantaneous center of rotation relative to the wheel rotation\ncenter, defining the motion modes and switching criteria for AWOISVs. A\ngeneralized \\( v\\)-\\(\\beta\\)-\\(r \\) dynamic model is developed with forward\nvelocity \\(v\\), sideslip angle \\(\\beta\\), and yaw rate \\(r\\) as states, and\n\\(\\theta_R\\) and \\(\\beta_R\\) as control inputs. This model decouples\nlongitudinal and lateral motions into forward and rotational motions, allowing\nseamless transitions across all motion modes under specific conditions. A\nfiltered tube-based linear time-varying MPC (FT-LTVMPC) strategy is proposed,\nachieving simultaneous tracking of lateral position and arbitrary heading\nangles, with robustness to model inaccuracies and parameter uncertainties.\nCo-simulation and hardware-in-loop (HIL) experiments confirm that FT-LTVMPC\nenables high-precision control of both position and heading while ensuring\nexcellent real-time performance.", "AI": {"tldr": "\u57fa\u4e8e\u65b0\u7684\u8f6c\u5411\u534a\u5f84\u89d2\u548c\u4fa7\u6ed1\u89d2\u8868\u793a\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7b5b\u6ce2\u7ba1\u57fa\u7ebf\u6027\u65f6\u53d8MPC\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5168\u8f6e\u5168\u5411\u72ec\u7acb\u8f6e\u8f6c\u8f66\u8f86\u7684\u9ad8\u7cbe\u5ea6\u4f4d\u7f6e\u548c\u65b9\u5411\u63a7\u5236", "motivation": "\u89e3\u51b3\u5168\u8f6e\u5168\u5411\u72ec\u7acb\u8f6e\u8f6c\u8f66\u8f86\u5728\u6240\u6709\u8fd0\u52a8\u6a21\u5f0f\u4e0b\u7684\u5e73\u6ed1\u8fc7\u6e21\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u5bf9\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u6027", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7edf\u4e00\u7684v-\u03b2-r\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7b5b\u6ce2\u7ba1\u57fa\u7ebf\u6027\u65f6\u53d8MPC\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u8f6e\u8f6c\u4e2d\u5fc3\u76f8\u5bf9\u4f4d\u7f6e\u6765\u5b9a\u4e49\u8fd0\u52a8\u6a21\u5f0f\u548c\u5207\u6362\u51c6\u5219", "result": "\u4e32\u8054\u6a21\u62df\u548c\u786c\u4ef6\u5728\u73af\u5b9e\u9a8c\u8bc1\u660e\uff0cFT-LTVMPC\u7b56\u7565\u80fd\u591f\u540c\u65f6\u8fbe\u5230\u9ad8\u7cbe\u5ea6\u4f4d\u7f6e\u548c\u4efb\u610f\u65b9\u5411\u89d2\u8ddf\u8e2a\uff0c\u5e76\u4fdd\u8bc1\u4f18\u79c0\u7684\u5b9e\u65f6\u6027\u80fd", "conclusion": "\u8be5\u7814\u7a76\u4e3aAWOISV\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u63a7\u5236\u65b9\u6848\uff0c\u80fd\u591f\u5728\u5404\u79cd\u8fd0\u52a8\u6a21\u5f0f\u4e0b\u5b9e\u73b0\u5e73\u6ed1\u8fc7\u6e21\u548c\u9c81\u68d2\u6027\u63a7\u5236\uff0c\u4e3a\u8fd9\u7c7b\u7279\u6b8a\u8f66\u8f86\u7684\u5b9e\u9645\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2508.13459", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.13459", "abs": "https://arxiv.org/abs/2508.13459", "authors": ["Rohan Chandra", "Shubham Singh", "Abhishek Jha", "Dannon Andrade", "Hriday Sainathuni", "Katia Sycara"], "title": "Multi-Robot Navigation in Social Mini-Games: Definitions, Taxonomy, and Algorithms", "comment": null, "summary": "The ``Last Mile Challenge'' has long been considered an important, yet\nunsolved, challenge for autonomous vehicles, public service robots, and\ndelivery robots. A central issue in this challenge is the ability of robots to\nnavigate constrained and cluttered environments (e.g., doorways, hallways,\ncorridor intersections), often while competing for space with other robots and\nhumans. We refer to these environments as ``Social Mini-Games'' (SMGs). SMGs\nare tightly coupled, high-agency interactions that arise within general\nmulti-robot navigation (MRN) scenarios. They are identified through certain\ndistinct characteristics and require specialized metrics to evaluate them.\nTraditional navigation approaches designed for MRN do not perform well in SMGs,\nwhich has led to focused research on dedicated SMG solvers (navigation methods\nspecialized to navigate in SMGs), which has flourished in recent years.\nHowever, publications on SMG navigation research make different assumptions (on\ncentralized versus decentralized, observability, communication, cooperation,\netc.), and have different objective functions (safety versus liveness). These\nassumptions and objectives are sometimes implicitly assumed or described\ninformally. This makes it difficult to establish appropriate baselines for\ncomparison in research papers, as well as making it difficult for practitioners\nto find the papers relevant to their concrete application. Such ad-hoc\nrepresentation of the field also presents a barrier to new researchers wanting\nto start research in this area. SMG navigation research requires its own\ntaxonomy, definitions, and evaluation protocols to guide effective research\nmoving forward. This survey is the first to catalog SMG solvers using a\nwell-defined and unified taxonomy and to classify existing methods accordingly.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u63d0\u51fa\u793e\u4f1a\u8ff7\u4f60\u6e38\u620f(SMG)\u5bfc\u822a\u7684\u7edf\u4e00\u5206\u7c7b\u6cd5\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86\u4e13\u95e8\u89e3\u51b3\u673a\u5668\u4eba\"\u6700\u540e\u4e00\u82f1\u91cc\"\u6311\u6218\u7684\u5bfc\u822a\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u4f53\u7cfb\u7684\u7a7a\u767d\u3002", "motivation": "\u4f20\u7edf\u591a\u673a\u5668\u4eba\u5bfc\u822a\u65b9\u6cd5\u5728\u53d7\u9650\u62e5\u6324\u73af\u5883(SMG)\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u73b0\u6709SMG\u5bfc\u822a\u7814\u7a76\u7f3a\u4e4f\u7edf\u4e00\u5047\u8bbe\u548c\u8bc4\u4f30\u6807\u51c6\uff0c\u5bfc\u81f4\u96be\u4ee5\u5efa\u7acb\u57fa\u51c6\u6bd4\u8f83\u548c\u5b9e\u9645\u5e94\u7528\u9009\u62e9\u56f0\u96be\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49SMG\u7684\u72ec\u7279\u7279\u5f81\u548c\u4e13\u7528\u8bc4\u4f30\u6307\u6807\uff0c\u5efa\u7acb\u7edf\u4e00\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u5bf9\u73b0\u6709SMG\u6c42\u89e3\u5668\u8fdb\u884c\u7cfb\u7edf\u5206\u7c7b\u548c\u6574\u7406\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2aSMG\u5bfc\u822a\u65b9\u6cd5\u7684\u6807\u51c6\u5316\u5206\u7c7b\u6846\u67b6\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u9886\u57df\u5730\u56fe\u548c\u6bd4\u8f83\u57fa\u51c6\u3002", "conclusion": "SMG\u5bfc\u822a\u7814\u7a76\u9700\u8981\u4e13\u95e8\u7684\u5206\u7c7b\u6cd5\u3001\u5b9a\u4e49\u548c\u8bc4\u4f30\u534f\u8bae\u6765\u63a8\u52a8\u6709\u6548\u7814\u7a76\uff0c\u672c\u8c03\u67e5\u4e3a\u8be5\u9886\u57df\u5efa\u7acb\u4e86\u5fc5\u8981\u7684\u7406\u8bba\u57fa\u7840\u548c\u6807\u51c6\u5316\u6846\u67b6\u3002"}}
{"id": "2508.13488", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13488", "abs": "https://arxiv.org/abs/2508.13488", "authors": ["Jingwen Yu", "Jiayi Yang", "Anjun Hu", "Jiankun Wang", "Ping Tan", "Hong Zhang"], "title": "ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments", "comment": "8 pages, 9 figures", "summary": "Loop closure detection is important for simultaneous localization and mapping\n(SLAM), which associates current observations with historical keyframes,\nachieving drift correction and global relocalization. However, a falsely\ndetected loop can be fatal, and this is especially difficult in repetitive\nenvironments where appearance-based features fail due to the high similarity.\nTherefore, verification of a loop closure is a critical step in avoiding false\npositive detections. Existing works in loop closure verification predominantly\nfocus on learning invariant appearance features, neglecting the prior knowledge\nof the robot's spatial-temporal motion cue, i.e., trajectory. In this letter,\nwe propose ROVER, a loop closure verification method that leverages the\nhistorical trajectory as a prior constraint to reject false loops in\nchallenging repetitive environments. For each loop candidate, it is first used\nto estimate the robot trajectory with pose-graph optimization. This trajectory\nis then submitted to a scoring scheme that assesses its compliance with the\ntrajectory without the loop, which we refer to as the trajectory prior, to\ndetermine if the loop candidate should be accepted. Benchmark comparisons and\nreal-world experiments demonstrate the effectiveness of the proposed method.\nFurthermore, we integrate ROVER into state-of-the-art SLAM systems to verify\nits robustness and efficiency. Our source code and self-collected dataset are\navailable at https://github.com/jarvisyjw/ROVER.", "AI": {"tldr": "ROVER\u662f\u4e00\u79cd\u5229\u7528\u5386\u53f2\u8f68\u8ff9\u4f5c\u4e3a\u5148\u9a8c\u7ea6\u675f\u7684\u95ed\u73af\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u91cd\u590d\u73af\u5883\u4e2d\u5916\u89c2\u7279\u5f81\u5931\u6548\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8f68\u8ff9\u4f18\u5316\u548c\u8bc4\u5206\u673a\u5236\u6765\u62d2\u7edd\u9519\u8bef\u95ed\u73af\u68c0\u6d4b\u3002", "motivation": "\u5728\u91cd\u590d\u6027\u73af\u5883\u4e2d\uff0c\u57fa\u4e8e\u5916\u89c2\u7279\u5f81\u7684\u95ed\u73af\u68c0\u6d4b\u5bb9\u6613\u4ea7\u751f\u8bef\u5224\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u673a\u5668\u4eba\u65f6\u7a7a\u8fd0\u52a8\u8f68\u8ff9\u8fd9\u4e00\u91cd\u8981\u5148\u9a8c\u77e5\u8bc6\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5229\u7528\u8f68\u8ff9\u4fe1\u606f\u8fdb\u884c\u9a8c\u8bc1\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faROVER\u65b9\u6cd5\uff1a\u5bf9\u4e8e\u6bcf\u4e2a\u95ed\u73af\u5019\u9009\uff0c\u9996\u5148\u901a\u8fc7\u4f4d\u59ff\u56fe\u4f18\u5316\u4f30\u8ba1\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u7136\u540e\u901a\u8fc7\u8bc4\u5206\u65b9\u6848\u8bc4\u4f30\u8be5\u8f68\u8ff9\u4e0e\u65e0\u95ed\u73af\u65f6\u7684\u8f68\u8ff9\u5148\u9a8c\u7684\u7b26\u5408\u7a0b\u5ea6\uff0c\u51b3\u5b9a\u662f\u5426\u63a5\u53d7\u8be5\u95ed\u73af\u5019\u9009\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u96c6\u6210\u5230\u6700\u5148\u8fdb\u7684SLAM\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u5229\u7528\u5386\u53f2\u8f68\u8ff9\u4f5c\u4e3a\u5148\u9a8c\u7ea6\u675f\u80fd\u591f\u6709\u6548\u62d2\u7edd\u91cd\u590d\u73af\u5883\u4e2d\u7684\u9519\u8bef\u95ed\u73af\u68c0\u6d4b\uff0c\u63d0\u9ad8SLAM\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2508.13513", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13513", "abs": "https://arxiv.org/abs/2508.13513", "authors": ["Maolin Lei", "Edoardo Romiti", "Arturo Laurenzi", "Cheng Zhou", "Wanli Xing", "Liang Lu", "Nikos G. Tsagarakis"], "title": "Unified Hierarchical MPC in Task Executing for Modular Manipulators across Diverse Morphologies", "comment": null, "summary": "This work proposes a unified Hierarchical Model Predictive Control (H-MPC)\nfor modular manipulators across various morphologies, as the controller can\nadapt to different configurations to execute the given task without extensive\nparameter tuning in the controller. The H-MPC divides the control process into\ntwo levels: a high-level MPC and a low-level MPC. The high-level MPC predicts\nfuture states and provides trajectory information, while the low-level MPC\nrefines control actions by updating the predictive model based on this\nhigh-level information. This hierarchical structure allows for the integration\nof kinematic constraints and ensures smooth joint-space trajectories, even near\nsingular configurations. Moreover, the low-level MPC incorporates secondary\nlinearization by leveraging predictive information from the high-level MPC,\neffectively capturing the second-order Taylor expansion information of the\nkinematic model while still maintaining a linearized model formulation. This\napproach not only preserves the simplicity of a linear control model but also\nenhances the accuracy of the kinematic representation, thereby improving\noverall control precision and reliability. To validate the effectiveness of the\ncontrol policy, we conduct extensive evaluations across different manipulator\nmorphologies and demonstrate the execution of pick-and-place tasks in\nreal-world scenarios.", "AI": {"tldr": "\u7edf\u4e00\u7684\u5c42\u6b21\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u65b9\u6cd5\uff0c\u53ef\u9002\u914d\u4e0d\u540c\u69cd\u5f0f\u7684\u6a21\u5757\u5316\u64cd\u7eb3\u673a\u68b0\u624b\uff0c\u65e0\u9700\u6df1\u5ea6\u8c03\u53c2\u5373\u53ef\u6267\u884c\u4efb\u52a1", "motivation": "\u89e3\u51b3\u4f20\u7edf\u63a7\u5236\u5668\u5728\u4e0d\u540c\u69cd\u5f0f\u6a21\u5757\u5316\u64cd\u7eb3\u673a\u68b0\u624b\u4e0a\u9700\u8981\u91cd\u65b0\u8c03\u6574\u53c2\u6570\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u63a7\u5236\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u7075\u6d3b\u6027", "method": "\u91c7\u7528\u4e24\u5c42\u6b21MPC\u7ed3\u6784\uff1a\u9ad8\u5c42MPC\u9884\u6d4b\u672a\u6765\u72b6\u6001\u548c\u63d0\u4f9b\u8f68\u8ff9\u4fe1\u606f\uff0c\u4f4e\u5c42MPC\u57fa\u4e8e\u8fd9\u4e9b\u4fe1\u606f\u7cbe\u70bc\u63a7\u5236\u52a8\u4f5c\u5e76\u66f4\u65b0\u9884\u6d4b\u6a21\u578b\uff0c\u5305\u542b\u4e8c\u6b21\u7ebf\u6027\u5316\u6280\u672f", "result": "\u5728\u591a\u79cd\u64cd\u7eb3\u673a\u68b0\u624b\u69cd\u5f0f\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u5b9e\u9645\u573a\u666f\u4e2d\u6210\u529f\u6267\u884c\u4e86\u6458\u653e\u4efb\u52a1\uff0c\u663e\u793a\u4e86\u63a7\u5236\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u7684\u63d0\u5347", "conclusion": "H-MPC\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u8fd0\u52a8\u5b66\u7ea6\u675f\uff0c\u786e\u4fdd\u5e73\u6ed1\u7684\u5173\u8282\u7a7a\u95f4\u8f68\u8ff9\uff0c\u5728\u4fdd\u6301\u7ebf\u6027\u6a21\u578b\u7b80\u6d01\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u8fd0\u52a8\u5b66\u8868\u793a\u7684\u51c6\u786e\u6027"}}
{"id": "2508.13531", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13531", "abs": "https://arxiv.org/abs/2508.13531", "authors": ["Bolin Li", "Gewei Zuo", "Zhixiang Wang", "Xiaotian Ke", "Lijun Zhu", "Han Ding"], "title": "A Three-Level Whole-Body Disturbance Rejection Control Framework for Dynamic Motions in Legged Robots", "comment": null, "summary": "This paper presents a control framework designed to enhance the stability and\nrobustness of legged robots in the presence of uncertainties, including model\nuncertainties, external disturbances, and faults. The framework enables the\nfull-state feedback estimator to estimate and compensate for uncertainties in\nwhole-body dynamics of the legged robots. First, we propose a novel moving\nhorizon extended state observer (MH-ESO) to estimate uncertainties and mitigate\nnoise in legged systems, which can be integrated into the framework for\ndisturbance compensation. Second, we introduce a three-level whole-body\ndisturbance rejection control framework (T-WB-DRC). Unlike the previous\ntwo-level approach, this three-level framework considers both the plan based on\nwhole-body dynamics without uncertainties and the plan based on dynamics with\nuncertainties, significantly improving payload transportation, external\ndisturbance rejection, and fault tolerance. Third, simulations of both humanoid\nand quadruped robots in the Gazebo simulator demonstrate the effectiveness and\nversatility of T-WB-DRC. Finally, extensive experimental trials on a quadruped\nrobot validate the robustness and stability of the system when using T-WB-DRC\nunder various disturbance conditions.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u7ea7\u5168\u8eab\u5e72\u6270\u62b1\u6297\u63a7\u5236\u6846\u67b6(T-WB-DRC)\uff0c\u901a\u8fc7\u65b0\u9898\u79fb\u52a8\u6c34\u5e73\u6269\u5c55\u72b6\u6001\u89c2\u6d4b\u5668(MH-ESO)\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u817f\u5f0f\u673a\u5668\u4eba\u5728\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3001\u5916\u90e8\u5e72\u6270\u548c\u6545\u969c\u6761\u4ef6\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u5f3a\u58c1\u6027\u3002", "motivation": "\u89e3\u51b3\u817f\u5f0f\u673a\u5668\u4eba\u5728\u5b58\u5728\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3001\u5916\u90e8\u5e72\u6270\u548c\u6545\u969c\u65f6\u7684\u7a33\u5b9a\u6027\u548c\u5f3a\u58c1\u6027\u95ee\u9898\uff0c\u5145\u5206\u5229\u7528\u5168\u72b6\u6001\u53cd\u9988\u4f30\u8ba1\u5668\u6765\u4f30\u8ba1\u548c\u8865\u507f\u817f\u5f0f\u673a\u5668\u4eba\u5168\u8eab\u52a8\u529b\u5b66\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u9996\u5148\u63d0\u51fa\u65b0\u9898\u79fb\u52a8\u6c34\u5e73\u6269\u5c55\u72b6\u6001\u89c2\u6d4b\u5668(MH-ESO)\u6765\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u5e76\u51cf\u5c11\u566a\u58f0\uff0c\u7136\u540e\u63d0\u51fa\u4e09\u7ea7\u5168\u8eab\u5e72\u6270\u62b1\u6297\u63a7\u5236\u6846\u67b6(T-WB-DRC)\uff0c\u8003\u8651\u4e86\u65e0\u4e0d\u786e\u5b9a\u6027\u548c\u6709\u4e0d\u786e\u5b9a\u6027\u4e24\u79cd\u60c5\u51b5\u4e0b\u7684\u5168\u8eab\u52a8\u529b\u5b66\u89c4\u5212\u3002", "result": "\u5728Gazebo\u6a21\u62df\u5668\u4e2d\u5bf9\u4eba\u5f62\u548c\u56db\u8db3\u673a\u5668\u4eba\u8fdb\u884c\u4e86\u6a21\u62df\uff0c\u9a8c\u8bc1\u4e86T-WB-DRC\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002\u5728\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u5404\u79cd\u5e72\u6270\u6761\u4ef6\u4e0b\u4f7f\u7528T-WB-DRC\u65f6\u7684\u5f3a\u58c1\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u63a7\u5236\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u817f\u5f0f\u673a\u5668\u4eba\u7684\u8d1f\u8f7d\u8fd0\u8f93\u80fd\u529b\u3001\u5916\u90e8\u5e72\u6270\u62b1\u6297\u80fd\u529b\u548c\u6545\u969c\u5bbd\u5bb9\u6027\uff0c\u4e3a\u817f\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u7a33\u5b9a\u8fd0\u884c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13534", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13534", "abs": "https://arxiv.org/abs/2508.13534", "authors": ["Chao Tang", "Anxing Xiao", "Yuhong Deng", "Tianrun Hu", "Wenlong Dong", "Hanbo Zhang", "David Hsu", "Hong Zhang"], "title": "MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence", "comment": "Accepted to CoRL 2025", "summary": "Imitating tool manipulation from human videos offers an intuitive approach to\nteaching robots, while also providing a promising and scalable alternative to\nlabor-intensive teleoperation data collection for visuomotor policy learning.\nWhile humans can mimic tool manipulation behavior by observing others perform a\ntask just once and effortlessly transfer the skill to diverse tools for\nfunctionally equivalent tasks, current robots struggle to achieve this level of\ngeneralization. A key challenge lies in establishing function-level\ncorrespondences, considering the significant geometric variations among\nfunctionally similar tools, referred to as intra-function variations. To\naddress this challenge, we propose MimicFunc, a framework that establishes\nfunctional correspondences with function frame, a function-centric local\ncoordinate frame constructed with keypoint-based abstraction, for imitating\ntool manipulation skills. Experiments demonstrate that MimicFunc effectively\nenables the robot to generalize the skill from a single RGB-D human video to\nmanipulating novel tools for functionally equivalent tasks. Furthermore,\nleveraging MimicFunc's one-shot generalization capability, the generated\nrollouts can be used to train visuomotor policies without requiring\nlabor-intensive teleoperation data collection for novel objects. Our code and\nvideo are available at https://sites.google.com/view/mimicfunc.", "AI": {"tldr": "MimicFunc\u662f\u4e00\u4e2a\u901a\u8fc7\u529f\u80fd\u5e27\u5efa\u7acb\u529f\u80fd\u5bf9\u5e94\u5173\u7cfb\u7684\u6846\u67b6\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u4ece\u5355\u4e2a\u4eba\u7c7bRGB-D\u89c6\u9891\u4e2d\u5b66\u4e60\u5de5\u5177\u64cd\u4f5c\u6280\u80fd\uff0c\u5e76\u6cdb\u5316\u5230\u65b0\u5de5\u5177\u4e0a\uff0c\u65e0\u9700\u7e41\u7410\u7684\u9065\u64cd\u4f5c\u6570\u636e\u6536\u96c6\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u901a\u8fc7\u89c2\u5bdf\u4e00\u6b21\u5de5\u5177\u64cd\u4f5c\u5c31\u6a21\u4eff\u5e76\u6cdb\u5316\u5230\u529f\u80fd\u7b49\u6548\u7684\u4e0d\u540c\u5de5\u5177\u4e0a\uff0c\u800c\u73b0\u6709\u673a\u5668\u4eba\u96be\u4ee5\u8fbe\u5230\u8fd9\u79cd\u6cdb\u5316\u6c34\u5e73\u3002\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u5904\u7406\u529f\u80fd\u76f8\u4f3c\u5de5\u5177\u4e4b\u95f4\u7684\u51e0\u4f55\u5dee\u5f02\uff08\u529f\u80fd\u5185\u53d8\u5f02\uff09\u3002", "method": "\u63d0\u51faMimicFunc\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u62bd\u8c61\u6784\u5efa\u529f\u80fd\u4e2d\u5fc3\u5c40\u90e8\u5750\u6807\u7cfb\uff08\u529f\u80fd\u5e27\uff09\uff0c\u5efa\u7acb\u529f\u80fd\u7ea7\u522b\u7684\u5bf9\u5e94\u5173\u7cfb\u6765\u6a21\u4eff\u5de5\u5177\u64cd\u4f5c\u6280\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMimicFunc\u80fd\u6709\u6548\u8ba9\u673a\u5668\u4eba\u4ece\u5355\u4e2a\u4eba\u7c7b\u89c6\u9891\u6cdb\u5316\u5230\u64cd\u4f5c\u65b0\u5de5\u5177\u6267\u884c\u529f\u80fd\u7b49\u6548\u4efb\u52a1\uff0c\u751f\u6210\u7684rollout\u53ef\u7528\u4e8e\u8bad\u7ec3\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u89e3\u51b3\u5de5\u5177\u64cd\u4f5c\u6280\u80fd\u6a21\u4eff\u4e2d\u7684\u529f\u80fd\u5bf9\u5e94\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5355\u6b21\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u51cf\u5c11\u9065\u64cd\u4f5c\u6570\u636e\u6536\u96c6\u9700\u6c42\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2508.13699", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13699", "abs": "https://arxiv.org/abs/2508.13699", "authors": ["Maren Raab", "Linda Miller", "Zhe Zeng", "Pascal Jansen", "Martin Baumann", "Johannes Kraus"], "title": "Assessing Pedestrian Behavior Around Autonomous Cleaning Robots in Public Spaces: Findings from a Field Observation", "comment": null, "summary": "As autonomous robots become more common in public spaces, spontaneous\nencounters with laypersons are more frequent. For this, robots need to be\nequipped with communication strategies that enhance momentary transparency and\nreduce the probability of critical situations. Adapting these robotic\nstrategies requires consideration of robot movements, environmental conditions,\nand user characteristics and states. While numerous studies have investigated\nthe impact of distraction on pedestrians' movement behavior, limited research\nhas examined this behavior in the presence of autonomous robots. This research\naddresses the impact of robot type and robot movement pattern on distracted and\nundistracted pedestrians' movement behavior. In a field setting, unaware\npedestrians were videotaped while moving past two working, autonomous cleaning\nrobots. Out of N=498 observed pedestrians, approximately 8% were distracted by\nsmartphones. Distracted and undistracted pedestrians did not exhibit\nsignificant differences in their movement behaviors around the robots. Instead,\nboth the larger sweeping robot and the offset rectangular movement pattern\nsignificantly increased the number of lateral adaptations compared to the\nsmaller cleaning robot and the circular movement pattern. The offset\nrectangular movement pattern also led to significantly more close lateral\nadaptations. Depending on the robot type, the movement patterns led to\ndifferences in the distances of lateral adaptations. The study provides initial\ninsights into pedestrian movement behavior around an autonomous cleaning robot\nin public spaces, contributing to the growing field of HRI research.", "AI": {"tldr": "\u7814\u7a76\u8c03\u67e5\u4e86\u81ea\u4e3b\u6e05\u626b\u673a\u5668\u4eba\u7c7b\u578b\u548c\u8fd0\u52a8\u6a21\u5f0f\u5bf9\u5206\u5fc3\u548c\u672a\u5206\u5fc3\u884c\u4eba\u8fd0\u52a8\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u673a\u5668\u4eba\u5927\u5c0f\u548c\u8fd0\u52a8\u6a21\u5f0f\u6bd4\u884c\u4eba\u662f\u5426\u5206\u5fc3\u66f4\u91cd\u8981", "motivation": "\u968f\u7740\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u516c\u5171\u7a7a\u95f4\u7684\u666e\u53ca\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u901a\u4fe1\u7b56\u7565\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u51cf\u5c11\u5371\u9669\u60c5\u51b5\uff0c\u800c\u5f53\u524d\u5bf9\u5206\u5fc3\u884c\u4eba\u5728\u673a\u5668\u4eba\u8eab\u8fb9\u7684\u884c\u4e3a\u7814\u7a76\u8f83\u5c11", "method": "\u5728\u73b0\u573a\u8bbe\u7f6e\u4e2d\uff0c\u5bf9N=498\u540d\u672a\u77e5\u60c5\u7684\u884c\u4eba\u8fdb\u884c\u89c6\u9891\u5f55\u5236\uff0c\u89c2\u5bdf\u4ed6\u4eec\u7a7f\u8fc7\u4e24\u4e2a\u81ea\u4e3b\u6e05\u626b\u673a\u5668\u4eba\u65f6\u7684\u8fd0\u52a8\u884c\u4e3a\uff0c\u5176\u4e2d\u7ea68%\u884c\u4eba\u4f7f\u7528\u624b\u673a\u5206\u5fc3", "result": "\u5206\u5fc3\u548c\u672a\u5206\u5fc3\u884c\u4eba\u5728\u673a\u5668\u4eba\u9644\u8fd1\u7684\u8fd0\u52a8\u884c\u4e3a\u6ca1\u6709\u663e\u8457\u5dee\u5f02\uff1b\u66f4\u5927\u7684\u6e05\u626b\u673a\u5668\u4eba\u548c\u504f\u79fb\u77e9\u5f62\u8fd0\u52a8\u6a21\u5f0f\u663e\u8457\u589e\u52a0\u4e86\u884c\u4eba\u7684\u6a2a\u5411\u8c03\u6574\u6b21\u6570\uff1b\u504f\u79fb\u77e9\u5f62\u6a21\u5f0f\u8fd8\u5bfc\u81f4\u66f4\u591a\u7684\u8fd1\u8ddd\u79bb\u6a2a\u5411\u8c03\u6574", "conclusion": "\u7814\u7a76\u4e3a\u516c\u5171\u7a7a\u95f4\u4e2d\u884c\u4eba\u5728\u81ea\u4e3b\u6e05\u626b\u673a\u5668\u4eba\u9644\u8fd1\u7684\u8fd0\u52a8\u884c\u4e3a\u63d0\u4f9b\u4e86\u521d\u6b65\u89c1\u89e3\uff0c\u5bf9HRI\u7814\u7a76\u9886\u57df\u505a\u51fa\u4e86\u8d21\u732e"}}
{"id": "2508.13785", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13785", "abs": "https://arxiv.org/abs/2508.13785", "authors": ["Liyang Liu", "Ehsan Mihankhah", "Nathan Wallace", "Javier Martinez", "Andrew J. Hill"], "title": "Blast Hole Seeking and Dipping -- The Navigation and Perception Framework in a Mine Site Inspection Robot", "comment": null, "summary": "In open-pit mining, holes are drilled into the surface of the excavation site\nand detonated with explosives to facilitate digging. These blast holes need to\nbe inspected internally for investigation of downhole material types and\nproperties. Knowing these properties can lead to significant savings in\nmaterial handling costs in downstream processes. Manual hole inspection is slow\nand expensive, with major limitations in revealing the geometric and geological\nproperties of the holes and their contents. This has been the motivation for\nthe development of our autonomous mine-site inspection robot - \"DIPPeR\". In\nthis paper, the automation aspect of the project is explained. We present a\nrobust blast hole seeking and detection framework that enables target-based\nnavigation and accurate down-hole sensor positioning. The pipeline first\nprocesses point-cloud data collected by the on-board LiDAR sensors, extracting\nthe cone-shaped volume of drill-waste above the ground. By projecting the 3D\ncone points into a virtual depth image, segmentation is achieved in the 2D\ndomain, yielding a circular hole at the image centre and a collared cone face.\nWe then identify the hole centre using a robust detection module while\nsuppressing non-maximum candidates, ensuring precise sensor placement for\ndown-hole inspection and avoiding collisions with the cavity wall. To enable\nautonomous hole-seeking, the pipeline automatically adjusts its projection\nparameters during robot navigation to account for variations in point sparsity\nand hole opening size, ensuring a consistent hole appearance in 2D images. This\nallows continuous tracking of the target hole as the robot approaches the goal\npoint. We demonstrate the effectiveness of our navigation and perception system\nin both high-fidelity simulation environments and on-site field tests. A\ndemonstration video is available at\n\"https://www.youtube.com/watch?v=fRNbcBcaSqE\".", "AI": {"tldr": "\u5f00\u91c7\u77ff\u5c71\u7206\u7834\u5b54\u81ea\u4e3b\u68c0\u6d4b\u673a\u5668\u4ebaDIPPeR\uff0c\u5229\u7528LiDAR\u70b9\u4e91\u6570\u636e\u8fdb\u884c\u51f8\u53f0\u63a2\u6d4b\u548c\u5b54\u6f0f\u5b9a\u4f4d\uff0c\u5b9e\u73b0\u7cbe\u51c6\u4e0b\u5b54\u4f4d\u7f6e\u63a7\u5236\u548c\u81ea\u4e3b\u5bfb\u5b54\u5bfc\u822a", "motivation": "\u4f20\u7edf\u4eba\u5de5\u5b54\u6f0f\u68c0\u6d4b\u6548\u7387\u4f4e\u3001\u6210\u672c\u9ad8\uff0c\u65e0\u6cd5\u5168\u9762\u83b7\u53d6\u5b54\u6f0f\u51e0\u4f55\u548c\u5730\u8d28\u7279\u6027\u4fe1\u606f\uff0c\u4e0b\u6e38\u6750\u6599\u5904\u7406\u6210\u672c\u8f83\u9ad8", "method": "\u901a\u8fc7LiDAR\u6536\u96c6\u70b9\u4e91\u6570\u636e\uff0c\u63d0\u53d6\u5730\u9762\u51f8\u53f0\u4f53\u79ef\uff0c\u5c063D\u70b9\u6295\u5f71\u5230\u865a\u62df\u6df1\u5ea6\u56fe\u8fdb\u884c2D\u5206\u5272\uff0c\u8bc6\u522b\u5706\u5f62\u5b54\u6f0f\u4e2d\u5fc3\uff0c\u4f7f\u7528\u975e\u6700\u5927\u503e\u5236\u6a21\u5757\u786e\u4fdd\u7cbe\u51c6\u4f4d\u7f6e\uff0c\u81ea\u52a8\u8c03\u6574\u6295\u5f71\u53c2\u6570\u9002\u5e94\u4e0d\u540c\u70b9\u4e91\u5bc6\u5ea6\u548c\u5b54\u5f84", "result": "\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u73af\u5883\u548c\u73b0\u573a\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5bfc\u822a\u548c\u611f\u77e5\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u5b54\u6f0f\u5b9a\u4f4d\u548c\u8fde\u7eed\u8ddf\u8e2a", "conclusion": "DIPPeR\u673a\u5668\u4eba\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5f00\u91c7\u77ff\u5c71\u5b54\u6f0f\u81ea\u52a8\u68c0\u6d4b\u95ee\u9898\uff0c\u63d0\u9ad8\u68c0\u6d4b\u6548\u7387\u548c\u51c6\u786e\u5ea6\uff0c\u4e3a\u4e0b\u6e38\u6750\u6599\u5904\u7406\u8282\u7701\u6210\u672c\u63d0\u4f9b\u6280\u672f\u652f\u6491"}}
{"id": "2508.13795", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13795", "abs": "https://arxiv.org/abs/2508.13795", "authors": ["Haitham El-Hussieny"], "title": "Trajectory Tracking and Stabilization of Quadrotors Using Deep Koopman Model Predictive Control", "comment": null, "summary": "This paper presents a data-driven control framework for quadrotor systems\nthat integrates a deep Koopman operator with model predictive control (DK-MPC).\nThe deep Koopman operator is trained on sampled flight data to construct a\nhigh-dimensional latent representation in which the nonlinear quadrotor\ndynamics are approximated by linear models. This linearization enables the\napplication of MPC to efficiently optimize control actions over a finite\nprediction horizon, ensuring accurate trajectory tracking and stabilization.\nThe proposed DK-MPC approach is validated through a series of\ntrajectory-following and point-stabilization numerical experiments, where it\ndemonstrates superior tracking accuracy and significantly lower computation\ntime compared to conventional nonlinear MPC. These results highlight the\npotential of Koopman-based learning methods to handle complex quadrotor\ndynamics while meeting the real-time requirements of embedded flight control.\nFuture work will focus on extending the framework to more agile flight\nscenarios and improving robustness against external disturbances.", "AI": {"tldr": "\u6df1\u5ea6Koopman\u7b97\u5b50\u7edf\u4e00\u6df1\u5ea6\u5b66\u4e60\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u901a\u8fc7\u7ebf\u6027\u5316\u65b9\u5f0f\u5904\u7406\u56db\u65cb\u7ffc\u975e\u7ebf\u6027\u52a8\u529b\u5b66\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8f68\u8ff9\u8ddf\u968f\u548c\u5feb\u901f\u8ba1\u7b97\u3002", "motivation": "\u89e3\u51b3\u56db\u65cb\u7ffc\u7cfb\u7edf\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u5bfc\u81f4\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u5b9e\u65f6\u6027\u5dee\u7684\u95ee\u9898\uff0c\u5bfb\u6c42\u65b0\u7684\u63a7\u5236\u6846\u67b6\u6765\u7b80\u5316\u6a21\u578b\u5e76\u4fdd\u6301\u63a7\u5236\u6027\u80fd\u3002", "method": "\u4f7f\u7526\u6df1\u5ea6Koopman\u7b97\u5b50\u4ece\u98de\u884c\u6570\u636e\u4e2d\u5b66\u4e60\u6784\u5efa\u9ad8\u7ef4\u6f5c\u5728\u7a7a\u95f4\uff0c\u5728\u8be5\u7a7a\u95f4\u4e2d\u5c06\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u8fd1\u4f3c\u4e3a\u7ebf\u6027\u6a21\u578b\uff0c\u7136\u540e\u5e94\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u8fdb\u884c\u6709\u9650\u9884\u6d4b\u8def\u5f84\u4f18\u5316\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8f68\u8ff9\u8ddf\u968f\u548c\u70b9\u7a33\u5b9a\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u8ddf\u968f\u7cbe\u5ea6\u548c\u663e\u8457\u66f4\u4f4e\u7684\u8ba1\u7b97\u65f6\u95f4\uff0c\u8f83\u4f20\u7edf\u975e\u7ebf\u6027MPC\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u6df1\u5ea6Koopman\u7b97\u5b50\u7edf\u4e00MPC\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u5904\u7406\u56db\u65cb\u7ffc\u590d\u6742\u52a8\u529b\u5b66\uff0c\u6ee1\u8db3\u5d4c\u5165\u5f0f\u98de\u884c\u63a7\u5236\u7684\u5b9e\u65f6\u8981\u6c42\uff0c\u4e3a\u7075\u6d3b\u98de\u884c\u573a\u666f\u548c\u5e72\u6270\u5f3a\u5316\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2508.13877", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13877", "abs": "https://arxiv.org/abs/2508.13877", "authors": ["Rathnam Vidushika Rasanji", "Jin Wei-Kocsis", "Jiansong Zhang", "Dongming Gan", "Ragu Athinarayanan", "Paul Asunda"], "title": "Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided Decision Transformer", "comment": null, "summary": "Reinforcement learning (RL) has demonstrated great potential in robotic\noperations. However, its data-intensive nature and reliance on the Markov\nDecision Process (MDP) assumption limit its practical deployment in real-world\nscenarios involving complex dynamics and long-term temporal dependencies, such\nas multi-robot manipulation. Decision Transformers (DTs) have emerged as a\npromising offline alternative by leveraging causal transformers for sequence\nmodeling in RL tasks. However, their applications to multi-robot manipulations\nstill remain underexplored. To address this gap, we propose a novel framework,\nSymbolically-Guided Decision Transformer (SGDT), which integrates a\nneuro-symbolic mechanism with a causal transformer to enable deployable\nmulti-robot collaboration. In the proposed SGDT framework, a neuro-symbolic\nplanner generates a high-level task-oriented plan composed of symbolic\nsubgoals. Guided by these subgoals, a goal-conditioned decision transformer\n(GCDT) performs low-level sequential decision-making for multi-robot\nmanipulation. This hierarchical architecture enables structured, interpretable,\nand generalizable decision making in complex multi-robot collaboration tasks.\nWe evaluate the performance of SGDT across a range of task scenarios, including\nzero-shot and few-shot scenarios. To our knowledge, this is the first work to\nexplore DT-based technology for multi-robot manipulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86Symbolically-Guided Decision Transformer (SGDT)\u6846\u67b6\uff0c\u7ed3\u5408\u795e\u7ecf\u7b26\u53f7\u673a\u5236\u548c\u56e0\u679c\u53d8\u6362\u5668\uff0c\u7528\u4e8e\u53ef\u90e8\u7f72\u7684\u591a\u673a\u5668\u4eba\u534f\u4f5c\u64cd\u4f5c\u4efb\u52a1", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u6570\u636e\u5bc6\u96c6\u4e14\u4f9d\u8d56MDP\u5047\u8bbe\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u52a8\u6001\u548c\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u7684\u591a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u3002\u51b3\u7b56\u53d8\u6362\u5668\u4f5c\u4e3a\u79bb\u7ebf\u66ff\u4ee3\u65b9\u6848\u5728\u591a\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22", "method": "SGDT\u6846\u67b6\u5305\u542b\u795e\u7ecf\u7b26\u53f7\u89c4\u5212\u5668\u751f\u6210\u7b26\u53f7\u5b50\u76ee\u6807\u7684\u9ad8\u7ea7\u4efb\u52a1\u5bfc\u5411\u8ba1\u5212\uff0c\u4ee5\u53ca\u76ee\u6807\u6761\u4ef6\u51b3\u7b56\u53d8\u6362\u5668\u8fdb\u884c\u4f4e\u7ea7\u987a\u5e8f\u51b3\u7b56\u7684\u5c42\u6b21\u5316\u67b6\u6784", "result": "\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8bc4\u4f30\u4e86SGDT\u7684\u6027\u80fd\u8868\u73b0", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u63a2\u7d22\u57fa\u4e8e\u51b3\u7b56\u53d8\u6362\u5668\u6280\u672f\u7528\u4e8e\u591a\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5de5\u4f5c\uff0c\u5b9e\u73b0\u4e86\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6cdb\u5316\u7684\u590d\u6742\u591a\u673a\u5668\u4eba\u534f\u4f5c\u51b3\u7b56"}}
{"id": "2508.13881", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13881", "abs": "https://arxiv.org/abs/2508.13881", "authors": ["Zhaokun Chen", "Chaopeng Zhang", "Xiaohan Li", "Wenshuo Wang", "Gentiane Venture", "Junqiang Xi"], "title": "Driving Style Recognition Like an Expert Using Semantic Privileged Information from Large Language Models", "comment": null, "summary": "Existing driving style recognition systems largely depend on low-level\nsensor-derived features for training, neglecting the rich semantic reasoning\ncapability inherent to human experts. This discrepancy results in a fundamental\nmisalignment between algorithmic classifications and expert judgments. To\nbridge this gap, we propose a novel framework that integrates Semantic\nPrivileged Information (SPI) derived from large language models (LLMs) to align\nrecognition outcomes with human-interpretable reasoning. First, we introduce\nDriBehavGPT, an interactive LLM-based module that generates natural-language\ndescriptions of driving behaviors. These descriptions are then encoded into\nmachine learning-compatible representations via text embedding and\ndimensionality reduction. Finally, we incorporate them as privileged\ninformation into Support Vector Machine Plus (SVM+) for training, enabling the\nmodel to approximate human-like interpretation patterns. Experiments across\ndiverse real-world driving scenarios demonstrate that our SPI-enhanced\nframework outperforms conventional methods, achieving F1-score improvements of\n7.6% (car-following) and 7.9% (lane-changing). Importantly, SPI is exclusively\nused during training, while inference relies solely on sensor data, ensuring\ncomputational efficiency without sacrificing performance. These results\nhighlight the pivotal role of semantic behavioral representations in improving\nrecognition accuracy while advancing interpretable, human-centric driving\nsystems.", "AI": {"tldr": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8bed\u4e49\u7279\u6743\u4fe1\u606f(SPI)\u6765\u5bf9\u9f50\u9a7e\u9a76\u98ce\u683c\u8bc6\u522b\u7b97\u6cd5\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5224\u65ad\uff0c\u63d0\u9ad8\u8bc6\u522b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027", "motivation": "\u73b0\u6709\u9a7e\u9a76\u98ce\u683c\u8bc6\u522b\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u4f4e\u7ea7\u4f20\u611f\u5668\u7279\u5f81\uff0c\u5ffd\u89c6\u4e86\u4eba\u7c7b\u4e13\u5bb6\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u7b97\u6cd5\u5206\u7c7b\u4e0e\u4e13\u5bb6\u5224\u65ad\u4e4b\u95f4\u7684\u5bf9\u4e0d\u9f50", "method": "\u9996\u5148\u5f00\u53d1DriBehavGPT\u6a21\u5757\u751f\u6210\u81ea\u7136\u8bed\u8a00\u9a7e\u9a76\u884c\u4e3a\u63cf\u8ff0\uff0c\u901a\u8fc7\u6587\u672c\u5d4c\u5165\u548c\u964d\u7ef4\u8f6c\u6362\u4e3a\u673a\u5668\u5b66\u4e60\u53ef\u5904\u7406\u8868\u5f81\uff0c\u7136\u540e\u4f5c\u4e3a\u7279\u6743\u4fe1\u606f\u96c6\u6210\u5230SVM+\u4e2d\u8fdb\u884c\u8bad\u7ec3", "result": "\u5728\u591a\u79cd\u5b9e\u9645\u9a7e\u9a76\u573a\u666f\u4e2d\uff0cSPI\u589e\u5f3a\u6846\u67b6\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\uff0cF1\u5206\u6570\u63d0\u53478.6%\uff08\u8ddf\u8f66\uff09\u548c7.9%\uff08\u53d8\u9053\uff09\uff0c\u4e14\u4ec5\u5728\u8bad\u7ec3\u4f7f\u7528SPI\uff0c\u63a8\u7406\u4ec5\u9700\u4f20\u611f\u5668\u6570\u636e", "conclusion": "\u8bed\u4e49\u884c\u4e3a\u8868\u5f81\u5728\u63d0\u9ad8\u8bc6\u522b\u51c6\u786e\u6027\u548c\u63a8\u8fdb\u53ef\u89e3\u91ca\u6027\u3001\u4ee5\u4eba\u4e3a\u672c\u7684\u9a7e\u9a76\u7cfb\u7edf\u65b9\u9762\u53d1\u6325\u4e86\u5173\u952e\u4f5c\u7528"}}
{"id": "2508.13901", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13901", "abs": "https://arxiv.org/abs/2508.13901", "authors": ["Yihao Lu", "Hao Tang"], "title": "Multimodal Data Storage and Retrieval for Embodied AI: A Survey", "comment": null, "summary": "Embodied AI (EAI) agents continuously interact with the physical world,\ngenerating vast, heterogeneous multimodal data streams that traditional\nmanagement systems are ill-equipped to handle. In this survey, we first\nsystematically evaluate five storage architectures (Graph Databases,\nMulti-Model Databases, Data Lakes, Vector Databases, and Time-Series\nDatabases), focusing on their suitability for addressing EAI's core\nrequirements, including physical grounding, low-latency access, and dynamic\nscalability. We then analyze five retrieval paradigms (Fusion Strategy-Based\nRetrieval, Representation Alignment-Based Retrieval, Graph-Structure-Based\nRetrieval, Generation Model-Based Retrieval, and Efficient Retrieval-Based\nOptimization), revealing a fundamental tension between achieving long-term\nsemantic coherence and maintaining real-time responsiveness. Based on this\ncomprehensive analysis, we identify key bottlenecks, spanning from the\nfoundational Physical Grounding Gap to systemic challenges in cross-modal\nintegration, dynamic adaptation, and open-world generalization. Finally, we\noutline a forward-looking research agenda encompassing physics-aware data\nmodels, adaptive storage-retrieval co-optimization, and standardized\nbenchmarking, to guide future research toward principled data management\nsolutions for EAI. Our survey is based on a comprehensive review of more than\n180 related studies, providing a rigorous roadmap for designing the robust,\nhigh-performance data management frameworks essential for the next generation\nof autonomous embodied systems.", "AI": {"tldr": "\u672c\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e94\u79cd\u5b58\u50a8\u67b6\u6784\u548c\u4e94\u79cd\u68c0\u7d22\u8303\u5f0f\u5728\u4f53\u73b0\u5f0fAI\u6570\u636e\u7ba1\u7406\u4e2d\u7684\u9002\u7528\u6027\uff0c\u6307\u51fa\u4e86\u957f\u671f\u8bed\u4e49\u4e00\u81f4\u6027\u4e0e\u5b9e\u65f6\u54cd\u5e94\u80fd\u529b\u4e4b\u95f4\u7684\u6838\u5fc3\u77db\u76fe\uff0c\u5e76\u63d0\u51fa\u4e86\u7269\u7406\u611f\u77e5\u6570\u636e\u6a21\u578b\u3001\u9002\u5e94\u6027\u5b58\u50a8\u68c0\u7d22\u534f\u540c\u4f18\u5316\u7b49\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f53\u73b0\u5f0fAI\u4ea7\u751f\u7684\u5e02\u573a\u3001\u591a\u6a21\u6001\u6570\u636e\u6d41\u5bf9\u4f20\u7edf\u6570\u636e\u7ba1\u7406\u7cfb\u7edf\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u627e\u5230\u80fd\u591f\u6ee1\u8db3\u7269\u7406\u57fa\u7840\u3001\u4f4e\u5ef6\u8fdf\u8bbf\u95ee\u548c\u52a8\u6001\u6269\u5c55\u6027\u7b49\u6838\u5fc3\u9700\u6c42\u7684\u6570\u636e\u7ba1\u7406\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u8bc4\u4f305\u79cd\u5b58\u50a8\u67b6\u6784\uff08\u56fe\u6570\u636e\u5e93\u3001\u591a\u6a21\u578b\u6570\u636e\u5e93\u3001\u6570\u636e\u6e56\u3001\u5411\u91cf\u6570\u636e\u5e93\u3001\u65f6\u5e8f\u6570\u636e\u5e93\uff09\u548c5\u79cd\u68c0\u7d22\u8303\u5f0f\uff08\u878d\u5408\u7b56\u7565\u3001\u8868\u5f81\u5bf9\u9f50\u3001\u56fe\u7ed3\u6784\u3001\u751f\u6210\u6a21\u578b\u3001\u9ad8\u6548\u68c0\u7d22\u4f18\u5316\uff09\uff0c\u57fa\u4e8e\u5bf9180\u591a\u4efd\u76f8\u5173\u7814\u7a76\u7684\u5168\u9762\u5206\u6790\u3002", "result": "\u8bc6\u522b\u4e86\u4ece\u57fa\u7840\u7684\u7269\u7406\u57fa\u7840\u95f4\u9694\u5230\u7cfb\u7edf\u6027\u6311\u6218\u7684\u5173\u952e\u74f6\u9888\uff0c\u5305\u62ec\u8de8\u6a21\u6001\u96c6\u6210\u3001\u52a8\u6001\u9002\u5e94\u548c\u5f00\u653e\u4e16\u754c\u666e\u9002\u6027\u7b49\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u8bed\u4e49\u4e00\u81f4\u6027\u4e0e\u5b9e\u65f6\u6027\u4e4b\u95f4\u7684\u6838\u5fc3\u77db\u76fe\u3002", "conclusion": "\u63d0\u51fa\u4e86\u5305\u542b\u7269\u7406\u611f\u77e5\u6570\u636e\u6a21\u578b\u3001\u9002\u5e94\u6027\u5b58\u50a8\u68c0\u7d22\u534f\u540c\u4f18\u5316\u548c\u6807\u51c6\u5316\u6d4b\u8bd5\u57fa\u51c6\u5728\u5185\u7684\u524d\u77bb\u6027\u7814\u7a76\u8bae\u7a0b\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u81ea\u4e3b\u4f53\u73b0\u7cfb\u7edf\u7684\u7a33\u5065\u9ad8\u6027\u80fd\u6570\u636e\u7ba1\u7406\u6846\u67b6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e25\u8c28\u7684\u8def\u7ebf\u56fe\u3002"}}
{"id": "2508.13964", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13964", "abs": "https://arxiv.org/abs/2508.13964", "authors": ["Martijn Cramer", "Yanming Wu", "David De Schepper", "Eric Demeester"], "title": "Augmenting cobots for sheet-metal SMEs with 3D object recognition and localisation", "comment": "13 pages, 25 figures", "summary": "Due to high-mix-low-volume production, sheet-metal workshops today are\nchallenged by small series and varying orders. As standard automation solutions\ntend to fall short, SMEs resort to repetitive manual labour impacting\nproduction costs and leading to tech-skilled workforces not being used to their\nfull potential. The COOCK+ ROBUST project aims to transform cobots into mobile\nand reconfigurable production assistants by integrating existing technologies,\nincluding 3D object recognition and localisation. This article explores both\nthe opportunities and challenges of enhancing cobotic systems with these\ntechnologies in an industrial setting, outlining the key steps involved in the\nprocess. Additionally, insights from a past project, carried out by the ACRO\nresearch unit in collaboration with an industrial partner, serves as a concrete\nimplementation example throughout.", "AI": {"tldr": "\u63a2\u8ba8\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u901a\u8fc73D\u7269\u4f53\u8bc6\u522b\u548c\u5b9a\u4f4d\u6280\u672f\u589e\u5f3a\u534f\u4f5c\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u673a\u9047\u4e0e\u6311\u6218\uff0c\u57fa\u4e8eCOOCK+ ROBUST\u9879\u76ee\u5c06\u534f\u4f5c\u673a\u5668\u4eba\u8f6c\u53d8\u4e3a\u79fb\u52a8\u53ef\u91cd\u6784\u751f\u4ea7\u52a9\u624b", "motivation": "\u5c0f\u6279\u91cf\u591a\u54c1\u79cd\u751f\u4ea7\u6a21\u5f0f\u4e0b\uff0c\u4e2d\u5c0f\u4f01\u4e1a\u9762\u4e34\u6807\u51c6\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u91cd\u590d\u6027\u4f53\u529b\u52b3\u52a8\u589e\u52a0\u751f\u4ea7\u6210\u672c\uff0c\u6280\u672f\u719f\u7ec3\u52b3\u52a8\u529b\u672a\u80fd\u5145\u5206\u53d1\u6325\u6f5c\u529b", "method": "\u96c6\u6210\u73b0\u6709\u6280\u672f\uff08\u5305\u62ec3D\u7269\u4f53\u8bc6\u522b\u548c\u5b9a\u4f4d\uff09\uff0c\u5c06\u534f\u4f5c\u673a\u5668\u4eba\u8f6c\u53d8\u4e3a\u79fb\u52a8\u53ef\u91cd\u6784\u751f\u4ea7\u52a9\u624b\uff0c\u5e76\u901a\u8fc7ACRO\u7814\u7a76\u5355\u4f4d\u4e0e\u5de5\u4e1a\u5408\u4f5c\u4f19\u4f34\u7684\u5b9e\u9645\u9879\u76ee\u6848\u4f8b\u8fdb\u884c\u9a8c\u8bc1", "result": "\u9879\u76ee\u65e8\u5728\u89e3\u51b3\u4e2d\u5c0f\u4f01\u4e1a\u5728\u9ad8\u6df7\u5408\u4f4e\u4ea7\u91cf\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u81ea\u52a8\u5316\u6311\u6218\uff0c\u901a\u8fc7\u589e\u5f3a\u534f\u4f5c\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u5347\u751f\u4ea7\u7075\u6d3b\u6027\u548c\u6548\u7387", "conclusion": "\u901a\u8fc7\u96c6\u62103D\u8bc6\u522b\u548c\u5b9a\u4f4d\u6280\u672f\u589e\u5f3a\u534f\u4f5c\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u53ef\u4ee5\u4e3a\u4e2d\u5c0f\u4f01\u4e1a\u63d0\u4f9b\u53ef\u884c\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5b9e\u65bd\u8fc7\u7a0b\u4e2d\u9700\u8981\u514b\u670d\u6280\u672f\u548c\u96c6\u6210\u65b9\u9762\u7684\u6311\u6218"}}
{"id": "2508.13976", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13976", "abs": "https://arxiv.org/abs/2508.13976", "authors": ["Carlo Mazzola", "Hassan Ali", "Krist\u00edna Malinovsk\u00e1", "Igor Farka\u0161"], "title": "Toward an Interaction-Centered Approach to Robot Trustworthiness", "comment": "4 pages, presented at TRUST workshop, organised in conjunction with\n  the IEEE RO-MAN 2025 conference, held in Eindhoven, Netherlands", "summary": "As robots get more integrated into human environments, fostering\ntrustworthiness in embodied robotic agents becomes paramount for an effective\nand safe human-robot interaction (HRI). To achieve that, HRI applications must\npromote human trust that aligns with robot skills and avoid misplaced trust or\novertrust, which can pose safety risks and ethical concerns. To achieve that,\nHRI applications must promote human trust that aligns with robot skills and\navoid misplaced trust or overtrust, which can pose safety risks and ethical\nconcerns. In this position paper, we outline an interaction-based framework for\nbuilding trust through mutual understanding between humans and robots. We\nemphasize two main pillars: human awareness and transparency, referring to the\nrobot ability to interpret human actions accurately and to clearly communicate\nits intentions and goals, respectively. By integrating these two pillars,\nrobots can behave in a manner that aligns with human expectations and needs\nwhile providing their human partners with both comprehension and control over\ntheir actions. We also introduce four components that we think are important\nfor bridging the gap between a human-perceived sense of trust and a robot true\ncapabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4ea4\u4e92\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u673a\u76f8\u4e92\u7406\u89e3\u6765\u5efa\u7acb\u4fe1\u4efb\uff0c\u5f3a\u8c03\u4eba\u7c7b\u610f\u8bc6\u548c\u900f\u660e\u5ea6\u4e24\u4e2a\u652f\u67f1\uff0c\u65e8\u5728\u4f7f\u673a\u5668\u4eba\u884c\u4e3a\u7b26\u5408\u4eba\u7c7b\u671f\u671b\u5e76\u63d0\u4f9b\u63a7\u5236\u6743\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u66f4\u6df1\u5165\u878d\u5165\u4eba\u7c7b\u73af\u5883\uff0c\u5efa\u7acb\u53ef\u4fe1\u8d56\u7684\u5177\u8eab\u673a\u5668\u4eba\u4ee3\u7406\u5bf9\u4e8e\u6709\u6548\u548c\u5b89\u5168\u7684\u4eba\u673a\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u907f\u514d\u9519\u8bef\u4fe1\u4efb\u6216\u8fc7\u5ea6\u4fe1\u4efb\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\u548c\u4f26\u7406\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u4ea4\u4e92\u7684\u4fe1\u4efb\u5efa\u7acb\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u652f\u67f1\uff1a\u4eba\u7c7b\u610f\u8bc6\uff08\u673a\u5668\u4eba\u51c6\u786e\u89e3\u91ca\u4eba\u7c7b\u884c\u4e3a\u7684\u80fd\u529b\uff09\u548c\u900f\u660e\u5ea6\uff08\u6e05\u6670\u4f20\u8fbe\u673a\u5668\u4eba\u610f\u56fe\u548c\u76ee\u6807\uff09\u3002\u8fd8\u5f15\u5165\u4e86\u56db\u4e2a\u91cd\u8981\u7ec4\u4ef6\u6765\u5f25\u5408\u4eba\u7c7b\u611f\u77e5\u4fe1\u4efb\u4e0e\u673a\u5668\u4eba\u771f\u5b9e\u80fd\u529b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "result": "\u8be5\u6846\u67b6\u65e8\u5728\u4f7f\u673a\u5668\u4eba\u884c\u4e3a\u4e0e\u4eba\u7c7b\u671f\u671b\u548c\u9700\u6c42\u4fdd\u6301\u4e00\u81f4\uff0c\u540c\u65f6\u4e3a\u4eba\u7c7b\u4f19\u4f34\u63d0\u4f9b\u5bf9\u5176\u884c\u4e3a\u7684\u7406\u89e3\u548c\u63a7\u5236\u6743\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u4eba\u7c7b\u610f\u8bc6\u548c\u900f\u660e\u5ea6\u8fd9\u4e24\u4e2a\u5173\u952e\u8981\u7d20\uff0c\u53ef\u4ee5\u4fc3\u8fdb\u4eba\u673a\u4e4b\u95f4\u57fa\u4e8e\u76f8\u4e92\u7406\u89e3\u7684\u4fe1\u4efb\u5173\u7cfb\u5efa\u7acb\uff0c\u5b9e\u73b0\u66f4\u5b89\u5168\u548c\u6709\u6548\u7684\u4eba\u673a\u4ea4\u4e92\u3002"}}
{"id": "2508.13982", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.MA", "I.2.9; I.2"], "pdf": "https://arxiv.org/pdf/2508.13982", "abs": "https://arxiv.org/abs/2508.13982", "authors": ["Sydney Thompson", "Kate Candon", "Marynel V\u00e1zquez"], "title": "The Social Context of Human-Robot Interactions", "comment": "To be published in Annual Review of Control, Robotics, and Autonomous\n  Systems", "summary": "The Human-Robot Interaction (HRI) community often highlights the social\ncontext of an interaction as a key consideration when designing, implementing,\nand evaluating robot behavior. Unfortunately, researchers use the term \"social\ncontext\" in varied ways. This can lead to miscommunication, making it\nchallenging to draw connections between related work on understanding and\nmodeling the social contexts of human-robot interactions. To address this gap,\nwe survey the HRI literature for existing definitions and uses of the term\n\"social context\". Then, we propose a conceptual model for describing the social\ncontext of a human-robot interaction. We apply this model to existing work, and\nwe discuss a range of attributes of social contexts that can help researchers\nplan for interactions, develop behavior models for robots, and gain insights\nafter interactions have taken place. We conclude with a discussion of open\nresearch questions in relation to understanding and modeling the social\ncontexts of human-robot interactions.", "AI": {"tldr": "\u672c\u6587\u5bf9HRI\u9886\u57df\u4e2d\"\u793e\u4ea4\u60c5\u5883\"\u672f\u8bed\u7684\u4f7f\u7528\u6df7\u4e71\u95ee\u9898\u8fdb\u884c\u4e86\u7cfb\u7edf\u68b3\u7406\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u6a21\u578b\u6765\u7edf\u4e00\u63cf\u8ff0\u4eba\u673a\u4ea4\u4e92\u7684\u793e\u4ea4\u60c5\u5883\uff0c\u5e76\u8ba8\u8bba\u4e86\u8be5\u6a21\u578b\u7684\u5e94\u7528\u4ef7\u503c\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "HRI\u7814\u7a76\u793e\u533a\u5728\u8bbe\u8ba1\u548c\u8bc4\u4f30\u673a\u5668\u4eba\u884c\u4e3a\u65f6\u7ecf\u5e38\u5f3a\u8c03\u793e\u4ea4\u60c5\u5883\u7684\u91cd\u8981\u6027\uff0c\u4f46\u7814\u7a76\u4eba\u5458\u5bf9\"\u793e\u4ea4\u60c5\u5883\"\u8fd9\u4e00\u672f\u8bed\u7684\u4f7f\u7528\u65b9\u5f0f\u5404\u4e0d\u76f8\u540c\uff0c\u5bfc\u81f4\u6c9f\u901a\u969c\u788d\u548c\u7814\u7a76\u6210\u679c\u96be\u4ee5\u6574\u5408\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u68b3\u7406HRI\u9886\u57df\u4e2d\u73b0\u6709\u7684\"\u793e\u4ea4\u60c5\u5883\"\u5b9a\u4e49\u548c\u4f7f\u7528\u65b9\u5f0f\uff0c\u7136\u540e\u63d0\u51fa\u4e00\u4e2a\u6982\u5ff5\u6a21\u578b\u6765\u63cf\u8ff0\u4eba\u673a\u4ea4\u4e92\u7684\u793e\u4ea4\u60c5\u5883\uff0c\u5e76\u5c06\u8be5\u6a21\u578b\u5e94\u7528\u4e8e\u73b0\u6709\u7814\u7a76\u5de5\u4f5c\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u80fd\u591f\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u89c4\u5212\u4ea4\u4e92\u3001\u5f00\u53d1\u673a\u5668\u4eba\u884c\u4e3a\u6a21\u578b\u4ee5\u53ca\u5728\u4ea4\u4e92\u53d1\u751f\u540e\u83b7\u5f97\u6d1e\u5bdf\u7684\u793e\u4ea4\u60c5\u5883\u5c5e\u6027\u6846\u67b6\uff0c\u4e3aHRI\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6982\u5ff5\u5de5\u5177\u3002", "conclusion": "\u867d\u7136\u63d0\u51fa\u4e86\u6982\u5ff5\u6a21\u578b\u6765\u89e3\u51b3\u672f\u8bed\u6df7\u4e71\u95ee\u9898\uff0c\u4f46\u5728\u7406\u89e3\u548c\u5efa\u6a21\u4eba\u673a\u4ea4\u4e92\u793e\u4ea4\u60c5\u5883\u65b9\u9762\u4ecd\u5b58\u5728\u8bb8\u591a\u5f00\u653e\u7684\u7814\u7a76\u95ee\u9898\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2508.13998", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13998", "abs": "https://arxiv.org/abs/2508.13998", "authors": ["Yifu Yuan", "Haiqin Cui", "Yaoting Huang", "Yibin Chen", "Fei Ni", "Zibin Dong", "Pengyi Li", "Yan Zheng", "Jianye Hao"], "title": "Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation", "comment": "Embodied-R1 technical report", "summary": "Generalization in embodied AI is hindered by the \"seeing-to-doing gap,\" which\nstems from data scarcity and embodiment heterogeneity. To address this, we\npioneer \"pointing\" as a unified, embodiment-agnostic intermediate\nrepresentation, defining four core embodied pointing abilities that bridge\nhigh-level vision-language comprehension with low-level action primitives. We\nintroduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed\nfor embodied reasoning and pointing. We use a wide range of embodied and\ngeneral visual reasoning datasets as sources to construct a large-scale\ndataset, Embodied-Points-200K, which supports key embodied pointing\ncapabilities. We then train Embodied-R1 using a two-stage Reinforced\nFine-tuning (RFT) curriculum with a specialized multi-task reward design.\nEmbodied-R1 achieves state-of-the-art performance on 11 embodied spatial and\npointing benchmarks. Critically, it demonstrates robust zero-shot\ngeneralization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5%\nacross 8 real-world XArm tasks without any task-specific fine-tuning,\nrepresenting a 62% improvement over strong baselines. Furthermore, the model\nexhibits high robustness against diverse visual disturbances. Our work shows\nthat a pointing-centric representation, combined with an RFT training paradigm,\noffers an effective and generalizable pathway to closing the perception-action\ngap in robotics.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\"\u6307\u5411\"\u4e3a\u7edf\u4e00\u4e2d\u95f4\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7Embodied-R1\u6a21\u578b\u548c\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f53\u73b0\u5f0fAI\u4e2d\u7684\"\u89c6\u89c9\u5230\u52a8\u4f5c\u95f4\u9694\"\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u72ec\u521b\u6027\u80fd\u5e76\u663e\u793a\u51fa\u5f3a\u592b\u7684\u96f6\u6837\u672c\u6f14\u7eed\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4f53\u73b0\u5f0fAI\u4e2d\u7684\"\u89c6\u89c9\u5230\u52a8\u4f5c\u95f4\u9694\"\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u6765\u81ea\u4e8e\u6570\u636e\u7a00\u7f3a\u548c\u4f53\u73b0\u5f02\u8d28\u6027\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u666e\u9002\u6027\u3002", "method": "\u63d0\u51fa\u4ee5\"\u6307\u5411\"\u4f5c\u4e3a\u7edf\u4e00\u7684\u4f53\u73b0\u65e0\u5173\u4e2d\u95f4\u8868\u793a\uff0c\u5b9a\u4e49\u56db\u79cd\u6838\u5fc3\u4f53\u73b0\u6307\u5411\u80fd\u529b\uff1b\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6Embodied-Points-200K\uff1b\u8bbe\u8ba1\u4e86\u4e13\u95e8\u76843B\u89c6\u89c9-\u8bed\u8a00\u6a21\u578bEmbodied-R1\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u8c03\u4f18(RFT)\u8bad\u7ec3\u8ba1\u5212\u548c\u4e13\u95e8\u7684\u591a\u4efb\u52a1\u5956\u52b1\u8bbe\u8ba1\u3002", "result": "\u572811\u4e2a\u4f53\u73b0\u7a7a\u95f4\u548c\u6307\u5411\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f73\u6027\u80fd\uff1b\u5728SIMPLEREnv\u4e2d\u8fbe\u523056.2%\u6210\u529f\u7387\uff0c\u57288\u4e2a\u5b9e\u9645XArm\u4efb\u52a1\u4e2d\u8fbe\u523087.5%\u6210\u529f\u7387\uff0c\u8f83\u5f3a\u57fa\u51c6\u63d0\u534762%\uff1b\u663e\u793a\u51fa\u5bf9\u591a\u6837\u89c6\u89c9\u5e72\u6270\u7684\u9ad8\u7a33\u5065\u6027\u3002", "conclusion": "\u6307\u5411\u4e2d\u5fc3\u8868\u793a\u7ed3\u5408RFT\u8bad\u7ec3\u8303\u5f0f\u4e3a\u95ed\u5408\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u611f\u77e5-\u52a8\u4f5c\u95f4\u9694\u63d0\u4f9b\u4e86\u4e00\u6761\u9ad8\u6548\u4e14\u53ef\u666e\u9002\u7684\u9014\u5f84\u3002"}}
{"id": "2508.14042", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14042", "abs": "https://arxiv.org/abs/2508.14042", "authors": ["Zhuoling Li", "Xiaoyang Wu", "Zhenhua Xu", "Hengshuang Zhao"], "title": "Train Once, Deploy Anywhere: Realize Data-Efficient Dynamic Object Manipulation", "comment": null, "summary": "Realizing generalizable dynamic object manipulation is important for\nenhancing manufacturing efficiency, as it eliminates specialized engineering\nfor various scenarios. To this end, imitation learning emerges as a promising\nparadigm, leveraging expert demonstrations to teach a policy manipulation\nskills. Although the generalization of an imitation learning policy can be\nimproved by increasing demonstrations, demonstration collection is\nlabor-intensive. To address this problem, this paper investigates whether\nstrong generalization in dynamic object manipulation is achievable with only a\nfew demonstrations. Specifically, we develop an entropy-based theoretical\nframework to quantify the optimization of imitation learning. Based on this\nframework, we propose a system named Generalizable Entropy-based Manipulation\n(GEM). Extensive experiments in simulated and real tasks demonstrate that GEM\ncan generalize across diverse environment backgrounds, robot embodiments,\nmotion dynamics, and object geometries. Notably, GEM has been deployed in a\nreal canteen for tableware collection. Without any in-scene demonstration, it\nachieves a success rate of over 97% across more than 10,000 operations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u71b5\u7684\u7406\u8bba\u6846\u67b6\u548cGEM\u7cfb\u7edf\uff0c\u4ec5\u9700\u5c11\u91cf\u6f14\u793a\u5c31\u80fd\u5b9e\u73b0\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u771f\u5b9e\u98df\u5802\u573a\u666f\u4e2d\u53d6\u5f97\u4e8697%\u7684\u6210\u529f\u7387", "motivation": "\u89e3\u51b3\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\u4e2d\u6f14\u793a\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u662f\u5426\u80fd\u7528\u5c11\u91cf\u6f14\u793a\u5b9e\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b", "method": "\u5f00\u53d1\u57fa\u4e8e\u71b5\u7684\u7406\u8bba\u6846\u67b6\u91cf\u5316\u6a21\u4eff\u5b66\u4e60\u4f18\u5316\uff0c\u63d0\u51faGeneralizable Entropy-based Manipulation (GEM)\u7cfb\u7edf", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4efb\u52a1\u4e2d\u5e7f\u6cdb\u5b9e\u9a8c\uff0cGEM\u80fd\u591f\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u73af\u5883\u80cc\u666f\u3001\u673a\u5668\u4eba\u5f62\u6001\u3001\u8fd0\u52a8\u52a8\u529b\u5b66\u548c\u7269\u4f53\u51e0\u4f55\u5f62\u72b6\u3002\u5728\u771f\u5b9e\u98df\u5802\u9910\u5177\u6536\u96c6\u4efb\u52a1\u4e2d\uff0c\u65e0\u9700\u573a\u666f\u5185\u6f14\u793a\uff0c\u8d85\u8fc710,000\u6b21\u64cd\u4f5c\u7684\u6210\u529f\u7387\u8d85\u8fc797%", "conclusion": "GEM\u7cfb\u7edf\u8bc1\u660e\u4e86\u4ec5\u7528\u5c11\u91cf\u6f14\u793a\u5c31\u80fd\u5b9e\u73b0\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5236\u9020\u4e1a\u6548\u7387\u63d0\u5347\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
