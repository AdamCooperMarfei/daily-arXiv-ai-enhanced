<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 74]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [RaFD: Flow-Guided Radar Detection for Robust Autonomous Driving](https://arxiv.org/abs/2509.16261)
*Shuocheng Yang,Zikun Xu,Jiahao Wang,Shahid Nawaz,Jianqiang Wang,Shaobing Xu*

Main category: cs.RO

TL;DR: RaFD是一个基于雷达的目标检测框架，通过估计帧间鸟瞰图流并利用几何线索来提高检测精度，在RADIATE数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 原始雷达图像通常受到噪声和"鬼影"伪影的影响，仅基于语义特征进行目标检测具有挑战性，需要结合几何信息来有效解释雷达信号。

Method: 设计监督流估计辅助任务与检测网络联合训练，利用估计的流引导从前一帧到当前帧的特征传播。

Result: 在RADIATE数据集上实现了最先进的性能。

Conclusion: 结合几何信息对于有效解释语义上固有模糊的雷达信号至关重要。

Abstract: Radar has shown strong potential for robust perception in autonomous driving;
however, raw radar images are frequently degraded by noise and "ghost"
artifacts, making object detection based solely on semantic features highly
challenging. To address this limitation, we introduce RaFD, a radar-based
object detection framework that estimates inter-frame bird's-eye-view (BEV)
flow and leverages the resulting geometric cues to enhance detection accuracy.
Specifically, we design a supervised flow estimation auxiliary task that is
jointly trained with the detection network. The estimated flow is further
utilized to guide feature propagation from the previous frame to the current
one. Our flow-guided, radar-only detector achieves achieves state-of-the-art
performance on the RADIATE dataset, underscoring the importance of
incorporating geometric information to effectively interpret radar signals,
which are inherently ambiguous in semantics.

</details>


### [2] [Tactile-Based Human Intent Recognition for Robot Assistive Navigation](https://arxiv.org/abs/2509.16353)
*Shaoting Peng,Dakarai Crowder,Wenzhen Yuan,Katherine Driggs-Campbell*

Main category: cs.RO

TL;DR: Tac-Nav是一个机器人辅助导航系统，使用圆柱形触觉皮肤和CK-SVM算法来识别人类导航意图，相比传统接口更自然高效


<details>
  <summary>Details</summary>
Motivation: 现有机器人辅助导航系统缺乏人与看护者之间直观的物理交流，限制了系统有效性

Method: 开发了Tac-Nav系统，使用圆柱形触觉皮肤和专门设计的Cylindrical Kernel Support Vector Machine (CK-SVM)算法来分类触觉数据

Result: CK-SVM在模拟数据集上达到97.1%准确率，真实数据集上90.8%准确率，优于四个基线模型；用户研究显示用户更偏好Tac-Nav触觉接口

Conclusion: Tac-Nav系统通过触觉接口和专门算法显著提升了机器人辅助导航的自然性和效率

Abstract: Robot assistive navigation (RAN) is critical for enhancing the mobility and
independence of the growing population of mobility-impaired individuals.
However, existing systems often rely on interfaces that fail to replicate the
intuitive and efficient physical communication observed between a person and a
human caregiver, limiting their effectiveness. In this paper, we introduce
Tac-Nav, a RAN system that leverages a cylindrical tactile skin mounted on a
Stretch 3 mobile manipulator to provide a more natural and efficient interface
for human navigational intent recognition. To robustly classify the tactile
data, we developed the Cylindrical Kernel Support Vector Machine (CK-SVM), an
algorithm that explicitly models the sensor's cylindrical geometry and is
consequently robust to the natural rotational shifts present in a user's grasp.
Comprehensive experiments were conducted to demonstrate the effectiveness of
our classification algorithm and the overall system. Results show that CK-SVM
achieved superior classification accuracy on both simulated (97.1%) and
real-world (90.8%) datasets compared to four baseline models. Furthermore, a
pilot study confirmed that users more preferred the Tac-Nav tactile interface
over conventional joystick and voice-based controls.

</details>


### [3] [Dynamic Objects Relocalization in Changing Environments with Flow Matching](https://arxiv.org/abs/2509.16398)
*Francesco Argenziano,Miguel Saavedra-Ruiz,Sacha Morin,Daniele Nardi,Liam Paull*

Main category: cs.RO

TL;DR: FlowMaps是一种基于流匹配的模型，用于在动态环境中推断物体的多模态位置，帮助机器人处理任务和运动规划中的物体重定位问题。


<details>
  <summary>Details</summary>
Motivation: 在动态环境（如家庭或仓库）中，由于人类活动导致物体位置频繁变化，机器人需要重新定位物体才能完成任务。人类与物体的交互通常遵循习惯和重复模式，这些线索可用于预测物体最可能的位置。

Method: 提出FlowMaps模型，基于流匹配技术，能够推断物体在空间和时间上的多模态位置分布。

Result: 实验结果提供了统计证据支持假设，表明该方法能有效解决动态环境中的物体重定位问题。

Conclusion: FlowMaps为处理动态环境中的未知重定位问题开辟了新途径，代码已公开。

Abstract: Task and motion planning are long-standing challenges in robotics, especially
when robots have to deal with dynamic environments exhibiting long-term
dynamics, such as households or warehouses. In these environments, long-term
dynamics mostly stem from human activities, since previously detected objects
can be moved or removed from the scene. This adds the necessity to find such
objects again before completing the designed task, increasing the risk of
failure due to missed relocalizations. However, in these settings, the nature
of such human-object interactions is often overlooked, despite being governed
by common habits and repetitive patterns. Our conjecture is that these cues can
be exploited to recover the most likely objects' positions in the scene,
helping to address the problem of unknown relocalization in changing
environments. To this end we propose FlowMaps, a model based on Flow Matching
that is able to infer multimodal object locations over space and time. Our
results present statistical evidence to support our hypotheses, opening the way
to more complex applications of our approach. The code is publically available
at https://github.com/Fra-Tsuna/flowmaps

</details>


### [4] [Subteaming and Adaptive Formation Control for Coordinated Multi-Robot Navigation](https://arxiv.org/abs/2509.16412)
*Zihao Deng,Peng Gao,Williard Joshua Jose,Maggie Wigness,John Rogers,Brian Reily,Christopher Reardon,Hao Zhang*

Main category: cs.RO

TL;DR: STAF方法通过分层学习框架实现多机器人导航中的动态分组和自适应编队控制，在复杂场景下表现出色


<details>
  <summary>Details</summary>
Motivation: 多机器人导航需要保持特定编队，但在狭窄走廊等复杂场景中，刚性保持预定义编队不可行，需要动态分组和自适应控制能力

Method: 采用统一的分层学习框架：高层深度图分割用于团队拆分，中层图学习促进子团队间协调导航，底层策略学习控制单个机器人到达目标位置并避免碰撞

Result: 在室内外环境中进行的机器人仿真和物理机器人实验表明，STAF实现了子团队和自适应编队控制的新能力，在挑战性场景中取得了良好性能

Conclusion: STAF方法为多机器人导航提供了有效的动态分组和自适应编队控制解决方案，在复杂环境中具有应用前景

Abstract: Coordinated multi-robot navigation is essential for robots to operate as a
team in diverse environments. During navigation, robot teams usually need to
maintain specific formations, such as circular formations to protect human
teammates at the center. However, in complex scenarios such as narrow
corridors, rigidly preserving predefined formations can become infeasible.
Therefore, robot teams must be capable of dynamically splitting into smaller
subteams and adaptively controlling the subteams to navigate through such
scenarios while preserving formations. To enable this capability, we introduce
a novel method for SubTeaming and Adaptive Formation (STAF), which is built
upon a unified hierarchical learning framework: (1) high-level deep graph cut
for team splitting, (2) intermediate-level graph learning for facilitating
coordinated navigation among subteams, and (3) low-level policy learning for
controlling individual mobile robots to reach their goal positions while
avoiding collisions. To evaluate STAF, we conducted extensive experiments in
both indoor and outdoor environments using robotics simulations and physical
robot teams. Experimental results show that STAF enables the novel capability
for subteaming and adaptive formation control, and achieves promising
performance in coordinated multi-robot navigation through challenging
scenarios. More details are available on the project website:
https://hcrlab.gitlab.io/project/STAF.

</details>


### [5] [End-to-end RL Improves Dexterous Grasping Policies](https://arxiv.org/abs/2509.16434)
*Ritvik Singh,Karl Van Wyk,Pieter Abbeel,Jitendra Malik,Nathan Ratliff,Ankur Handa*

Main category: cs.RO

TL;DR: 本文提出了一种新的模拟器与RL训练解耦方法，通过在多个GPU上分别运行模拟器和PPO算法，显著提高了基于视觉的端到端强化学习在灵巧抓取任务中的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 基于视觉的端到端强化学习虽然能够产生主动视觉行为，但由于内存效率低导致批量大小受限，难以有效应用PPO等算法。现有模拟器的数据并行方法存在瓶颈。

Method: 将模拟器和RL训练（包括经验缓冲区）解耦到不同的GPU上运行，在4个GPU的节点上，3个运行模拟器，1个运行PPO算法，相比标准数据并行方法环境数量翻倍。

Result: 该方法使基于深度视觉的端到端训练成为可能，深度蒸馏到立体RGB网络的效果优于状态蒸馏，在仿真和现实中都取得了更好的性能，提升了真实世界的部署效果。

Conclusion: 解耦模拟方法通过增加批量大小改善了真实世界性能，基于端到端策略的视觉方法在真实世界中超越了之前的最先进结果。

Abstract: This work explores techniques to scale up image-based end-to-end learning for
dexterous grasping with an arm + hand system. Unlike state-based RL,
vision-based RL is much more memory inefficient, resulting in relatively low
batch sizes, which is not amenable for algorithms like PPO. Nevertheless, it is
still an attractive method as unlike the more commonly used techniques which
distill state-based policies into vision networks, end-to-end RL can allow for
emergent active vision behaviors. We identify a key bottleneck in training
these policies is the way most existing simulators scale to multiple GPUs using
traditional data parallelism techniques. We propose a new method where we
disaggregate the simulator and RL (both training and experience buffers) onto
separate GPUs. On a node with four GPUs, we have the simulator running on three
of them, and PPO running on the fourth. We are able to show that with the same
number of GPUs, we can double the number of existing environments compared to
the previous baseline of standard data parallelism. This allows us to train
vision-based environments, end-to-end with depth, which were previously
performing far worse with the baseline. We train and distill both depth and
state-based policies into stereo RGB networks and show that depth distillation
leads to better results, both in simulation and reality. This improvement is
likely due to the observability gap between state and vision policies which
does not exist when distilling depth policies into stereo RGB. We further show
that the increased batch size brought about by disaggregated simulation also
improves real world performance. When deploying in the real world, we improve
upon the previous state-of-the-art vision-based results using our end-to-end
policies.

</details>


### [6] [FiLM-Nav: Efficient and Generalizable Navigation via VLM Fine-tuning](https://arxiv.org/abs/2509.16445)
*Naoki Yokoyama,Sehoon Ha*

Main category: cs.RO

TL;DR: FiLM-Nav是一种通过直接微调预训练视觉语言模型作为导航策略的方法，用于解决机器人在复杂环境中基于自由语言描述定位物体的导航问题。


<details>
  <summary>Details</summary>
Motivation: 虽然基础模型（特别是视觉语言模型）具有强大的语义理解能力，但如何有效将其网络规模知识适应于具身决策仍是一个关键挑战。现有方法主要采用零样本方式或用于地图标注，缺乏直接适应导航任务的能力。

Method: FiLM-Nav直接微调预训练VLM作为导航策略，通过基于原始视觉轨迹历史和导航目标来选择最佳探索边界。利用目标模拟具身经验，使VLM能够将其预训练表示与目标驱动导航相关的特定动态和视觉模式相结合。关键是在多样数据混合上进行微调，包括ObjectNav、OVON、ImageNav和辅助空间推理任务。

Result: FiLM-Nav在HM3D ObjectNav上取得了开放词汇方法中最高的SPL和成功率，并在具有挑战性的HM3D-OVON基准上创造了SPL的新纪录，展示了对未见物体类别的强大泛化能力。

Conclusion: 直接在多样化模拟具身数据上微调VLM是实现可泛化和高效语义导航能力的有效途径，验证了该方法在机器人导航领域的有效性。

Abstract: Enabling robotic assistants to navigate complex environments and locate
objects described in free-form language is a critical capability for real-world
deployment. While foundation models, particularly Vision-Language Models
(VLMs), offer powerful semantic understanding, effectively adapting their
web-scale knowledge for embodied decision-making remains a key challenge. We
present FiLM-Nav (Fine-tuned Language Model for Navigation), an approach that
directly fine-tunes pre-trained VLM as the navigation policy. In contrast to
methods that use foundation models primarily in a zero-shot manner or for map
annotation, FiLM-Nav learns to select the next best exploration frontier by
conditioning directly on raw visual trajectory history and the navigation goal.
Leveraging targeted simulated embodied experience allows the VLM to ground its
powerful pre-trained representations in the specific dynamics and visual
patterns relevant to goal-driven navigation. Critically, fine-tuning on a
diverse data mixture combining ObjectNav, OVON, ImageNav, and an auxiliary
spatial reasoning task proves essential for achieving robustness and broad
generalization. FiLM-Nav sets a new state-of-the-art in both SPL and success
rate on HM3D ObjectNav among open-vocabulary methods, and sets a
state-of-the-art SPL on the challenging HM3D-OVON benchmark, demonstrating
strong generalization to unseen object categories. Our work validates that
directly fine-tuning VLMs on diverse simulated embodied data is a highly
effective pathway towards generalizable and efficient semantic navigation
capabilities.

</details>


### [7] [A Framework for Optimal Ankle Design of Humanoid Robots](https://arxiv.org/abs/2509.16469)
*Guglielmo Cervettini,Roberto Mauceri,Alex Coppola,Fabio Bergonti,Luca Fiorio,Marco Maggiali,Daniele Pucci*

Main category: cs.RO

TL;DR: 提出了一种用于设计和评估并联踝关节机构的统一方法，通过多目标优化合成机构几何形状，并使用标量成本函数评估关键性能指标，比较了SPU和RSU两种架构。


<details>
  <summary>Details</summary>
Motivation: 人形机器人踝关节设计对安全高效的地面交互至关重要，机械柔顺性和电机质量分布等关键因素推动了并联机构架构的应用，但最优配置选择取决于执行器可用性和任务需求。

Method: 采用多目标优化方法合成机构几何形状，使用标量成本函数聚合关键性能指标进行跨架构比较。重点研究了SPU和RSU两种代表性架构，解析了运动学，并为RSU引入了确保工作空间可行性并加速优化的参数化方法。

Result: 通过重新设计现有人形机器人的踝关节验证了该方法。优化的RSU在成本函数上分别比原始串联设计和传统工程RSU降低了41%和14%。

Conclusion: 提出的统一设计评估方法有效，优化的RSU架构在性能上显著优于传统设计，为并联踝关节机构的选择和优化提供了系统化框架。

Abstract: The design of the humanoid ankle is critical for safe and efficient ground
interaction. Key factors such as mechanical compliance and motor mass
distribution have driven the adoption of parallel mechanism architectures.
However, selecting the optimal configuration depends on both actuator
availability and task requirements. We propose a unified methodology for the
design and evaluation of parallel ankle mechanisms. A multi-objective
optimization synthesizes the mechanism geometry, the resulting solutions are
evaluated using a scalar cost function that aggregates key performance metrics
for cross-architecture comparison. We focus on two representative
architectures: the Spherical-Prismatic-Universal (SPU) and the
Revolute-Spherical-Universal (RSU). For both, we resolve the kinematics, and
for the RSU, introduce a parameterization that ensures workspace feasibility
and accelerates optimization. We validate our approach by redesigning the ankle
of an existing humanoid robot. The optimized RSU consistently outperforms both
the original serial design and a conventionally engineered RSU, reducing the
cost function by up to 41% and 14%, respectively.

</details>


### [8] [Robot Conga: A Leader-Follower Walking Approach to Sequential Path Following in Multi-Agent Systems](https://arxiv.org/abs/2509.16482)
*Pranav Tiwari,Soumyodipta Nath*

Main category: cs.RO

TL;DR: 本文提出了Robot Conga算法，一种基于空间位移而非时间参数的领导者-跟随者控制策略，用于解决多机器人系统中的顺序路径跟随问题。


<details>
  <summary>Details</summary>
Motivation: 传统编队控制技术依赖时间参数化轨迹和路径积分，容易导致同步问题和刚性行为。需要一种更灵活的方法来实现机器人在共同轨迹上保持固定空间间隔的顺序路径跟随。

Method: 采用领导者-跟随者控制策略，基于领导者的空间位移而非时间来更新每个智能体的期望状态，假设存在全局位置参考系统（如动作捕捉、视觉跟踪或UWB定位系统）。

Result: 在TurtleBot3和四足机器人（Laikago）上的仿真验证显示，算法实现了精确的轨迹跟踪、稳定的智能体间距和快速收敛。四足机器人在250个时间步（约0.25秒）内完成对齐，TurtleBot3几乎瞬时对齐。

Conclusion: Robot Conga算法有效解决了顺序路径跟随问题，在室内环境下具有实际应用价值，为自动化物流、监控和协作探索等场景提供了可行的解决方案。

Abstract: Coordinated path following in multi-agent systems is a key challenge in
robotics, with applications in automated logistics, surveillance, and
collaborative exploration. Traditional formation control techniques often rely
on time-parameterized trajectories and path integrals, which can result in
synchronization issues and rigid behavior. In this work, we address the problem
of sequential path following, where agents maintain fixed spatial separation
along a common trajectory, guided by a leader under centralized control. We
introduce Robot Conga, a leader-follower control strategy that updates each
agent's desired state based on the leader's spatial displacement rather than
time, assuming access to a global position reference, an assumption valid in
indoor environments equipped with motion capture, vision-based tracking, or UWB
localization systems. The algorithm was validated in simulation using both
TurtleBot3 and quadruped (Laikago) robots. Results demonstrate accurate
trajectory tracking, stable inter-agent spacing, and fast convergence, with all
agents aligning within 250 time steps (approx. 0.25 seconds) in the quadruped
case, and almost instantaneously in the TurtleBot3 implementation.

</details>


### [9] [Substrate-Timing-Independence for Meta-State Stability of Distributed Robotic Swarms](https://arxiv.org/abs/2509.16492)
*Tinapat Limsila,Mehul Sharma,Paulo Garcia*

Main category: cs.RO

TL;DR: 提出了一种基于并发进程演算（CSP）的形式化方法来验证机器人群体设计的正确性，能够自动识别并修正由于时序不可预测性导致的故障元状态问题。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中由于时序不可预测性会出现涌现特性，异步状态演化可能导致系统进入故障元状态。机器人群体系统相比软件系统状态空间更大，验证正确性成本过高。

Method: 利用通信顺序进程（CSP）等并发进程演算，提出一种底材时序无关的形式化推理方法，能够自动识别故障元状态的潜在原因并修正设计。

Result: 在仿真和现实环境中对存在明确故障的机器人群体进行验证，结果显示修正前群体会到达非法元状态，修正后行为始终保持正确。

Conclusion: 该方法可跨不同设计方法学移植，为机器人学家提供了形式化方法的工具箱，能够确保群体系统在底材变化导致的时序变化下保持稳定。

Abstract: Emergent properties in distributed systems arise due to timing
unpredictability; asynchronous state evolution within each sub-system may lead
the macro-system to faulty meta-states. Empirical validation of correctness is
often prohibitively expensive, as the size of the state-space is too large to
be tractable. In robotic swarms this problem is exacerbated, when compared to
software systems, by the variability of the implementation substrate across the
design, or even the deployment, process. We present an approach for formally
reasoning about the correctness of robotic swarm design in a
substrate-timing-independent way. By leveraging concurrent process calculi
(namely, Communicating Sequential Processes), we introduce a methodology that
can automatically identify possible causes of faulty meta-states and correct
such designs such that meta-states are consistently stable, even in the
presence of timing variability due to substrate changes. We evaluate this
approach on a robotic swarm with a clearly identified fault, realized in both
simulation and reality. Results support the research hypothesis, showing that
the swarm reaches an illegal meta-state before the correction is applied, but
behaves consistently correctly after the correction. Our techniques are
transferable across different design methodologies, contributing to the toolbox
of formal methods for roboticists.

</details>


### [10] [No Need for Real 3D: Fusing 2D Vision with Pseudo 3D Representations for Robotic Manipulation Learning](https://arxiv.org/abs/2509.16532)
*Run Yu,Yangdi Liu,Wen-Da Wei,Chen Li*

Main category: cs.RO

TL;DR: 提出NoReal3D框架，通过3DStructureFormer将单目图像转换为几何有意义的伪点云特征，避免真实3D点云数据采集的高成本，实现与3D点云方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 3D点云方法在机器人操作中性能优于2D图像方法，但3D数据采集成本高昂限制了其可扩展性和实际部署。需要找到既能利用3D信息优势又避免高成本的方法。

Method: 设计3DStructureFormer模块将单目图像转换为保留几何拓扑结构的伪点云特征，与2D编码器特征融合，并使用伪点云编码器保持几何特性。

Result: 在多个任务上的实验验证表明，该框架无需真实点云数据即可达到与3D点云方法相当的性能。

Conclusion: NoReal3D框架成功解决了3D点云数据采集成本高的问题，通过伪点云特征实现了3D空间结构的理解，具有重要的实际应用价值。

Abstract: Recently,vision-based robotic manipulation has garnered significant attention
and witnessed substantial advancements. 2D image-based and 3D point cloud-based
policy learning represent two predominant paradigms in the field, with recent
studies showing that the latter consistently outperforms the former in terms of
both policy performance and generalization, thereby underscoring the value and
significance of 3D information. However, 3D point cloud-based approaches face
the significant challenge of high data acquisition costs, limiting their
scalability and real-world deployment. To address this issue, we propose a
novel framework NoReal3D: which introduces the 3DStructureFormer, a learnable
3D perception module capable of transforming monocular images into
geometrically meaningful pseudo-point cloud features, effectively fused with
the 2D encoder output features. Specially, the generated pseudo-point clouds
retain geometric and topological structures so we design a pseudo-point cloud
encoder to preserve these properties, making it well-suited for our framework.
We also investigate the effectiveness of different feature fusion
strategies.Our framework enhances the robot's understanding of 3D spatial
structures while completely eliminating the substantial costs associated with
3D point cloud acquisition.Extensive experiments across various tasks validate
that our framework can achieve performance comparable to 3D point cloud-based
methods, without the actual point cloud data.

</details>


### [11] [TranTac: Leveraging Transient Tactile Signals for Contact-Rich Robotic Manipulation](https://arxiv.org/abs/2509.16550)
*Yinghao Wu,Shuhong Hou,Haowen Zheng,Yichen Li,Weiyi Lu,Xun Zhou,Yitian Shao*

Main category: cs.RO

TL;DR: TranTac是一个低成本、数据高效的多模态触觉传感与控制框架，通过在机器人夹爪的弹性尖端集成单个接触敏感的6轴IMU，实现微米级的动态平移和扭转变形检测，结合Transformer编码器和扩散策略，模仿人类插入行为，在精细插入任务中显著优于仅视觉或力/力矩传感方法。


<details>
  <summary>Details</summary>
Motivation: 机器人精细操作任务（如钥匙插入锁孔、USB插入端口）在视觉感知不足以检测微小偏差时会失败，需要触觉传感来监控任务状态并进行精确调整。现有触觉传感方案要么对细微变化不敏感，要么需要过多传感器数据。

Method: 在机器人夹爪弹性尖端集成单个接触敏感的6轴IMU，检测微米级的动态平移和扭转变形；使用Transformer编码器和扩散策略，基于夹爪尖端检测到的瞬时触觉线索模仿人类插入行为，动态控制被抓取物体的6自由度位姿。

Result: 结合视觉时，TranTac在物体抓取和插入任务中平均成功率达79%，优于仅视觉策略和增强末端力/力矩传感的方法；仅触觉的错位插入任务平均成功率达88%；在未见过的USB插头和金属钥匙上测试，平均成功率接近70%。

Conclusion: TranTac框架为精细操作任务提供了新的机器人触觉传感系统思路，展示了低成本、数据高效的触觉传感在复杂操作任务中的潜力。

Abstract: Robotic manipulation tasks such as inserting a key into a lock or plugging a
USB device into a port can fail when visual perception is insufficient to
detect misalignment. In these situations, touch sensing is crucial for the
robot to monitor the task's states and make precise, timely adjustments.
Current touch sensing solutions are either insensitive to detect subtle changes
or demand excessive sensor data. Here, we introduce TranTac, a data-efficient
and low-cost tactile sensing and control framework that integrates a single
contact-sensitive 6-axis inertial measurement unit within the elastomeric tips
of a robotic gripper for completing fine insertion tasks. Our customized
sensing system can detect dynamic translational and torsional deformations at
the micrometer scale, enabling the tracking of visually imperceptible pose
changes of the grasped object. By leveraging transformer-based encoders and
diffusion policy, TranTac can imitate human insertion behaviors using transient
tactile cues detected at the gripper's tip during insertion processes. These
cues enable the robot to dynamically control and correct the 6-DoF pose of the
grasped object. When combined with vision, TranTac achieves an average success
rate of 79% on object grasping and insertion tasks, outperforming both
vision-only policy and the one augmented with end-effector 6D force/torque
sensing. Contact localization performance is also validated through
tactile-only misaligned insertion tasks, achieving an average success rate of
88%. We assess the generalizability by training TranTac on a single prism-slot
pair and testing it on unseen data, including a USB plug and a metal key, and
find that the insertion tasks can still be completed with an average success
rate of nearly 70%. The proposed framework may inspire new robotic tactile
sensing systems for delicate manipulation tasks.

</details>


### [12] [Video-to-BT: Generating Reactive Behavior Trees from Human Demonstration Videos for Robotic Assembly](https://arxiv.org/abs/2509.16611)
*Xiwei Zhao,Yiwei Wang,Yansong Wu,Fan Wu,Teng Sun,Zhonghua Miao,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: 提出了Video-to-BT框架，利用视觉语言模型将人类演示视频分解为子任务并生成行为树，实现机器人装配任务的高层认知规划与低层反应控制的集成


<details>
  <summary>Details</summary>
Motivation: 传统机器人装配系统依赖专家编程，缺乏对产品变化的灵活性和处理变异的鲁棒性，需要更灵活可靠的解决方案

Method: 使用VLM分解演示视频生成行为树，结合实时场景解释进行反应式控制，并在执行失败时触发VLM驱动的重规划

Result: 在真实装配任务实验中表现出高规划可靠性、长时程任务的稳健性能，以及在多样化和扰动条件下的强泛化能力

Conclusion: 该闭环架构确保了系统的稳定性和适应性，为现代制造业提供了灵活可靠的机器人装配解决方案

Abstract: Modern manufacturing demands robotic assembly systems with enhanced
flexibility and reliability. However, traditional approaches often rely on
programming tailored to each product by experts for fixed settings, which are
inherently inflexible to product changes and lack the robustness to handle
variations. As Behavior Trees (BTs) are increasingly used in robotics for their
modularity and reactivity, we propose a novel hierarchical framework,
Video-to-BT, that seamlessly integrates high-level cognitive planning with
low-level reactive control, with BTs serving both as the structured output of
planning and as the governing structure for execution. Our approach leverages a
Vision-Language Model (VLM) to decompose human demonstration videos into
subtasks, from which Behavior Trees are generated. During the execution, the
planned BTs combined with real-time scene interpretation enable the system to
operate reactively in the dynamic environment, while VLM-driven replanning is
triggered upon execution failure. This closed-loop architecture ensures
stability and adaptivity. We validate our framework on real-world assembly
tasks through a series of experiments, demonstrating high planning reliability,
robust performance in long-horizon assembly tasks, and strong generalization
across diverse and perturbed conditions. Project website:
https://video2bt.github.io/video2bt_page/

</details>


### [13] [ORN-CBF: Learning Observation-conditioned Residual Neural Control Barrier Functions via Hypernetworks](https://arxiv.org/abs/2509.16614)
*Bojan Derajić,Sebastian Bernhard,Wolfgang Hönig*

Main category: cs.RO

TL;DR: 该论文提出了一种基于Hamilton-Jacobi可达性分析的观测条件神经控制屏障函数方法，用于解决自主系统的安全关键控制问题。该方法通过神经CBF近似最大安全集，并利用超网络架构设计观测条件安全滤波器，在仿真和硬件实验中显示出比基线方法更高的成功率和更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 控制屏障函数(CBFs)在自主系统的安全关键控制中已被证明是有效的方法，但其设计仍然具有挑战性。现有学习方法存在安全集次优、部分可观测环境适用性差、缺乏严格安全保证等问题。

Method: 基于Hamilton-Jacobi可达性分析，提出观测条件神经CBFs，利用HJ值函数的数学特性确保预测的安全集不与观测到的故障集相交。采用超网络架构设计观测条件安全滤波器。

Result: 在轮式机器人和四旋翼飞行器的仿真和硬件实验中，该方法相比基线方法显示出更高的成功率和向域外环境的更好泛化能力。

Conclusion: 所提出的观测条件神经CBF方法能够有效解决自主系统的安全控制问题，特别是在部分可观测环境下具有更好的性能和泛化能力。

Abstract: Control barrier functions (CBFs) have been demonstrated as an effective
method for safety-critical control of autonomous systems. Although CBFs are
simple to deploy, their design remains challenging, motivating the development
of learning-based approaches. Yet, issues such as suboptimal safe sets,
applicability in partially observable environments, and lack of rigorous safety
guarantees persist. In this work, we propose observation-conditioned neural
CBFs based on Hamilton-Jacobi (HJ) reachability analysis, which approximately
recover the maximal safe sets. We exploit certain mathematical properties of
the HJ value function, ensuring that the predicted safe set never intersects
with the observed failure set. Moreover, we leverage a hypernetwork-based
architecture that is particularly suitable for the design of
observation-conditioned safety filters. The proposed method is examined both in
simulation and hardware experiments for a ground robot and a quadcopter. The
results show improved success rates and generalization to out-of-domain
environments compared to the baselines.

</details>


### [14] [LLM-Guided Task- and Affordance-Level Exploration in Reinforcement Learning](https://arxiv.org/abs/2509.16615)
*Jelle Luijkx,Runyu Ma,Zlatan Ajanović,Jens Kober*

Main category: cs.RO

TL;DR: LLM-TALE是一个利用大型语言模型（LLM）规划能力直接指导强化学习（RL）探索的框架，通过任务级和可操作性级规划提高学习效率，在机器人操作任务中表现出更好的样本效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在机器人操作任务中存在样本效率低、需要大量探索的问题。虽然现有方法利用LLM的常识知识来指导探索，但LLM生成的计划可能存在语义合理但物理不可行的问题。

Method: LLM-TALE框架在任务级和可操作性级两个层面集成规划，能够在线纠正次优计划，并在无需人工监督的情况下探索多模态可操作性级计划。

Result: 在标准RL基准测试的拾取放置任务中，LLM-TALE相比强基线方法在样本效率和成功率方面都有显著提升。真实机器人实验显示出有前景的零样本仿真到真实迁移能力。

Conclusion: LLM-TALE通过结合LLM的规划能力和RL的学习能力，有效解决了机器人操作任务中的探索效率问题，为LLM与RL的融合提供了新的思路。

Abstract: Reinforcement learning (RL) is a promising approach for robotic manipulation,
but it can suffer from low sample efficiency and requires extensive exploration
of large state-action spaces. Recent methods leverage the commonsense knowledge
and reasoning abilities of large language models (LLMs) to guide exploration
toward more meaningful states. However, LLMs can produce plans that are
semantically plausible yet physically infeasible, yielding unreliable behavior.
We introduce LLM-TALE, a framework that uses LLMs' planning to directly steer
RL exploration. LLM-TALE integrates planning at both the task level and the
affordance level, improving learning efficiency by directing agents toward
semantically meaningful actions. Unlike prior approaches that assume optimal
LLM-generated plans or rewards, LLM-TALE corrects suboptimality online and
explores multimodal affordance-level plans without human supervision. We
evaluate LLM-TALE on pick-and-place tasks in standard RL benchmarks, observing
improvements in both sample efficiency and success rates over strong baselines.
Real-robot experiments indicate promising zero-shot sim-to-real transfer. Code
and supplementary material are available at https://llm-tale.github.io.

</details>


### [15] [KungfuBot2: Learning Versatile Motion Skills for Humanoid Whole-Body Control](https://arxiv.org/abs/2509.16638)
*Jinrui Han,Weiji Xie,Jiakun Zheng,Jiyuan Shi,Weinan Zhang,Ting Xiao,Chenjia Bai*

Main category: cs.RO

TL;DR: VMS是一个统一的人形机器人全身控制器，能够通过单一策略学习多样化的动态行为，结合混合跟踪目标和正交专家混合架构，实现准确的运动模仿和长期稳定性。


<details>
  <summary>Details</summary>
Motivation: 学习能够跟踪各种人体运动的通用全身技能是开发通用人形机器人的关键步骤，但挑战在于单一策略需要掌握广泛的运动技能同时确保长期稳定性。

Method: 提出VMS框架，包含混合跟踪目标（平衡局部运动保真度和全局轨迹一致性）、正交专家混合架构（促进技能专业化同时增强运动间泛化能力），以及段级跟踪奖励机制。

Result: 在仿真和真实世界实验中验证了VMS的有效性，展示了动态技能的准确模仿、分钟级序列的稳定性能，以及对未见运动的强泛化能力。

Conclusion: VMS作为可扩展的人形机器人全身控制基础具有巨大潜力，为通用人形机器人的发展提供了重要技术支撑。

Abstract: Learning versatile whole-body skills by tracking various human motions is a
fundamental step toward general-purpose humanoid robots. This task is
particularly challenging because a single policy must master a broad repertoire
of motion skills while ensuring stability over long-horizon sequences. To this
end, we present VMS, a unified whole-body controller that enables humanoid
robots to learn diverse and dynamic behaviors within a single policy. Our
framework integrates a hybrid tracking objective that balances local motion
fidelity with global trajectory consistency, and an Orthogonal
Mixture-of-Experts (OMoE) architecture that encourages skill specialization
while enhancing generalization across motions. A segment-level tracking reward
is further introduced to relax rigid step-wise matching, enhancing robustness
when handling global displacements and transient inaccuracies. We validate VMS
extensively in both simulation and real-world experiments, demonstrating
accurate imitation of dynamic skills, stable performance over minute-long
sequences, and strong generalization to unseen motions. These results highlight
the potential of VMS as a scalable foundation for versatile humanoid whole-body
control. The project page is available at
https://kungfubot2-humanoid.github.io.

</details>


### [16] [HDMI: Learning Interactive Humanoid Whole-Body Control from Human Videos](https://arxiv.org/abs/2509.16757)
*Haoyang Weng,Yitang Li,Nikhil Sobanbabu,Zihan Wang,Zhengyi Luo,Tairan He,Deva Ramanan,Guanya Shi*

Main category: cs.RO

TL;DR: HDMI是一个从单目RGB视频学习全身人形机器人-物体交互技能的框架，通过提取人类运动轨迹、训练强化学习策略，并在真实人形机器人上零样本部署。


<details>
  <summary>Details</summary>
Motivation: 解决全身人形机器人-物体交互的挑战，包括运动数据稀缺和接触密集性，直接从人类视频中学习交互技能。

Method: 1) 从视频提取并重定向人类和物体轨迹构建结构化数据集；2) 训练RL策略，采用统一物体表示、残差动作空间和通用交互奖励；3) 零样本部署到真实机器人。

Result: 在Unitree G1人形机器人上实现67次连续门穿越，在真实世界完成6个不同的移动操作任务，在仿真中完成14个任务。

Conclusion: HDMI是一个简单通用的框架，能够从人类视频中获取交互性人形机器人技能，具有鲁棒性和通用性。

Abstract: Enabling robust whole-body humanoid-object interaction (HOI) remains
challenging due to motion data scarcity and the contact-rich nature. We present
HDMI (HumanoiD iMitation for Interaction), a simple and general framework that
learns whole-body humanoid-object interaction skills directly from monocular
RGB videos. Our pipeline (i) extracts and retargets human and object
trajectories from unconstrained videos to build structured motion datasets,
(ii) trains a reinforcement learning (RL) policy to co-track robot and object
states with three key designs: a unified object representation, a residual
action space, and a general interaction reward, and (iii) zero-shot deploys the
RL policies on real humanoid robots. Extensive sim-to-real experiments on a
Unitree G1 humanoid demonstrate the robustness and generality of our approach:
HDMI achieves 67 consecutive door traversals and successfully performs 6
distinct loco-manipulation tasks in the real world and 14 tasks in simulation.
Our results establish HDMI as a simple and general framework for acquiring
interactive humanoid skills from human videos.

</details>


### [17] [Improve bounding box in Carla Simulator](https://arxiv.org/abs/2509.16773)
*Mohamad Mofeed Chaar,Jamal Raiyn,Galia Weidl*

Main category: cs.RO

TL;DR: CARLA模拟器是自动驾驶领域的重要测试平台，但其主要方法在物体检测和边界框标注方面存在挑战，如幽灵框问题。本文提出了一种改进方法，旨在过滤掉不需要的边界框，并取得了高精度。


<details>
  <summary>Details</summary>
Motivation: CARLA模拟器在生成自动驾驶数据时，主要方法会产生虚假检测（如幽灵框），即检测到被障碍物遮挡的物体。这影响了数据质量和算法评估的准确性。

Method: 在CARLA模拟器中，通过捕获地图上所有物体的坐标，并将其与自我车辆的传感器坐标系对齐，然后从自我车辆视角生成边界框。改进方法在此基础上增加了过滤机制，以消除由遮挡引起的虚假边界框。

Result: 性能分析表明，改进后的方法实现了高精度，有效减少了虚假检测，提高了边界框标注的质量。

Conclusion: 通过增强CARLA模拟器中的边界框生成方法，成功解决了幽灵框等虚假检测问题，为自动驾驶算法的测试和数据生成提供了更可靠的工具。

Abstract: The CARLA simulator (Car Learning to Act) serves as a robust platform for
testing algorithms and generating datasets in the field of Autonomous Driving
(AD). It provides control over various environmental parameters, enabling
thorough evaluation. Development bounding boxes are commonly utilized tools in
deep learning and play a crucial role in AD applications. The predominant
method for data generation in the CARLA Simulator involves identifying and
delineating objects of interest, such as vehicles, using bounding boxes. The
operation in CARLA entails capturing the coordinates of all objects on the map,
which are subsequently aligned with the sensor's coordinate system at the ego
vehicle and then enclosed within bounding boxes relative to the ego vehicle's
perspective. However, this primary approach encounters challenges associated
with object detection and bounding box annotation, such as ghost boxes.
Although these procedures are generally effective at detecting vehicles and
other objects within their direct line of sight, they may also produce false
positives by identifying objects that are obscured by obstructions. We have
enhanced the primary approach with the objective of filtering out unwanted
boxes. Performance analysis indicates that the improved approach has achieved
high accuracy.

</details>


### [18] [SMART-3D: Three-Dimensional Self-Morphing Adaptive Replanning Tree](https://arxiv.org/abs/2509.16812)
*Priyanshu Agrawal,Shalabh Gupta,Zongyuan Shen*

Main category: cs.RO

TL;DR: SMART-3D是基于SMART算法的三维扩展，是一种用于动态环境的树基自适应重规划算法，通过热节点概念提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 将SMART算法扩展到三维环境，解决动态环境中快速移动障碍物的路径规划问题，满足实时机载应用需求。

Method: 使用树基自适应重规划方法，通过热节点实现树的变形，在路径被障碍物阻挡时实时寻找新路径，移除了网格分解要求。

Result: 在2D和3D环境中进行广泛模拟测试，SMART-3D实现了高成功率和低重规划时间。

Conclusion: SMART-3D算法计算效率高、可扩展性强，适合实时机载应用。

Abstract: This paper presents SMART-3D, an extension of the SMART algorithm to 3D
environments. SMART-3D is a tree-based adaptive replanning algorithm for
dynamic environments with fast moving obstacles. SMART-3D morphs the underlying
tree to find a new path in real-time whenever the current path is blocked by
obstacles. SMART-3D removed the grid decomposition requirement of the SMART
algorithm by replacing the concept of hot-spots with that of hot-nodes, thus
making it computationally efficient and scalable to 3D environments. The
hot-nodes are nodes which allow for efficient reconnections to morph the
existing tree to find a new safe and reliable path. The performance of SMART-3D
is evaluated by extensive simulations in 2D and 3D environments populated with
randomly moving dynamic obstacles. The results show that SMART-3D achieves high
success rates and low replanning times, thus highlighting its suitability for
real-time onboard applications.

</details>


### [19] [Factorizing Diffusion Policies for Observation Modality Prioritization](https://arxiv.org/abs/2509.16830)
*Omkar Patil,Prabin Rath,Kartikay Pangaonkar,Eric Rosen,Nakul Gopalan*

Main category: cs.RO

TL;DR: 提出了Factorized Diffusion Policies (FDP)，一种新的策略公式，通过设计使观测模态在动作扩散过程中具有不同的影响力，从而学习到更优先某些观测模态的策略。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型策略在处理多模态观测时，未能捕捉到不同任务中观测模态影响力的差异，导致在某些情况下性能不佳。

Method: FDP通过因子化扩散过程的观测条件来实现模态优先级，使得某些观测模态（如视觉>触觉或本体感觉>视觉）可以优先于其他模态。

Result: 在低数据情况下，FDP相比标准扩散策略在多个模拟基准上实现了15%的绝对成功率提升；在分布偏移情况下（如视觉干扰或相机遮挡），FDP在多个视觉运动任务上实现了40%的更高绝对成功率。

Conclusion: FDP为现实世界部署提供了比标准扩散策略更安全、更鲁棒的替代方案。

Abstract: Diffusion models have been extensively leveraged for learning robot skills
from demonstrations. These policies are conditioned on several observational
modalities such as proprioception, vision and tactile. However, observational
modalities have varying levels of influence for different tasks that diffusion
polices fail to capture. In this work, we propose 'Factorized Diffusion
Policies' abbreviated as FDP, a novel policy formulation that enables
observational modalities to have differing influence on the action diffusion
process by design. This results in learning policies where certain observations
modalities can be prioritized over the others such as $\texttt{vision>tactile}$
or $\texttt{proprioception>vision}$. FDP achieves modality prioritization by
factorizing the observational conditioning for diffusion process, resulting in
more performant and robust policies. Our factored approach shows strong
performance improvements in low-data regimes with $15\%$ absolute improvement
in success rate on several simulated benchmarks when compared to a standard
diffusion policy that jointly conditions on all input modalities. Moreover, our
benchmark and real-world experiments show that factored policies are naturally
more robust with $40\%$ higher absolute success rate across several visuomotor
tasks under distribution shifts such as visual distractors or camera
occlusions, where existing diffusion policies fail catastrophically. FDP thus
offers a safer and more robust alternative to standard diffusion policies for
real-world deployment. Videos are available at
https://fdp-policy.github.io/fdp-policy/ .

</details>


### [20] [Robot Learning with Sparsity and Scarcity](https://arxiv.org/abs/2509.16834)
*Jingxi Xu*

Main category: cs.RO

TL;DR: 这篇论文讨论了机器人学习中的两大挑战：触觉传感的数据稀疏性和康复机器人的数据稀缺性，并提出了相应的机器学习解决方案。


<details>
  <summary>Details</summary>
Motivation: 机器人学习面临缺乏大规模数据资源的问题，具体表现为数据稀疏性（如触觉传感）和数据稀缺性（如康复机器人）。作者旨在解决这两个领域的数据挑战。

Method: 对于触觉传感，采用无模型强化学习来学习仅依赖触觉的探索和操作策略；对于康复机器人，开发了半监督学习、元学习和生成式AI等方法，在数据稀缺条件下实现意图推断。

Result: 开发了能够高效利用稀疏触觉信息的策略，以及在有限数据下实现准确意图推断的算法，为康复机器人提供适时的物理辅助。

Conclusion: 通过针对性的机器学习方法，可以有效解决机器人学习中的数据稀疏性和稀缺性问题，推动触觉传感和康复机器人领域的发展。

Abstract: Unlike in language or vision, one of the fundamental challenges in robot
learning is the lack of access to vast data resources. We can further break
down the problem into (1) data sparsity from the angle of data representation
and (2) data scarcity from the angle of data quantity. In this thesis, I will
discuss selected works on two domains: (1) tactile sensing and (2)
rehabilitation robots, which are exemplars of data sparsity and scarcity,
respectively. Tactile sensing is an essential modality for robotics, but
tactile data are often sparse, and for each interaction with the physical
world, tactile sensors can only obtain information about the local area of
contact. I will discuss my work on learning vision-free tactile-only
exploration and manipulation policies through model-free reinforcement learning
to make efficient use of sparse tactile information. On the other hand,
rehabilitation robots are an example of data scarcity to the extreme due to the
significant challenge of collecting biosignals from disabled-bodied subjects at
scale for training. I will discuss my work in collaboration with the medical
school and clinicians on intent inferral for stroke survivors, where a hand
orthosis developed in our lab collects a set of biosignals from the patient and
uses them to infer the activity that the patient intends to perform, so the
orthosis can provide the right type of physical assistance at the right moment.
My work develops machine learning algorithms that enable intent inferral with
minimal data, including semi-supervised, meta-learning, and generative AI
methods.

</details>


### [21] [Benchmarking Offline Reinforcement Learning for Emotion-Adaptive Social Robotics](https://arxiv.org/abs/2509.16858)
*Soon Jynn Chu,Raju Gottumukkala,Alan Barhorst*

Main category: cs.RO

TL;DR: 本文研究使用离线强化学习作为在线强化学习的实用替代方案，用于开发情感自适应社交机器人，通过预收集数据实现情感响应能力，避免在线数据收集的高成本和风险。


<details>
  <summary>Details</summary>
Motivation: 在线强化学习在社交机器人情感响应开发中成本高昂且存在安全风险，需要寻找更实用的替代方案。

Method: 提出集成多模态感知识别、决策制定和自适应响应的系统架构，在有限的人机游戏数据集上比较BCQ、CQL、NFQ、DQN和DDQN等离线强化学习算法。

Result: BCQ和CQL对数据稀疏性更鲁棒，相比NFQ、DQN和DDQN获得更高的状态-动作价值。

Conclusion: 为情感自适应机器人中的离线强化学习基准测试奠定基础，为未来在对话代理、教育伙伴和个人助理等现实HRI场景中的部署提供指导。

Abstract: The ability of social robots to respond to human emotions is crucial for
building trust and acceptance in human-robot collaborative environments.
However, developing such capabilities through online reinforcement learning is
sometimes impractical due to the prohibitive cost of data collection and the
risk of generating unsafe behaviors. In this paper, we study the use of offline
reinforcement learning as a practical and efficient alternative. This technique
uses pre-collected data to enable emotion-adaptive social robots. We present a
system architecture that integrates multimodal sensing and recognition,
decision-making, and adaptive responses. Using a limited dataset from a
human-robot game-playing scenario, we establish a benchmark for comparing
offline reinforcement learning algorithms that do not require an online
environment. Our results show that BCQ and CQL are more robust to data
sparsity, achieving higher state-action values compared to NFQ, DQN, and DDQN.
This work establishes a foundation for benchmarking offline RL in
emotion-adaptive robotics and informs future deployment in real-world HRI. Our
findings provide empirical insight into the performance of offline
reinforcement learning algorithms in data-constrained HRI. This work
establishes a foundation for benchmarking offline RL in emotion-adaptive
robotics and informs its future deployment in real-world HRI, such as in
conversational agents, educational partners, and personal assistants, require
reliable emotional responsiveness.

</details>


### [22] [HOGraspFlow: Exploring Vision-based Generative Grasp Synthesis with Hand-Object Priors and Taxonomy Awareness](https://arxiv.org/abs/2509.16871)
*Yitian Shi,Zicheng Guo,Rosa Wolf,Edgar Welte,Rania Rayyes*

Main category: cs.RO

TL;DR: HO行为流是一种以效能为中心的方法，能够将单张RGB手物交互图像重定向为多模态的平行齿捏取动作，而无需显式的物体几何先验知识。


<details>
  <summary>Details</summary>
Motivation: 目的是从人类示范中实现可靠的、与物体无关的捏取合成，避免对显式手物交互接触输入或物体几何的依赖。

Method: 基于手部重建和视觉的基础模型，利用去噪流匹配技术，在三种互补素条的条件下合成SE(3)捏取姿势：RGB基础特征、手物交互接触重建和分类先验知识。

Result: 在真实世界实验中实现了过83%的平均成功率，且在SE(3)空间中实现了更高的分布保真度和更稳定的优化。

Conclusion: 该方法在无显式手物交互接触输入或物体几何的情况下仍能实现高保真度的捏取合成，且在SE(3)空间中表现超过传播基变体。

Abstract: We propose Hand-Object\emph{(HO)GraspFlow}, an affordance-centric approach
that retargets a single RGB with hand-object interaction (HOI) into multi-modal
executable parallel jaw grasps without explicit geometric priors on target
objects. Building on foundation models for hand reconstruction and vision, we
synthesize $SE(3)$ grasp poses with denoising flow matching (FM), conditioned
on the following three complementary cues: RGB foundation features as visual
semantics, HOI contact reconstruction, and taxonomy-aware prior on grasp types.
Our approach demonstrates high fidelity in grasp synthesis without explicit HOI
contact input or object geometry, while maintaining strong contact and taxonomy
recognition. Another controlled comparison shows that \emph{HOGraspFlow}
consistently outperforms diffusion-based variants (\emph{HOGraspDiff}),
achieving high distributional fidelity and more stable optimization in $SE(3)$.
We demonstrate a reliable, object-agnostic grasp synthesis from human
demonstrations in real-world experiments, where an average success rate of over
$83\%$ is achieved.

</details>


### [23] [End2Race: Efficient End-to-End Imitation Learning for Real-Time F1Tenth Racing](https://arxiv.org/abs/2509.16894)
*Zhijie Qiao,Haowei Li,Zhong Cao,Henry X. Liu*

Main category: cs.RO

TL;DR: End2Race是一种用于F1Tenth自动驾驶赛车对抗竞赛的端到端模仿学习算法，采用GRU架构处理时序依赖关系，通过sigmoid归一化处理LiDAR数据，在模拟器中实现了94.2%的安全率和59.2%的超车成功率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶赛车需要在高速动态环境中进行实时决策，传统自动驾驶算法无法满足需求，且高速操作限制了模型容量，因此需要开发专门的高效算法。

Method: 提出End2Race端到端模仿学习算法，使用GRU架构捕获连续时序依赖，采用sigmoid归一化函数将原始LiDAR扫描转换为空间压力标记，实现高效训练和收敛。

Result: 在F1Tenth模拟器中，End2Race在2400个超车场景中达到94.2%的安全率，59.2%的超车成功率，推理时间小于0.5毫秒，优于现有方法。

Conclusion: End2Race成为F1Tenth赛车测试平台的领先解决方案，证明了端到端模仿学习在高速自动驾驶赛车中的有效性。

Abstract: F1Tenth is a widely adopted reduced-scale platform for developing and testing
autonomous racing algorithms, hosting annual competitions worldwide. With high
operating speeds, dynamic environments, and head-to-head interactions,
autonomous racing requires algorithms that diverge from those in classical
autonomous driving. Training such algorithms is particularly challenging: the
need for rapid decision-making at high speeds severely limits model capacity.
To address this, we propose End2Race, a novel end-to-end imitation learning
algorithm designed for head-to-head autonomous racing. End2Race leverages a
Gated Recurrent Unit (GRU) architecture to capture continuous temporal
dependencies, enabling both short-term responsiveness and long-term strategic
planning. We also adopt a sigmoid-based normalization function that transforms
raw LiDAR scans into spatial pressure tokens, facilitating effective model
training and convergence. The algorithm is extremely efficient, achieving an
inference time of less than 0.5 milliseconds on a consumer-class GPU.
Experiments in the F1Tenth simulator demonstrate that End2Race achieves a 94.2%
safety rate across 2,400 overtaking scenarios, each with an 8-second time
limit, and successfully completes overtakes in 59.2% of cases. This surpasses
previous methods and establishes ours as a leading solution for the F1Tenth
racing testbed. Code is available at
https://github.com/michigan-traffic-lab/End2Race.

</details>


### [24] [SwarmChat: An LLM-Based, Context-Aware Multimodal Interaction System for Robotic Swarms](https://arxiv.org/abs/2509.16920)
*Ettilla Mohiuddin Eumi,Hussein Abbass,Nadine Marcus*

Main category: cs.RO

TL;DR: SwarmChat是一个基于大语言模型的多模态人机交互系统，通过自然语言命令控制机器人群体，解决了传统HSI方法缺乏直观实时自适应界面的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的人机群体交互方法缺乏直观的实时自适应界面，导致决策速度慢、认知负荷增加，限制了命令的灵活性。

Method: 系统集成四个LLM模块：上下文生成器、意图识别器、任务规划器和模态选择器，采用三层架构提供动态界面，支持文本、语音或遥控等多种交互方式。

Result: 初步评估显示SwarmChat的LLM模块能够准确解释上下文、识别相关意图并有效传递命令，获得了较高的用户满意度。

Conclusion: SwarmChat通过LLM驱动的多模态交互系统，为人机群体交互提供了更直观、灵活且认知负荷较低的控制方案。

Abstract: Traditional Human-Swarm Interaction (HSI) methods often lack intuitive
real-time adaptive interfaces, making decision making slower and increasing
cognitive load while limiting command flexibility. To solve this, we present
SwarmChat, a context-aware, multimodal interaction system powered by Large
Language Models (LLMs). SwarmChat enables users to issue natural language
commands to robotic swarms using multiple modalities, such as text, voice, or
teleoperation. The system integrates four LLM-based modules: Context Generator,
Intent Recognition, Task Planner, and Modality Selector. These modules
collaboratively generate context from keywords, detect user intent, adapt
commands based on real-time robot state, and suggest optimal communication
modalities. Its three-layer architecture offers a dynamic interface with both
fixed and customizable command options, supporting flexible control while
optimizing cognitive effort. The preliminary evaluation also shows that the
SwarmChat's LLM modules provide accurate context interpretation, relevant
intent recognition, and effective command delivery, achieving high user
satisfaction.

</details>


### [25] [A Reliable Robot Motion Planner in Complex Real-world Environments via Action Imagination](https://arxiv.org/abs/2509.16963)
*Chengjin Wang,Yanmin Zhou,Zhipeng Wang,Zheng Yan,Feng Luan,Shuo Jiang,Runjie Shen,Hongrui Sang,Bin He*

Main category: cs.RO

TL;DR: 本文提出了一种受生物智能启发的想象启发运动规划器（I-MP）框架，通过想象可能的空间状态来增强机器人在未知非结构化环境中的动作可靠性。


<details>
  <summary>Details</summary>
Motivation: 受人类和动物通过想象动作结果来实时调整运动以防止意外运动失败的启发，旨在解决机器人在复杂系统中因物理交互引起的不确定性问题。

Method: 通过拓扑化工作空间，构建感知-动作循环使机器人自主建立接触模型。利用不动点理论和Hausdorff距离计算在交互特性和任务约束下的收敛空间状态，并通过功均匀表示多维环境特征，实时计算能量梯度来接近想象的空间状态。

Result: 实验结果表明，I-MP在复杂杂乱环境中具有实用性和鲁棒性。

Conclusion: 想象启发运动规划器框架有效提升了机器人在非结构化环境中的动作可靠性，为复杂系统中的实时运动调整提供了新思路。

Abstract: Humans and animals can make real-time adjustments to movements by imagining
their action outcomes to prevent unanticipated or even catastrophic motion
failures in unknown unstructured environments. Action imagination, as a refined
sensorimotor strategy, leverages perception-action loops to handle physical
interaction-induced uncertainties in perception and system modeling within
complex systems. Inspired by the action-awareness capability of animal
intelligence, this study proposes an imagination-inspired motion planner (I-MP)
framework that specifically enhances robots' action reliability by imagining
plausible spatial states for approaching. After topologizing the workspace,
I-MP build perception-action loop enabling robots autonomously build contact
models. Leveraging fixed-point theory and Hausdorff distance, the planner
computes convergent spatial states under interaction characteristics and
mission constraints. By homogenously representing multi-dimensional
environmental characteristics through work, the robot can approach the imagined
spatial states via real-time computation of energy gradients. Consequently,
experimental results demonstrate the practicality and robustness of I-MP in
complex cluttered environments.

</details>


### [26] [Geometric Interpolation of Rigid Body Motions](https://arxiv.org/abs/2509.16966)
*Andreas Mueller*

Main category: cs.RO

TL;DR: 本文提出了刚体运动插值问题的两种变体：k阶初值轨迹插值问题（k-IV-TIP）和k阶边界值轨迹插值问题（k-BV-TIP），并给出了k=1到4的解决方案，特别提出了一种新颖的三次插值方法。


<details>
  <summary>Details</summary>
Motivation: 解决刚体运动在空间轨迹插值中的问题，满足初始和终端位姿的约束条件，包括刚体扭转及其高阶导数。

Method: 提出了k-IV-TIP和k-BV-TIP两种插值问题的数学框架，给出了k=1到4的具体解决方案，并介绍了一种新的三次插值方法，当扭转设为零时自动退化为最小加速度曲线。

Result: 成功推导了高阶解决方案的一般方法，并通过两个数值示例展示了方法的有效性。

Conclusion: 本文为刚体运动插值问题提供了系统的解决方案框架，特别是提出的三次插值方法在特定条件下具有最小加速度曲线的优良特性。

Abstract: The problem of interpolating a rigid body motion is to find a spatial
trajectory between a prescribed initial and terminal pose. Two variants of this
interpolation problem are addressed. The first is to find a solution that
satisfies initial conditions on the k-1 derivatives of the rigid body twist.
This is called the kth-order initial value trajectory interpolation problem
(k-IV-TIP). The second is to find a solution that satisfies conditions on the
rigid body twist and its k-1 derivatives at the initial and terminal pose. This
is called the kth-order boundary value trajectory interpolation problem
(k-BV-TIP). Solutions to the k-IV-TIP for k=1,...,4, i.e. the initial twist and
up to the 4th time derivative are prescribed. Further, a solution to the
1-IV-TBP is presented, i.e. the initial and terminal twist are prescribed. The
latter is a novel cubic interpolation between two spatial configurations with
given initial and terminal twist. This interpolation is automatically identical
to the minimum acceleration curve when the twists are set to zero. The general
approach to derive higher-order solutions is presented. Numerical results are
shown for two examples.

</details>


### [27] [IDfRA: Self-Verification for Iterative Design in Robotic Assembly](https://arxiv.org/abs/2509.16998)
*Nishka Khendry,Christos Margadji,Sebastian W. Pattinson*

Main category: cs.RO

TL;DR: 提出了IDfRA框架，通过迭代的规划-执行-验证-重新规划循环，利用机器人实际执行而非物理模拟来优化产品设计，实现73.3%的语义识别准确率和86.9%的装配成功率


<details>
  <summary>Details</summary>
Motivation: 传统机器人装配设计依赖人工规划，耗时昂贵且复杂对象不实用。现有LLM方法依赖启发式策略和硬编码物理模拟器，无法适应真实装配环境

Method: IDfRA框架采用迭代循环：规划、执行、验证和重新规划，每个环节都基于自我评估，在固定但初始未完全指定的环境中逐步提升设计质量

Result: IDfRA在语义识别方面达到73.3%的top-1准确率，优于基线方法；装配计划展现出86.9%的整体构建成功率，设计质量随迭代提升

Conclusion: 通过整合自我验证和上下文感知适应，IDfRA框架在非结构化制造场景中展现出强大部署潜力，消除了对物理模拟的依赖

Abstract: As robots proliferate in manufacturing, Design for Robotic Assembly (DfRA),
which is designing products for efficient automated assembly, is increasingly
important. Traditional approaches to DfRA rely on manual planning, which is
time-consuming, expensive and potentially impractical for complex objects.
Large language models (LLM) have exhibited proficiency in semantic
interpretation and robotic task planning, stimulating interest in their
application to the automation of DfRA. But existing methodologies typically
rely on heuristic strategies and rigid, hard-coded physics simulators that may
not translate into real-world assembly contexts. In this work, we present
Iterative Design for Robotic Assembly (IDfRA), a framework using iterative
cycles of planning, execution, verification, and re-planning, each informed by
self-assessment, to progressively enhance design quality within a fixed yet
initially under-specified environment, thereby eliminating the physics
simulation with the real world itself. The framework accepts as input a target
structure together with a partial environmental representation. Through
successive refinement, it converges toward solutions that reconcile semantic
fidelity with physical feasibility. Empirical evaluation demonstrates that
IDfRA attains 73.3\% top-1 accuracy in semantic recognisability, surpassing the
baseline on this metric. Moreover, the resulting assembly plans exhibit robust
physical feasibility, achieving an overall 86.9\% construction success rate,
with design quality improving across iterations, albeit not always
monotonically. Pairwise human evaluation further corroborates the advantages of
IDfRA relative to alternative approaches. By integrating self-verification with
context-aware adaptation, the framework evidences strong potential for
deployment in unstructured manufacturing scenarios.

</details>


### [28] [Generalized Momenta-Based Koopman Formalism for Robust Control of Euler-Lagrangian Systems](https://arxiv.org/abs/2509.17010)
*Rajpal Singh,Aditya Singh,Chidre Shravista Kashyap,Jishnu Keshavan*

Main category: cs.RO

TL;DR: 本文提出了一种基于广义动量的隐式Koopman算子方法，用于欧拉-拉格朗日动力学建模，通过解耦线性驱动通道与状态依赖动力学，显著降低了模型复杂度和学习参数数量。


<details>
  <summary>Details</summary>
Motivation: 传统显式Koopman模型将输入与状态依赖项非线性耦合，需要构建计算昂贵的双线性模型。本文旨在开发更高效的线性Koopman建模方法，提高数据效率和预测性能。

Method: 采用隐式广义动量状态空间表示，分离线性驱动通道；提出两种神经网络架构构建Koopman嵌入；集成线性广义扩展状态观测器(GESO)进行实时扰动估计和补偿。

Result: 在机器人操作器上的轨迹跟踪仿真和实验验证表明，该方法相比现有技术具有更高的精度、鲁棒性和学习效率，线性模型性能优于传统双线性模型。

Conclusion: 所提出的动量基Koopman与GESO框架为欧拉-拉格朗日系统提供了一种高效、鲁棒的建模方案，显著降低了模型复杂度同时保持了优越性能。

Abstract: This paper presents a novel Koopman operator formulation for Euler Lagrangian
dynamics that employs an implicit generalized momentum-based state space
representation, which decouples a known linear actuation channel from state
dependent dynamics and makes the system more amenable to linear Koopman
modeling. By leveraging this structural separation, the proposed formulation
only requires to learn the unactuated dynamics rather than the complete
actuation dependent system, thereby significantly reducing the number of
learnable parameters, improving data efficiency, and lowering overall model
complexity. In contrast, conventional explicit formulations inherently couple
inputs with the state dependent terms in a nonlinear manner, making them more
suitable for bilinear Koopman models, which are more computationally expensive
to train and deploy. Notably, the proposed scheme enables the formulation of
linear models that achieve superior prediction performance compared to
conventional bilinear models while remaining substantially more efficient. To
realize this framework, we present two neural network architectures that
construct Koopman embeddings from actuated or unactuated data, enabling
flexible and efficient modeling across different tasks. Robustness is ensured
through the integration of a linear Generalized Extended State Observer (GESO),
which explicitly estimates disturbances and compensates for them in real time.
The combined momentum-based Koopman and GESO framework is validated through
comprehensive trajectory tracking simulations and experiments on robotic
manipulators, demonstrating superior accuracy, robustness, and learning
efficiency relative to state of the art alternatives.

</details>


### [29] [Orchestrate, Generate, Reflect: A VLM-Based Multi-Agent Collaboration Framework for Automated Driving Policy Learning](https://arxiv.org/abs/2509.17042)
*Zengqi Peng,Yusen Xie,Yubin Wang,Rui Yang,Qifeng Chen,Jun Ma*

Main category: cs.RO

TL;DR: OGR是一个基于视觉语言模型的多智能体协作框架，用于自动化驾驶策略学习，通过分层智能体系统自动生成奖励函数和训练课程，减少人工设计需求。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶策略学习需要大量人工设计奖励函数和训练课程，这是一个劳动密集且耗时的过程。

Method: 提出OGR框架，包含编排器、生成模块、反思模块和记忆模块，利用VLM的推理和多模态理解能力进行分层协作，采用并行生成和人机协同技术增强鲁棒性。

Result: 在CARLA模拟器中实验显示优越性能，在不同城市场景下具有强泛化能力，与多种RL算法兼容，真实世界实验验证了实用性。

Conclusion: OGR框架通过多智能体协作和丰富多模态信息，实现了自动驾驶策略的在线进化，具有实际应用价值。

Abstract: The advancement of foundation models fosters new initiatives for policy
learning in achieving safe and efficient autonomous driving. However, a
critical bottleneck lies in the manual engineering of reward functions and
training curricula for complex and dynamic driving tasks, which is a
labor-intensive and time-consuming process. To address this problem, we propose
OGR (Orchestrate, Generate, Reflect), a novel automated driving policy learning
framework that leverages vision-language model (VLM)-based multi-agent
collaboration. Our framework capitalizes on advanced reasoning and multimodal
understanding capabilities of VLMs to construct a hierarchical agent system.
Specifically, a centralized orchestrator plans high-level training objectives,
while a generation module employs a two-step analyze-then-generate process for
efficient generation of reward-curriculum pairs. A reflection module then
facilitates iterative optimization based on the online evaluation. Furthermore,
a dedicated memory module endows the VLM agents with the capabilities of
long-term memory. To enhance robustness and diversity of the generation
process, we introduce a parallel generation scheme and a human-in-the-loop
technique for augmentation of the reward observation space. Through efficient
multi-agent cooperation and leveraging rich multimodal information, OGR enables
the online evolution of reinforcement learning policies to acquire
interaction-aware driving skills. Extensive experiments in the CARLA simulator
demonstrate the superior performance, robust generalizability across distinct
urban scenarios, and strong compatibility with various RL algorithms. Further
real-world experiments highlight the practical viability and effectiveness of
our framework. The source code will be available upon acceptance of the paper.

</details>


### [30] [FILIC: Dual-Loop Force-Guided Imitation Learning with Impedance Torque Control for Contact-Rich Manipulation Tasks](https://arxiv.org/abs/2509.17053)
*Haizhou Ge,Yufei Jia,Zheng Li,Yue Li,Zhixing Chen,Ruqi Huang,Guyue Zhou*

Main category: cs.RO

TL;DR: FILIC是一个力引导的模仿学习框架，通过阻抗扭矩控制和双环结构实现接触丰富的柔顺操作，无需昂贵的力/扭矩传感器


<details>
  <summary>Details</summary>
Motivation: 解决传统模仿学习策略缺乏力感知的问题，以及为协作机器人添加力/扭矩传感器成本高的问题

Method: 结合Transformer-based IL策略和阻抗控制器，使用关节扭矩测量通过雅可比逆矩阵估计末端执行器力，并设计手持触觉和VR可视化的力反馈框架

Result: FILIC显著优于仅基于视觉和关节扭矩的方法，实现了更安全、更柔顺和适应性更强的接触丰富操作

Conclusion: 该框架为接触丰富的操作任务提供了一种成本效益高且有效的解决方案

Abstract: Contact-rich manipulation is crucial for robots to perform tasks requiring
precise force control, such as insertion, assembly, and in-hand manipulation.
However, most imitation learning (IL) policies remain position-centric and lack
explicit force awareness, and adding force/torque sensors to collaborative
robot arms is often costly and requires additional hardware design. To overcome
these issues, we propose FILIC, a Force-guided Imitation Learning framework
with impedance torque control. FILIC integrates a Transformer-based IL policy
with an impedance controller in a dual-loop structure, enabling compliant
force-informed, force-executed manipulation. For robots without force/torque
sensors, we introduce a cost-effective end-effector force estimator using joint
torque measurements through analytical Jacobian-based inversion while
compensating with model-predicted torques from a digital twin. We also design
complementary force feedback frameworks via handheld haptics and VR
visualization to improve demonstration quality. Experiments show that FILIC
significantly outperforms vision-only and joint-torque-based methods, achieving
safer, more compliant, and adaptable contact-rich manipulation. Our code can be
found in https://github.com/TATP-233/FILIC.

</details>


### [31] [RoboManipBaselines: A Unified Framework for Imitation Learning in Robotic Manipulation across Real and Simulated Environments](https://arxiv.org/abs/2509.17057)
*Masaki Murooka,Tomohiro Motoda,Ryoichi Nakajo,Hanbit Oh,Koshi Makihara,Keisuke Shirai,Yukiyasu Domae*

Main category: cs.RO

TL;DR: RoboManipBaselines是一个开源的机器人模仿学习框架，统一了仿真和真实机器人的数据收集、训练和评估流程


<details>
  <summary>Details</summary>
Motivation: 为机器人模仿学习提供一个系统化的基准测试平台，强调集成性、通用性、可扩展性和可重复性

Method: 开发了一个开放框架，统一数据收集、训练和评估流程，支持多种任务、机器人和多模态策略

Result: 成功创建了一个能够进行系统性基准测试的平台

Conclusion: RoboManipBaselines为机器人模仿学习研究提供了一个重要的基准测试工具

Abstract: RoboManipBaselines is an open framework for robot imitation learning that
unifies data collection, training, and evaluation across simulation and real
robots. We introduce it as a platform enabling systematic benchmarking of
diverse tasks, robots, and multimodal policies with emphasis on integration,
generality, extensibility, and reproducibility.

</details>


### [32] [CoPlanner: An Interactive Motion Planner with Contingency-Aware Diffusion for Autonomous Driving](https://arxiv.org/abs/2509.17080)
*Ruiguo Zhong,Ruoyu Yao,Pei Liu,Xiaolong Chen,Rui Yang,Jun Ma*

Main category: cs.RO

TL;DR: CoPlanner是一个统一框架，通过联合建模多智能体交互轨迹生成和应急感知运动规划，解决自动驾驶中预测与规划模块解耦导致的问题，在nuPlan基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统采用先生成后评估的方法，通常只采用最可能的结果，导致在关键场景中缺乏应急策略，且预测与规划模块解耦会造成社会不一致的联合轨迹。

Method: 提出应急感知扩散规划器(CoPlanner)，使用枢轴条件扩散机制锚定轨迹采样，保持时间一致性，同时随机生成多样化长期分支；设计应急感知多场景评分策略，平衡安全性、进展和舒适性。

Result: 在nuPlan基准测试的Val14和Test14数据集上，CoPlanner在反应性和非反应性设置下均显著优于现有最优方法，在安全性和舒适性方面实现显著改进。

Conclusion: CoPlanner的集成设计保留了可行的应急选项，增强了不确定性下的鲁棒性，实现了更真实的交互感知规划，为自动驾驶系统提供了更安全的决策框架。

Abstract: Accurate trajectory prediction and motion planning are crucial for autonomous
driving systems to navigate safely in complex, interactive environments
characterized by multimodal uncertainties. However, current
generation-then-evaluation frameworks typically construct multiple plausible
trajectory hypotheses but ultimately adopt a single most likely outcome,
leading to overconfident decisions and a lack of fallback strategies that are
vital for safety in rare but critical scenarios. Moreover, the usual decoupling
of prediction and planning modules could result in socially inconsistent or
unrealistic joint trajectories, especially in highly interactive traffic. To
address these challenges, we propose a contingency-aware diffusion planner
(CoPlanner), a unified framework that jointly models multi-agent interactive
trajectory generation and contingency-aware motion planning. Specifically, the
pivot-conditioned diffusion mechanism anchors trajectory sampling on a
validated, shared short-term segment to preserve temporal consistency, while
stochastically generating diverse long-horizon branches that capture multimodal
motion evolutions. In parallel, we design a contingency-aware multi-scenario
scoring strategy that evaluates candidate ego trajectories across multiple
plausible long-horizon evolution scenarios, balancing safety, progress, and
comfort. This integrated design preserves feasible fallback options and
enhances robustness under uncertainty, leading to more realistic
interaction-aware planning. Extensive closed-loop experiments on the nuPlan
benchmark demonstrate that CoPlanner consistently surpasses state-of-the-art
methods on both Val14 and Test14 datasets, achieving significant improvements
in safety and comfort under both reactive and non-reactive settings. Code and
model will be made publicly available upon acceptance.

</details>


### [33] [Imagine2Act: Leveraging Object-Action Motion Consistency from Imagined Goals for Robotic Manipulation](https://arxiv.org/abs/2509.17125)
*Liang Heng,Jiadong Xu,Yiwen Wang,Xiaoqi Li,Muhe Cai,Yan Shen,Juan Zhu,Guanghui Ren,Hao Dong*

Main category: cs.RO

TL;DR: Imagine2Act是一个3D模仿学习框架，通过生成想象的目标图像和点云来整合语义和几何约束，解决关系物体重排列任务中的高精度操作问题


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖预收集的演示数据难以捕捉复杂几何约束，要么生成目标状态观测但未能显式耦合物体变换与动作预测，导致生成噪声引起的错误

Method: 首先生成基于语言指令的想象目标图像并重建对应3D点云，将这些点云作为策略模型的额外输入，同时使用软姿态监督的对象-动作一致性策略显式对齐预测的末端执行器运动与生成的物体变换

Result: 在仿真和真实世界实验中，Imagine2Act超越了之前最先进的策略

Conclusion: 该设计使Imagine2Act能够推理物体间的语义和几何关系，并在多样化任务中预测准确动作

Abstract: Relational object rearrangement (ROR) tasks (e.g., insert flower to vase)
require a robot to manipulate objects with precise semantic and geometric
reasoning. Existing approaches either rely on pre-collected demonstrations that
struggle to capture complex geometric constraints or generate goal-state
observations to capture semantic and geometric knowledge, but fail to
explicitly couple object transformation with action prediction, resulting in
errors due to generative noise. To address these limitations, we propose
Imagine2Act, a 3D imitation-learning framework that incorporates semantic and
geometric constraints of objects into policy learning to tackle high-precision
manipulation tasks. We first generate imagined goal images conditioned on
language instructions and reconstruct corresponding 3D point clouds to provide
robust semantic and geometric priors. These imagined goal point clouds serve as
additional inputs to the policy model, while an object-action consistency
strategy with soft pose supervision explicitly aligns predicted end-effector
motion with generated object transformation. This design enables Imagine2Act to
reason about semantic and geometric relationships between objects and predict
accurate actions across diverse tasks. Experiments in both simulation and the
real world demonstrate that Imagine2Act outperforms previous state-of-the-art
policies. More visualizations can be found at
https://sites.google.com/view/imagine2act.

</details>


### [34] [History-Aware Visuomotor Policy Learning via Point Tracking](https://arxiv.org/abs/2509.17141)
*Jingjing Chen,Hongjie Fang,Chenxi Wang,Shiquan Wang,Cewu Lu*

Main category: cs.RO

TL;DR: 提出基于点跟踪的对象中心历史表示方法，用于解决视觉运动策略中的记忆需求问题，通过抽象过去观察为紧凑结构化形式，提升任务性能和决策准确性


<details>
  <summary>Details</summary>
Motivation: 大多数视觉运动策略依赖马尔可夫假设，难以处理重复状态或长时程依赖，现有方法无法满足多样化的记忆需求

Method: 基于点跟踪的对象中心历史表示，将跟踪点编码并在对象级别聚合，生成紧凑的历史表示，可无缝集成到各种视觉运动策略中

Result: 在多样化操作任务上的广泛评估表明，该方法能解决任务阶段识别、空间记忆、动作计数等多种记忆需求，性能优于马尔可夫基线和现有历史方法

Conclusion: 提出的对象中心历史表示方法提供了完整的历史感知能力，计算效率高，能显著提升整体任务性能和决策准确性

Abstract: Many manipulation tasks require memory beyond the current observation, yet
most visuomotor policies rely on the Markov assumption and thus struggle with
repeated states or long-horizon dependencies. Existing methods attempt to
extend observation horizons but remain insufficient for diverse memory
requirements. To this end, we propose an object-centric history representation
based on point tracking, which abstracts past observations into a compact and
structured form that retains only essential task-relevant information. Tracked
points are encoded and aggregated at the object level, yielding a compact
history representation that can be seamlessly integrated into various
visuomotor policies. Our design provides full history-awareness with high
computational efficiency, leading to improved overall task performance and
decision accuracy. Through extensive evaluations on diverse manipulation tasks,
we show that our method addresses multiple facets of memory requirements - such
as task stage identification, spatial memorization, and action counting, as
well as longer-term demands like continuous and pre-loaded memory - and
consistently outperforms both Markovian baselines and prior history-based
approaches. Project website: http://tonyfang.net/history

</details>


### [35] [MAST: Multi-Agent Spatial Transformer for Learning to Collaborate](https://arxiv.org/abs/2509.17195)
*Damian Owerko,Frederic Vatnsdal,Saurav Agarwal,Vijay Kumar,Alejandro Ribeiro*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的多智能体空间变换器（MAST），用于在大规模去中心化协作多机器人系统（DC-MRS）中学习通信策略。MAST通过改进的位置编码策略和注意力操作，解决了DC-MRS中的局部可观测状态、有限通信范围和独立行动执行等挑战。


<details>
  <summary>Details</summary>
Motivation: DC-MRS面临三个主要挑战：（i）机器人只能进行局部感知导致部分可观测状态；（ii）无中央服务器的有限通信范围；（iii）独立执行行动。机器人需要优化共同的任务目标，这必须在受限设置下通过展现期望协作行为的通信策略来实现。

Method: MAST是一种去中心化的变换器架构，通过学习通信策略来计算与其他智能体共享的抽象信息，并将接收到的信息与机器人自身观测相结合处理。MAST扩展了标准变换器，采用新的位置编码策略和注意力操作，使用窗口化来限制MRS的感知野，实现局部计算、平移等变性和排列等变性。

Result: 在去中心化分配与导航（DAN）和去中心化覆盖控制任务上验证了MAST的有效性。通过集中式设置下的模仿学习高效训练后，去中心化的MAST策略对通信延迟具有鲁棒性，能够扩展到大规模团队，性能优于基线和其他基于学习的方法。

Conclusion: MAST是为DC-MRS设计的有前景的方法，其新颖的变换器架构设计使其能够有效处理大规模去中心化协作系统中的通信和学习问题，展现出良好的可扩展性和鲁棒性。

Abstract: This article presents a novel multi-agent spatial transformer (MAST) for
learning communication policies in large-scale decentralized and collaborative
multi-robot systems (DC-MRS). Challenges in collaboration in DC-MRS arise from:
(i) partial observable states as robots make only localized perception, (ii)
limited communication range with no central server, and (iii) independent
execution of actions. The robots need to optimize a common task-specific
objective, which, under the restricted setting, must be done using a
communication policy that exhibits the desired collaborative behavior. The
proposed MAST is a decentralized transformer architecture that learns
communication policies to compute abstract information to be shared with other
agents and processes the received information with the robot's own
observations. The MAST extends the standard transformer with new positional
encoding strategies and attention operations that employ windowing to limit the
receptive field for MRS. These are designed for local computation,
shift-equivariance, and permutation equivariance, making it a promising
approach for DC-MRS. We demonstrate the efficacy of MAST on decentralized
assignment and navigation (DAN) and decentralized coverage control. Efficiently
trained using imitation learning in a centralized setting, the decentralized
MAST policy is robust to communication delays, scales to large teams, and
performs better than the baselines and other learning-based approaches.

</details>


### [36] [Certifiably Optimal Doppler Positioning using Opportunistic LEO Satellites](https://arxiv.org/abs/2509.17198)
*Baoshan Song,Weisong Wen,Qi Zhang,Bing Xu,Li-Ta Hsu*

Main category: cs.RO

TL;DR: 提出一种基于凸优化的可证明最优LEO多普勒定位方法，通过GWA算法和SDP松弛实现，无需初始估计即可获得全局最优解


<details>
  <summary>Details</summary>
Motivation: 传统多普勒定位方法需要精确初始估计，在未知环境中容易陷入局部最优，需要一种无需初始化的全局优化方法

Method: 采用渐进权重逼近(GWA)算法和半定规划(SDP)松弛技术，推导了无噪声情况下的最优性必要条件和有噪声情况下的充分噪声边界条件

Result: 实际测试显示该方法在无需初始估计情况下获得140米3D定位误差，而传统方法在初始点偏离1000公里时会陷入局部最优

Conclusion: 该方法能提供可证明的最优解，也可作为局部搜索方法的初始化，将定位误差进一步降低到130米

Abstract: To provide backup and augmentation to global navigation satellite system
(GNSS), Doppler shift from Low Earth Orbit (LEO) satellites can be employed as
signals of opportunity (SOP) for position, navigation and timing (PNT). Since
the Doppler positioning problem is non-convex, local searching methods may
produce two types of estimates: a global optimum without notice or a local
optimum given an inexact initial estimate. As exact initialization is
unavailable in some unknown environments, a guaranteed global optimization
method in no need of initialization becomes necessary. To achieve this goal, we
propose a certifiably optimal LEO Doppler positioning method by utilizing
convex optimization. In this paper, the certifiable positioning method is
implemented through a graduated weight approximation (GWA) algorithm and
semidefinite programming (SDP) relaxation. To guarantee the optimality, we
derive the necessary conditions for optimality in ideal noiseless cases and
sufficient noise bounds conditions in noisy cases. Simulation and real tests
are conducted to evaluate the effectiveness and robustness of the proposed
method. Specially, the real test using Iridium-NEXT satellites shows that the
proposed method estimates an certifiably optimal solution with an 3D
positioning error of 140 m without initial estimates while Gauss-Newton and
Dog-Leg are trapped in local optima when the initial point is equal or larger
than 1000 km away from the ground truth. Moreover, the certifiable estimation
can also be used as initialization in local searching methods to lower down the
3D positioning error to 130 m.

</details>


### [37] [Ratatouille: Imitation Learning Ingredients for Real-world Social Robot Navigation](https://arxiv.org/abs/2509.17204)
*James R. Han,Mithun Vanniasinghe,Hshmat Sahak,Nicholas Rhinehart,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: Ratatouille是一个用于社交机器人导航的离线模仿学习管道和模型架构，通过精心设计的架构和训练方法，相比简单的行为克隆方法，将每米碰撞次数减少6倍，成功率提高3倍。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习方法在社交机器人导航中既数据密集又不安全，而简单的离线模仿学习（行为克隆）性能不足，需要更有效的架构设计来提升安全性和可靠性。

Method: 提出Ratatouille管道和模型架构，通过精心设计的架构和训练选择，在不改变数据的情况下改进离线模仿学习性能，在仿真和真实环境中进行验证。

Result: 在密集大学校园收集超过11小时数据，结果显示碰撞率降低6倍，成功率提高3倍，并在公共美食广场展示了定性结果。

Conclusion: 研究表明，深思熟虑的模仿学习设计比增加数据更能显著提高真实世界社交导航的安全性和可靠性。

Abstract: Scaling Reinforcement Learning to in-the-wild social robot navigation is both
data-intensive and unsafe, since policies must learn through direct interaction
and inevitably encounter collisions. Offline Imitation learning (IL) avoids
these risks by collecting expert demonstrations safely, training entirely
offline, and deploying policies zero-shot. However, we find that naively
applying Behaviour Cloning (BC) to social navigation is insufficient; achieving
strong performance requires careful architectural and training choices. We
present Ratatouille, a pipeline and model architecture that, without changing
the data, reduces collisions per meter by 6 times and improves success rate by
3 times compared to naive BC. We validate our approach in both simulation and
the real world, where we collected over 11 hours of data on a dense university
campus. We further demonstrate qualitative results in a public food court. Our
findings highlight that thoughtful IL design, rather than additional data, can
substantially improve safety and reliability in real-world social navigation.
Video: https://youtu.be/tOdLTXsaYLQ. Code will be released after acceptance.

</details>


### [38] [Combining Performance and Passivity in Linear Control of Series Elastic Actuators](https://arxiv.org/abs/2509.17210)
*Shaunak A. Mehta,Dylan P. Losey*

Main category: cs.RO

TL;DR: 本文探讨了串联弹性执行器（SEAs）在安全性和性能之间的权衡问题，通过比较不同的线性控制和机械配置，发现执行器侧PD控制结合弹性传动中的阻尼器能够实现高精度性能并确保用户安全。


<details>
  <summary>Details</summary>
Motivation: 在人与机器人物理交互时，需要机器人既安全又高效。串联弹性执行器通过引入柔性驱动提高了安全性，但弹簧的加入会引入振荡并降低运动精度。因此需要在物理安全性和性能之间找到平衡点。

Method: 枚举串联弹性执行器的不同线性控制和机械配置，分析每种选择对柔顺性、无源性和跟踪性能的影响。重点研究执行器侧控制，采用简单的PD控制器，并结合弹性传动中的阻尼器。

Result: 执行器侧PD控制允许更宽的安全控制增益范围，结合弹性传动中的阻尼器可实现高性能。通过设计低物理刚度和高控制器增益的系统，能够在确保用户安全的同时实现精确性能。

Conclusion: 通过低物理刚度和高控制器增益的设计，结合执行器侧PD控制和弹性传动阻尼器，可以同时实现串联弹性执行器的高精度性能和碰撞安全性。

Abstract: When humans physically interact with robots, we need the robots to be both
safe and performant. Series elastic actuators (SEAs) fundamentally advance
safety by introducing compliant actuation. On the one hand, adding a spring
mitigates the impact of accidental collisions between human and robot; but on
the other hand, this spring introduces oscillations and fundamentally decreases
the robot's ability to perform precise, accurate motions. So how should we
trade off between physical safety and performance? In this paper, we enumerate
the different linear control and mechanical configurations for series elastic
actuators, and explore how each choice affects the rendered compliance,
passivity, and tracking performance. While prior works focus on load side
control, we find that actuator side control has significant benefits. Indeed,
simple PD controllers on the actuator side allow for a much wider range of
control gains that maintain safety, and combining these with a damper in the
elastic transmission yields high performance. Our simulations and real world
experiments suggest that, by designing a system with low physical stiffness and
high controller gains, this solution enables accurate performance while also
ensuring user safety during collisions.

</details>


### [39] [Neural Network and ANFIS based auto-adaptive MPC for path tracking in autonomous vehicles](https://arxiv.org/abs/2509.17213)
*Yassine Kebbati,Naima Ait-Oufroukh,Vincent Vigneron,Dalil Ichala*

Main category: cs.RO

TL;DR: 本文设计了一种用于自动驾驶汽车路径跟踪的自适应MPC控制器，采用改进的粒子群优化算法进行调参，并使用神经网络和ANFIS进行在线参数自适应。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车在动态环境中运行，面临各种不确定性和干扰，使得传统控制器在横向控制方面效果不佳。

Method: 设计自适应MPC控制器，通过改进的粒子群优化算法进行参数调优，并利用神经网络和ANFIS实现在线参数自适应。

Result: 在三车道变换和轨迹跟踪场景中，设计的控制器相比标准MPC表现出更好的性能。

Conclusion: 所提出的自适应MPC控制器在自动驾驶路径跟踪任务中具有良好应用前景。

Abstract: Self-driving cars operate in constantly changing environments and are exposed
to a variety of uncertainties and disturbances. These factors render classical
controllers ineffective, especially for lateral control. Therefore, an adaptive
MPC controller is designed in this paper for the path tracking task, tuned by
an improved particle swarm optimization algorithm. Online parameter adaptation
is performed using Neural Networks and ANFIS. The designed controller showed
promising results compared to standard MPC in triple lane change and trajectory
tracking scenarios. Code can be found here:
https://github.com/yassinekebbati/NN_MPC-vs-ANFIS_MPC

</details>


### [40] [Scalable Multi Agent Diffusion Policies for Coverage Control](https://arxiv.org/abs/2509.17244)
*Frederic Vatnsdal,Romina Garcia Camargo,Saurav Agarwal,Alejandro Ribeiro*

Main category: cs.RO

TL;DR: MADP是一种基于扩散模型的去中心化机器人群体协作方法，通过扩散模型生成复杂高维动作分布，解决覆盖控制问题


<details>
  <summary>Details</summary>
Motivation: 解决去中心化机器人群体在复杂环境中协作的挑战，特别是覆盖控制这类典型多智能体导航问题

Method: 使用扩散模型生成智能体间相互依赖的动作分布，每个机器人基于自身观察和同伴感知嵌入的条件进行策略采样，通过模仿学习从专家策略训练

Result: 实验表明MADP继承了扩散模型的优良特性，能够跨智能体密度和环境泛化，性能优于现有最优基线方法

Conclusion: MADP为去中心化机器人群体协作提供了一种有效的扩散模型方法，在覆盖控制任务中表现出强大的泛化能力和鲁棒性

Abstract: We propose MADP, a novel diffusion-model-based approach for collaboration in
decentralized robot swarms. MADP leverages diffusion models to generate samples
from complex and high-dimensional action distributions that capture the
interdependencies between agents' actions. Each robot conditions policy
sampling on a fused representation of its own observations and perceptual
embeddings received from peers. To evaluate this approach, we task a team of
holonomic robots piloted by MADP to address coverage control-a canonical multi
agent navigation problem. The policy is trained via imitation learning from a
clairvoyant expert on the coverage control problem, with the diffusion process
parameterized by a spatial transformer architecture to enable decentralized
inference. We evaluate the system under varying numbers, locations, and
variances of importance density functions, capturing the robustness demands of
real-world coverage tasks. Experiments demonstrate that our model inherits
valuable properties from diffusion models, generalizing across agent densities
and environments, and consistently outperforming state-of-the-art baselines.

</details>


### [41] [Learning and Optimization with 3D Orientations](https://arxiv.org/abs/2509.17274)
*Alexandros Ntagkas,Constantinos Tsakonas,Chairi Kiourt,Konstantinos Chatzilygeroudis*

Main category: cs.RO

TL;DR: 本文系统性地综述了3D方向表示方法，并针对机器人学中的典型场景进行了基准测试，提供了基于实证的推荐指南和参考实现。


<details>
  <summary>Details</summary>
Motivation: 现有的3D方向表示方法众多且各有优劣，但在机器人学领域缺乏统一的综述和实证比较。研究者需要查阅大量文献才能全面了解各种可能性，且缺乏基于实际场景的性能基准测试来指导方法选择。

Method: 1）以统一符号清晰简洁地呈现所有可用的3D方向表示方法及相关技巧（包括李群代数）；2）在四种代表性机器人场景中进行基准测试：直接优化、模仿/监督学习（神经网络控制器）、强化学习、基于微分动态规划的轨迹优化。

Result: 通过系统实验比较了不同表示方法在各种场景下的性能表现，获得了基于实证数据的性能对比结果。

Conclusion: 根据不同场景提供了具体的使用指南，并开源了所有方向数学的参考实现，为研究者和实践者提供了实用的选择依据和工具支持。

Abstract: There exist numerous ways of representing 3D orientations. Each
representation has both limitations and unique features. Choosing the best
representation for one task is often a difficult chore, and there exist
conflicting opinions on which representation is better suited for a set of
family of tasks. Even worse, when dealing with scenarios where we need to learn
or optimize functions with orientations as inputs and/or outputs, the set of
possibilities (representations, loss functions, etc.) is even larger and it is
not easy to decide what is best for each scenario. In this paper, we attempt to
a) present clearly, concisely and with unified notation all available
representations, and "tricks" related to 3D orientations (including Lie Group
algebra), and b) benchmark them in representative scenarios. The first part
feels like it is missing from the robotics literature as one has to read many
different textbooks and papers in order have a concise and clear understanding
of all possibilities, while the benchmark is necessary in order to come up with
recommendations based on empirical evidence. More precisely, we experiment with
the following settings that attempt to cover most widely used scenarios in
robotics: 1) direct optimization, 2) imitation/supervised learning with a
neural network controller, 3) reinforcement learning, and 4) trajectory
optimization using differential dynamic programming. We finally provide
guidelines depending on the scenario, and make available a reference
implementation of all the orientation math described.

</details>


### [42] [Event-Based Visual Teach-and-Repeat via Fast Fourier-Domain Cross-Correlation](https://arxiv.org/abs/2509.17287)
*Gokul B. Nair,Alejandro Fontan,Michael Milford,Tobias Fischer*

Main category: cs.RO

TL;DR: 提出了首个基于事件相机的视觉示教重复导航系统，通过频域互相关框架实现300Hz以上的处理速度，比传统帧式方法快一个数量级。


<details>
  <summary>Details</summary>
Motivation: 传统帧式相机30-60Hz的固定帧率限制了系统响应性，在环境变化与控制响应之间存在固有延迟。

Method: 开发频域互相关框架，将事件流匹配问题转化为计算高效的傅里叶空间乘法；利用事件帧的二进制特性和图像压缩技术提升计算速度。

Result: 在室内外4000+米轨迹上成功实现自主导航，ATE低于24cm，保持高频控制更新；处理速率超过300Hz，显著高于传统帧式系统。

Conclusion: 事件感知在实时机器人导航中具有实际可行性，能够实现更高的更新速率和更好的响应性能。

Abstract: Visual teach-and-repeat navigation enables robots to autonomously traverse
previously demonstrated paths by comparing current sensory input with recorded
trajectories. However, conventional frame-based cameras fundamentally limit
system responsiveness: their fixed frame rates (typically 30-60 Hz) create
inherent latency between environmental changes and control responses. Here we
present the first event-camera-based visual teach-and-repeat system. To achieve
this, we develop a frequency-domain cross-correlation framework that transforms
the event stream matching problem into computationally efficient Fourier space
multiplications, capable of exceeding 300Hz processing rates, an order of
magnitude faster than frame-based approaches. By exploiting the binary nature
of event frames and applying image compression techniques, we further enhance
the computational speed of the cross-correlation process without sacrificing
localization accuracy. Extensive experiments using a Prophesee EVK4 HD event
camera mounted on an AgileX Scout Mini robot demonstrate successful autonomous
navigation across 4000+ meters of indoor and outdoor trajectories. Our system
achieves ATEs below 24 cm while maintaining consistent high-frequency control
updates. Our evaluations show that our approach achieves substantially higher
update rates compared to conventional frame-based systems, underscoring the
practical viability of event-based perception for real-time robotic navigation.

</details>


### [43] [Automated Coral Spawn Monitoring for Reef Restoration: The Coral Spawn and Larvae Imaging Camera System (CSLICS)](https://arxiv.org/abs/2509.17299)
*Dorian Tsai,Christopher A. Brunner,Riki Lamont,F. Mikaela Nordborg,Andrea Severati,Java Terry,Karen Jackel,Matthew Dunbabin,Tobias Fischer,Scarlett Raine*

Main category: cs.RO

TL;DR: CSLICS系统通过低成本模块化摄像头和基于人机交互标注的目标检测器，实现了珊瑚产卵的自动计数，显著减少了人工劳动时间并提高了珊瑚养殖效率。


<details>
  <summary>Details</summary>
Motivation: 当前珊瑚养殖中的产卵计数方法劳动密集，成为珊瑚生产流程的关键瓶颈，需要自动化解决方案来提升珊瑚礁恢复工作的规模。

Method: 使用低成本模块化摄像头系统，结合基于人机交互标注训练的目标检测器，对幼虫饲养池中的珊瑚产卵进行检测、分类和计数。

Result: 实验结果显示表面产卵检测F1分数达82.4%，水下产卵检测F1分数65.3%，每次产卵事件可节省5720小时人工劳动。

Conclusion: CSLICS系统能够准确测量受精成功率和产卵数量，增强了珊瑚养殖过程，有助于扩大珊瑚礁恢复规模以应对气候变化威胁。

Abstract: Coral aquaculture for reef restoration requires accurate and continuous spawn
counting for resource distribution and larval health monitoring, but current
methods are labor-intensive and represent a critical bottleneck in the coral
production pipeline. We propose the Coral Spawn and Larvae Imaging Camera
System (CSLICS), which uses low cost modular cameras and object detectors
trained using human-in-the-loop labeling approaches for automated spawn
counting in larval rearing tanks. This paper details the system engineering,
dataset collection, and computer vision techniques to detect, classify and
count coral spawn. Experimental results from mass spawning events demonstrate
an F1 score of 82.4\% for surface spawn detection at different embryogenesis
stages, 65.3\% F1 score for sub-surface spawn detection, and a saving of 5,720
hours of labor per spawning event compared to manual sampling methods at the
same frequency. Comparison of manual counts with CSLICS monitoring during a
mass coral spawning event on the Great Barrier Reef demonstrates CSLICS'
accurate measurement of fertilization success and sub-surface spawn counts.
These findings enhance the coral aquaculture process and enable upscaling of
coral reef restoration efforts to address climate change threats facing
ecosystems like the Great Barrier Reef.

</details>


### [44] [Pose Estimation of a Cable-Driven Serpentine Manipulator Utilizing Intrinsic Dynamics via Physical Reservoir Computing](https://arxiv.org/abs/2509.17308)
*Kazutoshi Tanaka,Tomoya Takahashi,Masashi Hamaya*

Main category: cs.RO

TL;DR: 提出了一种基于物理储层计算的位姿估计方法，用于解决轻量化缆驱蛇形机械臂的灵活性引起的位姿估计挑战。


<details>
  <summary>Details</summary>
Motivation: 缆驱蛇形机械臂在非结构化环境中具有巨大潜力，但轻量化设计带来的灵活性变化（如缆绳松弛、伸长和连杆变形）导致解析预测与实际位姿存在差异，使位姿估计更加困难。

Method: 利用机械臂固有的非线性动力学作为高维储层，提出基于物理储层计算的位姿估计方法。

Result: 实验结果显示，该方法平均位姿误差为4.3毫米，优于LSTM网络的4.4毫米和解析方法的39.5毫米。

Conclusion: 这项工作为利用轻量化缆驱蛇形机械臂固有动力学的控制和感知策略提供了新方向。

Abstract: Cable-driven serpentine manipulators hold great potential in unstructured
environments, offering obstacle avoidance, multi-directional force application,
and a lightweight design. By placing all motors and sensors at the base and
employing plastic links, we can further reduce the arm's weight. To demonstrate
this concept, we developed a 9-degree-of-freedom cable-driven serpentine
manipulator with an arm length of 545 mm and a total mass of only 308 g.
However, this design introduces flexibility-induced variations, such as cable
slack, elongation, and link deformation. These variations result in
discrepancies between analytical predictions and actual link positions, making
pose estimation more challenging. To address this challenge, we propose a
physical reservoir computing based pose estimation method that exploits the
manipulator's intrinsic nonlinear dynamics as a high-dimensional reservoir.
Experimental results show a mean pose error of 4.3 mm using our method,
compared to 4.4 mm with a baseline long short-term memory network and 39.5 mm
with an analytical approach. This work provides a new direction for control and
perception strategies in lightweight cable-driven serpentine manipulators
leveraging their intrinsic dynamics.

</details>


### [45] [OpenGVL - Benchmarking Visual Temporal Progress for Data Curation](https://arxiv.org/abs/2509.17321)
*Paweł Budzianowski,Emilia Wiśnios,Gracjan Góral,Igor Kulakov,Viktor Petrenko,Krzysztof Walas*

Main category: cs.RO

TL;DR: OpenGVL是一个用于评估任务进度预测的基准测试，专注于机器人操作任务，旨在解决机器人数据稀缺问题并实现大规模数据自动标注和筛选。


<details>
  <summary>Details</summary>
Motivation: 机器人领域面临数据稀缺的挑战，而野外可用的机器人数据正在指数级增长。可靠的时间任务完成预测可以帮助自动标注和筛选这些大规模数据。

Method: 基于生成价值学习(GVL)方法，利用视觉语言模型(VLMs)从视觉观察中预测任务进度。OpenGVL提供了一个全面的基准测试，涵盖各种具有挑战性的机器人操作任务。

Result: 评估显示开源基础模型在时间进度预测任务上表现显著低于闭源模型，仅达到闭源模型约70%的性能。OpenGVL被证明可以作为自动化数据筛选和管理的实用工具。

Conclusion: OpenGVL基准测试的发布为机器人社区提供了评估任务进度预测能力的标准工具，有助于推动大规模机器人数据的有效利用和质量评估。

Abstract: Data scarcity remains one of the most limiting factors in driving progress in
robotics. However, the amount of available robotics data in the wild is growing
exponentially, creating new opportunities for large-scale data utilization.
Reliable temporal task completion prediction could help automatically annotate
and curate this data at scale. The Generative Value Learning (GVL) approach was
recently proposed, leveraging the knowledge embedded in vision-language models
(VLMs) to predict task progress from visual observations. Building upon GVL, we
propose OpenGVL, a comprehensive benchmark for estimating task progress across
diverse challenging manipulation tasks involving both robotic and human
embodiments. We evaluate the capabilities of publicly available open-source
foundation models, showing that open-source model families significantly
underperform closed-source counterparts, achieving only approximately $70\%$ of
their performance on temporal progress prediction tasks. Furthermore, we
demonstrate how OpenGVL can serve as a practical tool for automated data
curation and filtering, enabling efficient quality assessment of large-scale
robotics datasets. We release the benchmark along with the complete codebase at
\href{github.com/budzianowski/opengvl}{OpenGVL}.

</details>


### [46] [AERO-MPPI: Anchor-Guided Ensemble Trajectory Optimization for Agile Mapless Drone Navigation](https://arxiv.org/abs/2509.17340)
*Xin Chen,Rui Huang,Longbin Tang,Lin Zhao*

Main category: cs.RO

TL;DR: AERO-MPPI是一个完全GPU加速的框架，通过锚引导的MPPI优化器集合统一感知和规划，实现无人机在杂乱3D环境中的实时敏捷无地图导航。


<details>
  <summary>Details</summary>
Motivation: 传统的地图-规划-控制流水线在杂乱3D环境中计算成本高且会传播估计误差，需要一种更高效的导航方法。

Method: 设计多分辨率LiDAR点云表示快速提取空间分布的"锚点"作为前瞻中间端点，构建多项式轨迹引导探索不同同伦路径类，并行运行多个MPPI实例并使用两阶段多目标成本函数评估。

Result: 在森林、垂直和倾斜环境中实现超过7m/s的持续可靠飞行，成功率超过80%，轨迹比现有基线更平滑。在配备LiDAR的四旋翼无人机上实时运行，在复杂杂乱环境中实现安全、敏捷和鲁棒的飞行。

Conclusion: AERO-MPPI通过GPU加速和并行MPPI优化，有效解决了单MPPI方法的局部极小值问题，实现了无人机在复杂3D环境中的实时高性能导航。

Abstract: Agile mapless navigation in cluttered 3D environments poses significant
challenges for autonomous drones. Conventional mapping-planning-control
pipelines incur high computational cost and propagate estimation errors. We
present AERO-MPPI, a fully GPU-accelerated framework that unifies perception
and planning through an anchor-guided ensemble of Model Predictive Path
Integral (MPPI) optimizers. Specifically, we design a multi-resolution LiDAR
point-cloud representation that rapidly extracts spatially distributed
"anchors" as look-ahead intermediate endpoints, from which we construct
polynomial trajectory guides to explore distinct homotopy path classes. At each
planning step, we run multiple MPPI instances in parallel and evaluate them
with a two-stage multi-objective cost that balances collision avoidance and
goal reaching. Implemented entirely with NVIDIA Warp GPU kernels, AERO-MPPI
achieves real-time onboard operation and mitigates the local-minima failures of
single-MPPI approaches. Extensive simulations in forests, verticals, and
inclines demonstrate sustained reliable flight above 7 m/s, with success rates
above 80% and smoother trajectories compared to state-of-the-art baselines.
Real-world experiments on a LiDAR-equipped quadrotor with NVIDIA Jetson Orin NX
16G confirm that AERO-MPPI runs in real time onboard and consistently achieves
safe, agile, and robust flight in complex cluttered environments. The code will
be open-sourced upon acceptance of the paper.

</details>


### [47] [DyDexHandover: Human-like Bimanual Dynamic Dexterous Handover using RGB-only Perception](https://arxiv.org/abs/2509.17350)
*Haoran Zhou,Yangwei You,Shuaijun Wang*

Main category: cs.RO

TL;DR: DyDexHandover是一个基于多智能体强化学习的框架，使用RGB视觉实现双手机器人的空中物体投掷和捕捉，无需动力学模型或深度感知，实现了接近人类的自然动作。


<details>
  <summary>Details</summary>
Motivation: 解决双手机器人空中交接物体的基本挑战，克服现有方法依赖动力学模型、强先验或深度感知的限制，提高泛化能力和动作自然性。

Method: 采用多智能体强化学习训练端到端的RGB策略，通过人类策略正则化方案引导投掷策略，鼓励流畅自然的动作，并在Isaac Sim中构建双臂仿真环境进行实验评估。

Result: 在训练物体上达到接近99%的成功率，在未见物体上达到75%的成功率，同时生成类似人类的投掷和捕捉行为。

Conclusion: 这是首个仅使用原始RGB感知实现双手机器人空中交接的方法，展示了良好的泛化能力和自然动作生成。

Abstract: Dynamic in air handover is a fundamental challenge for dual-arm robots,
requiring accurate perception, precise coordination, and natural motion. Prior
methods often rely on dynamics models, strong priors, or depth sensing,
limiting generalization and naturalness. We present DyDexHandover, a novel
framework that employs multi-agent reinforcement learning to train an end to
end RGB based policy for bimanual object throwing and catching. To achieve more
human-like behavior, the throwing policy is guided by a human policy
regularization scheme, encouraging fluid and natural motion, and enhancing the
generalization capability of the policy. A dual arm simulation environment was
built in Isaac Sim for experimental evaluation. DyDexHandover achieves nearly
99 percent success on training objects and 75 percent on unseen objects, while
generating human-like throwing and catching behaviors. To our knowledge, it is
the first method to realize dual-arm in-air handover using only raw RGB
perception.

</details>


### [48] [Fast Trajectory Planner with a Reinforcement Learning-based Controller for Robotic Manipulators](https://arxiv.org/abs/2509.17381)
*Yongliang Wang,Hamidreza Kasaei*

Main category: cs.RO

TL;DR: 提出了一种结合视觉任务空间路径规划和强化学习关节空间避障的快速轨迹规划系统，通过改进PPO算法提高机器人在复杂环境中的避障效率和目标到达精度。


<details>
  <summary>Details</summary>
Motivation: 在非结构化和杂乱环境中为机械臂生成无障碍轨迹仍然是一个重大挑战，现有方法需要额外计算量来求解运动学或动力学方程。

Method: 将框架分为两个关键组件：1）基于视觉的任务空间轨迹规划器，结合快速分割模型和B样条优化的运动学路径搜索；2）改进的PPO算法，集成动作集成和策略反馈机制。

Result: 实验结果表明PPO增强方法的有效性，以及仿真到仿真和仿真到现实的迁移能力，提高了模型在复杂场景中的鲁棒性和规划器效率。

Conclusion: 该方法使机器人能够在遮挡环境中执行避障和实时轨迹规划，增强了算法在不同机器人任务中的适应性。

Abstract: Generating obstacle-free trajectories for robotic manipulators in
unstructured and cluttered environments remains a significant challenge.
Existing motion planning methods often require additional computational effort
to generate the final trajectory by solving kinematic or dynamic equations.
This paper highlights the strong potential of model-free reinforcement learning
methods over model-based approaches for obstacle-free trajectory planning in
joint space. We propose a fast trajectory planning system for manipulators that
combines vision-based path planning in task space with reinforcement
learning-based obstacle avoidance in joint space. We divide the framework into
two key components. The first introduces an innovative vision-based trajectory
planner in task space, leveraging the large-scale fast segment anything (FSA)
model in conjunction with basis spline (B-spline)-optimized kinodynamic path
searching. The second component enhances the proximal policy optimization (PPO)
algorithm by integrating action ensembles (AE) and policy feedback (PF), which
greatly improve precision and stability in goal-reaching and obstacle avoidance
within the joint space. These PPO enhancements increase the algorithm's
adaptability across diverse robotic tasks, ensuring consistent execution of
commands from the first component by the manipulator, while also enhancing both
obstacle avoidance efficiency and reaching accuracy. The experimental results
demonstrate the effectiveness of PPO enhancements, as well as
simulation-to-simulation (Sim-to-Sim) and simulation-to-reality (Sim-to-Real)
transfer, in improving model robustness and planner efficiency in complex
scenarios. These enhancements allow the robot to perform obstacle avoidance and
real-time trajectory planning in obstructed environments. Project page
available at: https://sites.google.com/view/ftp4rm/home

</details>


### [49] [High-Precision and High-Efficiency Trajectory Tracking for Excavators Based on Closed-Loop Dynamics](https://arxiv.org/abs/2509.17387)
*Ziqing Zou,Cong Wang,Yue Hu,Xiao Liu,Bowen Xu,Rong Xiong,Changjie Fan,Yingfeng Chen,Yue Wang*

Main category: cs.RO

TL;DR: EfficientTrack是一种用于液压挖掘机轨迹跟踪的方法，结合了基于模型的学习来处理非线性动力学，利用闭环动力学提高学习效率，显著减少跟踪误差。


<details>
  <summary>Details</summary>
Motivation: 液压挖掘机的复杂非线性动力学（如时间延迟和控制耦合）给高精度轨迹跟踪带来挑战，传统控制方法难以有效处理这些非线性特性，而常用学习型方法需要大量环境交互导致效率低下。

Method: 提出EfficientTrack方法，集成基于模型的学习来管理非线性动力学，并利用闭环动力学提高学习效率。通过仿真和真实挖掘机实验进行验证。

Result: 仿真对比实验表明，该方法优于现有学习型方法，以最少的交互次数实现了最高的跟踪精度和平滑度。真实实验证明该方法在负载条件下仍然有效，并具备持续学习能力。

Conclusion: EfficientTrack方法具有实际应用价值，在液压挖掘机轨迹跟踪任务中表现出色，兼具高精度和高效率的特点。

Abstract: The complex nonlinear dynamics of hydraulic excavators, such as time delays
and control coupling, pose significant challenges to achieving high-precision
trajectory tracking. Traditional control methods often fall short in such
applications due to their inability to effectively handle these nonlinearities,
while commonly used learning-based methods require extensive interactions with
the environment, leading to inefficiency. To address these issues, we introduce
EfficientTrack, a trajectory tracking method that integrates model-based
learning to manage nonlinear dynamics and leverages closed-loop dynamics to
improve learning efficiency, ultimately minimizing tracking errors. We validate
our method through comprehensive experiments both in simulation and on a
real-world excavator. Comparative experiments in simulation demonstrate that
our method outperforms existing learning-based approaches, achieving the
highest tracking precision and smoothness with the fewest interactions.
Real-world experiments further show that our method remains effective under
load conditions and possesses the ability for continual learning, highlighting
its practical applicability. For implementation details and source code, please
refer to https://github.com/ZiqingZou/EfficientTrack.

</details>


### [50] [3D Printable Soft Liquid Metal Sensors for Delicate Manipulation Tasks](https://arxiv.org/abs/2509.17389)
*Lois Liow,Jonty Milford,Emre Uygun,Andre Farinha,Vinoth Viswanathan,Josh Pinskier,David Howard*

Main category: cs.RO

TL;DR: 提出了一种打印高传感器化软体'物理孪生'的新方法，用于保护生态系统中脆弱样本的自动化处理，通过3D扫描创建可定制的软传感结构，应用于珊瑚处理等精细操作任务。


<details>
  <summary>Details</summary>
Motivation: 解决在保护生态系统中与脆弱样本交互时的损伤问题，以及为基于学习的操作策略提供安全数据采集途径，避免对活体珊瑚等生物的实验需求。

Method: 开发自动化设计工作流程，从3D扫描或模型创建复杂可定制的3D软传感结构，使用软液态金属传感器精确复制自然几何形状。

Result: 传感珊瑚能够检测低于0.5N的抓取力，有效捕捉珊瑚处理所需的精细交互和轻微接触力，在实验室识别和机器人珊瑚养殖等应用中表现优异。

Conclusion: 物理孪生技术为处理脆弱物品提供了比传统传感器更丰富的抓取反馈，为自主珊瑚处理和软体操作提供了伦理且可扩展的途径。

Abstract: Robotics and automation are key enablers to increase throughput in ongoing
conservation efforts across various threatened ecosystems. Cataloguing,
digitisation, husbandry, and similar activities require the ability to interact
with delicate, fragile samples without damaging them. Additionally,
learning-based solutions to these tasks require the ability to safely acquire
data to train manipulation policies through, e.g., reinforcement learning. To
address these twin needs, we introduce a novel method to print free-form,
highly sensorised soft 'physical twins'. We present an automated design
workflow to create complex and customisable 3D soft sensing structures on
demand from 3D scans or models. Compared to the state of the art, our soft
liquid metal sensors faithfully recreate complex natural geometries and display
excellent sensing properties suitable for validating performance in delicate
manipulation tasks. We demonstrate the application of our physical twins as
'sensing corals': high-fidelity, 3D printed replicas of scanned corals that
eliminate the need for live coral experimentation, whilst increasing data
quality, offering an ethical and scalable pathway for advancing autonomous
coral handling and soft manipulation broadly. Through extensive bench-top
manipulation and underwater grasping experiments, we show that our sensing
coral is able to detect grasps under 0.5 N, effectively capturing the delicate
interactions and light contact forces required for coral handling. Finally, we
showcase the value of our physical twins across two demonstrations: (i)
automated coral labelling for lab identification and (ii) robotic coral
aquaculture. Sensing physical twins such as ours can provide richer grasping
feedback than conventional sensors providing experimental validation of prior
to deployment in handling fragile and delicate items.

</details>


### [51] [FGGS-LiDAR: Ultra-Fast, GPU-Accelerated Simulation from General 3DGS Models to LiDAR](https://arxiv.org/abs/2509.17390)
*Junzhe Wu,Yufei Jia,Yiyi Yan,Zhixing Chen,Tiao Tan,Zifan Wang,Guangyu Wang*

Main category: cs.RO

TL;DR: FGGS-LiDAR是一个将预训练的3D高斯泼溅模型转换为高保真水密网格的框架，实现与高性能LiDAR仿真的无缝集成，无需LiDAR特定监督或架构修改。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术在逼真渲染方面取得突破，但其资产生态系统与机器人技术和自动驾驶的关键工具——高性能LiDAR仿真不兼容。

Method: 通过体积离散化和截断符号距离场提取的通用流程转换3DGS模型，并配合高度优化的GPU加速光线投射模块。

Result: 在室内外场景验证中表现出卓越的几何保真度，LiDAR仿真速度超过500 FPS。

Conclusion: 该框架通过直接重用3DGS资产进行几何精确的深度感知，将其效用扩展到可视化之外，为可扩展的多模态仿真解锁新能力。

Abstract: While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic
rendering, its vast ecosystem of assets remains incompatible with
high-performance LiDAR simulation, a critical tool for robotics and autonomous
driving. We present \textbf{FGGS-LiDAR}, a framework that bridges this gap with
a truly plug-and-play approach. Our method converts \textit{any} pretrained
3DGS model into a high-fidelity, watertight mesh without requiring
LiDAR-specific supervision or architectural alterations. This conversion is
achieved through a general pipeline of volumetric discretization and Truncated
Signed Distance Field (TSDF) extraction. We pair this with a highly optimized,
GPU-accelerated ray-casting module that simulates LiDAR returns at over 500
FPS. We validate our approach on indoor and outdoor scenes, demonstrating
exceptional geometric fidelity; By enabling the direct reuse of 3DGS assets for
geometrically accurate depth sensing, our framework extends their utility
beyond visualization and unlocks new capabilities for scalable, multimodal
simulation. Our open-source implementation is available at
https://github.com/TATP-233/FGGS-LiDAR.

</details>


### [52] [GPS Denied IBVS-Based Navigation and Collision Avoidance of UAV Using a Low-Cost RGB Camera](https://arxiv.org/abs/2509.17435)
*Xiaoyu Wang,Yan Rui Tan,William Leong,Sunan Huang,Rodney Teo,Cheng Xiang*

Main category: cs.RO

TL;DR: 提出了一种基于图像的视觉伺服框架，使用RGB相机实现无人机导航和避障，无需显式路径规划，完全在Jetson平台上运行


<details>
  <summary>Details</summary>
Motivation: 虽然无人机导航已被广泛研究，但在涉及多个视觉目标和避障的任务中应用IBVS仍然具有挑战性，现有方法通常依赖立体相机或外部工作站

Method: 通过AI-based单目深度估计从RGB图像实现避障，无需立体相机或外部工作站，框架完全在Jetson平台上运行

Result: 实验验证了无人机能够在GPS拒止环境中有效导航多个AprilTag并避开障碍物

Conclusion: 该方法提供了一个自包含且可部署的系统，在GPS拒止环境中实现了有效的导航和避障

Abstract: This paper proposes an image-based visual servoing (IBVS) framework for UAV
navigation and collision avoidance using only an RGB camera. While UAV
navigation has been extensively studied, it remains challenging to apply IBVS
in missions involving multiple visual targets and collision avoidance. The
proposed method achieves navigation without explicit path planning, and
collision avoidance is realized through AI-based monocular depth estimation
from RGB images. Unlike approaches that rely on stereo cameras or external
workstations, our framework runs fully onboard a Jetson platform, ensuring a
self-contained and deployable system. Experimental results validate that the
UAV can navigate across multiple AprilTags and avoid obstacles effectively in
GPS-denied environments.

</details>


### [53] [Learning Dexterous Manipulation with Quantized Hand State](https://arxiv.org/abs/2509.17450)
*Ying Feng,Hongjie Fang,Yinong He,Jingjing Chen,Chenxi Wang,Zihao He,Ruonan Liu,Cewu Lu*

Main category: cs.RO

TL;DR: DQ-RISE提出了一种新的灵巧操作策略，通过量化手部状态简化动作空间，实现更平衡的臂-手协调学习


<details>
  <summary>Details</summary>
Motivation: 现有视觉运动策略将臂和手动作合并到单一空间中，导致高维手部动作主导耦合动作空间，影响臂控制的精确性

Method: 量化手部状态以简化手部运动预测，同时应用连续松弛方法使臂动作能与这些紧凑的手部状态联合扩散

Result: 实验表明DQ-RISE实现了更平衡和高效的学习

Conclusion: 该方法为结构化且可泛化的灵巧操作铺平了道路

Abstract: Dexterous robotic hands enable robots to perform complex manipulations that
require fine-grained control and adaptability. Achieving such manipulation is
challenging because the high degrees of freedom tightly couple hand and arm
motions, making learning and control difficult. Successful dexterous
manipulation relies not only on precise hand motions, but also on accurate
spatial positioning of the arm and coordinated arm-hand dynamics. However, most
existing visuomotor policies represent arm and hand actions in a single
combined space, which often causes high-dimensional hand actions to dominate
the coupled action space and compromise arm control. To address this, we
propose DQ-RISE, which quantizes hand states to simplify hand motion prediction
while preserving essential patterns, and applies a continuous relaxation that
allows arm actions to diffuse jointly with these compact hand states. This
design enables the policy to learn arm-hand coordination from data while
preventing hand actions from overwhelming the action space. Experiments show
that DQ-RISE achieves more balanced and efficient learning, paving the way
toward structured and generalizable dexterous manipulation. Project website:
http://rise-policy.github.io/DQ-RISE/

</details>


### [54] [Morphologies of a sagging elastica with intrinsic sensing and actuation](https://arxiv.org/abs/2509.17572)
*Vishnu Deo Mishra,S Ganga Prasath*

Main category: cs.RO

TL;DR: 该论文研究软体机器人的形态控制，通过简单的比例反馈策略（驱动与感知曲率成正比）来补偿自重引起的下垂，分析了传感器和驱动器数量有限对形态稳定性和形状变形精度的影响。


<details>
  <summary>Details</summary>
Motivation: 软体机器人的形态控制面临几何非线性、实验系统建模误差以及传感/驱动能力限制等挑战，需要开发有效的控制策略来应对这些困难。

Method: 将软体机器人建模为弹性杆，采用比例反馈控制策略，通过有限数量的传感器和驱动器来研究形态稳定性，分析滤波器宽度和驱动增益对形状变形的影响。

Result: 发现了在重力-弯曲数、非维度传感/反馈增益和滤波器宽度构成的相空间中存在形态不稳定性层次结构，形状变形误差在固定滤波器宽度下选择合适的驱动增益时最小。

Conclusion: 该模型为设计和研究具有有限传感和驱动能力的细长软体设备提供了定量分析框架，揭示了在复杂机动应用中传感器间距和驱动器尺寸之间的权衡关系。

Abstract: The morphology of a slender soft-robot can be modified by sensing its shape
via sensors and exerting moments via actuators embedded along its body. The
actuating moments required to morph these soft-robots to a desired shape are
often difficult to compute due to the geometric non-linearity associated with
the structure, the errors in modeling the experimental system, and the
limitations in sensing and feedback/actuation capabilities. In this article, we
explore the effect of a simple feedback strategy (actuation being proportional
to the sensed curvature) on the shape of a soft-robot, modeled as an elastica.
The finite number of sensors and actuators, often seen in experiments, is
captured in the model via filters of specified widths. Using proportional
feedback, we study the simple task of straightening the device by compensating
for the sagging introduced by its self-weight. The device undergoes a hierarchy
of morphological instabilities defined in the phase-space given by the
gravito-bending number, non-dimensional sensing/feedback gain, and the scaled
width of the filter. For complex shape-morphing tasks, given a perfect model of
the device with limited sensing and actuating capabilities, we find that a
trade-off arises (set by the sensor spacing & actuator size) between capturing
the long and short wavelength features. We show that the error in
shape-morphing is minimal for a fixed filter width when we choose an
appropriate actuating gain (whose magnitude goes as a square of the filter
width). Our model provides a quantitative lens to study and design slender soft
devices with limited sensing and actuating capabilities for complex maneuvering
applications.

</details>


### [55] [GeCCo - a Generalist Contact-Conditioned Policy for Loco-Manipulation Skills on Legged Robots](https://arxiv.org/abs/2509.17582)
*Vassil Atanassov,Wanming Yu,Siddhant Gangapurwala,James Wilson,Ioannis Havoutis*

Main category: cs.RO

TL;DR: GeCCo是一个基于深度强化学习的通用接触条件策略，能够在四足机器人上跟踪任意接触点，避免了为每个新任务重新训练控制器的需要。


<details>
  <summary>Details</summary>
Motivation: 传统的端到端深度强化学习方法需要为每个新问题重新定义和调整奖励函数，这既耗时又难以扩展。作者希望开发一个通用的低层控制器，能够适用于各种高层任务。

Method: 使用深度强化学习训练一个通用的接触条件策略（GeCCo），该策略能够跟踪四足机器人的任意接触点。该方法将任务分解为高层接触规划器和预训练的低层通用策略的组合。

Result: GeCCo在各种运动和操作任务中表现出良好的可扩展性和鲁棒性，包括多种步态、复杂地形导航（如楼梯和斜坡）、以及物体交互任务（如按钮按压和轨迹跟踪）。

Conclusion: 该方法通过将任务特定的高层规划与预训练的通用低层策略相结合，能够更高效地获得新行为，为四足机器人控制提供了一个模块化和可重用的解决方案。

Abstract: Most modern approaches to quadruped locomotion focus on using Deep
Reinforcement Learning (DRL) to learn policies from scratch, in an end-to-end
manner. Such methods often fail to scale, as every new problem or application
requires time-consuming and iterative reward definition and tuning. We present
Generalist Contact-Conditioned Policy (GeCCo) -- a low-level policy trained
with Deep Reinforcement Learning that is capable of tracking arbitrary contact
points on a quadruped robot. The strength of our approach is that it provides a
general and modular low-level controller that can be reused for a wider range
of high-level tasks, without the need to re-train new controllers from scratch.
We demonstrate the scalability and robustness of our method by evaluating on a
wide range of locomotion and manipulation tasks in a common framework and under
a single generalist policy. These include a variety of gaits, traversing
complex terrains (eg. stairs and slopes) as well as previously unseen
stepping-stones and narrow beams, and interacting with objects (eg. pushing
buttons, tracking trajectories). Our framework acquires new behaviors more
efficiently, simply by combining a task-specific high-level contact planner and
the pre-trained generalist policy. A supplementary video can be found at
https://youtu.be/o8Dd44MkG2E.

</details>


### [56] [Robust and Resilient Soft Robotic Object Insertion with Compliance-Enabled Contact Formation and Failure Recovery](https://arxiv.org/abs/2509.17666)
*Mimo Shirasaka,Cristian C. Beltran-Hernandez,Masashi Hamaya,Yoshitaka Ushiku*

Main category: cs.RO

TL;DR: 提出一种使用被动柔顺软腕的鲁棒物体插入方法，通过大变形实现安全接触吸收，无需高频控制或力传感，并集成了基于视觉语言模型的自动故障恢复策略


<details>
  <summary>Details</summary>
Motivation: 传统物体插入任务在姿态不确定性和环境变化下容易失败，需要手动微调或控制器重新训练，需要一种更鲁棒和弹性的解决方案

Method: 将插入任务结构化为柔顺性启发的接触形成过程，使用预训练的视觉语言模型评估技能执行、识别故障模式并选择恢复动作，通过软腕的柔顺性实现安全的重复恢复尝试

Result: 在仿真中实现了83%的成功率，能够从随机条件下（包括5度抓取偏差、20mm孔位误差、5倍摩擦力增加以及未见过的方形/矩形插头）引发的故障中恢复

Conclusion: 柔顺性启发的故障恢复方法显著提高了物体插入任务的鲁棒性和弹性，为不确定环境下的机器人操作提供了有效解决方案

Abstract: Object insertion tasks are prone to failures under pose uncertainties and
environmental variations, traditionally requiring manual finetuning or
controller retraining. We present a novel approach for robust and resilient
object insertion using a passively compliant soft wrist that enables safe
contact absorption through large deformations, without high-frequency control
or force sensing. Our method structures insertion as compliance-enabled contact
formations, sequential contact states that progressively constrain degrees of
freedom, and integrates automated failure recovery strategies. Our key insight
is that wrist compliance permits safe, repeated recovery attempts; hence, we
refer to it as compliance-enabled failure recovery. We employ a pre-trained
vision-language model (VLM) that assesses each skill execution from terminal
poses and images, identifies failure modes, and proposes recovery actions by
selecting skills and updating goals. In simulation, our method achieved an 83%
success rate, recovering from failures induced by randomized
conditions--including grasp misalignments up to 5 degrees, hole-pose errors up
to 20mm, fivefold increases in friction, and previously unseen
square/rectangular pegs--and we further validate the approach on a real robot.

</details>


### [57] [Towards Learning Boulder Excavation with Hydraulic Excavators](https://arxiv.org/abs/2509.17683)
*Jonas Gruetter,Lorenzo Terenzi,Pascal Egli,Marco Hutter*

Main category: cs.RO

TL;DR: 该论文提出了一种使用强化学习让标准挖掘机铲斗自主提取大型不规则岩石的方法，能够在恶劣的室外环境中处理稀疏感知和变化的土壤条件。


<details>
  <summary>Details</summary>
Motivation: 传统方法要么无法处理大型不规则岩石，要么需要不切实际的工具更换。人类操作员使用标准铲斗提取岩石，但需要适应变化的土壤条件和感知挑战。

Method: 在仿真环境中使用刚体动力学和解析土壤模型训练强化学习策略，处理稀疏LiDAR点云（每个岩石仅20个点）和本体感受反馈，控制标准挖掘机铲斗。

Result: 现场测试在12吨挖掘机上实现了70%的成功率（人类操作员为83%），能够根据土壤阻力发现不同的策略：在硬土中表面拖动，在软土中直接穿透。

Conclusion: 研究表明标准建筑设备能够在稀疏感知和恶劣室外条件下学习复杂操作，为自主岩石提取提供了实用解决方案。

Abstract: Construction sites frequently require removing large rocks before excavation
or grading can proceed. Human operators typically extract these boulders using
only standard digging buckets, avoiding time-consuming tool changes to
specialized grippers. This task demands manipulating irregular objects with
unknown geometries in harsh outdoor environments where dust, variable lighting,
and occlusions hinder perception. The excavator must adapt to varying soil
resistance--dragging along hard-packed surfaces or penetrating soft
ground--while coordinating multiple hydraulic joints to secure rocks using a
shovel. Current autonomous excavation focuses on continuous media (soil,
gravel) or uses specialized grippers with detailed geometric planning for
discrete objects. These approaches either cannot handle large irregular rocks
or require impractical tool changes that interrupt workflow. We train a
reinforcement learning policy in simulation using rigid-body dynamics and
analytical soil models. The policy processes sparse LiDAR points (just 20 per
rock) from vision-based segmentation and proprioceptive feedback to control
standard excavator buckets. The learned agent discovers different strategies
based on soil resistance: dragging along the surface in hard soil and
penetrating directly in soft conditions. Field tests on a 12-ton excavator
achieved 70% success across varied rocks (0.4-0.7m) and soil types, compared to
83% for human operators. This demonstrates that standard construction equipment
can learn complex manipulation despite sparse perception and challenging
outdoor conditions.

</details>


### [58] [EigenSafe: A Spectral Framework for Learning-Based Stochastic Safety Filtering](https://arxiv.org/abs/2509.17750)
*Inkyu Jang,Jonghae Park,Chams E. Mballo,Sihyun Cho,Claire J. Tomlin,H. Jin Kim*

Main category: cs.RO

TL;DR: EigenSafe是一个基于算子理论的框架，用于学习随机系统的安全关键控制。该框架通过推导安全概率动态规划原理的线性算子，利用其主导特征对来评估系统安全性，并离线学习安全备份策略。


<details>
  <summary>Details</summary>
Motivation: 在机器人系统中，由于传感噪声和环境干扰等因素，动力学通常建模为随机系统。传统方法如Hamilton-Jacobi可达性和控制屏障函数难以提供全面的安全度量，因此需要新的框架来处理随机系统的安全控制问题。

Method: 推导安全概率动态规划原理的线性算子，发现其主导特征对能提供个体状态和闭环系统整体的安全信息。提出的EigenSafe学习框架离线联合学习主导特征对和安全备份策略，然后使用学习的特征函数构建安全过滤器来检测潜在不安全情况并回退到备份策略。

Result: 该框架在三个模拟的随机安全关键控制任务中进行了验证，证明了其有效性。

Conclusion: EigenSafe为随机系统的安全关键控制提供了一个有效的算子理论框架，能够通过主导特征对分析系统安全性，并实现安全过滤功能。

Abstract: We present EigenSafe, an operator-theoretic framework for learning-enabled
safety-critical control for stochastic systems. In many robotic systems where
dynamics are best modeled as stochastic systems due to factors such as sensing
noise and environmental disturbances, it is challenging for conventional
methods such as Hamilton-Jacobi reachability and control barrier functions to
provide a holistic measure of safety. We derive a linear operator governing the
dynamic programming principle for safety probability, and find that its
dominant eigenpair provides information about safety for both individual states
and the overall closed-loop system. The proposed learning framework, called
EigenSafe, jointly learns this dominant eigenpair and a safe backup policy in
an offline manner. The learned eigenfunction is then used to construct a safety
filter that detects potentially unsafe situations and falls back to the backup
policy. The framework is validated in three simulated stochastic
safety-critical control tasks.

</details>


### [59] [MotionTrans: Human VR Data Enable Motion-Level Learning for Robotic Manipulation Policies](https://arxiv.org/abs/2509.17759)
*Chengbo Yuan,Rui Zhou,Mengzhen Liu,Yingdong Hu,Shengjie Wang,Li Yi,Chuan Wen,Shanghang Zhang,Yang Gao*

Main category: cs.RO

TL;DR: MotionTrans是一个通过人类-机器人协同训练框架，直接从人类数据中学习运动知识并迁移到机器人策略的系统，实现了13个任务的零样本运动迁移。


<details>
  <summary>Details</summary>
Motivation: 解决机器人模仿学习中真实数据稀缺的问题，探索如何利用人类丰富的操作行为数据来直接学习新运动完成任务。

Method: 提出MotionTrans框架，包括数据收集系统、人类数据转换管道和加权协同训练策略，通过30个人类-机器人任务的同时协同训练。

Result: 成功将13个任务的人类运动直接迁移到可部署的端到端机器人策略，其中9个任务实现零样本非平凡成功率，预训练-微调性能提升40%。

Conclusion: 人类数据在运动层面学习具有巨大潜力，协同训练和广泛的任务相关运动覆盖是成功的关键因素。

Abstract: Scaling real robot data is a key bottleneck in imitation learning, leading to
the use of auxiliary data for policy training. While other aspects of robotic
manipulation such as image or language understanding may be learned from
internet-based datasets, acquiring motion knowledge remains challenging. Human
data, with its rich diversity of manipulation behaviors, offers a valuable
resource for this purpose. While previous works show that using human data can
bring benefits, such as improving robustness and training efficiency, it
remains unclear whether it can realize its greatest advantage: enabling robot
policies to directly learn new motions for task completion. In this paper, we
systematically explore this potential through multi-task human-robot
cotraining. We introduce MotionTrans, a framework that includes a data
collection system, a human data transformation pipeline, and a weighted
cotraining strategy. By cotraining 30 human-robot tasks simultaneously, we
direcly transfer motions of 13 tasks from human data to deployable end-to-end
robot policies. Notably, 9 tasks achieve non-trivial success rates in zero-shot
manner. MotionTrans also significantly enhances pretraining-finetuning
performance (+40% success rate). Through ablation study, we also identify key
factors for successful motion learning: cotraining with robot data and broad
task-related motion coverage. These findings unlock the potential of
motion-level learning from human data, offering insights into its effective use
for training robotic manipulation policies. All data, code, and model weights
are open-sourced https://motiontrans.github.io/.

</details>


### [60] [Enhancing the NAO: Extending Capabilities of Legacy Robots for Long-Term Research](https://arxiv.org/abs/2509.17760)
*Austin Wilson,Sahar Kapasi,Zane Greene,Alexis E. Block*

Main category: cs.RO

TL;DR: 本文提出了增强版NAO机器人，通过升级麦克风、RGB-D和热成像相机以及计算资源，为过时的机器人平台提供现代化感知和交互能力，显著提升了对话质量和用户体验。


<details>
  <summary>Details</summary>
Motivation: 许多研究团队面临旧版机器人平台失去厂商支持、无法集成现代感知和交互能力的挑战。本文旨在通过硬件升级和软件框架，延长旧版机器人的使用寿命和研究价值。

Method: 开发了增强版NAO机器人，集成了波束成形麦克风、RGB-D相机、热成像相机和额外计算资源，采用云端和本地模型结合的感知与对话系统，同时保留NAO原有的身体表达和行为。

Result: 在验证研究中，增强版NAO相比NAO AI版显著提升了对话质量和用户偏好，且未增加响应延迟。波束成形麦克风和低延迟音频处理减少了自听效应，改善了多说话人分离。

Conclusion: 该框架为延长旧版机器人寿命和研究价值提供了平台无关的策略，确保它们继续作为人机交互的有价值工具。

Abstract: Many research groups face challenges when legacy (unsupported) robotic
platforms lose manufacturer support and cannot accommodate modern sensing,
speech, and interaction capabilities. We present the Enhanced NAO, a
revitalized version of Aldebaran's NAO robot that uses upgraded microphones,
RGB-D and thermal cameras, and additional compute resources in a fully
self-contained package. This system combines cloud and local models for
perception and dialogue, while preserving the NAO's expressive body and
behaviors. In a pilot validation study, the Enhanced NAO delivered
significantly higher conversational quality and stronger user preference
compared to the NAO AI Edition, without increasing response latency. Key
upgrades, such as beamforming microphones and low-latency audio processing,
reduced artifacts like self-hearing and improved multi-party separation.
Expanded visual and thermal sensing established a foundation for future
interaction capabilities. Beyond the NAO, our framework provides a
platform-agnostic strategy for extending the lifespan and research utility of
legacy robots, ensuring they remain valuable tools for human-robot interaction.

</details>


### [61] [RoboSeek: You Need to Interact with Your Objects](https://arxiv.org/abs/2509.17783)
*Yibo Peng,Jiahao Yang,Shenhao Yan,Ziyu Huang,Shuang Li,Shuguang Cui,Yiming Zhao,Yatong Han*

Main category: cs.RO

TL;DR: RoboSeek是一个基于具身认知理论的机器人操作框架，通过仿真训练和real2sim2real迁移实现长时程任务的鲁棒执行，在多个机器人平台上平均成功率79%，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 受具身认知理论启发，针对长时程机器人操作任务中顺序决策、物理约束和感知不确定性等挑战，提出通过交互式经验优化动作执行的方法。

Method: 首先通过3D重建在仿真中复制真实环境，然后利用强化学习和交叉熵方法结合视觉先验训练策略，最后通过real2sim2real迁移管道在真实机器人上部署执行。

Result: 在8个涉及顺序交互、工具使用和物体操作的长时程任务中，平均成功率79%，显著优于成功率低于50%的基线方法。

Conclusion: 实验验证了训练框架在复杂动态真实环境中的有效性，证明了real2sim2real迁移机制的稳定性，为更通用的具身机器人学习铺平了道路。

Abstract: Optimizing and refining action execution through
  exploration and interaction is a promising way for robotic
  manipulation. However, practical approaches to interaction driven robotic
learning are still underexplored, particularly for
  long-horizon tasks where sequential decision-making, physical
  constraints, and perceptual uncertainties pose significant chal lenges.
Motivated by embodied cognition theory, we propose
  RoboSeek, a framework for embodied action execution that
  leverages interactive experience to accomplish manipulation
  tasks. RoboSeek optimizes prior knowledge from high-level
  perception models through closed-loop training in simulation
  and achieves robust real-world execution via a real2sim2real
  transfer pipeline. Specifically, we first replicate real-world
  environments in simulation using 3D reconstruction to provide
  visually and physically consistent environments., then we train
  policies in simulation using reinforcement learning and the
  cross-entropy method leveraging visual priors. The learned
  policies are subsequently deployed on real robotic platforms
  for execution. RoboSeek is hardware-agnostic and is evaluated
  on multiple robotic platforms across eight long-horizon ma nipulation tasks
involving sequential interactions, tool use, and
  object handling. Our approach achieves an average success rate
  of 79%, significantly outperforming baselines whose success
  rates remain below 50%, highlighting its generalization and
  robustness across tasks and platforms. Experimental results
  validate the effectiveness of our training framework in complex,
  dynamic real-world settings and demonstrate the stability of the
  proposed real2sim2real transfer mechanism, paving the way for
  more generalizable embodied robotic learning. Project Page:
  https://russderrick.github.io/Roboseek/

</details>


### [62] [Tac2Motion: Contact-Aware Reinforcement Learning with Tactile Feedback for Robotic Hand Manipulation](https://arxiv.org/abs/2509.17812)
*Yitaek Kim,Casper Hewson Rask,Christoffer Sloth*

Main category: cs.RO

TL;DR: Tac2Motion是一个基于接触感知的强化学习框架，用于学习接触丰富的灵巧手操作任务，如打开盖子。通过触觉感知奖励塑造和嵌入观察空间，提高数据效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决接触丰富的灵巧手操作任务学习困难的问题，特别是在确保牢固抓握和平滑手指运动方面的挑战。

Method: 提出触觉感知奖励塑造方法，将触觉感知嵌入观察空间，设计奖励函数鼓励牢固抓握和平滑手指步态。

Result: 在打开盖子场景中验证了框架的有效性，展示了策略对不同物体类型和动态特性的泛化能力，并在Shadow Robot上成功实现真实世界转移。

Conclusion: Tac2Motion框架能够有效学习接触丰富的操作任务，具有高数据效率和鲁棒性，且学习到的策略可以成功转移到真实机器人上。

Abstract: This paper proposes Tac2Motion, a contact-aware reinforcement learning
framework to facilitate the learning of contact-rich in-hand manipulation
tasks, such as removing a lid. To this end, we propose tactile sensing-based
reward shaping and incorporate the sensing into the observation space through
embedding. The designed rewards encourage an agent to ensure firm grasping and
smooth finger gaiting at the same time, leading to higher data efficiency and
robust performance compared to the baseline. We verify the proposed framework
on the opening a lid scenario, showing generalization of the trained policy
into a couple of object types and various dynamics such as torsional friction.
Lastly, the learned policy is demonstrated on the multi-fingered robot, Shadow
Robot, showing that the control policy can be transferred to the real world.
The video is available: https://youtu.be/poeJBPR7urQ.

</details>


### [63] [SocialTraj: Two-Stage Socially-Aware Trajectory Prediction for Autonomous Driving via Conditional Diffusion Model](https://arxiv.org/abs/2509.17850)
*Xiao Zhou,Zengqi Peng,Jun Ma*

Main category: cs.RO

TL;DR: 提出SocialTraj框架，通过社会价值取向(SVO)整合社会心理学原理，使用贝叶斯逆强化学习估计周围车辆SVO，结合条件去噪扩散模型生成行为一致且社会合规的轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 当前方法在高度动态复杂交通场景中难以有效捕捉驾驶员多模态行为，导致预测轨迹与实际未来运动存在偏差。

Method: 使用贝叶斯逆强化学习估计周围车辆SVO，将估计的SVO嵌入条件去噪扩散模型，并显式结合自车规划轨迹来增强交互建模。

Result: 在NGSIM和HighD数据集上的实验表明，SocialTraj能适应高度动态交互场景，生成社会合规且行为一致的轨迹预测，优于现有基线方法。

Conclusion: 动态SVO估计和显式自车规划组件显著提高了预测精度并大幅减少推理时间，证明了社会心理学原理在轨迹预测中的有效性。

Abstract: Accurate trajectory prediction of surrounding vehicles (SVs) is crucial for
autonomous driving systems to avoid misguided decisions and potential
accidents. However, achieving reliable predictions in highly dynamic and
complex traffic scenarios remains a significant challenge. One of the key
impediments lies in the limited effectiveness of current approaches to capture
the multi-modal behaviors of drivers, which leads to predicted trajectories
that deviate from actual future motions. To address this issue, we propose
SocialTraj, a novel trajectory prediction framework integrating social
psychology principles through social value orientation (SVO). By utilizing
Bayesian inverse reinforcement learning (IRL) to estimate the SVO of SVs, we
obtain the critical social context to infer the future interaction trend. To
ensure modal consistency in predicted behaviors, the estimated SVOs of SVs are
embedded into a conditional denoising diffusion model that aligns generated
trajectories with historical driving styles. Additionally, the planned future
trajectory of the ego vehicle (EV) is explicitly incorporated to enhance
interaction modeling. Extensive experiments on NGSIM and HighD datasets
demonstrate that SocialTraj is capable of adapting to highly dynamic and
interactive scenarios while generating socially compliant and behaviorally
consistent trajectory predictions, outperforming existing baselines. Ablation
studies demonstrate that dynamic SVO estimation and explicit ego-planning
components notably improve prediction accuracy and substantially reduce
inference time.

</details>


### [64] [Sight Over Site: Perception-Aware Reinforcement Learning for Efficient Robotic Inspection](https://arxiv.org/abs/2509.17877)
*Richard Kuhlmann,Jakob Wolfram,Boyang Sun,Jiaxu Xing,Davide Scaramuzza,Marc Pollefeys,Cesar Cadena*

Main category: cs.RO

TL;DR: 提出了一种基于强化学习的端到端感知感知检查框架，将目标可见性作为主要目标，使机器人能够找到保证与目标视觉接触的最短轨迹，无需依赖地图。


<details>
  <summary>Details</summary>
Motivation: 传统检查任务往往简化为导航问题，但实际检查中目标可能在到达精确坐标前就已可见，进一步移动变得冗余低效。真正重要的是让机器人定位到能够观察目标的最佳视角。

Method: 开发了端到端强化学习框架，明确将目标可见性作为主要目标，结合感知和本体感受传感，完全在模拟环境中训练，然后部署到真实机器人。还开发了计算真实最短检查路径的算法用于评估。

Result: 通过大量实验证明，该方法在模拟和真实世界设置中都优于现有的经典和基于学习的导航方法，产生更高效的检查轨迹。

Conclusion: 从感知感知角度重新审视检查问题，提出的方法能够有效解决传统导航方法在检查任务中的局限性，实现更高效的自主检查。

Abstract: Autonomous inspection is a central problem in robotics, with applications
ranging from industrial monitoring to search-and-rescue. Traditionally,
inspection has often been reduced to navigation tasks, where the objective is
to reach a predefined location while avoiding obstacles. However, this
formulation captures only part of the real inspection problem. In real-world
environments, the inspection targets may become visible well before their exact
coordinates are reached, making further movement both redundant and
inefficient. What matters more for inspection is not simply arriving at the
target's position, but positioning the robot at a viewpoint from which the
target becomes observable. In this work, we revisit inspection from a
perception-aware perspective. We propose an end-to-end reinforcement learning
framework that explicitly incorporates target visibility as the primary
objective, enabling the robot to find the shortest trajectory that guarantees
visual contact with the target without relying on a map. The learned policy
leverages both perceptual and proprioceptive sensing and is trained entirely in
simulation, before being deployed to a real-world robot. We further develop an
algorithm to compute ground-truth shortest inspection paths, which provides a
reference for evaluation. Through extensive experiments, we show that our
method outperforms existing classical and learning-based navigation approaches,
yielding more efficient inspection trajectories in both simulated and
real-world settings. The project is avialable at
https://sight-over-site.github.io/

</details>


### [65] [The Surprising Effectiveness of Linear Models for Whole-Body Model-Predictive Control](https://arxiv.org/abs/2509.17884)
*Arun L. Bishop,Juan Alvarez-Padilla,Sam Schoedel,Ibrahima Sory Sow,Juee Chandrachud,Sheitej Sharma,Will Kraus,Beomyeong Park,Robert J. Griffin,John M. Dolan,Zachary Manchester*

Main category: cs.RO

TL;DR: 本文展示了一种使用线性时不变近似的全身模型预测控制器，能够在复杂腿式机器人上执行基本运动任务，无需在线非线性动力学评估或矩阵求逆。


<details>
  <summary>Details</summary>
Motivation: 研究何时运动控制器需要考虑非线性问题，探索使用简化线性模型实现复杂机器人运动控制的可能性。

Method: 采用全身模型预测控制器，使用简单的线性时不变近似来建模全身动力学，避免在线非线性动力学计算和矩阵求逆。

Result: 在四足机器人上实现了行走、扰动抑制和目标位置导航（无需单独的步态规划器），在液压人形机器人上实现了动态行走，该机器人具有显著的肢体惯性、复杂执行器动力学和大仿真到现实的差距。

Conclusion: 线性时不变近似方法能够有效处理复杂腿式机器人的基本运动任务，即使在存在显著非线性因素的情况下也能实现稳定的运动控制。

Abstract: When do locomotion controllers require reasoning about nonlinearities? In
this work, we show that a whole-body model-predictive controller using a simple
linear time-invariant approximation of the whole-body dynamics is able to
execute basic locomotion tasks on complex legged robots. The formulation
requires no online nonlinear dynamics evaluations or matrix inversions. We
demonstrate walking, disturbance rejection, and even navigation to a goal
position without a separate footstep planner on a quadrupedal robot. In
addition, we demonstrate dynamic walking on a hydraulic humanoid, a robot with
significant limb inertia, complex actuator dynamics, and large sim-to-real gap.

</details>


### [66] [DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving](https://arxiv.org/abs/2509.17940)
*Shuyao Shang,Yuntao Chen,Yuqi Wang,Yingyan Li,Zhaoxiang Zhang*

Main category: cs.RO

TL;DR: DriveDPO是一个端到端自动驾驶框架，通过结合人类模仿相似性和基于规则的安全评分进行直接策略优化，解决了现有方法在安全性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法通过模仿学习训练，但无法区分看似人类驾驶但潜在不安全的轨迹。一些近期方法尝试回归多个规则驱动评分，但监督与策略优化解耦导致性能不佳。

Method: 首先从人类模仿相似性和基于规则的安全评分中提取统一的策略分布进行直接策略优化；然后引入迭代式直接偏好优化阶段，将其制定为轨迹级别的偏好对齐。

Result: 在NAVSIM基准测试中达到90.0的PDMS新最佳成绩，在多样化挑战场景下产生更安全可靠的驾驶行为。

Conclusion: DriveDPO框架通过安全直接偏好优化策略学习，有效提升了自动驾驶系统的安全性和可靠性，在基准测试中取得了最先进的性能。

Abstract: End-to-end autonomous driving has substantially progressed by directly
predicting future trajectories from raw perception inputs, which bypasses
traditional modular pipelines. However, mainstream methods trained via
imitation learning suffer from critical safety limitations, as they fail to
distinguish between trajectories that appear human-like but are potentially
unsafe. Some recent approaches attempt to address this by regressing multiple
rule-driven scores but decoupling supervision from policy optimization,
resulting in suboptimal performance. To tackle these challenges, we propose
DriveDPO, a Safety Direct Preference Optimization Policy Learning framework.
First, we distill a unified policy distribution from human imitation similarity
and rule-based safety scores for direct policy optimization. Further, we
introduce an iterative Direct Preference Optimization stage formulated as
trajectory-level preference alignment. Extensive experiments on the NAVSIM
benchmark demonstrate that DriveDPO achieves a new state-of-the-art PDMS of
90.0. Furthermore, qualitative results across diverse challenging scenarios
highlight DriveDPO's ability to produce safer and more reliable driving
behaviors.

</details>


### [67] [ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion](https://arxiv.org/abs/2509.17941)
*Zichao Hu,Chen Tang,Michael J. Munje,Yifeng Zhu,Alex Liu,Shuijing Liu,Garrett Warnell,Peter Stone,Joydeep Biswas*

Main category: cs.RO

TL;DR: 该论文提出ComposableNav方法，通过组合式学习解决机器人导航中指令规范组合爆炸的问题。使用扩散模型分别学习运动基元，在部署时并行组合以满足训练中未见过的规范组合。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中机器人导航指令规范组合爆炸的挑战。每个指令可能包含多个规范，随着机器人技能集的扩展，规范组合数量呈指数级增长。

Method: 提出ComposableNav方法：1）使用扩散模型分别学习每个运动基元；2）部署时并行组合基元以满足新规范组合；3）两阶段训练：监督预训练学习基础扩散模型，强化学习微调形成不同运动基元。

Result: 通过仿真和真实世界实验证明，ComposableNav能够生成满足多样且未见过的规范组合的轨迹，显著优于非组合式VLM策略和成本图组合基线方法。

Conclusion: ComposableNav通过组合式学习方法有效解决了机器人导航中的规范组合问题，实现了对复杂指令的灵活响应，为动态环境中的机器人导航提供了新思路。

Abstract: This paper considers the problem of enabling robots to navigate dynamic
environments while following instructions. The challenge lies in the
combinatorial nature of instruction specifications: each instruction can
include multiple specifications, and the number of possible specification
combinations grows exponentially as the robot's skill set expands. For example,
"overtake the pedestrian while staying on the right side of the road" consists
of two specifications: "overtake the pedestrian" and "walk on the right side of
the road." To tackle this challenge, we propose ComposableNav, based on the
intuition that following an instruction involves independently satisfying its
constituent specifications, each corresponding to a distinct motion primitive.
Using diffusion models, ComposableNav learns each primitive separately, then
composes them in parallel at deployment time to satisfy novel combinations of
specifications unseen in training. Additionally, to avoid the onerous need for
demonstrations of individual motion primitives, we propose a two-stage training
procedure: (1) supervised pre-training to learn a base diffusion model for
dynamic navigation, and (2) reinforcement learning fine-tuning that molds the
base model into different motion primitives. Through simulation and real-world
experiments, we show that ComposableNav enables robots to follow instructions
by generating trajectories that satisfy diverse and unseen combinations of
specifications, significantly outperforming both non-compositional VLM-based
policies and costmap composing baselines. Videos and additional materials can
be found on the project page: https://amrl.cs.utexas.edu/ComposableNav/

</details>


### [68] [Guided Multi-Fidelity Bayesian Optimization for Data-driven Controller Tuning with Digital Twins](https://arxiv.org/abs/2509.17952)
*Mahdi Nobar,Jürg Keller,Alessandro Forino,John Lygeros,Alisa Rupenyan*

Main category: cs.RO

TL;DR: 提出了一种引导式多保真度贝叶斯优化框架，通过结合修正的数字孪生仿真和真实世界测量数据，实现数据高效的控制器调优。


<details>
  <summary>Details</summary>
Motivation: 针对闭环系统中仿真保真度有限或存在廉价近似的情况，解决模型不匹配问题，提高控制器调优效率。

Method: 构建多保真度代理模型，包含学习到的修正模型来根据真实数据优化数字孪生估计；使用自适应成本感知采集函数平衡预期改进、保真度和采样成本。

Result: 在机器人驱动硬件和数值研究中验证，相比标准贝叶斯优化和多保真度方法，显著提高了调优效率。

Conclusion: 该方法能够动态适应新测量数据，重新评估数字孪生准确性，确保准确模型被更频繁使用，不准确模型被适当降权。

Abstract: We propose a \textit{guided multi-fidelity Bayesian optimization} framework
for data-efficient controller tuning that integrates corrected digital twin
(DT) simulations with real-world measurements. The method targets closed-loop
systems with limited-fidelity simulations or inexpensive approximations. To
address model mismatch, we build a multi-fidelity surrogate with a learned
correction model that refines DT estimates from real data. An adaptive
cost-aware acquisition function balances expected improvement, fidelity, and
sampling cost. Our method ensures adaptability as new measurements arrive. The
accuracy of DTs is re-estimated, dynamically adapting both cross-source
correlations and the acquisition function. This ensures that accurate DTs are
used more frequently, while inaccurate DTs are appropriately downweighted.
Experiments on robotic drive hardware and supporting numerical studies
demonstrate that our method enhances tuning efficiency compared to standard
Bayesian optimization (BO) and multi-fidelity methods.

</details>


### [69] [M3ET: Efficient Vision-Language Learning for Robotics based on Multimodal Mamba-Enhanced Transformer](https://arxiv.org/abs/2509.18005)
*Yanxin Zhang,Liang He,Zeyi Kang,Zuheng Ming,Kaixing Zhao*

Main category: cs.RO

TL;DR: 提出M3ET轻量级多模态学习模型，通过Mamba模块和语义自适应注意力机制优化特征融合和对齐，在保持VQA任务准确率0.74的同时，推理速度提升2.3倍，参数量减少0.67倍


<details>
  <summary>Details</summary>
Motivation: 当前多模态学习方法难以充分利用文本模态，依赖监督预训练模型，在无监督机器人环境中语义提取受限，且计算密集导致资源消耗高

Method: 提出Multi Modal Mamba Enhanced Transformer (M3ET)，结合Mamba模块和语义自适应注意力机制，优化特征融合、对齐和模态重建

Result: M3ET提升了跨任务性能，预训练推理速度提高2.3倍，VQA任务准确率保持0.74，模型参数量减少0.67倍，但EQA任务性能有限

Conclusion: M3ET的轻量级设计使其特别适合在资源受限的机器人平台上部署

Abstract: In recent years, multimodal learning has become essential in robotic vision
and information fusion, especially for understanding human behavior in complex
environments. However, current methods struggle to fully leverage the textual
modality, relying on supervised pretrained models, which limits semantic
extraction in unsupervised robotic environments, particularly with significant
modality loss. These methods also tend to be computationally intensive, leading
to high resource consumption in real-world applications. To address these
challenges, we propose the Multi Modal Mamba Enhanced Transformer (M3ET), a
lightweight model designed for efficient multimodal learning, particularly on
mobile platforms. By incorporating the Mamba module and a semantic-based
adaptive attention mechanism, M3ET optimizes feature fusion, alignment, and
modality reconstruction. Our experiments show that M3ET improves cross-task
performance, with a 2.3 times increase in pretraining inference speed. In
particular, the core VQA task accuracy of M3ET remains at 0.74, while the
model's parameter count is reduced by 0.67. Although performance on the EQA
task is limited, M3ET's lightweight design makes it well suited for deployment
on resource-constrained robotic platforms.

</details>


### [70] [Prepare Before You Act: Learning From Humans to Rearrange Initial States](https://arxiv.org/abs/2509.18043)
*Yinlong Dai,Andre Keyser,Dylan P. Losey*

Main category: cs.RO

TL;DR: ReSET算法通过让机器人在执行策略前先重新布置环境，解决模仿学习在面对分布外观测时的泛化问题，从而提高任务执行的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 模仿学习策略在面对目标物体位置未知或被遮挡等分布外观测时表现不佳，需要大量演示数据才能达到鲁棒性。人类在这种情况下会先重新布置环境以创造更有利的执行条件，因此希望让机器人也具备这种能力。

Method: 提出ReSET算法，结合动作无关的人类视频和任务无关的遥操作数据，实现三个功能：决定何时修改场景、预测人类会采取的简化动作、将这些预测映射到机器人动作基元。

Result: 理论分析表明这种两步过程（先重新布置环境再执行策略）可以减少泛化差距。与扩散策略、VLAs等基线方法相比，使用ReSET准备环境能在相同训练数据量下实现更鲁棒的任务执行。

Conclusion: ReSET算法通过让机器人具备环境准备能力，有效提高了模仿学习策略在面对分布外观测时的鲁棒性和泛化性能。

Abstract: Imitation learning (IL) has proven effective across a wide range of
manipulation tasks. However, IL policies often struggle when faced with
out-of-distribution observations; for instance, when the target object is in a
previously unseen position or occluded by other objects. In these cases,
extensive demonstrations are needed for current IL methods to reach robust and
generalizable behaviors. But when humans are faced with these sorts of atypical
initial states, we often rearrange the environment for more favorable task
execution. For example, a person might rotate a coffee cup so that it is easier
to grasp the handle, or push a box out of the way so they can directly grasp
their target object. In this work we seek to equip robot learners with the same
capability: enabling robots to prepare the environment before executing their
given policy. We propose ReSET, an algorithm that takes initial states -- which
are outside the policy's distribution -- and autonomously modifies object poses
so that the restructured scene is similar to training data. Theoretically, we
show that this two step process (rearranging the environment before rolling out
the given policy) reduces the generalization gap. Practically, our ReSET
algorithm combines action-agnostic human videos with task-agnostic
teleoperation data to i) decide when to modify the scene, ii) predict what
simplifying actions a human would take, and iii) map those predictions into
robot action primitives. Comparisons with diffusion policies, VLAs, and other
baselines show that using ReSET to prepare the environment enables more robust
task execution with equal amounts of total training data. See videos at our
project website: https://reset2025paper.github.io/

</details>


### [71] [HuMam: Humanoid Motion Control via End-to-End Deep Reinforcement Learning with Mamba](https://arxiv.org/abs/2509.18046)
*Yinuo Wang,Yuanyang Qi,Jinzhao Zhou,Gavin Tao*

Main category: cs.RO

TL;DR: HuMam是一个基于Mamba编码器的端到端强化学习框架，用于人形机器人步态控制，通过融合机器人状态、足部目标位置和相位时钟，实现了高效、稳定且节能的行走控制。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端强化学习方法在人形机器人步态控制中存在训练不稳定、特征融合效率低和驱动成本高的问题，需要一种更高效的解决方案。

Method: 使用单层Mamba编码器融合机器人中心状态、定向足部目标和连续相位时钟，通过PPO算法优化策略输出关节位置目标，采用六项奖励函数平衡接触质量、摆动平滑度、足部放置、姿态和身体稳定性。

Result: 在JVRC-1人形机器人上，HuMam相比前馈基线方法显著提高了学习效率、训练稳定性和任务性能，同时降低了功耗和扭矩峰值。

Conclusion: 这是首个采用Mamba作为融合骨干的端到端人形机器人强化学习控制器，在效率、稳定性和控制经济性方面取得了实质性进展。

Abstract: End-to-end reinforcement learning (RL) for humanoid locomotion is appealing
for its compact perception-action mapping, yet practical policies often suffer
from training instability, inefficient feature fusion, and high actuation cost.
We present HuMam, a state-centric end-to-end RL framework that employs a
single-layer Mamba encoder to fuse robot-centric states with oriented footstep
targets and a continuous phase clock. The policy outputs joint position targets
tracked by a low-level PD loop and is optimized with PPO. A concise six-term
reward balances contact quality, swing smoothness, foot placement, posture, and
body stability while implicitly promoting energy saving. On the JVRC-1 humanoid
in mc-mujoco, HuMam consistently improves learning efficiency, training
stability, and overall task performance over a strong feedforward baseline,
while reducing power consumption and torque peaks. To our knowledge, this is
the first end-to-end humanoid RL controller that adopts Mamba as the fusion
backbone, demonstrating tangible gains in efficiency, stability, and control
economy.

</details>


### [72] [V2V-GoT: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multimodal Large Language Models and Graph-of-Thoughts](https://arxiv.org/abs/2509.18053)
*Hsu-kuang Chiu,Ryo Hachiuma,Chien-Yi Wang,Yu-Chiang Frank Wang,Min-Hung Chen,Stephen F. Smith*

Main category: cs.RO

TL;DR: 提出了一种基于多模态大语言模型（MLLM）的图思考框架，用于解决自动驾驶中因遮挡导致的安全关键问题，通过车辆间协作感知和规划提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统在遇到大型物体遮挡局部传感器时面临安全风险，车辆间协作自动驾驶是解决方案，但现有研究未充分利用图思考推理在MLLM中的潜力。

Method: 设计了专门用于MLLM协作自动驾驶的图思考框架，包含遮挡感知感知和规划感知预测的新颖思想，构建了V2V-GoT-QA数据集并开发了V2V-GoT模型。

Result: 实验结果表明，该方法在协作感知、预测和规划任务中优于其他基线方法。

Conclusion: 提出的图思考框架有效提升了协作自动驾驶系统的性能，证明了图思考推理在MLLM协作驾驶中的价值。

Abstract: Current state-of-the-art autonomous vehicles could face safety-critical
situations when their local sensors are occluded by large nearby objects on the
road. Vehicle-to-vehicle (V2V) cooperative autonomous driving has been proposed
as a means of addressing this problem, and one recently introduced framework
for cooperative autonomous driving has further adopted an approach that
incorporates a Multimodal Large Language Model (MLLM) to integrate cooperative
perception and planning processes. However, despite the potential benefit of
applying graph-of-thoughts reasoning to the MLLM, this idea has not been
considered by previous cooperative autonomous driving research. In this paper,
we propose a novel graph-of-thoughts framework specifically designed for
MLLM-based cooperative autonomous driving. Our graph-of-thoughts includes our
proposed novel ideas of occlusion-aware perception and planning-aware
prediction. We curate the V2V-GoT-QA dataset and develop the V2V-GoT model for
training and testing the cooperative driving graph-of-thoughts. Our
experimental results show that our method outperforms other baselines in
cooperative perception, prediction, and planning tasks.

</details>


### [73] [RadarSFD: Single-Frame Diffusion with Pretrained Priors for Radar Point Clouds](https://arxiv.org/abs/2509.18068)
*Bin Zhao,Nakul Garg*

Main category: cs.RO

TL;DR: RadarSFD是一个条件潜在扩散框架，能够从单帧毫米波雷达数据重建密集的LiDAR级点云，无需运动或多帧聚合，适用于紧凑型机器人系统。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达在雾、烟、灰尘和低光条件下具有鲁棒性，但现有雷达成像方法依赖合成孔径或多帧聚合来提高分辨率，这对于小型空中、检测或可穿戴系统不实用。

Method: 采用条件潜在扩散框架，从预训练的单目深度估计器中转移几何先验知识，通过通道级潜在连接将其锚定到雷达输入，并使用结合潜在空间和像素空间损失的双空间目标进行输出正则化。

Result: 在RadarHD基准测试中，RadarSFD达到35厘米Chamfer距离和28厘米改进Hausdorff距离，优于单帧基线（56厘米，45厘米），并与使用5-41帧的多帧方法保持竞争力。

Conclusion: 该方法建立了首个实用的单帧、无需SAR的毫米波雷达管道，可用于紧凑机器人系统的密集点云感知，具有强泛化能力。

Abstract: Millimeter-wave radar provides perception robust to fog, smoke, dust, and low
light, making it attractive for size, weight, and power constrained robotic
platforms. Current radar imaging methods, however, rely on synthetic aperture
or multi-frame aggregation to improve resolution, which is impractical for
small aerial, inspection, or wearable systems. We present RadarSFD, a
conditional latent diffusion framework that reconstructs dense LiDAR-like point
clouds from a single radar frame without motion or SAR. Our approach transfers
geometric priors from a pretrained monocular depth estimator into the diffusion
backbone, anchors them to radar inputs via channel-wise latent concatenation,
and regularizes outputs with a dual-space objective combining latent and
pixel-space losses. On the RadarHD benchmark, RadarSFD achieves 35 cm Chamfer
Distance and 28 cm Modified Hausdorff Distance, improving over the single-frame
RadarHD baseline (56 cm, 45 cm) and remaining competitive with multi-frame
methods using 5-41 frames. Qualitative results show recovery of fine walls and
narrow gaps, and experiments across new environments confirm strong
generalization. Ablation studies highlight the importance of pretrained
initialization, radar BEV conditioning, and the dual-space loss. Together,
these results establish the first practical single-frame, no-SAR mmWave radar
pipeline for dense point cloud perception in compact robotic systems.

</details>


### [74] [ByteWrist: A Parallel Robotic Wrist Enabling Flexible and Anthropomorphic Motion for Confined Spaces](https://arxiv.org/abs/2509.18084)
*Jiawen Tian,Liqun Huang,Zhongren Cui,Jingchao Qiao,Jiafeng Xu,Xiao Ma,Zeyu Ren*

Main category: cs.RO

TL;DR: ByteWrist是一种新型高度灵活且拟人化的并联手腕，通过紧凑的三级并联驱动机制和弧形末端连杆，解决了现有串并联手腕在狭窄空间操作中的关键限制。


<details>
  <summary>Details</summary>
Motivation: 解决现有串并联手腕在狭窄空间操作中的局限性，为复杂非结构化环境（如家庭服务、医疗辅助和精密装配）提供更紧凑、高效的解决方案。

Method: 采用嵌套三级电机驱动连杆结构最小化体积，弧形末端连杆优化力传递和扩大运动范围，中央支撑球作为球铰增强结构刚度而不影响灵活性，并提供完整的运动学建模（正向/逆向运动学和数值雅可比解）。

Result: ByteWrist在狭窄空间机动性和双臂协同操作任务中表现出色，性能优于Kinova系统，在紧凑性、效率和刚度方面相比传统设计有显著改进。

Conclusion: ByteWrist为受限环境下的下一代机器人操作提供了一个有前景的解决方案，在紧凑性、灵活性和性能方面具有明显优势。

Abstract: This paper introduces ByteWrist, a novel highly-flexible and anthropomorphic
parallel wrist for robotic manipulation. ByteWrist addresses the critical
limitations of existing serial and parallel wrists in narrow-space operations
through a compact three-stage parallel drive mechanism integrated with
arc-shaped end linkages. The design achieves precise RPY (Roll-Pitch-Yaw)
motion while maintaining exceptional compactness, making it particularly
suitable for complex unstructured environments such as home services, medical
assistance, and precision assembly. The key innovations include: (1) a nested
three-stage motor-driven linkages that minimize volume while enabling
independent multi-DOF control, (2) arc-shaped end linkages that optimize force
transmission and expand motion range, and (3) a central supporting ball
functioning as a spherical joint that enhances structural stiffness without
compromising flexibility. Meanwhile, we present comprehensive kinematic
modeling including forward / inverse kinematics and a numerical Jacobian
solution for precise control. Empirically, we observe ByteWrist demonstrates
strong performance in narrow-space maneuverability and dual-arm cooperative
manipulation tasks, outperforming Kinova-based systems. Results indicate
significant improvements in compactness, efficiency, and stiffness compared to
traditional designs, establishing ByteWrist as a promising solution for
next-generation robotic manipulation in constrained environments.

</details>
