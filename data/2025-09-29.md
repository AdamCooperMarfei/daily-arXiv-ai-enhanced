<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 47]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Language-in-the-Loop Culvert Inspection on the Erie Canal](https://arxiv.org/abs/2509.21370)
*Yashom Dighe,Yash Turkar,Karthik Dantu*

Main category: cs.RO

TL;DR: VISION系统结合视觉语言模型和约束视角规划，实现自主检查运河涵洞，无需领域特定微调即可生成专家对齐的高分辨率检查报告。


<details>
  <summary>Details</summary>
Motivation: 传统人工检查运河涵洞面临年龄、几何形状、光照差、天气和难以接近等挑战，需要自动化解决方案。

Method: 使用端到端语言循环自主系统，结合web规模视觉语言模型和约束视角规划，通过VLM获取感兴趣区域提案，融合立体深度恢复尺度，规划器指挥重新定位拍摄特写。

Result: 在伊利运河涵洞部署的四足机器人上，初始ROI提案与专家达成61.4%一致，重新成像后评估达到80%一致。

Conclusion: VISION系统能够将初步假设转化为基于事实的专家对齐发现，展示了语言引导自主检查的可行性。

Abstract: Culverts on canals such as the Erie Canal, built originally in 1825, require
frequent inspections to ensure safe operation. Human inspection of culverts is
challenging due to age, geometry, poor illumination, weather, and lack of easy
access. We introduce VISION, an end-to-end, language-in-the-loop autonomy
system that couples a web-scale vision-language model (VLM) with constrained
viewpoint planning for autonomous inspection of culverts. Brief prompts to the
VLM solicit open-vocabulary ROI proposals with rationales and confidences,
stereo depth is fused to recover scale, and a planner -- aware of culvert
constraints -- commands repositioning moves to capture targeted close-ups.
Deployed on a quadruped in a culvert under the Erie Canal, VISION closes the
see, decide, move, re-image loop on-board and produces high-resolution images
for detailed reporting without domain-specific fine-tuning. In an external
evaluation by New York Canal Corporation personnel, initial ROI proposals
achieved 61.4\% agreement with subject-matter experts, and final
post-re-imaging assessments reached 80\%, indicating that VISION converts
tentative hypotheses into grounded, expert-aligned findings.

</details>


### [2] [Developing a Mono-Actuated Compliant GeoGami Robot](https://arxiv.org/abs/2509.21445)
*Archie Webster,Lee Skull,Seyed Amir Tafrishi*

Main category: cs.RO

TL;DR: GeoGami是一个单驱动的软刚性机器人平台，利用折纸表面柔顺性和几何柔顺骨架实现形状变换和运动，仅需一个执行器即可完成变形和滚动运动。


<details>
  <summary>Details</summary>
Motivation: 解决折纸表面自由度多、需要大量执行器的问题，通过集成表面柔顺性来提高可重复性，开发能够通过形状变换进入不同环境并利用形状变换进行运动的机器人。

Method: 提出单驱动GeoGami移动平台，结合折纸表面柔顺性和几何柔顺骨架，开发刚度模型和中央齿轮箱机构，分析替代的缆线驱动执行方法以实现表面变换。

Result: 成功演示了机器人平台，实现了形状变换和滚动能力，展示了单执行器驱动的变形和运动功能。

Conclusion: GeoGami平台为能够通过形状变换进入不同环境并利用形状变换进行运动的机器人开辟了新能力。

Abstract: This paper presents the design of a new soft-rigid robotic platform,
"GeoGami". We leverage origami surface capabilities to achieve shape
contraction and to support locomotion with underactuated forms. A key challenge
is that origami surfaces have high degrees of freedom and typically require
many actuators; we address repeatability by integrating surface compliance. We
propose a mono-actuated GeoGami mobile platform that combines origami surface
compliance with a geometric compliant skeleton, enabling the robot to transform
and locomote using a single actuator. We demonstrate the robot, develop a
stiffness model, and describe the central gearbox mechanism. We also analyze
alternative cable-driven actuation methods for the skeleton to enable surface
transformation. Finally, we evaluate the GeoGami platform for capabilities,
including shape transformation and rolling. This platform opens new
capabilities for robots that change shape to access different environments and
that use shape transformation for locomotion.

</details>


### [3] [Wall Inspector: Quadrotor Control in Wall-proximity Through Model Compensation](https://arxiv.org/abs/2509.21496)
*Peiwen Yang,Weisong Wen,Runqiu Yang,Yingming Chen,Cheuk Chi Tsang*

Main category: cs.RO

TL;DR: 提出了一种用于四旋翼无人机近壁飞行的吸力补偿模型预测控制框架，通过物理建模和优化控制解决壁面附近复杂涡流引起的吸力干扰问题。


<details>
  <summary>Details</summary>
Motivation: 四旋翼无人机在近壁城市或室内环境中运行时，壁面附近产生的未建模空气动力学效应（如复杂涡流引起的吸力）会导致不稳定振动或碰撞风险，需要专门的控制策略来确保安全操作。

Method: 开发了基于物理的吸力模型，明确表征转子速度和壁面距离的依赖关系；设计了吸力补偿模型预测控制框架，将增强的动态模型整合为因子图优化问题，包含系统动力学约束、轨迹跟踪目标、控制输入平滑性和执行器物理限制。

Result: 实验验证显示SC-MPC在X轴和Y轴位置控制中分别达到2.1cm和2.0cm的均方根误差，相比级联PID控制分别提升74%和79%，相比标准MPC分别提升60%和53%；平均绝对误差指标同样优于两个基线方法。

Conclusion: SC-MPC框架能够有效补偿壁面附近空气动力学效应，显著提高四旋翼无人机在近壁环境中的轨迹跟踪精度和稳定性，为检查、搜救等任务提供了可靠解决方案。

Abstract: The safe operation of quadrotors in near-wall urban or indoor environments
(e.g., inspection and search-and-rescue missions) is challenged by unmodeled
aerodynamic effects arising from wall-proximity. It generates complex vortices
that induce destabilizing suction forces, potentially leading to hazardous
vibrations or collisions. This paper presents a comprehensive solution
featuring (1) a physics-based suction force model that explicitly characterizes
the dependency on both rotor speed and wall distance, and (2) a
suction-compensated model predictive control (SC-MPC) framework designed to
ensure accurate and stable trajectory tracking during wall-proximity
operations. The proposed SC-MPC framework incorporates an enhanced dynamics
model that accounts for suction force effects, formulated as a factor graph
optimization problem integrating system dynamics constraints, trajectory
tracking objectives, control input smoothness requirements, and actuator
physical limitations. The suction force model parameters are systematically
identified through extensive experimental measurements across varying
operational conditions. Experimental validation demonstrates SC-MPC's superior
performance, achieving 2.1 cm root mean squared error (RMSE) in X-axis and 2.0
cm RMSE in Y-axis position control - representing 74% and 79% improvements over
cascaded proportional-integral-derivative (PID) control, and 60% and 53%
improvements over standard MPC respectively. The corresponding mean absolute
error (MAE) metrics (1.2 cm X-axis, 1.4 cm Y-axis) similarly outperform both
baselines. The evaluation platform employs a ducted quadrotor design that
provides collision protection while maintaining aerodynamic efficiency. To
facilitate reproducibility and community adoption, we have open-sourced our
complete implementation, available at
https://anonymous.4open.science/r/SC-MPC-6A61.

</details>


### [4] [DroneFL: Federated Learning for Multi-UAV Visual Target Tracking](https://arxiv.org/abs/2509.21523)
*Xiaofan Yu,Yuwei Wu,Katherine Mao,Ye Tian,Vijay Kumar,Tajana Rosing*

Main category: cs.RO

TL;DR: DroneFL是首个专门为多无人机目标跟踪设计的联邦学习框架，通过轻量级本地模型、位置不变架构和云端多无人机预测融合，显著提升了跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 多机器人目标跟踪在农业、环境监测等领域有重要应用，但联邦学习在多无人机目标跟踪中的应用尚未充分探索，面临计算资源有限、数据异构性强以及轨迹预测与规划紧密耦合等挑战。

Method: 设计轻量级本地模型（使用冻结YOLO骨干和浅层transformer），引入位置不变架构和基于高度的自适应实例归一化来缓解数据异构性，在云端进行模型聚合和多无人机预测融合以生成最优轨迹。

Result: 相比分布式非联邦学习框架，DroneFL将预测误差降低了6%-83%，跟踪距离减少了0.4%-4.6%，在树莓派5上实时运行，平均云数据率仅为1.56 KBps。

Conclusion: DroneFL成功解决了多无人机目标跟踪中的关键挑战，证明了联邦学习在该领域的可行性和有效性，为资源受限环境下的分布式智能系统提供了实用解决方案。

Abstract: Multi-robot target tracking is a fundamental problem that requires
coordinated monitoring of dynamic entities in applications such as precision
agriculture, environmental monitoring, disaster response, and security
surveillance. While Federated Learning (FL) has the potential to enhance
learning across multiple robots without centralized data aggregation, its use
in multi-Unmanned Aerial Vehicle (UAV) target tracking remains largely
underexplored. Key challenges include limited onboard computational resources,
significant data heterogeneity in FL due to varying targets and the fields of
view, and the need for tight coupling between trajectory prediction and
multi-robot planning. In this paper, we introduce DroneFL, the first federated
learning framework specifically designed for efficient multi-UAV target
tracking. We design a lightweight local model to predict target trajectories
from sensor inputs, using a frozen YOLO backbone and a shallow transformer for
efficient onboard training. The updated models are periodically aggregated in
the cloud for global knowledge sharing. To alleviate the data heterogeneity
that hinders FL convergence, DroneFL introduces a position-invariant model
architecture with altitude-based adaptive instance normalization. Finally, we
fuse predictions from multiple UAVs in the cloud and generate optimal
trajectories that balance target prediction accuracy and overall tracking
performance. Our results show that DroneFL reduces prediction error by 6%-83%
and tracking distance by 0.4%-4.6% compared to a distributed non-FL framework.
In terms of efficiency, DroneFL runs in real time on a Raspberry Pi 5 and has
on average just 1.56 KBps data rate to the cloud.

</details>


### [5] [Plan2Evolve: LLM Self-Evolution for Improved Planning Capability via Automated Domain Generation](https://arxiv.org/abs/2509.21543)
*Jinbang Huang,Zhiyuan Li,Zhanguang Zhang,Xingyue Quan,Jianye Hao,Yingxue Zhang*

Main category: cs.RO

TL;DR: Plan2Evolve是一个LLM自进化框架，通过让基础模型生成规划领域来产生符号化问题-规划对作为推理轨迹，然后将这些对转化为扩展的思维链轨迹，从而提升模型的规划能力和跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要将规划领域视为搜索工具，忽视了其作为可扩展推理数据源的潜力；同时机器人领域的思维链监督仍依赖昂贵的人工标注数据集。

Method: 基础模型生成规划领域作为引擎，产生符号化问题-规划对作为推理轨迹，然后由同一模型将这些对转化为自然语言解释的扩展思维链轨迹，实现符号规划结构与自然语言推理的显式对齐。

Result: 生成的数据超越了模型内在的规划能力，通过微调产生了规划增强的LLM，具有改进的规划成功率、更强的跨任务泛化能力和降低的推理成本。

Conclusion: Plan2Evolve框架通过自生成规划领域和推理数据，成功提升了LLM在机器人任务规划中的性能，实现了无需人工标注的自我进化。

Abstract: Large Language Models (LLMs) have recently shown strong potential in robotic
task planning, particularly through automatic planning domain generation that
integrates symbolic search. Prior approaches, however, have largely treated
these domains as search utilities, with limited attention to their potential as
scalable sources of reasoning data. At the same time, progress in reasoning
LLMs has been driven by chain-of-thought (CoT) supervision, whose application
in robotics remains dependent on costly, human-curated datasets. We propose
Plan2Evolve, an LLM self-evolving framework in which the base model generates
planning domains that serve as engines for producing symbolic problem-plan
pairs as reasoning traces. These pairs are then transformed into extended CoT
trajectories by the same model through natural-language explanations, thereby
explicitly aligning symbolic planning structures with natural language
reasoning. The resulting data extend beyond the model's intrinsic planning
capacity, enabling model fine-tuning that yields a planning-enhanced LLM with
improved planning success, stronger cross-task generalization, and reduced
inference costs.

</details>


### [6] [PL-VIWO2: A Lightweight, Fast and Robust Visual-Inertial-Wheel Odometry Using Points and Lines](https://arxiv.org/abs/2509.21563)
*Zhixin Zhang,Liang Zhao,Pawel Ladosz*

Main category: cs.RO

TL;DR: PL-VIWO2是一个基于滤波器的视觉-惯性-轮式里程计系统，通过整合IMU、轮式编码器和相机，在复杂城市环境中实现长期鲁棒的状态估计。


<details>
  <summary>Details</summary>
Motivation: 视觉里程计在自动驾驶中应用广泛，但在复杂室外城市环境中性能会下降，需要更鲁棒的解决方案。

Method: 提出三个主要贡献：(1) 新颖的线特征处理框架，利用2D特征点和线之间的几何关系；(2) SE(2)约束的SE(3)轮式预积分方法；(3) 高效的运动一致性检查来过滤动态特征。

Result: 在蒙特卡洛模拟和公开自动驾驶数据集上的广泛实验表明，PL-VIWO2在准确性、效率和鲁棒性方面优于最先进的方法。

Conclusion: PL-VIWO2系统能够有效解决复杂城市环境中的视觉里程计性能下降问题，提供长期鲁棒的状态估计。

Abstract: Vision-based odometry has been widely adopted in autonomous driving owing to
its low cost and lightweight setup; however, its performance often degrades in
complex outdoor urban environments. To address these challenges, we propose
PL-VIWO2, a filter-based visual-inertial-wheel odometry system that integrates
an IMU, wheel encoder, and camera (supporting both monocular and stereo) for
long-term robust state estimation. The main contributions are: (i) a novel line
feature processing framework that exploits the geometric relationship between
2D feature points and lines, enabling fast and robust line tracking and
triangulation while ensuring real-time performance; (ii) an SE(2)-constrained
SE(3) wheel pre-integration method that leverages the planar motion
characteristics of ground vehicles for accurate wheel updates; and (iii) an
efficient motion consistency check (MCC) that filters out dynamic features by
jointly using IMU and wheel measurements. Extensive experiments on Monte Carlo
simulations and public autonomous driving datasets demonstrate that PL-VIWO2
outperforms state-of-the-art methods in terms of accuracy, efficiency, and
robustness.

</details>


### [7] [Autonomous UAV-Quadruped Docking in Complex Terrains via Active Posture Alignment and Constraint-Aware Control](https://arxiv.org/abs/2509.21571)
*HaoZhe Xu,Cheng Cheng,HongRui Sang,Zhipeng Wang,Qiyong He,Xiuxian Li,Bin He*

Main category: cs.RO

TL;DR: 提出了一种用于GPS缺失环境下的无人机-四足机器人自主对接框架，通过混合内部模型稳定四足机器人躯干，采用三阶段策略实现无人机精确对接，在复杂地形上成功验证。


<details>
  <summary>Details</summary>
Motivation: 解决异构系统中无人机与地面机器人自主对接问题，特别是针对四足机器人姿态变化频繁导致难以提供稳定着陆表面的挑战，扩展在复杂地形中的探索能力。

Method: 四足机器人侧使用混合内部模型水平对齐(HIM-HA)稳定躯干；无人机侧采用三阶段策略：中值滤波YOLOv8检测器进行远距离获取、约束感知控制器进行近距离跟踪、安全周期机制指导终端下降。

Result: 在仿真和真实场景中验证了框架有效性，成功在高于17厘米的室外楼梯和超过30度的陡峭斜坡上实现对接。

Conclusion: 该框架能够有效解决无人机与四足机器人在复杂地形中的自主对接问题，为异构系统在GPS缺失环境下的协作提供了可行方案。

Abstract: Autonomous docking between Unmanned Aerial Vehicles (UAVs) and ground robots
is essential for heterogeneous systems, yet most existing approaches target
wheeled platforms whose limited mobility constrains exploration in complex
terrains. Quadruped robots offer superior adaptability but undergo frequent
posture variations, making it difficult to provide a stable landing surface for
UAVs. To address these challenges, we propose an autonomous UAV-quadruped
docking framework for GPS-denied environments. On the quadruped side, a Hybrid
Internal Model with Horizontal Alignment (HIM-HA), learned via deep
reinforcement learning, actively stabilizes the torso to provide a level
platform. On the UAV side, a three-phase strategy is adopted, consisting of
long-range acquisition with a median-filtered YOLOv8 detector, close-range
tracking with a constraint-aware controller that integrates a Nonsingular Fast
Terminal Sliding Mode Controller (NFTSMC) and a logarithmic Barrier Function
(BF) to guarantee finite-time error convergence under field-of-view (FOV)
constraints, and terminal descent guided by a Safety Period (SP) mechanism that
jointly verifies tracking accuracy and platform stability. The proposed
framework is validated in both simulation and real-world scenarios,
successfully achieving docking on outdoor staircases higher than 17 cm and
rough slopes steeper than 30 degrees. Supplementary materials and videos are
available at: https://uav-quadruped-docking.github.io.

</details>


### [8] [Real-Time Indoor Object SLAM with LLM-Enhanced Priors](https://arxiv.org/abs/2509.21602)
*Yang Jiao,Yiding Qiu,Henrik I. Christensen*

Main category: cs.RO

TL;DR: 利用大型语言模型为物体级SLAM提供几何属性常识知识作为先验，解决稀疏观测导致的优化约束不足问题，在TUM RGB-D和3RScan数据集上比最新基线提升36.8%的建图精度。


<details>
  <summary>Details</summary>
Motivation: 物体级SLAM因稀疏观测导致优化约束不足，传统方法获取常识知识先验费时且缺乏跨类别泛化能力。

Method: 利用LLM提供物体几何属性（尺寸和朝向）常识知识作为图SLAM框架中的先验因子，特别在观测有限的初始阶段发挥作用，实现稀疏物体特征的鲁棒数据关联和实时物体SLAM。

Result: 在TUM RGB-D和3RScan数据集上，比最新基线提升36.8%的建图精度，补充视频展示了实时性能。

Conclusion: LLM提供的常识知识先验能有效解决物体级SLAM的稀疏观测问题，提升建图精度和实时性能。

Abstract: Object-level Simultaneous Localization and Mapping (SLAM), which incorporates
semantic information for high-level scene understanding, faces challenges of
under-constrained optimization due to sparse observations. Prior work has
introduced additional constraints using commonsense knowledge, but obtaining
such priors has traditionally been labor-intensive and lacks generalizability
across diverse object categories. We address this limitation by leveraging
large language models (LLMs) to provide commonsense knowledge of object
geometric attributes, specifically size and orientation, as prior factors in a
graph-based SLAM framework. These priors are particularly beneficial during the
initial phase when object observations are limited. We implement a complete
pipeline integrating these priors, achieving robust data association on sparse
object-level features and enabling real-time object SLAM. Our system, evaluated
on the TUM RGB-D and 3RScan datasets, improves mapping accuracy by 36.8\% over
the latest baseline. Additionally, we present real-world experiments in the
supplementary video, demonstrating its real-time performance.

</details>


### [9] [Generating Stable Placements via Physics-guided Diffusion Models](https://arxiv.org/abs/2509.21664)
*Philippe Nadeau,Miguel Rogel,Ivan Bilić,Ivan Petrović,Jonathan Kelly*

Main category: cs.RO

TL;DR: 提出了一种将稳定性直接集成到扩散模型采样过程中的方法，通过查询离线采样规划器收集多模态放置标签，训练扩散模型生成稳定放置，无需额外训练即可提高稳定性。


<details>
  <summary>Details</summary>
Motivation: 在多物体场景中稳定放置物体是机器人操作的基本挑战，现有方法依赖模拟引擎或启发式外观评估，需要更高效准确的稳定性评估方法。

Method: 使用离线采样规划器收集多模态放置标签，训练条件扩散模型生成稳定放置，利用基于分数的生成模型组合性将学习先验与稳定性感知损失结合。

Result: 在四个基准场景中，物理引导模型实现的放置对强力扰动的鲁棒性提高56%，运行时间比最先进几何方法减少47%。

Conclusion: 该方法成功将稳定性直接集成到扩散模型采样过程，无需额外训练即可显著提高放置稳定性和效率。

Abstract: Stably placing an object in a multi-object scene is a fundamental challenge
in robotic manipulation, as placements must be penetration-free, establish
precise surface contact, and result in a force equilibrium. To assess
stability, existing methods rely on running a simulation engine or resort to
heuristic, appearance-based assessments. In contrast, our approach integrates
stability directly into the sampling process of a diffusion model. To this end,
we query an offline sampling-based planner to gather multi-modal placement
labels and train a diffusion model to generate stable placements. The diffusion
model is conditioned on scene and object point clouds, and serves as a
geometry-aware prior. We leverage the compositional nature of score-based
generative models to combine this learned prior with a stability-aware loss,
thereby increasing the likelihood of sampling from regions of high stability.
Importantly, this strategy requires no additional re-training or fine-tuning,
and can be directly applied to off-the-shelf models. We evaluate our method on
four benchmark scenes where stability can be accurately computed. Our
physics-guided models achieve placements that are 56% more robust to forceful
perturbations while reducing runtime by 47% compared to a state-of-the-art
geometric method.

</details>


### [10] [Towards Versatile Humanoid Table Tennis: Unified Reinforcement Learning with Prediction Augmentation](https://arxiv.org/abs/2509.21690)
*Muqun Hu,Wenxi Chen,Wenjing Li,Falak Mandali,Zijian He,Renhong Zhang,Praveen Krisna,Katherine Christian,Leo Benaharon,Dizhi Ma,Karthik Ramani,Yan Gu*

Main category: cs.RO

TL;DR: 提出一个强化学习框架，将乒乓球位置观测直接映射到全身关节指令，结合预测信号和物理引导的密集奖励，实现人形机器人端到端的乒乓球击球和移动控制。


<details>
  <summary>Details</summary>
Motivation: 人形机器人乒乓球需要快速感知、主动全身运动和敏捷步法，这些能力在统一控制器中仍然难以实现。

Method: 使用强化学习框架，通过轻量级学习预测器估计未来球状态，结合基于物理的预测器构建密集奖励，实现端到端的全身控制。

Result: 在模拟中达到高命中率（≥96%）和成功率（≥92%），在物理机器人上零样本部署产生协调的步法和准确快速回球。

Conclusion: 该框架为人形机器人乒乓球提供了一条实用的端到端学习路径，预测器和预测奖励设计对学习效果至关重要。

Abstract: Humanoid table tennis (TT) demands rapid perception, proactive whole-body
motion, and agile footwork under strict timing -- capabilities that remain
difficult for unified controllers. We propose a reinforcement learning
framework that maps ball-position observations directly to whole-body joint
commands for both arm striking and leg locomotion, strengthened by predictive
signals and dense, physics-guided rewards. A lightweight learned predictor, fed
with recent ball positions, estimates future ball states and augments the
policy's observations for proactive decision-making. During training, a
physics-based predictor supplies precise future states to construct dense,
informative rewards that lead to effective exploration. The resulting policy
attains strong performance across varied serve ranges (hit rate $\geq$ 96% and
success rate $\geq$ 92%) in simulations. Ablation studies confirm that both the
learned predictor and the predictive reward design are critical for end-to-end
learning. Deployed zero-shot on a physical Booster T1 humanoid with 23 revolute
joints, the policy produces coordinated lateral and forward-backward footwork
with accurate, fast returns, suggesting a practical path toward versatile,
competitive humanoid TT.

</details>


### [11] [VLBiMan: Vision-Language Anchored One-Shot Demonstration Enables Generalizable Robotic Bimanual Manipulation](https://arxiv.org/abs/2509.21723)
*Huayi Zhou,Kui Jia*

Main category: cs.RO

TL;DR: VLBiMan从单个人类演示中提取可重用技能，通过任务感知分解和视觉语言基础实现动态适应，大幅减少演示需求并支持跨平台技能迁移。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法面临的困境：模仿策略学习需要大量演示来覆盖任务变化，而模块化方法在动态场景中缺乏灵活性。

Method: 通过任务感知分解将技能分解为不变基元和可调整组件，利用视觉语言基础动态适应场景变化，无需策略重新训练。

Result: 在工具使用和多对象任务中验证了：(1)大幅减少演示需求，(2)通过原子技能拼接实现组合泛化，(3)对新颖对象和外部干扰的鲁棒性，(4)跨平台技能迁移能力。

Conclusion: 通过将人类先验知识与视觉语言锚定适应相结合，为非结构化环境中的实用双臂操作迈出了重要一步。

Abstract: Achieving generalizable bimanual manipulation requires systems that can learn
efficiently from minimal human input while adapting to real-world uncertainties
and diverse embodiments. Existing approaches face a dilemma: imitation policy
learning demands extensive demonstrations to cover task variations, while
modular methods often lack flexibility in dynamic scenes. We introduce VLBiMan,
a framework that derives reusable skills from a single human example through
task-aware decomposition, preserving invariant primitives as anchors while
dynamically adapting adjustable components via vision-language grounding. This
adaptation mechanism resolves scene ambiguities caused by background changes,
object repositioning, or visual clutter without policy retraining, leveraging
semantic parsing and geometric feasibility constraints. Moreover, the system
inherits human-like hybrid control capabilities, enabling mixed synchronous and
asynchronous use of both arms. Extensive experiments validate VLBiMan across
tool-use and multi-object tasks, demonstrating: (1) a drastic reduction in
demonstration requirements compared to imitation baselines, (2) compositional
generalization through atomic skill splicing for long-horizon tasks, (3)
robustness to novel but semantically similar objects and external disturbances,
and (4) strong cross-embodiment transfer, showing that skills learned from
human demonstrations can be instantiated on different robotic platforms without
retraining. By bridging human priors with vision-language anchored adaptation,
our work takes a step toward practical and versatile dual-arm manipulation in
unstructured settings.

</details>


### [12] [The Turkish Ice Cream Robot: Examining Playful Deception in Social Human-Robot Interactions](https://arxiv.org/abs/2509.21776)
*Hyeonseong Kim,Roy El-Helou,Seungbeen Lee,Sungjoon Choi,Matthew Pan*

Main category: cs.RO

TL;DR: 该研究探索了在机器人交互中应用土耳其冰淇淋式游戏性欺骗对用户体验的影响，发现这种欺骗能显著提升乐趣和参与度，但会降低安全感和信任度。


<details>
  <summary>Details</summary>
Motivation: 游戏性欺骗在人类社交中很常见，但在人机交互中研究不足。受土耳其冰淇淋商贩互动的启发，研究有界、文化熟悉的欺骗形式如何影响用户信任、乐趣和参与度。

Method: 设计了一个配备定制末端执行器的机器人操纵器，实现了五种土耳其冰淇淋式欺骗策略，通过混合设计用户研究（91名参与者）评估游戏性欺骗和交互时长对用户体验的影响。

Result: 土耳其冰淇淋式欺骗显著提升了乐趣和参与度，但降低了感知安全性和信任度，表明在多维度方面存在结构化权衡。

Conclusion: 游戏性欺骗可以作为娱乐和参与导向的交互机器人的有价值设计策略，但需要仔细考虑其复杂的权衡关系。

Abstract: Playful deception, a common feature in human social interactions, remains
underexplored in Human-Robot Interaction (HRI). Inspired by the Turkish Ice
Cream (TIC) vendor routine, we investigate how bounded, culturally familiar
forms of deception influence user trust, enjoyment, and engagement during
robotic handovers. We design a robotic manipulator equipped with a custom
end-effector and implement five TIC-inspired trick policies that deceptively
delay the handover of an ice cream-shaped object. Through a mixed-design user
study with 91 participants, we evaluate the effects of playful deception and
interaction duration on user experience. Results reveal that TIC-inspired
deception significantly enhances enjoyment and engagement, though reduces
perceived safety and trust, suggesting a structured trade-off across the
multi-dimensional aspects. Our findings demonstrate that playful deception can
be a valuable design strategy for interactive robots in entertainment and
engagement-focused contexts, while underscoring the importance of deliberate
consideration of its complex trade-offs. You can find more information,
including demonstration videos, on
https://hyeonseong-kim98.github.io/turkish-ice-cream-robot/ .

</details>


### [13] [Learning Multi-Skill Legged Locomotion Using Conditional Adversarial Motion Priors](https://arxiv.org/abs/2509.21810)
*Ning Huang,Zhentao Xie,Qinchuan Li*

Main category: cs.RO

TL;DR: 提出基于条件对抗运动先验(CAMP)的多技能学习框架，使四足机器人能从专家演示中高效学习多种运动技能，实现精确技能重建和平滑转换。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以通过单一策略学习多种运动技能，且缺乏平滑的技能转换能力，限制了机器人在复杂环境中的敏捷导航。

Method: 使用条件对抗运动先验(CAMP)框架，结合新颖的技能判别器和技能条件奖励设计，实现多技能学习。

Result: 框架支持多种运动技能的主动控制和重用，为复杂环境中学习通用策略提供了实用解决方案。

Conclusion: 该多技能学习框架能有效解决四足机器人获取多样化运动技能的问题，实现精确技能重建和平滑转换。

Abstract: Despite growing interest in developing legged robots that emulate biological
locomotion for agile navigation of complex environments, acquiring a diverse
repertoire of skills remains a fundamental challenge in robotics. Existing
methods can learn motion behaviors from expert data, but they often fail to
acquire multiple locomotion skills through a single policy and lack smooth
skill transitions. We propose a multi-skill learning framework based on
Conditional Adversarial Motion Priors (CAMP), with the aim of enabling
quadruped robots to efficiently acquire a diverse set of locomotion skills from
expert demonstrations. Precise skill reconstruction is achieved through a novel
skill discriminator and skill-conditioned reward design. The overall framework
supports the active control and reuse of multiple skills, providing a practical
solution for learning generalizable policies in complex environments.

</details>


### [14] [Improved Vehicle Maneuver Prediction using Game Theoretic Priors](https://arxiv.org/abs/2509.21873)
*Nishant Doshi*

Main category: cs.RO

TL;DR: 提出一种结合博弈论和传统运动分类模型的车辆行为预测方法，利用Level-k博弈理论模拟车辆间交互，提高变道等复杂行为的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统基于轨迹分类的方法在预测变道等复杂行为时精度不足，需要整合整个场景信息。博弈论能模拟人类分层推理过程，为车辆交互建模提供更理性的决策基础。

Method: 使用Level-k博弈理论建模车辆间交互，将博弈论评估结果作为先验或与传统运动分类模型结合，通过在线优化求解目标车辆的最理性行为预测。

Result: 该方法能够更准确地预测车辆变道等复杂行为，为自适应巡航控制等决策系统提供更可靠的预测输入。

Conclusion: 结合博弈论和传统分类模型的方法能显著提升车辆行为预测精度，特别是在需要模拟车辆间交互的复杂场景中，有助于提高自动驾驶系统的决策质量和燃油效率。

Abstract: Conventional maneuver prediction methods use some sort of classification
model on temporal trajectory data to predict behavior of agents over a set time
horizon. Despite of having the best precision and recall, these models cannot
predict a lane change accurately unless they incorporate information about the
entire scene. Level-k game theory can leverage the human-like hierarchical
reasoning to come up with the most rational decisions each agent can make in a
group. This can be leveraged to model interactions between different vehicles
in presence of each other and hence compute the most rational decisions each
agent would make. The result of game theoretic evaluation can be used as a
"prior" or combined with a traditional motion-based classification model to
achieve more accurate predictions. The proposed approach assumes that the
states of the vehicles around the target lead vehicle are known. The module
will output the most rational maneuver prediction of the target vehicle based
on an online optimization solution. These predictions are instrumental in
decision making systems like Adaptive Cruise Control (ACC) or Traxen's
iQ-Cruise further improving the resulting fuel savings.

</details>


### [15] [WAVE: Worm Gear-based Adaptive Variable Elasticity for Decoupling Actuators from External Forces](https://arxiv.org/abs/2509.21878)
*Moses Gladson Selvamuthu,Tomoya Takahashi,Riichiro Tadakuma,Kazutoshi Tanaka*

Main category: cs.RO

TL;DR: WAVE是一种基于蜗轮的可变刚度执行器，通过非反向驱动的蜗轮将驱动电机与外部力解耦，实现精确的力传输和位置顺应性，同时通过弹簧存储冲击能量并实现连续刚度调节。


<details>
  <summary>Details</summary>
Motivation: 开发能够同时调节顺应性和刚度的机械臂执行器，以增强操作安全性和多功能性，特别是在接触密集型任务和挑战性环境中。

Method: 集成非反向驱动的蜗轮，将驱动电机与外部力解耦；使用弹簧吸收冲击能量并存储为弹性势能；通过改变弹簧预压缩长度实现连续刚度调节。

Result: 实验验证了刚度模型的有效性，在静止状态下即使有外部负载，电机负载也接近零；展示了在机械臂上的应用，成功实现了外部力的解耦。

Conclusion: WAVE执行器成功实现了外部力的解耦，其保护特性使其能够在接触密集型任务中长时间运行，并在挑战性环境中实现稳健的机器人应用。

Abstract: Robotic manipulators capable of regulating both compliance and stiffness
offer enhanced operational safety and versatility. Here, we introduce Worm
Gear-based Adaptive Variable Elasticity (WAVE), a variable stiffness actuator
(VSA) that integrates a non-backdrivable worm gear. By decoupling the driving
motor from external forces using this gear, WAVE enables precise force
transmission to the joint, while absorbing positional discrepancies through
compliance. WAVE is protected from excessive loads by converting impact forces
into elastic energy stored in a spring. In addition, the actuator achieves
continuous joint stiffness modulation by changing the spring's precompression
length. We demonstrate these capabilities, experimentally validate the proposed
stiffness model, show that motor loads approach zero at rest--even under
external loading--and present applications using a manipulator with WAVE. This
outcome showcases the successful decoupling of external forces. The protective
attributes of this actuator allow for extended operation in contact-intensive
tasks, and for robust robotic applications in challenging environments.

</details>


### [16] [SAGE: Scene Graph-Aware Guidance and Execution for Long-Horizon Manipulation Tasks](https://arxiv.org/abs/2509.21928)
*Jialiang Li,Wenzheng Wu,Gaojing Zhang,Yifan Han,Wenzhao Lian*

Main category: cs.RO

TL;DR: SAGE是一个用于长视野操作任务的场景图感知引导与执行框架，通过语义场景图连接高层任务规划和低层视觉运动控制，实现鲁棒的任务规划和目标条件操作。


<details>
  <summary>Details</summary>
Motivation: 解决长视野操作任务中高层符号规划与低层连续控制之间的鸿沟，现有方法在泛化能力和语义推理方面存在局限，图像条件控制方法难以适应未见任务。

Method: 使用语义场景图作为场景状态的结构化表示，包含基于场景图的任务规划器（使用VLM和LLM解析环境和推理场景状态转换序列）和解耦的结构化图像编辑管道（通过图像修复和合成将目标子目标图转换为对应图像）。

Result: 在多个不同的长视野任务上实现了最先进的性能。

Conclusion: SAGE框架通过语义场景图有效桥接了任务级语义推理和像素级视觉运动控制，成功解决了长视野操作任务的挑战。

Abstract: Successfully solving long-horizon manipulation tasks remains a fundamental
challenge. These tasks involve extended action sequences and complex object
interactions, presenting a critical gap between high-level symbolic planning
and low-level continuous control. To bridge this gap, two essential
capabilities are required: robust long-horizon task planning and effective
goal-conditioned manipulation. Existing task planning methods, including
traditional and LLM-based approaches, often exhibit limited generalization or
sparse semantic reasoning. Meanwhile, image-conditioned control methods
struggle to adapt to unseen tasks. To tackle these problems, we propose SAGE, a
novel framework for Scene Graph-Aware Guidance and Execution in Long-Horizon
Manipulation Tasks. SAGE utilizes semantic scene graphs as a structural
representation for scene states. A structural scene graph enables bridging
task-level semantic reasoning and pixel-level visuo-motor control. This also
facilitates the controllable synthesis of accurate, novel sub-goal images. SAGE
consists of two key components: (1) a scene graph-based task planner that uses
VLMs and LLMs to parse the environment and reason about physically-grounded
scene state transition sequences, and (2) a decoupled structural image editing
pipeline that controllably converts each target sub-goal graph into a
corresponding image through image inpainting and composition. Extensive
experiments have demonstrated that SAGE achieves state-of-the-art performance
on distinct long-horizon tasks.

</details>


### [17] [Learnable Conformal Prediction with Context-Aware Nonconformity Functions for Robotic Planning and Perception](https://arxiv.org/abs/2509.21955)
*Divake Kumar,Sina Tayebati,Francesco Migliarba,Ranganath Krishnan,Amit Ranjan Trivedi*

Main category: cs.RO

TL;DR: 提出可学习置信预测(LCP)，用轻量级神经网络替代传统置信预测的固定非一致性分数，生成上下文感知的不确定性集合，在保持理论保证的同时显著减小预测集大小并提高机器人任务的安全性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在机器人应用中输出点估计且置信度校准不佳，无法量化新输入、噪声或分布外输入的预测可靠性。传统置信预测依赖固定非一致性分数，忽略上下文信息，导致区间过于保守或不安全。

Method: LCP使用轻量级神经网络函数替代固定非一致性分数，利用几何、语义和任务特定特征生成上下文感知的不确定性集合，同时保持置信预测的理论覆盖保证。

Result: 在三个机器人任务的七个基准测试中，LCP始终优于标准置信预测和集成基线：分类任务预测集大小减少4.7-9.9%，目标检测边界框缩小46-54%，路径规划成功率从72%提升至91.5%。

Conclusion: LCP是轻量级方法（4.8%运行时开销，42KB内存），支持在线适应，适合资源受限的自主系统，在保持高帧率的同时比集成方法能效高7.4倍。

Abstract: Deep learning models in robotics often output point estimates with poorly
calibrated confidences, offering no native mechanism to quantify predictive
reliability under novel, noisy, or out-of-distribution inputs. Conformal
prediction (CP) addresses this gap by providing distribution-free coverage
guarantees, yet its reliance on fixed nonconformity scores ignores context and
can yield intervals that are overly conservative or unsafe. We address this
with Learnable Conformal Prediction (LCP), which replaces fixed scores with a
lightweight neural function that leverages geometric, semantic, and
task-specific features to produce context-aware uncertainty sets.
  LCP maintains CP's theoretical guarantees while reducing prediction set sizes
by 18% in classification, tightening detection intervals by 52%, and improving
path planning safety from 72% to 91% success with minimal overhead. Across
three robotic tasks on seven benchmarks, LCP consistently outperforms Standard
CP and ensemble baselines. In classification on CIFAR-100 and ImageNet, it
achieves smaller set sizes (4.7-9.9% reduction) at target coverage. For object
detection on COCO, BDD100K, and Cityscapes, it produces 46-54% tighter bounding
boxes. In path planning through cluttered environments, it improves success to
91.5% with only 4.5% path inflation, compared to 12.2% for Standard CP.
  The method is lightweight (approximately 4.8% runtime overhead, 42 KB memory)
and supports online adaptation, making it well suited to resource-constrained
autonomous systems. Hardware evaluation shows LCP adds less than 1% memory and
15.9% inference overhead, yet sustains 39 FPS on detection tasks while being
7.4 times more energy-efficient than ensembles.

</details>


### [18] [FlowDrive: moderated flow matching with data balancing for trajectory planning](https://arxiv.org/abs/2509.21961)
*Lingguang Wang,Ömer Şahin Taş,Marlon Steiner,Christoph Stiller*

Main category: cs.RO

TL;DR: FlowDrive是一个基于流匹配的轨迹规划器，通过条件修正流将噪声直接映射到轨迹分布，并使用调节引导增加轨迹多样性，在nuPlan和interPlan基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决基于学习的规划器对驾驶数据长尾分布的敏感性问题，常见操作在数据集中占主导地位，而危险或罕见场景稀疏，这种不平衡会导致模型偏向频繁情况，在关键场景上性能下降。

Method: 提出FlowDrive流匹配轨迹规划器，学习条件修正流以少量流匹配步骤将噪声直接映射到轨迹分布；引入调节的闭环引导，在流步骤之间注入小扰动来系统性地增加轨迹多样性，同时保持场景一致性。

Result: 在nuPlan和交互重点的interPlan基准测试中，FlowDrive在基于学习的规划器中达到最先进结果，接近带有基于规则改进的方法；添加调节引导和轻量后处理后，在几乎所有基准测试分割中都达到整体最先进性能。

Conclusion: 通过轨迹模式重新加权的平衡策略和流匹配方法有效解决了数据不平衡问题，FlowDrive在轨迹规划任务中表现出色，特别是在处理罕见但关键场景方面。

Abstract: Learning-based planners are sensitive to the long-tailed distribution of
driving data. Common maneuvers dominate datasets, while dangerous or rare
scenarios are sparse. This imbalance can bias models toward the frequent cases
and degrade performance on critical scenarios. To tackle this problem, we
compare balancing strategies for sampling training data and find reweighting by
trajectory pattern an effective approach. We then present FlowDrive, a
flow-matching trajectory planner that learns a conditional rectified flow to
map noise directly to trajectory distributions with few flow-matching steps. We
further introduce moderated, in-the-loop guidance that injects small
perturbation between flow steps to systematically increase trajectory diversity
while remaining scene-consistent. On nuPlan and the interaction-focused
interPlan benchmarks, FlowDrive achieves state-of-the-art results among
learning-based planners and approaches methods with rule-based refinements.
After adding moderated guidance and light post-processing (FlowDrive*), it
achieves overall state-of-the-art performance across nearly all benchmark
splits.

</details>


### [19] [Hybrid Diffusion for Simultaneous Symbolic and Continuous Planning](https://arxiv.org/abs/2509.21983)
*Sigmund Hennum Høeg,Aksel Vaaler,Chaoqi Liu,Olav Egeland,Yilun Du*

Main category: cs.RO

TL;DR: 提出了一种结合离散符号规划和连续轨迹生成的混合扩散方法，以解决生成模型在长时程机器人任务中的决策混淆问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的机器人轨迹生成方法在处理涉及复杂决策的长时程任务时容易混淆不同行为模式，导致失败。

Method: 通过同时生成高层符号规划和连续轨迹，结合离散变量扩散和连续扩散的混合扩散过程。

Result: 该方法显著优于基线方法，并支持基于部分或完整符号条件的灵活轨迹合成。

Conclusion: 混合扩散过程能有效提升长时程机器人任务的性能，实现更灵活的轨迹生成。

Abstract: Constructing robots to accomplish long-horizon tasks is a long-standing
challenge within artificial intelligence. Approaches using generative methods,
particularly Diffusion Models, have gained attention due to their ability to
model continuous robotic trajectories for planning and control. However, we
show that these models struggle with long-horizon tasks that involve complex
decision-making and, in general, are prone to confusing different modes of
behavior, leading to failure. To remedy this, we propose to augment continuous
trajectory generation by simultaneously generating a high-level symbolic plan.
We show that this requires a novel mix of discrete variable diffusion and
continuous diffusion, which dramatically outperforms the baselines. In
addition, we illustrate how this hybrid diffusion process enables flexible
trajectory synthesis, allowing us to condition synthesized actions on partial
and complete symbolic conditions.

</details>


### [20] [Developing Vision-Language-Action Model from Egocentric Videos](https://arxiv.org/abs/2509.21986)
*Tomoya Yoshida,Shuhei Kurita,Taichi Nishimura,Shinsuke Mori*

Main category: cs.RO

TL;DR: 提出了一种利用EgoScaler框架从原始自我中心视频中提取6DoF物体操作轨迹的方法，构建了大规模VLA预训练数据集，实验表明该数据集能显著提升任务成功率，且与真实机器人数据结合效果更佳。


<details>
  <summary>Details</summary>
Motivation: 自我中心视频捕捉了人类操作物体的方式，为学习物体操作提供了丰富的运动线索。相比昂贵的人工遥操作，自我中心视频提供了可扩展的替代方案，但现有方法通常依赖辅助标注，不清楚VLA能否直接从原始自我中心视频训练。

Method: 使用EgoScaler框架从四个大规模自我中心视频数据集中提取6DoF物体操作轨迹，自动精炼噪声或不完整的轨迹，构建新的VLA预训练数据集。

Result: 在模拟和真实机器人环境中的实验表明：(i) 使用该数据集预训练相比从头训练任务成功率提升超过20%；(ii) 性能与使用真实机器人数据集相当；(iii) 结合真实机器人数据能获得进一步改进。

Conclusion: 自我中心视频是推进VLA研究的有前景且可扩展的资源。

Abstract: Egocentric videos capture how humans manipulate objects and tools, providing
diverse motion cues for learning object manipulation. Unlike the costly,
expert-driven manual teleoperation commonly used in training
Vision-Language-Action models (VLAs), egocentric videos offer a scalable
alternative. However, prior studies that leverage such videos for training
robot policies typically rely on auxiliary annotations, such as detailed
hand-pose recordings. Consequently, it remains unclear whether VLAs can be
trained directly from raw egocentric videos. In this work, we address this
challenge by leveraging EgoScaler, a framework that extracts 6DoF object
manipulation trajectories from egocentric videos without requiring auxiliary
recordings. We apply EgoScaler to four large-scale egocentric video datasets
and automatically refine noisy or incomplete trajectories, thereby constructing
a new large-scale dataset for VLA pre-training. Our experiments with a
state-of-the-art $\pi_0$ architecture in both simulated and real-robot
environments yield three key findings: (i) pre-training on our dataset improves
task success rates by over 20\% compared to training from scratch, (ii) the
performance is competitive with that achieved using real-robot datasets, and
(iii) combining our dataset with real-robot data yields further improvements.
These results demonstrate that egocentric videos constitute a promising and
scalable resource for advancing VLA research.

</details>


### [21] [One-DoF Robotic Design of Overconstrained Limbs with Energy-Efficient, Self-Collision-Free Motion](https://arxiv.org/abs/2509.22002)
*Yuping Gu,Bangchao Huang,Haoran Sun,Ronghan Xu,Jiayi Yin,Wei Zhang,Fang Wan,Jia Pan,Chaoyang Song*

Main category: cs.RO

TL;DR: 本文提出了一种计算设计方法，用于创建单自由度过约束机器人肢体，实现期望空间轨迹、能量高效且无自碰撞的全周期旋转运动。


<details>
  <summary>Details</summary>
Motivation: 虽然多自由度仿生机器人肢体是发展趋势，但单自由度设计具有简单性、鲁棒性、成本效益和效率等优势。然而，单自由度系统在全周期运动范围内通常受自碰撞限制，需要机制设计来引入运动多样性。

Method: 首先提出连杆式机器人肢体的几何优化问题，建立自碰撞自由设计的通用公式；然后通过优化相似性和动态相关指标来制定过约束连杆的空间轨迹生成问题；进一步优化过约束连杆的几何形状，确保由单个执行器驱动的平滑无碰撞运动。

Result: 通过个性化自动机和仿生六足机器人等多种实验验证了所提方法。采用过约束机器人肢体的六足机器人在前进行走时表现出卓越的能量效率。

Conclusion: 该方法成功实现了单自由度过约束机器人肢体的计算设计，能够在全周期旋转中实现能量高效、自碰撞自由的运动，为简单而高效的机器人系统设计提供了有效解决方案。

Abstract: While it is expected to build robotic limbs with multiple degrees of freedom
(DoF) inspired by nature, a single DoF design remains fundamental, providing
benefits that include, but are not limited to, simplicity, robustness,
cost-effectiveness, and efficiency. Mechanisms, especially those with multiple
links and revolute joints connected in closed loops, play an enabling factor in
introducing motion diversity for 1-DoF systems, which are usually constrained
by self-collision during a full-cycle range of motion. This study presents a
novel computational approach to designing one-degree-of-freedom (1-DoF)
overconstrained robotic limbs for a desired spatial trajectory, while achieving
energy-efficient, self-collision-free motion in full-cycle rotations. Firstly,
we present the geometric optimization problem of linkage-based robotic limbs in
a generalized formulation for self-collision-free design. Next, we formulate
the spatial trajectory generation problem with the overconstrained linkages by
optimizing the similarity and dynamic-related metrics. We further optimize the
geometric shape of the overconstrained linkage to ensure smooth and
collision-free motion driven by a single actuator. We validated our proposed
method through various experiments, including personalized automata and
bio-inspired hexapod robots. The resulting hexapod robot, featuring
overconstrained robotic limbs, demonstrated outstanding energy efficiency
during forward walking.

</details>


### [22] [An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose](https://arxiv.org/abs/2509.22058)
*Qifeng Wang,Weigang Li,Lei Nie,Xin Xu,Wenping Liu,Zhe Xu*

Main category: cs.RO

TL;DR: 提出了一种基于自适应ICP的LiDAR里程计方法，通过可靠的初始位姿和自适应阈值机制，在动态环境中实现高精度的点云配准。


<details>
  <summary>Details</summary>
Motivation: 现有ICP方法存在两个问题：未考虑初始位姿的可靠性，可能导致收敛到局部最优；缺乏自适应机制，难以处理复杂动态环境导致配准精度下降。

Method: 1. 基于密度滤波的分布式粗配准获取初始位姿估计；2. 通过与运动预测位姿比较选择可靠初始位姿；3. 结合当前和历史误差动态调整自适应阈值；4. 基于可靠初始位姿和自适应阈值执行点对面自适应ICP配准。

Result: 在公开KITTI数据集上的大量实验表明，该方法优于现有方法，显著提高了LiDAR里程计的精度。

Conclusion: 提出的自适应ICP LiDAR里程计方法通过可靠的初始位姿和自适应阈值机制，有效解决了传统ICP方法在动态环境中的局限性，实现了更高的配准精度。

Abstract: As a key technology for autonomous navigation and positioning in mobile
robots, light detection and ranging (LiDAR) odometry is widely used in
autonomous driving applications. The Iterative Closest Point (ICP)-based
methods have become the core technique in LiDAR odometry due to their efficient
and accurate point cloud registration capability. However, some existing
ICP-based methods do not consider the reliability of the initial pose, which
may cause the method to converge to a local optimum. Furthermore, the absence
of an adaptive mechanism hinders the effective handling of complex dynamic
environments, resulting in a significant degradation of registration accuracy.
To address these issues, this paper proposes an adaptive ICP-based LiDAR
odometry method that relies on a reliable initial pose. First, distributed
coarse registration based on density filtering is employed to obtain the
initial pose estimation. The reliable initial pose is then selected by
comparing it with the motion prediction pose, reducing the initial error
between the source and target point clouds. Subsequently, by combining the
current and historical errors, the adaptive threshold is dynamically adjusted
to accommodate the real-time changes in the dynamic environment. Finally, based
on the reliable initial pose and the adaptive threshold, point-to-plane
adaptive ICP registration is performed from the current frame to the local map,
achieving high-precision alignment of the source and target point clouds.
Extensive experiments on the public KITTI dataset demonstrate that the proposed
method outperforms existing approaches and significantly enhances the accuracy
of LiDAR odometry.

</details>


### [23] [Effect of Gait Design on Proprioceptive Sensing of Terrain Properties in a Quadrupedal Robot](https://arxiv.org/abs/2509.22065)
*Ethan Fulcher,J. Diego Caporale,Yifeng Zhang,John Ruck,Feifei Qian*

Main category: cs.RO

TL;DR: 本文研究了步态对腿式机器人本体感知地形传感精度的影响，比较了传感导向的爬行步态和运动导向的小跑步态在测量松软地形强度和纹理方面的表现。


<details>
  <summary>Details</summary>
Motivation: 通过腿式机器人在移动过程中感知地形特性，可以为行星探测提供前所未有的采样速度和密度，同时访问以前因风险过高而无法采样的地形。

Method: 在实验室环境中，让机器人使用两种不同步态（传感导向的Crawl N' Sense和运动导向的Trot-Walk）在包含刚性表面、松散沙地和带合成表面结壳的松散沙地的样带上移动，测量其对地形强度和纹理的感知能力。

Result: 两种步态都能一致区分低阻力和高阻力基质的强度差异，但运动导向的小跑步态测量值幅度和方差更大。较慢的爬行步态检测表面结壳脆性破裂的准确率显著高于较快的小跑步态。

Conclusion: 研究结果为腿式机器人"移动中感知"的步态设计和规划提供了新见解，有助于在其他星球上进行地形侦察和科学测量，增进对其地质和形成过程的理解。

Abstract: In-situ robotic exploration is an important tool for advancing knowledge of
geological processes that describe the Earth and other Planetary bodies. To
inform and enhance operations for these roving laboratories, it is imperative
to understand the terramechanical properties of their environments, especially
for traversing on loose, deformable substrates. Recent research suggested that
legged robots with direct-drive and low-gear ratio actuators can sensitively
detect external forces, and therefore possess the potential to measure terrain
properties with their legs during locomotion, providing unprecedented sampling
speed and density while accessing terrains previously too risky to sample. This
paper explores these ideas by investigating the impact of gait on
proprioceptive terrain sensing accuracy, particularly comparing a
sensing-oriented gait, Crawl N' Sense, with a locomotion-oriented gait,
Trot-Walk. Each gait's ability to measure the strength and texture of
deformable substrate is quantified as the robot locomotes over a laboratory
transect consisting of a rigid surface, loose sand, and loose sand with
synthetic surface crusts. Our results suggest that with both the
sensing-oriented crawling gait and locomotion-oriented trot gait, the robot can
measure a consistent difference in the strength (in terms of penetration
resistance) between the low- and high-resistance substrates; however, the
locomotion-oriented trot gait contains larger magnitude and variance in
measurements. Furthermore, the slower crawl gait can detect brittle ruptures of
the surface crusts with significantly higher accuracy than the faster trot
gait. Our results offer new insights that inform legged robot "sensing during
locomotion" gait design and planning for scouting the terrain and producing
scientific measurements on other worlds to advance our understanding of their
geology and formation.

</details>


### [24] [Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation](https://arxiv.org/abs/2509.22093)
*Xiaohuan Pei,Yuxing Chen,Siyu Xu,Yunke Wang,Yuheng Shi,Chang Xu*

Main category: cs.RO

TL;DR: 提出ADP框架，通过动作感知的动态剪枝机制，根据机器人操作阶段的不同视觉冗余度自适应调整token保留比例，在保持性能的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在优化VLA模型推理速度时，忽视了机器人操作不同阶段的视觉冗余度差异。研究发现粗粒度操作阶段的视觉token冗余度高于细粒度操作，且与动作动态性相关。

Method: 提出动作感知动态剪枝(ADP)框架，结合文本驱动的token选择和动作感知的轨迹门控机制，利用过去动作窗口自适应调整token保留比例。

Result: 在LIBERO套件和真实场景实验中，显著降低FLOPs和动作推理延迟（如OpenVLA-OFT上1.35倍加速），同时保持竞争力的成功率（如OpenVLA上提升25.8%）。

Conclusion: ADP为高效机器人策略提供了简单的插件式路径，推进了机器人操作在效率和性能方面的前沿。

Abstract: Robotic manipulation with Vision-Language-Action models requires efficient
inference over long-horizon multi-modal context, where attention to dense
visual tokens dominates computational cost. Existing methods optimize inference
speed by reducing visual redundancy within VLA models, but they overlook the
varying redundancy across robotic manipulation stages. We observe that the
visual token redundancy is higher in coarse manipulation phase than in
fine-grained operations, and is strongly correlated with the action dynamic.
Motivated by this observation, we propose \textbf{A}ction-aware
\textbf{D}ynamic \textbf{P}runing (\textbf{ADP}), a multi-modal pruning
framework that integrates text-driven token selection with action-aware
trajectory gating. Our method introduces a gating mechanism that conditions the
pruning signal on recent action trajectories, using past motion windows to
adaptively adjust token retention ratios in accordance with dynamics, thereby
balancing computational efficiency and perceptual precision across different
manipulation stages. Extensive experiments on the LIBERO suites and diverse
real-world scenarios demonstrate that our method significantly reduces FLOPs
and action inference latency (\textit{e.g.} $1.35 \times$ speed up on
OpenVLA-OFT) while maintaining competitive success rates (\textit{e.g.} 25.8\%
improvements with OpenVLA) compared to baselines, thereby providing a simple
plug-in path to efficient robot policies that advances the efficiency and
performance frontier of robotic manipulation. Our project website is:
\href{https://vla-adp.github.io/}{ADP.com}.

</details>


### [25] [Multi-stage robust nonlinear model predictive control of a lower-limb exoskeleton robot](https://arxiv.org/abs/2509.22120)
*Alireza Aliyari,Gholamreza Vossoughi*

Main category: cs.RO

TL;DR: 提出了一种鲁棒非线性模型预测控制（RNMPC）方法，称为多阶段NMPC，用于控制二自由度外骨骼机器人，通过解决非线性优化问题来处理系统不确定性。


<details>
  <summary>Details</summary>
Motivation: 由于肌肉骨骼损伤增加，外骨骼机器人的使用日益增多，但其有效性严重依赖于控制系统设计。现有线性化方法在实现鲁棒MPC时会因机器人动力学的非线性而降低性能。

Method: 采用多阶段NMPC方法，使用多个场景来表示系统不确定性，专注于在摆动阶段最小化人机交互力，特别是在机器人携带未知负载的情况下。

Result: 仿真和实验测试表明，该方法显著提高了鲁棒性，优于非鲁棒NMPC。在2kg未知负载和外部干扰下，多阶段NMPC的大腿和小腿交互力RMS值分别比非鲁棒NMPC降低了77%和94%。

Conclusion: 多阶段NMPC方法能有效处理外骨骼机器人控制系统中的不确定性，显著降低跟踪误差和交互力，提高系统鲁棒性。

Abstract: The use of exoskeleton robots is increasing due to the rising number of
musculoskeletal injuries. However, their effectiveness depends heavily on the
design of control systems. Designing robust controllers is challenging because
of uncertainties in human-robot systems. Among various control strategies,
Model Predictive Control (MPC) is a powerful approach due to its ability to
handle constraints and optimize performance. Previous studies have used
linearization-based methods to implement robust MPC on exoskeletons, but these
can degrade performance due to nonlinearities in the robot's dynamics. To
address this gap, this paper proposes a Robust Nonlinear Model Predictive
Control (RNMPC) method, called multi-stage NMPC, to control a
two-degree-of-freedom exoskeleton by solving a nonlinear optimization problem.
This method uses multiple scenarios to represent system uncertainties. The
study focuses on minimizing human-robot interaction forces during the swing
phase, particularly when the robot carries unknown loads. Simulations and
experimental tests show that the proposed method significantly improves
robustness, outperforming non-robust NMPC. It achieves lower tracking errors
and interaction forces under various uncertainties. For instance, when a 2 kg
unknown payload is combined with external disturbances, the RMS values of thigh
and shank interaction forces for multi-stage NMPC are reduced by 77 and 94
percent, respectively, compared to non-robust NMPC.

</details>


### [26] [DemoGrasp: Universal Dexterous Grasping from a Single Demonstration](https://arxiv.org/abs/2509.22149)
*Haoqi Yuan,Ziye Huang,Ye Wang,Chuan Mao,Chaoyi Xu,Zongqing Lu*

Main category: cs.RO

TL;DR: DemoGrasp是一种简单有效的通用灵巧抓取学习方法，通过编辑单条成功演示轨迹来适应新物体和姿态，在模拟中达到95%成功率，并在真实世界中成功抓取110个未见物体。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的灵巧抓取方法需要复杂的奖励和课程设计，且在高维长时域探索中难以获得最优解。DemoGrasp旨在通过简单的轨迹编辑方法解决通用灵巧抓取问题。

Method: 从单条成功抓取演示轨迹出发，通过编辑机器人动作来适应新物体：改变手腕姿态决定抓取位置，改变手部关节角度决定抓取方式。将此轨迹编辑建模为单步马尔可夫决策过程，使用强化学习并行优化通用策略。

Result: 在模拟中，DemoGrasp在DexGraspNet物体上使用Shadow Hand达到95%成功率，优于之前最优方法。在六个未见物体数据集上平均成功率达84.6%，仅用175个物体训练。在真实世界中成功抓取110个未见物体，包括小而薄的物品。

Conclusion: DemoGrasp通过简单的轨迹编辑方法实现了高效的通用灵巧抓取，具有良好的泛化能力和迁移性，支持视觉引导和语言指导抓取。

Abstract: Universal grasping with multi-fingered dexterous hands is a fundamental
challenge in robotic manipulation. While recent approaches successfully learn
closed-loop grasping policies using reinforcement learning (RL), the inherent
difficulty of high-dimensional, long-horizon exploration necessitates complex
reward and curriculum design, often resulting in suboptimal solutions across
diverse objects. We propose DemoGrasp, a simple yet effective method for
learning universal dexterous grasping. We start from a single successful
demonstration trajectory of grasping a specific object and adapt to novel
objects and poses by editing the robot actions in this trajectory: changing the
wrist pose determines where to grasp, and changing the hand joint angles
determines how to grasp. We formulate this trajectory editing as a single-step
Markov Decision Process (MDP) and use RL to optimize a universal policy across
hundreds of objects in parallel in simulation, with a simple reward consisting
of a binary success term and a robot-table collision penalty. In simulation,
DemoGrasp achieves a 95% success rate on DexGraspNet objects using the Shadow
Hand, outperforming previous state-of-the-art methods. It also shows strong
transferability, achieving an average success rate of 84.6% across diverse
dexterous hand embodiments on six unseen object datasets, while being trained
on only 175 objects. Through vision-based imitation learning, our policy
successfully grasps 110 unseen real-world objects, including small, thin items.
It generalizes to spatial, background, and lighting changes, supports both RGB
and depth inputs, and extends to language-guided grasping in cluttered scenes.

</details>


### [27] [DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions](https://arxiv.org/abs/2509.22175)
*Quanzhou Li,Zhonghua Wu,Jingbo Wang,Chen Change Loy,Bo Dai*

Main category: cs.RO

TL;DR: 提出SymOpt管道构建大规模双手抓取数据集，并开发DHAGrasp文本引导双手抓取生成器，能够为未见物体生成语义一致的双手抓取动作。


<details>
  <summary>Details</summary>
Motivation: 现有抓取数据集主要关注单手交互且语义标注有限，缺乏能够生成尊重物体语义的双手抓取方法。

Method: 利用现有单手数据集和物体、手部对称性构建数据集；提出新颖的双手功能表示和两阶段设计，结合分割训练对象和未分割数据。

Result: 实验表明该方法能生成多样且语义一致的抓取动作，在抓取质量和泛化能力上优于强基线方法。

Conclusion: 该方法成功解决了双手抓取数据集稀缺问题，实现了对未见物体的语义感知双手抓取生成。

Abstract: Learning to generate dual-hand grasps that respect object semantics is
essential for robust hand-object interaction but remains largely underexplored
due to dataset scarcity. Existing grasp datasets predominantly focus on
single-hand interactions and contain only limited semantic part annotations. To
address these challenges, we introduce a pipeline, SymOpt, that constructs a
large-scale dual-hand grasp dataset by leveraging existing single-hand datasets
and exploiting object and hand symmetries. Building on this, we propose a
text-guided dual-hand grasp generator, DHAGrasp, that synthesizes Dual-Hand
Affordance-aware Grasps for unseen objects. Our approach incorporates a novel
dual-hand affordance representation and follows a two-stage design, which
enables effective learning from a small set of segmented training objects while
scaling to a much larger pool of unsegmented data. Extensive experiments
demonstrate that our method produces diverse and semantically consistent
grasps, outperforming strong baselines in both grasp quality and generalization
to unseen objects. The project page is at
https://quanzhou-li.github.io/DHAGrasp/.

</details>


### [28] [Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting](https://arxiv.org/abs/2509.22195)
*Asher J. Hancock,Xindi Wu,Lihan Zha,Olga Russakovsky,Anirudha Majumdar*

Main category: cs.RO

TL;DR: VLM2VLA是一种新的视觉语言动作模型训练范式，通过用自然语言表示低级动作来解决预训练与机器人数据之间的分布不匹配问题，使用LoRA微调避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 传统方法在视觉语言模型上微调机器人遥操作数据时，学习动作会削弱模型的推理和多模态理解能力，导致泛化能力下降。这是由于预训练数据与机器人数据之间的分布不匹配造成的。

Method: 提出VLM2VLA方法：1）用自然语言表示低级动作以解决数据分布不匹配；2）仅使用低秩适应(LoRA)进行微调，最小化对VLM骨干网络的修改；3）避免在互联网规模VLM数据集上进行昂贵的联合训练。

Result: 通过广泛的视觉问答研究和800多个真实世界机器人实验证明，VLM2VLA保留了VLM的核心能力，能够零样本泛化到需要开放世界语义推理和多语言指令跟随的新任务。

Conclusion: VLM2VLA通过数据层面的对齐和LoRA微调，成功解决了VLA训练中的灾难性遗忘问题，为训练通用机器人策略提供了有效解决方案。

Abstract: Fine-tuning vision-language models (VLMs) on robot teleoperation data to
create vision-language-action (VLA) models is a promising paradigm for training
generalist policies, but it suffers from a fundamental tradeoff: learning to
produce actions often diminishes the VLM's foundational reasoning and
multimodal understanding, hindering generalization to novel scenarios,
instruction following, and semantic understanding. We argue that this
catastrophic forgetting is due to a distribution mismatch between the VLM's
internet-scale pretraining corpus and the robotics fine-tuning data. Inspired
by this observation, we introduce VLM2VLA: a VLA training paradigm that first
resolves this mismatch at the data level by representing low-level actions with
natural language. This alignment makes it possible to train VLAs solely with
Low-Rank Adaptation (LoRA), thereby minimally modifying the VLM backbone and
averting catastrophic forgetting. As a result, the VLM can be fine-tuned on
robot teleoperation data without fundamentally altering the underlying
architecture and without expensive co-training on internet-scale VLM datasets.
Through extensive Visual Question Answering (VQA) studies and over 800
real-world robotics experiments, we demonstrate that VLM2VLA preserves the
VLM's core capabilities, enabling zero-shot generalization to novel tasks that
require open-world semantic reasoning and multilingual instruction following.

</details>


### [29] [MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training](https://arxiv.org/abs/2509.22199)
*Haoyun Li,Ivan Zhang,Runqi Ouyang,Xiaofeng Wang,Zheng Zhu,Zhiqin Yang,Zhentao Zhang,Boyuan Wang,Chaojun Ni,Wenkang Qin,Xinze Chen,Yun Ye,Guan Huang,Zhenbo Song,Xingang Wang*

Main category: cs.RO

TL;DR: MimicDreamer框架通过视觉、视角和动作三方面对齐，将低成本的人类演示视频转化为机器人可用的监督数据，显著提升VLA模型在真实机器人上的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 收集机器人交互数据成本高昂，而人类演示视频更具可扩展性和成本效益，但存在视觉、视角和动作动态的领域差距。

Method: 提出MimicDreamer框架：H2R Aligner用于视觉对齐生成机器人演示视频；EgoStabilizer用于视角稳定化；动作对齐通过轨迹映射和约束逆运动学求解器实现。

Result: 仅使用合成的人类到机器人视频训练的VLA模型在真实机器人上实现少样本执行，平均成功率在六个代表性操作任务上提升14.7%。

Conclusion: 通过将人类演示转化为机器人可用监督，可以显著提升VLA模型的性能，且扩展人类数据训练比仅使用真实机器人数据效果更好。

Abstract: Vision Language Action (VLA) models derive their generalization capability
from diverse training data, yet collecting embodied robot interaction data
remains prohibitively expensive. In contrast, human demonstration videos are
far more scalable and cost-efficient to collect, and recent studies confirm
their effectiveness in training VLA models. However, a significant domain gap
persists between human videos and robot-executed videos, including unstable
camera viewpoints, visual discrepancies between human hands and robotic arms,
and differences in motion dynamics. To bridge this gap, we propose
MimicDreamer, a framework that turns fast, low-cost human demonstrations into
robot-usable supervision by jointly aligning vision, viewpoint, and actions to
directly support policy training. For visual alignment, we propose H2R Aligner,
a video diffusion model that generates high-fidelity robot demonstration videos
by transferring motion from human manipulation footage. For viewpoint
stabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos
via homography and inpaints occlusions and distortions caused by warping. For
action alignment, we map human hand trajectories to the robot frame and apply a
constrained inverse kinematics solver to produce feasible, low-jitter joint
commands with accurate pose tracking. Empirically, VLA models trained purely on
our synthesized human-to-robot videos achieve few-shot execution on real
robots. Moreover, scaling training with human data significantly boosts
performance compared to models trained solely on real robot data; our approach
improves the average success rate by 14.7\% across six representative
manipulation tasks.

</details>


### [30] [From Watch to Imagine: Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment](https://arxiv.org/abs/2509.22205)
*Ke Ye,Jiaming Zhou,Yuanfeng Qiu,Jiayi Liu,Shihui Zhou,Kun-Yu Lin,Junwei Liang*

Main category: cs.RO

TL;DR: Super-Mimic是一个分层框架，通过从无脚本的人类演示视频中直接推断程序意图，实现零样本机器人模仿。


<details>
  <summary>Details</summary>
Motivation: 解决机器人零样本设置下泛化到长视野操作任务的核心挑战，当前多模态基础方法无法仅从静态视觉输入分解高级命令为可执行动作序列。

Method: 包含两个顺序模块：人类意图翻译器（HIT）使用多模态推理解析输入视频生成语言基础子任务序列；未来动态预测器（FDP）使用生成模型为每个步骤合成物理合理的视频展开。

Result: 在长视野操作任务套件上的广泛实验表明，Super-Mimic显著优于最先进的零样本方法超过20%。

Conclusion: 将视频驱动的意图解析与前瞻动态建模相结合是开发通用机器人系统的有效策略。

Abstract: Generalizing to long-horizon manipulation tasks in a zero-shot setting
remains a central challenge in robotics. Current multimodal foundation based
approaches, despite their capabilities, typically fail to decompose high-level
commands into executable action sequences from static visual input alone. To
address this challenge, we introduce Super-Mimic, a hierarchical framework that
enables zero-shot robotic imitation by directly inferring procedural intent
from unscripted human demonstration videos. Our framework is composed of two
sequential modules. First, a Human Intent Translator (HIT) parses the input
video using multimodal reasoning to produce a sequence of language-grounded
subtasks. These subtasks then condition a Future Dynamics Predictor (FDP),
which employs a generative model that synthesizes a physically plausible video
rollout for each step. The resulting visual trajectories are dynamics-aware,
explicitly modeling crucial object interactions and contact points to guide the
low-level controller. We validate this approach through extensive experiments
on a suite of long-horizon manipulation tasks, where Super-Mimic significantly
outperforms state-of-the-art zero-shot methods by over 20\%. These results
establish that coupling video-driven intent parsing with prospective dynamics
modeling is a highly effective strategy for developing general-purpose robotic
systems.

</details>


### [31] [Leveraging Large Language Models for Robot-Assisted Learning of Morphological Structures in Preschool Children with Language Vulnerabilities](https://arxiv.org/abs/2509.22287)
*Stina Sundstedt,Mattias Wingren,Susanne Hägglund,Daniel Ventus*

Main category: cs.RO

TL;DR: 开发基于Furhat对话机器人的语言学习应用，通过游戏化方式帮助有语言障碍的学龄前儿童改善表达性语言技能，利用大语言模型管理游戏流程并生成特定形态学目标。


<details>
  <summary>Details</summary>
Motivation: 帮助有语言障碍的学龄前儿童（如发展性语言障碍或移民相关语言挑战）改善表达性语言技能，减轻语言治疗师和教育者的教学负担。

Method: 开发基于Furhat对话机器人的应用，通过玩单词检索游戏"Alias"，利用大语言模型管理游戏、对话、情感响应和轮流互动，并计划生成特定形态学目标。

Result: 目前应用已开发完成，使用大语言模型管理游戏流程，下一步将扩展功能以生成特定形态学目标。

Conclusion: 机器人有望在此任务上超越人类表现，长期目标是创建基于大语言模型的机器人辅助语言学习干预系统，能够教授多种语言的形态学结构。

Abstract: Preschool children with language vulnerabilities -- such as developmental
language disorders or immigration related language challenges -- often require
support to strengthen their expressive language skills. Based on the principle
of implicit learning, speech-language therapists (SLTs) typically embed target
morphological structures (e.g., third person -s) into everyday interactions or
game-based learning activities. Educators are recommended by SLTs to do the
same. This approach demands precise linguistic knowledge and real-time
production of various morphological forms (e.g., "Daddy wears these when he
drives to work"). The task becomes even more demanding when educators or parent
also must keep children engaged and manage turn-taking in a game-based
activity. In the TalBot project our multiprofessional team have developed an
application in which the Furhat conversational robot plays the word retrieval
game "Alias" with children to improve language skills. Our application
currently employs a large language model (LLM) to manage gameplay, dialogue,
affective responses, and turn-taking. Our next step is to further leverage the
capacity of LLMs so the robot can generate and deliver specific morphological
targets during the game. We hypothesize that a robot could outperform humans at
this task. Novel aspects of this approach are that the robot could ultimately
serve as a model and tutor for both children and professionals and that using
LLM capabilities in this context would support basic communication needs for
children with language vulnerabilities. Our long-term goal is to create a
robust LLM-based Robot-Assisted Language Learning intervention capable of
teaching a variety of morphological structures across different languages.

</details>


### [32] [IMU-Preintegrated Radar Factors for Asynchronous Radar-LiDAR-Inertial SLAM](https://arxiv.org/abs/2509.22288)
*Johan Hatleskog,Morten Nissov,Kostas Alexis*

Main category: cs.RO

TL;DR: 提出一种IMU预积分雷达因子方法，通过高频IMU数据将最近LiDAR状态传播到雷达测量时间戳，将状态节点创建率降低50%，在保持定位精度的同时显著减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统固定滞后雷达-LiDAR-IMU平滑器为每个测量创建状态节点，导致状态创建率翻倍，计算成本高，难以在资源受限硬件上实现实时性能。

Method: 使用IMU预积分雷达因子，利用高频惯性数据将最新LiDAR状态传播到雷达测量时间戳，保持节点创建率与LiDAR测量频率一致。

Result: 在单板计算机上测试，保持与传统方法相同的绝对位姿误差，同时将因子图优化时间降低高达56%。

Conclusion: 该方法有效降低了计算复杂度，在资源受限硬件上实现了实时性能，同时保持了定位精度。

Abstract: Fixed-lag Radar-LiDAR-Inertial smoothers conventionally create one factor
graph node per measurement to compensate for the lack of time synchronization
between radar and LiDAR. For a radar-LiDAR sensor pair with equal rates, this
strategy results in a state creation rate of twice the individual sensor
frequencies. This doubling of the number of states per second yields high
optimization costs, inhibiting real-time performance on resource-constrained
hardware. We introduce IMU-preintegrated radar factors that use high-rate
inertial data to propagate the most recent LiDAR state to the radar measurement
timestamp. This strategy maintains the node creation rate at the LiDAR
measurement frequency. Assuming equal sensor rates, this lowers the number of
nodes by 50 % and consequently the computational costs. Experiments on a single
board computer (which has 4 cores each of 2.2 GHz A73 and 2 GHz A53 with 8 GB
RAM) show that our method preserves the absolute pose error of a conventional
baseline while simultaneously lowering the aggregated factor graph optimization
time by up to 56 %.

</details>


### [33] [Beyond Detection -- Orchestrating Human-Robot-Robot Assistance via an Internet of Robotic Things Paradigm](https://arxiv.org/abs/2509.22296)
*Joseph Hunt,Koyo Fujii,Aly Magassouba,Praminda Caleb-Solly*

Main category: cs.RO

TL;DR: 提出基于物联网机器人技术的系统架构，通过热感测和机器人协调实现主动式病人跌倒预防，能够预测离床意图并提供个性化协助。


<details>
  <summary>Details</summary>
Motivation: 解决传统跌倒预防系统的高误报率和反应式检测问题，关注病人离床的根本需求（如口渴、不适等），实现更人性化的护理。

Method: 采用隐私保护的热感测模型进行实时离床预测，整合两个协调的机器人代理，根据预测意图和病人输入动态响应。

Result: 展示了低分辨率热感测在准确预测离床方面的有效性，用户研究和系统误差分析为多智能体交互设计提供了指导。

Conclusion: 交互式连接的机器人系统能够超越被动监控，提供及时有效的协助，创造更安全、响应更快的护理环境。

Abstract: Hospital patient falls remain a critical and costly challenge worldwide.
While conventional fall prevention systems typically rely on post-fall
detection or reactive alerts, they also often suffer from high false positive
rates and fail to address the underlying patient needs that lead to bed-exit
attempts. This paper presents a novel system architecture that leverages the
Internet of Robotic Things (IoRT) to orchestrate human-robot-robot interaction
for proactive and personalized patient assistance. The system integrates a
privacy-preserving thermal sensing model capable of real-time bed-exit
prediction, with two coordinated robotic agents that respond dynamically based
on predicted intent and patient input. This orchestrated response could not
only reduce fall risk but also attend to the patient's underlying motivations
for movement, such as thirst, discomfort, or the need for assistance, before a
hazardous situation arises. Our contributions with this pilot study are
three-fold: (1) a modular IoRT-based framework enabling distributed sensing,
prediction, and multi-robot coordination; (2) a demonstration of low-resolution
thermal sensing for accurate, privacy-preserving preemptive bed-exit detection;
and (3) results from a user study and systematic error analysis that inform the
design of situationally aware, multi-agent interactions in hospital settings.
The findings highlight how interactive and connected robotic systems can move
beyond passive monitoring to deliver timely, meaningful assistance, empowering
safer, more responsive care environments.

</details>


### [34] [RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation](https://arxiv.org/abs/2509.22356)
*Enguang Liu,Siyuan Liang,Liming Lu,Xiyu Zeng,Xiaochun Cao,Aishan Liu,Shuchao Pang*

Main category: cs.RO

TL;DR: 提出了RoboView-Bias基准，首次系统量化机器人操作中的视觉偏见，通过因子隔离原则创建2127个任务实例，评估发现所有智能体都存在显著视觉偏见，并提出基于语义接地层的缓解策略可减少54.5%的偏见。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注泛化性和扰动下的鲁棒性，缺乏对视觉偏见的系统量化，限制了理解感知如何影响决策稳定性。

Method: 采用结构化变体生成框架和感知公平验证协议，创建2127个任务实例，评估三种代表性具身智能体在两种主流范式下的表现。

Result: 发现所有智能体都存在显著视觉偏见，相机视角是最关键因素；智能体在高度饱和颜色上成功率最高；视觉偏见存在强不对称耦合。

Conclusion: 系统分析视觉偏见是开发安全可靠通用具身智能体的先决条件，提出的缓解策略能显著减少视觉偏见。

Abstract: The safety and reliability of embodied agents rely on accurate and unbiased
visual perception. However, existing benchmarks mainly emphasize generalization
and robustness under perturbations, while systematic quantification of visual
bias remains scarce. This gap limits a deeper understanding of how perception
influences decision-making stability. To address this issue, we propose
RoboView-Bias, the first benchmark specifically designed to systematically
quantify visual bias in robotic manipulation, following a principle of factor
isolation. Leveraging a structured variant-generation framework and a
perceptual-fairness validation protocol, we create 2,127 task instances that
enable robust measurement of biases induced by individual visual factors and
their interactions. Using this benchmark, we systematically evaluate three
representative embodied agents across two prevailing paradigms and report three
key findings: (i) all agents exhibit significant visual biases, with camera
viewpoint being the most critical factor; (ii) agents achieve their highest
success rates on highly saturated colors, indicating inherited visual
preferences from underlying VLMs; and (iii) visual biases show strong,
asymmetric coupling, with viewpoint strongly amplifying color-related bias.
Finally, we demonstrate that a mitigation strategy based on a semantic
grounding layer substantially reduces visual bias by approximately 54.5\% on
MOKA. Our results highlight that systematic analysis of visual bias is a
prerequisite for developing safe and reliable general-purpose embodied agents.

</details>


### [35] [Learning-Based Collaborative Control for Bi-Manual Tactile-Reactive Grasping](https://arxiv.org/abs/2509.22421)
*Leonel Giacobbe,Jingdao Chen,Chuangchuang Sun*

Main category: cs.RO

TL;DR: 提出基于学习的触觉反应多智能体模型预测控制器，用于抓取不同软硬度和形状的物体，超越单智能体系统的能力限制


<details>
  <summary>Details</summary>
Motivation: 现有抓取系统主要针对刚性物体，处理易碎或可变形材料时性能显著下降，且单智能体系统无法抓取大型重物

Method: 使用两个Gelsight Mini触觉传感器提取实时纹理和刚度信息，通过多智能体MPC框架进行学习控制，结合触觉编码预测抓取稳定性并调整力和位置

Result: 在抓取不同尺寸和刚度物体的稳定抓取成功率方面优于独立的PD和MPC基线方法

Conclusion: 结合触觉传感和学习型多智能体MPC的方法为复杂环境中的协作抓取提供了鲁棒智能解决方案，显著提升了多智能体系统的能力

Abstract: Grasping is a core task in robotics with various applications. However, most
current implementations are primarily designed for rigid items, and their
performance drops considerably when handling fragile or deformable materials
that require real-time feedback. Meanwhile, tactile-reactive grasping focuses
on a single agent, which limits their ability to grasp and manipulate large,
heavy objects. To overcome this, we propose a learning-based, tactile-reactive
multi-agent Model Predictive Controller (MPC) for grasping a wide range of
objects with different softness and shapes, beyond the capabilities of
preexisting single-agent implementations. Our system uses two Gelsight Mini
tactile sensors [1] to extract real-time information on object texture and
stiffness. This rich tactile feedback is used to estimate contact dynamics and
object compliance in real time, enabling the system to adapt its control policy
to diverse object geometries and stiffness profiles. The learned controller
operates in a closed loop, leveraging tactile encoding to predict grasp
stability and adjust force and position accordingly. Our key technical
contributions include a multi-agent MPC formulation trained on real contact
interactions, a tactile-data driven method for inferring grasping states, and a
coordination strategy that enables collaborative control. By combining tactile
sensing and a learning-based multi-agent MPC, our method offers a robust,
intelligent solution for collaborative grasping in complex environments,
significantly advancing the capabilities of multi-agent systems. Our approach
is validated through extensive experiments against independent PD and MPC
baselines. Our pipeline outperforms the baselines regarding success rates in
achieving and maintaining stable grasps across objects of varying sizes and
stiffness.

</details>


### [36] [An Ontology for Unified Modeling of Tasks, Actions, Environments, and Capabilities in Personal Service Robotics](https://arxiv.org/abs/2509.22434)
*Margherita Martorana,Francesca Urgese,Ilaria Tiddi,Stefan Schlobach*

Main category: cs.RO

TL;DR: 提出了OntoBOT本体，统一表示任务、动作、环境和机器人能力，支持服务机器人的形式化推理和知识共享。


<details>
  <summary>Details</summary>
Motivation: 现有服务机器人解决方案通常与特定平台紧密耦合，导致孤立、硬编码的实现，限制了互操作性、可重用性和知识共享。现有本体如SOMA和DOLCE专注于特定领域，未能完全捕捉环境、动作、机器人能力和系统级集成之间的连接。

Method: 扩展现有本体，提出OntoBOT本体，提供任务、动作、环境和能力的统一表示。通过评估四个具身代理（TIAGo、HSR、UR3、Stretch）的能力问题来验证其通用性。

Result: OntoBOT支持上下文感知推理、面向任务的执行和知识共享，在四个不同机器人平台上展示了其通用性。

Conclusion: OntoBOT为服务机器人提供了一个统一的本体框架，解决了现有解决方案的平台依赖性和知识共享限制问题。

Abstract: Personal service robots are increasingly used in domestic settings to assist
older adults and people requiring support. Effective operation involves not
only physical interaction but also the ability to interpret dynamic
environments, understand tasks, and choose appropriate actions based on
context. This requires integrating both hardware components (e.g. sensors,
actuators) and software systems capable of reasoning about tasks, environments,
and robot capabilities. Frameworks such as the Robot Operating System (ROS)
provide open-source tools that help connect low-level hardware with
higher-level functionalities. However, real-world deployments remain tightly
coupled to specific platforms. As a result, solutions are often isolated and
hard-coded, limiting interoperability, reusability, and knowledge sharing.
Ontologies and knowledge graphs offer a structured way to represent tasks,
environments, and robot capabilities. Existing ontologies, such as the
Socio-physical Model of Activities (SOMA) and the Descriptive Ontology for
Linguistic and Cognitive Engineering (DOLCE), provide models for activities,
spatial relationships, and reasoning structures. However, they often focus on
specific domains and do not fully capture the connection between environment,
action, robot capabilities, and system-level integration. In this work, we
propose the Ontology for roBOts and acTions (OntoBOT), which extends existing
ontologies to provide a unified representation of tasks, actions, environments,
and capabilities. Our contributions are twofold: (1) we unify these aspects
into a cohesive ontology to support formal reasoning about task execution, and
(2) we demonstrate its generalizability by evaluating competency questions
across four embodied agents - TIAGo, HSR, UR3, and Stretch - showing how
OntoBOT enables context-aware reasoning, task-oriented execution, and knowledge
sharing in service robotics.

</details>


### [37] [UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation](https://arxiv.org/abs/2509.22441)
*Zhangyuan Wang,Yunpeng Zhu,Yuqi Yan,Xiaoyuan Tian,Xinhao Shao,Meixuan Li,Weikun Li,Guangsheng Su,Weicheng Cui,Dixia Fan*

Main category: cs.RO

TL;DR: UnderwaterVLA是一个用于自主水下导航的新框架，集成了多模态基础模型和具身智能系统，通过双脑架构、VLA模型和水动力学MPC方案，显著提高了在恶劣水下环境中的导航精度和任务完成率。


<details>
  <summary>Details</summary>
Motivation: 水下作业面临流体动力学扰动、有限通信带宽和浑浊水域感知能力下降等挑战，需要开发更鲁棒、适应性强的自主导航系统。

Method: 1. 双脑架构分离高层任务推理和低层反应控制；2. 首次将视觉-语言-动作模型应用于水下机器人，采用结构化思维链推理；3. 水动力学模型预测控制方案实时补偿流体效应。

Result: 现场测试显示，在视觉条件退化情况下，UnderwaterVLA减少了导航误差，任务完成率比基线提高了19%至27%。

Conclusion: 通过减少对水下特定训练数据的依赖并提高跨环境适应性，UnderwaterVLA为下一代智能AUV提供了可扩展且经济高效的路径。

Abstract: This paper presents UnderwaterVLA, a novel framework for autonomous
underwater navigation that integrates multimodal foundation models with
embodied intelligence systems. Underwater operations remain difficult due to
hydrodynamic disturbances, limited communication bandwidth, and degraded
sensing in turbid waters. To address these challenges, we introduce three
innovations. First, a dual-brain architecture decouples high-level mission
reasoning from low-level reactive control, enabling robust operation under
communication and computational constraints. Second, we apply
Vision-Language-Action(VLA) models to underwater robotics for the first time,
incorporating structured chain-of-thought reasoning for interpretable
decision-making. Third, a hydrodynamics-informed Model Predictive Control(MPC)
scheme compensates for fluid effects in real time without costly task-specific
training. Experimental results in field tests show that UnderwaterVLA reduces
navigation errors in degraded visual conditions while maintaining higher task
completion by 19% to 27% over baseline. By minimizing reliance on
underwater-specific training data and improving adaptability across
environments, UnderwaterVLA provides a scalable and cost-effective path toward
the next generation of intelligent AUVs.

</details>


### [38] [Uncertainty-Aware Multi-Robot Task Allocation With Strongly Coupled Inter-Robot Rewards](https://arxiv.org/abs/2509.22469)
*Ben Rossano,Jaein Lim,Jonathan P. How*

Main category: cs.RO

TL;DR: 提出了一种面向异构机器人团队的任务分配算法，在任务需求不确定的环境中通过概率建模和基于市场的方法优化团队目标。


<details>
  <summary>Details</summary>
Motivation: 解决异构机器人在任务需求不确定环境中的任务分配问题，通过互补技能配置主动预防任务失败，同时避免资源浪费。

Method: 使用概率分布建模任务需求，采用基于市场的方法优化联合团队目标，显式捕获机器人间的耦合奖励，提供多项式时间解。

Result: 与基准算法比较的实验证明了该方法的有效性，同时凸显了在分散式环境中纳入耦合奖励的挑战。

Conclusion: 该方法在分散式设置中实现了高效的任务分配，能够处理任务不确定性并优化团队性能。

Abstract: This paper proposes a task allocation algorithm for teams of heterogeneous
robots in environments with uncertain task requirements. We model these
requirements as probability distributions over capabilities and use this model
to allocate tasks such that robots with complementary skills naturally position
near uncertain tasks, proactively mitigating task failures without wasting
resources. We introduce a market-based approach that optimizes the joint team
objective while explicitly capturing coupled rewards between robots, offering a
polynomial-time solution in decentralized settings with strict communication
assumptions. Comparative experiments against benchmark algorithms demonstrate
the effectiveness of our approach and highlight the challenges of incorporating
coupled rewards in a decentralized formulation.

</details>


### [39] [Ontological foundations for contrastive explanatory narration of robot plans](https://arxiv.org/abs/2509.22493)
*Alberto Olivares-Alarcos,Sergi Foix,Júlia Borràs,Gerard Canal,Guillem Alenyà*

Main category: cs.RO

TL;DR: 提出了一种新的本体论模型来形式化和推理竞争计划之间的差异，并开发了一种算法来构建对比性解释叙事，使机器人能够解释其决策。


<details>
  <summary>Details</summary>
Motivation: 确保人机交互的可信度和成功需要人工智能代理能够理解并解释其决策，特别是当需要向人类解释为何选择某个计划而非其他竞争计划时。

Method: 首先提出了一种新颖的本体论模型来形式化和推理竞争计划之间的差异，然后开发了一种基于发散知识的算法来构建对比性解释叙事。

Result: 通过实证评估，该方法在生成解释方面优于基线方法，能够更有效地构建对比性叙事。

Conclusion: 该方法能够帮助机器人更好地解释其决策过程，提高人机交互的透明度和可信度。

Abstract: Mutual understanding of artificial agents' decisions is key to ensuring a
trustworthy and successful human-robot interaction. Hence, robots are expected
to make reasonable decisions and communicate them to humans when needed. In
this article, the focus is on an approach to modeling and reasoning about the
comparison of two competing plans, so that robots can later explain the
divergent result. First, a novel ontological model is proposed to formalize and
reason about the differences between competing plans, enabling the
classification of the most appropriate one (e.g., the shortest, the safest, the
closest to human preferences, etc.). This work also investigates the
limitations of a baseline algorithm for ontology-based explanatory narration.
To address these limitations, a novel algorithm is presented, leveraging
divergent knowledge between plans and facilitating the construction of
contrastive narratives. Through empirical evaluation, it is observed that the
explanations excel beyond the baseline method.

</details>


### [40] [HELIOS: Hierarchical Exploration for Language-grounded Interaction in Open Scenes](https://arxiv.org/abs/2509.22498)
*Katrina Ashton,Chahyon Ku,Shrey Shah,Wen Jiang,Kostas Daniilidis,Bernadette Bucher*

Main category: cs.RO

TL;DR: HELIOS是一个用于语言指定移动操作任务的分层场景表示方法，在部分观察的新环境中通过平衡探索和利用来高效搜索目标物体，在OVMM基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决在部分观察的新环境中执行语言指定移动操作任务时面临的挑战：场景部分观察、语言指令到部分观察场景的语义信息落地、以及主动更新场景知识。

Method: 构建包含相关语义和占据信息的2D地图，同时主动构建任务相关对象的3D高斯表示，融合多层表示中的观察结果，并明确建模每个对象检测的多视角一致性。

Result: 在Habitat模拟器的OVMM基准测试中取得最先进结果，该方法为零样本方法，无需额外数据即可迁移到真实世界，已在Spot机器人的真实办公环境中验证。

Conclusion: HELIOS通过分层场景表示和平衡探索与利用的搜索目标，有效解决了语言指定移动操作任务中的感知挑战，并在模拟和真实环境中都表现出色。

Abstract: Language-specified mobile manipulation tasks in novel environments
simultaneously face challenges interacting with a scene which is only partially
observed, grounding semantic information from language instructions to the
partially observed scene, and actively updating knowledge of the scene with new
observations. To address these challenges, we propose HELIOS, a hierarchical
scene representation and associated search objective to perform language
specified pick and place mobile manipulation tasks. We construct 2D maps
containing the relevant semantic and occupancy information for navigation while
simultaneously actively constructing 3D Gaussian representations of
task-relevant objects. We fuse observations across this multi-layered
representation while explicitly modeling the multi-view consistency of the
detections of each object. In order to efficiently search for the target
object, we formulate an objective function balancing exploration of unobserved
or uncertain regions with exploitation of scene semantic information. We
evaluate HELIOS on the OVMM benchmark in the Habitat simulator, a pick and
place benchmark in which perception is challenging due to large and complex
scenes with comparatively small target objects. HELIOS achieves
state-of-the-art results on OVMM. As our approach is zero-shot, HELIOS can also
transfer to the real world without requiring additional data, as we illustrate
by demonstrating it in a real world office environment on a Spot robot.

</details>


### [41] [An Intention-driven Lane Change Framework Considering Heterogeneous Dynamic Cooperation in Mixed-traffic Environment](https://arxiv.org/abs/2509.22550)
*Xiaoyun Qiu,Haichao Liu,Yue Pan,Jun Ma,Xinhu Zheng*

Main category: cs.RO

TL;DR: 提出了一种意图驱动的车道变换框架，通过驾驶风格识别、合作感知决策和协调运动规划，解决混合交通环境中AV与HV交互的挑战。


<details>
  <summary>Details</summary>
Motivation: 在混合交通环境中，自动驾驶车辆与多样化的人类驾驶车辆交互时，不可预测的意图和异质行为使得安全高效的车道变换变得极具挑战性。现有方法往往通过假设统一模式来过度简化这些交互。

Method: 使用在NGSIM数据集上训练的深度学习分类器实时识别人类驾驶风格；提出包含内在和交互组件的合作分数来估计周围驾驶员的意图并量化其合作意愿；结合行为克隆和逆强化学习进行决策；将模型预测控制与IRL意图推断集成用于轨迹生成。

Result: 实验显示该模型达到94.2%准确率和94.3% F1分数，在车道变换识别上比基于规则和基于学习的基线方法高出4-15%。

Conclusion: 结果表明建模驾驶员异质性的好处，并证明了该框架在复杂交通环境中推进情境感知和类人自动驾驶的潜力。

Abstract: In mixed-traffic environments, where autonomous vehicles (AVs) interact with
diverse human-driven vehicles (HVs), unpredictable intentions and heterogeneous
behaviors make safe and efficient lane change maneuvers highly challenging.
Existing methods often oversimplify these interactions by assuming uniform
patterns. We propose an intention-driven lane change framework that integrates
driving-style recognition, cooperation-aware decision-making, and coordinated
motion planning. A deep learning classifier trained on the NGSIM dataset
identifies human driving styles in real time. A cooperation score with
intrinsic and interactive components estimates surrounding drivers' intentions
and quantifies their willingness to cooperate with the ego vehicle.
Decision-making combines behavior cloning with inverse reinforcement learning
to determine whether a lane change should be initiated. For trajectory
generation, model predictive control is integrated with IRL-based intention
inference to produce collision-free and socially compliant maneuvers.
Experiments show that the proposed model achieves 94.2\% accuracy and 94.3\%
F1-score, outperforming rule-based and learning-based baselines by 4-15\% in
lane change recognition. These results highlight the benefit of modeling
inter-driver heterogeneity and demonstrate the potential of the framework to
advance context-aware and human-like autonomous driving in complex traffic
environments.

</details>


### [42] [MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data](https://arxiv.org/abs/2509.22573)
*Farida Mohsen,Ali Safa*

Main category: cs.RO

TL;DR: 提出了一种仅使用RGB输入的帧级精度人机交互意图预测方法，通过合成序列生成和新的损失函数解决了数据不平衡问题，在性能上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人机交互意图检测方法大多依赖多模态输入（如RGB-D），需要较长的时序窗口，响应速度较慢。为了提高响应速度和服务质量，需要开发仅使用RGB输入且能实现帧级精度预测的方法。

Method: 提出了MINT-RVAE合成序列生成方法，结合新的损失函数和训练策略，仅使用RGB输入实现帧级精度的人机交互意图预测，解决了真实数据集中常见的类别不平衡问题。

Result: 该方法在AUROC指标上达到0.95，显著优于先前工作的0.90-0.912，同时仅需RGB输入并支持精确的帧级意图起始点预测。

Conclusion: 提出的RGB-only方法在人机交互意图预测任务中实现了最先进的性能，为解决数据不平衡问题提供了有效方案，并公开了新的帧级标注数据集以支持未来研究。

Abstract: Efficiently detecting human intent to interact with ubiquitous robots is
crucial for effective human-robot interaction (HRI) and collaboration. Over the
past decade, deep learning has gained traction in this field, with most
existing approaches relying on multimodal inputs, such as RGB combined with
depth (RGB-D), to classify time-sequence windows of sensory data as interactive
or non-interactive. In contrast, we propose a novel RGB-only pipeline for
predicting human interaction intent with frame-level precision, enabling faster
robot responses and improved service quality. A key challenge in intent
prediction is the class imbalance inherent in real-world HRI datasets, which
can hinder the model's training and generalization. To address this, we
introduce MINT-RVAE, a synthetic sequence generation method, along with new
loss functions and training strategies that enhance generalization on
out-of-sample data. Our approach achieves state-of-the-art performance (AUROC:
0.95) outperforming prior works (AUROC: 0.90-0.912), while requiring only RGB
input and supporting precise frame onset prediction. Finally, to support future
research, we openly release our new dataset with frame-level labeling of human
interaction intent.

</details>


### [43] [EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation](https://arxiv.org/abs/2509.22578)
*Yuan Xu,Jiabing Yang,Xiaofeng Wang,Yixiang Chen,Zheng Zhu,Bowen Fang,Guan Huang,Xinze Chen,Yun Ye,Qiang Zhang,Peiyan Li,Xiangnan Wu,Kai Wang,Bing Zhan,Shuo Lu,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang*

Main category: cs.RO

TL;DR: EgoDemoGen框架通过生成配对的新视角演示来解决模仿学习在机器人操作中的视角偏移问题，显著提升了策略在不同自我中心视角下的性能。


<details>
  <summary>Details</summary>
Motivation: 模仿学习策略在机器人操作中表现良好，但当训练视角单一且遇到自我中心视角偏移时性能会下降，需要解决视角鲁棒性问题。

Method: 提出EgoDemoGen框架，通过EgoViewTransfer生成模型在新视角下重定向动作并合成对应的观察视频，使用自监督双重重投影策略微调预训练视频生成模型。

Result: 在仿真中，标准视角性能提升17.0%，新视角提升17.7%；在真实机器人上分别提升18.3%和25.8%。性能随生成演示比例增加而持续提升。

Conclusion: EgoDemoGen为自我中心视角鲁棒的机器人操作提供了一条实用路径，能有效应对视角偏移问题。

Abstract: Imitation learning based policies perform well in robotic manipulation, but
they often degrade under *egocentric viewpoint shifts* when trained from a
single egocentric viewpoint. To address this issue, we present **EgoDemoGen**,
a framework that generates *paired* novel egocentric demonstrations by
retargeting actions in the novel egocentric frame and synthesizing the
corresponding egocentric observation videos with proposed generative video
repair model **EgoViewTransfer**, which is conditioned by a novel-viewpoint
reprojected scene video and a robot-only video rendered from the retargeted
joint actions. EgoViewTransfer is finetuned from a pretrained video generation
model using self-supervised double reprojection strategy. We evaluate
EgoDemoGen on both simulation (RoboTwin2.0) and real-world robot. After
training with a mixture of EgoDemoGen-generated novel egocentric demonstrations
and original standard egocentric demonstrations, policy success rate improves
**absolutely** by **+17.0%** for standard egocentric viewpoint and by
**+17.7%** for novel egocentric viewpoints in simulation. On real-world robot,
the **absolute** improvements are **+18.3%** and **+25.8%**. Moreover,
performance continues to improve as the proportion of EgoDemoGen-generated
demonstrations increases, with diminishing returns. These results demonstrate
that EgoDemoGen provides a practical route to egocentric viewpoint-robust
robotic manipulation.

</details>


### [44] [WoW: Towards a World omniscient World model Through Embodied Interaction](https://arxiv.org/abs/2509.22642)
*Xiaowei Chi,Peidong Jia,Chun-Kai Fan,Xiaozhu Ju,Weishi Mi,Kevin Zhang,Zhiyuan Qin,Wanxin Tian,Kuangzhi Ge,Hao Li,Zezhong Qian,Anthony Chen,Qiang Zhou,Yueru Jia,Jiaming Liu,Yong Dai,Qingpo Wuwu,Chengyu Bai,Yu-Kai Wang,Ying Li,Lizhang Chen,Yong Bao,Zhiyuan Jiang,Jiacheng Zhu,Kai Tang,Ruichuan An,Yulin Luo,Qiuxuan Feng,Siyuan Zhou,Chi-min Chan,Chengkai Hou,Wei Xue,Sirui Han,Yike Guo,Shanghang Zhang,Jian Tang*

Main category: cs.RO

TL;DR: WoW是一个140亿参数的世界模型，通过200万机器人交互轨迹训练，证明大规模真实世界交互对AI物理直觉发展至关重要。该模型能生成物理一致的视频，并通过SOPHIA系统约束物理真实性。


<details>
  <summary>Details</summary>
Motivation: 当前视频模型如Sora依赖被动观察，难以理解物理因果关系。作者假设真实的物理直觉必须基于与真实世界的大量因果丰富交互。

Method: 提出WoW模型，使用200万机器人交互轨迹训练。通过SOPHIA系统（视觉语言模型代理）评估DiT生成输出，并通过迭代演化语言指令来指导精炼。同时训练逆动力学模型将精炼计划转换为可执行机器人动作。

Result: WoW在WoWBench基准测试中取得最先进性能，在物理因果关系、碰撞动力学和物体持久性方面表现出色。模型理解物理的方式是可能结果的概率分布，会出现随机不稳定性和物理幻觉。

Conclusion: 大规模真实世界交互是发展AI物理直觉的基石。通过主动交互训练的世界模型在物理一致性方面优于被动观察模型。

Abstract: Humans develop an understanding of intuitive physics through active
interaction with the world. This approach is in stark contrast to current video
models, such as Sora, which rely on passive observation and therefore struggle
with grasping physical causality. This observation leads to our central
hypothesis: authentic physical intuition of the world model must be grounded in
extensive, causally rich interactions with the real world. To test this
hypothesis, we present WoW, a 14-billion-parameter generative world model
trained on 2 million robot interaction trajectories. Our findings reveal that
the model's understanding of physics is a probabilistic distribution of
plausible outcomes, leading to stochastic instabilities and physical
hallucinations. Furthermore, we demonstrate that this emergent capability can
be actively constrained toward physical realism by SOPHIA, where
vision-language model agents evaluate the DiT-generated output and guide its
refinement by iteratively evolving the language instructions. In addition, a
co-trained Inverse Dynamics Model translates these refined plans into
executable robotic actions, thus closing the imagination-to-action loop. We
establish WoWBench, a new benchmark focused on physical consistency and causal
reasoning in video, where WoW achieves state-of-the-art performance in both
human and autonomous evaluation, demonstrating strong ability in physical
causality, collision dynamics, and object permanence. Our work provides
systematic evidence that large-scale, real-world interaction is a cornerstone
for developing physical intuition in AI. Models, data, and benchmarks will be
open-sourced.

</details>


### [45] [VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search](https://arxiv.org/abs/2509.22643)
*Wenkai Guo,Guanxing Lu,Haoyuan Deng,Zhenyu Wu,Yansong Tang,Ziwei Wang*

Main category: cs.RO

TL;DR: VLA-Reasoner是一个插件框架，通过测试时扩展为现成的VLA模型提供预见未来状态的能力，使用世界模型和蒙特卡洛树搜索来优化长时程任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型(VLAs)只能预测短视的下一步动作，在长时程轨迹任务中因增量偏差而表现不佳。

Method: 提出VLA-Reasoner框架：1) 通过世界模型采样和推演动作轨迹来预见未来状态；2) 使用蒙特卡洛树搜索提高大动作空间中的搜索效率；3) 引入基于核密度估计的置信度采样机制；4) 采用离线奖励塑形策略评估中间状态。

Result: 在模拟器和真实世界中的广泛实验表明，VLA-Reasoner相比最先进的VLA模型取得了显著改进。

Conclusion: 该方法为机器人操作的可扩展测试时计算提供了一条潜在途径。

Abstract: Vision-Language-Action models (VLAs) achieve strong performance in general
robotic manipulation tasks by scaling imitation learning. However, existing
VLAs are limited to predicting short-sighted next-action, which struggle with
long-horizon trajectory tasks due to incremental deviations. To address this
problem, we propose a plug-in framework named VLA-Reasoner that effectively
empowers off-the-shelf VLAs with the capability of foreseeing future states via
test-time scaling. Specifically, VLA-Reasoner samples and rolls out possible
action trajectories where involved actions are rationales to generate future
states via a world model, which enables VLA-Reasoner to foresee and reason
potential outcomes and search for the optimal actions. We further leverage
Monte Carlo Tree Search (MCTS) to improve search efficiency in large action
spaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a
confidence sampling mechanism based on Kernel Density Estimation (KDE), to
enable efficient exploration in MCTS without redundant VLA queries. We evaluate
intermediate states in MCTS via an offline reward shaping strategy, to score
predicted futures and correct deviations with long-term feedback. We conducted
extensive experiments in both simulators and the real world, demonstrating that
our proposed VLA-Reasoner achieves significant improvements over the
state-of-the-art VLAs. Our method highlights a potential pathway toward
scalable test-time computation of robotic manipulation.

</details>


### [46] [Pixel Motion Diffusion is What We Need for Robot Control](https://arxiv.org/abs/2509.22652)
*E-Ro Nguyen,Yichi Zhang,Kanchana Ranasinghe,Xiang Li,Michael S. Ryoo*

Main category: cs.RO

TL;DR: DAWN是一个基于扩散模型的统一机器人控制框架，通过结构化像素运动表示连接高级运动意图和低级机器人动作，在CALVIN和MetaWorld基准测试中取得最先进结果，并展示了可靠的现实世界迁移能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器人控制中高级运动意图与低级动作之间的连接问题，需要一个能够产生可解释中间运动抽象的统一框架。

Method: 将高级和低级控制器都建模为扩散过程，使用结构化像素运动表示来桥接语言条件和机器人动作，构建完全可训练的端到端系统。

Result: 在CALVIN基准测试中取得最先进结果，表现出强大的多任务性能，在MetaWorld上验证了有效性，并展示了仅需少量微调即可实现可靠的现实世界迁移。

Conclusion: 扩散建模与运动中心表示的结合为可扩展和鲁棒的机器人学习提供了强有力的基准，展示了基于扩散的运动抽象在机器人控制中的实际可行性。

Abstract: We present DAWN (Diffusion is All We Need for robot control), a unified
diffusion-based framework for language-conditioned robotic manipulation that
bridges high-level motion intent and low-level robot action via structured
pixel motion representation. In DAWN, both the high-level and low-level
controllers are modeled as diffusion processes, yielding a fully trainable,
end-to-end system with interpretable intermediate motion abstractions. DAWN
achieves state-of-the-art results on the challenging CALVIN benchmark,
demonstrating strong multi-task performance, and further validates its
effectiveness on MetaWorld. Despite the substantial domain gap between
simulation and reality and limited real-world data, we demonstrate reliable
real-world transfer with only minimal finetuning, illustrating the practical
viability of diffusion-based motion abstractions for robotic control. Our
results show the effectiveness of combining diffusion modeling with
motion-centric representations as a strong baseline for scalable and robust
robot learning. Project page: https://nero1342.github.io/DAWN/

</details>


### [47] [See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation](https://arxiv.org/abs/2509.22653)
*Chih Yao Hu,Yang-Sen Lin,Yuna Lee,Chih-Hai Su,Jie-Ying Lee,Shr-Ruei Tsai,Chin-Yang Lin,Kuan-Wen Chen,Tsung-Wei Ke,Yu-Lun Liu*

Main category: cs.RO

TL;DR: SPF是一个无需训练的空中视觉语言导航框架，将动作预测视为2D空间定位任务，通过分解语言指令为2D航点，再转换为3D位移向量控制无人机，在动态环境中实现闭环导航。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的方法将动作预测视为文本生成任务，而SPF的关键洞察是将AVLN的动作预测视为2D空间定位任务，以处理任意类型自由形式指令和任意环境。

Method: 利用VLM将模糊语言指令分解为输入图像上的迭代2D航点标注，结合预测的行驶距离，将2D航点转换为3D位移向量作为无人机动作命令，并自适应调整行驶距离以提高导航效率。

Result: 在DRL仿真基准测试中达到新SOTA，比之前最佳方法绝对提升63%；在广泛真实世界评估中大幅优于强基线；展示了对不同VLM的显著泛化能力。

Conclusion: SPF通过将动作预测重新定义为2D空间定位任务，实现了无需训练的空中视觉语言导航，在仿真和真实环境中均表现出色，具有强大的泛化能力。

Abstract: We present See, Point, Fly (SPF), a training-free aerial vision-and-language
navigation (AVLN) framework built atop vision-language models (VLMs). SPF is
capable of navigating to any goal based on any type of free-form instructions
in any kind of environment. In contrast to existing VLM-based approaches that
treat action prediction as a text generation task, our key insight is to
consider action prediction for AVLN as a 2D spatial grounding task. SPF
harnesses VLMs to decompose vague language instructions into iterative
annotation of 2D waypoints on the input image. Along with the predicted
traveling distance, SPF transforms predicted 2D waypoints into 3D displacement
vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the
traveling distance to facilitate more efficient navigation. Notably, SPF
performs navigation in a closed-loop control manner, enabling UAVs to follow
dynamic targets in dynamic environments. SPF sets a new state of the art in DRL
simulation benchmark, outperforming the previous best method by an absolute
margin of 63%. In extensive real-world evaluations, SPF outperforms strong
baselines by a large margin. We also conduct comprehensive ablation studies to
highlight the effectiveness of our design choice. Lastly, SPF shows remarkable
generalization to different VLMs. Project page: https://spf-web.pages.dev

</details>
