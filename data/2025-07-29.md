<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 42]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Extending Group Relative Policy Optimization to Continuous Control: A Theoretical Framework for Robotic Reinforcement Learning](https://arxiv.org/abs/2507.19555)
*Rajat Khanda,Mohammad Baqar,Sambuddha Chakrabarti,Satyasaran Changdar*

Main category: cs.RO

TL;DR: GRPO扩展到连续控制环境，提出轨迹聚类、状态感知优势估计和正则化策略更新，适用于机器人任务。


<details>
  <summary>Details</summary>
Motivation: GRPO在离散动作空间中表现良好，但在连续控制中尚未探索，限制了其在机器人领域的应用。

Method: 提出轨迹聚类、状态感知优势估计和正则化策略更新，适用于高维动作空间和稀疏奖励。

Result: 提供了收敛性和计算复杂度的理论分析。

Conclusion: 为机器人系统的实证验证奠定了基础，适用于运动和操作任务。

Abstract: Group Relative Policy Optimization (GRPO) has shown promise in discrete
action spaces by eliminating value function dependencies through group-based
advantage estimation. However, its application to continuous control remains
unexplored, limiting its utility in robotics where continuous actions are
essential. This paper presents a theoretical framework extending GRPO to
continuous control environments, addressing challenges in high-dimensional
action spaces, sparse rewards, and temporal dynamics. Our approach introduces
trajectory-based policy clustering, state-aware advantage estimation, and
regularized policy updates designed for robotic applications. We provide
theoretical analysis of convergence properties and computational complexity,
establishing a foundation for future empirical validation in robotic systems
including locomotion and manipulation tasks.

</details>


### [2] [Reward-Augmented Reinforcement Learning for Continuous Control in Precision Autonomous Parking via Policy Optimization Methods](https://arxiv.org/abs/2507.19642)
*Ahmad Suleman,Misha Urooj Khan,Zeeshan Kaleem,Ali H. Alenezi,Iqra Shabbir Sinem Coleri,Chau Yuen*

Main category: cs.RO

TL;DR: 论文提出了一种奖励增强学习框架（RARLAP）用于解决自主停车（AP）中的复杂性问题，通过结构化奖励设计提升策略的适应性和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则和模型预测的方法在处理AP的非线性和环境依赖性时缺乏适应性和泛化能力。

Method: 提出RARLAP框架，利用结构化奖励设计（GOR、DPR、MAR）在高保真Unity仿真环境中训练策略。

Result: 实验表明，MAR策略的成功率达91%，轨迹更平滑且行为更鲁棒，而GOR和DPR效果不佳。

Conclusion: 奖励增强学习能有效解决AP的复杂性问题，提升策略优化效率和安全性。

Abstract: Autonomous parking (AP) represents a critical yet complex subset of
intelligent vehicle automation, characterized by tight spatial constraints,
frequent close-range obstacle interactions, and stringent safety margins.
However, conventional rule-based and model-predictive methods often lack the
adaptability and generalization needed to handle the nonlinear and
environment-dependent complexities of AP. To address these limitations, we
propose a reward-augmented learning framework for AP (RARLAP), that mitigates
the inherent complexities of continuous-domain control by leveraging structured
reward design to induce smooth and adaptable policy behavior, trained entirely
within a high-fidelity Unity-based custom 3D simulation environment. We
systematically design and assess three structured reward strategies: goal-only
reward (GOR), dense proximity reward (DPR), and milestone-augmented reward
(MAR), each integrated with both on-policy and off-policy optimization
paradigms. Empirical evaluations demonstrate that the on-policy MAR achieves a
91\% success rate, yielding smoother trajectories and more robust behavior,
while GOR and DPR fail to guide effective learning. Convergence and trajectory
analyses demonstrate that the proposed framework enhances policy adaptability,
accelerates training, and improves safety in continuous control. Overall,
RARLAP establishes that reward augmentation effectively addresses complex
autonomous parking challenges, enabling scalable and efficient policy
optimization with both on- and off-policy methods. To support reproducibility,
the code accompanying this paper is publicly available.

</details>


### [3] [GABRIL: Gaze-Based Regularization for Mitigating Causal Confusion in Imitation Learning](https://arxiv.org/abs/2507.19647)
*Amin Banayeeanzade,Fatemeh Bahrani,Yutai Zhou,Erdem Bıyık*

Main category: cs.RO

TL;DR: GABRIL利用人类注视数据改进模仿学习，通过正则化损失减少因果混淆，提升性能。


<details>
  <summary>Details</summary>
Motivation: 模仿学习常因因果混淆导致性能下降，需解决这一问题。

Method: 引入GABRIL，利用人类注视数据指导表示学习，通过正则化损失聚焦因果相关特征。

Result: 在Atari和CARLA中，GABRIL性能分别提升179%和76%，并提供额外可解释性。

Conclusion: GABRIL有效减少因果混淆，提升模仿学习性能与可解释性。

Abstract: Imitation Learning (IL) is a widely adopted approach which enables agents to
learn from human expert demonstrations by framing the task as a supervised
learning problem. However, IL often suffers from causal confusion, where agents
misinterpret spurious correlations as causal relationships, leading to poor
performance in testing environments with distribution shift. To address this
issue, we introduce GAze-Based Regularization in Imitation Learning (GABRIL), a
novel method that leverages the human gaze data gathered during the data
collection phase to guide the representation learning in IL. GABRIL utilizes a
regularization loss which encourages the model to focus on causally relevant
features identified through expert gaze and consequently mitigates the effects
of confounding variables. We validate our approach in Atari environments and
the Bench2Drive benchmark in CARLA by collecting human gaze datasets and
applying our method in both domains. Experimental results show that the
improvement of GABRIL over behavior cloning is around 179% more than the same
number for other baselines in the Atari and 76% in the CARLA setup. Finally, we
show that our method provides extra explainability when compared to regular IL
agents.

</details>


### [4] [RAKOMO: Reachability-Aware K-Order Markov Path Optimization for Quadrupedal Loco-Manipulation](https://arxiv.org/abs/2507.19652)
*Mattia Risiglione,Abdelrahman Abdalla,Victor Barasuol,Kim Tien Ly,Ioannis Havoutis,Claudio Semini*

Main category: cs.RO

TL;DR: 提出RAKOMO方法，结合KOMO和可达性边界，优化腿式机械臂的运动规划。


<details>
  <summary>Details</summary>
Motivation: 解决腿式机械臂运动规划中因接触不连续性和忽略腿部限制导致的挑战。

Method: 整合K-Order Markov Optimization与基于可达性边界的运动规划，利用神经网络预测边界并优化。

Result: RAKOMO在仿真中成功执行拾取任务，优于基线KOMO方法。

Conclusion: RAKOMO有效适应腿式机械臂的运动规划，提升任务执行效率。

Abstract: Legged manipulators, such as quadrupeds equipped with robotic arms, require
motion planning techniques that account for their complex kinematic constraints
in order to perform manipulation tasks both safely and effectively. However,
trajectory optimization methods often face challenges due to the hybrid
dynamics introduced by contact discontinuities, and tend to neglect leg
limitations during planning for computational reasons. In this work, we propose
RAKOMO, a path optimization technique that integrates the strengths of K-Order
Markov Optimization (KOMO) with a kinematically-aware criterion based on the
reachable region defined as reachability margin. We leverage a neural-network
to predict the margin and optimize it by incorporating it in the standard KOMO
formulation. This approach enables rapid convergence of gradient-based motion
planning -- commonly tailored for continuous systems -- while adapting it
effectively to legged manipulators, successfully executing loco-manipulation
tasks. We benchmark RAKOMO against a baseline KOMO approach through a set of
simulations for pick-and-place tasks with the HyQReal quadruped robot equipped
with a Kinova Gen3 robotic arm.

</details>


### [5] [PhysVarMix: Physics-Informed Variational Mixture Model for Multi-Modal Trajectory Prediction](https://arxiv.org/abs/2507.19701)
*Haichuan Li,Tomi Westerlund*

Main category: cs.RO

TL;DR: 提出了一种结合学习与物理约束的混合方法，用于多模态轨迹预测，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决复杂城市环境中多模态轨迹预测的挑战，确保预测既数据一致又物理合理。

Method: 使用变分贝叶斯混合模型捕捉多模态行为，结合物理边界条件和MPC平滑。

Result: 在基准数据集上表现优于现有方法，生成多样且可解释的轨迹。

Conclusion: 通过平衡数据驱动与物理约束，提供了一种鲁棒且可扩展的解决方案。

Abstract: Accurate prediction of future agent trajectories is a critical challenge for
ensuring safe and efficient autonomous navigation, particularly in complex
urban environments characterized by multiple plausible future scenarios. In
this paper, we present a novel hybrid approach that integrates learning-based
with physics-based constraints to address the multi-modality inherent in
trajectory prediction. Our method employs a variational Bayesian mixture model
to effectively capture the diverse range of potential future behaviors, moving
beyond traditional unimodal assumptions. Unlike prior approaches that
predominantly treat trajectory prediction as a data-driven regression task, our
framework incorporates physical realism through sector-specific boundary
conditions and Model Predictive Control (MPC)-based smoothing. These
constraints ensure that predicted trajectories are not only data-consistent but
also physically plausible, adhering to kinematic and dynamic principles.
Furthermore, our method produces interpretable and diverse trajectory
predictions, enabling enhanced downstream decision-making and planning in
autonomous driving systems. We evaluate our approach on two benchmark datasets,
demonstrating superior performance compared to existing methods. Comprehensive
ablation studies validate the contributions of each component and highlight
their synergistic impact on prediction accuracy and reliability. By balancing
data-driven insights with physics-informed constraints, our approach offers a
robust and scalable solution for navigating the uncertainties of real-world
urban environments.

</details>


### [6] [DOA: A Degeneracy Optimization Agent with Adaptive Pose Compensation Capability based on Deep Reinforcement Learning](https://arxiv.org/abs/2507.19742)
*Yanbin Li,Canran Xiao,Hongyang He,Shenghai Yuan,Zong Ke,Jiajie Yu,Zixiong Qin,Zhiguo Zhang,Wenzheng Chi,Wei Zhang*

Main category: cs.RO

TL;DR: 使用PPO训练自适应退化优化代理（DOA）解决SLAM中的退化问题，提出系统方法应对数据获取、样本质量和标注协议挑战，并通过实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 室内环境中长直走廊等场景会导致SLAM严重的退化问题，传统方法难以应对。

Method: 提出基于PPO的DOA，设计奖励函数和转移学习模块，动态调整传感器贡献。

Result: 实验证明DOA在退化检测和优化方面优于现有方法。

Conclusion: DOA能有效解决SLAM退化问题，具有跨环境泛化能力。

Abstract: Particle filter-based 2D-SLAM is widely used in indoor localization tasks due
to its efficiency. However, indoor environments such as long straight corridors
can cause severe degeneracy problems in SLAM. In this paper, we use Proximal
Policy Optimization (PPO) to train an adaptive degeneracy optimization agent
(DOA) to address degeneracy problem. We propose a systematic methodology to
address three critical challenges in traditional supervised learning
frameworks: (1) data acquisition bottlenecks in degenerate dataset, (2)
inherent quality deterioration of training samples, and (3) ambiguity in
annotation protocol design. We design a specialized reward function to guide
the agent in developing perception capabilities for degenerate environments.
Using the output degeneracy factor as a reference weight, the agent can
dynamically adjust the contribution of different sensors to pose optimization.
Specifically, the observation distribution is shifted towards the motion model
distribution, with the step size determined by a linear interpolation formula
related to the degeneracy factor. In addition, we employ a transfer learning
module to endow the agent with generalization capabilities across different
environments and address the inefficiency of training in degenerate
environments. Finally, we conduct ablation studies to demonstrate the
rationality of our model design and the role of transfer learning. We also
compare the proposed DOA with SOTA methods to prove its superior degeneracy
detection and optimization capabilities across various environments.

</details>


### [7] [Skin-Machine Interface with Multimodal Contact Motion Classifier](https://arxiv.org/abs/2507.19760)
*Alberto Confente,Takanori Jin,Taisuke Kobayashi,Julio Rogelio Guadarrama-Olvera,Gordon Cheng*

Main category: cs.RO

TL;DR: 提出了一种利用皮肤传感器作为复杂机器人操作界面的新框架，通过多模态触觉信息分类生成多样化机器人动作。


<details>
  <summary>Details</summary>
Motivation: 探索皮肤传感器作为新型机器人操作界面的潜力，利用多模态触觉信息实现更自然的交互。

Method: 采用基于循环神经网络的接触动作分类器，结合多模态传感器数据和柔性支撑设计。

Result: 分类器准确率超过95%，成功实现双臂移动机械臂的多样化任务执行。

Conclusion: 多模态传感和柔性支撑设计是提升分类性能和系统稳定性的关键，皮肤传感器界面具有广泛应用前景。

Abstract: This paper proposes a novel framework for utilizing skin sensors as a new
operation interface of complex robots. The skin sensors employed in this study
possess the capability to quantify multimodal tactile information at multiple
contact points. The time-series data generated from these sensors is
anticipated to facilitate the classification of diverse contact motions
exhibited by an operator. By mapping the classification results with robot
motion primitives, a diverse range of robot motions can be generated by
altering the manner in which the skin sensors are interacted with. In this
paper, we focus on a learning-based contact motion classifier employing
recurrent neural networks. This classifier is a pivotal factor in the success
of this framework. Furthermore, we elucidate the requisite conditions for
software-hardware designs. Firstly, multimodal sensing and its comprehensive
encoding significantly contribute to the enhancement of classification accuracy
and learning stability. Utilizing all modalities simultaneously as inputs to
the classifier proves to be an effective approach. Secondly, it is essential to
mount the skin sensors on a flexible and compliant support to enable the
activation of three-axis accelerometers. These accelerometers are capable of
measuring horizontal tactile information, thereby enhancing the correlation
with other modalities. Furthermore, they serve to absorb the noises generated
by the robot's movements during deployment. Through these discoveries, the
accuracy of the developed classifier surpassed 95 %, enabling the dual-arm
mobile manipulator to execute a diverse range of tasks via the Skin-Machine
Interface. https://youtu.be/UjUXT4Z4BC8

</details>


### [8] [Ag2x2: Robust Agent-Agnostic Visual Representations for Zero-Shot Bimanual Manipulation](https://arxiv.org/abs/2507.19817)
*Ziyin Xiong,Yinghan Chen,Puhao Li,Yixin Zhu,Tengyu Liu,Siyuan Huang*

Main category: cs.RO

TL;DR: Ag2x2是一个用于双手操作的计算框架，通过协调感知的视觉表示，在保持代理无关性的同时编码对象状态和手部运动模式，显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 双手操作因其协调控制的复杂性而具有挑战性，现有方法忽略了代理特定信息（如末端执行器位置），导致协调能力不足。

Method: 提出Ag2x2框架，结合协调感知的视觉表示，编码对象状态和手部运动模式，并在Bi-DexHands和PerAct2等任务中进行实验。

Result: 在13种双手任务中达到73.5%的成功率，优于基线方法，甚至超过专家设计奖励的策略。

Conclusion: Ag2x2为复杂双手机器人技能的可扩展学习提供了有效途径，无需人类演示或专家设计的奖励。

Abstract: Bimanual manipulation, fundamental to human daily activities, remains a
challenging task due to its inherent complexity of coordinated control. Recent
advances have enabled zero-shot learning of single-arm manipulation skills
through agent-agnostic visual representations derived from human videos;
however, these methods overlook crucial agent-specific information necessary
for bimanual coordination, such as end-effector positions. We propose Ag2x2, a
computational framework for bimanual manipulation through coordination-aware
visual representations that jointly encode object states and hand motion
patterns while maintaining agent-agnosticism. Extensive experiments demonstrate
that Ag2x2 achieves a 73.5% success rate across 13 diverse bimanual tasks from
Bi-DexHands and PerAct2, including challenging scenarios with deformable
objects like ropes. This performance outperforms baseline methods and even
surpasses the success rate of policies trained with expert-engineered rewards.
Furthermore, we show that representations learned through Ag2x2 can be
effectively leveraged for imitation learning, establishing a scalable pipeline
for skill acquisition without expert supervision. By maintaining robust
performance across diverse tasks without human demonstrations or engineered
rewards, Ag2x2 represents a step toward scalable learning of complex bimanual
robotic skills.

</details>


### [9] [A 4D Radar Camera Extrinsic Calibration Tool Based on 3D Uncertainty Perspective N Points](https://arxiv.org/abs/2507.19829)
*Chuan Cao,Xiaoning Wang,Wenqian Xi,Han Zhang,Weidong Chen,Jingchuan Wang*

Main category: cs.RO

TL;DR: 本文提出了一种针对4D成像雷达与相机系统的外在校准框架，通过3DUPnP算法建模雷达测量中的球坐标噪声传播，显著提升了校准性能。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达与相机系统的精确外在校准对机器人多模态感知至关重要，但由于传感器噪声和复杂误差传播，校准仍具挑战性。

Method: 采用空间3D不确定性感知PnP算法（3DUPnP），显式建模雷达测量中的球坐标噪声传播，并在坐标变换中补偿非零误差期望。

Result: 实验验证表明，该方法在仿真和物理实验中均优于现有CPnP基线，具有更高的一致性和精度。

Conclusion: 该研究为配备毫米波雷达和相机的机器人系统提供了一种鲁棒的校准方案，适用于自动驾驶和机器人感知应用。

Abstract: 4D imaging radar is a type of low-cost millimeter-wave radar(costing merely
10-20$\%$ of lidar systems) capable of providing range, azimuth, elevation, and
Doppler velocity information. Accurate extrinsic calibration between
millimeter-wave radar and camera systems is critical for robust multimodal
perception in robotics, yet remains challenging due to inherent sensor noise
characteristics and complex error propagation. This paper presents a systematic
calibration framework to address critical challenges through a spatial 3d
uncertainty-aware PnP algorithm (3DUPnP) that explicitly models spherical
coordinate noise propagation in radar measurements, then compensating for
non-zero error expectations during coordinate transformations. Finally,
experimental validation demonstrates significant performance improvements over
state-of-the-art CPnP baseline, including improved consistency in simulations
and enhanced precision in physical experiments. This study provides a robust
calibration solution for robotic systems equipped with millimeter-wave radar
and cameras, tailored specifically for autonomous driving and robotic
perception applications.

</details>


### [10] [Feeling the Force: A Nuanced Physics-based Traversability Sensor for Navigation in Unstructured Vegetation](https://arxiv.org/abs/2507.19831)
*Zaar Khizar,Johann Laconte,Roland Lenain,Romuald Aufrere*

Main category: cs.RO

TL;DR: 本文介绍了一种新型传感器，用于直接测量植被对机器人施加的力，为机器人导航提供量化指标。


<details>
  <summary>Details</summary>
Motivation: 机器人在自然环境中常遇到植被障碍，传统方法难以评估其安全性和可穿越性，需要更精确的力测量方法。

Method: 开发了一种新型传感器，直接捕捉植被对机器人的反作用力，并通过实验验证其有效性。

Result: 传感器能够精确测量力的细微变化，为导航决策提供量化依据。

Conclusion: 该传感器为机器人导航和未来学习算法的发展奠定了基础。

Abstract: In many applications, robots are increasingly deployed in unstructured and
natural environments where they encounter various types of vegetation.
Vegetation presents unique challenges as a traversable obstacle, where the
mechanical properties of the plants can influence whether a robot can safely
collide with and overcome the obstacle. A more nuanced approach is required to
assess the safety and traversability of these obstacles, as collisions can
sometimes be safe and necessary for navigating through dense or unavoidable
vegetation. This paper introduces a novel sensor designed to directly measure
the applied forces exerted by vegetation on a robot: by directly capturing the
push-back forces, our sensor provides a detailed understanding of the
interactions between the robot and its surroundings. We demonstrate the
sensor's effectiveness through experimental validations, showcasing its ability
to measure subtle force variations. This force-based approach provides a
quantifiable metric that can inform navigation decisions and serve as a
foundation for developing future learning algorithms.

</details>


### [11] [PlaneHEC: Efficient Hand-Eye Calibration for Multi-view Robotic Arm via Any Point Cloud Plane Detection](https://arxiv.org/abs/2507.19851)
*Ye Wang,Haodong Jing,Yang Liao,Yongqiang Ma,Nanning Zheng*

Main category: cs.RO

TL;DR: PlaneHEC是一种无需复杂模型、仅需深度相机的手眼标定方法，利用任意平面实现快速最优标定。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖精确几何模型或人工辅助，泛化性差且复杂低效。

Method: 基于平面约束设计标定方程，结合闭式解和迭代优化提高精度。

Result: 在仿真和真实环境中表现优异，优于其他点云标定方法。

Conclusion: PlaneHEC为多智能体系统和具身智能发展提供重要贡献。

Abstract: Hand-eye calibration is an important task in vision-guided robotic systems
and is crucial for determining the transformation matrix between the camera
coordinate system and the robot end-effector. Existing methods, for multi-view
robotic systems, usually rely on accurate geometric models or manual
assistance, generalize poorly, and can be very complicated and inefficient.
Therefore, in this study, we propose PlaneHEC, a generalized hand-eye
calibration method that does not require complex models and can be accomplished
using only depth cameras, which achieves the optimal and fastest calibration
results using arbitrary planar surfaces like walls and tables. PlaneHEC
introduces hand-eye calibration equations based on planar constraints, which
makes it strongly interpretable and generalizable. PlaneHEC also uses a
comprehensive solution that starts with a closed-form solution and improves it
withiterative optimization, which greatly improves accuracy. We comprehensively
evaluated the performance of PlaneHEC in both simulated and real-world
environments and compared the results with other point-cloud-based calibration
methods, proving its superiority. Our approach achieves universal and fast
calibration with an innovative design of computational models, providing a
strong contribution to the development of multi-agent systems and embodied
intelligence.

</details>


### [12] [Think, Act, Learn: A Framework for Autonomous Robotic Agents using Closed-Loop Large Language Models](https://arxiv.org/abs/2507.19854)
*Anjali R. Menon,Rohit K. Sharma,Priya Singh,Chengyu Wang,Aurora M. Ferreira,Mateja Novak*

Main category: cs.RO

TL;DR: 论文提出了一种名为“Think, Act, Learn”（T-A-L）的闭环框架，通过LLM驱动的自主学习和策略优化，显著提升了机器人在动态环境中的适应性和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的机器人系统多为开环规划，无法适应动态环境中的突发情况，导致性能脆弱。

Method: T-A-L框架通过“思考-行动-学习”闭环循环，结合LLM的任务分解、执行反馈和自我反思，实现策略优化。

Result: 实验表明，T-A-L在复杂任务中成功率超过97%，仅需9次试验即可稳定收敛，并展现出对新任务的强泛化能力。

Conclusion: T-A-L框架为开发更鲁棒、自适应和真正自主的机器人系统提供了重要进展。

Abstract: The integration of Large Language Models (LLMs) into robotics has unlocked
unprecedented capabilities in high-level task planning. However, most current
systems operate in an open-loop fashion, where LLMs act as one-shot planners,
rendering them brittle and unable to adapt to unforeseen circumstances in
dynamic physical environments. To overcome this limitation, this paper
introduces the "Think, Act, Learn" (T-A-L) framework, a novel architecture that
enables an embodied agent to autonomously learn and refine its policies through
continuous interaction. Our framework establishes a closed-loop cycle where an
LLM first "thinks" by decomposing high-level commands into actionable plans.
The robot then "acts" by executing these plans while gathering rich, multimodal
sensory feedback. Critically, the "learn" module processes this feedback to
facilitate LLM-driven self-reflection, allowing the agent to perform causal
analysis on its failures and generate corrective strategies. These insights are
stored in an experiential memory to guide future planning cycles. We
demonstrate through extensive experiments in both simulation and the real world
that our T-A-L agent significantly outperforms baseline methods, including
open-loop LLMs, Behavioral Cloning, and traditional Reinforcement Learning. Our
framework achieves over a 97% success rate on complex, long-horizon tasks,
converges to a stable policy in an average of just 9 trials, and exhibits
remarkable generalization to unseen tasks. This work presents a significant
step towards developing more robust, adaptive, and truly autonomous robotic
agents.

</details>


### [13] [Homotopy-aware Multi-agent Navigation via Distributed Model Predictive Control](https://arxiv.org/abs/2507.19860)
*Haoze Dong,Meng Guo,Chengyi He,Zhongkui Li*

Main category: cs.RO

TL;DR: 提出了一种分布式轨迹规划框架，结合全局路径和局部轨迹协作，有效减少多智能体在密集环境中的死锁问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体轨迹规划在密集环境中易发生死锁，尤其是在狭窄走廊中，亟需一种兼顾安全与效率的解决方案。

Method: 全局层面采用同伦感知的最优路径规划算法，局部层面使用基于模型预测控制的轨迹优化方法，并结合在线重规划策略。

Result: 实验表明，该方法显著减少死锁，成功率从4%-13%提升至90%以上。

Conclusion: 通过结合时间感知的同伦特性，该方法在多智能体密集环境中表现优异。

Abstract: Multi-agent trajectory planning requires ensuring both safety and efficiency,
yet deadlocks remain a significant challenge, especially in obstacle-dense
environments. Such deadlocks frequently occur when multiple agents attempt to
traverse the same long and narrow corridor simultaneously. To address this, we
propose a novel distributed trajectory planning framework that bridges the gap
between global path and local trajectory cooperation. At the global level, a
homotopy-aware optimal path planning algorithm is proposed, which fully
leverages the topological structure of the environment. A reference path is
chosen from distinct homotopy classes by considering both its spatial and
temporal properties, leading to improved coordination among agents globally. At
the local level, a model predictive control-based trajectory optimization
method is used to generate dynamically feasible and collision-free
trajectories. Additionally, an online replanning strategy ensures its
adaptability to dynamic environments. Simulations and experiments validate the
effectiveness of our approach in mitigating deadlocks. Ablation studies
demonstrate that by incorporating time-aware homotopic properties into the
underlying global paths, our method can significantly reduce deadlocks and
improve the average success rate from 4%-13% to over 90% in randomly generated
dense scenarios.

</details>


### [14] [Bridging Simulation and Usability: A User-Friendly Framework for Scenario Generation in CARLA](https://arxiv.org/abs/2507.19883)
*Ahmed Abouelazm,Mohammad Mahmoud,Conrad Walter,Oleksandr Shchetsura,Erne Hussong,Helen Gremmelmaier,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 提出了一种无需编程的交互式场景生成框架，用于自动驾驶系统的验证，通过图形界面降低使用门槛。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统的验证需要大量场景测试，但现有工具依赖编程知识，限制了非技术用户的使用。

Method: 开发了一个基于图形界面的无代码框架，支持手动和自动生成场景，并集成了基于深度学习的生成方法。

Result: 框架能够生成多样化和真实的测试数据集，简化了场景生成流程，提高了验证效率。

Conclusion: 该框架降低了仿真验证的门槛，支持更广泛的用户群体，提升了自动驾驶系统测试的效率和可及性。

Abstract: Autonomous driving promises safer roads, reduced congestion, and improved
mobility, yet validating these systems across diverse conditions remains a
major challenge. Real-world testing is expensive, time-consuming, and sometimes
unsafe, making large-scale validation impractical. In contrast, simulation
environments offer a scalable and cost-effective alternative for rigorous
verification and validation. A critical component of the validation process is
scenario generation, which involves designing and configuring traffic scenarios
to evaluate autonomous systems' responses to various events and uncertainties.
However, existing scenario generation tools often require programming
knowledge, limiting accessibility for non-technical users. To address this
limitation, we present an interactive, no-code framework for scenario
generation. Our framework features a graphical interface that enables users to
create, modify, save, load, and execute scenarios without needing coding
expertise or detailed simulation knowledge. Unlike script-based tools such as
Scenic or ScenarioRunner, our approach lowers the barrier to entry and supports
a broader user base. Central to our framework is a graph-based scenario
representation that facilitates structured management, supports both manual and
automated generation, and enables integration with deep learning-based scenario
and behavior generation methods. In automated mode, the framework can randomly
sample parameters such as actor types, behaviors, and environmental conditions,
allowing the generation of diverse and realistic test datasets. By simplifying
the scenario generation process, this framework supports more efficient testing
workflows and increases the accessibility of simulation-based validation for
researchers, engineers, and policymakers.

</details>


### [15] [High-Speed Event Vision-Based Tactile Roller Sensor for Large Surface Measurements](https://arxiv.org/abs/2507.19914)
*Akram Khairi,Hussain Sajwani,Abdallah Mohammad Alkilany,Laith AbuAssi,Mohamad Halwani,Islam Mohamed Zaid,Ahmed Awadalla,Dewald Swart,Abdulla Ayyad,Yahya Zweiri*

Main category: cs.RO

TL;DR: 提出了一种新型触觉传感器，结合神经形态相机和滚动机制，实现快速、连续、高分辨率的3D表面扫描。


<details>
  <summary>Details</summary>
Motivation: 传统视觉触觉传感器在大面积扫描中速度慢且受限于运动模糊，需要一种更高效的方法。

Method: 集成神经形态相机于滚动机制中，采用改进的事件多视角立体方法进行3D重建，并结合多参考贝叶斯融合策略提升精度。

Result: 扫描速度达0.5 m/s，平均绝对误差低于100微米，比现有方法快11倍；特征识别速度提升2.6倍。

Conclusion: 该传感器显著提升了扫描速度和精度，适用于工业表面检测。

Abstract: Inspecting large-scale industrial surfaces like aircraft fuselages for
quality control requires capturing their precise 3D surface geometry at high
resolution. Vision-based tactile sensors (VBTSs) offer high local resolution
but require slow 'press-and-lift' measurements stitched for large areas.
Approaches with sliding or roller/belt VBTS designs provide measurements
continuity. However, they face significant challenges respectively: sliding
struggles with friction/wear and both approaches are speed-limited by
conventional camera frame rates and motion blur, making large-area scanning
time consuming. Thus, a rapid, continuous, high-resolution method is needed. We
introduce a novel tactile sensor integrating a neuromorphic camera in a rolling
mechanism to achieve this. Leveraging its high temporal resolution and
robustness to motion blur, our system uses a modified event-based multi-view
stereo approach for 3D reconstruction. We demonstrate state-of-the-art scanning
speeds up to 0.5 m/s, achieving Mean Absolute Error below 100 microns -- 11
times faster than prior continuous tactile sensing methods. A multi-reference
Bayesian fusion strategy enhances accuracy (reducing MAE by 25.2\% compared to
EMVS) and mitigates curvature errors. We also validate high-speed feature
recognition via Braille reading 2.6 times faster than previous approaches.

</details>


### [16] [Spatial Language Likelihood Grounding Network for Bayesian Fusion of Human-Robot Observations](https://arxiv.org/abs/2507.19947)
*Supawich Sitdhipol,Waritwong Sukprasongdee,Ekapol Chuangsuwanich,Rina Tse*

Main category: cs.RO

TL;DR: FP-LGN模型通过三阶段课程学习，学习地图图像特征与空间关系语义，用于不确定性感知的异构信息融合，提升人机协作任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决机器人感知限制，通过融合人类观察信息，需一个基于不确定性的人类输入似然模型。

Method: 提出FP-LGN模型，通过三阶段课程学习训练概率估计器，捕捉人类语言中的随机不确定性。

Result: FP-LGN在NLL上匹配专家规则，鲁棒性更强，显著提升人机协作任务性能。

Conclusion: FP-LGN成功实现不确定性感知的异构信息融合，为人机协作提供有效支持。

Abstract: Fusing information from human observations can help robots overcome sensing
limitations in collaborative tasks. However, an uncertainty-aware fusion
framework requires a grounded likelihood representing the uncertainty of human
inputs. This paper presents a Feature Pyramid Likelihood Grounding Network
(FP-LGN) that grounds spatial language by learning relevant map image features
and their relationships with spatial relation semantics. The model is trained
as a probability estimator to capture aleatoric uncertainty in human language
using three-stage curriculum learning. Results showed that FP-LGN matched
expert-designed rules in mean Negative Log-Likelihood (NLL) and demonstrated
greater robustness with lower standard deviation. Collaborative sensing results
demonstrated that the grounded likelihood successfully enabled
uncertainty-aware fusion of heterogeneous human language observations and robot
sensor measurements, achieving significant improvements in human-robot
collaborative task performance.

</details>


### [17] [A roadmap for AI in robotics](https://arxiv.org/abs/2507.19975)
*Aude Billard,Alin Albu-Schaeffer,Michael Beetz,Wolfram Burgard,Peter Corke,Matei Ciocarlie,Ravinder Dahiya,Danica Kragic,Ken Goldberg,Yukie Nagai,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 本文评估了AI在机器人领域的成就，并提出了短期和中期的研究路线图，包括数据集更新、算法设计、人机协作等挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用AI技术解决机器人部署中的物理世界挑战，并推动机器人技术的进一步发展。

Method: 通过回顾AI在机器人领域的应用历史，提出研究路线图，涵盖数据集、算法设计、人机协作等方面。

Result: 提出了解决机器人领域挑战的具体方向，包括数据集多样性、算法通用性、透明性和安全性等。

Conclusion: 长期挑战在于设计具备终身学习能力、安全部署和可持续计算成本的机器人。

Abstract: AI technologies, including deep learning, large-language models have gone
from one breakthrough to the other. As a result, we are witnessing growing
excitement in robotics at the prospect of leveraging the potential of AI to
tackle some of the outstanding barriers to the full deployment of robots in our
daily lives. However, action and sensing in the physical world pose greater and
different challenges than analysing data in isolation. As the development and
application of AI in robotic products advances, it is important to reflect on
which technologies, among the vast array of network architectures and learning
models now available in the AI field, are most likely to be successfully
applied to robots; how they can be adapted to specific robot designs, tasks,
environments; which challenges must be overcome. This article offers an
assessment of what AI for robotics has achieved since the 1990s and proposes a
short- and medium-term research roadmap listing challenges and promises. These
range from keeping up-to-date large datasets, representatives of a diversity of
tasks robots may have to perform, and of environments they may encounter, to
designing AI algorithms tailored specifically to robotics problems but generic
enough to apply to a wide range of applications and transfer easily to a
variety of robotic platforms. For robots to collaborate effectively with
humans, they must predict human behavior without relying on bias-based
profiling. Explainability and transparency in AI-driven robot control are not
optional but essential for building trust, preventing misuse, and attributing
responsibility in accidents. We close on what we view as the primary long-term
challenges, that is, to design robots capable of lifelong learning, while
guaranteeing safe deployment and usage, and sustainable computational costs.

</details>


### [18] [CLASP: General-Purpose Clothes Manipulation with Semantic Keypoints](https://arxiv.org/abs/2507.19983)
*Yuhong Deng,Chao Tang,Cunjun Yu,Linfeng Li,David Hsu*

Main category: cs.RO

TL;DR: CLASP提出了一种基于语义关键点的通用衣物操作方法，通过稀疏的空间语义表示连接高层任务规划和低层动作执行，在多种衣物类型和任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有衣物操作方法局限于特定任务和衣物类型，难以应对复杂的高维几何结构，因此需要一种通用解决方案。

Method: 利用语义关键点（如“左袖”、“右肩”）作为中间表示，结合视觉语言模型进行任务规划，并通过预构建的技能库执行动作。

Result: 在仿真和真实机器人实验中，CLASP在多种任务和衣物类型上优于现有方法，表现出强大的性能和泛化能力。

Conclusion: CLASP通过语义关键点实现了通用衣物操作，为家庭服务机器人提供了高效、灵活的解决方案。

Abstract: Clothes manipulation, such as folding or hanging, is a critical capability
for home service robots. Despite recent advances, most existing methods remain
limited to specific tasks and clothes types, due to the complex,
high-dimensional geometry of clothes. This paper presents CLothes mAnipulation
with Semantic keyPoints (CLASP), which aims at general-purpose clothes
manipulation over different clothes types, T-shirts, shorts, skirts, long
dresses, ... , as well as different tasks, folding, flattening, hanging, ... .
The core idea of CLASP is semantic keypoints -- e.g., ''left sleeve'', ''right
shoulder'', etc. -- a sparse spatial-semantic representation that is salient
for both perception and action. Semantic keypoints of clothes can be reliably
extracted from RGB-D images and provide an effective intermediate
representation of clothes manipulation policies. CLASP uses semantic keypoints
to bridge high-level task planning and low-level action execution. At the high
level, it exploits vision language models (VLMs) to predict task plans over the
semantic keypoints. At the low level, it executes the plans with the help of a
simple pre-built manipulation skill library. Extensive simulation experiments
show that CLASP outperforms state-of-the-art baseline methods on multiple tasks
across diverse clothes types, demonstrating strong performance and
generalization. Further experiments with a Franka dual-arm system on four
distinct tasks -- folding, flattening, hanging, and placing -- confirm CLASP's
performance on a real robot.

</details>


### [19] [Robot Excavation and Manipulation of Geometrically Cohesive Granular Media](https://arxiv.org/abs/2507.19999)
*Laura Treers,Daniel Soto,Joonha Hwang,Michael A. D. Goodisman,Daniel I. Goldman*

Main category: cs.RO

TL;DR: 论文探讨了机器人群体如何通过操纵纠缠颗粒材料构建随机结构，提出了机器人物理模型，并研究了材料初始条件对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 传统建筑依赖预先设计的蓝图和材料，而随机结构依赖颗粒材料的特性。机器人群体可能成为构建此类结构的新工具。

Method: 开发了机器人物理模型，用于与几何粘性颗粒材料交互，测试了不同初始压缩状态下的机器人性能。

Result: 机器人性能受材料初始压缩状态影响显著，运输质量变化高达75%。拉伸测试揭示了材料强度与初始压缩的关系。

Conclusion: 材料特性（如堆积和纠缠）对机器人挖掘和构建功能至关重要，未来需进一步研究机器人与纠缠材料的交互机制。

Abstract: Construction throughout history typically assumes that its blueprints and
building blocks are pre-determined. However, recent work suggests that
alternative approaches can enable new paradigms for structure formation.
Aleatory architectures, or those which rely on the properties of their granular
building blocks rather than pre-planned design or computation, have thus far
relied on human intervention for their creation. We imagine that robotic swarms
could be valuable to create such aleatory structures by manipulating and
forming structures from entangled granular materials. To discover principles by
which robotic systems can effectively manipulate soft matter, we develop a
robophysical model for interaction with geometrically cohesive granular media
composed of u-shape particles. This robotic platform uses environmental signals
to autonomously coordinate excavation, transport, and deposition of material.
We test the effect of substrate initial conditions by characterizing robot
performance in two different material compaction states and observe as much as
a 75% change in transported mass depending on initial substrate compressive
loading. These discrepancies suggest the functional role that material
properties such as packing and cohesion/entanglement play in excavation and
construction. To better understand these material properties, we develop an
apparatus for tensile testing of the geometrically cohesive substrates, which
reveals how entangled material strength responds strongly to initial
compressive loading. These results explain the variation observed in robotic
performance and point to future directions for better understanding robotic
interaction mechanics with entangled materials.

</details>


### [20] [SuperMag: Vision-based Tactile Data Guided High-resolution Tactile Shape Reconstruction for Magnetic Tactile Sensors](https://arxiv.org/abs/2507.20002)
*Peiyao Hou,Danning Sun,Meng Wang,Yuzhe Huang,Zeyu Zhang,Hangxin Liu,Wanlin Li,Ziyuan Jiao*

Main category: cs.RO

TL;DR: SuperMag利用高分辨率视觉触觉传感器数据监督磁触觉传感器的超分辨率重建，提升其空间分辨率。


<details>
  <summary>Details</summary>
Motivation: 磁触觉传感器（MBTS）设计紧凑且高频操作，但空间分辨率受限。

Method: 提出SuperMag方法，通过条件变分自编码器从低分辨率MBTS输入推断高分辨率形状。

Result: MBTS采样频率达125 Hz，形状重建推理时间小于2.5 ms。

Conclusion: 跨模态协同提升了MBTS的触觉感知能力，有望用于高精度机器人任务。

Abstract: Magnetic-based tactile sensors (MBTS) combine the advantages of compact
design and high-frequency operation but suffer from limited spatial resolution
due to their sparse taxel arrays. This paper proposes SuperMag, a tactile shape
reconstruction method that addresses this limitation by leveraging
high-resolution vision-based tactile sensor (VBTS) data to supervise MBTS
super-resolution. Co-designed, open-source VBTS and MBTS with identical contact
modules enable synchronized data collection of high-resolution shapes and
magnetic signals via a symmetric calibration setup. We frame tactile shape
reconstruction as a conditional generative problem, employing a conditional
variational auto-encoder to infer high-resolution shapes from low-resolution
MBTS inputs. The MBTS achieves a sampling frequency of 125 Hz, whereas the
shape reconstruction sustains an inference time within 2.5 ms. This
cross-modality synergy advances tactile perception of the MBTS, potentially
unlocking its new capabilities in high-precision robotic tasks.

</details>


### [21] [When Engineering Outruns Intelligence: A Re-evaluation of Instruction-Guided Navigation](https://arxiv.org/abs/2507.20021)
*Matin Aghaei,Mohammad Ali Alomrani,Yingxue Zhang,Mahdi Biparva*

Main category: cs.RO

TL;DR: 研究发现，在对象目标导航中，几何启发式方法（DWFE）比大型语言模型（LLM）更能提升性能，而轻量级语言先验（SHF）进一步优化了路径规划。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLM）在对象目标导航中的实际贡献，验证其是否真正提升了规划能力。

Method: 1. 移除InstructNav的动态导航提示和语义检测器，改用几何启发式方法（DWFE）。2. 引入轻量级语言先验（SHF）进一步优化。

Result: DWFE将成功率从58.0%提升至61.1%，SPL从20.9%提升至36.0%；SHF进一步增加2%成功率和0.9% SPL，并缩短路径。

Conclusion: 几何启发式是性能提升的主要驱动力，LLM的贡献有限；未来需结合度量感知提示或离线语义图以充分利用LLM。

Abstract: Large language models (LLMs) are often credited with recent leaps in
ObjectGoal Navigation, yet the extent to which they improve planning remains
unclear. We revisit this question on the HM3D-v1 validation split. First, we
strip InstructNav of its Dynamic Chain-of-Navigation prompt, open-vocabulary
GLEE detector and Intuition saliency map, and replace them with a simple
Distance-Weighted Frontier Explorer (DWFE). This geometry-only heuristic raises
Success from 58.0% to 61.1% and lifts SPL from 20.9% to 36.0% over 2 000
validation episodes, outperforming all previous training-free baselines.
Second, we add a lightweight language prior (SHF); on a 200-episode subset this
yields a further +2% Success and +0.9% SPL while shortening paths by five steps
on average. Qualitative trajectories confirm the trend: InstructNav back-tracks
and times-out, DWFE reaches the goal after a few islands, and SHF follows an
almost straight route. Our results indicate that frontier geometry, not
emergent LLM reasoning, drives most reported gains, and suggest that
metric-aware prompts or offline semantic graphs are necessary before
attributing navigation success to "LLM intelligence."

</details>


### [22] [Digital and Robotic Twinning for Validation of Proximity Operations and Formation Flying](https://arxiv.org/abs/2507.20034)
*Aviad Golan,Gregory Zin,Zahra Ahmed,Emily Bates,Toby Bell,Pol Francesch Huc,Samuel Y. W. Low,Juergen Bosse,Simone D'Amico*

Main category: cs.RO

TL;DR: 本文提出了一种统一的数字和机器人孪生框架，用于验证多模态GNC系统，通过仿真和硬件测试确保其性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于太空环境的复杂性，验证GNC系统具有挑战性，需要一种能够连接仿真和实际行为的V&V流程。

Method: 开发了一个端到端的数字和机器人孪生框架，包括三个测试平台（GRAND、TRON和OS），用于验证RF和视觉导航技术。

Result: 数字和机器人孪生结果一致，验证了该框架在GNC系统评估中的可靠性。

Conclusion: 该混合孪生框架为GNC系统的现实评估和验证提供了可靠方法。

Abstract: In spacecraft Rendezvous, Proximity Operations (RPO), and Formation Flying
(FF), the Guidance Navigation and Control (GNC) system is safety-critical and
must meet strict performance requirements. However, validating such systems is
challenging due to the complexity of the space environment, necessitating a
verification and validation (V&V) process that bridges simulation and
real-world behavior. The key contribution of this paper is a unified,
end-to-end digital and robotic twinning framework that enables software- and
hardware-in-the-loop testing for multi-modal GNC systems. The robotic twin
includes three testbeds at Stanford's Space Rendezvous Laboratory (SLAB): the
GNSS and Radiofrequency Autonomous Navigation Testbed for Distributed Space
Systems (GRAND) to validate RF-based navigation techniques, and the Testbed for
Rendezvous and Optical Navigation (TRON) and Optical Stimulator (OS) to
validate vision-based methods. The test article for this work is an integrated
multi-modal GNC software stack for RPO and FF developed at SLAB. This paper
introduces the hybrid framework and summarizes calibration and error
characterization for the robotic twin. Then, the GNC stack's performance and
robustness is characterized using the integrated digital and robotic twinning
pipeline for a full-range RPO mission scenario in Low-Earth Orbit (LEO). The
results shown in the paper demonstrate consistency between digital and robotic
twins, validating the hybrid twinning pipeline as a reliable framework for
realistic assessment and verification of GNC systems.

</details>


### [23] [A real-time full-chain wearable sensor-based musculoskeletal simulation: an OpenSim-ROS Integration](https://arxiv.org/abs/2507.20049)
*Frederico Belmonte Klein,Zhaoyuan Wan,Huawei Wang,Ruoli Wang*

Main category: cs.RO

TL;DR: 提出了一种基于OpenSimRT、ROS和可穿戴传感器的实时集成框架，用于解决肌肉骨骼建模和仿真中的高成本、复杂性和集成问题。


<details>
  <summary>Details</summary>
Motivation: 当前肌肉骨骼建模技术因高成本传感器、实验室设置、计算复杂性和软件工具集成不足而受限，阻碍了广泛应用。

Method: 结合OpenSimRT、ROS和可穿戴传感器，开发实时框架，验证其在下肢和上肢逆运动学、踝关节逆动力学及肌肉激活估计中的有效性。

Result: 框架能准确描述逆运动学，估计踝关节逆动力学和下肢肌肉激活，适用于日常活动如行走、蹲起等。

Conclusion: 该研究为复杂实时可穿戴传感器系统奠定了基础，有望推动康复、机器人和外骨骼设计技术的发展。

Abstract: Musculoskeletal modeling and simulations enable the accurate description and
analysis of the movement of biological systems with applications such as
rehabilitation assessment, prosthesis, and exoskeleton design. However, the
widespread usage of these techniques is limited by costly sensors,
laboratory-based setups, computationally demanding processes, and the use of
diverse software tools that often lack seamless integration. In this work, we
address these limitations by proposing an integrated, real-time framework for
musculoskeletal modeling and simulations that leverages OpenSimRT, the robotics
operating system (ROS), and wearable sensors. As a proof-of-concept, we
demonstrate that this framework can reasonably well describe inverse kinematics
of both lower and upper body using either inertial measurement units or
fiducial markers. Additionally, we show that it can effectively estimate
inverse dynamics of the ankle joint and muscle activations of major lower limb
muscles during daily activities, including walking, squatting and sit to stand,
stand to sit when combined with pressure insoles. We believe this work lays the
groundwork for further studies with more complex real-time and wearable
sensor-based human movement analysis systems and holds potential to advance
technologies in rehabilitation, robotics and exoskeleton designs.

</details>


### [24] [Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots](https://arxiv.org/abs/2507.20217)
*Wei Cui,Haoyu Wang,Wenkang Qin,Yijie Guo,Gang Han,Wen Zhao,Jiahang Cao,Zhang Zhang,Jiaru Zhong,Jingkai Sun,Pihai Sun,Shuai Shi,Botuo Jiang,Jiahao Ma,Jiaxu Wang,Hao Cheng,Zhichao Liu,Yang Wang,Zheng Zhu,Guan Huang,Jian Tang,Qiang Zhang*

Main category: cs.RO

TL;DR: Humanoid Occupancy是一个多模态占用感知系统，结合硬件和软件，为仿人机器人提供全面的环境理解。


<details>
  <summary>Details</summary>
Motivation: 解决仿人机器人在复杂环境中的感知挑战，如运动干扰和遮挡。

Method: 采用多模态融合技术和网格占用表示，结合传感器布局策略和全景占用数据集。

Result: 实现了鲁棒的环境感知，为任务规划和导航提供支持。

Conclusion: Humanoid Occupancy为仿人机器人的标准化视觉模块奠定了基础，推动其在复杂场景中的广泛应用。

Abstract: Humanoid robot technology is advancing rapidly, with manufacturers
introducing diverse heterogeneous visual perception modules tailored to
specific scenarios. Among various perception paradigms, occupancy-based
representation has become widely recognized as particularly suitable for
humanoid robots, as it provides both rich semantic and 3D geometric information
essential for comprehensive environmental understanding. In this work, we
present Humanoid Occupancy, a generalized multimodal occupancy perception
system that integrates hardware and software components, data acquisition
devices, and a dedicated annotation pipeline. Our framework employs advanced
multi-modal fusion techniques to generate grid-based occupancy outputs encoding
both occupancy status and semantic labels, thereby enabling holistic
environmental understanding for downstream tasks such as task planning and
navigation. To address the unique challenges of humanoid robots, we overcome
issues such as kinematic interference and occlusion, and establish an effective
sensor layout strategy. Furthermore, we have developed the first panoramic
occupancy dataset specifically for humanoid robots, offering a valuable
benchmark and resource for future research and development in this domain. The
network architecture incorporates multi-modal feature fusion and temporal
information integration to ensure robust perception. Overall, Humanoid
Occupancy delivers effective environmental perception for humanoid robots and
establishes a technical foundation for standardizing universal visual modules,
paving the way for the widespread deployment of humanoid robots in complex
real-world scenarios.

</details>


### [25] [Tactile-Guided Robotic Ultrasound: Mapping Preplanned Scan Paths for Intercostal Imaging](https://arxiv.org/abs/2507.20282)
*Yifan Zhang,Dianye Huang,Nassir Navab,Zhongliang Jiang*

Main category: cs.RO

TL;DR: 论文提出了一种基于触觉信号的机器人超声扫描路径生成方法，用于解决肋间成像中的挑战，并通过实验验证了其准确性。


<details>
  <summary>Details</summary>
Motivation: 解决机器人超声系统在肋间成像中因缺乏有效扫描路径生成方法而受限的问题。

Method: 利用触觉信号提取肋骨表面点云，通过稀疏点云插值和配准生成扫描路径，并引入自动倾斜角调整方法。

Result: 扫描路径映射的平均最近邻距离和Hausdorff距离误差分别为3.41 mm和3.65 mm，重建对象的误差为0.69 mm和2.2 mm。

Conclusion: 该方法有效提高了肋间超声扫描的准确性和效率，为临床应用提供了新思路。

Abstract: Medical ultrasound (US) imaging is widely used in clinical examinations due
to its portability, real-time capability, and radiation-free nature. To address
inter- and intra-operator variability, robotic ultrasound systems have gained
increasing attention. However, their application in challenging intercostal
imaging remains limited due to the lack of an effective scan path generation
method within the constrained acoustic window. To overcome this challenge, we
explore the potential of tactile cues for characterizing subcutaneous rib
structures as an alternative signal for ultrasound segmentation-free bone
surface point cloud extraction. Compared to 2D US images, 1D tactile-related
signals offer higher processing efficiency and are less susceptible to acoustic
noise and artifacts. By leveraging robotic tracking data, a sparse tactile
point cloud is generated through a few scans along the rib, mimicking human
palpation. To robustly map the scanning trajectory into the intercostal space,
the sparse tactile bone location point cloud is first interpolated to form a
denser representation. This refined point cloud is then registered to an
image-based dense bone surface point cloud, enabling accurate scan path mapping
for individual patients. Additionally, to ensure full coverage of the object of
interest, we introduce an automated tilt angle adjustment method to visualize
structures beneath the bone. To validate the proposed method, we conducted
comprehensive experiments on four distinct phantoms. The final scanning
waypoint mapping achieved Mean Nearest Neighbor Distance (MNND) and Hausdorff
distance (HD) errors of 3.41 mm and 3.65 mm, respectively, while the
reconstructed object beneath the bone had errors of 0.69 mm and 2.2 mm compared
to the CT ground truth.

</details>


### [26] [Decentralized Uncertainty-Aware Multi-Agent Collision Avoidance With Model Predictive Path Integral](https://arxiv.org/abs/2507.20293)
*Stepan Dergachev,Konstantin Yakovlev*

Main category: cs.RO

TL;DR: 提出了一种结合MPPI和概率化ORCA的新方法，用于多智能体导航，确保安全高效。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体导航中的不确定性和噪声问题，需要兼顾安全性和效率。

Method: 将MPPI与概率化ORCA结合，通过SOCP将安全约束直接纳入采样过程。

Result: 在密集环境中表现优于ORCA-DD和B-UAVC，成功率高，并在Gazebo中验证了实用性。

Conclusion: 该方法在多智能体导航中具有高效性和安全性，适用于实际机器人平台。

Abstract: Decentralized multi-agent navigation under uncertainty is a complex task that
arises in numerous robotic applications. It requires collision avoidance
strategies that account for both kinematic constraints, sensing and action
execution noise. In this paper, we propose a novel approach that integrates the
Model Predictive Path Integral (MPPI) with a probabilistic adaptation of
Optimal Reciprocal Collision Avoidance. Our method ensures safe and efficient
multi-agent navigation by incorporating probabilistic safety constraints
directly into the MPPI sampling process via a Second-Order Cone Programming
formulation. This approach enables agents to operate independently using local
noisy observations while maintaining safety guarantees. We validate our
algorithm through extensive simulations with differential-drive robots and
benchmark it against state-of-the-art methods, including ORCA-DD and B-UAVC.
Results demonstrate that our approach outperforms them while achieving high
success rates, even in densely populated environments. Additionally, validation
in the Gazebo simulator confirms its practical applicability to robotic
platforms.

</details>


### [27] [Advancing Shared and Multi-Agent Autonomy in Underwater Missions: Integrating Knowledge Graphs and Retrieval-Augmented Generation](https://arxiv.org/abs/2507.20370)
*Michele Grimaldi,Carlo Cernicchiaro,Sebastian Realpe Rua,Alaaeddine El-Masri-El-Chaarani,Markus Buchholz,Loizos Michael,Pere Ridao Rodriguez,Ignacio Carlucho,Yvan R. Petillot*

Main category: cs.RO

TL;DR: 论文探讨了水下机器人平台在复杂环境中的自主性挑战，提出结合知识图谱和检索增强生成（RAG）系统，实现多智能体自主决策与人机交互，验证了100%任务完成率。


<details>
  <summary>Details</summary>
Motivation: 水下环境的复杂性和动态性（如低可见性、不可预测水流和通信限制）对机器人自主性提出高要求，同时需确保操作者的信任与监督。

Method: 采用知识图谱和RAG系统，结合大型语言模型（LLM），实现多智能体自主决策与人机交互。

Result: 实验表明，该方法实现了100%的任务验证和行为完整性；若缺少知识图谱或领域分类支持，LLM易产生幻觉，影响决策质量。

Conclusion: 知识图谱和RAG系统能有效提升水下机器人的自主性和决策质量，支持多智能体协作与人机交互。

Abstract: Robotic platforms have become essential for marine operations by providing
regular and continuous access to offshore assets, such as underwater
infrastructure inspection, environmental monitoring, and resource exploration.
However, the complex and dynamic nature of underwater environments,
characterized by limited visibility, unpredictable currents, and communication
constraints, presents significant challenges that demand advanced autonomy
while ensuring operator trust and oversight. Central to addressing these
challenges are knowledge representation and reasoning techniques, particularly
knowledge graphs and retrieval-augmented generation (RAG) systems, that enable
robots to efficiently structure, retrieve, and interpret complex environmental
data. These capabilities empower robotic agents to reason, adapt, and respond
effectively to changing conditions. The primary goal of this work is to
demonstrate both multi-agent autonomy and shared autonomy, where multiple
robotic agents operate independently while remaining connected to a human
supervisor. We show how a RAG-powered large language model, augmented with
knowledge graph data and domain taxonomy, enables autonomous multi-agent
decision-making and facilitates seamless human-robot interaction, resulting in
100\% mission validation and behavior completeness. Finally, ablation studies
reveal that without structured knowledge from the graph and/or taxonomy, the
LLM is prone to hallucinations, which can compromise decision quality.

</details>


### [28] [Bipedalism for Quadrupedal Robots: Versatile Loco-Manipulation through Risk-Adaptive Reinforcement Learning](https://arxiv.org/abs/2507.20382)
*Yuyou Zhang,Radu Corcodel,Ding Zhao*

Main category: cs.RO

TL;DR: 提出一种双足行走的四足机器人方法，释放前腿用于环境交互，并通过风险自适应的强化学习框架实现稳定性和性能的平衡。


<details>
  <summary>Details</summary>
Motivation: 四足机器人使用腿作为操纵器会影响运动能力，而加装机械臂会增加系统复杂性，因此提出双足行走以解决这一问题。

Method: 采用风险自适应的分布强化学习框架，动态调整风险偏好以适应不稳定任务，训练中基于回报分布的不确定性调整策略。

Result: 仿真实验显示方法优于基线，实际部署展示了策略的多样性和鲁棒性，支持如推车、探测障碍和运输等任务。

Conclusion: 双足行走结合自适应强化学习框架有效提升了四足机器人的多功能性和稳定性。

Abstract: Loco-manipulation of quadrupedal robots has broadened robotic applications,
but using legs as manipulators often compromises locomotion, while mounting
arms complicates the system. To mitigate this issue, we introduce bipedalism
for quadrupedal robots, thus freeing the front legs for versatile interactions
with the environment. We propose a risk-adaptive distributional Reinforcement
Learning (RL) framework designed for quadrupedal robots walking on their hind
legs, balancing worst-case conservativeness with optimal performance in this
inherently unstable task. During training, the adaptive risk preference is
dynamically adjusted based on the uncertainty of the return, measured by the
coefficient of variation of the estimated return distribution. Extensive
experiments in simulation show our method's superior performance over
baselines. Real-world deployment on a Unitree Go2 robot further demonstrates
the versatility of our policy, enabling tasks like cart pushing, obstacle
probing, and payload transport, while showcasing robustness against challenging
dynamics and external disturbances.

</details>


### [29] [Model-Structured Neural Networks to Control the Steering Dynamics of Autonomous Race Cars](https://arxiv.org/abs/2507.20427)
*Mattia Piccinini,Aniello Mungiello,Georg Jank,Gastone Pietro Rosati Papini,Francesco Biral,Johannes Betz*

Main category: cs.RO

TL;DR: 本文提出了一种名为MS-NN-steer的新型模型结构神经网络，用于自动驾驶赛车中的转向控制，通过整合非线性车辆动力学的先验知识，提高了模型的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶赛车作为测试运动规划和控制方法的安全环境，其安全性和鲁棒性要求对决策算法有透彻理解。然而，传统的神经网络因其黑盒特性难以满足这一需求。

Method: 提出MS-NN-steer，将非线性车辆动力学的先验知识融入神经网络架构中，用于车辆转向控制。

Result: 实验基于阿布扎比自动驾驶赛车联赛（A2RL）的真实数据，MS-NN-steer在小训练数据集下表现优于通用神经网络，且对权重初始化不敏感，性能超过A2RL冠军团队的转向控制器。

Conclusion: MS-NN-steer通过整合先验知识，显著提升了自动驾驶赛车转向控制的准确性和鲁棒性，为相关领域提供了开源实现。

Abstract: Autonomous racing has gained increasing attention in recent years, as a safe
environment to accelerate the development of motion planning and control
methods for autonomous driving. Deep learning models, predominantly based on
neural networks (NNs), have demonstrated significant potential in modeling the
vehicle dynamics and in performing various tasks in autonomous driving.
However, their black-box nature is critical in the context of autonomous
racing, where safety and robustness demand a thorough understanding of the
decision-making algorithms. To address this challenge, this paper proposes
MS-NN-steer, a new Model-Structured Neural Network for vehicle steering
control, integrating the prior knowledge of the nonlinear vehicle dynamics into
the neural architecture. The proposed controller is validated using real-world
data from the Abu Dhabi Autonomous Racing League (A2RL) competition, with
full-scale autonomous race cars. In comparison with general-purpose NNs,
MS-NN-steer is shown to achieve better accuracy and generalization with small
training datasets, while being less sensitive to the weights' initialization.
Also, MS-NN-steer outperforms the steering controller used by the A2RL winning
team. Our implementation is available open-source in a GitHub repository.

</details>


### [30] [Learning Physical Interaction Skills from Human Demonstrations](https://arxiv.org/abs/2507.20445)
*Tianyu Li,Hengbo Ma,Sehoon Ha,Kwonjoon Lee*

Main category: cs.RO

TL;DR: 提出了一种名为BuddyImitation的框架，通过Embedded Interaction Graph（EIG）从人类演示中学习跨形态的全身交互行为。


<details>
  <summary>Details</summary>
Motivation: 解决形态差异大的代理学习物理交互技能的挑战，避免依赖手工目标或形态相似性。

Method: 提取EIG作为交互动态的紧凑表示，并用于训练控制策略。

Result: 在多种代理和交互场景中验证了有效性，如舞蹈、握手等。

Conclusion: 为跨形态协调行为提供了一种可行路径。

Abstract: Learning physical interaction skills, such as dancing, handshaking, or
sparring, remains a fundamental challenge for agents operating in human
environments, particularly when the agent's morphology differs significantly
from that of the demonstrator. Existing approaches often rely on handcrafted
objectives or morphological similarity, limiting their capacity for
generalization. Here, we introduce a framework that enables agents with diverse
embodiments to learn wholebbody interaction behaviors directly from human
demonstrations. The framework extracts a compact, transferable representation
of interaction dynamics, called the Embedded Interaction Graph (EIG), which
captures key spatiotemporal relationships between the interacting agents. This
graph is then used as an imitation objective to train control policies in
physics-based simulations, allowing the agent to generate motions that are both
semantically meaningful and physically feasible. We demonstrate BuddyImitation
on multiple agents, such as humans, quadrupedal robots with manipulators, or
mobile manipulators and various interaction scenarios, including sparring,
handshaking, rock-paper-scissors, or dancing. Our results demonstrate a
promising path toward coordinated behaviors across morphologically distinct
characters via cross embodiment interaction learning.

</details>


### [31] [LLMs-guided adaptive compensator: Bringing Adaptivity to Automatic Control Systems with Large Language Models](https://arxiv.org/abs/2507.20509)
*Zhongchao Zhou,Yuxi Lu,Yaonan Zhu,Yifan Zhao,Bin He,Liang He,Wenwen Yu,Yusuke Iwasawa*

Main category: cs.RO

TL;DR: 该论文提出了一种基于大语言模型（LLM）的自适应补偿器框架，用于机器人控制，通过将数学推导转化为推理任务，显著降低了复杂性并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究在机器人控制中应用LLM时多关注高层任务或简化系统，缺乏实际验证。本文旨在探索LLM在自适应控制中的应用，提升实际部署能力。

Method: 提出LLM引导的自适应补偿器框架，利用未知系统与参考系统的差异设计补偿器，避免从头设计控制器。实验比较了五种方法。

Result: LLM引导的自适应补偿器在性能和推理复杂度上优于传统方法，并表现出强泛化性、适应性和鲁棒性。

Conclusion: 该研究为LLM在自动控制领域的应用开辟了新方向，具有更高的实用性和部署潜力。

Abstract: With rapid advances in code generation, reasoning, and problem-solving, Large
Language Models (LLMs) are increasingly applied in robotics. Most existing work
focuses on high-level tasks such as task decomposition. A few studies have
explored the use of LLMs in feedback controller design; however, these efforts
are restricted to overly simplified systems, fixed-structure gain tuning, and
lack real-world validation. To further investigate LLMs in automatic control,
this work targets a key subfield: adaptive control. Inspired by the framework
of model reference adaptive control (MRAC), we propose an LLM-guided adaptive
compensator framework that avoids designing controllers from scratch. Instead,
the LLMs are prompted using the discrepancies between an unknown system and a
reference system to design a compensator that aligns the response of the
unknown system with that of the reference, thereby achieving adaptivity.
Experiments evaluate five methods: LLM-guided adaptive compensator, LLM-guided
adaptive controller, indirect adaptive control, learning-based adaptive
control, and MRAC, on soft and humanoid robots in both simulated and real-world
environments. Results show that the LLM-guided adaptive compensator outperforms
traditional adaptive controllers and significantly reduces reasoning complexity
compared to the LLM-guided adaptive controller. The Lyapunov-based analysis and
reasoning-path inspection demonstrate that the LLM-guided adaptive compensator
enables a more structured design process by transforming mathematical
derivation into a reasoning task, while exhibiting strong generalizability,
adaptability, and robustness. This study opens a new direction for applying
LLMs in the field of automatic control, offering greater deployability and
practicality compared to vision-language models.

</details>


### [32] [Large-Scale LiDAR-Inertial Dataset for Degradation-Robust High-Precision Mapping](https://arxiv.org/abs/2507.20516)
*Xiaofeng Jin,Ningbo Bu,Shijie Wang,Jianfei Ge,Jiangjian Xiao,Matteo Matteucci*

Main category: cs.RO

TL;DR: 该论文介绍了一个大规模、高精度的LiDAR-惯性里程计（LIO）数据集，用于解决现有研究中LIO系统在复杂现实场景中验证不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究中对LIO系统在复杂现实场景中的验证不足，缺乏大规模、高精度的数据集。

Method: 使用定制背包平台，配备多光束LiDAR、工业级IMU和RTK-GNSS模块，在四种不同环境中收集数据，并通过SLAM优化与RTK-GNSS锚定融合生成高精度地面真实值。

Result: 数据集覆盖60,000至750,000平方米的复杂场景，提供长轨迹和高精度地面真实值，验证了轨迹精度。

Conclusion: 该数据集为评估LIO系统在实际高精度测绘场景中的泛化能力提供了全面基准。

Abstract: This paper introduces a large-scale, high-precision LiDAR-Inertial Odometry
(LIO) dataset, aiming to address the insufficient validation of LIO systems in
complex real-world scenarios in existing research. The dataset covers four
diverse real-world environments spanning 60,000 to 750,000 square meters,
collected using a custom backpack-mounted platform equipped with multi-beam
LiDAR, an industrial-grade IMU, and RTK-GNSS modules. The dataset includes long
trajectories, complex scenes, and high-precision ground truth, generated by
fusing SLAM-based optimization with RTK-GNSS anchoring, and validated for
trajectory accuracy through the integration of oblique photogrammetry and
RTK-GNSS. This dataset provides a comprehensive benchmark for evaluating the
generalization ability of LIO systems in practical high-precision mapping
scenarios.

</details>


### [33] [Uni-Mapper: Unified Mapping Framework for Multi-modal LiDARs in Complex and Dynamic Environments](https://arxiv.org/abs/2507.20538)
*Gilhwan Kang,Hogyun Kim,Byunghee Choi,Seokhwan Jeong,Young-Sik Shin,Younggun Cho*

Main category: cs.RO

TL;DR: Uni-Mapper是一个动态感知的3D点云地图合并框架，用于多模态LiDAR系统，解决了动态环境和多传感器差异带来的地图统一挑战。


<details>
  <summary>Details</summary>
Motivation: 统一不同地图对于多会话和多机器人协作场景至关重要，但动态环境和传感器差异导致点云分布和场景一致性不一致，影响地图对齐。

Method: Uni-Mapper包括动态物体移除、动态感知闭环和多模态LiDAR地图合并模块，通过体素自由空间哈希图和全局描述符实现鲁棒的闭环检测和地图对齐。

Result: 在动态环境和异构LiDAR数据集上表现优异，优于现有方法。

Conclusion: Uni-Mapper在多模态LiDAR系统和动态环境中实现了高效的地图合并和全局一致性。

Abstract: The unification of disparate maps is crucial for enabling scalable robot
operation across multiple sessions and collaborative multi-robot scenarios.
However, achieving a unified map robust to sensor modalities and dynamic
environments remains a challenging problem. Variations in LiDAR types and
dynamic elements lead to differences in point cloud distribution and scene
consistency, hindering reliable descriptor generation and loop closure
detection essential for accurate map alignment. To address these challenges,
this paper presents Uni-Mapper, a dynamic-aware 3D point cloud map merging
framework for multi-modal LiDAR systems. It comprises dynamic object removal,
dynamic-aware loop closure, and multi-modal LiDAR map merging modules. A
voxel-wise free space hash map is built in a coarse-to-fine manner to identify
and reject dynamic objects via temporal occupancy inconsistencies. The removal
module is integrated with a LiDAR global descriptor, which encodes preserved
static local features to ensure robust place recognition in dynamic
environments. In the final stage, multiple pose graph optimizations are
conducted for both intra-session and inter-map loop closures. We adopt a
centralized anchor-node strategy to mitigate intra-session drift errors during
map merging. In the final stage, centralized anchor-node-based pose graph
optimization is performed to address intra- and inter-map loop closures for
globally consistent map merging. Our framework is evaluated on diverse
real-world datasets with dynamic objects and heterogeneous LiDARs, showing
superior performance in loop detection across sensor modalities, robust mapping
in dynamic environments, and accurate multi-map alignment over existing
methods. Project Page: https://sparolab.github.io/research/uni_mapper.

</details>


### [34] [Methods for the Segmentation of Reticular Structures Using 3D LiDAR Data: A Comparative Evaluation](https://arxiv.org/abs/2507.20589)
*Francisco J. Soler Mora,Adrián Peidró Vidal,Marc Fabregat-Jaén,Luis Payá Castelló,Óscar Reinoso García*

Main category: cs.RO

TL;DR: 论文提出了一种在桁架结构中检测可导航表面的方法，比较了分析算法和深度学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 桁架结构的检查和维护成本高且危险，现有研究较少关注机器人自主导航，本文旨在填补这一空白。

Method: 采用分析算法和深度学习模型（如PointNet、PointNet++等）对3D点云进行二元分割，识别可导航表面。

Result: 分析算法参数调整简单且性能接近深度学习模型，而深度学习模型（如PointTransformerV3）在分割精度上表现更优（mIoU约97%）。

Conclusion: 研究展示了分析和深度学习方法在复杂桁架环境中提升自主导航的潜力，为未来研究和实际应用提供了参考。

Abstract: Reticular structures form the backbone of major infrastructure like bridges,
pylons, and airports, but their inspection and maintenance are costly and
hazardous, often requiring human intervention. While prior research has focused
on fault detection via images or robotic platform design, the autonomous
navigation of robots within these structures is less explored. This study
addresses that gap by proposing methods to detect navigable surfaces in truss
structures, enhancing the autonomy of climbing robots. The paper introduces
several approaches for binary segmentation of navigable surfaces versus
background from 3D point clouds of metallic trusses. These methods fall into
two categories: analytical algorithms and deep learning models. The analytical
approach features a custom algorithm that segments structures by analyzing the
eigendecomposition of planar patches in the point cloud. In parallel, advanced
deep learning models PointNet, PointNet++, MinkUNet34C, and PointTransformerV3
are trained and evaluated for the same task. Comparative analysis shows that
the analytical algorithm offers easier parameter tuning and performance
comparable to deep learning models, which, while more computationally
intensive, excel in segmentation accuracy. Notably, PointTransformerV3 achieves
a Mean Intersection Over Union (mIoU) of about 97%. The study demonstrates the
promise of both analytical and deep learning methods for improving autonomous
navigation in complex truss environments. The results highlight the trade-offs
between computational efficiency and segmentation performance, providing
valuable guidance for future research and practical applications in autonomous
infrastructure inspection and maintenance.

</details>


### [35] [FMimic: Foundation Models are Fine-grained Action Learners from Human Videos](https://arxiv.org/abs/2507.20622)
*Guangyan Chen,Meiling Wang,Te Cui,Yao Mu,Haoyang Lu,Zicai Peng,Mengxiao Hu,Tianxing Zhou,Mengyin Fu,Yi Yang,Yufeng Yue*

Main category: cs.RO

TL;DR: FMimic利用基础模型直接从少量人类视频中学习细粒度动作技能，显著提升了视觉模仿学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预定义动作基元执行物理交互，限制了机器人系统的能力。FMimic旨在通过基础模型直接学习通用技能。

Method: FMimic利用视觉语言模型（VLMs）直接从人类视频中学习细粒度动作，无需预定义动作基元。

Result: FMimic在单个人类视频下表现优异，五个视频时显著优于其他方法。在多任务和真实世界任务中分别提升39%和29%，在高精度和长时任务中分别超过基线34%和47%。

Conclusion: FMimic展示了基础模型在视觉模仿学习中的潜力，能够直接从视频中学习通用技能，显著提升性能。

Abstract: Visual imitation learning (VIL) provides an efficient and intuitive strategy
for robotic systems to acquire novel skills. Recent advancements in foundation
models, particularly Vision Language Models (VLMs), have demonstrated
remarkable capabilities in visual and linguistic reasoning for VIL tasks.
Despite this progress, existing approaches primarily utilize these models for
learning high-level plans from human demonstrations, relying on pre-defined
motion primitives for executing physical interactions, which remains a major
bottleneck for robotic systems. In this work, we present FMimic, a novel
paradigm that harnesses foundation models to directly learn generalizable
skills at even fine-grained action levels, using only a limited number of human
videos. Extensive experiments demonstrate that our FMimic delivers strong
performance with a single human video, and significantly outperforms all other
methods with five videos. Furthermore, our method exhibits significant
improvements of over 39% and 29% in RLBench multi-task experiments and
real-world manipulation tasks, respectively, and exceeds baselines by more than
34% in high-precision tasks and 47% in long-horizon tasks.

</details>


### [36] [A Strawberry Harvesting Tool with Minimal Footprint](https://arxiv.org/abs/2507.20784)
*Mohamed Sorour,Mohamed Heshmat,Khaled Elgeneidy,Pål Johan From*

Main category: cs.RO

TL;DR: 提出了一种新型草莓采摘原型，通过激光切割茎干以延长果实保质期，并优化了激光参数。


<details>
  <summary>Details</summary>
Motivation: 解决草莓采摘过程中可能传播植物病害的问题，同时延长果实保质期。

Method: 使用平滑捕捉器将茎干引导至精确位置，通过高温激光切割茎干，杀死病菌并防止病害传播。

Result: 成功实现室内采摘，切割时间为2.88秒，循环时间为5.56秒，并优化了激光参数。

Conclusion: 该原型高效且卫生，适用于草莓采摘，并显著延长果实保质期。

Abstract: In this paper, a novel prototype for harvesting table-top grown strawberries
is presented, that is minimalist in its footprint interacting with the fruit.
In our methodology, a smooth trapper manipulates the stem into a precise groove
location at which a distant laser beam is focused. The tool reaches
temperatures as high as 188{\deg} Celsius and as such killing germs and
preventing the spread of local plant diseases. The burnt stem wound preserves
water content and in turn the fruit shelf life. Cycle and cut times achieved
are 5.56 and 2.88 seconds respectively in successful in-door harvesting
demonstration. Extensive experiments are performed to optimize the laser spot
diameter and lateral speed against the cutting time.

</details>


### [37] [LanternNet: A Novel Hub-and-Spoke System to Seek and Suppress Spotted Lanternfly Populations](https://arxiv.org/abs/2507.20800)
*Vinil Polepalli*

Main category: cs.RO

TL;DR: LanternNet是一种新型自主机器人系统，用于检测和抑制斑点灯笼蝇（SLF），比传统方法更高效、环保且可扩展。


<details>
  <summary>Details</summary>
Motivation: 斑点灯笼蝇对农业和生态系统造成严重威胁，现有控制方法效率低且对环境有害。

Method: LanternNet采用中心-辐条系统，结合YOLOv8计算机视觉模型和专用机器人，实现SLF检测和抑制。

Result: 实地测试显示SLF种群显著减少（p < 0.01），树木健康状况改善，且成本效益更高。

Conclusion: LanternNet展示了机器人与AI在入侵物种管理中的潜力，具有广泛生态应用前景。

Abstract: The invasive spotted lanternfly (SLF) poses a significant threat to
agriculture and ecosystems, causing widespread damage. Current control methods,
such as egg scraping, pesticides, and quarantines, prove labor-intensive,
environmentally hazardous, and inadequate for long-term SLF suppression. This
research introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system
designed for scalable detection and suppression of SLF populations. A central,
tree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF
identification. Three specialized robotic spokes perform targeted tasks: pest
neutralization, environmental monitoring, and navigation/mapping. Field
deployment across multiple infested sites over 5 weeks demonstrated
LanternNet's efficacy. Quantitative analysis revealed significant reductions (p
< 0.01, paired t-tests) in SLF populations and corresponding improvements in
tree health indicators across the majority of test sites. Compared to
conventional methods, LanternNet offers substantial cost advantages and
improved scalability. Furthermore, the system's adaptability for enhanced
autonomy and targeting of other invasive species presents significant potential
for broader ecological impact. LanternNet demonstrates the transformative
potential of integrating robotics and AI for advanced invasive species
management and improved environmental outcomes.

</details>


### [38] [Hanging Around: Cognitive Inspired Reasoning for Reactive Robotics](https://arxiv.org/abs/2507.20832)
*Mihai Pomarlan,Stefano De Giorgis,Rachel Ringe,Maria M. Hedblom,Nikolaos Tsiogkas*

Main category: cs.RO

TL;DR: 论文提出了一种结合神经与符号处理的模块化架构，用于提升机器人在自然环境中对目标相关环境元素的识别与监控能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在自然环境中面临的挑战，如空间感知、对象功能检测、动态变化和不可预测性，特别是如何识别和监控与其目标相关的环境元素。

Method: 采用神经符号模块化架构，结合神经网络的物体识别与图像处理技术（如光流）以及符号表示与推理。推理系统基于具身认知范式，通过本体结构整合图像图式知识。

Result: 在模拟环境中，机器人通过学习识别与支撑关系相关的物体部分，能够自主获取训练数据并调整感知能力，从而更有效地规划复杂任务。

Conclusion: 该架构展示了机器人通过观察系统性地扩展知识的能力，凸显了深度推理与感知结合的潜力。

Abstract: Situationally-aware artificial agents operating with competence in natural
environments face several challenges: spatial awareness, object affordance
detection, dynamic changes and unpredictability. A critical challenge is the
agent's ability to identify and monitor environmental elements pertinent to its
objectives. Our research introduces a neurosymbolic modular architecture for
reactive robotics. Our system combines a neural component performing object
recognition over the environment and image processing techniques such as
optical flow, with symbolic representation and reasoning. The reasoning system
is grounded in the embodied cognition paradigm, via integrating image schematic
knowledge in an ontological structure. The ontology is operatively used to
create queries for the perception system, decide on actions, and infer
entities' capabilities derived from perceptual data. The combination of
reasoning and image processing allows the agent to focus its perception for
normal operation as well as discover new concepts for parts of objects involved
in particular interactions. The discovered concepts allow the robot to
autonomously acquire training data and adjust its subsymbolic perception to
recognize the parts, as well as making planning for more complex tasks feasible
by focusing search on those relevant object parts. We demonstrate our approach
in a simulated world, in which an agent learns to recognize parts of objects
involved in support relations. While the agent has no concept of handle
initially, by observing examples of supported objects hanging from a hook it
learns to recognize the parts involved in establishing support and becomes able
to plan the establishment/destruction of the support relation. This underscores
the agent's capability to expand its knowledge through observation in a
systematic way, and illustrates the potential of combining deep reasoning
[...].

</details>


### [39] [Free Energy-Inspired Cognitive Risk Integration for AV Navigation in Pedestrian-Rich Environments](https://arxiv.org/abs/2507.20850)
*Meiting Dang,Yanping Wu,Yafei Wang,Dezong Zhao,David Flynn,Chongfeng Wei*

Main category: cs.RO

TL;DR: 提出了一种基于自由能原理的新框架，用于模拟自动驾驶车辆与行人之间的交互，提升安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶车辆在复杂多代理交互环境中与弱势道路用户（如行人）实现类人预测和决策的挑战。

Method: 结合自由能原理的认知过程建模，提出行人认知风险社会力模型和基于图卷积网络的动态风险感知决策框架。

Result: 仿真结果表明，该框架在安全性、效率和导航平滑性上优于现有方法。

Conclusion: 该框架为自动驾驶车辆与行人的交互提供了更真实的动态模拟和更合理的决策支持。

Abstract: Recent advances in autonomous vehicle (AV) behavior planning have shown
impressive social interaction capabilities when interacting with other road
users. However, achieving human-like prediction and decision-making in
interactions with vulnerable road users remains a key challenge in complex
multi-agent interactive environments. Existing research focuses primarily on
crowd navigation for small mobile robots, which cannot be directly applied to
AVs due to inherent differences in their decision-making strategies and dynamic
boundaries. Moreover, pedestrians in these multi-agent simulations follow fixed
behavior patterns that cannot dynamically respond to AV actions. To overcome
these limitations, this paper proposes a novel framework for modeling
interactions between the AV and multiple pedestrians. In this framework, a
cognitive process modeling approach inspired by the Free Energy Principle is
integrated into both the AV and pedestrian models to simulate more realistic
interaction dynamics. Specifically, the proposed pedestrian Cognitive-Risk
Social Force Model adjusts goal-directed and repulsive forces using a fused
measure of cognitive uncertainty and physical risk to produce human-like
trajectories. Meanwhile, the AV leverages this fused risk to construct a
dynamic, risk-aware adjacency matrix for a Graph Convolutional Network within a
Soft Actor-Critic architecture, allowing it to make more reasonable and
informed decisions. Simulation results indicate that our proposed framework
effectively improves safety, efficiency, and smoothness of AV navigation
compared to the state-of-the-art method.

</details>


### [40] [Uncertainty-aware Planning with Inaccurate Models for Robotized Liquid Handling](https://arxiv.org/abs/2507.20861)
*Marco Faroni,Carlo Odesco,Andrea Zanchettin,Paolo Rocco*

Main category: cs.RO

TL;DR: 提出了一种基于不确定性的蒙特卡洛树搜索（MCTS）算法，用于提高机器人任务中模型预测的准确性，特别是在液体倾倒任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 物理仿真和学习模型在复杂机器人任务中常因认知不确定性或仿真与现实的差距而精度不足，例如液体倾倒任务。

Method: 通过将模型不确定性估计融入MCTS算法，优先选择预测不确定性较低的动作。

Result: 在液体倾倒任务中，该方法即使在数据有限的情况下也能提高成功率，优于传统方法。

Conclusion: 该方法为机器人决策提供了更可靠的规划策略，尤其在不确定性条件下表现突出。

Abstract: Physics-based simulations and learning-based models are vital for complex
robotics tasks like deformable object manipulation and liquid handling.
However, these models often struggle with accuracy due to epistemic uncertainty
or the sim-to-real gap. For instance, accurately pouring liquid from one
container to another poses challenges, particularly when models are trained on
limited demonstrations and may perform poorly in novel situations. This paper
proposes an uncertainty-aware Monte Carlo Tree Search (MCTS) algorithm designed
to mitigate these inaccuracies. By incorporating estimates of model
uncertainty, the proposed MCTS strategy biases the search towards actions with
lower predicted uncertainty. This approach enhances the reliability of planning
under uncertain conditions. Applied to a liquid pouring task, our method
demonstrates improved success rates even with models trained on minimal data,
outperforming traditional methods and showcasing its potential for robust
decision-making in robotics.

</details>


### [41] [A Human-in-the-loop Approach to Robot Action Replanning through LLM Common-Sense Reasoning](https://arxiv.org/abs/2507.20870)
*Elena Merlo,Marta Lagomarsino,Arash Ajoudani*

Main category: cs.RO

TL;DR: 论文提出了一种结合视觉和自然语言输入的人机协作方法，通过大型语言模型（LLM）增强机器人执行计划，提高适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为非专家提供更易用的机器人编程工具，解决仅依赖视觉输入的局限性，如扩展性和错误缓解不足。

Method: 基于单次RGB视频生成机器人执行计划，并通过自然语言输入LLM进行调整，结合用户目标和常识推理优化计划。

Result: 实验证明该方法能直观有效地纠正视觉错误并调整计划，无需额外演示。

Conclusion: 交互式计划优化和幻觉修正增强了系统鲁棒性，为非专家提供了更高效的机器人编程方案。

Abstract: To facilitate the wider adoption of robotics, accessible programming tools
are required for non-experts. Observational learning enables intuitive human
skills transfer through hands-on demonstrations, but relying solely on visual
input can be inefficient in terms of scalability and failure mitigation,
especially when based on a single demonstration. This paper presents a
human-in-the-loop method for enhancing the robot execution plan, automatically
generated based on a single RGB video, with natural language input to a Large
Language Model (LLM). By including user-specified goals or critical task
aspects and exploiting the LLM common-sense reasoning, the system adjusts the
vision-based plan to prevent potential failures and adapts it based on the
received instructions. Experiments demonstrated the framework intuitiveness and
effectiveness in correcting vision-derived errors and adapting plans without
requiring additional demonstrations. Moreover, interactive plan refinement and
hallucination corrections promoted system robustness.

</details>


### [42] [PixelNav: Towards Model-based Vision-Only Navigation with Topological Graphs](https://arxiv.org/abs/2507.20892)
*Sergey Bakulin,Timur Akhtyamov,Denis Fatykhov,German Devchich,Gonzalo Ferrer*

Main category: cs.RO

TL;DR: 提出一种结合深度学习和模型规划的混合方法，用于移动机器人的纯视觉导航，解决了端到端模型的数据需求大和可解释性差的问题。


<details>
  <summary>Details</summary>
Motivation: 端到端模型虽然灵活，但需要大量训练数据且可解释性有限，限制了实际应用。

Method: 采用分层系统，结合模型预测控制、可通行性估计、视觉地点识别和位姿估计，使用拓扑图表示环境。

Result: 实验证明该方法高效且比端到端方法更具可解释性。

Conclusion: 提出的混合方法在纯视觉导航中表现出色，兼具可扩展性和可解释性。

Abstract: This work proposes a novel hybrid approach for vision-only navigation of
mobile robots, which combines advances of both deep learning approaches and
classical model-based planning algorithms. Today, purely data-driven end-to-end
models are dominant solutions to this problem. Despite advantages such as
flexibility and adaptability, the requirement of a large amount of training
data and limited interpretability are the main bottlenecks for their practical
applications. To address these limitations, we propose a hierarchical system
that utilizes recent advances in model predictive control, traversability
estimation, visual place recognition, and pose estimation, employing
topological graphs as a representation of the target environment. Using such a
combination, we provide a scalable system with a higher level of
interpretability compared to end-to-end approaches. Extensive real-world
experiments show the efficiency of the proposed method.

</details>
