<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 28]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [FORTE: Tactile Force and Slip Sensing on Compliant Fingers for Delicate Manipulation](https://arxiv.org/abs/2506.18960)
*Siqi Shang,Mingyo Seo,Yuke Zhu,Lilly Chin*

Main category: cs.RO

TL;DR: FORTE是一种嵌入柔性夹爪的触觉传感系统，通过3D打印的鳍射线夹爪和内部气通道提供低延迟的力和滑动反馈，适用于抓取易碎物体。


<details>
  <summary>Details</summary>
Motivation: 解决刚性平行夹爪在抓取易碎物体时过度依赖视觉反馈的问题，结合触觉传感和软机器人技术提高响应性和适应性。

Method: 使用3D打印的鳍射线夹爪和内部气通道设计，实现低延迟的力和滑动反馈，确保抓取时施加的力适中且不损坏物体。

Result: FORTE能准确估计0-8N的抓取力（平均误差0.2N），并在100ms内检测滑动事件，抓取易碎物体成功率达98.6%，滑动检测准确率为93%。

Conclusion: FORTE是一种实用且稳健的解决方案，适用于易碎物体的机器人操作。

Abstract: Handling delicate and fragile objects remains a major challenge for robotic
manipulation, especially for rigid parallel grippers. While the simplicity and
versatility of parallel grippers have led to widespread adoption, these
grippers are limited by their heavy reliance on visual feedback. Tactile
sensing and soft robotics can add responsiveness and compliance. However,
existing methods typically involve high integration complexity or suffer from
slow response times. In this work, we introduce FORTE, a tactile sensing system
embedded in compliant gripper fingers. FORTE uses 3D-printed fin-ray grippers
with internal air channels to provide low-latency force and slip feedback.
FORTE applies just enough force to grasp objects without damaging them, while
remaining easy to fabricate and integrate. We find that FORTE can accurately
estimate grasping forces from 0-8 N with an average error of 0.2 N, and detect
slip events within 100 ms of occurring. We demonstrate FORTE's ability to grasp
a wide range of slippery, fragile, and deformable objects. In particular, FORTE
grasps fragile objects like raspberries and potato chips with a 98.6% success
rate, and achieves 93% accuracy in detecting slip events. These results
highlight FORTE's potential as a robust and practical solution for enabling
delicate robotic manipulation. Project page: https://merge-lab.github.io/FORTE

</details>


### [2] [Faster Motion Planning via Restarts](https://arxiv.org/abs/2506.19016)
*Nancy Amato,Stav Ashur,Sariel Har-Peled%*

Main category: cs.RO

TL;DR: 论文提出通过随机重启技术改进PRM和RRT等随机运动规划算法，显著提升运行速度、路径质量和多线程性能。


<details>
  <summary>Details</summary>
Motivation: 随机方法如PRM和RRT在运动规划中广泛应用，但其运行时间存在不稳定性，导致性能下降。

Method: 应用随机重启技术（包括新方法）加速Las Vegas算法。

Result: 实验显示新算法运行更快、路径更短，多线程性能提升显著。

Conclusion: 新算法在理论和实践中均表现优异，且易于部署和使用。

Abstract: Randomized methods such as PRM and RRT are widely used in motion planning.
However, in some cases, their running-time suffers from inherent instability,
leading to ``catastrophic'' performance even for relatively simple instances.
We apply stochastic restart techniques, some of them new, for speeding up Las
Vegas algorithms, that provide dramatic speedups in practice (a factor of $3$
[or larger] in many cases).
  Our experiments demonstrate that the new algorithms have faster runtimes,
shorter paths, and greater gains from multi-threading (when compared with
straightforward parallel implementation). We prove the optimality of the new
variants. Our implementation is open source, available on github, and is easy
to deploy and use.

</details>


### [3] [Multimodal Anomaly Detection with a Mixture-of-Experts](https://arxiv.org/abs/2506.19077)
*Christoph Willibald,Daniel Sliwowski,Dongheui Lee*

Main category: cs.RO

TL;DR: 提出了一种混合专家框架，结合视觉语言模型和高斯混合回归检测器，用于机器人操作中的多模态异常检测，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在多样化应用中的部署增加，需要更鲁棒的多模态异常检测方法，以同时应对机器人驱动和环境驱动的异常。

Method: 采用混合专家框架，结合视觉语言模型和高斯混合回归检测器，并通过置信度融合机制动态选择最可靠的检测器。

Result: 在家庭和工业任务中测试，检测延迟减少60%，帧级异常检测性能优于单一检测器。

Conclusion: 该方法有效整合了互补检测机制，显著提升了机器人操作中的异常检测能力。

Abstract: With a growing number of robots being deployed across diverse applications,
robust multimodal anomaly detection becomes increasingly important. In robotic
manipulation, failures typically arise from (1) robot-driven anomalies due to
an insufficient task model or hardware limitations, and (2) environment-driven
anomalies caused by dynamic environmental changes or external interferences.
Conventional anomaly detection methods focus either on the first by low-level
statistical modeling of proprioceptive signals or the second by deep
learning-based visual environment observation, each with different
computational and training data requirements. To effectively capture anomalies
from both sources, we propose a mixture-of-experts framework that integrates
the complementary detection mechanisms with a visual-language model for
environment monitoring and a Gaussian-mixture regression-based detector for
tracking deviations in interaction forces and robot motions. We introduce a
confidence-based fusion mechanism that dynamically selects the most reliable
detector for each situation. We evaluate our approach on both household and
industrial tasks using two robotic systems, demonstrating a 60% reduction in
detection delay while improving frame-wise anomaly detection performance
compared to individual detectors.

</details>


### [4] [Analysis and experiments of the dissipative Twistcar: direction reversal and asymptotic approximations](https://arxiv.org/abs/2506.19112)
*Rom Levy,Ari Dantus,Zitao Yu,Yizhar Or*

Main category: cs.RO

TL;DR: 研究了一种考虑滚动摩擦的Twistcar两连杆模型，揭示了通过改变几何和质量属性可实现运动方向反转，并构建了机器人原型验证。


<details>
  <summary>Details</summary>
Motivation: 现有研究未考虑摩擦能量耗散，本研究填补了这一空白，探索了Twistcar在摩擦作用下的动力学行为。

Method: 建立理论两连杆模型，结合滚动摩擦，分析小振幅稳态周期动力学，并通过机器人原型实验验证。

Result: 发现运动方向反转现象，并通过参数拟合提高了理论与实验的一致性。

Conclusion: 摩擦对Twistcar动力学有显著影响，方向反转现象为实际应用提供了新思路。

Abstract: Underactuated wheeled vehicles are commonly studied as nonholonomic systems
with periodic actuation. Twistcar is a classical example inspired by a riding
toy, which has been analyzed using a planar model of a dynamical system with
nonholonomic constraints. Most of the previous analyses did not account for
energy dissipation due to friction. In this work, we study a theoretical
two-link model of the Twistcar while incorporating dissipation due to rolling
resistance. We obtain asymptotic expressions for the system's small-amplitude
steady-state periodic dynamics, which reveals the possibility of reversing the
direction of motion upon varying the geometric and mass properties of the
vehicle. Next, we design and construct a robotic prototype of the Twistcar
whose center-of-mass position can be shifted by adding and removing a massive
block, enabling demonstration of the Twistcar's direction reversal phenomenon.
We also conduct parameter fitting for the frictional resistance in order to
improve agreement with experiments.

</details>


### [5] [CUPID: Curating Data your Robot Loves with Influence Functions](https://arxiv.org/abs/2506.19121)
*Christopher Agia,Rohan Sinha,Jingyun Yang,Rika Antonova,Marco Pavone,Haruki Nishimura,Masha Itkina,Jeannette Bohg*

Main category: cs.RO

TL;DR: CUPID是一种基于影响函数理论的机器人数据筛选方法，用于优化模仿学习策略的性能。


<details>
  <summary>Details</summary>
Motivation: 模仿学习中策略性能与演示数据质量密切相关，但理解单个演示对任务成功的影响仍具挑战性。

Method: 提出CUPID，通过影响函数理论估计每个训练演示对策略预期回报的影响，从而筛选和优化数据。

Result: 实验表明，CUPID能显著提升策略性能，仅用33%的筛选数据即可达到最佳效果，并在硬件实验中验证了其鲁棒性。

Conclusion: CUPID为机器人模仿学习提供了一种高效的数据筛选方法，显著提升了策略性能和鲁棒性。

Abstract: In robot imitation learning, policy performance is tightly coupled with the
quality and composition of the demonstration data. Yet, developing a precise
understanding of how individual demonstrations contribute to downstream
outcomes - such as closed-loop task success or failure - remains a persistent
challenge. We propose CUPID, a robot data curation method based on a novel
influence function-theoretic formulation for imitation learning policies. Given
a set of evaluation rollouts, CUPID estimates the influence of each training
demonstration on the policy's expected return. This enables ranking and
selection of demonstrations according to their impact on the policy's
closed-loop performance. We use CUPID to curate data by 1) filtering out
training demonstrations that harm policy performance and 2) subselecting newly
collected trajectories that will most improve the policy. Extensive simulated
and hardware experiments show that our approach consistently identifies which
data drives test-time performance. For example, training with less than 33% of
curated data can yield state-of-the-art diffusion policies on the simulated
RoboMimic benchmark, with similar gains observed in hardware. Furthermore,
hardware experiments show that our method can identify robust strategies under
distribution shift, isolate spurious correlations, and even enhance the
post-training of generalist robot policies. Additional materials are made
available at: https://cupid-curation.github.io.

</details>


### [6] [Situated Haptic Interaction: Exploring the Role of Context in Affective Perception of Robotic Touch](https://arxiv.org/abs/2506.19179)
*Qiaoqiao Ren,Tony Belpaeme*

Main category: cs.RO

TL;DR: 研究探讨情境情绪线索如何影响机器人触觉信号的感知，发现视觉情境常主导触觉信号的效价感知，触觉反馈能增强或减弱交互的情感强度。


<details>
  <summary>Details</summary>
Motivation: 情感计算中触觉反馈在动态情感交流中的作用尚未充分探索，研究旨在填补这一空白。

Method: 32名参与者观看机器人经历不同情感动作的视频，并通过穿戴振动袖接收触觉反馈，评估机器人的情感状态。

Result: 视觉情境主导触觉信号的效价感知，负面触觉线索增强交互效价，正面线索减弱效价，触觉反馈还影响唤醒度感知。

Conclusion: 情境触觉反馈能丰富人机情感交互，为机器情感交流提供更细致和具身化的方法。

Abstract: Affective interaction is not merely about recognizing emotions; it is an
embodied, situated process shaped by context and co-created through
interaction. In affective computing, the role of haptic feedback within dynamic
emotional exchanges remains underexplored. This study investigates how
situational emotional cues influence the perception and interpretation of
haptic signals given by a robot. In a controlled experiment, 32 participants
watched video scenarios in which a robot experienced either positive actions
(such as being kissed), negative actions (such as being slapped) or neutral
actions. After each video, the robot conveyed its emotional response through
haptic communication, delivered via a wearable vibration sleeve worn by the
participant. Participants rated the robot's emotional state-its valence
(positive or negative) and arousal (intensity)-based on the video, the haptic
feedback, and the combination of the two. The study reveals a dynamic interplay
between visual context and touch. Participants' interpretation of haptic
feedback was strongly shaped by the emotional context of the video, with visual
context often overriding the perceived valence of the haptic signal. Negative
haptic cues amplified the perceived valence of the interaction, while positive
cues softened it. Furthermore, haptics override the participants' perception of
arousal of the video. Together, these results offer insights into how situated
haptic feedback can enrich affective human-robot interaction, pointing toward
more nuanced and embodied approaches to emotional communication with machines.

</details>


### [7] [The MOTIF Hand: A Robotic Hand for Multimodal Observations with Thermal, Inertial, and Force Sensors](https://arxiv.org/abs/2506.19201)
*Hanyang Zhou,Haozhe Lou,Wenhao Liu,Enyu Zhao,Yue Wang,Daniel Seita*

Main category: cs.RO

TL;DR: 提出了一种名为MOTIF的多模态机器人手，扩展了LEAP手的功能，集成了多种传感器，用于增强灵巧操作。


<details>
  <summary>Details</summary>
Motivation: 现有机器人手缺乏热感和扭矩感测能力，限制了其在复杂任务中的应用。

Method: 设计了MOTIF手，集成了密集触觉信息、深度传感器、热像仪、IMU传感器和视觉传感器。

Result: 实验验证了MOTIF手在温度感知抓取和区分外观相同但质量不同的物体方面的能力。

Conclusion: MOTIF手是一种低成本、易复制的多模态机器人手，扩展了机器人手的感知能力。

Abstract: Advancing dexterous manipulation with multi-fingered robotic hands requires
rich sensory capabilities, while existing designs lack onboard thermal and
torque sensing. In this work, we propose the MOTIF hand, a novel multimodal and
versatile robotic hand that extends the LEAP hand by integrating: (i) dense
tactile information across the fingers, (ii) a depth sensor, (iii) a thermal
camera, (iv), IMU sensors, and (v) a visual sensor. The MOTIF hand is designed
to be relatively low-cost (under 4000 USD) and easily reproducible. We validate
our hand design through experiments that leverage its multimodal sensing for
two representative tasks. First, we integrate thermal sensing into 3D
reconstruction to guide temperature-aware, safe grasping. Second, we show how
our hand can distinguish objects with identical appearance but different masses
- a capability beyond methods that use vision only.

</details>


### [8] [Preserving Sense of Agency: User Preferences for Robot Autonomy and User Control across Household Tasks](https://arxiv.org/abs/2506.19202)
*Claire Yang,Heer Patel,Max Kleiman-Weiner,Maya Cakmak*

Main category: cs.RO

TL;DR: 研究探讨用户对辅助机器人自主性的偏好及其对用户控制感的影响，发现自主性和第三方参与是关键因素，高风险任务中用户更倾向控制。


<details>
  <summary>Details</summary>
Motivation: 探讨用户是否偏好高度自主的辅助机器人，以及自主性如何影响用户的控制感。

Method: 通过实验让参与者评估不同自主性水平下对机器人的控制感，并针对不同家庭任务排名偏好。

Result: 用户控制感受到机器人自主性和第三方参与的影响；高风险任务中用户更倾向控制。

Conclusion: 辅助机器人设计需平衡自主性与用户控制感，尤其在高风险任务中。

Abstract: Roboticists often design with the assumption that assistive robots should be
fully autonomous. However, it remains unclear whether users prefer highly
autonomous robots, as prior work in assistive robotics suggests otherwise. High
robot autonomy can reduce the user's sense of agency, which represents feeling
in control of one's environment. How much control do users, in fact, want over
the actions of robots used for in-home assistance? We investigate how robot
autonomy levels affect users' sense of agency and the autonomy level they
prefer in contexts with varying risks. Our study asked participants to rate
their sense of agency as robot users across four distinct autonomy levels and
ranked their robot preferences with respect to various household tasks. Our
findings revealed that participants' sense of agency was primarily influenced
by two factors: (1) whether the robot acts autonomously, and (2) whether a
third party is involved in the robot's programming or operation. Notably, an
end-user programmed robot highly preserved users' sense of agency, even though
it acts autonomously. However, in high-risk settings, e.g., preparing a snack
for a child with allergies, they preferred robots that prioritized their
control significantly more. Additional contextual factors, such as trust in a
third party operator, also shaped their preferences.

</details>


### [9] [Scaffolding Dexterous Manipulation with Vision-Language Models](https://arxiv.org/abs/2506.19212)
*Vincent de Bakker,Joey Hejna,Tyler Ga Wei Lum,Onur Celik,Aleksandar Taranovic,Denis Blessing,Gerhard Neumann,Jeannette Bohg,Dorsa Sadigh*

Main category: cs.RO

TL;DR: 论文提出了一种利用视觉语言模型（VLM）生成任务相关轨迹的方法，用于训练灵巧机器人手的强化学习策略，无需人工演示或手工设计奖励。


<details>
  <summary>Details</summary>
Motivation: 灵巧机器人手的训练面临数据收集和高维控制的挑战，传统强化学习依赖任务特定的奖励函数，限制了可扩展性和泛化能力。

Method: 使用现成的VLM识别任务关键点并合成3D轨迹，训练低层强化学习策略跟踪这些轨迹。

Result: 在模拟任务中成功学习到鲁棒的灵巧操作策略，并能迁移到真实机器人手。

Conclusion: 该方法通过VLM生成的轨迹解决了数据瓶颈，实现了无需人工干预的灵巧操作训练。

Abstract: Dexterous robotic hands are essential for performing complex manipulation
tasks, yet remain difficult to train due to the challenges of demonstration
collection and high-dimensional control. While reinforcement learning (RL) can
alleviate the data bottleneck by generating experience in simulation, it
typically relies on carefully designed, task-specific reward functions, which
hinder scalability and generalization. Thus, contemporary works in dexterous
manipulation have often bootstrapped from reference trajectories. These
trajectories specify target hand poses that guide the exploration of RL
policies and object poses that enable dense, task-agnostic rewards. However,
sourcing suitable trajectories - particularly for dexterous hands - remains a
significant challenge. Yet, the precise details in explicit reference
trajectories are often unnecessary, as RL ultimately refines the motion. Our
key insight is that modern vision-language models (VLMs) already encode the
commonsense spatial and semantic knowledge needed to specify tasks and guide
exploration effectively. Given a task description (e.g., "open the cabinet")
and a visual scene, our method uses an off-the-shelf VLM to first identify
task-relevant keypoints (e.g., handles, buttons) and then synthesize 3D
trajectories for hand motion and object motion. Subsequently, we train a
low-level residual RL policy in simulation to track these coarse trajectories
or "scaffolds" with high fidelity. Across a number of simulated tasks involving
articulated objects and semantic understanding, we demonstrate that our method
is able to learn robust dexterous manipulation policies. Moreover, we showcase
that our method transfers to real-world robotic hands without any human
demonstrations or handcrafted rewards.

</details>


### [10] [AnchorDP3: 3D Affordance Guided Sparse Diffusion Policy for Robotic Manipulation](https://arxiv.org/abs/2506.19269)
*Ziyan Zhao,Ke Fan,He-Yang Xu,Ning Qiao,Bo Peng,Wenlong Gao,Dongjiang Li,Hui Shen*

Main category: cs.RO

TL;DR: AnchorDP3是一种用于双臂机器人操作的扩散策略框架，通过三项创新技术在高度随机化环境中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决在高度随机化环境中机器人操作的挑战，减少对人类示范的依赖。

Method: 结合模拟器监督语义分割、任务条件特征编码器和基于关键姿势的扩散策略，简化预测空间并提升效率。

Result: 在RoboTwin基准测试中达到98.7%的平均成功率。

Conclusion: AnchorDP3框架有望实现完全自主的视觉运动策略生成，无需人类示范。

Abstract: We present AnchorDP3, a diffusion policy framework for dual-arm robotic
manipulation that achieves state-of-the-art performance in highly randomized
environments. AnchorDP3 integrates three key innovations: (1)
Simulator-Supervised Semantic Segmentation, using rendered ground truth to
explicitly segment task-critical objects within the point cloud, which provides
strong affordance priors; (2) Task-Conditioned Feature Encoders, lightweight
modules processing augmented point clouds per task, enabling efficient
multi-task learning through a shared diffusion-based action expert; (3)
Affordance-Anchored Keypose Diffusion with Full State Supervision, replacing
dense trajectory prediction with sparse, geometrically meaningful action
anchors, i.e., keyposes such as pre-grasp pose, grasp pose directly anchored to
affordances, drastically simplifying the prediction space; the action expert is
forced to predict both robot joint angles and end-effector poses
simultaneously, which exploits geometric consistency to accelerate convergence
and boost accuracy. Trained on large-scale, procedurally generated simulation
data, AnchorDP3 achieves a 98.7% average success rate in the RoboTwin benchmark
across diverse tasks under extreme randomization of objects, clutter, table
height, lighting, and backgrounds. This framework, when integrated with the
RoboTwin real-to-sim pipeline, has the potential to enable fully autonomous
generation of deployable visuomotor policies from only scene and instruction,
totally eliminating human demonstrations from learning manipulation skills.

</details>


### [11] [Ontology Neural Network and ORTSF: A Framework for Topological Reasoning and Delay-Robust Control](https://arxiv.org/abs/2506.19277)
*Jaehong Oh*

Main category: cs.RO

TL;DR: 论文提出了一种结合本体神经网络（ONN）和本体实时语义框架（ORTSF）的统一架构，以解决自主机器人系统在关系语义和认知透明度方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有机器人系统在几何推理和动态稳定性方面表现优异，但在关系语义表示、上下文推理和认知透明度方面存在不足，阻碍了其在动态、以人为中心的环境中的协作能力。

Method: ONN通过动态拓扑过程形式化关系语义推理，嵌入Forman-Ricci曲率、持久同调和语义张量结构，确保关系完整性和拓扑一致性。ORTSF将推理痕迹转化为控制命令，并补偿系统延迟，确保控制信号的连续性和相位裕度。

Result: 实证研究表明，ONN + ORTSF框架能够统一语义认知和鲁棒控制，为认知机器人提供了数学上严谨且实际可行的解决方案。

Conclusion: 该框架填补了现有机器人系统在语义和认知能力上的空白，为动态、人机协作环境提供了新的解决方案。

Abstract: The advancement of autonomous robotic systems has led to impressive
capabilities in perception, localization, mapping, and control. Yet, a
fundamental gap remains: existing frameworks excel at geometric reasoning and
dynamic stability but fall short in representing and preserving relational
semantics, contextual reasoning, and cognitive transparency essential for
collaboration in dynamic, human-centric environments. This paper introduces a
unified architecture comprising the Ontology Neural Network (ONN) and the
Ontological Real-Time Semantic Fabric (ORTSF) to address this gap. The ONN
formalizes relational semantic reasoning as a dynamic topological process. By
embedding Forman-Ricci curvature, persistent homology, and semantic tensor
structures within a unified loss formulation, ONN ensures that relational
integrity and topological coherence are preserved as scenes evolve over time.
The ORTSF transforms reasoning traces into actionable control commands while
compensating for system delays. It integrates predictive and delay-aware
operators that ensure phase margin preservation and continuity of control
signals, even under significant latency conditions. Empirical studies
demonstrate the ONN + ORTSF framework's ability to unify semantic cognition and
robust control, providing a mathematically principled and practically viable
solution for cognitive robotics.

</details>


### [12] [Robotic Perception with a Large Tactile-Vision-Language Model for Physical Property Inference](https://arxiv.org/abs/2506.19303)
*Zexiang Guo,Hengxiang Chen,Xinheng Mai,Qiusang Qiu,Gan Ma,Zhanat Kappassov,Qiang Li,Nutan Chen*

Main category: cs.RO

TL;DR: 提出了一种新的跨模态感知框架，结合视觉和触觉数据，通过多模态视觉语言模型提升机器人对物体物理属性的推理能力。


<details>
  <summary>Details</summary>
Motivation: 通过更全面地捕捉物体物理属性，提升机器人安全高效操作物体的能力。

Method: 采用分层特征对齐机制和改进的提示策略，整合视觉和触觉数据。

Result: 在35种不同物体上评估，性能优于现有基线，并展示了强大的零样本泛化能力。

Conclusion: 跨模态感知框架显著提升了机器人对物体物理属性的推理能力，具有实际应用潜力。

Abstract: Inferring physical properties can significantly enhance robotic manipulation
by enabling robots to handle objects safely and efficiently through adaptive
grasping strategies. Previous approaches have typically relied on either
tactile or visual data, limiting their ability to fully capture properties. We
introduce a novel cross-modal perception framework that integrates visual
observations with tactile representations within a multimodal vision-language
model. Our physical reasoning framework, which employs a hierarchical feature
alignment mechanism and a refined prompting strategy, enables our model to make
property-specific predictions that strongly correlate with ground-truth
measurements. Evaluated on 35 diverse objects, our approach outperforms
existing baselines and demonstrates strong zero-shot generalization. Keywords:
tactile perception, visual-tactile fusion, physical property inference,
multimodal integration, robot perception

</details>


### [13] [Zero-Shot Parameter Learning of Robot Dynamics Using Bayesian Statistics and Prior Knowledge](https://arxiv.org/abs/2506.19350)
*Carsten Reiners,Minh Trinh,Lukas Gründel,Sven Tauchmann,David Bitterolf,Oliver Petrovic,Christian Brecher*

Main category: cs.RO

TL;DR: 提出了一种基于贝叶斯统计的惯性参数识别方法，结合先验知识，减少测量需求，实现零样本学习。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如最小二乘法或机器学习）未充分利用机器人先验信息且需大量测量，限制了效率和泛化能力。

Method: 采用贝叶斯统计方法，结合先验知识，通过少量或无额外测量（零样本学习）识别惯性、机械和基础参数。

Result: 成功识别MABI Max 100机器人的参数，确保物理可行性并提供置信区间；还为6自由度串联机器人提供了先验类型。

Conclusion: 该方法显著提升了参数识别的泛化能力和效率，适用于先验信息有限或无CAD模型的场景。

Abstract: Inertial parameter identification of industrial robots is an established
process, but standard methods using Least Squares or Machine Learning do not
consider prior information about the robot and require extensive measurements.
Inspired by Bayesian statistics, this paper presents an identification method
with improved generalization that incorporates prior knowledge and is able to
learn with only a few or without additional measurements (Zero-Shot Learning).
Furthermore, our method is able to correctly learn not only the inertial but
also the mechanical and base parameters of the MABI Max 100 robot while
ensuring physical feasibility and specifying the confidence intervals of the
results. We also provide different types of priors for serial robots with 6
degrees of freedom, where datasheets or CAD models are not available.

</details>


### [14] [A Survey on Soft Robot Adaptability: Implementations, Applications, and Prospects](https://arxiv.org/abs/2506.19397)
*Zixi Chen,Di Wu,Qinghua Guan,David Hardman,Federico Renda,Josie Hughes,Thomas George Thuruthel,Cosimo Della Santina,Barbara Mazzolai,Huichan Zhao,Cesare Stefanini*

Main category: cs.RO

TL;DR: 本文综述了软体机器人适应性的分类、实现方法及其应用，并探讨了当前局限性和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 软体机器人因其高自由度、柔顺性和安全性在多个领域应用广泛，适应性是其关键特性之一。本文旨在全面分析软体机器人的适应性，以推动其进一步发展。

Method: 将适应性分为外部和内部两类，总结了通过设计、传感和控制策略提升适应性的方法，并评估了其在手术、可穿戴设备等领域的应用。

Result: 适应性在软体机器人中具有显著重要性，但其实现仍面临局限，如材料老化和控制策略的通用性。

Conclusion: 未来研究应关注克服现有局限，进一步提升软体机器人的适应性，以拓展其应用范围。

Abstract: Soft robots, compared to rigid robots, possess inherent advantages, including
higher degrees of freedom, compliance, and enhanced safety, which have
contributed to their increasing application across various fields. Among these
benefits, adaptability is particularly noteworthy. In this paper, adaptability
in soft robots is categorized into external and internal adaptability. External
adaptability refers to the robot's ability to adjust, either passively or
actively, to variations in environments, object properties, geometries, and
task dynamics. Internal adaptability refers to the robot's ability to cope with
internal variations, such as manufacturing tolerances or material aging, and to
generalize control strategies across different robots. As the field of soft
robotics continues to evolve, the significance of adaptability has become
increasingly pronounced. In this review, we summarize various approaches to
enhancing the adaptability of soft robots, including design, sensing, and
control strategies. Additionally, we assess the impact of adaptability on
applications such as surgery, wearable devices, locomotion, and manipulation.
We also discuss the limitations of soft robotics adaptability and prospective
directions for future research. By analyzing adaptability through the lenses of
implementation, application, and challenges, this paper aims to provide a
comprehensive understanding of this essential characteristic in soft robotics
and its implications for diverse applications.

</details>


### [15] [Ground-Effect-Aware Modeling and Control for Multicopters](https://arxiv.org/abs/2506.19424)
*Tiankai Yang,Kaixin Chai,Jialin Ji,Yuze Wu,Chao Xu,Fei Gao*

Main category: cs.RO

TL;DR: 论文研究了多旋翼飞行器在近地飞行时的地面效应问题，提出了结合动态逆和扰动模型的控制方法，显著降低了控制误差。


<details>
  <summary>Details</summary>
Motivation: 地面效应对多旋翼飞行器带来额外升力、外部扭矩引起的振荡以及地面气流对模型的影响等挑战，需要解决。

Method: 通过力测量平台和实际飞行收集数据，总结地面效应下的外部扭矩数学模型，验证地面气流对转子阻力和混合矩阵的影响，提出动态逆与扰动模型结合的控制方法。

Result: 实验表明，该方法将控制误差（RMSE）降低了45.3%。

Conclusion: 提出的控制方法有效解决了地面效应对多旋翼飞行器控制的影响，确保了高低空飞行时的一致性控制效果。

Abstract: The ground effect on multicopters introduces several challenges, such as
control errors caused by additional lift, oscillations that may occur during
near-ground flight due to external torques, and the influence of ground airflow
on models such as the rotor drag and the mixing matrix. This article collects
and analyzes the dynamics data of near-ground multicopter flight through
various methods, including force measurement platforms and real-world flights.
For the first time, we summarize the mathematical model of the external torque
of multicopters under ground effect. The influence of ground airflow on rotor
drag and the mixing matrix is also verified through adequate experimentation
and analysis. Through simplification and derivation, the differential flatness
of the multicopter's dynamic model under ground effect is confirmed. To
mitigate the influence of these disturbance models on control, we propose a
control method that combines dynamic inverse and disturbance models, ensuring
consistent control effectiveness at both high and low altitudes. In this
method, the additional thrust and variations in rotor drag under ground effect
are both considered and compensated through feedforward models. The leveling
torque of ground effect can be equivalently represented as variations in the
center of gravity and the moment of inertia. In this way, the leveling torque
does not explicitly appear in the dynamic model. The final experimental results
show that the method proposed in this paper reduces the control error (RMSE) by
\textbf{45.3\%}. Please check the supplementary material at:
https://github.com/ZJU-FAST-Lab/Ground-effect-controller.

</details>


### [16] [T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models](https://arxiv.org/abs/2506.19498)
*Yiteng Chen,Wenbo Li,Shiyi Wang,Huiping Zhuang,Qingyao Wu*

Main category: cs.RO

TL;DR: T-Rex是一个任务自适应的空间表示提取框架，动态选择最适合任务需求的表示方案，提升机器人操作的效率与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型（VLM）的机器人方法采用固定空间表示方案，导致表示能力不足或提取时间过长。

Method: 提出T-Rex框架，根据任务复杂度动态选择空间表示类型和粒度。

Result: 实验表明，T-Rex在空间理解、效率和稳定性方面具有显著优势，且无需额外训练。

Conclusion: T-Rex通过任务自适应表示提取，为机器人操作提供了更高效和稳定的解决方案。

Abstract: Building a general robotic manipulation system capable of performing a wide
variety of tasks in real-world settings is a challenging task. Vision-Language
Models (VLMs) have demonstrated remarkable potential in robotic manipulation
tasks, primarily due to the extensive world knowledge they gain from
large-scale datasets. In this process, Spatial Representations (such as points
representing object positions or vectors representing object orientations) act
as a bridge between VLMs and real-world scene, effectively grounding the
reasoning abilities of VLMs and applying them to specific task scenarios.
However, existing VLM-based robotic approaches often adopt a fixed spatial
representation extraction scheme for various tasks, resulting in insufficient
representational capability or excessive extraction time. In this work, we
introduce T-Rex, a Task-Adaptive Framework for Spatial Representation
Extraction, which dynamically selects the most appropriate spatial
representation extraction scheme for each entity based on specific task
requirements. Our key insight is that task complexity determines the types and
granularity of spatial representations, and Stronger representational
capabilities are typically associated with Higher overall system operation
costs. Through comprehensive experiments in real-world robotic environments, we
show that our approach delivers significant advantages in spatial
understanding, efficiency, and stability without additional training.

</details>


### [17] [Fake or Real, Can Robots Tell? Evaluating Embodied Vision-Language Models on Real and 3D-Printed Objects](https://arxiv.org/abs/2506.19579)
*Federico Tavella,Kathryn Mearns,Angelo Cangelosi*

Main category: cs.RO

TL;DR: 比较了机器人场景理解中不同视觉语言模型（VLMs）的标题生成策略，评估了单视角与多视角、真实物体与3D打印物体的识别效果。


<details>
  <summary>Details</summary>
Motivation: 研究机器人如何利用视觉语言模型（VLMs）生成环境描述，以提升场景理解能力。

Method: 通过机器人手臂采集多视角图像，比较BLIP和VLMs等模型的标题生成性能，分析单视角与多视角、真实物体与3D打印物体的差异。

Result: VLMs在常见物体识别中表现良好，但对新颖表征泛化能力不足。

Conclusion: 研究为实际部署基础模型于机器人场景提供了实用见解。

Abstract: Robotic scene understanding increasingly relies on vision-language models
(VLMs) to generate natural language descriptions of the environment. In this
work, we present a comparative study of captioning strategies for tabletop
scenes captured by a robotic arm equipped with an RGB camera. The robot
collects images of objects from multiple viewpoints, and we evaluate several
models that generate scene descriptions. We compare the performance of various
captioning models, like BLIP and VLMs. Our experiments examine the trade-offs
between single-view and multi-view captioning, and difference between
recognising real-world and 3D printed objects. We quantitatively evaluate
object identification accuracy, completeness, and naturalness of the generated
captions. Results show that VLMs can be used in robotic settings where common
objects need to be recognised, but fail to generalise to novel representations.
Our findings provide practical insights into deploying foundation models for
embodied agents in real-world settings.

</details>


### [18] [Robotics Under Construction: Challenges on Job Sites](https://arxiv.org/abs/2506.19597)
*Haruki Uchiito,Akhilesh Bhat,Koji Kusaka,Xiaoya Zhang,Hiraku Kinjo,Honoka Uehara,Motoki Koyama,Shinji Natsume*

Main category: cs.RO

TL;DR: 本文提出了一种基于CD110R-3履带式运输车的自主载荷运输系统，旨在解决建筑行业劳动力短缺和生产力停滞问题，并探索无人建筑工地的可能性。


<details>
  <summary>Details</summary>
Motivation: 建筑行业面临劳动力短缺和生产力停滞的挑战，自动化成为可持续基础设施发展的关键。

Method: 系统集成了自主导航、车队管理和基于GNSS的定位技术，用于建筑工地环境中的材料运输。

Result: 初步结果揭示了导航、环境感知和传感器优化等潜在挑战。

Conclusion: 本文为机器人驱动的建筑自动化提供了基础性见解，并指出了未来技术发展的关键方向。

Abstract: As labor shortages and productivity stagnation increasingly challenge the
construction industry, automation has become essential for sustainable
infrastructure development. This paper presents an autonomous payload
transportation system as an initial step toward fully unmanned construction
sites. Our system, based on the CD110R-3 crawler carrier, integrates autonomous
navigation, fleet management, and GNSS-based localization to facilitate
material transport in construction site environments. While the current system
does not yet incorporate dynamic environment adaptation algorithms, we have
begun fundamental investigations into external-sensor based perception and
mapping system. Preliminary results highlight the potential challenges,
including navigation in evolving terrain, environmental perception under
construction-specific conditions, and sensor placement optimization for
improving autonomy and efficiency. Looking forward, we envision a construction
ecosystem where collaborative autonomous agents dynamically adapt to site
conditions, optimizing workflow and reducing human intervention. This paper
provides foundational insights into the future of robotics-driven construction
automation and identifies critical areas for further technological development.

</details>


### [19] [Soft Robotic Delivery of Coiled Anchors for Cardiac Interventions](https://arxiv.org/abs/2506.19602)
*Leonardo Zamora Yanez,Jacob Rogatinsky,Dominic Recco,Sang-Yoep Lee,Grace Matthews,Andrew P. Sabelhaus,Tommaso Ranzani*

Main category: cs.RO

TL;DR: 论文介绍了一种用于心脏内锚定线圈植入的机器人平台，解决了现有导管平台灵活性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前导管平台在复杂心脏内手术中缺乏灵活性、施力和顺应性，限制了其在高风险患者中的应用。

Method: 开发了一种机器人平台，建立了其运动静力学模型，并展示了低位置误差。

Result: 在多锚定植入过程中，机器人平台表现出毫米级精度。

Conclusion: 该机器人平台为复杂心脏内手术提供了一种更灵活、精确的解决方案。

Abstract: Trans-catheter cardiac intervention has become an increasingly available
option for high-risk patients without the complications of open heart surgery.
However, current catheterbased platforms suffer from a lack of dexterity, force
application, and compliance required to perform complex intracardiac
procedures. An exemplary task that would significantly ease minimally invasive
intracardiac procedures is the implantation of anchor coils, which can be used
to fix and implant various devices in the beating heart. We introduce a robotic
platform capable of delivering anchor coils. We develop a kineto-statics model
of the robotic platform and demonstrate low positional error. We leverage the
passive compliance and high force output of the actuator in a multi-anchor
delivery procedure against a motile in-vitro simulator with millimeter level
accuracy.

</details>


### [20] [Probabilistic modelling and safety assurance of an agriculture robot providing light-treatment](https://arxiv.org/abs/2506.19620)
*Mustafa Adam,Kangfeng Ye,David A. Anisi,Ana Cavalcanti,Jim Woodcock,Robert Morris*

Main category: cs.RO

TL;DR: 本文提出了一种用于农业机器人安全保证的概率建模和风险分析框架，重点关注其检测、跟踪和避障能力。


<details>
  <summary>Details</summary>
Motivation: 农民对农业机器人可靠性、鲁棒性和安全性的信任是其持续采用的关键，因此需要确保其安全性。

Method: 通过危险识别和风险评估矩阵，使用三个状态机建模机器人平台、传感器系统及人类行为，并利用PRISM概率模型检查器分析。

Result: 量化了风险缓解措施的效果，比较了不同设计概念（如高性能物体检测系统或更复杂的警告系统）的风险降低。

Conclusion: 该框架不仅适用于概念开发阶段，还可用于实施、部署和操作阶段。

Abstract: Continued adoption of agricultural robots postulates the farmer's trust in
the reliability, robustness and safety of the new technology. This motivates
our work on safety assurance of agricultural robots, particularly their ability
to detect, track and avoid obstacles and humans. This paper considers a
probabilistic modelling and risk analysis framework for use in the early
development phases. Starting off with hazard identification and a risk
assessment matrix, the behaviour of the mobile robot platform, sensor and
perception system, and any humans present are captured using three state
machines. An auto-generated probabilistic model is then solved and analysed
using the probabilistic model checker PRISM. The result provides unique insight
into fundamental development and engineering aspects by quantifying the effect
of the risk mitigation actions and risk reduction associated with distinct
design concepts. These include implications of adopting a higher performance
and more expensive Object Detection System or opting for a more elaborate
warning system to increase human awareness. Although this paper mainly focuses
on the initial concept-development phase, the proposed safety assurance
framework can also be used during implementation, and subsequent deployment and
operation phases.

</details>


### [21] [A Verification Methodology for Safety Assurance of Robotic Autonomous Systems](https://arxiv.org/abs/2506.19622)
*Mustafa Adam,David A. Anisi,Pedro Ribeiro*

Main category: cs.RO

TL;DR: 本文提出了一种用于农业自主机器人安全验证的工作流程，涵盖从概念设计到运行时验证的整个开发生命周期。


<details>
  <summary>Details</summary>
Motivation: 在动态、非结构化的农业环境中，确保自主机器人与人类安全交互并应对潜在危险，需要严格的安全验证。

Method: 通过系统性的危险分析和风险评估确定安全需求，开发安全控制器的形式化模型以验证其满足安全属性。

Result: 该方法成功应用于农业现场机器人，验证了安全关键属性并早期识别设计问题。

Conclusion: 该工作流程有助于开发更安全的自主机器人系统。

Abstract: Autonomous robots deployed in shared human environments, such as agricultural
settings, require rigorous safety assurance to meet both functional reliability
and regulatory compliance. These systems must operate in dynamic, unstructured
environments, interact safely with humans, and respond effectively to a wide
range of potential hazards. This paper presents a verification workflow for the
safety assurance of an autonomous agricultural robot, covering the entire
development life-cycle, from concept study and design to runtime verification.
The outlined methodology begins with a systematic hazard analysis and risk
assessment to identify potential risks and derive corresponding safety
requirements. A formal model of the safety controller is then developed to
capture its behaviour and verify that the controller satisfies the specified
safety properties with respect to these requirements. The proposed approach is
demonstrated on a field robot operating in an agricultural setting. The results
show that the methodology can be effectively used to verify safety-critical
properties and facilitate the early identification of design issues,
contributing to the development of safer robots and autonomous systems.

</details>


### [22] [UniTac-NV: A Unified Tactile Representation For Non-Vision-Based Tactile Sensors](https://arxiv.org/abs/2506.19699)
*Jian Hou,Xin Zhou,Qihan Yang,Adam J. Spiers*

Main category: cs.RO

TL;DR: 提出一种编码器-解码器架构，用于统一非视觉触觉传感器的数据，实现跨传感器数据转换，并在下游任务中直接应用。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注光学触觉传感器的跨传感器转换，而忽略了非光学传感器。本文旨在填补这一空白。

Method: 采用传感器特定编码器构建传感器无关的潜在空间，通过联合自编码器训练实现数据对齐。实验使用两种商业触觉传感器（Xela uSkin uSPa 46和Contactile PapillArray）进行验证。

Result: 模型在潜在空间中对齐数据，实现了低误差的跨传感器转换，并在几何估计任务中展示了无需重新训练的直接应用能力。

Conclusion: 该方法为非光学触觉传感器的通用化算法提供了可行方案，具有实际应用潜力。

Abstract: Generalizable algorithms for tactile sensing remain underexplored, primarily
due to the diversity of sensor modalities. Recently, many methods for
cross-sensor transfer between optical (vision-based) tactile sensors have been
investigated, yet little work focus on non-optical tactile sensors. To address
this gap, we propose an encoder-decoder architecture to unify tactile data
across non-vision-based sensors. By leveraging sensor-specific encoders, the
framework creates a latent space that is sensor-agnostic, enabling cross-sensor
data transfer with low errors and direct use in downstream applications. We
leverage this network to unify tactile data from two commercial tactile
sensors: the Xela uSkin uSPa 46 and the Contactile PapillArray. Both were
mounted on a UR5e robotic arm, performing force-controlled pressing sequences
against distinct object shapes (circular, square, and hexagonal prisms) and two
materials (rigid PLA and flexible TPU). Another more complex unseen object was
also included to investigate the model's generalization capabilities. We show
that alignment in latent space can be implicitly learned from joint autoencoder
training with matching contacts collected via different sensors. We further
demonstrate the practical utility of our approach through contact geometry
estimation, where downstream models trained on one sensor's latent
representation can be directly applied to another without retraining.

</details>


### [23] [Estimating Spatially-Dependent GPS Errors Using a Swarm of Robots](https://arxiv.org/abs/2506.19712)
*Praneeth Somisetty,Robert Griffin,Victor M. Baez,Miguel F. Arevalo-Castiblanco,Aaron T. Becker,Jason M. O'Kane*

Main category: cs.RO

TL;DR: 提出了一种用于估计GPS偏差的状态偏差估计算法（SBE），并结合高斯过程回归（GPR）和信息路径规划（IPP）优化数据收集。


<details>
  <summary>Details</summary>
Motivation: 解决因城市峡谷和干扰导致的GPS不准确问题，通过机器人团队估计静态空间变化的误差函数。

Method: 使用SBE算法估计GPS偏差，结合GPR模型训练，并通过IPP算法规划路径以最大化信息增益。

Result: 在模拟中评估了SBE和IPP，并与开环策略进行了比较。

Conclusion: SBE和IPP方法能有效估计和优化GPS偏差，提升环境位置偏差的理解。

Abstract: External factors, including urban canyons and adversarial interference, can
lead to Global Positioning System (GPS) inaccuracies that vary as a function of
the position in the environment. This study addresses the challenge of
estimating a static, spatially-varying error function using a team of robots.
We introduce a State Bias Estimation Algorithm (SBE) whose purpose is to
estimate the GPS biases. The central idea is to use sensed estimates of the
range and bearing to the other robots in the team to estimate changes in bias
across the environment. A set of drones moves in a 2D environment, each
sampling data from GPS, range, and bearing sensors. The biases calculated by
the SBE at estimated positions are used to train a Gaussian Process Regression
(GPR) model. We use a Sparse Gaussian process-based Informative Path Planning
(IPP) algorithm that identifies high-value regions of the environment for data
collection. The swarm plans paths that maximize information gain in each
iteration, further refining their understanding of the environment's positional
bias landscape. We evaluated SBE and IPP in simulation and compared the IPP
methodology to an open-loop strategy.

</details>


### [24] [The Starlink Robot: A Platform and Dataset for Mobile Satellite Communication](https://arxiv.org/abs/2506.19781)
*Boyi Liu,Qianyi Zhang,Qiang Yang,Jianhao Jiao,Jagmohan Chauhan,Dimitrios Kanoulas*

Main category: cs.RO

TL;DR: 论文介绍了首个配备Starlink卫星互联网的移动机器人平台，用于研究运动和环境遮挡下的卫星通信性能。


<details>
  <summary>Details</summary>
Motivation: 卫星通信在移动设备中的应用潜力巨大，但其在运动和遮挡环境下的性能尚不明确，需要系统研究。

Method: 开发了Starlink Robot平台，配备多种传感器，收集同步的通信指标、运动动态、天空可见性和3D环境数据。

Result: 生成了多模态数据集，涵盖不同运动状态和遮挡条件下的通信性能。

Conclusion: 该平台和数据集为开发运动感知通信协议和优化卫星通信提供了基础。

Abstract: The integration of satellite communication into mobile devices represents a
paradigm shift in connectivity, yet the performance characteristics under
motion and environmental occlusion remain poorly understood. We present the
Starlink Robot, the first mobile robotic platform equipped with Starlink
satellite internet, comprehensive sensor suite including upward-facing camera,
LiDAR, and IMU, designed to systematically study satellite communication
performance during movement. Our multi-modal dataset captures synchronized
communication metrics, motion dynamics, sky visibility, and 3D environmental
context across diverse scenarios including steady-state motion, variable
speeds, and different occlusion conditions. This platform and dataset enable
researchers to develop motion-aware communication protocols, predict
connectivity disruptions, and optimize satellite communication for emerging
mobile applications from smartphones to autonomous vehicles. The project is
available at https://github.com/StarlinkRobot.

</details>


### [25] [ReactEMG: Zero-Shot, Low-Latency Intent Detection via sEMG](https://arxiv.org/abs/2506.19815)
*Runsheng Wang,Xinyue Zhu,Ava Chen,Jingxi Xu,Lauren Winterbottom,Dawn M. Nilsen,Joel Stein,Matei Ciocarlie*

Main category: cs.RO

TL;DR: 提出一种基于sEMG信号的意图检测框架，通过分段策略和掩码建模实现快速意图识别，适用于康复和假肢领域。


<details>
  <summary>Details</summary>
Motivation: 解决现有sEMG系统在不同用户间快速、可靠识别意图且无需耗时校准的问题。

Method: 采用分段策略实时标记意图，并引入掩码建模对齐肌肉激活与用户意图。

Result: 在零样本迁移条件下，性能优于基线方法，适用于可穿戴机器人和下一代假肢系统。

Conclusion: 该框架为快速、稳定的意图检测提供了新思路，具有实际应用潜力。

Abstract: Surface electromyography (sEMG) signals show promise for effective
human-computer interfaces, particularly in rehabilitation and prosthetics.
However, challenges remain in developing systems that respond quickly and
reliably to user intent, across different subjects and without requiring
time-consuming calibration. In this work, we propose a framework for EMG-based
intent detection that addresses these challenges. Unlike traditional gesture
recognition models that wait until a gesture is completed before classifying
it, our approach uses a segmentation strategy to assign intent labels at every
timestep as the gesture unfolds. We introduce a novel masked modeling strategy
that aligns muscle activations with their corresponding user intents, enabling
rapid onset detection and stable tracking of ongoing gestures. In evaluations
against baseline methods, considering both accuracy and stability for device
control, our approach surpasses state-of-the-art performance in zero-shot
transfer conditions, demonstrating its potential for wearable robotics and
next-generation prosthetic systems. Our project page is available at:
https://reactemg.github.io

</details>


### [26] [CronusVLA: Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation](https://arxiv.org/abs/2506.19816)
*Hao Li,Shuai Yang,Yilun Chen,Yang Tian,Xiaoda Yang,Xinyi Chen,Hanqing Wang,Tai Wang,Feng Zhao,Dahua Lin,Jiangmiao Pang*

Main category: cs.RO

TL;DR: CronusVLA扩展单帧VLA模型到多帧范式，通过高效后训练阶段提升性能，减少计算冗余。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型受限于单帧观察，无法充分利用多帧运动信息，计算成本高。

Method: 包括单帧预训练、多帧编码和跨帧解码，通过特征分块和动作适应机制优化。

Result: 在SimplerEnv上成功率70.9%，LIBERO上提升12.7%，实际机器人实验表现优异。

Conclusion: CronusVLA通过多帧处理和高效推理，显著提升VLA模型的性能和鲁棒性。

Abstract: Recent vision-language-action (VLA) models built on pretrained
vision-language models (VLMs) have demonstrated strong generalization across
manipulation tasks. However, they remain constrained by a single-frame
observation paradigm and cannot fully benefit from the motion information
offered by aggregated multi-frame historical observations, as the large
vision-language backbone introduces substantial computational cost and
inference latency. We propose CronusVLA, a unified framework that extends
single-frame VLA models to the multi-frame paradigm through an efficient
post-training stage. CronusVLA comprises three key components: (1) single-frame
pretraining on large-scale embodied datasets with autoregressive action tokens
prediction, which establishes an embodied vision-language foundation; (2)
multi-frame encoding, adapting the prediction of vision-language backbones from
discrete action tokens to motion features during post-training, and aggregating
motion features from historical frames into a feature chunking; (3) cross-frame
decoding, which maps the feature chunking to accurate actions via a shared
decoder with cross-attention. By reducing redundant token computation and
caching past motion features, CronusVLA achieves efficient inference. As an
application of motion features, we further propose an action adaptation
mechanism based on feature-action retrieval to improve model performance during
finetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with
70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world
Franka experiments also show the strong performance and robustness.

</details>


### [27] [Look to Locate: Vision-Based Multisensory Navigation with 3-D Digital Maps for GNSS-Challenged Environments](https://arxiv.org/abs/2506.19827)
*Ola Elmaghraby,Eslam Mounier,Paulo Ricardo Marques de Araujo,Aboelmagd Noureldin*

Main category: cs.RO

TL;DR: 本文提出了一种低成本、基于视觉的多传感器导航系统，结合单目深度估计、语义过滤和视觉地图注册技术，在GNSS信号缺失环境中实现高精度车辆定位。


<details>
  <summary>Details</summary>
Motivation: 在GNSS信号缺失的环境（如室内停车场或密集城市峡谷）中，实现准确且鲁棒的车辆定位是一个重大挑战。

Method: 系统整合了单目深度估计、语义过滤和视觉地图注册（VMR）技术，并与3D数字地图结合。

Result: 在真实室内外驾驶场景中测试，系统实现了室内92%、室外80%以上的亚米级精度，水平定位和航向的平均均方根误差分别为0.98米和1.25度。相比基线方法，定位精度平均提升约88%。

Conclusion: 研究表明，低成本单目视觉系统结合3D地图在陆地车辆导航中具有可扩展性和独立性，无需依赖GNSS。

Abstract: In Global Navigation Satellite System (GNSS)-denied environments such as
indoor parking structures or dense urban canyons, achieving accurate and robust
vehicle positioning remains a significant challenge. This paper proposes a
cost-effective, vision-based multi-sensor navigation system that integrates
monocular depth estimation, semantic filtering, and visual map registration
(VMR) with 3-D digital maps. Extensive testing in real-world indoor and outdoor
driving scenarios demonstrates the effectiveness of the proposed system,
achieving sub-meter accuracy of 92% indoors and more than 80% outdoors, with
consistent horizontal positioning and heading average root mean-square errors
of approximately 0.98 m and 1.25 {\deg}, respectively. Compared to the
baselines examined, the proposed solution significantly reduced drift and
improved robustness under various conditions, achieving positioning accuracy
improvements of approximately 88% on average. This work highlights the
potential of cost-effective monocular vision systems combined with 3D maps for
scalable, GNSS-independent navigation in land vehicles.

</details>


### [28] [ManiGaussian++: General Robotic Bimanual Manipulation with Hierarchical Gaussian World Model](https://arxiv.org/abs/2506.19842)
*Tengbo Yu,Guanxing Lu,Zaijia Yang,Haoyuan Deng,Season Si Chen,Jiwen Lu,Wenbo Ding,Guoqiang Hu,Yansong Tang,Ziwei Wang*

Main category: cs.RO

TL;DR: ManiGaussian++扩展了ManiGaussian框架，通过分层高斯世界模型改进多任务双手机器人操作，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 双手机器人操作任务复杂，现有方法ManiGaussian忽略了多体交互，导致性能下降。

Method: 提出分层高斯世界模型，通过任务导向的高斯点云和领导者-跟随者架构建模多体时空动态。

Result: 在10个模拟任务中性能提升20.2%，在9个真实任务中平均成功率达60%。

Conclusion: ManiGaussian++显著优于现有双手机器人操作技术，适用于复杂任务。

Abstract: Multi-task robotic bimanual manipulation is becoming increasingly popular as
it enables sophisticated tasks that require diverse dual-arm collaboration
patterns. Compared to unimanual manipulation, bimanual tasks pose challenges to
understanding the multi-body spatiotemporal dynamics. An existing method
ManiGaussian pioneers encoding the spatiotemporal dynamics into the visual
representation via Gaussian world model for single-arm settings, which ignores
the interaction of multiple embodiments for dual-arm systems with significant
performance drop. In this paper, we propose ManiGaussian++, an extension of
ManiGaussian framework that improves multi-task bimanual manipulation by
digesting multi-body scene dynamics through a hierarchical Gaussian world
model. To be specific, we first generate task-oriented Gaussian Splatting from
intermediate visual features, which aims to differentiate acting and
stabilizing arms for multi-body spatiotemporal dynamics modeling. We then build
a hierarchical Gaussian world model with the leader-follower architecture,
where the multi-body spatiotemporal dynamics is mined for intermediate visual
representation via future scene prediction. The leader predicts Gaussian
Splatting deformation caused by motions of the stabilizing arm, through which
the follower generates the physical consequences resulted from the movement of
the acting arm. As a result, our method significantly outperforms the current
state-of-the-art bimanual manipulation techniques by an improvement of 20.2% in
10 simulated tasks, and achieves 60% success rate on average in 9 challenging
real-world tasks. Our code is available at
https://github.com/April-Yz/ManiGaussian_Bimanual.

</details>
