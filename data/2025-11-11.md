<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 44]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [VLM-driven Skill Selection for Robotic Assembly Tasks](https://arxiv.org/abs/2511.05680)
*Jeong-Jung Kim,Doo-Yeol Koh,Chang-Hyun Kim*

Main category: cs.RO

TL;DR: 提出结合视觉语言模型与模仿学习的机器人装配框架，通过视觉感知、自然语言理解和原始技能实现灵活的自适应装配操作


<details>
  <summary>Details</summary>
Motivation: 解决机器人装配任务中需要同时处理视觉感知、语言理解和复杂操作的挑战，实现更灵活和自适应的装配能力

Method: 使用配备夹爪的机器人在3D空间中移动执行装配操作，集成视觉感知、自然语言理解和学习的原始技能

Result: 在装配场景中取得高成功率，同时通过结构化的原始技能分解保持可解释性

Conclusion: 该框架有效结合了VLMs和模仿学习，为机器人装配任务提供了灵活且可解释的解决方案

Abstract: This paper presents a robotic assembly framework that combines Vision-Language Models (VLMs) with imitation learning for assembly manipulation tasks. Our system employs a gripper-equipped robot that moves in 3D space to perform assembly operations. The framework integrates visual perception, natural language understanding, and learned primitive skills to enable flexible and adaptive robotic manipulation. Experimental results demonstrate the effectiveness of our approach in assembly scenarios, achieving high success rates while maintaining interpretability through the structured primitive skill decomposition.

</details>


### [2] [A Unified Stochastic Mechanism Underlying Collective Behavior in Ants, Physical Systems, and Robotic Swarms](https://arxiv.org/abs/2511.05785)
*Lianhao Yin,Haiping Yu,Pascal Spino,Daniela Rus*

Main category: cs.RO

TL;DR: 该研究提出了一个统一随机模型，将生物、物理和机器人群体联系起来，揭示了在能量函数约束下最大化原理的共享统计机制。


<details>
  <summary>Details</summary>
Motivation: 生物群体通过分散随机行为实现集体目标，而物理系统中的随机粒子运动受熵最大化支配但无法实现集体目标，目前缺乏解释这两类系统随机行为的统一框架。

Method: 通过研究Formica polyctena蚂蚁获得实证证据，展示共享的统计机制，并构建遵循该原理的机器人群体来验证。

Result: 发现生物和物理系统都遵循在不同能量函数约束下的最大化原理，机器人群体能够展现可扩展的分散合作，模仿物理相变行为且仅需最小个体计算。

Conclusion: 建立了一个连接生物、物理和机器人群体的统一随机模型，为设计稳健智能的群体机器人提供了可扩展原则。

Abstract: Biological swarms, such as ant colonies, achieve collective goals through decentralized and stochastic individual behaviors. Similarly, physical systems composed of gases, liquids, and solids exhibit random particle motion governed by entropy maximization, yet do not achieve collective objectives. Despite this analogy, no unified framework exists to explain the stochastic behavior in both biological and physical systems. Here, we present empirical evidence from \textit{Formica polyctena} ants that reveals a shared statistical mechanism underlying both systems: maximization under different energy function constraints. We further demonstrate that robotic swarms governed by this principle can exhibit scalable, decentralized cooperation, mimicking physical phase-like behaviors with minimal individual computation. These findings established a unified stochastic model linking biological, physical, and robotic swarms, offering a scalable principle for designing robust and intelligent swarm robotics.

</details>


### [3] [VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models](https://arxiv.org/abs/2511.05791)
*Manav Kulshrestha,S. Talha Bukhari,Damon Conover,Aniket Bera*

Main category: cs.RO

TL;DR: VLAD-Grasp是一个基于视觉语言模型的零样本抓取检测方法，无需训练即可实现与监督方法竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器人抓取方法依赖大规模专家标注数据，且需要为新物体重新训练，限制了泛化能力。

Method: 通过视觉语言模型生成目标图像（用直杆"刺穿"物体表示抓取），预测深度和分割将其提升到3D，然后通过PCA和无对应优化对齐点云来恢复可执行抓取姿态。

Result: 在Cornell和Jacquard数据集上达到或优于最先进监督模型的性能，并在真实机器人上展示了零样本泛化能力。

Conclusion: 视觉语言基础模型可作为机器人操作的强大先验知识，实现无需训练的零样本抓取检测。

Abstract: Robotic grasping is a fundamental capability for autonomous manipulation; however, most existing methods rely on large-scale expert annotations and necessitate retraining to handle new objects. We present VLAD-Grasp, a Vision-Language model Assisted zero-shot approach for Detecting grasps. From a single RGB-D image, our method (1) prompts a large vision-language model to generate a goal image where a straight rod "impales" the object, representing an antipodal grasp, (2) predicts depth and segmentation to lift this generated image into 3D, and (3) aligns generated and observed object point clouds via principal component analysis and correspondence-free optimization to recover an executable grasp pose. Unlike prior work, our approach is training-free and does not rely on curated grasp datasets. Despite this, VLAD-Grasp achieves performance that is competitive with or superior to that of state-of-the-art supervised models on the Cornell and Jacquard datasets. We further demonstrate zero-shot generalization to novel real-world objects on a Franka Research 3 robot, highlighting vision-language foundation models as powerful priors for robotic manipulation.

</details>


### [4] [ViTaMIn-B: A Reliable and Efficient Visuo-Tactile Bimanual Manipulation Interface](https://arxiv.org/abs/2511.05858)
*Chuanyu Li,Chaoyi Liu,Daotan Wang,Shuyu Zhang,Lusong Li,Zecui Zeng,Fangchen Liu,Jing Xu,Rui Chen*

Main category: cs.RO

TL;DR: ViTaMIn-B是一个用于双手操作任务的手持数据收集系统，包含新型柔性视觉触觉传感器DuoTact和基于Meta Quest控制器的6自由度双手姿态跟踪方案


<details>
  <summary>Details</summary>
Motivation: 现有手持设备系统缺乏鲁棒的触觉感知和可靠的姿态跟踪能力，难以处理复杂的双手接触密集型交互任务

Method: 设计柔性框架的DuoTact传感器捕获高分辨率接触几何，将传感器全局变形重建为3D点云作为策略输入，开发基于Meta Quest控制器的6-DoF双手姿态获取流程

Result: 用户研究证实系统在初学者和专家操作者中都具有高效性和高可用性，在四个双手操作任务中表现出优于现有系统的性能

Conclusion: ViTaMIn-B系统为复杂双手接触密集型任务提供了更强大高效的数据收集解决方案

Abstract: Handheld devices have opened up unprecedented opportunities to collect large-scale, high-quality demonstrations efficiently. However, existing systems often lack robust tactile sensing or reliable pose tracking to handle complex interaction scenarios, especially for bimanual and contact-rich tasks. In this work, we propose ViTaMIn-B, a more capable and efficient handheld data collection system for such tasks. We first design DuoTact, a novel compliant visuo-tactile sensor built with a flexible frame to withstand large contact forces during manipulation while capturing high-resolution contact geometry. To enhance the cross-sensor generalizability, we propose reconstructing the sensor's global deformation as a 3D point cloud and using it as the policy input. We further develop a robust, unified 6-DoF bimanual pose acquisition process using Meta Quest controllers, which eliminates the trajectory drift issue in common SLAM-based methods. Comprehensive user studies confirm the efficiency and high usability of ViTaMIn-B among novice and expert operators. Furthermore, experiments on four bimanual manipulation tasks demonstrate its superior task performance relative to existing systems.

</details>


### [5] [PlaCo: a QP-based robot planning and control framework](https://arxiv.org/abs/2511.06141)
*Marc Duclusaud,Grégoire Passault,Vincent Padois,Olivier Ly*

Main category: cs.RO

TL;DR: PlaCo是一个用于简化机器人系统QP规划和控制问题建模与求解的软件框架


<details>
  <summary>Details</summary>
Motivation: 简化QP问题的数学建模过程，让用户能够以模块化和直观的方式指定任务和约束

Method: 提供高层接口抽象QP问题的底层数学公式，支持Python绑定用于快速原型开发和C++实现用于实时性能

Result: 开发了一个支持快速原型开发和实时性能的QP规划控制框架

Conclusion: PlaCo框架成功简化了机器人系统QP规划控制问题的建模和求解过程

Abstract: This article introduces PlaCo, a software framework designed to simplify the formulation and solution of Quadratic Programming (QP)-based planning and control problems for robotic systems. PlaCo provides a high-level interface that abstracts away the low-level mathematical formulation of QP problems, allowing users to specify tasks and constraints in a modular and intuitive manner. The framework supports both Python bindings for rapid prototyping and a C++ implementation for real-time performance.

</details>


### [6] [OpenVLN: Open-world aerial Vision-Language Navigation](https://arxiv.org/abs/2511.06182)
*Peican Lin,Gan Sun,Chenxi Liu,Fazeng Li,Weihong Ren,Yang Cong*

Main category: cs.RO

TL;DR: 提出了一个数据高效的开放世界空中视觉语言导航框架OpenVLN，能够在有限数据约束下执行语言引导飞行，并增强复杂空中环境中的长时程轨迹规划能力。


<details>
  <summary>Details</summary>
Motivation: 解决室外空中环境中数据获取困难和无人机长时程轨迹规划需求带来的新挑战，这些挑战在基于地面的视觉语言导航中更为复杂。

Method: 重新配置强化学习框架来优化VLM用于无人机导航任务，在有限训练数据下使用基于规则的策略高效微调VLM；同时引入长时程规划器进行轨迹合成，通过基于价值的奖励动态生成精确的无人机动作。

Result: 在TravelUAV基准测试中，方法相比基线方法在成功率上提升4.34%，在Oracle成功率上提升6.19%，在路径长度加权成功率上提升4.07%。

Conclusion: 验证了该方法在复杂空中环境中长时程无人机导航部署的有效性。

Abstract: Vision-language models (VLMs) have been widely-applied in ground-based vision-language navigation (VLN). However, the vast complexity of outdoor aerial environments compounds data acquisition challenges and imposes long-horizon trajectory planning requirements on Unmanned Aerial Vehicles (UAVs), introducing novel complexities for aerial VLN. To address these challenges, we propose a data-efficient Open-world aerial Vision-Language Navigation (i.e., OpenVLN) framework, which could execute language-guided flight with limited data constraints and enhance long-horizon trajectory planning capabilities in complex aerial environments. Specifically, we reconfigure a reinforcement learning framework to optimize the VLM for UAV navigation tasks, which can efficiently fine-tune VLM by using rule-based policies under limited training data. Concurrently, we introduce a long-horizon planner for trajectory synthesis that dynamically generates precise UAV actions via value-based rewards. To the end, we conduct sufficient navigation experiments on the TravelUAV benchmark with dataset scaling across diverse reward settings. Our method demonstrates consistent performance gains of up to 4.34% in Success Rate, 6.19% in Oracle Success Rate, and 4.07% in Success weighted by Path Length over baseline methods, validating its deployment efficacy for long-horizon UAV navigation in complex aerial environments.

</details>


### [7] [ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval](https://arxiv.org/abs/2511.06202)
*Shahram Najam Syed,Yatharth Ahuja,Arthur Jakobsson,Jeff Ichnowski*

Main category: cs.RO

TL;DR: ExpReS-VLA通过经验回放和检索机制专门化预训练的视觉-语言-动作模型，在防止灾难性遗忘的同时显著提升特定任务性能，内存使用减少97%，在仿真和真实机器人实验中均取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在零样本泛化方面表现优异，但在适应新部署环境时效率低下，而许多实际应用更需要在有限任务集上保持稳定高性能。

Method: 使用冻结视觉骨干网络存储紧凑特征表示而非原始图像-动作对，通过余弦相似度检索相关历史经验指导适应，采用优先级经验回放强调成功轨迹，并引入阈值混合对比损失从成功和失败尝试中学习。

Result: 在LIBERO仿真基准上，空间推理任务成功率从82.6%提升至93.1%，长视野任务从61%提升至72.3%；在真实机器人5个操作任务中，在已见和未见设置下均达到98%成功率，而朴素微调分别为84.7%和32%。

Conclusion: ExpReS-VLA提供了一种实用高效的机器人部署方法，仅需31秒和12个演示即可完成适应，在保持泛化能力的同时显著提升特定任务性能。

Abstract: Vision-Language-Action models such as OpenVLA show impressive zero-shot generalization across robotic manipulation tasks but often fail to adapt efficiently to new deployment environments. In many real-world applications, consistent high performance on a limited set of tasks is more important than broad generalization. We propose ExpReS-VLA, a method for specializing pre-trained VLA models through experience replay and retrieval while preventing catastrophic forgetting. ExpReS-VLA stores compact feature representations from the frozen vision backbone instead of raw image-action pairs, reducing memory usage by approximately 97 percent. During deployment, relevant past experiences are retrieved using cosine similarity and used to guide adaptation, while prioritized experience replay emphasizes successful trajectories. We also introduce Thresholded Hybrid Contrastive Loss, which enables learning from both successful and failed attempts. On the LIBERO simulation benchmark, ExpReS-VLA improves success rates from 82.6 to 93.1 percent on spatial reasoning tasks and from 61 to 72.3 percent on long-horizon tasks. On physical robot experiments with five manipulation tasks, it reaches 98 percent success on both seen and unseen settings, compared to 84.7 and 32 percent for naive fine-tuning. Adaptation takes 31 seconds using 12 demonstrations on a single RTX 5090 GPU, making the approach practical for real robot deployment.

</details>


### [8] [Affordance-Guided Coarse-to-Fine Exploration for Base Placement in Open-Vocabulary Mobile Manipulation](https://arxiv.org/abs/2511.06240)
*Tzu-Jung Lin,Jia-Fong Yeh,Hung-Ting Su,Chung-Yi Lin,Yi-Ting Chen,Winston H. Hsu*

Main category: cs.RO

TL;DR: 提出Affordance-Guided Coarse-to-Fine Exploration框架，通过结合视觉语言模型的语义理解和几何可行性，实现零样本的机器人基座放置规划，显著提升开放词汇移动操作任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常基于接近度导航而不考虑功能可用性，导致频繁的操作失败。需要一种能够同时考虑语义理解和几何约束的基座放置方法。

Method: 构建跨模态表示（Affordance RGB和Obstacle Map+），结合视觉语言模型的粗粒度语义先验和几何约束的细粒度优化，通过迭代优化过程实现从语义引导到几何精化的基座放置。

Result: 在五个不同的开放词汇移动操作任务中，系统达到85%的成功率，显著优于传统几何规划器和基于VLM的方法。

Conclusion: 功能可用性感知和多模态推理在开放词汇移动操作中具有广阔前景，能够实现通用化的指令条件规划。

Abstract: In open-vocabulary mobile manipulation (OVMM), task success often hinges on the selection of an appropriate base placement for the robot. Existing approaches typically navigate to proximity-based regions without considering affordances, resulting in frequent manipulation failures. We propose Affordance-Guided Coarse-to-Fine Exploration, a zero-shot framework for base placement that integrates semantic understanding from vision-language models (VLMs) with geometric feasibility through an iterative optimization process. Our method constructs cross-modal representations, namely Affordance RGB and Obstacle Map+, to align semantics with spatial context. This enables reasoning that extends beyond the egocentric limitations of RGB perception. To ensure interaction is guided by task-relevant affordances, we leverage coarse semantic priors from VLMs to guide the search toward task-relevant regions and refine placements with geometric constraints, thereby reducing the risk of convergence to local optima. Evaluated on five diverse open-vocabulary mobile manipulation tasks, our system achieves an 85% success rate, significantly outperforming classical geometric planners and VLM-based methods. This demonstrates the promise of affordance-aware and multimodal reasoning for generalizable, instruction-conditioned planning in OVMM.

</details>


### [9] [Robust Differentiable Collision Detection for General Objects](https://arxiv.org/abs/2511.06267)
*Jiayi Chen,Wei Zhao,Liangwang Ruan,Baoquan Chen,He Wang*

Main category: cs.RO

TL;DR: 提出了一个鲁棒高效的微分碰撞检测框架，支持凸形和凹形物体，通过距离基础的随机平滑、自适应采样和等效梯度传输来实现稳健的梯度计算。


<details>
  <summary>Details</summary>
Motivation: 传统碰撞检测算法如GJK+EPA是非微分的，阻碍了梯度流和基于梯度的优化在接触丰富任务中的应用。现有微分方法仅限于凸形物体且对复杂几何体缺乏鲁棒性。

Method: 采用距离基础的一阶随机平滑、自适应采样和等效梯度传输技术，构建支持凸形和凹形物体的微分碰撞检测框架。

Result: 在DexGraspNet和Objaverse的复杂网格上实验显示，相比现有基线方法有显著改进，并成功应用于灵巧抓取合成以提升抓取质量。

Conclusion: 该方法为接触丰富的机器人任务提供了高效鲁棒的微分碰撞检测解决方案，支持复杂几何形状和多样化配置。

Abstract: Collision detection is a core component of robotics applications such as simulation, control, and planning. Traditional algorithms like GJK+EPA compute witness points (i.e., the closest or deepest-penetration pairs between two objects) but are inherently non-differentiable, preventing gradient flow and limiting gradient-based optimization in contact-rich tasks such as grasping and manipulation. Recent work introduced efficient first-order randomized smoothing to make witness points differentiable; however, their direction-based formulation is restricted to convex objects and lacks robustness for complex geometries. In this work, we propose a robust and efficient differentiable collision detection framework that supports both convex and concave objects across diverse scales and configurations. Our method introduces distance-based first-order randomized smoothing, adaptive sampling, and equivalent gradient transport for robust and informative gradient computation. Experiments on complex meshes from DexGraspNet and Objaverse show significant improvements over existing baselines. Finally, we demonstrate a direct application of our method for dexterous grasp synthesis to refine the grasp quality. The code is available at https://github.com/JYChen18/DiffCollision.

</details>


### [10] [External Photoreflective Tactile Sensing Based on Surface Deformation Measurement](https://arxiv.org/abs/2511.06311)
*Seiichi Yamamoto,Hiroki Ishizuka,Takumi Kawasetsu,Koh Hosoda,Takayuki Kameoka,Kango Yanagida,Takato Horii,Sei Ikeda,Osamu Oshiro*

Main category: cs.RO

TL;DR: 提出了一种基于软体机器人机械柔顺性的触觉传感方法，使用外部可附加的光反射模块读取硅胶皮肤表面变形来估计接触力，无需嵌入触觉传感器。


<details>
  <summary>Details</summary>
Motivation: 将传感器置于接触界面之外可降低损坏风险、保持柔软性，并简化制造和维护。相比液体填充或导线嵌入的触觉皮肤，模块化附加架构增强了耐用性、减少了布线复杂性。

Method: 使用光反射模块读取软体硅胶皮肤的表面变形，通过表征光学传感元件和柔顺皮肤，设计原型触觉传感器。压缩实验验证方法有效性。

Result: 压缩实验显示力输出关系与理论一致，具有低滞后性、高重复性和对压痕速度的小响应。在软体机器人抓手上成功集成，可靠检测抓取事件。

Conclusion: 利用表面柔顺性与外部光学模块为软体机器人提供力感知，同时保持结构灵活性和可制造性，为机器人应用和安全人机协作铺平道路。

Abstract: We present a tactile sensing method enabled by the mechanical compliance of soft robots; an externally attachable photoreflective module reads surface deformation of silicone skin to estimate contact force without embedding tactile transducers. Locating the sensor off the contact interface reduces damage risk, preserves softness, and simplifies fabrication and maintenance. We first characterize the optical sensing element and the compliant skin, thendetermine the design of a prototype tactile sensor. Compression experiments validate the approach, exhibiting a monotonic force output relationship consistent with theory, low hysteresis, high repeatability over repeated cycles, and small response indentation speeds. We further demonstrate integration on a soft robotic gripper, where the module reliably detects grasp events. Compared with liquid filled or wireembedded tactile skins, the proposed modular add on architecture enhances durability, reduces wiring complexity, and supports straightforward deployment across diverse robot geometries. Because the sensing principle reads skin strain patterns, it also suggests extensions to other somatosensory cues such as joint angle or actuator state estimation from surface deformation. Overall, leveraging surface compliance with an external optical module provides a practical and robust route to equip soft robots with force perception while preserving structural flexibility and manufacturability, paving the way for robotic applications and safe human robot collaboration.

</details>


### [11] [Towards Adaptive Humanoid Control via Multi-Behavior Distillation and Reinforced Fine-Tuning](https://arxiv.org/abs/2511.06371)
*Yingnan Zhao,Xinmiao Wang,Dewei Wang,Xinzhe Liu,Dan Lu,Qilong Han,Peng Liu,Chenjia Bai*

Main category: cs.RO

TL;DR: 提出自适应人形控制(AHC)方法，通过两阶段框架学习跨技能和地形的自适应人形运动控制器，在Unitree G1机器人上验证了在各种情况和地形下的强适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要为每个技能训练独立策略，导致行为特定控制器在复杂地形和多样化情境下泛化能力有限且性能脆弱。

Method: 采用两阶段框架：首先训练多个主要运动策略并进行多行为蒸馏获得基础多行为控制器；然后通过在更多样化地形上收集在线反馈进行强化微调。

Result: 在仿真和Unitree G1机器人真实实验中，该方法在各种情况和地形下表现出强适应性。

Conclusion: AHC方法能够学习自适应的人形运动控制器，有效解决现有方法泛化能力不足的问题。

Abstract: Humanoid robots are promising to learn a diverse set of human-like locomotion behaviors, including standing up, walking, running, and jumping. However, existing methods predominantly require training independent policies for each skill, yielding behavior-specific controllers that exhibit limited generalization and brittle performance when deployed on irregular terrains and in diverse situations. To address this challenge, we propose Adaptive Humanoid Control (AHC) that adopts a two-stage framework to learn an adaptive humanoid locomotion controller across different skills and terrains. Specifically, we first train several primary locomotion policies and perform a multi-behavior distillation process to obtain a basic multi-behavior controller, facilitating adaptive behavior switching based on the environment. Then, we perform reinforced fine-tuning by collecting online feedback in performing adaptive behaviors on more diverse terrains, enhancing terrain adaptability for the controller. We conduct experiments in both simulation and real-world experiments in Unitree G1 robots. The results show that our method exhibits strong adaptability across various situations and terrains. Project website: https://ahc-humanoid.github.io.

</details>


### [12] [ArtReg: Visuo-Tactile based Pose Tracking and Manipulation of Unseen Articulated Objects](https://arxiv.org/abs/2511.06378)
*Prajval Kumar Murali,Mohsen Kaboli*

Main category: cs.RO

TL;DR: 提出了一种名为ArtReg的新方法，用于在机器人交互过程中无需先验知识即可跟踪未知物体（单个、多个或铰接式）的姿态，通过视觉-触觉点云集成和SE(3)李群中的无迹卡尔曼滤波实现点云配准。


<details>
  <summary>Details</summary>
Motivation: 机器人在真实环境中经常遇到具有复杂结构和铰接组件的未知物体，如门、抽屉、柜子和工具。无需先验几何或运动学知识即可感知、跟踪和操纵这些物体的能力仍然是机器人技术中的基本挑战。

Method: ArtReg方法在SE(3)李群中使用无迹卡尔曼滤波集成视觉-触觉点云进行点云配准。通过有目的的操纵动作（如推动或保持拉动）检测可能的铰接关节，并开发了用于目标驱动操纵的闭环控制器。

Result: 在真实机器人实验中广泛评估了该方法，展示了在不同质心、低光照条件和具有挑战性视觉背景的物体上的鲁棒性。在标准铰接物体数据集上的基准测试表明，在姿态精度方面优于最先进方法。

Conclusion: 利用视觉-触觉信息的鲁棒准确姿态跟踪使机器人能够感知和交互未见过的复杂铰接物体（具有旋转或棱柱关节）。

Abstract: Robots operating in real-world environments frequently encounter unknown objects with complex structures and articulated components, such as doors, drawers, cabinets, and tools. The ability to perceive, track, and manipulate these objects without prior knowledge of their geometry or kinematic properties remains a fundamental challenge in robotics. In this work, we present a novel method for visuo-tactile-based tracking of unseen objects (single, multiple, or articulated) during robotic interaction without assuming any prior knowledge regarding object shape or dynamics. Our novel pose tracking approach termed ArtReg (stands for Articulated Registration) integrates visuo-tactile point clouds in an unscented Kalman Filter formulation in the SE(3) Lie Group for point cloud registration. ArtReg is used to detect possible articulated joints in objects using purposeful manipulation maneuvers such as pushing or hold-pulling with a two-robot team. Furthermore, we leverage ArtReg to develop a closed-loop controller for goal-driven manipulation of articulated objects to move the object into the desired pose configuration. We have extensively evaluated our approach on various types of unknown objects through real robot experiments. We also demonstrate the robustness of our method by evaluating objects with varying center of mass, low-light conditions, and with challenging visual backgrounds. Furthermore, we benchmarked our approach on a standard dataset of articulated objects and demonstrated improved performance in terms of pose accuracy compared to state-of-the-art methods. Our experiments indicate that robust and accurate pose tracking leveraging visuo-tactile information enables robots to perceive and interact with unseen complex articulated objects (with revolute or prismatic joints).

</details>


### [13] [From Demonstrations to Safe Deployment: Path-Consistent Safety Filtering for Diffusion Policies](https://arxiv.org/abs/2511.06385)
*Ralf Römer,Julian Balletshofer,Jakob Thumm,Marco Pavone,Angela P. Schoellig,Matthias Althoff*

Main category: cs.RO

TL;DR: 提出路径一致安全过滤(PACS)方法，为扩散策略提供形式化安全保证，同时保持任务成功率


<details>
  <summary>Details</summary>
Motivation: 扩散策略在复杂操作任务中表现出色，但无法保证安全行为，而外部安全机制会改变动作导致性能下降

Method: 在生成的动作序列轨迹上执行路径一致制动，使用基于集合的可达性分析进行实时安全验证

Result: PACS在动态环境中提供形式化安全保证，保持任务成功率，比反应式安全方法任务成功率提高68%

Conclusion: PACS方法成功解决了扩散策略的安全性问题，在保证安全的同时保持了策略的原始性能

Abstract: Diffusion policies (DPs) achieve state-of-the-art performance on complex manipulation tasks by learning from large-scale demonstration datasets, often spanning multiple embodiments and environments. However, they cannot guarantee safe behavior, so external safety mechanisms are needed. These, however, alter actions in ways unseen during training, causing unpredictable behavior and performance degradation. To address these problems, we propose path-consistent safety filtering (PACS) for DPs. Our approach performs path-consistent braking on a trajectory computed from the sequence of generated actions. In this way, we keep execution consistent with the policy's training distribution, maintaining the learned, task-completing behavior. To enable a real-time deployment and handle uncertainties, we verify safety using set-based reachability analysis. Our experimental evaluation in simulation and on three challenging real-world human-robot interaction tasks shows that PACS (a) provides formal safety guarantees in dynamic environments, (b) preserves task success rates, and (c) outperforms reactive safety approaches, such as control barrier functions, by up to 68% in terms of task success. Videos are available at our project website: https://tum-lsy.github.io/pacs/.

</details>


### [14] [Whole-Body Control With Terrain Estimation of A 6-DoF Wheeled Bipedal Robot](https://arxiv.org/abs/2511.06397)
*Cong Wen,Yunfei Li,Kexin Liu,Yixin Qiu,Xuanhong Liao,Tianyu Wang,Dingchuan Liu,Tao Zhang,Ximin Lyu*

Main category: cs.RO

TL;DR: 开发了完整的动力学模型和全身控制框架，用于6自由度轮式双足机器人，结合地形估计以在不平坦地形上稳定运动。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多忽略腿部动力学，限制了机器人的运动潜力，且机器人在不平坦地形上运动面临挑战。

Method: 建立包含闭环动力学和地面接触模型的完整动力学模型，使用LiDAR惯性里程计和改进的主成分分析进行地形估计，采用PD控制和LQR进行姿态和平衡控制，使用分层优化解决全身控制问题。

Result: 通过仿真和真实实验验证了地形估计算法的性能，展示了算法在不平坦地形上的鲁棒性和穿越能力。

Conclusion: 提出的完整动力学模型和全身控制框架有效提升了轮式双足机器人在复杂地形上的运动性能。

Abstract: Wheeled bipedal robots have garnered increasing attention in exploration and inspection. However, most research simplifies calculations by ignoring leg dynamics, thereby restricting the robot's full motion potential. Additionally, robots face challenges when traversing uneven terrain. To address the aforementioned issue, we develop a complete dynamics model and design a whole-body control framework with terrain estimation for a novel 6 degrees of freedom wheeled bipedal robot. This model incorporates the closed-loop dynamics of the robot and a ground contact model based on the estimated ground normal vector. We use a LiDAR inertial odometry framework and improved Principal Component Analysis for terrain estimation. Task controllers, including PD control law and LQR, are employed for pose control and centroidal dynamics-based balance control, respectively. Furthermore, a hierarchical optimization approach is used to solve the whole-body control problem. We validate the performance of the terrain estimation algorithm and demonstrate the algorithm's robustness and ability to traverse uneven terrain through both simulation and real-world experiments.

</details>


### [15] [Real Garment Benchmark (RGBench): A Comprehensive Benchmark for Robotic Garment Manipulation featuring a High-Fidelity Scalable Simulator](https://arxiv.org/abs/2511.06434)
*Wenkang Hu,Xincheng Tang,Yanzhi E,Yitong Li,Zhengjie Shu,Wei Li,Huamin Wang,Ruigang Yang*

Main category: cs.RO

TL;DR: 提出了Real Garment Benchmark (RGBench)，这是一个用于机器人服装操作的综合性基准测试，包含6000多个服装网格模型、高性能模拟器以及评估模拟质量的协议。


<details>
  <summary>Details</summary>
Motivation: 尽管在利用模拟数据学习刚性物体机器人操作方面取得了显著进展，但由于缺乏可变形物体模型和逼真的非刚体模拟器，这一成功难以应用于可变形物体。

Method: 开发了包含多样化服装网格模型的数据集、新的高性能模拟器，以及通过真实服装动力学测量来评估模拟质量的综合协议。

Result: 实验表明，该模拟器在性能上大幅优于现有布料模拟器，模拟误差减少20%，同时速度提升3倍。

Conclusion: RGBench将公开发布，以加速未来机器人服装操作的研究。

Abstract: While there has been significant progress to use simulated data to learn robotic manipulation of rigid objects, applying its success to deformable objects has been hindered by the lack of both deformable object models and realistic non-rigid body simulators. In this paper, we present Real Garment Benchmark (RGBench), a comprehensive benchmark for robotic manipulation of garments. It features a diverse set of over 6000 garment mesh models, a new high-performance simulator, and a comprehensive protocol to evaluate garment simulation quality with carefully measured real garment dynamics. Our experiments demonstrate that our simulator outperforms currently available cloth simulators by a large margin, reducing simulation error by 20% while maintaining a speed of 3 times faster. We will publicly release RGBench to accelerate future research in robotic garment manipulation. Website: https://rgbench.github.io/

</details>


### [16] [Sim-to-Real Transfer in Deep Reinforcement Learning for Bipedal Locomotion](https://arxiv.org/abs/2511.06465)
*Lingfan Bao,Tianhu Peng,Chengxu Zhou*

Main category: cs.RO

TL;DR: 本章分析了双足机器人深度强化学习中的仿真到现实迁移问题，诊断了仿真差距的主要来源，并提出了缩小差距和增强策略鲁棒性的两种互补解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决双足机器人深度强化学习在仿真到现实迁移中面临的挑战，即仿真与现实环境之间的差距问题。

Method: 通过模型中心策略提高仿真器物理保真度来缩小差距，同时通过鲁棒性训练和后部署适应来增强策略的弹性。

Result: 构建了一个战略框架，为开发和评估鲁棒的仿真到现实解决方案提供了清晰路线图。

Conclusion: 结合缩小仿真差距和增强策略鲁棒性的两种哲学，可以有效解决双足机器人深度强化学习的仿真到现实迁移问题。

Abstract: This chapter addresses the critical challenge of simulation-to-reality (sim-to-real) transfer for deep reinforcement learning (DRL) in bipedal locomotion. After contextualizing the problem within various control architectures, we dissect the ``curse of simulation'' by analyzing the primary sources of sim-to-real gap: robot dynamics, contact modeling, state estimation, and numerical solvers. Building on this diagnosis, we structure the solutions around two complementary philosophies. The first is to shrink the gap through model-centric strategies that systematically improve the simulator's physical fidelity. The second is to harden the policy, a complementary approach that uses in-simulation robustness training and post-deployment adaptation to make the policy inherently resilient to model inaccuracies. The chapter concludes by synthesizing these philosophies into a strategic framework, providing a clear roadmap for developing and evaluating robust sim-to-real solutions.

</details>


### [17] [A Low-Rank Method for Vision Language Model Hallucination Mitigation in Autonomous Driving](https://arxiv.org/abs/2511.06496)
*Keke Long,Jiacheng Guo,Tianyun Zhang,Hongkai Yu,Xiaopeng Li*

Main category: cs.RO

TL;DR: 提出了一种基于低秩分解的自包含方法，仅使用多个VLM生成的候选描述本身来自动排序其幻觉程度，无需外部参考或模型访问。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，视觉语言模型会产生幻觉（虚假细节），但检测和缓解幻觉在缺乏真实参考和模型内部访问时具有挑战性。

Method: 通过构建句子嵌入矩阵并将其分解为低秩共识分量和稀疏残差，使用残差幅度对描述进行排序：选择残差最小的作为最无幻觉的描述。

Result: 在NuScenes数据集上实现了87%的选择准确率，比未过滤基线提高19%，比多智能体辩论方法提高6-10%。推理时间减少51-67%。

Conclusion: 该方法能有效识别无幻觉描述，排序结果与人类判断高度相关，且易于并行化，适用于实时自动驾驶应用。

Abstract: Vision Language Models (VLMs) are increasingly used in autonomous driving to help understand traffic scenes, but they sometimes produce hallucinations, which are false details not grounded in the visual input. Detecting and mitigating hallucinations is challenging when ground-truth references are unavailable and model internals are inaccessible. This paper proposes a novel self-contained low-rank approach to automatically rank multiple candidate captions generated by multiple VLMs based on their hallucination levels, using only the captions themselves without requiring external references or model access. By constructing a sentence-embedding matrix and decomposing it into a low-rank consensus component and a sparse residual, we use the residual magnitude to rank captions: selecting the one with the smallest residual as the most hallucination-free. Experiments on the NuScenes dataset demonstrate that our approach achieves 87% selection accuracy in identifying hallucination-free captions, representing a 19% improvement over the unfiltered baseline and a 6-10% improvement over multi-agent debate method. The sorting produced by sparse error magnitudes shows strong correlation with human judgments of hallucinations, validating our scoring mechanism. Additionally, our method, which can be easily parallelized, reduces inference time by 51-67% compared to debate approaches, making it practical for real-time autonomous driving applications.

</details>


### [18] [Adaptive PID Control for Robotic Systems via Hierarchical Meta-Learning and Reinforcement Learning with Physics-Based Data Augmentation](https://arxiv.org/abs/2511.06500)
*JiaHao Wu,ShengWen Yu*

Main category: cs.RO

TL;DR: 提出结合元学习和强化学习的层次控制框架，通过物理数据增强解决样本效率问题，在异构机器人平台上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: PID控制器在工业机器人中占主导地位，但手动调参耗时且需要专业知识，需要自动化解决方案。

Method: 层次控制框架：元学习用于PID初始化，强化学习用于在线适应；引入基于物理的数据增强策略，通过扰动物理参数生成虚拟机器人配置。

Result: 在Franka Panda上平均改进16.6%（MAE 6.26°），高负载关节J2改进80.4%；发现优化天花板效应：当元学习存在局部高误差关节时RL效果显著，但基线性能均匀强时无改进。

Conclusion: RL效果高度依赖于元学习基线质量和误差分布，为层次控制系统设计提供了重要指导。

Abstract: Proportional-Integral-Derivative (PID) controllers remain the predominant choice in industrial robotics due to their simplicity and reliability. However, manual tuning of PID parameters for diverse robotic platforms is time-consuming and requires extensive domain expertise. This paper presents a novel hierarchical control framework that combines meta-learning for PID initialization and reinforcement learning (RL) for online adaptation. To address the sample efficiency challenge, a \textit{physics-based data augmentation} strategy is introduced that generates virtual robot configurations by systematically perturbing physical parameters, enabling effective meta-learning with limited real robot data. The proposed approach is evaluated on two heterogeneous platforms: a 9-DOF Franka Panda manipulator and a 12-DOF Laikago quadruped robot. Experimental results demonstrate that the proposed method achieves 16.6\% average improvement on Franka Panda (6.26° MAE), with exceptional gains in high-load joints (J2: 80.4\% improvement from 12.36° to 2.42°). Critically, this work discovers the \textit{optimization ceiling effect}: RL achieves dramatic improvements when meta-learning exhibits localized high-error joints, but provides no benefit (0.0\%) when baseline performance is uniformly strong, as observed in Laikago. The method demonstrates robust performance under disturbances (parameter uncertainty: +19.2\%, no disturbance: +16.6\%, average: +10.0\%) with only 10 minutes of training time. Multi-seed analysis across 100 random initializations confirms stable performance (4.81+/-1.64\% average). These results establish that RL effectiveness is highly dependent on meta-learning baseline quality and error distribution, providing important design guidance for hierarchical control systems.

</details>


### [19] [Koopman global linearization of contact dynamics for robot locomotion and manipulation enables elaborate control](https://arxiv.org/abs/2511.06515)
*Cormac O'Neill,Jasmine Terrones,H. Harry Asada*

Main category: cs.RO

TL;DR: 使用Koopman算子将接触动力学统一为全局线性模型，实现机器人的实时凸优化控制


<details>
  <summary>Details</summary>
Motivation: 解决机器人与环境动态接触时的控制难题，特别是接触边界动力学切换导致的非凸优化问题

Method: 应用Koopman算子将分段接触动力学转化为嵌入空间中的统一全局线性模型，利用粘弹性接触特性实现无近似控制

Result: 成功实现了腿式机器人的凸模型预测控制和机械臂动态推动的实时控制，能够发现包含多次接触变化的复杂控制策略

Conclusion: 该方法能够处理多接触变化的复杂控制问题，且适用于机器人之外的广泛领域

Abstract: Controlling robots that dynamically engage in contact with their environment is a pressing challenge. Whether a legged robot making-and-breaking contact with a floor, or a manipulator grasping objects, contact is everywhere. Unfortunately, the switching of dynamics at contact boundaries makes control difficult. Predictive controllers face non-convex optimization problems when contact is involved. Here, we overcome this difficulty by applying Koopman operators to subsume the segmented dynamics due to contact changes into a unified, globally-linear model in an embedding space. We show that viscoelastic contact at robot-environment interactions underpins the use of Koopman operators without approximation to control inputs. This methodology enables the convex Model Predictive Control of a legged robot, and the real-time control of a manipulator engaged in dynamic pushing. In this work, we show that our method allows robots to discover elaborate control strategies in real-time over time horizons with multiple contact changes, and the method is applicable to broad fields beyond robotics.

</details>


### [20] [CoFineLLM: Conformal Finetuning of LLMs for Language-Instructed Robot Planning](https://arxiv.org/abs/2511.06575)
*Jun Wang,Yevgeniy Vorobeychik,Yiannis Kantaros*

Main category: cs.RO

TL;DR: CoFineLLM是一个针对LLM规划器的CP感知微调框架，通过显式减少预测集大小来降低用户干预频率，在语言指令机器人规划任务中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM规划器在长视野任务中可靠性不足，虽然使用Conformal Prediction包装输出确保正确性，但由于LLM训练时未考虑不确定性，在高置信度下会产生过大的预测集，导致频繁需要人工干预。

Method: 提出CoFineLLM框架，这是首个CP感知的LLM规划器微调方法，通过显式优化来减少预测集大小，从而降低用户帮助请求频率。

Result: 在多个语言指令机器人规划任务上的评估显示，该方法在预测集大小和帮助率方面均优于不确定性感知和不确定性不可知基线方法，并在硬件实验中展示了对外分布场景的鲁棒性。

Conclusion: CoFineLLM通过CP感知微调有效解决了LLM规划器预测集过大的问题，显著减少了用户干预需求，同时保持了规划正确性保证。

Abstract: Large Language Models (LLMs) have recently emerged as planners for language-instructed agents, generating sequences of actions to accomplish natural language tasks. However, their reliability remains a challenge, especially in long-horizon tasks, since they often produce overconfident yet wrong outputs. Conformal Prediction (CP) has been leveraged to address this issue by wrapping LLM outputs into prediction sets that contain the correct action with a user-defined confidence. When the prediction set is a singleton, the planner executes that action; otherwise, it requests help from a user. This has led to LLM-based planners that can ensure plan correctness with a user-defined probability. However, as LLMs are trained in an uncertainty-agnostic manner, without awareness of prediction sets, they tend to produce unnecessarily large sets, particularly at higher confidence levels, resulting in frequent human interventions limiting autonomous deployment. To address this, we introduce CoFineLLM (Conformal Finetuning for LLMs), the first CP-aware finetuning framework for LLM-based planners that explicitly reduces prediction-set size and, in turn, the need for user interventions. We evaluate our approach on multiple language-instructed robot planning problems and show consistent improvements over uncertainty-aware and uncertainty-agnostic finetuning baselines in terms of prediction-set size, and help rates. Finally, we demonstrate robustness of our method to out-of-distribution scenarios in hardware experiments.

</details>


### [21] [Underactuated Biomimetic Autonomous Underwater Vehicle for Ecosystem Monitoring](https://arxiv.org/abs/2511.06578)
*Kaustubh Singh,Shivam Kumar,Shashikant Pawar,Sandeep Manjanna*

Main category: cs.RO

TL;DR: 开发了一种欠驱动的仿生水下机器人，通过强化学习技术学习最小驱动行为，用于海洋和淡水环境的生态系统监测


<details>
  <summary>Details</summary>
Motivation: 需要开发适合海洋和淡水环境生态系统监测的仿生水下机器人

Method: 更新了鱼形机器人的机械设计，提出了尾鳍摆动机制，并使用强化学习技术学习最小驱动行为，在FishGym模拟器上进行测试

Result: 提出了初步的机械设计，并在模拟器中展示了游泳行为

Conclusion: 成功开发了欠驱动的仿生水下机器人原型，为生态系统监测提供了新的解决方案

Abstract: In this paper, we present an underactuated biomimetic underwater robot that is suitable for ecosystem monitoring in both marine and freshwater environments. We present an updated mechanical design for a fish-like robot and propose minimal actuation behaviors learned using reinforcement learning techniques. We present our preliminary mechanical design of the tail oscillation mechanism and illustrate the swimming behaviors on FishGym simulator, where the reinforcement learning techniques will be tested on

</details>


### [22] [How Do VLAs Effectively Inherit from VLMs?](https://arxiv.org/abs/2511.06619)
*Chuheng Zhang,Rushuai Yang,Xiaoyu Chen,Kaixin Wang,Li Zhao,Yi Chen,Jiang Bian*

Main category: cs.RO

TL;DR: 本文提出了GrinningFace诊断基准，通过表情符号桌面操作任务评估视觉-语言-动作模型如何有效继承视觉-语言模型的先验知识，并比较了多种知识迁移技术。


<details>
  <summary>Details</summary>
Motivation: 解决视觉-语言-动作模型如何有效继承视觉-语言模型先验知识这一关键问题，表情符号任务设计能够清晰揭示知识迁移效果。

Method: 使用表情符号桌面操作任务，在模拟环境和真实机器人上实现，比较参数高效微调、VLM冻结、联合训练、离散动作预测和潜在动作预测等多种技术。

Result: 系统评估表明，保持VLM先验知识对于VLA模型的泛化能力至关重要。

Conclusion: 为开发真正可泛化的具身AI系统建立了指导方针，强调了VLM先验知识在具身控制中的重要性。

Abstract: Vision-language-action (VLA) models hold the promise to attain generalizable embodied control. To achieve this, a pervasive paradigm is to leverage the rich vision-semantic priors of large vision-language models (VLMs). However, the fundamental question persists: How do VLAs effectively inherit the prior knowledge from VLMs? To address this critical question, we introduce a diagnostic benchmark, GrinningFace, an emoji tabletop manipulation task where the robot arm is asked to place objects onto printed emojis corresponding to language instructions. This task design is particularly revealing -- knowledge associated with emojis is ubiquitous in Internet-scale datasets used for VLM pre-training, yet emojis themselves are largely absent from standard robotics datasets. Consequently, they provide a clean proxy: successful task completion indicates effective transfer of VLM priors to embodied control. We implement this diagnostic task in both simulated environment and a real robot, and compare various promising techniques for knowledge transfer. Specifically, we investigate the effects of parameter-efficient fine-tuning, VLM freezing, co-training, predicting discretized actions, and predicting latent actions. Through systematic evaluation, our work not only demonstrates the critical importance of preserving VLM priors for the generalization of VLA but also establishes guidelines for future research in developing truly generalizable embodied AI systems.

</details>


### [23] [Rapidly Learning Soft Robot Control via Implicit Time-Stepping](https://arxiv.org/abs/2511.06667)
*Andrew Choi,Dezhong Tong*

Main category: cs.RO

TL;DR: 该论文展示了通过隐式时间步进实现快速软体机器人策略学习，使用DisMech模拟器在接触丰富场景中速度提升高达40倍，且不牺牲准确性。


<details>
  <summary>Details</summary>
Motivation: 软体机器人模拟框架稀缺且计算成本高，导致策略学习不可行，需要开发快速高效的软体机器人学习框架。

Method: 采用DisMech全隐式软体模拟器，结合delta自然曲率控制方法，通过隐式时间步进和并行环境实现加速。

Result: 在四个软体操作任务中，500个并行环境在非接触情况下速度提升6倍，接触丰富场景中提升40倍，且sim-to-sim评估显示准确性无损失。

Conclusion: 隐式时间步进为软体机器人策略学习提供了罕见的高效解决方案，实现了显著加速而不牺牲精度。

Abstract: With the explosive growth of rigid-body simulators, policy learning in simulation has become the de facto standard for most rigid morphologies. In contrast, soft robotic simulation frameworks remain scarce and are seldom adopted by the soft robotics community. This gap stems partly from the lack of easy-to-use, general-purpose frameworks and partly from the high computational cost of accurately simulating continuum mechanics, which often renders policy learning infeasible. In this work, we demonstrate that rapid soft robot policy learning is indeed achievable via implicit time-stepping. Our simulator of choice, DisMech, is a general-purpose, fully implicit soft-body simulator capable of handling both soft dynamics and frictional contact. We further introduce delta natural curvature control, a method analogous to delta joint position control in rigid manipulators, providing an intuitive and effective means of enacting control for soft robot learning. To highlight the benefits of implicit time-stepping and delta curvature control, we conduct extensive comparisons across four diverse soft manipulator tasks against one of the most widely used soft-body frameworks, Elastica. With implicit time-stepping, parallel stepping of 500 environments achieves up to 6x faster speeds for non-contact cases and up to 40x faster for contact-rich scenarios. Finally, a comprehensive sim-to-sim gap evaluation--training policies in one simulator and evaluating them in another--demonstrates that implicit time-stepping provides a rare free lunch: dramatic speedups achieved without sacrificing accuracy.

</details>


### [24] [Programmable Telescopic Soft Pneumatic Actuators for Deployable and Shape Morphing Soft Robots](https://arxiv.org/abs/2511.06673)
*Joel Kemp,Andre Farinha,David Howard,Krishna Manaswi Digumarti,Josh Pinskier*

Main category: cs.RO

TL;DR: 提出了一种可编程伸缩软气动执行器（PTSPA），通过参数化设计解决软机器人设计中的维度灾难问题，实现可部署结构和在受限空间中的操作。


<details>
  <summary>Details</summary>
Motivation: 软机器人具有丰富的自由形式和连续体设备能力，但由于维度灾难，目前缺乏可处理且直接利用设计自由度的有效方法。参数化设计集为创建具有丰富体现行为的模块化软机器人提供了可行途径。

Method: 开发了参数化几何生成器，从高级输入定制执行器模型，通过半自动化实验和系统参数探索来研究新的设计空间。

Result: 表征了执行器的伸缩/弯曲、膨胀和刚度性能，揭示了关键设计参数与性能之间的明确关系，并在可部署软四足机器人中进行了应用演示。

Conclusion: PTSPA为可部署和形状变形结构以及需要大长度变化的应用提供了新的设计范式。

Abstract: Soft Robotics presents a rich canvas for free-form and continuum devices capable of exerting forces in any direction and transforming between arbitrary configurations. However, there is no current way to tractably and directly exploit the design freedom due to the curse of dimensionality. Parameterisable sets of designs offer a pathway towards tractable, modular soft robotics that appropriately harness the behavioural freeform of soft structures to create rich embodied behaviours. In this work, we present a parametrised class of soft actuators, Programmable Telescopic Soft Pneumatic Actuators (PTSPAs). PTSPAs expand axially on inflation for deployable structures and manipulation in challenging confined spaces. We introduce a parametric geometry generator to customise actuator models from high-level inputs, and explore the new design space through semi-automated experimentation and systematic exploration of key parameters. Using it we characterise the actuators' extension/bending, expansion, and stiffness and reveal clear relationships between key design parameters and performance. Finally we demonstrate the application of the actuators in a deployable soft quadruped whose legs deploy to walk, enabling automatic adaptation to confined spaces. PTSPAs present new design paradigm for deployable and shape morphing structures and wherever large length changes are required.

</details>


### [25] [Physically-Grounded Goal Imagination: Physics-Informed Variational Autoencoder for Self-Supervised Reinforcement Learning](https://arxiv.org/abs/2511.06745)
*Lan Thi Ha Nguyen,Kien Ton Manh,Anh Do Duc,Nam Pham Hai*

Main category: cs.RO

TL;DR: 提出了PI-RIG方法，通过增强的物理信息变分自编码器将物理约束集成到VAE训练中，生成物理一致且可达的目标，解决了自监督目标条件强化学习中的目标设置问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法如RIG使用VAE在潜在空间中生成目标，但会产生物理上不可行的目标，阻碍学习效率。需要生成物理一致且可达的目标来提升探索效果和技能获取。

Method: PI-RIG方法通过增强的物理信息变分自编码器(Enhanced p3-VAE)，将潜在空间显式分离为控制物体动力学的物理变量和捕捉视觉外观的环境因素，并通过微分方程约束和守恒定律强制物理一致性。

Result: 实验表明，这种物理信息目标生成显著提高了所提出目标的质量，在视觉机器人操作任务（包括到达、推动和抓放场景）中实现了更有效的探索和更好的技能获取。

Conclusion: 将物理约束直接集成到目标生成过程中能够生成物理一致且可达的目标，显著提升自监督目标条件强化学习的性能。

Abstract: Self-supervised goal-conditioned reinforcement learning enables robots to autonomously acquire diverse skills without human supervision. However, a central challenge is the goal setting problem: robots must propose feasible and diverse goals that are achievable in their current environment. Existing methods like RIG (Visual Reinforcement Learning with Imagined Goals) use variational autoencoder (VAE) to generate goals in a learned latent space but have the limitation of producing physically implausible goals that hinder learning efficiency. We propose Physics-Informed RIG (PI-RIG), which integrates physical constraints directly into the VAE training process through a novel Enhanced Physics-Informed Variational Autoencoder (Enhanced p3-VAE), enabling the generation of physically consistent and achievable goals. Our key innovation is the explicit separation of the latent space into physics variables governing object dynamics and environmental factors capturing visual appearance, while enforcing physical consistency through differential equation constraints and conservation laws. This enables the generation of physically consistent and achievable goals that respect fundamental physical principles such as object permanence, collision constraints, and dynamic feasibility. Through extensive experiments, we demonstrate that this physics-informed goal generation significantly improves the quality of proposed goals, leading to more effective exploration and better skill acquisition in visual robotic manipulation tasks including reaching, pushing, and pick-and-place scenarios.

</details>


### [26] [Semi-distributed Cross-modal Air-Ground Relative Localization](https://arxiv.org/abs/2511.06749)
*Weining Lu,Deer Bin,Lian Ma,Ming Ma,Zhihao Ma,Xiangyang Chen,Longfei Wang,Yixiao Feng,Zhouxian Jiang,Yongliang Shi,Bin Liang*

Main category: cs.RO

TL;DR: 提出了一种半分布式跨模态空地相对定位框架，通过解耦相对定位与状态估计，仅传输关键点像素和描述符，将通信带宽限制在0.3 Mbps以下，实现了高效准确的空地协作定位。


<details>
  <summary>Details</summary>
Motivation: 当前机器人相对定位方法主要采用相同传感器配置的分布式多机器人SLAM系统，紧密耦合所有机器人的状态估计，限制了灵活性和准确性。

Method: UGV和UAV独立执行SLAM并提取深度学习关键点和全局描述符，UGV使用LiDAR、相机和IMU进行局部Bundle Adjustment，采用稀疏关键点优化和两阶段BA过程，并实现基于深度学习的增量闭环检测算法。

Result: 实验结果表明该方法在准确性和效率方面表现优异，通信带宽被有效限制在0.3 Mbps以下。

Conclusion: 该方法为空地协作任务提供了一种高效、准确且灵活的相对定位解决方案，优于传统的多机器人SLAM方法。

Abstract: Efficient, accurate, and flexible relative localization is crucial in air-ground collaborative tasks. However, current approaches for robot relative localization are primarily realized in the form of distributed multi-robot SLAM systems with the same sensor configuration, which are tightly coupled with the state estimation of all robots, limiting both flexibility and accuracy. To this end, we fully leverage the high capacity of Unmanned Ground Vehicle (UGV) to integrate multiple sensors, enabling a semi-distributed cross-modal air-ground relative localization framework. In this work, both the UGV and the Unmanned Aerial Vehicle (UAV) independently perform SLAM while extracting deep learning-based keypoints and global descriptors, which decouples the relative localization from the state estimation of all agents. The UGV employs a local Bundle Adjustment (BA) with LiDAR, camera, and an IMU to rapidly obtain accurate relative pose estimates. The BA process adopts sparse keypoint optimization and is divided into two stages: First, optimizing camera poses interpolated from LiDAR-Inertial Odometry (LIO), followed by estimating the relative camera poses between the UGV and UAV. Additionally, we implement an incremental loop closure detection algorithm using deep learning-based descriptors to maintain and retrieve keyframes efficiently. Experimental results demonstrate that our method achieves outstanding performance in both accuracy and efficiency. Unlike traditional multi-robot SLAM approaches that transmit images or point clouds, our method only transmits keypoint pixels and their descriptors, effectively constraining the communication bandwidth under 0.3 Mbps. Codes and data will be publicly available on https://github.com/Ascbpiac/cross-model-relative-localization.git.

</details>


### [27] [SlotVLA: Towards Modeling of Object-Relation Representations in Robotic Manipulation](https://arxiv.org/abs/2511.06754)
*Taisei Hanyu,Nhat Chung,Huy Le,Toan Nguyen,Yuki Ikebe,Anthony Gunderman,Duy Nguyen Ho Minh,Khoa Vo,Tung Kieu,Kashu Yamazaki,Chase Rainwater,Anh Nguyen,Ngan Le*

Main category: cs.RO

TL;DR: 提出了LIBERO+数据集和SlotVLA框架，通过对象中心的关系表示来实现紧凑、可解释的多任务机器人操作。


<details>
  <summary>Details</summary>
Motivation: 现有机器人多任务模型依赖密集嵌入，混淆了物体和背景线索，存在效率和可解释性问题。受人类推理离散物体及其关系的启发，探索对象关系中心表示作为结构化、高效、可解释的视觉运动控制基础。

Method: 1) 引入LIBERO+细粒度基准数据集，提供对象中心标注（边界框、掩码标签和实例级时间跟踪）；2) 提出SlotVLA框架：基于槽注意力的视觉分词器保持时间一致性对象表示，关系中心解码器生成任务相关嵌入，LLM驱动模块将嵌入转换为可执行动作。

Result: 在LIBERO+上的实验表明，对象中心槽和对象关系槽表示大幅减少所需视觉标记数量，同时提供有竞争力的泛化性能。

Conclusion: LIBERO+和SlotVLA为推进对象关系中心的机器人操作提供了紧凑、可解释且有效的基础。

Abstract: Inspired by how humans reason over discrete objects and their relationships, we explore whether compact object-centric and object-relation representations can form a foundation for multitask robotic manipulation. Most existing robotic multitask models rely on dense embeddings that entangle both object and background cues, raising concerns about both efficiency and interpretability. In contrast, we study object-relation-centric representations as a pathway to more structured, efficient, and explainable visuomotor control. Our contributions are two-fold. First, we introduce LIBERO+, a fine-grained benchmark dataset designed to enable and evaluate object-relation reasoning in robotic manipulation. Unlike prior datasets, LIBERO+ provides object-centric annotations that enrich demonstrations with box- and mask-level labels as well as instance-level temporal tracking, supporting compact and interpretable visuomotor representations. Second, we propose SlotVLA, a slot-attention-based framework that captures both objects and their relations for action decoding. It uses a slot-based visual tokenizer to maintain consistent temporal object representations, a relation-centric decoder to produce task-relevant embeddings, and an LLM-driven module that translates these embeddings into executable actions. Experiments on LIBERO+ demonstrate that object-centric slot and object-relation slot representations drastically reduce the number of required visual tokens, while providing competitive generalization. Together, LIBERO+ and SlotVLA provide a compact, interpretable, and effective foundation for advancing object-relation-centric robotic manipulation.

</details>


### [28] [Human-Level Actuation for Humanoids](https://arxiv.org/abs/2511.06796)
*MD-Nazmus Sunbeam*

Main category: cs.RO

TL;DR: 提出了一个量化评估人形机器人"人类水平"驱动性能的综合框架，包括标准化关节坐标系、人类等效包络和人类水平驱动评分，通过可重复实验测量扭矩、功率、效率等六个物理因素。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人声称达到"人类水平"驱动性能的说法很常见但很少量化，峰值扭矩或速度规格无法反映在任务相关姿态和速率下能否提供正确的扭矩、功率和耐力组合。

Method: 1) 基于ISB标准的自由度图谱标准化关节坐标系和运动范围；2) 人类等效包络定义每个关节在相同角度和速率下同时满足人类扭矩和功率的要求；3) 人类水平驱动评分整合六个物理因素：工作空间覆盖、HEE覆盖、扭矩模式带宽、效率和热可持续性。

Result: 提供了详细的测量协议，使用测力计、电功率监测和热测试，通过可重复实验获得所有HLAS输入。通过多关节人形机器人的实例演示了HLAS计算，揭示了峰值扭矩规格所掩盖的驱动器权衡。

Conclusion: 该框架既可作为人形机器人开发的设计规范，也可作为比较驱动系统的基准标准，所有组件都基于已发表的人类生物力学数据。

Abstract: Claims that humanoid robots achieve ``human-level'' actuation are common but rarely quantified. Peak torque or speed specifications tell us little about whether a joint can deliver the right combination of torque, power, and endurance at task-relevant postures and rates. We introduce a comprehensive framework that makes ``human-level'' measurable and comparable across systems. Our approach has three components. First, a kinematic \emph{DoF atlas} standardizes joint coordinate systems and ranges of motion using ISB-based conventions, ensuring that human and robot joints are compared in the same reference frames. Second, \emph{Human-Equivalence Envelopes (HEE)} define per-joint requirements by measuring whether a robot meets human torque \emph{and} power simultaneously at the same joint angle and rate $(q,ω)$, weighted by positive mechanical work in task-specific bands (walking, stairs, lifting, reaching, and hand actions). Third, the \emph{Human-Level Actuation Score (HLAS)} aggregates six physically grounded factors: workspace coverage (ROM and DoF), HEE coverage, torque-mode bandwidth, efficiency, and thermal sustainability. We provide detailed measurement protocols using dynamometry, electrical power monitoring, and thermal testing that yield every HLAS input from reproducible experiments. A worked example demonstrates HLAS computation for a multi-joint humanoid, showing how the score exposes actuator trade-offs (gearing ratio versus bandwidth and efficiency) that peak-torque specifications obscure. The framework serves as both a design specification for humanoid development and a benchmarking standard for comparing actuation systems, with all components grounded in published human biomechanics data.

</details>


### [29] [Vision-Aided Online A* Path Planning for Efficient and Safe Navigation of Service Robots](https://arxiv.org/abs/2511.06801)
*Praveen Kumar,Tushar Sandhan*

Main category: cs.RO

TL;DR: 提出了一种将轻量级语义感知与实时路径规划相结合的新框架，使低成本机器人能够在复杂环境中进行上下文感知导航，区分重要物体与普通障碍物。


<details>
  <summary>Details</summary>
Motivation: 传统导航系统依赖昂贵的LiDAR，虽然几何精度高但缺乏语义理解能力，无法区分重要文档与普通垃圾，限制了服务机器人在人类环境中的部署。

Method: 采用轻量级语义分割模型识别用户定义的视觉约束，将语义感知与几何数据融合，通过在线A*规划器将视觉约束投影为全局地图中的非几何障碍物。

Result: 在高保真仿真和真实机器人平台上验证，展示了稳健的实时性能，证明低成本机器人能够安全导航复杂环境并尊重传统规划器无法识别的关键视觉线索。

Conclusion: 该框架成功填补了感知与规划之间的关键空白，实现了在低成本嵌入式硬件上的上下文感知导航，为自主服务机器人在人类中心环境中的部署提供了可行解决方案。

Abstract: The deployment of autonomous service robots in human-centric environments is hindered by a critical gap in perception and planning. Traditional navigation systems rely on expensive LiDARs that, while geometrically precise, are seman- tically unaware, they cannot distinguish a important document on an office floor from a harmless piece of litter, treating both as physically traversable. While advanced semantic segmentation exists, no prior work has successfully integrated this visual intelligence into a real-time path planner that is efficient enough for low-cost, embedded hardware. This paper presents a frame- work to bridge this gap, delivering context-aware navigation on an affordable robotic platform. Our approach centers on a novel, tight integration of a lightweight perception module with an online A* planner. The perception system employs a semantic segmentation model to identify user-defined visual constraints, enabling the robot to navigate based on contextual importance rather than physical size alone. This adaptability allows an operator to define what is critical for a given task, be it sensitive papers in an office or safety lines in a factory, thus resolving the ambiguity of what to avoid. This semantic perception is seamlessly fused with geometric data. The identified visual constraints are projected as non-geometric obstacles onto a global map that is continuously updated from sensor data, enabling robust navigation through both partially known and unknown environments. We validate our framework through extensive experiments in high-fidelity simulations and on a real-world robotic platform. The results demonstrate robust, real-time performance, proving that a cost- effective robot can safely navigate complex environments while respecting critical visual cues invisible to traditional planners.

</details>


### [30] [Vision-Based System Identification of a Quadrotor](https://arxiv.org/abs/2511.06839)
*Selim Ahmet Iz,Mustafa Unel*

Main category: cs.RO

TL;DR: 该论文研究了基于视觉的系统辨识技术在四旋翼建模与控制中的应用，通过灰箱建模和LQR控制器设计，验证了机载视觉系统在四旋翼建模中的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决四旋翼建模中的复杂性和局限性，特别是在推力和阻力系数方面存在的不确定性，探索基于视觉的系统辨识技术如何提升四旋翼建模与控制性能。

Method: 采用灰箱建模方法缓解不确定性，评估机载视觉系统的有效性，基于机载视觉系统数据设计LQR控制器进行系统辨识建模。

Result: 结果显示模型间性能一致，验证了基于视觉的系统辨识技术的有效性，证明了该技术在提升四旋翼建模和控制方面的潜力。

Conclusion: 基于视觉的系统辨识技术能够有效增强四旋翼建模与控制，为未来四旋翼性能提升、故障检测和决策过程研究奠定了基础。

Abstract: This paper explores the application of vision-based system identification techniques in quadrotor modeling and control. Through experiments and analysis, we address the complexities and limitations of quadrotor modeling, particularly in relation to thrust and drag coefficients. Grey-box modeling is employed to mitigate uncertainties, and the effectiveness of an onboard vision system is evaluated. An LQR controller is designed based on a system identification model using data from the onboard vision system. The results demonstrate consistent performance between the models, validating the efficacy of vision based system identification. This study highlights the potential of vision-based techniques in enhancing quadrotor modeling and control, contributing to improved performance and operational capabilities. Our findings provide insights into the usability and consistency of these techniques, paving the way for future research in quadrotor performance enhancement, fault detection, and decision-making processes.

</details>


### [31] [Multi-Agent AI Framework for Road Situation Detection and C-ITS Message Generation](https://arxiv.org/abs/2511.06892)
*Kailin Tong,Selim Solmaz,Kenan Mujkic,Gottfried Allmer,Bo Leng*

Main category: cs.RO

TL;DR: 提出多智能体AI框架，结合多模态大语言模型和视觉感知进行道路状况监测，在自定义数据集上评估显示100%召回率但存在误检问题。


<details>
  <summary>Details</summary>
Motivation: 传统道路状况检测方法在预定义场景中表现良好，但在未知情况下失效且缺乏语义解释，这对可靠的交通推荐至关重要。

Method: 使用多智能体框架处理摄像头数据，协调专用智能体进行状况检测、距离估计、决策制定和C-ITS消息生成，评估基于TAD数据集的103张图像。

Result: 状况检测召回率达100%，消息模式正确性完美，但存在误检问题，在车道数、行驶车道状态和原因代码方面性能下降。Gemini-2.5-Flash在检测准确性和语义理解方面表现不如Gemini-2.0-Flash且延迟更高。

Conclusion: 需要进一步研究针对智能交通应用专门优化的LLMs或MLLMs的微调工作。

Abstract: Conventional road-situation detection methods achieve strong performance in predefined scenarios but fail in unseen cases and lack semantic interpretation, which is crucial for reliable traffic recommendations. This work introduces a multi-agent AI framework that combines multimodal large language models (MLLMs) with vision-based perception for road-situation monitoring. The framework processes camera feeds and coordinates dedicated agents for situation detection, distance estimation, decision-making, and Cooperative Intelligent Transport System (C-ITS) message generation. Evaluation is conducted on a custom dataset of 103 images extracted from 20 videos of the TAD dataset. Both Gemini-2.0-Flash and Gemini-2.5-Flash were evaluated. The results show 100\% recall in situation detection and perfect message schema correctness; however, both models suffer from false-positive detections and have reduced performance in terms of number of lanes, driving lane status and cause code. Surprisingly, Gemini-2.5-Flash, though more capable in general tasks, underperforms Gemini-2.0-Flash in detection accuracy and semantic understanding and incurs higher latency (Table II). These findings motivate further work on fine-tuning specialized LLMs or MLLMs tailored for intelligent transportation applications.

</details>


### [32] [Integration of Visual SLAM into Consumer-Grade Automotive Localization](https://arxiv.org/abs/2511.06919)
*Luis Diener,Jens Kalkkuhl,Markus Enzweiler*

Main category: cs.RO

TL;DR: 提出一种融合视觉SLAM与车辆横向动力学模型的框架，用于在线校准陀螺仪，提高消费级车辆的定位精度。


<details>
  <summary>Details</summary>
Motivation: 消费级车辆目前依赖轮式里程计和IMU进行自我运动估计，但这些传感器存在系统误差和校准问题。视觉惯性SLAM在机器人领域已是标准，但在汽车自我运动估计中的应用仍待探索。

Method: 开发了一个融合视觉SLAM与车辆横向动力学模型的框架，能够在实际驾驶条件下在线校准陀螺仪。

Result: 实验结果表明，基于视觉的集成显著提高了陀螺仪校准精度，从而提升了整体定位性能。在公共基准测试中相比最先进方法表现出更优越的定位精度。

Conclusion: 视觉SLAM与车辆动力学模型的融合为提高汽车定位精度提供了一条有前景的技术路径。

Abstract: Accurate ego-motion estimation in consumer-grade vehicles currently relies on proprioceptive sensors, i.e. wheel odometry and IMUs, whose performance is limited by systematic errors and calibration. While visual-inertial SLAM has become a standard in robotics, its integration into automotive ego-motion estimation remains largely unexplored. This paper investigates how visual SLAM can be integrated into consumer-grade vehicle localization systems to improve performance. We propose a framework that fuses visual SLAM with a lateral vehicle dynamics model to achieve online gyroscope calibration under realistic driving conditions. Experimental results demonstrate that vision-based integration significantly improves gyroscope calibration accuracy and thus enhances overall localization performance, highlighting a promising path toward higher automotive localization accuracy. We provide results on both proprietary and public datasets, showing improved performance and superior localization accuracy on a public benchmark compared to state-of-the-art methods.

</details>


### [33] [Raspi$^2$USBL: An open-source Raspberry Pi-Based Passive Inverted Ultra-Short Baseline Positioning System for Underwater Robotics](https://arxiv.org/abs/2511.06998)
*Jin Huang,Yingqiang Wang,Ying Chen*

Main category: cs.RO

TL;DR: Raspi²USBL是一个基于树莓派的开源被动倒置超短基线定位系统，为水下机器人研究提供低成本、可访问的定位解决方案。


<details>
  <summary>Details</summary>
Motivation: 由于GNSS信号无法穿透海面，精确的水下定位仍然是水下机器人的基本挑战。需要开发低成本、可访问的解决方案来降低水下机器人研究的门槛。

Method: 系统采用模块化硬件架构，包括水听器阵列、多通道前置放大器、OCXO、树莓派5和MCC DAQ板。使用开源C++软件框架进行高精度时钟同步、实时信号处理（匹配滤波、阵列波束成形、自适应增益控制）来估计飞行时间和信号到达方向。

Result: 在消声池、淡水湖和公海试验中验证，斜距精度优于0.1%，方位精度在0.1°以内，在1.3公里距离内保持稳定性能。

Conclusion: 低成本、可复制的硬件可以提供研究级的水下定位精度。开源硬件和软件平台降低了水下机器人实验室的入门门槛，促进了水下声学导航和群体机器人的协作创新。

Abstract: Precise underwater positioning remains a fundamental challenge for underwater robotics since global navigation satellite system (GNSS) signals cannot penetrate the sea surface. This paper presents Raspi$^2$USBL, an open-source, Raspberry Pi-based passive inverted ultra-short baseline (piUSBL) positioning system designed to provide a low-cost and accessible solution for underwater robotic research. The system comprises a passive acoustic receiver and an active beacon. The receiver adopts a modular hardware architecture that integrates a hydrophone array, a multichannel preamplifier, an oven-controlled crystal oscillator (OCXO), a Raspberry Pi 5, and an MCC-series data acquisition (DAQ) board. Apart from the Pi 5, OCXO, and MCC board, the beacon comprises an impedance-matching network, a power amplifier, and a transmitting transducer. An open-source C++ software framework provides high-precision clock synchronization and triggering for one-way travel-time (OWTT) messaging, while performing real-time signal processing, including matched filtering, array beamforming, and adaptive gain control, to estimate the time of flight (TOF) and direction of arrival (DOA) of received signals. The Raspi$^2$USBL system was experimentally validated in an anechoic tank, freshwater lake, and open-sea trials. Results demonstrate a slant-range accuracy better than 0.1%, a bearing accuracy within 0.1$^\circ$, and stable performance over operational distances up to 1.3 km. These findings confirm that low-cost, reproducible hardware can deliver research-grade underwater positioning accuracy. By releasing both the hardware and software as open-source, Raspi$^2$USBL provides a unified reference platform that lowers the entry barrier for underwater robotics laboratories, fosters reproducibility, and promotes collaborative innovation in underwater acoustic navigation and swarm robotics.

</details>


### [34] [HDCNet: A Hybrid Depth Completion Network for Grasping Transparent and Reflective Objects](https://arxiv.org/abs/2511.07081)
*Guanghu Xie,Mingxu Li,Songwei Wu,Yang Liu,Zongwu Xie,Baoshi Cao,Hong Liu*

Main category: cs.RO

TL;DR: HDCNet是一个用于透明和反射物体深度感知的深度补全网络，结合了Transformer、CNN和Mamba架构的优势，在多个数据集上达到SOTA性能，并能显著提高机器人抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 传统深度传感器在透明和反射表面上无法提供可靠的测量数据，限制了机器人在感知和抓取任务中的性能。

Method: 提出HDCNet网络，采用双分支Transformer-CNN编码器提取模态特定特征，在浅层引入轻量级多模态融合模块，在网络瓶颈处开发Transformer-Mamba混合融合模块实现深度信息融合。

Result: 在多个公共数据集上达到最先进的深度补全性能，机器人抓取实验显示对透明和反射物体的抓取成功率最高提升60%。

Conclusion: HDCNet通过有效融合多模态特征，显著提高了透明和反射物体的深度感知精度和机器人抓取性能。

Abstract: Depth perception of transparent and reflective objects has long been a critical challenge in robotic manipulation.Conventional depth sensors often fail to provide reliable measurements on such surfaces, limiting the performance of robots in perception and grasping tasks. To address this issue, we propose a novel depth completion network,HDCNet,which integrates the complementary strengths of Transformer,CNN and Mamba architectures.Specifically,the encoder is designed as a dual-branch Transformer-CNN framework to extract modality-specific features. At the shallow layers of the encoder, we introduce a lightweight multimodal fusion module to effectively integrate low-level features. At the network bottleneck,a Transformer-Mamba hybrid fusion module is developed to achieve deep integration of high-level semantic and global contextual information, significantly enhancing depth completion accuracy and robustness. Extensive evaluations on multiple public datasets demonstrate that HDCNet achieves state-of-the-art(SOTA) performance in depth completion tasks.Furthermore,robotic grasping experiments show that HDCNet substantially improves grasp success rates for transparent and reflective objects,achieving up to a 60% increase.

</details>


### [35] [Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2511.07155)
*Thomas Steinecker,Alexander Bienemann,Denis Trescher,Thorsten Luettel,Mirko Maehlisch*

Main category: cs.RO

TL;DR: 提出了一个通过空间和时间对齐策略将虚拟车辆与真实系统解耦的框架，实现了强化学习运动规划从仿真到现实的零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 由于车辆动力学复杂性和仿真与现实之间的不匹配，将强化学习部署到真实车辆上仍然具有挑战性。轮胎特性、路面条件、空气动力学扰动和车辆负载等因素使得准确建模真实世界动力学不可行，阻碍了在仿真中训练的RL智能体直接迁移到现实。

Method: 首先在仿真中使用运动学自行车模型训练RL智能体输出连续控制动作，然后将其行为提炼为预测轨迹的智能体生成有限视野的自车轨迹，实现虚拟和真实车辆的同步。部署时，Stanley控制器控制横向动力学，而纵向对齐通过自适应更新机制维持，补偿虚拟和真实轨迹之间的偏差。

Result: 在真实车辆上验证了该方法，证明所提出的对齐策略能够实现基于RL的运动规划从仿真到现实的鲁棒零样本迁移，成功地将高级轨迹生成与低级车辆控制解耦。

Conclusion: 该框架通过将运动规划与车辆控制解耦，并通过空间和时间对齐策略实现虚拟和真实车辆的同步，有效解决了RL在真实车辆部署中的仿真到现实迁移问题。

Abstract: Reinforcement learning (RL) has shown promise in robotics, but deploying RL on real vehicles remains challenging due to the complexity of vehicle dynamics and the mismatch between simulation and reality. Factors such as tire characteristics, road surface conditions, aerodynamic disturbances, and vehicle load make it infeasible to model real-world dynamics accurately, which hinders direct transfer of RL agents trained in simulation. In this paper, we present a framework that decouples motion planning from vehicle control through a spatial and temporal alignment strategy between a virtual vehicle and the real system. An RL agent is first trained in simulation using a kinematic bicycle model to output continuous control actions. Its behavior is then distilled into a trajectory-predicting agent that generates finite-horizon ego-vehicle trajectories, enabling synchronization between virtual and real vehicles. At deployment, a Stanley controller governs lateral dynamics, while longitudinal alignment is maintained through adaptive update mechanisms that compensate for deviations between virtual and real trajectories. We validate our approach on a real vehicle and demonstrate that the proposed alignment strategy enables robust zero-shot transfer of RL-based motion planning from simulation to reality, successfully decoupling high-level trajectory generation from low-level vehicle control.

</details>


### [36] [Automated Generation of Continuous-Space Roadmaps for Routing Mobile Robot Fleets](https://arxiv.org/abs/2511.07175)
*Marvin Rüdt,Constantin Enke,Kai Furmans*

Main category: cs.RO

TL;DR: 提出了一种自动化路线图生成方法，在连续空间中结合运输需求和最小距离约束，为移动机器人车队提供高效鲁棒的路径规划。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么基于网格牺牲几何精度，要么在连续空间中忽略实际约束，需要一种兼顾几何精度和实际应用需求的路线图生成方法。

Method: 结合自由空间离散化、运输需求驱动的K最短路径优化和路径平滑，在连续空间中生成满足节点和边最小距离约束的路线图。

Result: 在多个内部物流用例中，该方法始终优于4连通网格、8连通网格和随机采样等基准方法，实现了更低的结构复杂度、更高的冗余度和接近最优的路径长度。

Conclusion: 该方法能够生成适合内部物流应用的定制化路线图，支持移动机器人车队的高效鲁棒路由。

Abstract: Efficient routing of mobile robot fleets is crucial in intralogistics, where delays and deadlocks can substantially reduce system throughput. Roadmap design, specifying feasible transport routes, directly affects fleet coordination and computational performance. Existing approaches are either grid-based, compromising geometric precision, or continuous-space approaches that disregard practical constraints. This paper presents an automated roadmap generation approach that bridges this gap by operating in continuous-space, integrating station-to-station transport demand and enforcing minimum distance constraints for nodes and edges. By combining free space discretization, transport demand-driven $K$-shortest-path optimization, and path smoothing, the approach produces roadmaps tailored to intralogistics applications. Evaluation across multiple intralogistics use cases demonstrates that the proposed approach consistently outperforms established baselines (4-connected grid, 8-connected grid, and random sampling), achieving lower structural complexity, higher redundancy, and near-optimal path lengths, enabling efficient and robust routing of mobile robot fleets.

</details>


### [37] [Robotic versus Human Teleoperation for Remote Ultrasound](https://arxiv.org/abs/2511.07275)
*David Black,Septimiu Salcudean*

Main category: cs.RO

TL;DR: 比较了人类远程操作和机器人远程操作在远程超声检查中的表现，发现两者在完成时间和位置精度上没有显著差异，人类远程操作在力控制方面更一致，且更具实用性和可及性。


<details>
  <summary>Details</summary>
Motivation: 解决农村地区缺乏超声检查专业人员的困境，通过远程操作技术让非专业人员在专家指导下进行超声检查，比较人类远程操作与机器人远程操作的相对优势。

Method: 评估人类和机器人远程操作在设置时间、灵活性等实用方面的差异，并通过实验比较完成时间、位置跟踪和力一致性等性能指标。

Result: 人类远程操作在完成时间和位置精度上与机器人远程操作无显著差异（平均差异分别为1.8%和0.5%），在力应用方面更一致，且更具实用性和可及性。

Conclusion: 人类远程操作在性能上与机器人远程操作相当，但由于成本更低、复杂性更小，对于小型社区更具实用价值。

Abstract: Diagnostic medical ultrasound is widely used, safe, and relatively low cost but requires a high degree of expertise to acquire and interpret the images. Personnel with this expertise are often not available outside of larger cities, leading to difficult, costly travel and long wait times for rural populations. To address this issue, tele-ultrasound techniques are being developed, including robotic teleoperation and recently human teleoperation, in which a novice user is remotely guided in a hand-over-hand manner through mixed reality to perform an ultrasound exam. These methods have not been compared, and their relative strengths are unknown. Human teleoperation may be more practical than robotics for small communities due to its lower cost and complexity, but this is only relevant if the performance is comparable. This paper therefore evaluates the differences between human and robotic teleoperation, examining practical aspects such as setup time and flexibility and experimentally comparing performance metrics such as completion time, position tracking, and force consistency. It is found that human teleoperation does not lead to statistically significant differences in completion time or position accuracy, with mean differences of 1.8% and 0.5%, respectively, and provides more consistent force application despite being substantially more practical and accessible.

</details>


### [38] [PlanT 2.0: Exposing Biases and Structural Flaws in Closed-Loop Driving](https://arxiv.org/abs/2511.07292)
*Simon Gerstenecker,Andreas Geiger,Katrin Renz*

Main category: cs.RO

TL;DR: 该论文提出了PlanT 2.0，一个用于自动驾驶研究的轻量级、以对象为中心的规划变换器，在CARLA基准测试中实现了最先进性能，并通过系统化分析揭示了模型失败的根本原因。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶研究过于关注基准性能和方法创新，而缺乏对模型失败、偏见和捷径学习的深入分析，导致改进有限且缺乏对失败原因的理解。

Method: 引入PlanT 2.0，一个基于对象级表示的轻量级规划变换器，通过可控的输入扰动（如改变对象位置或增删对象）来系统分析模型行为。

Result: 在CARLA Leaderboard 2.0的挑战性场景中，PlanT 2.0在Longest6 v2、Bench2Drive和CARLA验证路线上实现了最先进的性能，同时揭示了场景理解不足、专家行为僵化导致可被利用的捷径、以及对固定专家轨迹的过拟合等问题。

Conclusion: 基于分析结果，作者主张转向以数据为中心的开发方法，关注构建更丰富、更鲁棒、偏见更少的数据集，并开源了代码和模型。

Abstract: Most recent work in autonomous driving has prioritized benchmark performance and methodological innovation over in-depth analysis of model failures, biases, and shortcut learning. This has led to incremental improvements without a deep understanding of the current failures. While it is straightforward to look at situations where the model fails, it is hard to understand the underlying reason. This motivates us to conduct a systematic study, where inputs to the model are perturbed and the predictions observed. We introduce PlanT 2.0, a lightweight, object-centric planning transformer designed for autonomous driving research in CARLA. The object-level representation enables controlled analysis, as the input can be easily perturbed (e.g., by changing the location or adding or removing certain objects), in contrast to sensor-based models. To tackle the scenarios newly introduced by the challenging CARLA Leaderboard 2.0, we introduce multiple upgrades to PlanT, achieving state-of-the-art performance on Longest6 v2, Bench2Drive, and the CARLA validation routes. Our analysis exposes insightful failures, such as a lack of scene understanding caused by low obstacle diversity, rigid expert behaviors leading to exploitable shortcuts, and overfitting to a fixed set of expert trajectories. Based on these findings, we argue for a shift toward data-centric development, with a focus on richer, more robust, and less biased datasets. We open-source our code and model at https://github.com/autonomousvision/plant2.

</details>


### [39] [Exact Smooth Reformulations for Trajectory Optimization Under Signal Temporal Logic Specifications](https://arxiv.org/abs/2511.07375)
*Shaohang Han,Joris Verhagen,Jana Tumova*

Main category: cs.RO

TL;DR: 提出了一个基于信号时序逻辑（STL）的运动规划方法，通过精确重构最大和最小算子，将STL合成转化为可微的轨迹优化问题。


<details>
  <summary>Details</summary>
Motivation: 信号时序逻辑（STL）是描述时空需求的有用形式化工具，但STL合成通常面临非可微性问题，需要开发精确且可微的求解方法。

Method: 利用STL鲁棒性语义，将STL合成构建为轨迹优化问题，并引入最大和最小算子的精确重构，确保问题可微且无近似误差。

Result: 该方法在数值模拟中得到验证，展示了其实际性能，证明方法具有精确性、平滑性和可靠性。

Conclusion: 提出的STL运动规划方法能够精确处理时空规范，为满足复杂时空需求的运动规划提供了有效的解决方案。

Abstract: We study motion planning under Signal Temporal Logic (STL), a useful formalism for specifying spatial-temporal requirements. We pose STL synthesis as a trajectory optimization problem leveraging the STL robustness semantics. To obtain a differentiable problem without approximation error, we introduce an exact reformulation of the max and min operators. The resulting method is exact, smooth, and sound. We validate it in numerical simulations, demonstrating its practical performance.

</details>


### [40] [Residual Rotation Correction using Tactile Equivariance](https://arxiv.org/abs/2511.07381)
*Yizhe Zhu,Zhang Ye,Boce Hu,Haibo Zhao,Yu Qi,Dian Wang,Robert Platt*

Main category: cs.RO

TL;DR: EquiTac是一个利用SO(2)对称性提升触觉策略学习样本效率和泛化能力的框架，通过等变网络预测旋转动作来增强基础视觉运动策略。


<details>
  <summary>Details</summary>
Motivation: 触觉数据收集成本高，需要提高触觉策略学习的样本效率。

Method: 从视觉触觉传感器的RGB输入重建表面法线，使用SO(2)等变网络预测旋转动作残差来增强基础策略。

Result: 在真实机器人上，EquiTac仅需少量训练样本就能实现对新方向的无泛化，而基线方法即使使用更多数据也无法达到。

Conclusion: 这是首个在策略学习中显式编码触觉等变性的方法，提供了一个轻量级、对称感知的模块，提高了接触丰富任务的可靠性。

Abstract: Visuotactile policy learning augments vision-only policies with tactile input, facilitating contact-rich manipulation. However, the high cost of tactile data collection makes sample efficiency the key requirement for developing visuotactile policies. We present EquiTac, a framework that exploits the inherent SO(2) symmetry of in-hand object rotation to improve sample efficiency and generalization for visuotactile policy learning. EquiTac first reconstructs surface normals from raw RGB inputs of vision-based tactile sensors, so rotations of the normal vector field correspond to in-hand object rotations. An SO(2)-equivariant network then predicts a residual rotation action that augments a base visuomotor policy at test time, enabling real-time rotation correction without additional reorientation demonstrations. On a real robot, EquiTac accurately achieves robust zero-shot generalization to unseen in-hand orientations with very few training samples, where baselines fail even with more training data. To our knowledge, this is the first tactile learning method to explicitly encode tactile equivariance for policy learning, yielding a lightweight, symmetry-aware module that improves reliability in contact-rich tasks.

</details>


### [41] [Unified Humanoid Fall-Safety Policy from a Few Demonstrations](https://arxiv.org/abs/2511.07407)
*Zhengjie Xu,Ye Li,Kwan-yee Lin,Stella X. Yu*

Main category: cs.RO

TL;DR: 提出了一种统一策略，将跌倒预防、冲击缓解和快速恢复整合到一个策略中，通过融合稀疏人类演示、强化学习和自适应扩散记忆来实现安全自主的跌倒恢复过程。


<details>
  <summary>Details</summary>
Motivation: 人形机器人跌倒是一个固有风险，现有方法只关注跌倒的孤立方面（避免跌倒、控制下降或站起），缺乏应对真实跌倒情况的综合策略。

Method: 融合稀疏人类演示与强化学习，结合自适应扩散记忆的安全反应记忆，学习自适应全身行为。

Result: 在仿真和Unitree G1机器人上的实验展示了稳健的仿真到现实迁移、更低的冲击力和跨不同干扰的持续快速恢复。

Conclusion: 该方法为实现真实环境中更安全、更具韧性的人形机器人指明了方向。

Abstract: Falling is an inherent risk of humanoid mobility. Maintaining stability is thus a primary safety focus in robot control and learning, yet no existing approach fully averts loss of balance. When instability does occur, prior work addresses only isolated aspects of falling: avoiding falls, choreographing a controlled descent, or standing up afterward. Consequently, humanoid robots lack integrated strategies for impact mitigation and prompt recovery when real falls defy these scripts. We aim to go beyond keeping balance to make the entire fall-and-recovery process safe and autonomous: prevent falls when possible, reduce impact when unavoidable, and stand up when fallen. By fusing sparse human demonstrations with reinforcement learning and an adaptive diffusion-based memory of safe reactions, we learn adaptive whole-body behaviors that unify fall prevention, impact mitigation, and rapid recovery in one policy. Experiments in simulation and on a Unitree G1 demonstrate robust sim-to-real transfer, lower impact forces, and consistently fast recovery across diverse disturbances, pointing towards safer, more resilient humanoids in real environments. Videos are available at https://firm2025.github.io/.

</details>


### [42] [Using Vision Language Models as Closed-Loop Symbolic Planners for Robotic Applications: A Control-Theoretic Perspective](https://arxiv.org/abs/2511.07410)
*Hao Wang,Sathwik Karnik,Bea Lim,Somil Bansal*

Main category: cs.RO

TL;DR: 研究从控制理论视角探讨如何将视觉语言模型用作机器人应用的闭环符号规划器，重点分析控制时域和预热启动对规划性能的影响。


<details>
  <summary>Details</summary>
Motivation: LLMs和VLMs在具身符号规划中广泛应用，但如何有效用于闭环符号规划仍待探索。由于这些模型是黑箱，会产生不可预测或代价高昂的错误，使其在机器人高层规划中面临挑战。

Method: 从控制理论视角研究VLM作为闭环符号规划器，设计受控实验分析控制时域和预热启动对规划性能的影响。

Result: 通过实验获得了对VLM作为闭环符号规划器的深入理解，提出了可提高VLM符号规划器性能的建议。

Conclusion: 研究为有效利用VLM作为闭环符号规划器提供了控制理论视角的见解和实用建议。

Abstract: Large Language Models (LLMs) and Vision Language Models (VLMs) have been widely used for embodied symbolic planning. Yet, how to effectively use these models for closed-loop symbolic planning remains largely unexplored. Because they operate as black boxes, LLMs and VLMs can produce unpredictable or costly errors, making their use in high-level robotic planning especially challenging. In this work, we investigate how to use VLMs as closed-loop symbolic planners for robotic applications from a control-theoretic perspective. Concretely, we study how the control horizon and warm-starting impact the performance of VLM symbolic planners. We design and conduct controlled experiments to gain insights that are broadly applicable to utilizing VLMs as closed-loop symbolic planners, and we discuss recommendations that can help improve the performance of VLM symbolic planners.

</details>


### [43] [Robot Learning from a Physical World Model](https://arxiv.org/abs/2511.07416)
*Jiageng Mao,Sicheng He,Hao-Ning Wu,Yang You,Shuyang Sun,Zhicheng Wang,Yanan Bao,Huizhong Chen,Leonidas Guibas,Vitor Guizilini,Howard Zhou,Yue Wang*

Main category: cs.RO

TL;DR: PhysWorld是一个通过物理世界建模从视频生成中学习机器人操作的框架，将生成视频中的像素运动转化为物理准确的机器人动作


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型可以从语言命令和图像合成逼真的视觉演示，但直接将像素运动重定向到机器人会忽略物理约束，导致操作不准确

Method: 通过耦合视频生成与物理世界重建，从单张图像和任务命令生成任务条件视频，重建底层物理世界，并通过基于对象的残差强化学习将视频运动转化为物理准确动作

Result: 在多样化的真实世界任务实验中，PhysWorld相比先前方法显著提高了操作精度

Conclusion: 该框架将隐式视觉指导转化为物理可执行的机器人轨迹，无需真实机器人数据收集，实现零样本可泛化的机器人操作

Abstract: We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit \href{https://pointscoder.github.io/PhysWorld_Web/}{the project webpage} for details.

</details>


### [44] [Lightning Grasp: High Performance Procedural Grasp Synthesis with Contact Fields](https://arxiv.org/abs/2511.07418)
*Zhao-Heng Yin,Pieter Abbeel*

Main category: cs.RO

TL;DR: Lightning Grasp是一种高性能的程序化抓取合成算法，相比现有方法实现数量级加速，能够为不规则工具类物体生成无监督抓取。


<details>
  <summary>Details</summary>
Motivation: 尽管经过多年研究，灵巧手的实时多样化抓取合成仍然是机器人和计算机图形学中未解决的核心挑战。现有方法存在需要精心调整能量函数和敏感初始化等限制。

Method: 通过关键洞察：使用简单高效的数据结构——接触场，将复杂几何计算与搜索过程解耦。这种抽象降低了问题复杂度，实现了前所未有的程序化搜索速度。

Result: 实现了比最先进方法快几个数量级的加速，同时能够为不规则工具类物体生成无监督抓取。

Conclusion: 该方法突破了现有方法的限制，开源系统以推动机器人操作的进一步创新。

Abstract: Despite years of research, real-time diverse grasp synthesis for dexterous hands remains an unsolved core challenge in robotics and computer graphics. We present Lightning Grasp, a novel high-performance procedural grasp synthesis algorithm that achieves orders-of-magnitude speedups over state-of-the-art approaches, while enabling unsupervised grasp generation for irregular, tool-like objects. The method avoids many limitations of prior approaches, such as the need for carefully tuned energy functions and sensitive initialization. This breakthrough is driven by a key insight: decoupling complex geometric computation from the search process via a simple, efficient data structure - the Contact Field. This abstraction collapses the problem complexity, enabling a procedural search at unprecedented speeds. We open-source our system to propel further innovation in robotic manipulation.

</details>
