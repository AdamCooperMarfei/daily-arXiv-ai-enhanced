{"id": "2509.10570", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10570", "abs": "https://arxiv.org/abs/2509.10570", "authors": ["Wei Dai", "Shengen Wu", "Wei Wu", "Zhenhao Wang", "Sisuo Lyu", "Haicheng Liao", "Limin Yu", "Weiping Ding", "Runwei Guan", "Yutao Yue"], "title": "Large Foundation Models for Trajectory Prediction in Autonomous Driving: A Comprehensive Survey", "comment": "22 pages, 6 figures", "summary": "Trajectory prediction serves as a critical functionality in autonomous\ndriving, enabling the anticipation of future motion paths for traffic\nparticipants such as vehicles and pedestrians, which is essential for driving\nsafety. Although conventional deep learning methods have improved accuracy,\nthey remain hindered by inherent limitations, including lack of\ninterpretability, heavy reliance on large-scale annotated data, and weak\ngeneralization in long-tail scenarios. The rise of Large Foundation Models\n(LFMs) is transforming the research paradigm of trajectory prediction. This\nsurvey offers a systematic review of recent advances in LFMs, particularly\nLarge Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for\ntrajectory prediction. By integrating linguistic and scene semantics, LFMs\nfacilitate interpretable contextual reasoning, significantly enhancing\nprediction safety and generalization in complex environments. The article\nhighlights three core methodologies: trajectory-language mapping, multimodal\nfusion, and constraint-based reasoning. It covers prediction tasks for both\nvehicles and pedestrians, evaluation metrics, and dataset analyses. Key\nchallenges such as computational latency, data scarcity, and real-world\nrobustness are discussed, along with future research directions including\nlow-latency inference, causality-aware modeling, and motion foundation models.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u7cfb\u7edf\u56de\u987e\u4e86\u5927\u578b\u57fa\u7840\u6a21\u578b\uff08LFMs\uff09\u5728\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86\u6838\u5fc3\u65b9\u6cd5\u3001\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u8f68\u8ff9\u9884\u6d4b\u4e2d\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u3001\u957f\u5c3e\u573a\u666f\u6cdb\u5316\u80fd\u529b\u5f31\u7b49\u5c40\u9650\u6027\uff0c\u800c\u5927\u578b\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u6574\u5408\u8bed\u8a00\u548c\u573a\u666f\u8bed\u4e49\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u63d0\u5347\u9884\u6d4b\u5b89\u5168\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u6838\u5fc3\u65b9\u6cd5\uff1a\u8f68\u8ff9-\u8bed\u8a00\u6620\u5c04\u3001\u591a\u6a21\u6001\u878d\u5408\u548c\u57fa\u4e8e\u7ea6\u675f\u7684\u63a8\u7406\uff0c\u6db5\u76d6\u4e86\u8f66\u8f86\u548c\u884c\u4eba\u7684\u9884\u6d4b\u4efb\u52a1\u3001\u8bc4\u4f30\u6307\u6807\u548c\u6570\u636e\u96c6\u5206\u6790\u3002", "result": "LFMs\u901a\u8fc7\u8bed\u8a00\u548c\u573a\u666f\u8bed\u4e49\u7684\u6574\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e0b\u7684\u9884\u6d4b\u5b89\u5168\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u8f68\u8ff9\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u8303\u5f0f\u3002", "conclusion": "\u5c3d\u7ba1LFMs\u5728\u8f68\u8ff9\u9884\u6d4b\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u9762\u4e34\u8ba1\u7b97\u5ef6\u8fdf\u3001\u6570\u636e\u7a00\u7f3a\u548c\u73b0\u5b9e\u4e16\u754c\u9c81\u68d2\u6027\u7b49\u6311\u6218\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u4f4e\u5ef6\u8fdf\u63a8\u7406\u3001\u56e0\u679c\u611f\u77e5\u5efa\u6a21\u548c\u8fd0\u52a8\u57fa\u7840\u6a21\u578b\u7b49\u3002"}}
{"id": "2509.10692", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10692", "abs": "https://arxiv.org/abs/2509.10692", "authors": ["Giuseppe Silano", "Amr Afifi", "Martin Saska", "Antonio Franchi"], "title": "STL-Based Motion Planning and Uncertainty-Aware Risk Analysis for Human-Robot Collaboration with a Multi-Rotor Aerial Vehicle", "comment": "39 pages, 13 figures", "summary": "This paper presents a novel approach to motion planning and risk analysis for\nenhancing human-robot collaboration using a Multi-Rotor Aerial Vehicle (MRAV).\nThe proposed method uses Signal Temporal Logic (STL) to encode key mission\nobjectives, such as safety, timing, and human preferences, with a strong focus\non ergonomics and comfort. An optimization framework generates dynamically\nfeasible trajectories while considering the MRAV's physical constraints. Given\nthe nonlinear and non-convex nature of the problem, smooth approximations and\ngradient-based techniques assist in handling the problem's computational\ncomplexity. Additionally, an uncertainty-aware risk analysis is incorporated to\nassess potential deviations from the mission specifications, providing insights\ninto the likelihood of mission success under uncertain conditions. Further, an\nevent-triggered replanning strategy is implemented to respond to unforeseen\nevents and external disturbances. The approach is validated through MATLAB and\nGazebo simulations, using an object handover task in a mock-up environment\ninspired by power line maintenance scenarios. The results highlight the\nmethod's effectiveness in achieving safe, efficient, and resilient human-robot\ncollaboration.", "AI": {"tldr": "\u57fa\u4e8e\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\u7684\u591a\u65cb\u7ffc\u98de\u884c\u5668\u4eba\u673a\u534f\u4f5c\u8fd0\u52a8\u89c4\u5212\u4e0e\u98ce\u9669\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6846\u67b6\u751f\u6210\u52a8\u6001\u53ef\u884c\u8f68\u8ff9\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u98ce\u9669\u5206\u6790\u548c\u4e8b\u4ef6\u89e6\u53d1\u91cd\u89c4\u5212\u7b56\u7565\uff0c\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684\u4eba\u673a\u534f\u4f5c\u3002", "motivation": "\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u4e2d\u591a\u65cb\u7ffc\u98de\u884c\u5668\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u7535\u529b\u7ebf\u8def\u7ef4\u62a4\u7b49\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\uff0c\u9700\u8981\u540c\u65f6\u8003\u8651\u5b89\u5168\u6027\u3001\u65f6\u95f4\u7ea6\u675f\u548c\u4eba\u4f53\u5de5\u7a0b\u5b66\u8212\u9002\u5ea6\u3002", "method": "\u4f7f\u7528\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91(STL)\u7f16\u7801\u4efb\u52a1\u76ee\u6807\uff0c\u6784\u5efa\u4f18\u5316\u6846\u67b6\u751f\u6210\u52a8\u6001\u53ef\u884c\u8f68\u8ff9\uff0c\u91c7\u7528\u5e73\u6ed1\u8fd1\u4f3c\u548c\u68af\u5ea6\u6280\u672f\u5904\u7406\u975e\u7ebf\u6027\u975e\u51f8\u95ee\u9898\uff0c\u96c6\u6210\u4e0d\u786e\u5b9a\u6027\u98ce\u9669\u5206\u6790\u548c\u4e8b\u4ef6\u89e6\u53d1\u91cd\u89c4\u5212\u7b56\u7565\u3002", "result": "MATLAB\u548cGazebo\u4eff\u771f\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u7535\u529b\u7ebf\u8def\u7ef4\u62a4\u573a\u666f\u7684\u5bf9\u8c61\u4ea4\u63a5\u4efb\u52a1\u4e2d\u80fd\u591f\u5b9e\u73b0\u5b89\u5168\u3001\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u4eba\u673a\u534f\u4f5c\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u65cb\u7ffc\u98de\u884c\u5668\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u548c\u98ce\u9669\u5206\u6790\u95ee\u9898\uff0c\u4e3a\u9ad8\u98ce\u9669\u73af\u5883\u4e0b\u7684\u5b89\u5168\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10730", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10730", "abs": "https://arxiv.org/abs/2509.10730", "authors": ["Yunfan Ren", "Yixi Cai", "Haotian Li", "Nan Chen", "Fangcheng Zhu", "Longji Yin", "Fanze Kong", "Rundong Li", "Fu Zhang"], "title": "A Survey on LiDAR-based Autonomous Aerial Vehicles", "comment": null, "summary": "This survey offers a comprehensive overview of recent advancements in\nLiDAR-based autonomous Unmanned Aerial Vehicles (UAVs), covering their design,\nperception, planning, and control strategies. Over the past decade, LiDAR\ntechnology has become a crucial enabler for high-speed, agile, and reliable UAV\nnavigation, especially in GPS-denied environments. The paper begins by\nexamining the evolution of LiDAR sensors, emphasizing their unique advantages\nsuch as high accuracy, long-range depth measurements, and robust performance\nunder various lighting conditions, making them particularly well-suited for UAV\napplications. The integration of LiDAR with UAVs has significantly enhanced\ntheir autonomy, enabling complex missions in diverse and challenging\nenvironments. Subsequently, we explore essential software components, including\nperception technologies for state estimation and mapping, as well as trajectory\nplanning and control methodologies, and discuss their adoption in LiDAR-based\nUAVs. Additionally, we analyze various practical applications of the\nLiDAR-based UAVs, ranging from industrial operations to supporting different\naerial platforms and UAV swarm deployments. The survey concludes by discussing\nexisting challenges and proposing future research directions to advance\nLiDAR-based UAVs and enhance multi-UAV collaboration. By synthesizing recent\ndevelopments, this paper aims to provide a valuable resource for researchers\nand practitioners working to push the boundaries of LiDAR-based UAV systems.", "AI": {"tldr": "\u6fc0\u5149\u96f7\u8fbe\u65e0\u4eba\u673a\u6280\u672f\u7efc\u8ff0\uff1a\u6db5\u76d6\u8bbe\u8ba1\u3001\u611f\u77e5\u3001\u89c4\u5212\u4e0e\u63a7\u5236\u7b56\u7565\uff0c\u91cd\u70b9\u5206\u6790\u6fc0\u5149\u96f7\u8fbe\u5728GPS\u62d2\u6b62\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u4f18\u52bf\u53ca\u5e94\u7528\u524d\u666f", "motivation": "\u6fc0\u5149\u96f7\u8fbe\u6280\u672f\u5728\u8fc7\u53bb\u5341\u5e74\u4e2d\u6210\u4e3a\u9ad8\u901f\u3001\u654f\u6377\u3001\u53ef\u9760\u65e0\u4eba\u673a\u5bfc\u822a\u7684\u5173\u952e\u4f7f\u80fd\u6280\u672f\uff0c\u7279\u522b\u662f\u5728GPS\u62d2\u6b62\u73af\u5883\u4e2d\u3002\u672c\u6587\u65e8\u5728\u5168\u9762\u7efc\u8ff0\u6fc0\u5149\u96f7\u8fbe\u65e0\u4eba\u673a\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u8d44\u6e90", "method": "\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5206\u6790\u6fc0\u5149\u96f7\u8fbe\u4f20\u611f\u5668\u7684\u6f14\u8fdb\u3001\u8f6f\u4ef6\u7ec4\u4ef6\uff08\u72b6\u6001\u4f30\u8ba1\u3001\u5efa\u56fe\u3001\u8f68\u8ff9\u89c4\u5212\u4e0e\u63a7\u5236\u65b9\u6cd5\uff09\u4ee5\u53ca\u5b9e\u9645\u5e94\u7528\u6848\u4f8b", "result": "\u6fc0\u5149\u96f7\u8fbe\u4e0e\u65e0\u4eba\u673a\u7684\u96c6\u6210\u663e\u8457\u589e\u5f3a\u4e86\u81ea\u4e3b\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u591a\u6837\u5316\u548c\u6311\u6218\u6027\u73af\u5883\u4e2d\u6267\u884c\u590d\u6742\u4efb\u52a1\uff0c\u5305\u62ec\u5de5\u4e1a\u64cd\u4f5c\u3001\u591a\u5e73\u53f0\u652f\u6301\u548c\u65e0\u4eba\u673a\u7fa4\u90e8\u7f72", "conclusion": "\u6fc0\u5149\u96f7\u8fbe\u65e0\u4eba\u673a\u7cfb\u7edf\u5728\u611f\u77e5\u7cbe\u5ea6\u548c\u73af\u5883\u9002\u5e94\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63a8\u52a8\u6280\u672f\u53d1\u5c55\u548c\u591a\u65e0\u4eba\u673a\u534f\u4f5c"}}
{"id": "2509.10735", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10735", "abs": "https://arxiv.org/abs/2509.10735", "authors": ["Mohammad Rafiee Javazm", "Yash Kulkarni", "Jiaqi Xue", "Naruhiko Ikoma", "Farshid Alambeigi"], "title": "Analytical Design and Development of a Modular and Intuitive Framework for Robotizing and Enhancing the Existing Endoscopic Procedures", "comment": null, "summary": "Despite the widespread adoption of endoscopic devices for several cancer\nscreening procedures, manual control of these devices still remains challenging\nfor clinicians, leading to several critical issues such as increased workload,\nfatigue, and distractions. To address these issues, in this paper, we introduce\nthe design and development of an intuitive, modular, and easily installable\nmechatronic framework. This framework includes (i) a novel nested collet-chuck\ngripping mechanism that can readily be integrated and assembled with the\nexisting endoscopic devices and control their bending degrees-of-freedom\n(DoFs); (ii) a feeder mechanism that can control the insertion/retraction DoF\nof a colonoscope, and (iii) a complementary and intuitive user interface that\nenables simultaneous control of all DoFs during the procedure. To analyze the\ndesign of the proposed mechanisms, we also introduce a mathematical modeling\napproach and a design space for optimal selection of the parameters involved in\nthe design of gripping and feeder mechanisms. Our simulation and experimental\nstudies thoroughly demonstrate the performance of the proposed mathematical\nmodeling and robotic framework.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u4e3a\u5185\u955c\u8bbe\u5907\u8bbe\u8ba1\u7684\u6a21\u5757\u5316\u673a\u7535\u6846\u67b6\uff0c\u5305\u62ec\u65b0\u578b\u6346\u7d27\u673a\u5236\u3001\u8fdb\u7ed9\u673a\u5236\u548c\u76f4\u89c2\u7528\u6237\u754c\u9762\uff0c\u4ee5\u51cf\u8f7b\u533b\u751f\u624b\u52a8\u64cd\u4f5c\u5185\u955c\u7684\u8d1f\u62c5\u3002", "motivation": "\u624b\u52a8\u64cd\u4f5c\u5185\u955c\u8bbe\u5907\u5bf9\u4e34\u5e8a\u533b\u751f\u5177\u6709\u6311\u6218\u6027\uff0c\u5bfc\u81f4\u5de5\u4f5c\u8d1f\u8377\u91cd\u3001\u75b2\u52b3\u548c\u5206\u5fc3\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u9ad8\u8bca\u7597\u6548\u679c\u3002", "method": "\u8bbe\u8ba1\u548c\u5f00\u53d1\u5305\u542b\uff1a(1)\u65b0\u578b\u5d4c\u5957\u5934\u5934\u6346\u7d27\u673a\u5236\u63a7\u5236\u5f2f\u66f2\u5ea6\uff1b(2)\u8fdb\u7ed9\u673a\u5236\u63a7\u5236\u63d2\u5165/\u64a4\u56de\uff1b(3)\u76f4\u89c2\u7528\u6237\u754c\u9762\u540c\u65f6\u63a7\u5236\u6240\u6709\u5ea6\u7684\u81ea\u7531\u5ea6\u3002\u4f7f\u7528\u6570\u5b66\u5efa\u6a21\u8fdb\u884c\u8bbe\u8ba1\u5206\u6790\u548c\u53c2\u6570\u4f18\u5316\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u9a8c\u7814\u7a76\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u6570\u5b66\u6a21\u578b\u548c\u673a\u5668\u4eba\u6846\u67b6\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u4ee5\u7b80\u5316\u5185\u955c\u64cd\u4f5c\u3001\u51cf\u8f7b\u533b\u751f\u8d1f\u62c5\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u5185\u955c\u8bbe\u5907\u81ea\u52a8\u5316\u63a7\u5236\u9886\u57df\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2509.10757", "categories": ["cs.RO", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.10757", "abs": "https://arxiv.org/abs/2509.10757", "authors": ["Kimia Khabiri", "Parsa Hosseininejad", "Shishir Gopinath", "Karthik Dantu", "Steven Y. Ko"], "title": "FastTrack: GPU-Accelerated Tracking for Visual SLAM", "comment": "Accepted for presentation at IROS 2025, preprint", "summary": "The tracking module of a visual-inertial SLAM system processes incoming image\nframes and IMU data to estimate the position of the frame in relation to the\nmap. It is important for the tracking to complete in a timely manner for each\nframe to avoid poor localization or tracking loss. We therefore present a new\napproach which leverages GPU computing power to accelerate time-consuming\ncomponents of tracking in order to improve its performance. These components\ninclude stereo feature matching and local map tracking. We implement our design\ninside the ORB-SLAM3 tracking process using CUDA. Our evaluation demonstrates\nan overall improvement in tracking performance of up to 2.8x on a desktop and\nJetson Xavier NX board in stereo-inertial mode, using the well-known SLAM\ndatasets EuRoC and TUM-VI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528GPU\u52a0\u901f\u89c6\u89c9\u60ef\u6027SLAM\u7cfb\u7edf\u4e2d\u8ddf\u8e2a\u6a21\u5757\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728ORB-SLAM3\u4e2d\u5b9e\u73b0CUDA\u52a0\u901f\uff0c\u5c06\u7acb\u4f53\u7279\u5f81\u5339\u914d\u548c\u5c40\u90e8\u5730\u56fe\u8ddf\u8e2a\u6027\u80fd\u63d0\u5347\u6700\u9ad82.8\u500d", "motivation": "\u89c6\u89c9\u60ef\u6027SLAM\u7cfb\u7edf\u7684\u8ddf\u8e2a\u6a21\u5757\u9700\u8981\u53ca\u65f6\u5904\u7406\u56fe\u50cf\u5e27\u548cIMU\u6570\u636e\u6765\u4f30\u8ba1\u4f4d\u7f6e\uff0c\u5982\u679c\u5904\u7406\u4e0d\u53ca\u65f6\u4f1a\u5bfc\u81f4\u5b9a\u4f4d\u8d28\u91cf\u4e0b\u964d\u6216\u8ddf\u8e2a\u4e22\u5931\uff0c\u56e0\u6b64\u9700\u8981\u52a0\u901f\u8017\u65f6\u7ec4\u4ef6", "method": "\u5229\u7528GPU\u8ba1\u7b97\u80fd\u529b\u52a0\u901f\u8ddf\u8e2a\u4e2d\u7684\u8017\u65f6\u7ec4\u4ef6\uff0c\u5305\u62ec\u7acb\u4f53\u7279\u5f81\u5339\u914d\u548c\u5c40\u90e8\u5730\u56fe\u8ddf\u8e2a\uff0c\u5728ORB-SLAM3\u8ddf\u8e2a\u6d41\u7a0b\u4e2d\u4f7f\u7528CUDA\u5b9e\u73b0", "result": "\u5728\u684c\u9762\u548cJetson Xavier NX\u677f\u4e0a\uff0c\u4f7f\u7528EuRoC\u548cTUM-VI\u6570\u636e\u96c6\u8fdb\u884c\u7acb\u4f53\u60ef\u6027\u6a21\u5f0f\u6d4b\u8bd5\uff0c\u8ddf\u8e2a\u6027\u80fd\u6574\u4f53\u63d0\u5347\u6700\u9ad8\u8fbe2.8\u500d", "conclusion": "GPU\u52a0\u901f\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u89c6\u89c9\u60ef\u6027SLAM\u7cfb\u7edf\u7684\u8ddf\u8e2a\u6027\u80fd\uff0c\u786e\u4fdd\u5b9e\u65f6\u5904\u7406\u80fd\u529b\uff0c\u907f\u514d\u5b9a\u4f4d\u8d28\u91cf\u95ee\u9898\u548c\u8ddf\u8e2a\u4e22\u5931"}}
{"id": "2509.10771", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10771", "abs": "https://arxiv.org/abs/2509.10771", "authors": ["Clemens Schwarke", "Mayank Mittal", "Nikita Rudin", "David Hoeller", "Marco Hutter"], "title": "RSL-RL: A Learning Library for Robotics Research", "comment": null, "summary": "RSL-RL is an open-source Reinforcement Learning library tailored to the\nspecific needs of the robotics community. Unlike broad general-purpose\nframeworks, its design philosophy prioritizes a compact and easily modifiable\ncodebase, allowing researchers to adapt and extend algorithms with minimal\noverhead. The library focuses on algorithms most widely adopted in robotics,\ntogether with auxiliary techniques that address robotics-specific challenges.\nOptimized for GPU-only training, RSL-RL achieves high-throughput performance in\nlarge-scale simulation environments. Its effectiveness has been validated in\nboth simulation benchmarks and in real-world robotic experiments, demonstrating\nits utility as a lightweight, extensible, and practical framework to develop\nlearning-based robotic controllers. The library is open-sourced at:\nhttps://github.com/leggedrobotics/rsl_rl.", "AI": {"tldr": "RSL-RL\u662f\u4e00\u4e2a\u4e13\u4e3a\u673a\u5668\u4eba\u793e\u533a\u8bbe\u8ba1\u7684\u5f00\u6e90\u5f3a\u5316\u5b66\u4e60\u5e93\uff0c\u5177\u6709\u7d27\u51d1\u3001\u6613\u4fee\u6539\u7684\u4ee3\u7801\u67b6\u6784\uff0c\u4e13\u6ce8\u4e8e\u673a\u5668\u4eba\u9886\u57df\u5e38\u7528\u7b97\u6cd5\uff0c\u652f\u6301GPU\u8bad\u7ec3\u5e76\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u9488\u5bf9\u673a\u5668\u4eba\u793e\u533a\u5bf9\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u7684\u7279\u6b8a\u9700\u6c42\uff0c\u73b0\u6709\u901a\u7528\u6846\u67b6\u8fc7\u4e8e\u5e9e\u5927\u4e14\u96be\u4ee5\u4fee\u6539\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u4e14\u4e13\u6ce8\u4e8e\u673a\u5668\u4eba\u5e94\u7528\u7684\u5f3a\u5316\u5b66\u4e60\u5e93\u3002", "method": "\u91c7\u7528\u7d27\u51d1\u4e14\u6613\u4e8e\u4fee\u6539\u7684\u4ee3\u7801\u8bbe\u8ba1\uff0c\u4e13\u6ce8\u4e8e\u673a\u5668\u4eba\u9886\u57df\u5e7f\u6cdb\u91c7\u7528\u7684\u7b97\u6cd5\u548c\u8f85\u52a9\u6280\u672f\uff0c\u4f18\u5316GPU\u8bad\u7ec3\u4ee5\u5b9e\u73b0\u5927\u89c4\u6a21\u4eff\u771f\u73af\u5883\u7684\u9ad8\u541e\u5410\u91cf\u6027\u80fd\u3002", "result": "\u5728\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u548c\u5b9e\u7528\u6846\u67b6\u7684\u4ef7\u503c\u3002", "conclusion": "RSL-RL\u6210\u529f\u4e3a\u673a\u5668\u4eba\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e13\u95e8\u4f18\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5177\u6709\u8f7b\u91cf\u3001\u6613\u6269\u5c55\u548c\u5b9e\u7528\u7684\u7279\u70b9\uff0c\u5f00\u6e90\u5730\u5740\u4e3ahttps://github.com/leggedrobotics/rsl_rl\u3002"}}
{"id": "2509.10796", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10796", "abs": "https://arxiv.org/abs/2509.10796", "authors": ["Hanjing Ye", "Weixi Situ", "Jianwei Peng", "Yu Zhan", "Bingyi Xia", "Kuanqi Cai", "Hong Zhang"], "title": "Follow-Bench: A Unified Motion Planning Benchmark for Socially-Aware Robot Person Following", "comment": "TBD. All code, data, and deployment scripts are publicly available at\n  https://follow-bench.github.io/", "summary": "Robot person following (RPF) -- mobile robots that follow and assist a\nspecific person -- has emerging applications in personal assistance, security\npatrols, eldercare, and logistics. To be effective, such robots must follow the\ntarget while ensuring safety and comfort for both the target and surrounding\npeople. In this work, we present the first end-to-end study of RPF, which (i)\nsurveys representative scenarios, motion-planning methods, and evaluation\nmetrics with a focus on safety and comfort; (ii) introduces Follow-Bench, a\nunified benchmark simulating diverse scenarios, including various target\ntrajectory patterns, dynamic-crowd flows, and environmental layouts; and (iii)\nre-implements six popular RPF planners, ensuring that both safety and comfort\nare systematically considered. Moreover, we evaluate the two highest-performing\nplanners from our benchmark on a differential-drive robot to provide insights\ninto real-world deployment. Extensive simulation and real-world experiments\nprovide quantitative insights into the safety-comfort trade-offs of existing\nplanners, while revealing open challenges and future research directions.", "AI": {"tldr": "\u8fd9\u662f\u9996\u4e2a\u673a\u5668\u4eba\u8ddf\u968f\u7cfb\u7edf\u7684\u7ed3\u6784\u5316\u7814\u7a76\uff0c\u5305\u62ec\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0Follow-Bench\u3001\u516d\u79cd\u89c4\u5212\u5668\u91cd\u73b0\u8bc4\u6d4b\uff0c\u4ee5\u53ca\u5b89\u5168\u6027\u4e0e\u8212\u9002\u6027\u7684\u7cfb\u7edf\u5206\u6790\u3002", "motivation": "\u673a\u5668\u4eba\u8ddf\u968f\u5728\u4e2a\u4eba\u52a9\u7406\u3001\u5b89\u5168\u5de1\u68c0\u3001\u517b\u8001\u62a4\u7406\u7b49\u9886\u57df\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u9700\u8981\u7cfb\u7edf\u8003\u8651\u8ddf\u968f\u76ee\u6807\u7684\u540c\u65f6\u786e\u4fdd\u5b89\u5168\u6027\u548c\u8212\u9002\u6027\u3002", "method": "\u6784\u5efaFollow-Bench\u7edf\u4e00\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6a21\u62df\u591a\u6837\u5316\u573a\u666f\uff0c\u91cd\u65b0\u5b9e\u73b0\u516d\u79cd\u666e\u904dRPF\u89c4\u5212\u5668\uff0c\u5e76\u5728\u5dee\u5206\u9a71\u52a8\u673a\u5668\u4e0a\u8fdb\u884c\u5b9e\u9645\u90e8\u7f72\u8bc4\u6d4b\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u6a21\u62df\u548c\u5b9e\u9645\u5b9e\u9a8c\uff0c\u63d0\u4f9b\u4e86\u73b0\u6709\u89c4\u5212\u5668\u5728\u5b89\u5168\u6027\u4e0e\u8212\u9002\u6027\u4e4b\u95f4\u7684\u4ea4\u6362\u5173\u7cfb\u7684\u5b9a\u91cf\u5206\u6790\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u73b0\u6709\u6280\u672f\u7684\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u673a\u5668\u4eba\u8ddf\u968f\u7cfb\u7edf\u7684\u5b89\u5168\u8212\u9002\u6027\u4f18\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2509.10862", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10862", "abs": "https://arxiv.org/abs/2509.10862", "authors": ["Temma Suzuki", "Kento Kawaharazuka", "Kei Okada"], "title": "A Universal Wire Testing Machine for Enhancing the Performance of Wire-Driven Robots", "comment": "Accepted at Humanoids2025, website -\n  https://tenrobo18.github.io/wiretester-humanoids2025-webpage/", "summary": "Compared with gears and linkages, wires constitute a lightweight,\nlow-friction transmission mechanism. However, because wires are flexible\nmaterials, they tend to introduce large modeling errors, and their adoption in\nindustrial and research robots remains limited.In this study, we built a\nUniversal Wire Testing Machine that enables measurement and adjustment of wire\ncharacteristics to improve the performance of wire-driven mechanisms. Using\nthis testing machine, we carried out removal of initial wire stretch,\nmeasurement of tension transmission efficiency for eight different diameters of\npassive pulleys, and measurement of the dynamic behavior of variable-length\nwires. Finally, we applied the data obtained from this testing machine to the\nforce control of an actual wire-driven robot, reducing the end-effector force\nerror.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u9e1f\u7528\u4e8e\u6d4b\u91cf\u548c\u8c03\u6574\u7ebf\u7f06\u7279\u6027\u7684\u6d4b\u8bd5\u673a\uff0c\u901a\u8fc7\u5b9e\u9a8c\u6d4b\u91cf\u4e86\u7ebf\u7f06\u7684\u5f20\u529b\u4f20\u9012\u6548\u7387\u548c\u52a8\u6001\u884c\u4e3a\uff0c\u5e76\u5e94\u7528\u4e8e\u5b9e\u9645\u7ebf\u9a71\u52a8\u673a\u5668\u4eba\u7684\u529b\u63a7\u5236\uff0c\u964d\u4f4e\u4e86\u7aef\u6267\u884c\u5668\u529b\u9519\u8bef\u3002", "motivation": "\u867d\u7136\u7ebf\u7f06\u4f5c\u4e3a\u8f7b\u91cf\u5316\u3001\u4f4e\u6469\u64e6\u4f20\u52a8\u673a\u5236\u6709\u5176\u4f18\u52bf\uff0c\u4f46\u56e0\u4e3a\u7ebf\u7f06\u7684\u7075\u6d3b\u6027\u8d28\u5bfc\u81f4\u5efa\u6a21\u9519\u8bef\u8f83\u5927\uff0c\u5728\u5de5\u4e1a\u548c\u7814\u7a76\u673a\u5668\u4eba\u4e2d\u7684\u5e94\u7528\u4ecd\u7136\u6709\u9650\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u79cd\u5168\u80fd\u578b\u7ebf\u7f06\u6d4b\u8bd5\u673a\uff0c\u8fdb\u884c\u4e86\u4e09\u4e2a\u5b9e\u9a8c\uff1a1\uff09\u79fb\u9664\u7ebf\u7f06\u7684\u521d\u59cb\u4f38\u5c55 2\uff09\u6d4b\u91cf8\u79cd\u4e0d\u540c\u76f4\u5f84\u88c5\u7f6e\u6f06\u8f6e\u7684\u5f20\u529b\u4f20\u9012\u6548\u7387 3\uff09\u6d4b\u91cf\u53d8\u957f\u5ea6\u7ebf\u7f06\u7684\u52a8\u6001\u884c\u4e3a", "result": "\u901a\u8fc7\u6d4b\u8bd5\u673a\u83b7\u5f97\u4e86\u7ebf\u7f06\u7684\u5177\u4f53\u7279\u6027\u6570\u636e\uff0c\u5e76\u5c06\u8fd9\u4e9b\u6570\u636e\u5e94\u7528\u4e8e\u5b9e\u9645\u7ebf\u9a71\u52a8\u673a\u5668\u4eba\u7684\u529b\u63a7\u5236\u7cfb\u7edf\u4e2d", "conclusion": "\u8be5\u6d4b\u8bd5\u673a\u80fd\u591f\u6709\u6548\u63d0\u5347\u7ebf\u9a71\u52a8\u673a\u5236\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u7cbe\u786e\u6d4b\u91cf\u548c\u8c03\u6574\u7ebf\u7f06\u7279\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7aef\u6267\u884c\u5668\u7684\u529b\u63a7\u5236\u9519\u8bef\uff0c\u4e3a\u7ebf\u9a71\u52a8\u673a\u5668\u4eba\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2509.10884", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10884", "abs": "https://arxiv.org/abs/2509.10884", "authors": ["Qingxiang Liu", "Ting Huang", "Zeyu Zhang", "Hao Tang"], "title": "Nav-R1: Reasoning and Navigation in Embodied Scenes", "comment": null, "summary": "Embodied navigation requires agents to integrate perception, reasoning, and\naction for robust interaction in complex 3D environments. Existing approaches\noften suffer from incoherent and unstable reasoning traces that hinder\ngeneralization across diverse environments, and difficulty balancing\nlong-horizon semantic reasoning with low-latency control for real-time\nnavigation. To address these challenges, we propose Nav-R1, an embodied\nfoundation model that unifies reasoning in embodied environments. We first\nconstruct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought\n(CoT) for embodied tasks, which enables cold-start initialization with\nstructured reasoning. Building on this foundation, we design a GRPO-based\nreinforcement learning framework with three complementary rewards: format,\nunderstanding, and navigation, to improve structural adherence, semantic\ngrounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow\nreasoning paradigm, decoupling deliberate semantic reasoning from low-latency\nreactive control for efficient yet coherent navigation. Extensive evaluations\non embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms\nstrong baselines, with over 8% average improvement in reasoning and navigation\nperformance. Real-world deployment on a mobile robot further validates its\nrobustness under limited onboard resources. Code:\nhttps://github.com/AIGeeksGroup/Nav-R1. Website:\nhttps://aigeeksgroup.github.io/Nav-R1.", "AI": {"tldr": "Nav-R1\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5177\u8eab\u63a8\u7406\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21CoT\u6570\u636e\u96c6\u548cGRPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u63a8\u7406\u4e0d\u7a33\u5b9a\u548c\u5b9e\u65f6\u63a7\u5236\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u53478%\u4ee5\u4e0a\u3002", "motivation": "\u73b0\u6709\u5177\u8eab\u5bfc\u822a\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u8f68\u8ff9\u4e0d\u8fde\u8d2f\u4e0d\u7a33\u5b9a\u3001\u96be\u4ee5\u5e73\u8861\u957f\u65f6\u7a0b\u8bed\u4e49\u63a8\u7406\u4e0e\u4f4e\u5ef6\u8fdf\u5b9e\u65f6\u63a7\u5236\u7684\u95ee\u9898\uff0c\u963b\u788d\u4e86\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "1) \u6784\u5efaNav-CoT-110K\u5927\u89c4\u6a21\u9010\u6b65\u63a8\u7406\u6570\u636e\u96c6\u8fdb\u884c\u51b7\u542f\u52a8\u521d\u59cb\u5316\uff1b2) \u8bbe\u8ba1GRPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u683c\u5f0f\u3001\u7406\u89e3\u548c\u5bfc\u822a\u4e09\u4e2a\u4e92\u8865\u5956\u52b1\uff1b3) \u63d0\u51faFast-in-Slow\u63a8\u7406\u8303\u5f0f\uff0c\u5c06\u6df1\u601d\u719f\u8651\u7684\u8bed\u4e49\u63a8\u7406\u4e0e\u4f4e\u5ef6\u8fdf\u53cd\u5e94\u63a7\u5236\u89e3\u8026\u3002", "result": "\u5728\u5177\u8eabAI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u8d85\u8d8a\u5f3a\u57fa\u7ebf\uff0c\u63a8\u7406\u548c\u5bfc\u822a\u6027\u80fd\u5e73\u5747\u63d0\u5347\u8d85\u8fc78%\u3002\u5728\u79fb\u52a8\u673a\u5668\u4eba\u4e0a\u7684\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u9a8c\u8bc1\u4e86\u5176\u5728\u6709\u9650\u673a\u8f7d\u8d44\u6e90\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "Nav-R1\u901a\u8fc7\u7edf\u4e00\u63a8\u7406\u6846\u67b6\u548c\u521b\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5177\u8eab\u5bfc\u822a\u4e2d\u7684\u63a8\u7406\u7a33\u5b9a\u6027\u548c\u5b9e\u65f6\u63a7\u5236\u6311\u6218\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10888", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10888", "abs": "https://arxiv.org/abs/2509.10888", "authors": ["Weijie Liu", "Ziyi Qiu", "Shihang Wang", "Deqing Mei", "Yancheng Wang"], "title": "Design of scalable orthogonal digital encoding architecture for large-area flexible tactile sensing in robotics", "comment": "6 pages, 9 figures(Accepted to IEEE/RSJ International Conference on\n  Intelligent Robots and Systems, 2025)", "summary": "Human-like embodied tactile perception is crucial for the next-generation\nintelligent robotics. Achieving large-area, full-body soft coverage with high\nsensitivity and rapid response, akin to human skin, remains a formidable\nchallenge due to critical bottlenecks in encoding efficiency and wiring\ncomplexity in existing flexible tactile sensors, thus significantly hinder the\nscalability and real-time performance required for human skin-level tactile\nperception. Herein, we present a new architecture employing code division\nmultiple access-inspired orthogonal digital encoding to overcome these\nchallenges. Our decentralized encoding strategy transforms conventional serial\nsignal transmission by enabling parallel superposition of energy-orthogonal\nbase codes from distributed sensing nodes, drastically reducing wiring\nrequirements and increasing data throughput. We implemented and validated this\nstrategy with off-the-shelf 16-node sensing array to reconstruct the pressure\ndistribution, achieving a temporal resolution of 12.8 ms using only a single\ntransmission wire. Crucially, the architecture can maintain sub-20ms latency\nacross orders-of-magnitude variations in node number (to thousands of nodes).\nBy fundamentally redefining signal encoding paradigms in soft electronics, this\nwork opens new frontiers in developing scalable embodied intelligent systems\nwith human-like sensory capabilities.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eCDMA\u6b63\u4ea4\u6570\u5b57\u7f16\u7801\u7684\u65b0\u578b\u89e6\u89c9\u4f20\u611f\u67b6\u6784\uff0c\u901a\u8fc7\u5e76\u884c\u4fe1\u53f7\u4f20\u8f93\u5927\u5e45\u51cf\u5c11\u5e03\u7ebf\u9700\u6c42\uff0c\u5b9e\u73b0\u5355\u7ebf\u4f20\u8f93\u548c\u6beb\u79d2\u7ea7\u5ef6\u8fdf\uff0c\u4e3a\u5927\u89c4\u6a21\u8f6f\u4f53\u89e6\u89c9\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848", "motivation": "\u89e3\u51b3\u73b0\u6709\u67d4\u6027\u89e6\u89c9\u4f20\u611f\u5668\u5728\u7f16\u7801\u6548\u7387\u548c\u5e03\u7ebf\u590d\u6742\u5ea6\u65b9\u9762\u7684\u74f6\u9888\uff0c\u5b9e\u73b0\u7c7b\u4f3c\u4eba\u7c7b\u76ae\u80a4\u7684\u5927\u9762\u79ef\u3001\u9ad8\u7075\u654f\u5ea6\u3001\u5feb\u901f\u54cd\u5e94\u7684\u89e6\u89c9\u611f\u77e5\u80fd\u529b", "method": "\u91c7\u7528\u7801\u5206\u591a\u5740(CDMA)\u542f\u53d1\u7684\u6b63\u4ea4\u6570\u5b57\u7f16\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u4f20\u611f\u8282\u70b9\u7684\u80fd\u91cf\u6b63\u4ea4\u57fa\u7801\u5e76\u884c\u53e0\u52a0\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u4e32\u884c\u4fe1\u53f7\u4f20\u8f93", "result": "\u572816\u8282\u70b9\u4f20\u611f\u9635\u5217\u4e0a\u9a8c\u8bc1\uff0c\u4ec5\u7528\u5355\u6839\u4f20\u8f93\u7ebf\u5b9e\u73b012.8ms\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u538b\u529b\u5206\u5e03\u91cd\u5efa\uff0c\u5728\u6570\u5343\u8282\u70b9\u89c4\u6a21\u4e0b\u4ecd\u80fd\u4fdd\u6301\u4f4e\u4e8e20ms\u7684\u5ef6\u8fdf", "conclusion": "\u8be5\u67b6\u6784\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u8f6f\u7535\u5b50\u4fe1\u53f7\u7f16\u7801\u8303\u5f0f\uff0c\u4e3a\u5f00\u53d1\u5177\u6709\u7c7b\u4eba\u611f\u77e5\u80fd\u529b\u7684\u53ef\u6269\u5c55\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u524d\u6cbf"}}
{"id": "2509.10948", "categories": ["cs.RO", "cs.AI", "cs.CR", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.10948", "abs": "https://arxiv.org/abs/2509.10948", "authors": ["Navid Aftabi", "Philip Samaha", "Jin Ma", "Long Cheng", "Ramy Harik", "Dan Li"], "title": "ViSTR-GP: Online Cyberattack Detection via Vision-to-State Tensor Regression and Gaussian Processes in Automated Robotic Operations", "comment": null, "summary": "Industrial robotic systems are central to automating smart manufacturing\noperations. Connected and automated factories face growing cybersecurity risks\nthat can potentially cause interruptions and damages to physical operations.\nAmong these attacks, data-integrity attacks often involve sophisticated\nexploitation of vulnerabilities that enable an attacker to access and\nmanipulate the operational data and are hence difficult to detect with only\nexisting intrusion detection or model-based detection. This paper addresses the\nchallenges in utilizing existing side-channels to detect data-integrity attacks\nin robotic manufacturing processes by developing an online detection framework,\nViSTR-GP, that cross-checks encoder-reported measurements against a\nvision-based estimate from an overhead camera outside the controller's\nauthority. In this framework, a one-time interactive segmentation initializes\nSAM-Track to generate per-frame masks. A low-rank tensor-regression surrogate\nmaps each mask to measurements, while a matrix-variate Gaussian process models\nnominal residuals, capturing temporal structure and cross-joint correlations. A\nframe-wise test statistic derived from the predictive distribution provides an\nonline detector with interpretable thresholds. We validate the framework on a\nreal-world robotic testbed with synchronized video frame and encoder data,\ncollecting multiple nominal cycles and constructing replay attack scenarios\nwith graded end-effector deviations. Results on the testbed indicate that the\nproposed framework recovers joint angles accurately and detects data-integrity\nattacks earlier with more frequent alarms than all baselines. These\nimprovements are most evident in the most subtle attacks. These results show\nthat plants can detect data-integrity attacks by adding an independent physical\nchannel, bypassing the controller's authority, without needing complex\ninstrumentation.", "AI": {"tldr": "\u57fa\u4e8eV\u5f62\u611f\u77e5\u548c\u5faa\u73af\u56de\u5f52\u7684\u5728\u7ebf\u68c0\u6d4b\u6846\u67b6ViSTR-GP\uff0c\u901a\u8fc7\u8d85\u8d1f\u76f8\u673a\u76d1\u63a7\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u53ef\u53ca\u65e9\u68c0\u6d4b\u6570\u636e\u5b8c\u6574\u6027\u653b\u51fb\uff0c\u7279\u522b\u662f\u7ec6\u5fae\u653b\u51fb\u573a\u666f", "motivation": "\u5de5\u4e1a\u673a\u5668\u4eba\u7cfb\u7edf\u9762\u4e34\u65e5\u76ca\u4e25\u91cd\u7684\u7f51\u7edc\u5b89\u5168\u98ce\u9669\uff0c\u6570\u636e\u5b8c\u6574\u6027\u653b\u51fb\u96be\u4ee5\u901a\u8fc7\u4f20\u7edf\u5165\u4fb5\u68c0\u6d4b\u65b9\u6cd5\u53d1\u73b0\uff0c\u9700\u8981\u65b0\u7684\u68c0\u6d4b\u65b9\u6cd5", "method": "\u5f00\u53d1ViSTR-GP\u6846\u67b6\uff0c\u5229\u7528\u8d85\u8d1f\u76f8\u673a\u83b7\u53d6\u89c6\u89c9\u4f30\u8ba1\uff0c\u901a\u8fc7SAM-Track\u751f\u6210\u6bcf\u5e27\u906e\u7f69\u3001\u4f4e\u79e9\u5f20\u91cf\u56de\u5f52\u6620\u5c04\u5230\u6d4b\u91cf\u503c\uff0c\u4f7f\u7528\u77e9\u9635\u590d\u5408\u9ad8\u65af\u8fc7\u7a0b\u5efa\u6a21\u6b63\u5e38\u6b8b\u5dee", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u9a8c\u8bc1\uff0c\u6846\u67b6\u80fd\u51c6\u786e\u6062\u590d\u5173\u8282\u89d2\u5ea6\uff0c\u6bd4\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u66f4\u65e9\u68c0\u6d4b\u5230\u6570\u636e\u5b8c\u6574\u6027\u653b\u51fb\uff0c\u5728\u7ec6\u5fae\u653b\u51fb\u4e2d\u6536\u76ca\u6700\u660e\u663e", "conclusion": "\u7ecf\u9a8c\u8bc1\uff0c\u901a\u8fc7\u6dfb\u52a0\u72ec\u7acb\u7269\u7406\u901a\u9053\u53ef\u4ee5\u68c0\u6d4b\u6570\u636e\u5b8c\u6574\u6027\u653b\u51fb\uff0c\u7a81\u7834\u63a7\u5236\u5668\u6743\u9650\uff0c\u65e0\u9700\u590d\u6742\u4eea\u5668"}}
{"id": "2509.10952", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10952", "abs": "https://arxiv.org/abs/2509.10952", "authors": ["Yangcen Liu", "Woo Chul Shin", "Yunhai Han", "Zhenyang Chen", "Harish Ravichandar", "Danfei Xu"], "title": "ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation", "comment": "Conference of Robot Learning", "summary": "Learning robot manipulation from abundant human videos offers a scalable\nalternative to costly robot-specific data collection. However, domain gaps\nacross visual, morphological, and physical aspects hinder direct imitation. To\neffectively bridge the domain gap, we propose ImMimic, an embodiment-agnostic\nco-training framework that leverages both human videos and a small amount of\nteleoperated robot demonstrations. ImMimic uses Dynamic Time Warping (DTW) with\neither action- or visual-based mapping to map retargeted human hand poses to\nrobot joints, followed by MixUp interpolation between paired human and robot\ntrajectories. Our key insights are (1) retargeted human hand trajectories\nprovide informative action labels, and (2) interpolation over the mapped data\ncreates intermediate domains that facilitate smooth domain adaptation during\nco-training. Evaluations on four real-world manipulation tasks (Pick and Place,\nPush, Hammer, Flip) across four robotic embodiments (Robotiq, Fin Ray, Allegro,\nAbility) show that ImMimic improves task success rates and execution\nsmoothness, highlighting its efficacy to bridge the domain gap for robust robot\nmanipulation. The project website can be found at\nhttps://sites.google.com/view/immimic.", "AI": {"tldr": "ImMimic\u662f\u4e00\u4e2a\u8de8\u57df\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u4eba\u7c7b\u89c6\u9891\u548c\u5c11\u91cf\u673a\u5668\u4eba\u793a\u6559\u6570\u636e\uff0c\u901a\u8fc7\u52a8\u6001\u65f6\u95f4\u89c4\u6574\u548c\u6df7\u5408\u63d2\u503c\u6765\u5f25\u5408\u4eba\u673a\u9886\u57df\u5dee\u8ddd\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd", "motivation": "\u4ece\u4e30\u5bcc\u7684\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u673a\u5668\u4eba\u64cd\u4f5c\u53ef\u4ee5\u66ff\u4ee3\u6602\u8d35\u7684\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\uff0c\u4f46\u89c6\u89c9\u3001\u5f62\u6001\u548c\u7269\u7406\u65b9\u9762\u7684\u9886\u57df\u5dee\u8ddd\u963b\u788d\u4e86\u76f4\u63a5\u6a21\u4eff", "method": "\u4f7f\u7528\u52a8\u6001\u65f6\u95f4\u89c4\u6574(DTW)\u5c06\u91cd\u5b9a\u5411\u7684\u4eba\u7c7b\u624b\u90e8\u59ff\u6001\u6620\u5c04\u5230\u673a\u5668\u4eba\u5173\u8282\uff0c\u7136\u540e\u901a\u8fc7MixUp\u63d2\u503c\u5728\u914d\u5bf9\u7684\u4eba\u673a\u8f68\u8ff9\u4e4b\u95f4\u521b\u5efa\u4e2d\u95f4\u57df\uff0c\u5b9e\u73b0\u5e73\u6ed1\u7684\u9886\u57df\u81ea\u9002\u5e94", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u548c\u56db\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8bc4\u4f30\uff0cImMimic\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u6267\u884c\u6d41\u7545\u5ea6", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5f25\u5408\u4e86\u9886\u57df\u5dee\u8ddd\uff0c\u4e3a\u9c81\u68d2\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.10968", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10968", "abs": "https://arxiv.org/abs/2509.10968", "authors": ["Leo Cazenille", "Loona Macabre", "Nicolas Bredeche"], "title": "Pogosim -- a Simulator for Pogobot robots", "comment": "18 pages, 1 table, 7 figures", "summary": "Pogobots are a new type of open-source/open-hardware robots specifically\ndesigned for swarm robotics research. Their cost-effective and modular design,\ncomplemented by vibration-based and wheel-based locomotion, fast infrared\ncommunication and extensive software architecture facilitate the implementation\nof swarm intelligence algorithms. However, testing even simple distributed\nalgorithms directly on robots is particularly labor-intensive. Scaling to more\ncomplex problems or calibrate user code parameters will have a prohibitively\nhigh strain on available resources. In this article we present Pogosim, a fast\nand scalable simulator for Pogobots, designed to reduce as much as possible\nalgorithm development costs. The exact same code will be used in both\nsimulation and to experimentally drive real robots. This article details the\nsoftware architecture of Pogosim, explain how to write configuration files and\nuser programs and how simulations approximate or differ from experiments. We\ndescribe how a large set of simulations can be launched in parallel, how to\nretrieve and analyze the simulation results, and how to optimize user code\nparameters using optimization algorithms.", "AI": {"tldr": "Pogosim\u662f\u4e00\u4e2a\u4e13\u4e3aPogobots\u673a\u5668\u4eba\u8bbe\u8ba1\u7684\u5feb\u901f\u53ef\u6269\u5c55\u6a21\u62df\u5668\uff0c\u65e8\u5728\u964d\u4f4e\u7b97\u6cd5\u5f00\u53d1\u6210\u672c\uff0c\u5b9e\u73b0\u4ee3\u7801\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u7684\u65e0\u7f1d\u8fc1\u79fb\u3002", "motivation": "Pogobots\u673a\u5668\u4eba\u867d\u7136\u6210\u672c\u4f4e\u5ec9\u4e14\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u4f46\u76f4\u63a5\u5728\u673a\u5668\u4eba\u4e0a\u6d4b\u8bd5\u5206\u5e03\u5f0f\u7b97\u6cd5\u975e\u5e38\u8017\u65f6\uff0c\u590d\u6742\u95ee\u9898\u6216\u53c2\u6570\u6821\u51c6\u4f1a\u6d88\u8017\u8fc7\u591a\u8d44\u6e90\u3002", "method": "\u5f00\u53d1\u4e86Pogosim\u6a21\u62df\u5668\uff0c\u5177\u6709\u76f8\u540c\u7684\u8f6f\u4ef6\u67b6\u6784\uff0c\u652f\u6301\u5e76\u884c\u8fd0\u884c\u5927\u91cf\u6a21\u62df\uff0c\u63d0\u4f9b\u914d\u7f6e\u6587\u4ef6\u548c\u7528\u6237\u7a0b\u5e8f\u7f16\u5199\u6307\u5357\uff0c\u5e76\u652f\u6301\u4f7f\u7528\u4f18\u5316\u7b97\u6cd5\u8fdb\u884c\u53c2\u6570\u4f18\u5316\u3002", "result": "\u5b9e\u73b0\u4e86\u6a21\u62df\u4e0e\u5b9e\u9a8c\u73af\u5883\u7684\u4ee3\u7801\u4e00\u81f4\u6027\uff0c\u80fd\u591f\u9ad8\u6548\u8fdb\u884c\u5927\u89c4\u6a21\u4eff\u771f\u548c\u53c2\u6570\u4f18\u5316\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7b97\u6cd5\u5f00\u53d1\u6210\u672c\u3002", "conclusion": "Pogosim\u4e3aPogobots\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u4eff\u771f\u5e73\u53f0\uff0c\u89e3\u51b3\u4e86\u5b9e\u9645\u673a\u5668\u4eba\u6d4b\u8bd5\u7684\u8d44\u6e90\u74f6\u9888\u95ee\u9898\uff0c\u4fc3\u8fdb\u4e86\u7fa4\u4f53\u667a\u80fd\u7b97\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.10979", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.10979", "abs": "https://arxiv.org/abs/2509.10979", "authors": ["Dimitri Jacquemont", "Carlo Bosio", "Teaya Yang", "Ruiqi Zhang", "Ozgur Orun", "Shuai Li", "Reza Alam", "Thomas M. Schutzius", "Simo A. Makiharju", "Mark W. Mueller"], "title": "Autonomous Close-Proximity Photovoltaic Panel Coating Using a Quadcopter", "comment": "7 pages, 10 figures. Submitted to IEEE RA-L", "summary": "Photovoltaic (PV) panels are becoming increasingly widespread in the domain\nof renewable energy, and thus, small efficiency gains can have massive effects.\nAnti-reflective and self-cleaning coatings enhance panel performance but\ndegrade over time, requiring periodic reapplication. Uncrewed Aerial Vehicles\n(UAVs) offer a flexible and autonomous way to apply protective coatings more\noften and at lower cost compared to traditional manual coating methods. In this\nletter, we propose a quadcopter-based system, equipped with a liquid dispersion\nmechanism, designed to automate such tasks. The localization stack only uses\nonboard sensors, relying on visual-inertial odometry and the relative position\nof the PV panel detected with respect to the quadcopter. The control relies on\na model-based controller that accounts for the ground effect and the mass\ndecrease of the quadcopter during liquid dispersion. We validate the autonomy\ncapabilities of our system through extensive indoor and outdoor experiments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7684\u5149\u4f0f\u677f\u4fdd\u62a4\u6d82\u5c42\u81ea\u52a8\u55b7\u6d82\u7cfb\u7edf\uff0c\u4f7f\u7528\u673a\u8f7d\u4f20\u611f\u5668\u5b9a\u4f4d\u548c\u6a21\u578b\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u5ba4\u5185\u5916\u5b9e\u9a8c\u9a8c\u8bc1\u81ea\u4e3b\u6027\u80fd", "motivation": "\u5149\u4f0f\u677f\u9632\u53cd\u5c04\u548c\u81ea\u6e05\u6d01\u6d82\u5c42\u9700\u8981\u5b9a\u671f\u91cd\u65b0\u55b7\u6d82\uff0c\u4f20\u7edf\u4eba\u5de5\u65b9\u6cd5\u6210\u672c\u9ad8\u3002\u65e0\u4eba\u673a\u63d0\u4f9b\u7075\u6d3b\u81ea\u4e3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u66f4\u9891\u7e41\u3001\u4f4e\u6210\u672c\u5730\u5b9e\u65bd\u6d82\u5c42\u7ef4\u62a4", "method": "\u4f7f\u7528\u914d\u5907\u6db2\u4f53\u5206\u6563\u673a\u5236\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u4ec5\u4f9d\u8d56\u673a\u8f7d\u4f20\u611f\u5668\u8fdb\u884c\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\u5b9a\u4f4d\u548c\u5149\u4f0f\u677f\u76f8\u5bf9\u4f4d\u7f6e\u68c0\u6d4b\uff0c\u91c7\u7528\u8003\u8651\u5730\u9762\u6548\u5e94\u548c\u8d28\u91cf\u53d8\u5316\u7684\u6a21\u578b\u63a7\u5236\u5668", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5ba4\u5185\u548c\u5ba4\u5916\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u81ea\u4e3b\u80fd\u529b", "conclusion": "\u65e0\u4eba\u673a\u7cfb\u7edf\u4e3a\u5149\u4f0f\u677f\u4fdd\u62a4\u6d82\u5c42\u7684\u81ea\u52a8\u5316\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5177\u7075\u6d3b\u6027\u548c\u6210\u672c\u6548\u76ca"}}
{"id": "2509.11025", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.11025", "abs": "https://arxiv.org/abs/2509.11025", "authors": ["Peng Chen", "Jing Liang", "Hui Song", "Kang-Jia Qiao", "Cai-Tong Yue", "Kun-Jie Yu", "Ponnuthurai Nagaratnam Suganthan", "Witold Pedrycz"], "title": "Multi-objective task allocation for electric harvesting robots: a hierarchical route reconstruction approach", "comment": null, "summary": "The increasing labor costs in agriculture have accelerated the adoption of\nmulti-robot systems for orchard harvesting. However, efficiently coordinating\nthese systems is challenging due to the complex interplay between makespan and\nenergy consumption, particularly under practical constraints like\nload-dependent speed variations and battery limitations. This paper defines the\nmulti-objective agricultural multi-electrical-robot task allocation (AMERTA)\nproblem, which systematically incorporates these often-overlooked real-world\nconstraints. To address this problem, we propose a hybrid hierarchical route\nreconstruction algorithm (HRRA) that integrates several innovative mechanisms,\nincluding a hierarchical encoding structure, a dual-phase initialization\nmethod, task sequence optimizers, and specialized route reconstruction\noperators. Extensive experiments on 45 test instances demonstrate HRRA's\nsuperior performance against seven state-of-the-art algorithms. Statistical\nanalysis, including the Wilcoxon signed-rank and Friedman tests, empirically\nvalidates HRRA's competitiveness and its unique ability to explore previously\ninaccessible regions of the solution space. In general, this research\ncontributes to the theoretical understanding of multi-robot coordination by\noffering a novel problem formulation and an effective algorithm, thereby also\nproviding practical insights for agricultural automation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u519c\u4e1a\u591a\u7535\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u95ee\u9898(AMERTA)\u7684\u6df7\u5408\u5206\u5c42\u8def\u5f84\u91cd\u6784\u7b97\u6cd5(HRRA)\uff0c\u89e3\u51b3\u4e86\u591a\u673a\u5668\u4eba\u534f\u8c03\u4e2d\u7684\u5b8c\u5de5\u65f6\u95f4\u548c\u80fd\u8017\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u519c\u4e1a\u52b3\u52a8\u529b\u6210\u672c\u4e0a\u5347\u63a8\u52a8\u4e86\u591a\u673a\u5668\u4eba\u91c7\u6458\u7cfb\u7edf\u7684\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5f80\u5f80\u5ffd\u7565\u4e86\u8d1f\u8f7d\u4f9d\u8d56\u7684\u901f\u5ea6\u53d8\u5316\u548c\u7535\u6c60\u9650\u5236\u7b49\u5b9e\u9645\u7ea6\u675f\uff0c\u5bfc\u81f4\u534f\u8c03\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86HRRA\u7b97\u6cd5\uff0c\u5305\u542b\u5206\u5c42\u7f16\u7801\u7ed3\u6784\u3001\u53cc\u9636\u6bb5\u521d\u59cb\u5316\u65b9\u6cd5\u3001\u4efb\u52a1\u5e8f\u5217\u4f18\u5316\u5668\u548c\u4e13\u95e8\u7684\u8def\u5f84\u91cd\u6784\u64cd\u4f5c\u5668\u7b49\u521b\u65b0\u673a\u5236\u3002", "result": "\u572845\u4e2a\u6d4b\u8bd5\u5b9e\u4f8b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHRRA\u7b97\u6cd5\u4f18\u4e8e7\u79cd\u6700\u5148\u8fdb\u7b97\u6cd5\uff0c\u7edf\u8ba1\u68c0\u9a8c\u8bc1\u5b9e\u4e86\u5176\u7ade\u4e89\u529b\u548c\u63a2\u7d22\u65b0\u89e3\u7a7a\u95f4\u7684\u72ec\u7279\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u65b0\u9896\u7684\u95ee\u9898\u8868\u8ff0\u548c\u6709\u6548\u7b97\u6cd5\uff0c\u4e3a\u591a\u673a\u5668\u4eba\u534f\u8c03\u7406\u8bba\u7406\u89e3\u505a\u51fa\u8d21\u732e\uff0c\u5e76\u4e3a\u519c\u4e1a\u81ea\u52a8\u5316\u63d0\u4f9b\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2509.11109", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.11109", "abs": "https://arxiv.org/abs/2509.11109", "authors": ["Jiaxin Huang", "Hanyu Liu", "Yunsheng Ma", "Jian Shen", "Yilin Zheng", "Jiayi Wen", "Baishu Wan", "Pan Li", "Zhigong Song"], "title": "FEWT: Improving Humanoid Robot Perception with Frequency-Enhanced Wavelet-based Transformers", "comment": null, "summary": "The embodied intelligence bridges the physical world and information space.\nAs its typical physical embodiment, humanoid robots have shown great promise\nthrough robot learning algorithms in recent years. In this study, a hardware\nplatform, including humanoid robot and exoskeleton-style teleoperation cabin,\nwas developed to realize intuitive remote manipulation and efficient collection\nof anthropomorphic action data. To improve the perception representation of\nhumanoid robot, an imitation learning framework, termed Frequency-Enhanced\nWavelet-based Transformer (FEWT), was proposed, which consists of two primary\nmodules: Frequency-Enhanced Efficient Multi-Scale Attention (FE-EMA) and\nTime-Series Discrete Wavelet Transform (TS-DWT). By combining multi-scale\nwavelet decomposition with the residual network, FE-EMA can dynamically fuse\nfeatures from both time-domain and frequency-domain. This fusion is able to\ncapture feature information across various scales effectively, thereby\nenhancing model robustness. Experimental performance demonstrates that FEWT\nimproves the success rate of the state-of-the-art algorithm (Action Chunking\nwith Transformers, ACT baseline) by up to 30% in simulation and by 6-12% in\nreal-world.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u9891\u7387\u589e\u5f3a\u5c0f\u6ce2\u53d8\u6362\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6FEWT\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u5c3a\u5ea6\u5c0f\u6ce2\u5206\u89e3\u548c\u6b8b\u5dee\u7f51\u7edc\uff0c\u52a8\u6001\u878d\u5408\u65f6\u57df\u548c\u9891\u57df\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7684\u611f\u77e5\u8868\u793a\u548c\u64cd\u4f5c\u6210\u529f\u7387\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u4f5c\u4e3a\u5177\u8eab\u667a\u80fd\u7684\u5178\u578b\u7269\u7406\u4f53\u73b0\uff0c\u5728\u673a\u5668\u4eba\u5b66\u4e60\u7b97\u6cd5\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002\u9700\u8981\u5f00\u53d1\u786c\u4ef6\u5e73\u53f0\u548c\u7b97\u6cd5\u6846\u67b6\u6765\u5b9e\u73b0\u76f4\u89c2\u7684\u8fdc\u7a0b\u64cd\u4f5c\u548c\u9ad8\u6548\u7684\u4eba\u5f62\u52a8\u4f5c\u6570\u636e\u6536\u96c6\uff0c\u63d0\u5347\u673a\u5668\u4eba\u7684\u611f\u77e5\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u62ec\u4eba\u5f62\u673a\u5668\u4eba\u548c\u5916\u9aa8\u9abc\u5f0f\u9065\u64cd\u4f5c\u8231\u7684\u786c\u4ef6\u5e73\u53f0\u3002\u63d0\u51fa\u4e86FEWT\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u6a21\u5757\uff1a\u9891\u7387\u589e\u5f3a\u9ad8\u6548\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b(FE-EMA)\u548c\u65f6\u95f4\u5e8f\u5217\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362(TS-DWT)\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u5c0f\u6ce2\u5206\u89e3\u4e0e\u6b8b\u5dee\u7f51\u7edc\u7ed3\u5408\uff0c\u52a8\u6001\u878d\u5408\u65f6\u57df\u548c\u9891\u57df\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFEWT\u5728\u4eff\u771f\u4e2d\u5c06\u6700\u5148\u8fdb\u7b97\u6cd5(ACT\u57fa\u7ebf)\u7684\u6210\u529f\u7387\u63d0\u5347\u4e86\u9ad8\u8fbe30%\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u63d0\u5347\u4e866-12%\u3002", "conclusion": "FEWT\u6846\u67b6\u901a\u8fc7\u6709\u6548\u6355\u83b7\u591a\u5c3a\u5ea6\u7279\u5f81\u4fe1\u606f\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u7684\u611f\u77e5\u8868\u793a\u548c\u64cd\u4f5c\u6027\u80fd\u63d0\u4f9b\u4e86\u91cd\u8981\u6539\u8fdb\u3002"}}
{"id": "2509.11125", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.11125", "abs": "https://arxiv.org/abs/2509.11125", "authors": ["Zheng Li", "Pei Qu", "Yufei Jia", "Shihui Zhou", "Haizhou Ge", "Jiahang Cao", "Jinni Zhou", "Guyue Zhou", "Jun Ma"], "title": "ManiVID-3D: Generalizable View-Invariant Reinforcement Learning for Robotic Manipulation via Disentangled 3D Representations", "comment": "8 pages, 7 figures", "summary": "Deploying visual reinforcement learning (RL) policies in real-world\nmanipulation is often hindered by camera viewpoint changes. A policy trained\nfrom a fixed front-facing camera may fail when the camera is shifted--an\nunavoidable situation in real-world settings where sensor placement is hard to\nmanage appropriately. Existing methods often rely on precise camera calibration\nor struggle with large perspective changes. To address these limitations, we\npropose ManiVID-3D, a novel 3D RL architecture designed for robotic\nmanipulation, which learns view-invariant representations through\nself-supervised disentangled feature learning. The framework incorporates\nViewNet, a lightweight yet effective module that automatically aligns point\ncloud observations from arbitrary viewpoints into a unified spatial coordinate\nsystem without the need for extrinsic calibration. Additionally, we develop an\nefficient GPU-accelerated batch rendering module capable of processing over\n5000 frames per second, enabling large-scale training for 3D visual RL at\nunprecedented speeds. Extensive evaluation across 10 simulated and 5 real-world\ntasks demonstrates that our approach achieves a 44.7% higher success rate than\nstate-of-the-art methods under viewpoint variations while using 80% fewer\nparameters. The system's robustness to severe perspective changes and strong\nsim-to-real performance highlight the effectiveness of learning geometrically\nconsistent representations for scalable robotic manipulation in unstructured\nenvironments. Our project website can be found in\nhttps://zheng-joe-lee.github.io/manivid3d/.", "AI": {"tldr": "ManiVID-3D\u662f\u4e00\u4e2a\u65b0\u9896\u76843D\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u89e3\u8026\u7279\u5f81\u5b66\u4e60\u5b9e\u73b0\u89c6\u89d2\u4e0d\u53d8\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u56e0\u76f8\u673a\u89c6\u89d2\u53d8\u5316\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u673a\u5668\u4eba\u64cd\u4f5c\u90e8\u7f72\u65f6\uff0c\u76f8\u673a\u89c6\u89d2\u53d8\u5316\u4e0d\u53ef\u907f\u514d\uff0c\u4f20\u7edf\u57fa\u4e8e\u56fa\u5b9a\u524d\u89c6\u76f8\u673a\u7684\u7b56\u7565\u5728\u76f8\u673a\u4f4d\u7f6e\u53d8\u5316\u65f6\u4f1a\u5931\u6548\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7cbe\u786e\u76f8\u673a\u6807\u5b9a\u6216\u96be\u4ee5\u5904\u7406\u5927\u89c6\u89d2\u53d8\u5316\u3002", "method": "\u63d0\u51faViewNet\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u81ea\u52a8\u5c06\u4efb\u610f\u89c6\u89d2\u7684\u70b9\u4e91\u89c2\u6d4b\u5bf9\u9f50\u5230\u7edf\u4e00\u7a7a\u95f4\u5750\u6807\u7cfb\uff0c\u65e0\u9700\u5916\u90e8\u6807\u5b9a\uff1b\u5f00\u53d1\u9ad8\u6548GPU\u52a0\u901f\u6279\u91cf\u6e32\u67d3\u6a21\u5757\uff0c\u6bcf\u79d2\u5904\u74065000+\u5e27\uff1b\u901a\u8fc7\u81ea\u76d1\u7763\u89e3\u8026\u7279\u5f81\u5b66\u4e60\u5b9e\u73b0\u89c6\u89d2\u4e0d\u53d8\u8868\u793a\u3002", "result": "\u572810\u4e2a\u6a21\u62df\u548c5\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u8bc4\u4f30\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5728\u89c6\u89d2\u53d8\u5316\u4e0b\u6210\u529f\u7387\u63d0\u9ad844.7%\uff0c\u53c2\u6570\u91cf\u51cf\u5c1180%\uff1b\u5bf9\u4e25\u91cd\u89c6\u89d2\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u4eff\u771f\u5230\u771f\u5b9e\u6027\u80fd\u3002", "conclusion": "\u5b66\u4e60\u51e0\u4f55\u4e00\u81f4\u8868\u793a\u5bf9\u4e8e\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u673a\u5668\u4eba\u64cd\u4f5c\u975e\u5e38\u6709\u6548\uff0cManiVID-3D\u4e3a\u89e3\u51b3\u89c6\u89d2\u53d8\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11149", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11149", "abs": "https://arxiv.org/abs/2509.11149", "authors": ["Mintae Kim", "Jiaze Cai", "Koushil Sreenath"], "title": "RoVerFly: Robust and Versatile Learning-based Control of Quadrotor Across Payload Configurations", "comment": "8 pages", "summary": "Designing robust controllers for precise, arbitrary trajectory tracking with\nquadrotors is challenging due to nonlinear dynamics and underactuation, and\nbecomes harder with flexible cable-suspended payloads that introduce extra\ndegrees of freedom and hybridness. Classical model-based methods offer\nstability guarantees but require extensive tuning and often do not adapt when\nthe configuration changes, such as when a payload is added or removed, or when\nthe payload mass or cable length varies. We present RoVerFly, a unified\nlearning-based control framework in which a reinforcement learning (RL) policy\nserves as a robust and versatile tracking controller for standard quadrotors\nand for cable-suspended payload systems across a range of configurations.\nTrained with task and domain randomization, the controller is resilient to\ndisturbances and varying dynamics. It achieves strong zero-shot generalization\nacross payload settings, including no payload as well as varying mass and cable\nlength, without controller switching or re-tuning, while retaining the\ninterpretability and structure of a feedback tracking controller. Code and\nsupplementary materials are available at\nhttps://github.com/mintaeshkim/roverfly", "AI": {"tldr": "RoVerFly\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7edf\u4e00\u63a7\u5236\u6846\u67b6\uff0c\u4f7f\u7528RL\u7b56\u7565\u4f5c\u4e3a\u9c81\u68d2\u7684\u8ddf\u8e2a\u63a7\u5236\u5668\uff0c\u9002\u7528\u4e8e\u6807\u51c6\u56db\u65cb\u7ffc\u548c\u7f06\u7ef3\u60ac\u6302\u8f7d\u8377\u7cfb\u7edf\uff0c\u5177\u6709\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u56db\u65cb\u7ffc\u7cbe\u786e\u8f68\u8ff9\u8ddf\u8e2a\u63a7\u5236\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5e26\u6709\u67d4\u6027\u7f06\u7ef3\u60ac\u6302\u8f7d\u8377\u65f6\u3002\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8c03\u53c2\u4e14\u65e0\u6cd5\u9002\u5e94\u914d\u7f6e\u53d8\u5316\uff08\u5982\u8f7d\u8377\u589e\u51cf\u3001\u8d28\u91cf\u6216\u7f06\u957f\u53d8\u5316\uff09\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4f5c\u4e3a\u8ddf\u8e2a\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u4efb\u52a1\u548c\u9886\u57df\u968f\u673a\u5316\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f7f\u63a7\u5236\u5668\u5bf9\u5e72\u6270\u548c\u52a8\u6001\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002", "result": "\u63a7\u5236\u5668\u5b9e\u73b0\u4e86\u8de8\u8f7d\u8377\u8bbe\u7f6e\u7684\u5f3a\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5305\u62ec\u65e0\u8f7d\u8377\u3001\u4e0d\u540c\u8d28\u91cf\u548c\u7f06\u957f\u60c5\u51b5\uff0c\u65e0\u9700\u63a7\u5236\u5668\u5207\u6362\u6216\u91cd\u65b0\u8c03\u53c2\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53cd\u9988\u8ddf\u8e2a\u63a7\u5236\u5668\u7684\u53ef\u89e3\u91ca\u6027\u548c\u7ed3\u6784\u3002", "conclusion": "RoVerFly\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5b66\u4e60\u578b\u63a7\u5236\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u56db\u65cb\u7ffc\u548c\u7f06\u7ef3\u60ac\u6302\u8f7d\u8377\u7cfb\u7edf\u7684\u5404\u79cd\u914d\u7f6e\u53d8\u5316\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.11185", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.11185", "abs": "https://arxiv.org/abs/2509.11185", "authors": ["Kai Chen", "Zhihai Bi", "Guoyang Zhao", "Chunxin Zheng", "Yulin Li", "Hang Zhao", "Jun Ma"], "title": "SAMP: Spatial Anchor-based Motion Policy for Collision-Aware Robotic Manipulators", "comment": null, "summary": "Neural-based motion planning methods have achieved remarkable progress for\nrobotic manipulators, yet a fundamental challenge lies in simultaneously\naccounting for both the robot's physical shape and the surrounding environment\nwhen generating safe and feasible motions. Moreover, existing approaches often\nrely on simplified robot models or focus primarily on obstacle representation,\nwhich can lead to incomplete collision detection and degraded performance in\ncluttered scenes. To address these limitations, we propose spatial anchor-based\nmotion policy (SAMP), a unified framework that simultaneously encodes the\nenvironment and the manipulator using signed distance field (SDF) anchored on a\nshared spatial grid. SAMP incorporates a dedicated robot SDF network that\ncaptures the manipulator's precise geometry, enabling collision-aware reasoning\nbeyond coarse link approximations. These representations are fused on spatial\nanchors and used to train a neural motion policy that generates smooth,\ncollision-free trajectories in the proposed efficient feature alignment\nstrategy. Experiments conducted in both simulated and real-world environments\nconsistently show that SAMP outperforms existing methods, delivering an 11%\nincrease in success rate and a 7% reduction in collision rate. These results\nhighlight the benefits of jointly modelling robot and environment geometry,\ndemonstrating its practical value in challenging real-world environments.", "AI": {"tldr": "SAMP\u662f\u4e00\u4e2a\u57fa\u4e8e\u7a7a\u95f4\u951a\u70b9\u7684\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u7a7a\u95f4\u7f51\u683c\u4e0a\u7684\u7b26\u53f7\u8ddd\u79bb\u573a\u540c\u65f6\u7f16\u7801\u73af\u5883\u548c\u673a\u68b0\u81c2\u51e0\u4f55\u5f62\u72b6\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u78b0\u649e\u68c0\u6d4b\u548c\u8fd0\u52a8\u89c4\u5212", "motivation": "\u73b0\u6709\u795e\u7ecf\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u5f80\u5f80\u4f9d\u8d56\u7b80\u5316\u7684\u673a\u5668\u4eba\u6a21\u578b\u6216\u4e3b\u8981\u5173\u6ce8\u969c\u788d\u7269\u8868\u793a\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u573a\u666f\u4e2d\u78b0\u649e\u68c0\u6d4b\u4e0d\u5b8c\u6574\u548c\u6027\u80fd\u4e0b\u964d", "method": "\u63d0\u51fa\u7a7a\u95f4\u951a\u70b9\u8fd0\u52a8\u7b56\u7565(SAMP)\u6846\u67b6\uff0c\u4f7f\u7528\u5171\u4eab\u7a7a\u95f4\u7f51\u683c\u4e0a\u7684\u7b26\u53f7\u8ddd\u79bb\u573a\u540c\u65f6\u7f16\u7801\u73af\u5883\u548c\u673a\u68b0\u81c2\u7cbe\u786e\u51e0\u4f55\u5f62\u72b6\uff0c\u901a\u8fc7\u4e13\u7528\u673a\u5668\u4ebaSDF\u7f51\u7edc\u6355\u83b7\u673a\u68b0\u81c2\u7cbe\u786e\u51e0\u4f55\u4fe1\u606f\uff0c\u5e76\u91c7\u7528\u9ad8\u6548\u7279\u5f81\u5bf9\u9f50\u7b56\u7565\u8bad\u7ec3\u795e\u7ecf\u8fd0\u52a8\u7b56\u7565", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u4e2d\uff0cSAMP\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6210\u529f\u7387\u63d0\u9ad811%\uff0c\u78b0\u649e\u7387\u964d\u4f4e7%", "conclusion": "\u8054\u5408\u5efa\u6a21\u673a\u5668\u4eba\u548c\u73af\u5883\u51e0\u4f55\u5f62\u72b6\u7684\u65b9\u6cd5\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5728\u6311\u6218\u6027\u771f\u5b9e\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2509.11197", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.11197", "abs": "https://arxiv.org/abs/2509.11197", "authors": ["Yunheng Wang", "Yuetong Fang", "Taowen Wang", "Yixiao Feng", "Yawen Tan", "Shuning Zhang", "Peiran Liu", "Yiding Ji", "Renjing Xu"], "title": "DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation", "comment": null, "summary": "Vision-and-Language Navigation in Continuous Environments (VLN-CE), which\nlinks language instructions to perception and control in the real world, is a\ncore capability of embodied robots. Recently, large-scale pretrained foundation\nmodels have been leveraged as shared priors for perception, reasoning, and\naction, enabling zero-shot VLN without task-specific training. However,\nexisting zero-shot VLN methods depend on costly perception and passive scene\nunderstanding, collapsing control to point-level choices. As a result, they are\nexpensive to deploy, misaligned in action semantics, and short-sighted in\nplanning. To address these issues, we present DreamNav that focuses on the\nfollowing three aspects: (1) for reducing sensory cost, our EgoView Corrector\naligns viewpoints and stabilizes egocentric perception; (2) instead of\npoint-level actions, our Trajectory Predictor favors global trajectory-level\nplanning to better align with instruction semantics; and (3) to enable\nanticipatory and long-horizon planning, we propose an Imagination Predictor to\nendow the agent with proactive thinking capability. On VLN-CE and real-world\ntests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming the\nstrongest egocentric baseline with extra information by up to 7.49\\% and\n18.15\\% in terms of SR and SPL metrics. To our knowledge, this is the first\nzero-shot VLN method to unify trajectory-level planning and active imagination\nwhile using only egocentric inputs.", "AI": {"tldr": "DreamNav\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89d2\u6821\u6b63\u3001\u8f68\u8ff9\u7ea7\u89c4\u5212\u548c\u4e3b\u52a8\u60f3\u8c61\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u9ad8\u6210\u672c\u3001\u52a8\u4f5c\u8bed\u4e49\u4e0d\u5bf9\u9f50\u548c\u77ed\u89c6\u89c4\u5212\u95ee\u9898\uff0c\u5728VLN-CE\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u96f6\u6837\u672cVLN\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u611f\u77e5\u548c\u88ab\u52a8\u573a\u666f\u7406\u89e3\uff0c\u5c06\u63a7\u5236\u7b80\u5316\u4e3a\u70b9\u7ea7\u9009\u62e9\uff0c\u5bfc\u81f4\u90e8\u7f72\u6210\u672c\u9ad8\u3001\u52a8\u4f5c\u8bed\u4e49\u4e0d\u5bf9\u9f50\u548c\u89c4\u5212\u77ed\u89c6\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u8bed\u4e49\u5bf9\u9f50\u4e14\u5177\u6709\u524d\u77bb\u6027\u7684\u5bfc\u822a\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDreamNav\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1)EgoView Corrector\u5bf9\u9f50\u89c6\u89d2\u5e76\u7a33\u5b9a\u81ea\u6211\u4e2d\u5fc3\u611f\u77e5\uff1b(2)Trajectory Predictor\u8fdb\u884c\u5168\u5c40\u8f68\u8ff9\u7ea7\u89c4\u5212\uff1b(3)Imagination Predictor\u8d4b\u4e88\u667a\u80fd\u4f53\u4e3b\u52a8\u601d\u8003\u80fd\u529b\uff0c\u5b9e\u73b0\u9884\u671f\u6027\u548c\u957f\u65f6\u7a0b\u89c4\u5212\u3002", "result": "\u5728VLN-CE\u548c\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\uff0cDreamNav\u5b9e\u73b0\u4e86\u65b0\u7684\u96f6\u6837\u672cSOTA\uff0c\u5728SR\u548cSPL\u6307\u6807\u4e0a\u5206\u522b\u6bd4\u6700\u5f3a\u7684\u81ea\u6211\u4e2d\u5fc3\u57fa\u7ebf\u9ad8\u51fa7.49%\u548c18.15%\u3002", "conclusion": "DreamNav\u662f\u9996\u4e2a\u7edf\u4e00\u8f68\u8ff9\u7ea7\u89c4\u5212\u548c\u4e3b\u52a8\u60f3\u8c61\u7684\u96f6\u6837\u672cVLN\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u81ea\u6211\u4e2d\u5fc3\u8f93\u5165\uff0c\u4e3a\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u5bfc\u822a\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11225", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11225", "abs": "https://arxiv.org/abs/2509.11225", "authors": ["Youzhi Liang", "Eyan Noronha"], "title": "MEMBOT: Memory-Based Robot in Intermittent POMDP", "comment": null, "summary": "Robotic systems deployed in real-world environments often operate under\nconditions of partial and often intermittent observability, where sensor inputs\nmay be noisy, occluded, or entirely unavailable due to failures or\nenvironmental constraints. Traditional reinforcement learning (RL) approaches\nthat assume full state observability are ill-equipped for such challenges. In\nthis work, we introduce MEMBOT, a modular memory-based architecture designed to\naddress intermittent partial observability in robotic control tasks. MEMBOT\ndecouples belief inference from policy learning through a two-phase training\nprocess: an offline multi-task learning pretraining stage that learns a robust\ntask-agnostic latent belief encoder using a reconstruction losses, followed by\nfine-tuning of task-specific policies using behavior cloning. The belief\nencoder, implemented as a state-space model (SSM) and a LSTM, integrates\ntemporal sequences of observations and actions to infer latent state\nrepresentations that persist even when observations are dropped. We train and\nevaluate MEMBOT on 10 robotic manipulation benchmark tasks from MetaWorld and\nRobomimic under varying rates of observation dropout. Results show that MEMBOT\nconsistently outperforms both memoryless and naively recurrent baselines,\nmaintaining up to 80% of peak performance under 50% observation availability.\nThese findings highlight the effectiveness of explicit belief modeling in\nachieving robust, transferable, and data-efficient policies for real-world\npartially observable robotic systems.", "AI": {"tldr": "MEMBOT\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u8bb0\u5fc6\u67b6\u6784\uff0c\u901a\u8fc7\u89e3\u8026\u4fe1\u5ff5\u63a8\u65ad\u548c\u7b56\u7565\u5b66\u4e60\u6765\u89e3\u51b3\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u95f4\u6b47\u6027\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\uff0c\u5728\u89c2\u6d4b\u4e22\u5931\u7387\u9ad8\u8fbe50%\u65f6\u4ecd\u80fd\u4fdd\u630180%\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u7ecf\u5e38\u5728\u90e8\u5206\u548c\u95f4\u6b47\u6027\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\u8fd0\u884c\uff0c\u4f20\u611f\u5668\u8f93\u5165\u53ef\u80fd\u56e0\u566a\u58f0\u3001\u906e\u6321\u6216\u6545\u969c\u800c\u4e0d\u53ef\u7528\uff0c\u4f20\u7edf\u57fa\u4e8e\u5168\u72b6\u6001\u53ef\u89c2\u6d4b\u5047\u8bbe\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8fc7\u7a0b\uff1a\u79bb\u7ebf\u591a\u4efb\u52a1\u5b66\u4e60\u9884\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u91cd\u6784\u635f\u5931\u5b66\u4e60\u9c81\u68d2\u7684\u4efb\u52a1\u65e0\u5173\u6f5c\u5728\u4fe1\u5ff5\u7f16\u7801\u5668\uff0c\u7136\u540e\u901a\u8fc7\u884c\u4e3a\u514b\u9686\u5fae\u8c03\u4efb\u52a1\u7279\u5b9a\u7b56\u7565\u3002\u4fe1\u5ff5\u7f16\u7801\u5668\u4f7f\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b(SSM)\u548cLSTM\u6574\u5408\u89c2\u6d4b\u548c\u52a8\u4f5c\u7684\u65f6\u95f4\u5e8f\u5217\u6765\u63a8\u65ad\u6f5c\u5728\u72b6\u6001\u8868\u793a\u3002", "result": "\u5728MetaWorld\u548cRobomimic\u768410\u4e2a\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u4efb\u52a1\u4e0a\u6d4b\u8bd5\uff0cMEMBOT\u5728\u5404\u79cd\u89c2\u6d4b\u4e22\u5931\u7387\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u65e0\u8bb0\u5fc6\u548c\u7b80\u5355\u5faa\u73af\u57fa\u7ebf\u65b9\u6cd5\uff0c\u572850%\u89c2\u6d4b\u53ef\u7528\u6027\u4e0b\u4ecd\u80fd\u4fdd\u630180%\u7684\u5cf0\u503c\u6027\u80fd\u3002", "conclusion": "\u663e\u5f0f\u4fe1\u5ff5\u5efa\u6a21\u5bf9\u4e8e\u5b9e\u73b0\u73b0\u5b9e\u4e16\u754c\u90e8\u5206\u53ef\u89c2\u6d4b\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u9c81\u68d2\u3001\u53ef\u8fc1\u79fb\u548c\u6570\u636e\u9ad8\u6548\u7b56\u7565\u5177\u6709\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2509.11240", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.11240", "abs": "https://arxiv.org/abs/2509.11240", "authors": ["Yechen Zhang", "Bin Gao", "Gang Wang", "Jian Sun", "Zhuo Li"], "title": "CORB-Planner: Corridor as Observations for RL Planning in High-Speed Flight", "comment": "11 pages, 8 figures. Submitted to IEEE/ASME T-MECH. Code available at\n  https://github.com/ChenzycBIT/CORB-planner", "summary": "Reinforcement learning (RL) has shown promise in a large number of robotic\ncontrol tasks. Nevertheless, its deployment on unmanned aerial vehicles (UAVs)\nremains challenging, mainly because of reliance on accurate dynamic models and\nplatform-specific sensing, which hinders cross-platform transfer. This paper\npresents the CORB-Planner (Corridor-as-Observations for RL B-spline planner), a\nreal-time, RL-based trajectory planning framework for high-speed autonomous UAV\nflight across heterogeneous platforms. The key idea is to combine B-spline\ntrajectory generation with the RL policy producing successive control points\nwith a compact safe flight corridor (SFC) representation obtained via heuristic\nsearch. The SFC abstracts obstacle information in a low-dimensional form,\nmitigating overfitting to platform-specific details and reducing sensitivity to\nmodel inaccuracies. To narrow the sim-to-real gap, we adopt an easy-to-hard\nprogressive training pipeline in simulation. A value-based soft\ndecomposed-critic Q (SDCQ) algorithm is used to learn effective policies within\napproximately ten minutes of training. Benchmarks in simulation and real-world\ntests demonstrate real-time planning on lightweight onboard hardware and\nsupport maximum flight speeds up to 8.2m/s in dense, cluttered environments\nwithout external positioning. Compatibility with various UAV configurations\n(quadrotors, hexarotors) and modest onboard compute underlines the generality\nand robustness of CORB-Planner for practical deployment.", "AI": {"tldr": "CORB-Planner\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5b9e\u65f6\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7B\u6837\u6761\u8f68\u8ff9\u751f\u6210\u548c\u7d27\u51d1\u5b89\u5168\u98de\u884c\u8d70\u5eca\u8868\u793a\uff0c\u5b9e\u73b0\u65e0\u4eba\u673a\u5728\u5f02\u6784\u5e73\u53f0\u4e0a\u7684\u9ad8\u901f\u81ea\u4e3b\u98de\u884c", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728\u65e0\u4eba\u673a\u90e8\u7f72\u4e2d\u7684\u6311\u6218\uff0c\u5305\u62ec\u5bf9\u7cbe\u786e\u52a8\u6001\u6a21\u578b\u7684\u4f9d\u8d56\u548c\u5e73\u53f0\u7279\u5b9a\u4f20\u611f\u7684\u9650\u5236\uff0c\u4fc3\u8fdb\u8de8\u5e73\u53f0\u8fc1\u79fb", "method": "\u7ed3\u5408B\u6837\u6761\u8f68\u8ff9\u751f\u6210\u548cRL\u7b56\u7565\u4ea7\u751f\u8fde\u7eed\u63a7\u5236\u70b9\uff0c\u4f7f\u7528\u542f\u53d1\u5f0f\u641c\u7d22\u83b7\u5f97\u7d27\u51d1\u5b89\u5168\u98de\u884c\u8d70\u5eca\u8868\u793a\uff0c\u91c7\u7528\u57fa\u4e8e\u503c\u7684\u8f6f\u5206\u89e3\u6279\u8bc4\u5668Q\u7b97\u6cd5\u548c\u6613\u5230\u96be\u7684\u6e10\u8fdb\u8bad\u7ec3\u7ba1\u9053", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\u5c55\u793a\u5b9e\u65f6\u89c4\u5212\u80fd\u529b\uff0c\u652f\u6301\u6700\u9ad88.2m/s\u98de\u884c\u901f\u5ea6\uff0c\u517c\u5bb9\u591a\u79cd\u65e0\u4eba\u673a\u914d\u7f6e\uff0c\u8ba1\u7b97\u9700\u6c42\u9002\u4e2d", "conclusion": "CORB-Planner\u5177\u6709\u901a\u7528\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\uff0c\u89e3\u51b3\u4e86\u6a21\u578b\u4e0d\u51c6\u786e\u6027\u548c\u5e73\u53f0\u7279\u5b9a\u7ec6\u8282\u7684\u8fc7\u62df\u5408\u95ee\u9898"}}
{"id": "2509.11270", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11270", "abs": "https://arxiv.org/abs/2509.11270", "authors": ["Ziwen He", "Zhigang Wang", "Yanlong Peng", "Pengxu Chang", "Hong Yang", "Ming Chen"], "title": "Embodied Intelligence in Disassembly: Multimodal Perception Cross-validation and Continual Learning in Neuro-Symbolic TAMP", "comment": "8 pages, 3 figures. Accepted at CASE2025. This arXiv version contains\n  minor corrections", "summary": "With the rapid development of the new energy vehicle industry, the efficient\ndisassembly and recycling of power batteries have become a critical challenge\nfor the circular economy. In current unstructured disassembly scenarios, the\ndynamic nature of the environment severely limits the robustness of robotic\nperception, posing a significant barrier to autonomous disassembly in\nindustrial applications. This paper proposes a continual learning framework\nbased on Neuro-Symbolic task and motion planning (TAMP) to enhance the\nadaptability of embodied intelligence systems in dynamic environments. Our\napproach integrates a multimodal perception cross-validation mechanism into a\nbidirectional reasoning flow: the forward working flow dynamically refines and\noptimizes action strategies, while the backward learning flow autonomously\ncollects effective data from historical task executions to facilitate continual\nsystem learning, enabling self-optimization. Experimental results show that the\nproposed framework improves the task success rate in dynamic disassembly\nscenarios from 81.68% to 100%, while reducing the average number of perception\nmisjudgments from 3.389 to 1.128. This research provides a new paradigm for\nenhancing the robustness and adaptability of embodied intelligence in complex\nindustrial environments.", "AI": {"tldr": "\u901a\u8fc7\u7ec4\u5408\u795e\u7ecf\u7b26\u53f7\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u52a8\u6001\u73af\u5883\u4e0b\u673a\u5668\u4eba\u611f\u77e5\u7684\u7a33\u5065\u6027\u548c\u9002\u5e94\u6027\uff0c\u5c06\u52a8\u529b\u7535\u6c60\u89e3\u4f53\u4efb\u52a1\u6210\u529f\u7387\u4ece81.68%\u63d0\u5347\u5230100%", "motivation": "\u65b0\u80fd\u6e90\u6c7d\u8f66\u884c\u4e1a\u5feb\u901f\u53d1\u5c55\uff0c\u52a8\u529b\u7535\u6c60\u7684\u9ad8\u6548\u89e3\u4f53\u56de\u6536\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002\u975e\u7ed3\u6784\u5316\u89e3\u4f53\u573a\u666f\u4e2d\u73af\u5883\u7684\u52a8\u6001\u6027\u4e25\u91cd\u9650\u5236\u4e86\u673a\u5668\u4eba\u611f\u77e5\u7684\u7a33\u5065\u6027\uff0c\u969c\u788d\u4e86\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u81ea\u4e3b\u89e3\u4f53", "method": "\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u7b26\u53f7\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212(TAMP)\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u96c6\u6210\u591a\u6a21\u6001\u611f\u77e5\u4ea4\u53c9\u9a8c\u8bc1\u673a\u5236\u5230\u53cc\u5411\u63a8\u7406\u6d41\u7a0b\u4e2d\uff1a\u524d\u5411\u5de5\u4f5c\u6d41\u52a8\u6001\u7cbe\u70bc\u4f18\u5316\u52a8\u4f5c\u7b56\u7565\uff0c\u540e\u5411\u5b66\u4e60\u6d41\u81ea\u4e3b\u4ece\u5386\u53f2\u4efb\u52a1\u6267\u884c\u4e2d\u6536\u96c6\u6709\u6548\u6570\u636e\u4ee5\u652f\u6301\u6301\u7eed\u7cfb\u7edf\u5b66\u4e60", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u5728\u52a8\u6001\u89e3\u4f53\u573a\u666f\u4e2d\u5c06\u4efb\u52a1\u6210\u529f\u7387\u4ece81.68%\u63d0\u5347\u5230100%\uff0c\u5e76\u5c06\u5e73\u5747\u611f\u77e5\u8bef\u5224\u6b21\u6570\u4ece3.389\u964d\u4f4e\u52301.128", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u63d0\u5347\u4f53\u73b0\u667a\u80fd\u7cfb\u7edf\u5728\u590d\u6742\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u7a33\u5065\u6027\u548c\u9002\u5e94\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f"}}
{"id": "2509.11297", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11297", "abs": "https://arxiv.org/abs/2509.11297", "authors": ["Carl Bettosi", "Lynne Ballie", "Susan Shenkin", "Marta Romeo"], "title": "Policy Learning for Social Robot-Led Physiotherapy", "comment": null, "summary": "Social robots offer a promising solution for autonomously guiding patients\nthrough physiotherapy exercise sessions, but effective deployment requires\nadvanced decision-making to adapt to patient needs. A key challenge is the\nscarcity of patient behavior data for developing robust policies. To address\nthis, we engaged 33 expert healthcare practitioners as patient proxies, using\ntheir interactions with our robot to inform a patient behavior model capable of\ngenerating exercise performance metrics and subjective scores on perceived\nexertion. We trained a reinforcement learning-based policy in simulation,\ndemonstrating that it can adapt exercise instructions to individual exertion\ntolerances and fluctuating performance, while also being applicable to patients\nat different recovery stages with varying exercise plans.", "AI": {"tldr": "\u4f7f\u7528\u533b\u7597\u4e13\u5bb6\u4f5c\u4e3a\u60a3\u8005\u4ee3\u7406\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u673a\u5668\u4eba\u81ea\u9002\u5e94\u7269\u7406\u6cbb\u7597\u6307\u5bfc\u7b56\u7565\uff0c\u89e3\u51b3\u60a3\u8005\u884c\u4e3a\u6570\u636e\u7a00\u7f3a\u95ee\u9898", "motivation": "\u793e\u4ea4\u673a\u5668\u4eba\u53ef\u81ea\u4e3b\u6307\u5bfc\u7269\u7406\u6cbb\u7597\uff0c\u4f46\u7f3a\u4e4f\u60a3\u8005\u884c\u4e3a\u6570\u636e\u6765\u5f00\u53d1\u9c81\u68d2\u7684\u81ea\u9002\u5e94\u51b3\u7b56\u7b56\u7565", "method": "\u9080\u8bf733\u540d\u533b\u7597\u4e13\u5bb6\u4f5c\u4e3a\u60a3\u8005\u4ee3\u7406\u4e0e\u673a\u5668\u4eba\u4e92\u52a8\uff0c\u5efa\u7acb\u60a3\u8005\u884c\u4e3a\u6a21\u578b\u751f\u6210\u8fd0\u52a8\u8868\u73b0\u6307\u6807\u548c\u4e3b\u89c2\u7528\u529b\u8bc4\u5206\uff0c\u5728\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u7b56\u7565", "result": "\u8bad\u7ec3\u7684\u7b56\u7565\u80fd\u591f\u6839\u636e\u4e2a\u4f53\u7528\u529b\u8010\u53d7\u5ea6\u548c\u6ce2\u52a8\u8868\u73b0\u81ea\u9002\u5e94\u8c03\u6574\u8fd0\u52a8\u6307\u5bfc\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u5eb7\u590d\u9636\u6bb5\u548c\u8fd0\u52a8\u8ba1\u5212\u7684\u60a3\u8005", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u60a3\u8005\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u5bf9\u7269\u7406\u6cbb\u7597\u7ec3\u4e60\u7684\u81ea\u9002\u5e94\u6307\u5bfc"}}
{"id": "2509.11306", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.11306", "abs": "https://arxiv.org/abs/2509.11306", "authors": ["Carl Bettosi", "Emilyann Nault", "Lynne Baillie", "Markus Garschall", "Marta Romeo", "Beatrix Wais-Zechmann", "Nicole Binderlehner", "Theodoros Georgio"], "title": "Brain-Robot Interface for Exercise Mimicry", "comment": null, "summary": "For social robots to maintain long-term engagement as exercise instructors,\nrapport-building is essential. Motor mimicry--imitating one's physical\nactions--during social interaction has long been recognized as a powerful tool\nfor fostering rapport, and it is widely used in rehabilitation exercises where\npatients mirror a physiotherapist or video demonstration. We developed a novel\nBrain-Robot Interface (BRI) that allows a social robot instructor to mimic a\npatient's exercise movements in real-time, using mental commands derived from\nthe patient's intention. The system was evaluated in an exploratory study with\n14 participants (3 physiotherapists and 11 hemiparetic patients recovering from\nstroke or other injuries). We found our system successfully demonstrated\nexercise mimicry in 12 sessions; however, accuracy varied. Participants had\npositive perceptions of the robot instructor, with high trust and acceptance\nlevels, which were not affected by the introduction of BRI technology.", "AI": {"tldr": "\u793e\u4ea4\u673a\u5668\u4eba\u901a\u8fc7\u8111\u673a\u63a5\u53e3\u5b9e\u65f6\u6a21\u4eff\u60a3\u8005\u8fd0\u52a8\u52a8\u4f5c\uff0c\u63d0\u9ad8\u5eb7\u590d\u8bad\u7ec3\u7684\u4fe1\u4efb\u548c\u63a5\u53d7\u5ea6", "motivation": "\u4e3a\u4e86\u901a\u8fc7\u8fd0\u52a8\u6a21\u4eff\u6784\u5efa\u793e\u4ea4\u673a\u5668\u4eba\u4e0e\u60a3\u8005\u4e4b\u95f4\u7684\u620f\u62df\u5173\u7cfb\uff0c\u7ef4\u6301\u957f\u671f\u5eb7\u590d\u8bad\u7ec3\u7684\u53c2\u4e0e\u5ea6", "method": "\u5f00\u53d1\u4e86\u65b0\u578b\u8111\u673a\u63a5\u53e3(BRI)\u7cfb\u7edf\uff0c\u8ba9\u793e\u4ea4\u673a\u5668\u80fd\u591f\u901a\u8fc7\u60a3\u8005\u7684\u610f\u56fe\u547d\u4ee4\u5b9e\u65f6\u6a21\u4eff\u5176\u8fd0\u52a8\u52a8\u4f5c\uff0c\u572814\u540d\u53c2\u4e0e\u8005(3\u540d\u7269\u7406\u6cbb\u7597\u5e08\u548c11\u540d\u504f\u762b\u75c5\u4eba)\u4e2d\u8fdb\u884c\u4e86\u63a2\u7d22\u6027\u7814\u7a76", "result": "\u7cfb\u7edf\u572812\u6b21\u8bad\u7ec3\u4e2d\u6210\u529f\u5c55\u793a\u4e86\u8fd0\u52a8\u6a21\u4eff\u80fd\u529b\uff0c\u4f46\u51c6\u786e\u6027\u6709\u6240\u53d8\u5316\u3002\u53c2\u4e0e\u8005\u5bf9\u673a\u5668\u6559\u7ec3\u5448\u73b0\u51fa\u79ef\u6781\u6001\u5ea6\uff0c\u5177\u6709\u9ad8\u5ea6\u4fe1\u4efb\u548c\u63a5\u53d7\u5ea6\uff0c\u4e14BRI\u6280\u672f\u7684\u5f15\u5165\u6ca1\u6709\u5f71\u54cd\u8fd9\u4e9b\u79ef\u6781\u611f\u77e5", "conclusion": "\u57fa\u4e8e\u8111\u673a\u63a5\u53e3\u7684\u8fd0\u52a8\u6a21\u4eff\u6280\u672f\u5728\u5eb7\u590d\u8bad\u7ec3\u4e2d\u5177\u6709\u5f00\u53d1\u6f5c\u529b\uff0c\u80fd\u591f\u63d0\u5347\u60a3\u8005\u5bf9\u793e\u4ea4\u673a\u5668\u6559\u7ec3\u7684\u4fe1\u4efb\u548c\u63a5\u53d7\u5ea6"}}
{"id": "2509.11364", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.11364", "abs": "https://arxiv.org/abs/2509.11364", "authors": ["Sheng Liu", "Zhe Li", "Weiheng Wang", "Han Sun", "Heng Zhang", "Hongpeng Chen", "Yusen Qin", "Arash Ajoudani", "Yizhao Wang"], "title": "ActivePose: Active 6D Object Pose Estimation and Tracking for Robotic Manipulation", "comment": "6D Pose, Diffusion Policy", "summary": "Accurate 6-DoF object pose estimation and tracking are critical for reliable\nrobotic manipulation. However, zero-shot methods often fail under\nviewpoint-induced ambiguities and fixed-camera setups struggle when objects\nmove or become self-occluded. To address these challenges, we propose an active\npose estimation pipeline that combines a Vision-Language Model (VLM) with\n\"robotic imagination\" to dynamically detect and resolve ambiguities in real\ntime. In an offline stage, we render a dense set of views of the CAD model,\ncompute the FoundationPose entropy for each view, and construct a\ngeometric-aware prompt that includes low-entropy (unambiguous) and high-entropy\n(ambiguous) examples. At runtime, the system: (1) queries the VLM on the live\nimage for an ambiguity score; (2) if ambiguity is detected, imagines a discrete\nset of candidate camera poses by rendering virtual views, scores each based on\na weighted combination of VLM ambiguity probability and FoundationPose entropy,\nand then moves the camera to the Next-Best-View (NBV) to obtain a disambiguated\npose estimation. Furthermore, since moving objects may leave the camera's field\nof view, we introduce an active pose tracking module: a diffusion-policy\ntrained via imitation learning, which generates camera trajectories that\npreserve object visibility and minimize pose ambiguity. Experiments in\nsimulation and real-world show that our approach significantly outperforms\nclassical baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u673a\u5668\u4eba\u60f3\u8c61\u529b\u7684\u4e3b\u52a8\u4f4d\u59ff\u4f30\u8ba1\u4e0e\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u68c0\u6d4b\u548c\u89e3\u51b3\u89c6\u89d2\u5f15\u8d77\u7684\u6a21\u7cca\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e866\u81ea\u7531\u5ea6\u7269\u4f53\u4f4d\u59ff\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u96f6\u6837\u672c\u65b9\u6cd5\u5728\u89c6\u89d2\u5f15\u8d77\u7684\u6a21\u7cca\u6027\u4e0b\u5931\u6548\uff0c\u4ee5\u53ca\u56fa\u5b9a\u6444\u50cf\u5934\u5728\u7269\u4f53\u79fb\u52a8\u6216\u81ea\u906e\u6321\u65f6\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u9002\u5e94\u573a\u666f\u53d8\u5316\u7684\u4e3b\u52a8\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u79bb\u7ebf\u9636\u6bb5\u6e32\u67d3CAD\u6a21\u578b\u5bc6\u96c6\u89c6\u56fe\u5e76\u8ba1\u7b97\u71b5\u503c\uff0c\u6784\u5efa\u51e0\u4f55\u611f\u77e5\u63d0\u793a\u3002\u8fd0\u884c\u65f6\u4f7f\u7528VLM\u68c0\u6d4b\u6a21\u7cca\u6027\uff0c\u901a\u8fc7\u865a\u62df\u6e32\u67d3\u751f\u6210\u5019\u9009\u76f8\u673a\u4f4d\u59ff\uff0c\u7ed3\u5408VLM\u6982\u7387\u548c\u71b5\u503c\u8bc4\u5206\u9009\u62e9\u6700\u4f73\u89c6\u89d2\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u6269\u6563\u7b56\u7565\u7684\u4e3b\u52a8\u8ddf\u8e2a\u6a21\u5757\u4fdd\u6301\u7269\u4f53\u53ef\u89c1\u6027\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u89c6\u89d2\u6a21\u7cca\u548c\u7269\u4f53\u79fb\u52a8\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e3b\u52a8\u4f4d\u59ff\u4f30\u8ba1\u4e0e\u8ddf\u8e2a\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u673a\u5668\u4eba\u60f3\u8c61\u529b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u89d2\u6a21\u7cca\u548c\u7269\u4f53\u79fb\u52a8\u5e26\u6765\u7684\u6311\u6218\uff0c\u4e3a\u53ef\u9760\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11388", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.11388", "abs": "https://arxiv.org/abs/2509.11388", "authors": ["Romerik Lokossou", "Birhanu Shimelis Girma", "Ozan K. Tonguz", "Ahmed Biyabani"], "title": "Quantum deep reinforcement learning for humanoid robot navigation task", "comment": null, "summary": "Classical reinforcement learning (RL) methods often struggle in complex,\nhigh-dimensional environments because of their extensive parameter requirements\nand challenges posed by stochastic, non-deterministic settings. This study\nintroduces quantum deep reinforcement learning (QDRL) to train humanoid agents\nefficiently. While previous quantum RL models focused on smaller environments,\nsuch as wheeled robots and robotic arms, our work pioneers the application of\nQDRL to humanoid robotics, specifically in environments with substantial\nobservation and action spaces, such as MuJoCo's Humanoid-v4 and Walker2d-v4.\nUsing parameterized quantum circuits, we explored a hybrid quantum-classical\nsetup to directly navigate high-dimensional state spaces, bypassing traditional\nmapping and planning. By integrating quantum computing with deep RL, we aim to\ndevelop models that can efficiently learn complex navigation tasks in humanoid\nrobots. We evaluated the performance of the Soft Actor-Critic (SAC) in\nclassical RL against its quantum implementation. The results show that the\nquantum SAC achieves an 8% higher average return (246.40) than the classical\nSAC (228.36) after 92% fewer steps, highlighting the accelerated learning\npotential of quantum computing in RL tasks.", "AI": {"tldr": "\u91cf\u5b50\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60(QDRL)\u5728\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u76f8\u6bd4\u7ecf\u5178\u65b9\u6cd5\u5b9e\u73b092%\u66f4\u5c11\u6b65\u6570\u4e0b8%\u66f4\u9ad8\u56de\u62a5", "motivation": "\u7ecf\u5178\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u9ad8\u7ef4\u590d\u6742\u73af\u5883\u4e2d\u53c2\u6570\u9700\u6c42\u5927\u4e14\u9762\u4e34\u968f\u673a\u6027\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5", "method": "\u4f7f\u7528\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u6784\u5efa\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u67b6\u6784\uff0c\u76f4\u63a5\u5728MuJoCo Humanoid-v4\u548cWalker2d-v4\u7b49\u9ad8\u7ef4\u73af\u5883\u4e2d\u8bad\u7ec3\uff0c\u5bf9\u6bd4\u91cf\u5b50SAC\u4e0e\u7ecf\u5178SAC\u6027\u80fd", "result": "\u91cf\u5b50SAC\u5e73\u5747\u56de\u62a5246.40\uff0c\u6bd4\u7ecf\u5178SAC(228.36)\u9ad88%\uff0c\u4e14\u5b66\u4e60\u6b65\u6570\u51cf\u5c1192%", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5c55\u73b0\u51fa\u52a0\u901f\u5b66\u4e60\u6f5c\u529b\uff0c\u7279\u522b\u9002\u5408\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u7684\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1"}}
{"id": "2509.11402", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.11402", "abs": "https://arxiv.org/abs/2509.11402", "authors": ["Alessandra Rossi", "Patrick Holthaus", "Gabriella Lakatos", "S\u00edlvia Moros", "Ali Fallahi", "Murat Kirtay", "Marie Postma", "Erhan Oztop"], "title": "TRUST 2025: SCRITA and RTSS @ RO-MAN 2025", "comment": "TRUST 2025 workshop proceedings containing 7 papers", "summary": "The TRUST workshop is the result of a collaboration between two established\nworkshops in the field of Human-Robot Interaction: SCRITA (Trust, Acceptance\nand Social Cues in Human-Robot Interaction) and RTSS (Robot Trust for Symbiotic\nSocieties). This joint initiative brings together the complementary goals of\nthese workshops to advance research on trust from both the human and robot\nperspectives.\n  Website: https://scrita.herts.ac.uk/2025/", "AI": {"tldr": "TRUST\u7814\u8ba8\u4f1a\u662fSCRITA\u548cRTSS\u4e24\u4e2aHRI\u9886\u57df\u6210\u719f\u7814\u8ba8\u4f1a\u7684\u5408\u4f5c\u6210\u679c\uff0c\u6574\u5408\u4e86\u4ece\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u53cc\u91cd\u89c6\u89d2\u63a8\u8fdb\u4fe1\u4efb\u7814\u7a76\u7684\u76ee\u6807", "motivation": "\u6574\u5408\u4e24\u4e2a\u4e92\u8865\u7684HRI\u7814\u8ba8\u4f1a\uff08SCRITA\u5173\u6ce8\u4fe1\u4efb\u3001\u63a5\u53d7\u5ea6\u548c\u793e\u4f1a\u7ebf\u7d22\uff0cRTSS\u5173\u6ce8\u673a\u5668\u4eba\u4fe1\u4efb\u4e0e\u5171\u751f\u793e\u4f1a\uff09\u7684\u4f18\u52bf\uff0c\u5171\u540c\u63a8\u8fdb\u4eba\u673a\u4fe1\u4efb\u7814\u7a76", "method": "\u901a\u8fc7\u8054\u5408\u7814\u8ba8\u4f1a\u7684\u5f62\u5f0f\uff0c\u6c47\u96c6\u4e24\u4e2a\u5de5\u4f5c\u574a\u7684\u4e13\u5bb6\u8d44\u6e90\u548c\u7814\u7a76\u89c6\u89d2\uff0c\u5efa\u7acb\u5408\u4f5c\u5e73\u53f0", "result": "\u6210\u529f\u521b\u5efa\u4e86TRUST\u8054\u5408\u7814\u8ba8\u4f1a\uff0c\u4e3a\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7684\u4fe1\u4efb\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u4ea4\u6d41\u5e73\u53f0", "conclusion": "\u8fd9\u79cd\u5408\u4f5c\u6a21\u5f0f\u6709\u6548\u6574\u5408\u4e86\u4eba\u673a\u4fe1\u4efb\u7814\u7a76\u7684\u4e0d\u540c\u89c6\u89d2\uff0c\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u7684\u534f\u540c\u53d1\u5c55\u548c\u521b\u65b0"}}
{"id": "2509.11417", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11417", "abs": "https://arxiv.org/abs/2509.11417", "authors": ["Shresth Grover", "Akshay Gopalkrishnan", "Bo Ai", "Henrik I. Christensen", "Hao Su", "Xuanlin Li"], "title": "Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations", "comment": "Project Page: https://gen-vla.github.io/", "summary": "Vision-language-action (VLA) models finetuned from vision-language models\n(VLMs) hold the promise of leveraging rich pretrained representations to build\ngeneralist robots across diverse tasks and environments. However, direct\nfine-tuning on robot data often disrupts these representations and limits\ngeneralization. We present a framework that better preserves pretrained\nfeatures while adapting them for robot manipulation. Our approach introduces\nthree components: (i) a dual-encoder design with one frozen vision encoder to\nretain pretrained features and another trainable for task adaptation, (ii) a\nstring-based action tokenizer that casts continuous actions into character\nsequences aligned with the model's pretraining domain, and (iii) a co-training\nstrategy that combines robot demonstrations with vision-language datasets\nemphasizing spatial reasoning and affordances. Evaluations in simulation and on\nreal robots show that our method improves robustness to visual perturbations,\ngeneralization to novel instructions and environments, and overall task success\ncompared to baselines.", "AI": {"tldr": "\u901a\u8fc7\u51bb\u7ed3\u89c6\u89c9\u7f16\u7801\u5668\u3001\u5b57\u7b26\u4e32\u52a8\u4f5c\u6807\u8bb0\u5316\u548c\u591a\u6570\u636e\u6e90\u517c\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u4fdd\u7559\u9884\u8bad\u7ec3\u7279\u5f81\u7684\u540c\u65f6\u9002\u914d\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1", "motivation": "\u76f4\u63a5\u5bf9\u673a\u5668\u4eba\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u4f1a\u7834\u574f\u9884\u8bad\u7ec3\u7684\u4e30\u5bcc\u8868\u5f81\uff0c\u9650\u5236\u6a21\u578b\u7684\u666e\u9002\u6027\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u9002\u914d\u673a\u5668\u4eba\u4efb\u52a1\u7684\u540c\u65f6\u4fdd\u7559\u8fd9\u4e9b\u9884\u8bad\u7ec3\u7279\u5f81", "method": "1\uff09\u53cc\u7f16\u7801\u5668\u8bbe\u8ba1\uff1a\u4e00\u4e2a\u51bb\u7ed3\u7684\u89c6\u89c9\u7f16\u7801\u5668\u4fdd\u7559\u9884\u8bad\u7ec3\u7279\u5f81\uff0c\u53e6\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u7f16\u7801\u5668\u7528\u4e8e\u4efb\u52a1\u9002\u914d\n2\uff09\u5b57\u7b26\u4e32\u52a8\u4f5c\u6807\u8bb0\u5316\u5668\uff1a\u5c06\u8fde\u7eed\u52a8\u4f5c\u8f6c\u6362\u4e3a\u5b57\u7b26\u5e8f\u5217\uff0c\u4e0e\u6a21\u578b\u9884\u8bad\u7ec3\u57df\u5bf9\u9f50\n3\uff09\u534f\u540c\u8bad\u7ec3\u7b56\u7565\uff1a\u7ed3\u5408\u673a\u5668\u4eba\u793a\u8303\u6570\u636e\u548c\u5f3a\u8c03\u7a7a\u95f4\u63a8\u7406\u53ca\u652f\u914d\u6027\u7684\u89c6\u89c9-\u8bed\u8a00\u6570\u636e\u96c6", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5bf9\u89c6\u89c9\u5e72\u6270\u7684\u7a33\u5065\u6027\u3001\u5bf9\u65b0\u6307\u4ee4\u548c\u73af\u5883\u7684\u666e\u9002\u6027\uff0c\u4ee5\u53ca\u6574\u4f53\u4efb\u52a1\u6210\u529f\u7387\uff0c\u8d85\u8fc7\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5730\u4fdd\u7559\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u7279\u5f81\uff0c\u540c\u65f6\u6210\u529f\u9002\u914d\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u4e3a\u5efa\u7acb\u666e\u9002\u6027\u66f4\u5f3a\u7684VLA\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84"}}
{"id": "2509.11433", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.11433", "abs": "https://arxiv.org/abs/2509.11433", "authors": ["Pedro Portugal", "Damian D. Venghaus", "Diego Lopez"], "title": "A Software-Only Post-Processor for Indexed Rotary Machining on GRBL-Based CNCs", "comment": "21 pages, 10 figures, Python post-processor source code and web\n  interface included", "summary": "Affordable desktop CNC routers are common in education, prototyping, and\nmakerspaces, but most lack a rotary axis, limiting fabrication of rotationally\nsymmetric or multi-sided parts. Existing solutions often require hardware\nretrofits, alternative controllers, or commercial CAM software, raising cost\nand complexity. This work presents a software-only framework for indexed rotary\nmachining on GRBL-based CNCs. A custom post-processor converts planar toolpaths\ninto discrete rotary steps, executed through a browser-based interface. While\nnot equivalent to continuous 4-axis machining, the method enables practical\nrotary-axis fabrication using only standard, off-the-shelf mechanics, without\nfirmware modification. By reducing technical and financial barriers, the\nframework expands access to multi-axis machining in classrooms, makerspaces,\nand small workshops, supporting hands-on learning and rapid prototyping.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3aGRBL\u57fa\u7840CNC\u673a\u5e8a\u8bbe\u8ba1\u7684\u8f6f\u4ef6\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8f6f\u4ef6\u540e\u5904\u7406\u5668\u5c06\u5e73\u9762\u5200\u5177\u8def\u5f84\u8f6c\u6362\u4e3a\u95f4\u8ddd\u65cb\u8f6c\u6b65\u9aa4\uff0c\u65e0\u9700\u786c\u4ef6\u6539\u9020\u5373\u53ef\u5b9e\u73b0\u591a\u9762\u5206\u52a8\u52a0\u5de5\u3002", "motivation": "\u89e3\u51b3\u666e\u901a\u684c\u9762CNC\u5200\u524d\u7f3a\u4e4f\u65cb\u8f6c\u8f74\u7684\u95ee\u9898\uff0c\u8fd9\u4e2a\u95ee\u9898\u9650\u5236\u4e86\u65cb\u8f6c\u5bf9\u79f0\u548c\u591a\u9762\u90e8\u4ef6\u7684\u5236\u9020\u80fd\u529b\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u9700\u8981\u786c\u4ef6\u6539\u9020\u3001\u66ff\u4ee3\u63a7\u5236\u5668\u6216\u5546\u4e1aCAM\u8f6f\u4ef6\uff0c\u6210\u672c\u9ad8\u4e14\u590d\u6742\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f6f\u4ef6\u4ec5\u7684\u6846\u67b6\uff0c\u5305\u542b\u81ea\u5b9a\u4e49\u540e\u5904\u7406\u5668\uff0c\u5c06\u5e73\u9762\u5200\u5177\u8def\u5f84\u8f6c\u6362\u4e3a\u79bb\u6563\u7684\u65cb\u8f6c\u6b65\u9aa4\uff0c\u901a\u8fc7\u6d4f\u89c8\u5668\u57fa\u7840\u754c\u9762\u6267\u884c\u3002\u65b9\u6cd5\u4e0d\u9700\u8981\u4fee\u6539\u786c\u4ef6\u56fa\u4ef6\u3002", "result": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u5b9e\u7528\u7684\u65cb\u8f8a\u8f74\u52a0\u5de5\u80fd\u529b\uff0c\u867d\u7136\u4e0d\u80fd\u6a21\u62df\u8fde\u7eed\u76844\u8f74\u52a0\u5de5\uff0c\u4f46\u53ef\u4ee5\u4f7f\u7528\u6807\u51c6\u673a\u68b0\u914d\u4ef6\u5b8c\u6210\u591a\u9762\u5206\u52a8\u52a0\u5de5\u3002", "conclusion": "\u901a\u8fc7\u964d\u4f4e\u6280\u672f\u548c\u8d44\u91d1\u95ef\u788d\uff0c\u8be5\u6846\u67b6\u6269\u5927\u4e86\u591a\u8f74\u52a0\u5de5\u5728\u6559\u5ba4\u3001\u5236\u9020\u7a7a\u95f4\u548c\u5c0f\u578b\u5de5\u4f5c\u5ba4\u4e2d\u7684\u5e94\u7528\uff0c\u652f\u6301\u624b\u6280\u5b66\u4e60\u548c\u5feb\u901f\u539f\u578b\u5236\u4f5c\u3002"}}
{"id": "2509.11481", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11481", "abs": "https://arxiv.org/abs/2509.11481", "authors": ["Jonas Eschmann", "Dario Albani", "Giuseppe Loianno"], "title": "RAPTOR: A Foundation Policy for Quadrotor Control", "comment": null, "summary": "Humans are remarkably data-efficient when adapting to new unseen conditions,\nlike driving a new car. In contrast, modern robotic control systems, like\nneural network policies trained using Reinforcement Learning (RL), are highly\nspecialized for single environments. Because of this overfitting, they are\nknown to break down even under small differences like the Simulation-to-Reality\n(Sim2Real) gap and require system identification and retraining for even\nminimal changes to the system. In this work, we present RAPTOR, a method for\ntraining a highly adaptive foundation policy for quadrotor control. Our method\nenables training a single, end-to-end neural-network policy to control a wide\nvariety of quadrotors. We test 10 different real quadrotors from 32 g to 2.4 kg\nthat also differ in motor type (brushed vs. brushless), frame type (soft vs.\nrigid), propeller type (2/3/4-blade), and flight controller\n(PX4/Betaflight/Crazyflie/M5StampFly). We find that a tiny, three-layer policy\nwith only 2084 parameters is sufficient for zero-shot adaptation to a wide\nvariety of platforms. The adaptation through In-Context Learning is made\npossible by using a recurrence in the hidden layer. The policy is trained\nthrough a novel Meta-Imitation Learning algorithm, where we sample 1000\nquadrotors and train a teacher policy for each of them using Reinforcement\nLearning. Subsequently, the 1000 teachers are distilled into a single, adaptive\nstudent policy. We find that within milliseconds, the resulting foundation\npolicy adapts zero-shot to unseen quadrotors. We extensively test the\ncapabilities of the foundation policy under numerous conditions (trajectory\ntracking, indoor/outdoor, wind disturbance, poking, different propellers).", "AI": {"tldr": "RAPTOR\u65b9\u6cd5\u8bad\u7ec3\u4e86\u4e00\u4e2a\u9ad8\u5ea6\u81ea\u9002\u5e94\u7684\u56db\u65cb\u7ffc\u63a7\u5236\u57fa\u7840\u7b56\u7565\uff0c\u4ec5\u75282084\u4e2a\u53c2\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u5c31\u80fd\u96f6\u6837\u672c\u9002\u5e9410\u79cd\u4e0d\u540c\u7684\u771f\u5b9e\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\uff0c\u901a\u8fc7\u5143\u6a21\u4eff\u5b66\u4e60\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\u3002", "motivation": "\u89e3\u51b3\u73b0\u4ee3\u673a\u5668\u4eba\u63a7\u5236\u7cfb\u7edf\uff08\u5982RL\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\uff09\u8fc7\u5ea6\u4e13\u4e1a\u5316\u7684\u95ee\u9898\uff0c\u4f7f\u5176\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u9ad8\u6548\u9002\u5e94\u65b0\u73af\u5883\uff0c\u514b\u670dSim2Real\u5dee\u8ddd\u548c\u7cfb\u7edf\u5fae\u5c0f\u53d8\u5316\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u5143\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\uff1a\u5148\u91c7\u68371000\u4e2a\u56db\u65cb\u7ffc\u5e76\u8bad\u7ec3\u5404\u81ea\u7684\u6559\u5e08\u7b56\u7565\uff0c\u7136\u540e\u84b8\u998f\u6210\u5355\u4e00\u81ea\u9002\u5e94\u5b66\u751f\u7b56\u7565\uff1b\u5229\u7528\u9690\u85cf\u5c42\u5faa\u73af\u5b9e\u73b0\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002", "result": "\u5728\u6beb\u79d2\u7ea7\u522b\u5185\u96f6\u6837\u672c\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u56db\u65cb\u7ffc\uff0c\u6210\u529f\u6d4b\u8bd5\u4e8610\u79cd\u4e0d\u540c\u771f\u5b9e\u56db\u65cb\u7ffc\uff0832g-2.4kg\uff09\uff0c\u6db5\u76d6\u591a\u79cd\u7535\u673a\u7c7b\u578b\u3001\u6846\u67b6\u7c7b\u578b\u3001\u87ba\u65cb\u6868\u7c7b\u578b\u548c\u98de\u884c\u63a7\u5236\u5668\u3002", "conclusion": "RAPTOR\u65b9\u6cd5\u80fd\u591f\u8bad\u7ec3\u51fa\u9ad8\u5ea6\u81ea\u9002\u5e94\u7684\u57fa\u7840\u63a7\u5236\u7b56\u7565\uff0c\u4ec5\u9700\u6781\u5c0f\u53c2\u6570\u5c31\u80fd\u5b9e\u73b0\u8de8\u5e73\u53f0\u7684\u96f6\u6837\u672c\u9002\u5e94\uff0c\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u8868\u73b0\u9c81\u68d2\u3002"}}
{"id": "2509.11504", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.11504", "abs": "https://arxiv.org/abs/2509.11504", "authors": ["Yidan Lu", "Yinzhao Dong", "Jiahui Zhang", "Ji Ma", "Peng Lu"], "title": "FR-Net: Learning Robust Quadrupedal Fall Recovery on Challenging Terrains through Mass-Contact Prediction", "comment": "Published in IEEE Robotics and Automation Letters, Vol. 10, No. 7,\n  pp. 6632-6639, 2025", "summary": "Fall recovery for legged robots remains challenging, particularly on complex\nterrains where traditional controllers fail due to incomplete terrain\nperception and uncertain interactions. We present \\textbf{FR-Net}, a\nlearning-based framework that enables quadrupedal robots to recover from\narbitrary fall poses across diverse environments. Central to our approach is a\nMass-Contact Predictor network that estimates the robot's mass distribution and\ncontact states from limited sensory inputs, facilitating effective recovery\nstrategies. Our carefully designed reward functions ensure safe recovery even\non steep stairs without dangerous rolling motions common to existing methods.\nTrained entirely in simulation using privileged learning, our framework guides\npolicy learning without requiring explicit terrain data during deployment. We\ndemonstrate the generalization capabilities of \\textbf{FR-Net} across different\nquadrupedal platforms in simulation and validate its performance through\nextensive real-world experiments on the Go2 robot in 10 challenging scenarios.\nOur results indicate that explicit mass-contact prediction is key to robust\nfall recovery, offering a promising direction for generalizable quadrupedal\nskills.", "AI": {"tldr": "FR-Net\u662f\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u56db\u8db3\u673a\u5668\u4eba\u8dcc\u5012\u6062\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u8d28\u91cf-\u63a5\u89e6\u9884\u6d4b\u7f51\u7edc\u4ece\u6709\u9650\u4f20\u611f\u5668\u8f93\u5165\u4f30\u8ba1\u8d28\u91cf\u5206\u5e03\u548c\u63a5\u89e6\u72b6\u6001\uff0c\u80fd\u5728\u590d\u6742\u5730\u5f62\u4e0a\u5b9e\u73b0\u5b89\u5168\u6062\u590d\u3002", "motivation": "\u4f20\u7edf\u63a7\u5236\u5668\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7531\u4e8e\u5730\u5f62\u611f\u77e5\u4e0d\u5b8c\u6574\u548c\u4ea4\u4e92\u4e0d\u786e\u5b9a\u6027\u800c\u5931\u8d25\uff0c\u56db\u8db3\u673a\u5668\u4eba\u7684\u8dcc\u5012\u6062\u590d\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u8d28\u91cf-\u63a5\u89e6\u9884\u6d4b\u5668\u7f51\u7edc\u4f30\u8ba1\u673a\u5668\u4eba\u7684\u8d28\u91cf\u5206\u5e03\u548c\u63a5\u89e6\u72b6\u6001\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u5728\u6a21\u62df\u4e2d\u8fdb\u884c\u7279\u6743\u5b66\u4e60\u8bad\u7ec3\uff0c\u65e0\u9700\u90e8\u7f72\u65f6\u7684\u663e\u5f0f\u5730\u5f62\u6570\u636e\u3002", "result": "\u5728\u6a21\u62df\u4e2d\u5c55\u793a\u4e86\u8de8\u4e0d\u540c\u56db\u8db3\u5e73\u53f0\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728Go2\u673a\u5668\u4eba\u4e0a\u768410\u4e2a\u6311\u6218\u6027\u573a\u666f\u4e2d\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6027\u80fd\u3002", "conclusion": "\u663e\u5f0f\u7684\u8d28\u91cf-\u63a5\u89e6\u9884\u6d4b\u662f\u9c81\u68d2\u8dcc\u5012\u6062\u590d\u7684\u5173\u952e\uff0c\u4e3a\u53ef\u6cdb\u5316\u7684\u56db\u8db3\u673a\u5668\u4eba\u6280\u80fd\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2509.11506", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.11506", "abs": "https://arxiv.org/abs/2509.11506", "authors": ["Takahiro Hattori", "Kento Kawaharazuka", "Kei Okada"], "title": "Design and Development of a Remotely Wire-Driven Walking Robot", "comment": "Accepted Humanoids2025, website -\n  https://hatofly.github.io/remote-wire-driven-quadruped/", "summary": "Operating in environments too harsh or inaccessible for humans is one of the\ncritical roles expected of robots. However, such environments often pose risks\nto electronic components as well. To overcome this, various approaches have\nbeen developed, including autonomous mobile robots without electronics,\nhydraulic remotely actuated mobile robots, and long-reach robot arms driven by\nwires. Among these, electronics-free autonomous robots cannot make complex\ndecisions, while hydraulically actuated mobile robots and wire-driven robot\narms are used in harsh environments such as nuclear power plants. Mobile robots\noffer greater reach and obstacle avoidance than robot arms, and wire mechanisms\noffer broader environmental applicability than hydraulics. However, wire-driven\nsystems have not been used for remote actuation of mobile robots. In this\nstudy, we propose a novel mechanism called Remote Wire Drive that enables\nremote actuation of mobile robots via wires. This mechanism is a series\nconnection of decoupled joints, a mechanism used in wire-driven robot arms,\nadapted for power transmission. We experimentally validated its feasibility by\nactuating a wire-driven quadruped robot, which we also developed in this study,\nthrough Remote Wire Drive.", "AI": {"tldr": "\u901a\u8fc7\u7ebf\u7efc\u9a71\u52a8\u673a\u5236\u5b9e\u73b0\u79bb\u7ebf\u63a7\u5236\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u89e3\u51b3\u6781\u7aef\u73af\u5883\u4e2d\u7535\u5b50\u5143\u4ef6\u5bb9\u6613\u635f\u574f\u7684\u95ee\u9898", "motivation": "\u5728\u6838\u7535\u7ad9\u7b49\u6781\u7aef\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u7684\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u9700\u8981\u7535\u5b50\u5143\u4ef6\uff0c\u800c\u65e0\u7535\u5b50\u81ea\u4e3b\u673a\u5668\u4eba\u53c8\u65e0\u6cd5\u505a\u590d\u6742\u51b3\u7b56\u3002\u9700\u8981\u4e00\u79cd\u65b0\u7684\u673a\u5236\u6765\u5b9e\u73b0\u79bb\u7ebf\u63a7\u5236\u7684\u79fb\u52a8\u673a\u5668\u4eba", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\"Remote Wire Drive\"\u7684\u65b0\u578b\u673a\u5236\uff0c\u901a\u8fc7\u7ebf\u7efc\u4f20\u52a8\u52a8\u529b\u6765\u8fdc\u7a0b\u9a71\u52a8\u79fb\u52a8\u673a\u5668\u4eba\u3002\u8be5\u673a\u5236\u662f\u5c06\u7528\u4e8e\u7ebf\u7efc\u9a71\u52a8\u673a\u68b0\u81c2\u7684\u89e3\u8026\u5173\u8282\u7cfb\u5217\u8fde\u63a5\u673a\u5236\u9002\u914d\u4e3a\u52a8\u529b\u4f20\u9012\u7cfb\u7edf", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u673a\u5236\u7684\u53ef\u884c\u6027\uff0c\u6210\u529f\u5730\u901a\u8fc7Remote Wire Drive\u9a71\u52a8\u4e86\u81ea\u884c\u5f00\u53d1\u7684\u7ebf\u7efc\u9a71\u52a8\u56db\u8db3\u673a\u5668\u4eba", "conclusion": "Remote Wire Drive\u673a\u5236\u4e3a\u6781\u7aef\u73af\u5883\u4e0b\u7684\u79fb\u52a8\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u4e86\u7ebf\u7efc\u9a71\u52a8\u7684\u5e7f\u6cdb\u73af\u5883\u9002\u7528\u6027\u548c\u79fb\u52a8\u673a\u5668\u4eba\u7684\u5927\u8303\u56f4\u5230\u8fbe\u80fd\u529b"}}
{"id": "2509.11516", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.11516", "abs": "https://arxiv.org/abs/2509.11516", "authors": ["Chengjin Wang", "Zheng Yan", "Yanmin Zhou", "Runjie Shen", "Zhipeng Wang", "Bin Cheng", "Bin He"], "title": "PaiP: An Operational Aware Interactive Planner for Unknown Cabinet Environments", "comment": null, "summary": "Box/cabinet scenarios with stacked objects pose significant challenges for\nrobotic motion due to visual occlusions and constrained free space. Traditional\ncollision-free trajectory planning methods often fail when no collision-free\npaths exist, and may even lead to catastrophic collisions caused by invisible\nobjects. To overcome these challenges, we propose an operational aware\ninteractive motion planner (PaiP) a real-time closed-loop planning framework\nutilizing multimodal tactile perception. This framework autonomously infers\nobject interaction features by perceiving motion effects at interaction\ninterfaces. These interaction features are incorporated into grid maps to\ngenerate operational cost maps. Building upon this representation, we extend\nsampling-based planning methods to interactive planning by optimizing both path\ncost and operational cost. Experimental results demonstrate that PaiP achieves\nrobust motion in narrow spaces.", "AI": {"tldr": "\u63d0\u51fa\u4e86PaiP\u4ea4\u4e92\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u89e6\u89c9\u611f\u77e5\u5b9e\u73b0\u5b9e\u65f6\u95ed\u73af\u89c4\u5212\uff0c\u89e3\u51b3\u7bb1\u67dc\u573a\u666f\u4e2d\u5806\u53e0\u7269\u4f53\u7684\u89c6\u89c9\u906e\u6321\u548c\u7a7a\u95f4\u53d7\u9650\u95ee\u9898", "motivation": "\u4f20\u7edf\u65e0\u78b0\u649e\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u5728\u7bb1\u67dc\u5806\u53e0\u573a\u666f\u4e2d\u7ecf\u5e38\u5931\u8d25\uff0c\u56e0\u4e3a\u4e0d\u5b58\u5728\u65e0\u78b0\u649e\u8def\u5f84\uff0c\u751a\u81f3\u53ef\u80fd\u56e0\u4e0d\u53ef\u89c1\u7269\u4f53\u5bfc\u81f4\u707e\u96be\u6027\u78b0\u649e", "method": "\u5229\u7528\u591a\u6a21\u6001\u89e6\u89c9\u611f\u77e5\u81ea\u4e3b\u63a8\u65ad\u7269\u4f53\u4ea4\u4e92\u7279\u5f81\uff0c\u5c06\u4ea4\u4e92\u7279\u5f81\u878d\u5165\u6805\u683c\u5730\u56fe\u751f\u6210\u64cd\u4f5c\u4ee3\u4ef7\u56fe\uff0c\u5728\u91c7\u6837\u89c4\u5212\u65b9\u6cd5\u57fa\u7840\u4e0a\u540c\u65f6\u4f18\u5316\u8def\u5f84\u4ee3\u4ef7\u548c\u64cd\u4f5c\u4ee3\u4ef7", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660ePaiP\u5728\u72ed\u7a84\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u8fd0\u52a8", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u4ea4\u4e92\u611f\u77e5\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u906e\u6321\u73af\u5883\u4e0b\u7684\u8fd0\u52a8\u89c4\u5212\u6311\u6218"}}
{"id": "2509.11567", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.11567", "abs": "https://arxiv.org/abs/2509.11567", "authors": ["Eron Ristich", "Jiahe Wang", "Lei Zhang", "Sultan Haidar Ali", "Wanxin Jin", "Yi Ren", "Jiefeng Sun"], "title": "Shape control of simulated multi-segment continuum robots via Koopman operators with per-segment projection", "comment": "7 pages (+2 pages of references), 8 figures", "summary": "Soft continuum robots can allow for biocompatible yet compliant motions, such\nas the ability of octopus arms to swim, crawl, and manipulate objects. However,\ncurrent state-of-the-art continuum robots can only achieve real-time task-space\ncontrol (i.e., tip control) but not whole-shape control, mainly due to the high\ncomputational cost from its infinite degrees of freedom. In this paper, we\npresent a data-driven Koopman operator-based approach for the shape control of\nsimulated multi-segment tendon-driven soft continuum robots with the Kirchhoff\nrod model. Using data collected from these simulated soft robots, we conduct a\nper-segment projection scheme on the state of the robots allowing for the\nidentification of control-affine Koopman models that are an order of magnitude\nmore accurate than without the projection scheme. Using these learned Koopman\nmodels, we use a linear model predictive control (MPC) to control the robots to\na collection of target shapes of varying complexity. Our method realizes\ncomputationally efficient closed-loop control, and demonstrates the feasibility\nof real-time shape control for soft robots. We envision this work can pave the\nway for practical shape control of soft continuum robots.", "AI": {"tldr": "\u57fa\u4e8eKoopman\u7b97\u5b50\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5b9e\u73b0\u8f6f\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u7684\u5f62\u72b6\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898", "motivation": "\u5f53\u524d\u8f6f\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u53ea\u80fd\u5b9e\u73b0\u5b9e\u65f6\u7aef\u70b9\u63a7\u5236\uff0c\u800c\u65e0\u6cd5\u5b9e\u73b0\u5168\u5f62\u72b6\u63a7\u5236\uff0c\u4e3b\u8981\u56e0\u4e3a\u65e0\u9650\u81ea\u7531\u5ea6\u5bfc\u81f4\u7684\u9ad8\u8ba1\u7b97\u6210\u672c", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u7684Koopman\u7b97\u5b50\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u8f6f\u673a\u5668\u4eba\u6536\u96c6\u6570\u636e\uff0c\u8fdb\u884c\u6bcf\u6bb5\u6295\u5f71\u65b9\u6848\uff0c\u8bc6\u522b\u63a7\u5236\u4f34\u968fKoopman\u6a21\u578b\uff0c\u4f7f\u7528\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236(MPC)\u5b9e\u73b0\u5f62\u72b6\u63a7\u5236", "result": "\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u95ed\u73af\u63a7\u5236\uff0c\u6a21\u578b\u51c6\u786e\u6027\u6bd4\u65e0\u6295\u5f71\u65b9\u6848\u63d0\u9ad8\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u8bc1\u660e\u4e86\u8f6f\u673a\u5668\u4eba\u5b9e\u65f6\u5f62\u72b6\u63a7\u5236\u7684\u53ef\u884c\u6027", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8f6f\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u7684\u5b9e\u7528\u5f62\u72b6\u63a7\u5236\u63a8\u5e7f\u4e86\u9053\u8def"}}
{"id": "2509.11594", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11594", "abs": "https://arxiv.org/abs/2509.11594", "authors": ["Jizhuo Chen", "Diwen Liu", "Jiaming Wang", "Harold Soh"], "title": "GBPP: Grasp-Aware Base Placement Prediction for Robots via Two-Stage Learning", "comment": "Jizhuo Chen and Diwen Liu contributed equally", "summary": "GBPP is a fast learning based scorer that selects a robot base pose for\ngrasping from a single RGB-D snapshot. The method uses a two stage curriculum:\n(1) a simple distance-visibility rule auto-labels a large dataset at low cost;\nand (2) a smaller set of high fidelity simulation trials refines the model to\nmatch true grasp outcomes. A PointNet++ style point cloud encoder with an MLP\nscores dense grids of candidate poses, enabling rapid online selection without\nfull task-and-motion optimization. In simulation and on a real mobile\nmanipulator, GBPP outperforms proximity and geometry only baselines, choosing\nsafer and more reachable stances and degrading gracefully when wrong. The\nresults offer a practical recipe for data efficient, geometry aware base\nplacement: use inexpensive heuristics for coverage, then calibrate with\ntargeted simulation.", "AI": {"tldr": "GBPP\u662f\u4e00\u79cd\u57fa\u4e8e\u5feb\u901f\u5b66\u4e60\u7684\u8bc4\u5206\u5668\uff0c\u901a\u8fc7\u5355\u6b21RGB-D\u5feb\u7167\u4e3a\u6293\u53d6\u4efb\u52a1\u9009\u62e9\u673a\u5668\u4eba\u57fa\u5ea7\u4f4d\u59ff\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff1a\u4f4e\u6210\u672c\u81ea\u52a8\u6807\u6ce8\u5927\u91cf\u6570\u636e\uff0c\u7136\u540e\u901a\u8fc7\u5c11\u91cf\u9ad8\u4fdd\u771f\u4eff\u771f\u8bd5\u9a8c\u7cbe\u8c03\u6a21\u578b\u4ee5\u5339\u914d\u771f\u5b9e\u6293\u53d6\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u4e2d\u57fa\u5ea7\u4f4d\u59ff\u9009\u62e9\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u590d\u6742\u7684\u4efb\u52a1\u548c\u8fd0\u52a8\u4f18\u5316\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\u3002", "method": "\u4f7f\u7528\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff1a1) \u57fa\u4e8e\u8ddd\u79bb-\u53ef\u89c1\u6027\u89c4\u5219\u7684\u7b80\u5355\u542f\u53d1\u5f0f\u65b9\u6cd5\u81ea\u52a8\u6807\u6ce8\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff1b2) \u901a\u8fc7\u5c11\u91cf\u9ad8\u4fdd\u771f\u4eff\u771f\u8bd5\u9a8c\u7cbe\u8c03\u6a21\u578b\u3002\u91c7\u7528PointNet++\u98ce\u683c\u7684\u70b9\u4e91\u7f16\u7801\u5668\u548cMLP\u5bf9\u5019\u9009\u4f4d\u59ff\u5bc6\u96c6\u7f51\u683c\u8fdb\u884c\u8bc4\u5206\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u79fb\u52a8\u673a\u68b0\u81c2\u4e0a\uff0cGBPP\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u63a5\u8fd1\u5ea6\u548c\u51e0\u4f55\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9009\u62e9\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u8fbe\u7684\u7ad9\u4f4d\uff0c\u5728\u9519\u8bef\u65f6\u80fd\u591f\u4f18\u96c5\u964d\u7ea7\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u3001\u51e0\u4f55\u611f\u77e5\u7684\u57fa\u5ea7\u4f4d\u59ff\u9009\u62e9\u5b9e\u7528\u65b9\u6848\uff1a\u4f7f\u7528\u5ec9\u4ef7\u542f\u53d1\u5f0f\u65b9\u6cd5\u5b9e\u73b0\u8986\u76d6\uff0c\u7136\u540e\u901a\u8fc7\u9488\u5bf9\u6027\u4eff\u771f\u8fdb\u884c\u6821\u51c6\u3002"}}
{"id": "2509.11617", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.11617", "abs": "https://arxiv.org/abs/2509.11617", "authors": ["Qi Zheng", "Chaoran Zhang", "Zijian Liang", "EnTe Lin", "Shubo Cui", "Qinghongbing Xie", "Zhaobo Xu", "Long Zeng"], "title": "AssemMate: Graph-Based LLM for Robotic Assembly Assistance", "comment": null, "summary": "Large Language Model (LLM)-based robotic assembly assistance has gained\nsignificant research attention. It requires the injection of domain-specific\nknowledge to guide the assembly process through natural language interaction\nwith humans. Despite some progress, existing methods represent knowledge in the\nform of natural language text. Due to the long context and redundant content,\nthey struggle to meet the robots' requirements for real-time and precise\nreasoning. In order to bridge this gap, we present AssemMate, which utilizes\nthe graph\\textemdash a concise and accurate form of knowledge\nrepresentation\\textemdash as input. This graph-based LLM enables knowledge\ngraph question answering (KGQA), supporting human-robot interaction and\nassembly task planning for specific products. Beyond interactive QA, AssemMate\nalso supports sensing stacked scenes and executing grasping to assist with\nassembly. Specifically, a self-supervised Graph Convolutional Network (GCN)\nencodes knowledge graph entities and relations into a latent space and aligns\nthem with LLM's representation, enabling the LLM to understand graph\ninformation. In addition, a vision-enhanced strategy is employed to address\nstacked scenes in grasping. Through training and evaluation, AssemMate\noutperforms existing methods, achieving 6.4\\% higher accuracy, 3 times faster\ninference, and 28 times shorter context length, while demonstrating strong\ngeneralization ability on random graphs. And our approach further demonstrates\nsuperiority through robotic grasping experiments in both simulated and\nreal-world settings. More details can be found on the project page:\nhttps://github.com/cristina304/AssemMate.git", "AI": {"tldr": "AssemMate\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684LLM\u7cfb\u7edf\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u88c5\u914d\u8f85\u52a9\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u8868\u793a\u77e5\u8bc6\uff0c\u6bd4\u4f20\u7edf\u6587\u672c\u65b9\u6cd5\u66f4\u9ad8\u6548\u51c6\u786e\uff0c\u652f\u6301\u95ee\u7b54\u3001\u4efb\u52a1\u89c4\u5212\u548c\u6293\u53d6\u64cd\u4f5c", "motivation": "\u73b0\u6709\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6587\u672c\u7684\u77e5\u8bc6\u8868\u793a\u65b9\u6cd5\u5b58\u5728\u4e0a\u4e0b\u6587\u8fc7\u957f\u3001\u5185\u5bb9\u5197\u4f59\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u673a\u5668\u4eba\u5b9e\u65f6\u7cbe\u786e\u63a8\u7406\u7684\u9700\u6c42\uff0c\u9700\u8981\u66f4\u7b80\u6d01\u51c6\u786e\u7684\u77e5\u8bc6\u8868\u793a\u5f62\u5f0f", "method": "\u4f7f\u7528\u77e5\u8bc6\u56fe\u8c31\u4f5c\u4e3a\u8f93\u5165\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u56fe\u5377\u79ef\u7f51\u7edc(GCN)\u5c06\u77e5\u8bc6\u56fe\u8c31\u5b9e\u4f53\u548c\u5173\u7cfb\u7f16\u7801\u5230\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u4e0eLLM\u8868\u793a\u5bf9\u9f50\uff0c\u4f7fLLM\u80fd\u591f\u7406\u89e3\u56fe\u4fe1\u606f\uff1b\u91c7\u7528\u89c6\u89c9\u589e\u5f3a\u7b56\u7565\u5904\u7406\u5806\u53e0\u573a\u666f\u6293\u53d6", "result": "\u5728\u8bad\u7ec3\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1a\u51c6\u786e\u7387\u63d0\u9ad86.4%\uff0c\u63a8\u7406\u901f\u5ea6\u5feb3\u500d\uff0c\u4e0a\u4e0b\u6587\u957f\u5ea6\u7f29\u77ed28\u500d\uff0c\u5728\u968f\u673a\u56fe\u8c31\u4e0a\u5c55\u793a\u5f3a\u6cdb\u5316\u80fd\u529b\uff1b\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u6293\u53d6\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u8d8a", "conclusion": "\u56fe\u7ed3\u6784\u7684\u77e5\u8bc6\u8868\u793a\u76f8\u6bd4\u81ea\u7136\u8bed\u8a00\u6587\u672c\u66f4\u9002\u7528\u4e8e\u673a\u5668\u4eba\u88c5\u914d\u8f85\u52a9\u4efb\u52a1\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u5b9e\u65f6\u3001\u7cbe\u786e\u7684\u63a8\u7406\u80fd\u529b\uff0cAssemMate\u7cfb\u7edf\u5728\u51c6\u786e\u7387\u3001\u901f\u5ea6\u548c\u6cdb\u5316\u6027\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347"}}
{"id": "2509.11621", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.11621", "abs": "https://arxiv.org/abs/2509.11621", "authors": ["Xiangtong Yao", "Yirui Zhou", "Yuan Meng", "Yanwen Liu", "Liangyu Dong", "Zitao Zhang", "Zhenshan Bing", "Kai Huang", "Fuchun Sun", "Alois Knoll"], "title": "Inference-stage Adaptation-projection Strategy Adapts Diffusion Policy to Cross-manipulators Scenarios", "comment": "2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Diffusion policies are powerful visuomotor models for robotic manipulation,\nyet they often fail to generalize to manipulators or end-effectors unseen\nduring training and struggle to accommodate new task requirements at inference\ntime. Addressing this typically requires costly data recollection and policy\nretraining for each new hardware or task configuration. To overcome this, we\nintroduce an adaptation-projection strategy that enables a diffusion policy to\nperform zero-shot adaptation to novel manipulators and dynamic task settings,\nentirely at inference time and without any retraining. Our method first trains\na diffusion policy in SE(3) space using demonstrations from a base manipulator.\nDuring online deployment, it projects the policy's generated trajectories to\nsatisfy the kinematic and task-specific constraints imposed by the new hardware\nand objectives. Moreover, this projection dynamically adapts to physical\ndifferences (e.g., tool-center-point offsets, jaw widths) and task requirements\n(e.g., obstacle heights), ensuring robust and successful execution. We validate\nour approach on real-world pick-and-place, pushing, and pouring tasks across\nmultiple manipulators, including the Franka Panda and Kuka iiwa 14, equipped\nwith a diverse array of end-effectors like flexible grippers, Robotiq 2F/3F\ngrippers, and various 3D-printed designs. Our results demonstrate consistently\nhigh success rates in these cross-manipulator scenarios, proving the\neffectiveness and practicality of our adaptation-projection strategy. The code\nwill be released after peer review.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u9002\u5e94\u7b56\u7565\uff0c\u4f7f\u6269\u6563\u7b56\u7565\u80fd\u591f\u5728\u63a8\u7406\u65f6\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u65b0\u673a\u68b0\u81c2\u548c\u52a8\u6001\u4efb\u52a1\u8bbe\u7f6e", "motivation": "\u6269\u6563\u7b56\u7565\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u5f3a\u5927\uff0c\u4f46\u96be\u4ee5\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u673a\u68b0\u81c2\u548c\u672b\u7aef\u6267\u884c\u5668\uff0c\u4e14\u96be\u4ee5\u9002\u5e94\u65b0\u7684\u4efb\u52a1\u9700\u6c42\uff0c\u901a\u5e38\u9700\u8981\u6602\u8d35\u7684\u6570\u636e\u91cd\u65b0\u6536\u96c6\u548c\u7b56\u7565\u91cd\u65b0\u8bad\u7ec3", "method": "\u91c7\u7528\u9002\u5e94-\u6295\u5f71\u7b56\u7565\uff1a\u9996\u5148\u5728SE(3)\u7a7a\u95f4\u4e2d\u8bad\u7ec3\u57fa\u7840\u6269\u6563\u7b56\u7565\uff0c\u7136\u540e\u5728\u5728\u7ebf\u90e8\u7f72\u65f6\u901a\u8fc7\u6295\u5f71\u5c06\u751f\u6210\u7684\u8f68\u8ff9\u8c03\u6574\u4ee5\u6ee1\u8db3\u65b0\u786c\u4ef6\u548c\u4efb\u52a1\u7684\u8fd0\u52a8\u5b66\u548c\u4efb\u52a1\u7279\u5b9a\u7ea6\u675f", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u62fe\u53d6\u653e\u7f6e\u3001\u63a8\u52a8\u548c\u503e\u5012\u4efb\u52a1\u4e2d\uff0c\u8de8\u591a\u4e2a\u673a\u68b0\u81c2\uff08Franka Panda\u548cKuka iiwa 14\uff09\u548c\u591a\u79cd\u672b\u7aef\u6267\u884c\u5668\u4e0a\u5b9e\u73b0\u4e86\u6301\u7eed\u9ad8\u6210\u529f\u7387", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u9002\u5e94-\u6295\u5f71\u7b56\u7565\u5728\u8de8\u673a\u68b0\u81c2\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u96f6\u6837\u672c\u9002\u5e94\u800c\u4e0d\u9700\u8981\u91cd\u65b0\u8bad\u7ec3"}}
{"id": "2509.11663", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.11663", "abs": "https://arxiv.org/abs/2509.11663", "authors": ["Haisheng Wang", "Weiming Zhi"], "title": "ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and Answering", "comment": "8 pages, 6 figures, 2026 IEEE Conference on Robotics and Automation\n  (ICRA 2026)", "summary": "This paper formulates the Embodied Questions Answering (EQsA) problem,\nintroduces a corresponding benchmark, and proposes a system to tackle the\nproblem. Classical Embodied Question Answering (EQA) is typically formulated as\nanswering one single question by actively exploring a 3D environment. Real\ndeployments, however, often demand handling multiple questions that may arrive\nasynchronously and carry different urgencies. We formalize this setting as\nEmbodied Questions Answering (EQsA) and present ParaEQsA, a framework for\nparallel, urgency-aware scheduling and answering. ParaEQsA leverages a group\nmemory module shared among questions to reduce redundant exploration, and a\npriority-planning module to dynamically schedule questions. To evaluate this\nsetting, we contribute the Parallel Asynchronous Embodied Questions (PAEQs)\nbenchmark containing 40 indoor scenes and five questions per scene (200 in\ntotal), featuring asynchronous follow-up questions and urgency labels. We\nfurther propose metrics for EQsA performance: Direct Answer Rate (DAR), and\nNormalized Urgency-Weighted Latency (NUWL), which jointly measure efficiency\nand responsiveness of this system. ParaEQsA consistently outperforms strong\nsequential baselines adapted from recent EQA systems, while reducing\nexploration and delay. Empirical evaluations investigate the relative\ncontributions of priority, urgency modeling, spatial scope, reward estimation,\nand dependency reasoning within our framework. Together, these results\ndemonstrate that urgency-aware, parallel scheduling is key to making embodied\nagents responsive and efficient under realistic, multi-question workloads.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u5e76\u884c\u4f53\u9a8c\u95ee\u9898\u7b54\u6848\uff08EQsA\uff09\u95ee\u9898\uff0c\u5efa\u7acb\u4e86PAEQs\u6807\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86ParaEQsA\u7cfb\u7edf\u6765\u5b9e\u73b0\u7d27\u6025\u6027\u611f\u77e5\u548c\u5e76\u884c\u8c03\u5ea6\uff0c\u5728\u591a\u95ee\u9898\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u54cd\u5e94\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u4f53\u9a8c\u95ee\u9898\u7b54\u6848\uff08EQA\uff09\u901a\u5e38\u53ea\u5904\u7406\u5355\u4e2a\u95ee\u9898\uff0c\u800c\u5b9e\u9645\u90e8\u7f72\u5e38\u9700\u8981\u540c\u65f6\u5904\u7406\u591a\u4e2a\u53ef\u80fd\u5f02\u6b65\u5230\u6765\u4e14\u5177\u6709\u4e0d\u540c\u7d27\u6025\u7a0b\u5ea6\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faParaEQsA\u6846\u67b6\uff0c\u5305\u542b\u4e00\u4e2a\u5171\u4eab\u7684\u7ec4\u5185\u5b58\u6a21\u5757\u6765\u51cf\u5c11\u91cd\u590d\u63a2\u7d22\uff0c\u4ee5\u53ca\u4e00\u4e2a\u4f18\u5148\u7ea7\u89c4\u5212\u6a21\u5757\u6765\u52a8\u6001\u8c03\u5ea6\u95ee\u9898\u3002\u5efa\u7acb\u4e86PAEQs\u6807\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b40\u4e2a\u5ba4\u5185\u573a\u666f\u548c\u6bcf\u4e2a\u573a\u666f5\u4e2a\u95ee\u9898\uff0c\u5177\u6709\u5f02\u6b65\u8ddf\u8fdb\u95ee\u9898\u548c\u7d27\u6025\u6807\u7b7e\u3002", "result": "ParaEQsA\u5728\u6548\u7387\u548c\u54cd\u5e94\u80fd\u529b\u65b9\u9762\u4e00\u8d74\u8d85\u8fc7\u4e86\u6765\u81ea\u6700\u65b0EQA\u7cfb\u7edf\u7684\u5f3a\u5927\u987a\u5e8f\u57fa\u7ebf\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u63a2\u7d22\u548c\u5ef6\u8fdf\u3002\u5b9e\u9a8c\u8bc4\u4f30\u8c03\u67e5\u4e86\u4f18\u5148\u7ea7\u3001\u7d27\u6025\u6025\u6025\u6025\u6a21\u578b\u3001\u7a7a\u95f4\u8303\u56f4\u3001\u5956\u52b1\u4f30\u8ba1\u548c\u4f9d\u8d56\u6027\u63a8\u7406\u5728\u6846\u67b6\u5185\u7684\u76f8\u5bf9\u8d21\u732e\u3002", "conclusion": "\u7d27\u6025\u6025\u6025\u6025\u611f\u77e5\u548c\u5e76\u884c\u8c03\u5ea6\u662f\u4f7f\u4f53\u9a8c\u4ee3\u7406\u5728\u73b0\u5b9e\u591a\u95ee\u9898\u5de5\u4f5c\u8d1f\u8377\u4e0b\u5177\u6709\u54cd\u5e94\u80fd\u529b\u548c\u9ad8\u6548\u6027\u7684\u5173\u952e\u3002"}}
{"id": "2509.11688", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.11688", "abs": "https://arxiv.org/abs/2509.11688", "authors": ["Mostafa Eslami", "Maryam Babazadeh"], "title": "Tensor Invariant Data-Assisted Control and Dynamic Decomposition of Multibody Systems", "comment": null, "summary": "The control of robotic systems in complex, shared collaborative workspaces\npresents significant challenges in achieving robust performance and safety when\nlearning from experienced or simulated data is employed in the pipeline. A\nprimary bottleneck is the reliance on coordinate-dependent models, which leads\nto profound data inefficiency by failing to generalize physical interactions\nacross different frames of reference. This forces learning algorithms to\nrediscover fundamental physical principles in every new orientation,\nartificially inflating the complexity of the learning task. This paper\nintroduces a novel framework that synergizes a coordinate-free, unreduced\nmultibody dynamics and kinematics model based on tensor mechanics with a\nData-Assisted Control (DAC) architecture. A non-recursive, closed-form\nNewton-Euler model in an augmented matrix form is derived that is optimized for\ntensor-based control design. This structure enables a principled decomposition\nof the system into a structurally certain, physically grounded part and an\nuncertain, empirical, and interaction-focused part, mediated by a virtual port\nvariable. Then, a complete, end-to-end tensor-invariant pipeline for modeling,\ncontrol, and learning is proposed. The coordinate-free control laws for the\nstructurally certain part provide a stable and abstract command interface,\nproven via Lyapunov analysis. Eventually, the model and closed-loop system are\nvalidated through simulations. This work provides a naturally ideal input for\ndata-efficient, frame-invariant learning algorithms, such as equivariant\nlearning, designed to learn the uncertain interaction. The synergy directly\naddresses the data-inefficiency problem, increases explainability and\ninterpretability, and paves the way for more robust and generalizable robotic\ncontrol in interactive environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f20\u91cf\u529b\u5b66\u7684\u5750\u6807\u65e0\u5173\u591a\u4f53\u52a8\u529b\u5b66\u6a21\u578b\u4e0e\u6570\u636e\u8f85\u52a9\u63a7\u5236\u76f8\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u534f\u4f5c\u7a7a\u95f4\u4e2d\u5b66\u4e60\u7684\u6570\u636e\u6548\u7387\u4f4e\u4e0b\u548c\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5750\u6807\u4f9d\u8d56\u6a21\u578b\u5bfc\u81f4\u6570\u636e\u6548\u7387\u4f4e\u4e0b\uff0c\u65e0\u6cd5\u5728\u4e0d\u540c\u53c2\u8003\u7cfb\u95f4\u6cdb\u5316\u7269\u7406\u4ea4\u4e92\uff0c\u8feb\u4f7f\u5b66\u4e60\u7b97\u6cd5\u5728\u6bcf\u4e2a\u65b0\u65b9\u5411\u91cd\u65b0\u53d1\u73b0\u7269\u7406\u539f\u7406\uff0c\u4eba\u4e3a\u589e\u52a0\u4e86\u5b66\u4e60\u590d\u6742\u5ea6\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u5f20\u91cf\u529b\u5b66\u7684\u5750\u6807\u65e0\u5173\u975e\u7b80\u5316\u591a\u4f53\u52a8\u529b\u5b66\u548c\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u7ed3\u5408\u6570\u636e\u8f85\u52a9\u63a7\u5236\u67b6\u6784\uff0c\u4f7f\u7528\u589e\u5f3a\u77e9\u9635\u5f62\u5f0f\u7684\u975e\u9012\u5f52\u95ed\u5f0f\u725b\u987f-\u6b27\u62c9\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u674e\u96c5\u666e\u8bfa\u592b\u5206\u6790\u8bc1\u660e\u7a33\u5b9a\u6027\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u6a21\u578b\u548c\u95ed\u73af\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6570\u636e\u9ad8\u6548\u7684\u6846\u67b6\u4e0d\u53d8\u5b66\u4e60\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u60f3\u8f93\u5165\uff0c\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u76f4\u63a5\u89e3\u51b3\u4e86\u6570\u636e\u6548\u7387\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u4ea4\u4e92\u73af\u5883\u4e2d\u66f4\u9c81\u68d2\u548c\u53ef\u6cdb\u5316\u7684\u673a\u5668\u4eba\u63a7\u5236\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.11740", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.11740", "abs": "https://arxiv.org/abs/2509.11740", "authors": ["Davide Peron", "Victor Nan Fernandez-Ayala", "Lukas Segelmark"], "title": "From Pixels to Shelf: End-to-End Algorithmic Control of a Mobile Manipulator for Supermarket Stocking and Fronting", "comment": "Submitted for publication at IEEE ICRA 2026", "summary": "Autonomous stocking in retail environments, particularly supermarkets,\npresents challenges due to dynamic human interactions, constrained spaces, and\ndiverse product geometries. This paper introduces an efficient end-to-end\nrobotic system for autonomous shelf stocking and fronting, integrating\ncommercially available hardware with a scalable algorithmic architecture. A\nmajor contribution of this work is the system integration of off-the-shelf\nhardware and ROS2-based perception, planning, and control into a single\ndeployable platform for retail environments. Our solution leverages Behavior\nTrees (BTs) for task planning, fine-tuned vision models for object detection,\nand a two-step Model Predictive Control (MPC) framework for precise shelf\nnavigation using ArUco markers. Laboratory experiments replicating realistic\nsupermarket conditions demonstrate reliable performance, achieving over 98%\nsuccess in pick-and-place operations across a total of more than 700 stocking\nevents. However, our comparative benchmarks indicate that the performance and\ncost-effectiveness of current autonomous systems remain inferior to that of\nhuman workers, which we use to highlight key improvement areas and quantify the\nprogress still required before widespread commercial deployment can\nrealistically be achieved.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8d85\u5e02\u81ea\u4e3b\u67b6\u4e0a\u8865\u8d27\u7684\u7aef\u5230\u7aef\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u96c6\u6210\u4e86\u5546\u7528\u786c\u4ef6\u548c\u9ad8\u6548\u7b97\u6cd5\uff0c\u5728\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\u8fbe\u5230\u4e86\u8d85\u8fc798%\u7684\u6210\u529f\u7387\uff0c\u4f46\u4ecd\u8f83\u4eba\u7c7b\u5de5\u4f5c\u8005\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u8d85\u5e02\u81ea\u4e3b\u8865\u8d27\u9762\u4e34\u52a8\u6001\u4eba\u673a\u4ea4\u4e92\u3001\u7a7a\u95f4\u7ea6\u675f\u548c\u591a\u6837\u5316\u5546\u54c1\u5f62\u72b6\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u53ef\u90e8\u7f72\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "method": "\u96c6\u6210\u5546\u7528\u786c\u4ef6\u548cROS2\u57fa\u7840\u7684\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\u7cfb\u7edf\uff0c\u91c7\u7528\u884c\u4e3a\u6811\u8fdb\u884c\u4efb\u52a1\u89c4\u5212\uff0c\u7cbe\u7ec6\u8c03\u6574\u7684\u89c6\u89c9\u6a21\u578b\u8fdb\u884c\u7269\u4f53\u68c0\u6d4b\uff0c\u4ee5\u53ca\u57fa\u4e8eArUco\u6807\u8bb0\u7684\u4e24\u6b65\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u8fdb\u884c\u7cbe\u786e\u67b6\u4e0a\u5bfc\u822a\u3002", "result": "\u5728\u6a21\u62df\u73b0\u5b9e\u8d85\u5e02\u6761\u4ef6\u7684\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\uff0c\u7cfb\u7edf\u5728\u8d85\u8fc7700\u6b21\u8865\u8d27\u4e8b\u4ef6\u4e2d\u8fbe\u5230\u4e86\u8d85\u8fc798%\u7684\u6458\u653e\u64cd\u4f5c\u6210\u529f\u7387\u3002", "conclusion": "\u5f53\u524d\u81ea\u4e3b\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u6210\u672c\u6548\u76ca\u4ecd\u4f4e\u4e8e\u4eba\u7c7b\u5de5\u4f5c\u8005\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u624d\u80fd\u5b9e\u73b0\u5927\u89c4\u6a21\u5546\u4e1a\u90e8\u7f72\u3002"}}
{"id": "2509.11742", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.11742", "abs": "https://arxiv.org/abs/2509.11742", "authors": ["Jianping Li", "Kaisong Zhu", "Zhongyuan Liu", "Rui Jin", "Xinhang Xu", "Pengfei Wan", "Lihua Xie"], "title": "Adaptive Motorized LiDAR Scanning Control for Robust Localization with OpenStreetMap", "comment": null, "summary": "LiDAR-to-OpenStreetMap (OSM) localization has gained increasing attention, as\nOSM provides lightweight global priors such as building footprints. These\npriors enhance global consistency for robot navigation, but OSM is often\nincomplete or outdated, limiting its reliability in real-world deployment.\nMeanwhile, LiDAR itself suffers from a limited field of view (FoV), where\nmotorized rotation is commonly used to achieve panoramic coverage. Existing\nmotorized LiDAR systems, however, typically employ constant-speed scanning that\ndisregards both scene structure and map priors, leading to wasted effort in\nfeature-sparse regions and degraded localization accuracy. To address these\nchallenges, we propose Adaptive LiDAR Scanning with OSM guidance, a framework\nthat integrates global priors with local observability prediction to improve\nlocalization robustness. Specifically, we augment uncertainty-aware model\npredictive control with an OSM-aware term that adaptively allocates scanning\neffort according to both scene-dependent observability and the spatial\ndistribution of OSM features. The method is implemented in ROS with a motorized\nLiDAR odometry backend and evaluated in both simulation and real-world\nexperiments. Results on campus roads, indoor corridors, and urban environments\ndemonstrate significant reductions in trajectory error compared to\nconstant-speed baselines, while maintaining scan completeness. These findings\nhighlight the potential of coupling open-source maps with adaptive LiDAR\nscanning to achieve robust and efficient localization in complex environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eOpenStreetMap\u5f15\u5bfc\u7684\u81ea\u9002\u5e94LiDAR\u626b\u63cf\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u5730\u56fe\u5148\u9a8c\u548c\u5c40\u90e8\u53ef\u89c2\u6d4b\u6027\u9884\u6d4b\u6765\u63d0\u5347\u5b9a\u4f4d\u9c81\u68d2\u6027\uff0c\u76f8\u6bd4\u6052\u5b9a\u901f\u5ea6\u626b\u63cf\u663e\u8457\u964d\u4f4e\u4e86\u8f68\u8ff9\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709LiDAR-to-OSM\u5b9a\u4f4d\u65b9\u6cd5\u9762\u4e34OSM\u6570\u636e\u4e0d\u5b8c\u6574\u6216\u8fc7\u65f6\u7684\u95ee\u9898\uff0c\u540c\u65f6\u6052\u5b9a\u901f\u5ea6\u626b\u63cf\u7684LiDAR\u7cfb\u7edf\u5728\u7279\u5f81\u7a00\u758f\u533a\u57df\u6d6a\u8d39\u626b\u63cf\u8d44\u6e90\uff0c\u5bfc\u81f4\u5b9a\u4f4d\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u589e\u52a0OSM\u611f\u77e5\u9879\uff0c\u6839\u636e\u573a\u666f\u4f9d\u8d56\u7684\u53ef\u89c2\u6d4b\u6027\u548cOSM\u7279\u5f81\u7a7a\u95f4\u5206\u5e03\u81ea\u9002\u5e94\u5206\u914d\u626b\u63cf\u8d44\u6e90\uff0c\u5728ROS\u4e2d\u5b9e\u73b0\u7535\u673a\u5316LiDAR\u91cc\u7a0b\u8ba1\u540e\u7aef\u3002", "result": "\u5728\u6821\u56ed\u9053\u8def\u3001\u5ba4\u5185\u8d70\u5eca\u548c\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u6052\u5b9a\u901f\u5ea6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8f68\u8ff9\u8bef\u5dee\u663e\u8457\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u626b\u63cf\u5b8c\u6574\u6027\u3002", "conclusion": "\u5c06\u5f00\u6e90\u5730\u56fe\u4e0e\u81ea\u9002\u5e94LiDAR\u626b\u63cf\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u9ad8\u6548\u7684\u5b9a\u4f4d\u3002"}}
{"id": "2509.11766", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.11766", "abs": "https://arxiv.org/abs/2509.11766", "authors": ["Andy Zhai", "Brae Liu", "Bruno Fang", "Chalse Cai", "Ellie Ma", "Ethan Yin", "Hao Wang", "Hugo Zhou", "James Wang", "Lights Shi", "Lucy Liang", "Make Wang", "Qian Wang", "Roy Gan", "Ryan Yu", "Shalfun Li", "Starrick Liu", "Sylas Chen", "Vincent Chen", "Zach Xu"], "title": "Igniting VLMs toward the Embodied Space", "comment": null, "summary": "While foundation models show remarkable progress in language and vision,\nexisting vision-language models (VLMs) still have limited spatial and\nembodiment understanding. Transferring VLMs to embodied domains reveals\nfundamental mismatches between modalities, pretraining distributions, and\ntraining objectives, leaving action comprehension and generation as a central\nbottleneck on the path to AGI.\n  We introduce WALL-OSS, an end-to-end embodied foundation model that leverages\nlarge-scale multimodal pretraining to achieve (1) embodiment-aware\nvision-language understanding, (2) strong language-action association, and (3)\nrobust manipulation capability.\n  Our approach employs a tightly coupled architecture and multi-strategies\ntraining curriculum that enables Unified Cross-Level CoT-seamlessly unifying\ninstruction reasoning, subgoal decomposition, and fine-grained action synthesis\nwithin a single differentiable framework.\n  Our results show that WALL-OSS attains high success on complex long-horizon\nmanipulations, demonstrates strong instruction-following capabilities, complex\nunderstanding and reasoning, and outperforms strong baselines, thereby\nproviding a reliable and scalable path from VLMs to embodied foundation models.", "AI": {"tldr": "WALL-OSS\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5177\u8eab\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u5b9e\u73b0\u5177\u8eab\u611f\u77e5\u7684\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u3001\u5f3a\u5927\u7684\u8bed\u8a00-\u52a8\u4f5c\u5173\u8054\u548c\u9c81\u68d2\u7684\u64cd\u63a7\u80fd\u529b\uff0c\u5728\u590d\u6742\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u548c\u5177\u8eab\u7406\u89e3\u65b9\u9762\u6709\u9650\uff0c\u8f6c\u79fb\u5230\u5177\u8eab\u9886\u57df\u5b58\u5728\u6a21\u6001\u3001\u9884\u8bad\u7ec3\u5206\u5e03\u548c\u8bad\u7ec3\u76ee\u6807\u7684\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u52a8\u4f5c\u7406\u89e3\u548c\u751f\u6210\u6210\u4e3aAGI\u8def\u5f84\u4e0a\u7684\u6838\u5fc3\u74f6\u9888\u3002", "method": "\u91c7\u7528\u7d27\u5bc6\u8026\u5408\u7684\u67b6\u6784\u548c\u591a\u7b56\u7565\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u901a\u8fc7\u7edf\u4e00\u8de8\u7ea7CoT\uff08\u601d\u7ef4\u94fe\uff09\u65b9\u6cd5\uff0c\u5728\u5355\u4e00\u53ef\u5fae\u5206\u6846\u67b6\u5185\u65e0\u7f1d\u7edf\u4e00\u6307\u4ee4\u63a8\u7406\u3001\u5b50\u76ee\u6807\u5206\u89e3\u548c\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u5408\u6210\u3002", "result": "WALL-OSS\u5728\u590d\u6742\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e0a\u53d6\u5f97\u9ad8\u6210\u529f\u7387\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3001\u590d\u6742\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u4ece\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5230\u5177\u8eab\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u5177\u8eab\u667a\u80fd\u4e2d\u7684\u6838\u5fc3\u6311\u6218\u3002"}}
{"id": "2509.11783", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.11783", "abs": "https://arxiv.org/abs/2509.11783", "authors": ["Shiqi Gong", "Sebastian Zudaire", "Chi Zhang", "Zhen Li"], "title": "Augmented Reality-Enhanced Robot Teleoperation for Collecting User Demonstrations", "comment": "Accepted by 2025 8th International Conference on Robotics, Control\n  and Automation Engineering (RCAE 2025)", "summary": "Traditional industrial robot programming is often complex and time-consuming,\ntypically requiring weeks or even months of effort from expert programmers.\nAlthough Programming by Demonstration (PbD) offers a more accessible\nalternative, intuitive interfaces for robot control and demonstration\ncollection remain challenging. To address this, we propose an Augmented Reality\n(AR)-enhanced robot teleoperation system that integrates AR-based control with\nspatial point cloud rendering, enabling intuitive, contact-free demonstrations.\nThis approach allows operators to control robots remotely without entering the\nworkspace or using conventional tools like the teach pendant. The proposed\nsystem is generally applicable and has been demonstrated on ABB robot\nplatforms, specifically validated with the IRB 1200 industrial robot and the\nGoFa 5 collaborative robot. A user study evaluates the impact of real-time\nenvironmental perception, specifically with and without point cloud rendering,\non task completion accuracy, efficiency, and user confidence. Results indicate\nthat enhanced perception significantly improves task performance by 28% and\nenhances user experience, as reflected by a 12% increase in the System\nUsability Scale (SUS) score. This work contributes to the advancement of\nintuitive robot teleoperation, AR interface design, environmental perception,\nand teleoperation safety mechanisms in industrial settings for demonstration\ncollection. The collected demonstrations may serve as valuable training data\nfor machine learning applications.", "AI": {"tldr": "\u57fa\u4e8e\u589e\u5f3a\u73b0\u5b9e\u7684\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u7a7a\u95f4\u70b9\u4e91\u6e32\u67d3\u5b9e\u73b0\u76f4\u89c2\u7684\u65e0\u63a5\u89e6\u793a\u8303\u6536\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5de5\u4e1a\u673a\u5668\u4eba\u7f16\u7a0b\u590d\u6742\u8017\u65f6\u7684\u95ee\u9898\uff0c\u5c0f\u5316\u7a0b\u5e8f\u7f16\u5199\u4e2d\u7684\u76f4\u89c2\u63a7\u5236\u548c\u793a\u8303\u6536\u96c6\u63a5\u53e3\u6311\u6218", "method": "\u5f00\u53d1AR\u589e\u5f3a\u7684\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u96c6\u6210AR\u57fa\u4e8e\u63a7\u5236\u4e0e\u7a7a\u95f4\u70b9\u4e91\u6e32\u67d3\u6280\u672f\uff0c\u652f\u6301\u65e0\u9700\u8fdb\u5165\u5de5\u4f5c\u533a\u6216\u4f7f\u7528\u4f20\u7edf\u6559\u5b66\u5782\u7684\u8fdc\u7a0b\u64cd\u4f5c", "result": "\u5b9e\u65f6\u73af\u5883\u611f\u77e5\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u5b8c\u6210\u51c6\u786e\u6027\u548c\u6548\u738728%\uff0c\u7528\u6237\u4f53\u9a8c\u63d0\u534712%\uff08SUS\u8bc4\u5206\uff09\uff0c\u5728ABB IRB 1200\u548cGoFa 5\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u6210\u529f", "conclusion": "\u8be5\u7cfb\u7edf\u63a8\u8fdb\u4e86\u76f4\u89c2\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u6280\u672f\uff0c\u4e3a\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u793a\u8303\u6536\u96c6\u63d0\u4f9b\u4e86\u5b89\u5168\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6536\u96c6\u7684\u793a\u8303\u6570\u636e\u53ef\u4f5c\u4e3a\u673a\u5668\u5b66\u4e60\u5e94\u7528\u7684\u4ef7\u503c\u8d44\u6e90"}}
{"id": "2509.11791", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11791", "abs": "https://arxiv.org/abs/2509.11791", "authors": ["Lauri Suomela", "Sasanka Kuruppu Arachchige", "German F. Torres", "Harry Edelman", "Joni-Kristian K\u00e4m\u00e4r\u00e4inen"], "title": "Synthetic vs. Real Training Data for Visual Navigation", "comment": "Presented at CoRL 2025 workshop on \"Making Sense of Data in Robotics\"", "summary": "This paper investigates how the performance of visual navigation policies\ntrained in simulation compares to policies trained with real-world data.\nPerformance degradation of simulator-trained policies is often significant when\nthey are evaluated in the real world. However, despite this well-known\nsim-to-real gap, we demonstrate that simulator-trained policies can match the\nperformance of their real-world-trained counterparts.\n  Central to our approach is a navigation policy architecture that bridges the\nsim-to-real appearance gap by leveraging pretrained visual representations and\nruns real-time on robot hardware. Evaluations on a wheeled mobile robot show\nthat the proposed policy, when trained in simulation, outperforms its\nreal-world-trained version by 31% and the prior state-of-the-art methods by 50%\nin navigation success rate. Policy generalization is verified by deploying the\nsame model onboard a drone.\n  Our results highlight the importance of diverse image encoder pretraining for\nsim-to-real generalization, and identify on-policy learning as a key advantage\nof simulated training over training with real data.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3\u7684\u89c6\u89c9\u5bfc\u822a\u7b56\u7565\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8868\u793a\u548c\u5b9e\u65f6\u8fd0\u884c\u67b6\u6784\uff0c\u80fd\u591f\u8d85\u8d8a\u771f\u5b9e\u4e16\u754c\u8bad\u7ec3\u7b56\u756531%\u7684\u6027\u80fd\uff0c\u5e76\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u9ad850%\u7684\u5bfc\u822a\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u6a21\u62df\u8bad\u7ec3\u7b56\u7565\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\u7684sim-to-real\u5dee\u8ddd\u95ee\u9898\uff0c\u8bc1\u660e\u6a21\u62df\u8bad\u7ec3\u7b56\u7565\u53ef\u4ee5\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u771f\u5b9e\u4e16\u754c\u8bad\u7ec3\u7b56\u7565\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u80fd\u591f\u6865\u63a5sim-to-real\u5916\u89c2\u5dee\u8ddd\u7684\u5bfc\u822a\u7b56\u7565\u67b6\u6784\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8868\u793a\uff0c\u5e76\u5728\u673a\u5668\u4eba\u786c\u4ef6\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002\u5728\u8f6e\u5f0f\u79fb\u52a8\u673a\u5668\u4eba\u548c\u65e0\u4eba\u673a\u4e0a\u8fdb\u884c\u90e8\u7f72\u9a8c\u8bc1\u3002", "result": "\u6a21\u62df\u8bad\u7ec3\u7684\u7b56\u7565\u6bd4\u771f\u5b9e\u4e16\u754c\u8bad\u7ec3\u7248\u672c\u6027\u80fd\u9ad8\u51fa31%\uff0c\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u9ad850%\u7684\u5bfc\u822a\u6210\u529f\u7387\u3002\u540c\u4e00\u6a21\u578b\u5728\u65e0\u4eba\u673a\u4e0a\u4e5f\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u591a\u6837\u5316\u7684\u56fe\u50cf\u7f16\u7801\u5668\u9884\u8bad\u7ec3\u5bf9sim-to-real\u6cdb\u5316\u81f3\u5173\u91cd\u8981\uff0c\u6a21\u62df\u8bad\u7ec3\u7684on-policy\u5b66\u4e60\u76f8\u6bd4\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u5177\u6709\u5173\u952e\u4f18\u52bf\u3002"}}
{"id": "2509.11793", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.11793", "abs": "https://arxiv.org/abs/2509.11793", "authors": ["Mihir Kulkarni", "Mihir Dharmadhikari", "Nikhil Khedekar", "Morten Nissov", "Mohit Singh", "Philipp Weiss", "Kostas Alexis"], "title": "UniPilot: Enabling GPS-Denied Autonomy Across Embodiments", "comment": null, "summary": "This paper presents UniPilot, a compact hardware-software autonomy payload\nthat can be integrated across diverse robot embodiments to enable autonomous\noperation in GPS-denied environments. The system integrates a multi-modal\nsensing suite including LiDAR, radar, vision, and inertial sensing for robust\noperation in conditions where uni-modal approaches may fail. UniPilot runs a\ncomplete autonomy software comprising multi-modal perception, exploration and\ninspection path planning, and learning-based navigation policies. The payload\nprovides robust localization, mapping, planning, and safety and control\ncapabilities in a single unit that can be deployed across a wide range of\nplatforms. A large number of experiments are conducted across diverse\nenvironments and on a variety of robot platforms to validate the mapping,\nplanning, and safe navigation capabilities enabled by the payload.", "AI": {"tldr": "UniPilot\u662f\u4e00\u4e2a\u7d27\u51d1\u7684\u786c\u4ef6-\u8f6f\u4ef6\u81ea\u4e3b\u8f7d\u8377\u7cfb\u7edf\uff0c\u53ef\u5728GPS\u62d2\u6b62\u73af\u5883\u4e2d\u4e3a\u591a\u79cd\u673a\u5668\u4eba\u63d0\u4f9b\u81ea\u4e3b\u64cd\u4f5c\u80fd\u529b\uff0c\u96c6\u6210\u4e86\u591a\u6a21\u6001\u611f\u77e5\u3001\u8def\u5f84\u89c4\u5212\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u5bfc\u822a\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3\u5728GPS\u62d2\u6b62\u73af\u5883\u4e0b\u5355\u4e00\u6a21\u6001\u611f\u77e5\u65b9\u6cd5\u5bb9\u6613\u5931\u6548\u7684\u95ee\u9898\uff0c\u4e3a\u591a\u6837\u5316\u673a\u5668\u4eba\u5e73\u53f0\u63d0\u4f9b\u7edf\u4e00\u7684\u81ea\u4e3b\u64cd\u4f5c\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u96c6\u6210LiDAR\u3001\u96f7\u8fbe\u3001\u89c6\u89c9\u548c\u60ef\u6027\u4f20\u611f\u7684\u591a\u6a21\u6001\u611f\u77e5\u5957\u4ef6\uff0c\u8fd0\u884c\u5b8c\u6574\u7684\u81ea\u4e3b\u8f6f\u4ef6\u5305\u62ec\u591a\u6a21\u6001\u611f\u77e5\u3001\u63a2\u7d22\u68c0\u67e5\u8def\u5f84\u89c4\u5212\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u5bfc\u822a\u7b56\u7565\u3002", "result": "\u5728\u591a\u6837\u5316\u73af\u5883\u548c\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u5efa\u56fe\u3001\u89c4\u5212\u548c\u5b89\u5168\u5bfc\u822a\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "UniPilot\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u5728\u5e7f\u6cdb\u5e73\u53f0\u4e0a\u90e8\u7f72\u7684\u5355\u4e00\u5355\u5143\uff0c\u5177\u5907\u9c81\u68d2\u7684\u5b9a\u4f4d\u3001\u5efa\u56fe\u3001\u89c4\u5212\u4ee5\u53ca\u5b89\u5168\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2509.11839", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.11839", "abs": "https://arxiv.org/abs/2509.11839", "authors": ["Jiacheng Liu", "Pengxiang Ding", "Qihang Zhou", "Yuxuan Wu", "Da Huang", "Zimian Peng", "Wei Xiao", "Weinan Zhang", "Lixin Yang", "Cewu Lu", "Donglin Wang"], "title": "TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning", "comment": null, "summary": "Imitation learning (IL) enables efficient skill acquisition from\ndemonstrations but often struggles with long-horizon tasks and high-precision\ncontrol due to compounding errors. Residual policy learning offers a promising,\nmodel-agnostic solution by refining a base policy through closed-loop\ncorrections. However, existing approaches primarily focus on local corrections\nto the base policy, lacking a global understanding of state evolution, which\nlimits robustness and generalization to unseen scenarios. To address this, we\npropose incorporating global dynamics modeling to guide residual policy\nupdates. Specifically, we leverage Koopman operator theory to impose linear\ntime-invariant structure in a learned latent space, enabling reliable state\ntransitions and improved extrapolation for long-horizon prediction and unseen\nenvironments. We introduce KORR (Koopman-guided Online Residual Refinement), a\nsimple yet effective framework that conditions residual corrections on\nKoopman-predicted latent states, enabling globally informed and stable action\nrefinement. We evaluate KORR on long-horizon, fine-grained robotic furniture\nassembly tasks under various perturbations. Results demonstrate consistent\ngains in performance, robustness, and generalization over strong baselines. Our\nfindings further highlight the potential of Koopman-based modeling to bridge\nmodern learning methods with classical control theory. For more details, please\nrefer to https://jiachengliu3.github.io/TrajBooster.", "AI": {"tldr": "\u63d0\u51fa\u4e86KORR\u6846\u67b6\uff0c\u901a\u8fc7Koopman\u7b97\u5b50\u7406\u8bba\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5efa\u7acb\u7ebf\u6027\u65f6\u4e0d\u53d8\u7ed3\u6784\uff0c\u4e3a\u6b8b\u5dee\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u5168\u5c40\u52a8\u6001\u5efa\u6a21\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u65f6\u7a0b\u7cbe\u7ec6\u63a7\u5236\u4efb\u52a1\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u548c\u9ad8\u7cbe\u5ea6\u63a7\u5236\u4e2d\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u73b0\u6709\u6b8b\u5dee\u7b56\u7565\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5c40\u90e8\u4fee\u6b63\uff0c\u7f3a\u4e4f\u5bf9\u72b6\u6001\u6f14\u5316\u7684\u5168\u5c40\u7406\u89e3\uff0c\u9650\u5236\u4e86\u5728\u672a\u89c1\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faKORR\u6846\u67b6\uff0c\u5229\u7528Koopman\u7b97\u5b50\u7406\u8bba\u5728\u5b66\u4e60\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u65bd\u52a0\u7ebf\u6027\u65f6\u4e0d\u53d8\u7ed3\u6784\uff0c\u4f7f\u6b8b\u5dee\u4fee\u6b63\u57fa\u4e8eKoopman\u9884\u6d4b\u7684\u6f5c\u5728\u72b6\u6001\uff0c\u5b9e\u73b0\u5168\u5c40\u4fe1\u606f\u6307\u5bfc\u7684\u7a33\u5b9a\u52a8\u4f5c\u7cbe\u70bc\u3002", "result": "\u5728\u957f\u65f6\u7a0b\u7cbe\u7ec6\u673a\u5668\u4eba\u5bb6\u5177\u88c5\u914d\u4efb\u52a1\u7684\u5404\u79cd\u6270\u52a8\u4e0b\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u5728\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u90fd\u6709\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u51f8\u663e\u4e86\u57fa\u4e8eKoopman\u7684\u5efa\u6a21\u5728\u8fde\u63a5\u73b0\u4ee3\u5b66\u4e60\u65b9\u6cd5\u4e0e\u7ecf\u5178\u63a7\u5236\u7406\u8bba\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u6b8b\u5dee\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5168\u5c40\u52a8\u6001\u6307\u5bfc\u6846\u67b6\u3002"}}
{"id": "2509.11865", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11865", "abs": "https://arxiv.org/abs/2509.11865", "authors": ["Travis Davies", "Yiqi Huang", "Yunxin Liu", "Xiang Chen", "Huxian Liu", "Luhui Hu"], "title": "Tenma: Robust Cross-Embodiment Robot Manipulation with Diffusion Transformer", "comment": "8 pages, 4 figures", "summary": "Scaling Transformer policies and diffusion models has advanced robotic\nmanipulation, yet combining these techniques in lightweight, cross-embodiment\nlearning settings remains challenging. We study design choices that most affect\nstability and performance for diffusion-transformer policies trained on\nheterogeneous, multimodal robot data, and introduce Tenma, a lightweight\ndiffusion-transformer for bi-manual arm control. Tenma integrates multiview\nRGB, proprioception, and language via a cross-embodiment normalizer that maps\ndisparate state/action spaces into a shared latent space; a Joint State-Time\nencoder for temporally aligned observation learning with inference speed\nboosts; and a diffusion action decoder optimized for training stability and\nlearning capacity. Across benchmarks and under matched compute, Tenma achieves\nan average success rate of 88.95% in-distribution and maintains strong\nperformance under object and scene shifts, substantially exceeding baseline\npolicies whose best in-distribution average is 18.12%. Despite using moderate\ndata scale, Tenma delivers robust manipulation and generalization, indicating\nthe great potential for multimodal and cross-embodiment learning strategies for\nfurther augmenting the capacity of transformer-based imitation learning\npolicies.", "AI": {"tldr": "Tenma\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6269\u6563-Transformer\u7b56\u7565\uff0c\u7528\u4e8e\u53cc\u81c2\u63a7\u5236\uff0c\u901a\u8fc7\u8de8\u5177\u8eab\u6807\u51c6\u5316\u5668\u3001\u8054\u5408\u72b6\u6001-\u65f6\u95f4\u7f16\u7801\u5668\u548c\u6269\u6563\u52a8\u4f5c\u89e3\u7801\u5668\uff0c\u5728\u5f02\u6784\u591a\u6a21\u6001\u673a\u5668\u4eba\u6570\u636e\u4e0a\u5b9e\u73b0\u4e8688.95%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136Transformer\u7b56\u7565\u548c\u6269\u6563\u6a21\u578b\u7684\u6269\u5c55\u5df2\u7ecf\u63a8\u52a8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53d1\u5c55\uff0c\u4f46\u5728\u8f7b\u91cf\u7ea7\u3001\u8de8\u5177\u8eab\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u7ed3\u5408\u8fd9\u4e9b\u6280\u672f\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5728\u5f02\u6784\u591a\u6a21\u6001\u673a\u5668\u4eba\u6570\u636e\u4e0a\u8bad\u7ec3\u6269\u6563-Transformer\u7b56\u7565\u65f6\uff0c\u6700\u5f71\u54cd\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u7684\u8bbe\u8ba1\u9009\u62e9\u3002", "method": "\u63d0\u51faTenma\u6846\u67b6\uff1a1\uff09\u8de8\u5177\u8eab\u6807\u51c6\u5316\u5668\u5c06\u4e0d\u540c\u7684\u72b6\u6001/\u52a8\u4f5c\u7a7a\u95f4\u6620\u5c04\u5230\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff1b2\uff09\u8054\u5408\u72b6\u6001-\u65f6\u95f4\u7f16\u7801\u5668\u5b9e\u73b0\u65f6\u95f4\u5bf9\u9f50\u7684\u89c2\u5bdf\u5b66\u4e60\u5e76\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff1b3\uff09\u6269\u6563\u52a8\u4f5c\u89e3\u7801\u5668\u9488\u5bf9\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u5b66\u4e60\u80fd\u529b\u8fdb\u884c\u4f18\u5316\u3002\u6574\u5408\u591a\u89c6\u89d2RGB\u3001\u672c\u4f53\u611f\u77e5\u548c\u8bed\u8a00\u4fe1\u606f\u3002", "result": "\u5728\u5339\u914d\u8ba1\u7b97\u8d44\u6e90\u4e0b\uff0cTenma\u5728\u5206\u5e03\u5185\u6d4b\u8bd5\u4e2d\u8fbe\u523088.95%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u5728\u7269\u4f53\u548c\u573a\u666f\u53d8\u5316\u4e0b\u4ecd\u4fdd\u6301\u5f3a\u52b2\u6027\u80fd\uff0c\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff08\u6700\u4f73\u5206\u5e03\u5185\u5e73\u5747\u4ec5\u4e3a18.12%\uff09\u3002", "conclusion": "\u5c3d\u7ba1\u4f7f\u7528\u4e2d\u7b49\u89c4\u6a21\u6570\u636e\uff0cTenma\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u64cd\u4f5c\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u8868\u660e\u591a\u6a21\u6001\u548c\u8de8\u5177\u8eab\u5b66\u4e60\u7b56\u7565\u5728\u8fdb\u4e00\u6b65\u589e\u5f3a\u57fa\u4e8eTransformer\u7684\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u80fd\u529b\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.11930", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.11930", "abs": "https://arxiv.org/abs/2509.11930", "authors": ["Ruijia Liu", "Ancheng Hou", "Shaoyuan Li", "Xiang Yin"], "title": "VH-Diffuser: Variable Horizon Diffusion Planner for Time-Aware Goal-Conditioned Trajectory Planning", "comment": null, "summary": "Diffusion-based planners have gained significant recent attention for their\nrobustness and performance in long-horizon tasks. However, most existing\nplanners rely on a fixed, pre-specified horizon during both training and\ninference. This rigidity often produces length-mismatch (trajectories that are\ntoo short or too long) and brittle performance across instances with varying\ngeometric or dynamical difficulty. In this paper, we introduce the Variable\nHorizon Diffuser (VHD) framework, which treats the horizon as a learned\nvariable rather than a fixed hyperparameter. Given a start-goal pair, we first\npredict an instance-specific horizon using a learned Length Predictor model,\nwhich guides a Diffusion Planner to generate a trajectory of the desired\nlength. Our design maintains compatibility with existing diffusion planners by\ncontrolling trajectory length through initial noise shaping and training on\nrandomly cropped sub-trajectories, without requiring architectural changes.\nEmpirically, VHD improves success rates and path efficiency in maze-navigation\nand robot-arm control benchmarks, showing greater robustness to horizon\nmismatch and unseen lengths, while keeping training simple and offline-only.", "AI": {"tldr": "\u63d0\u51fa\u4e86Variable Horizon Diffuser (VHD)\u6846\u67b6\uff0c\u5c06\u89c4\u5212\u65f6\u57df\u4f5c\u4e3a\u5b66\u4e60\u53d8\u91cf\u800c\u975e\u56fa\u5b9a\u8d85\u53c2\u6570\uff0c\u901a\u8fc7\u957f\u5ea6\u9884\u6d4b\u5668\u9884\u6d4b\u5b9e\u4f8b\u7279\u5b9a\u65f6\u57df\uff0c\u63d0\u5347\u6269\u6563\u89c4\u5212\u5668\u5728\u4e0d\u540c\u51e0\u4f55\u548c\u52a8\u529b\u5b66\u96be\u5ea6\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u89c4\u5212\u5668\u4f9d\u8d56\u56fa\u5b9a\u7684\u9884\u5b9a\u4e49\u65f6\u57df\uff0c\u5bfc\u81f4\u957f\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff08\u8f68\u8ff9\u8fc7\u77ed\u6216\u8fc7\u957f\uff09\uff0c\u5728\u4e0d\u540c\u5b9e\u4f8b\u95f4\u8868\u73b0\u8106\u5f31\uff0c\u65e0\u6cd5\u9002\u5e94\u53d8\u5316\u7684\u51e0\u4f55\u6216\u52a8\u529b\u5b66\u96be\u5ea6\u3002", "method": "\u4f7f\u7528\u5b66\u4e60\u7684\u957f\u5ea6\u9884\u6d4b\u5668\u6a21\u578b\u9884\u6d4b\u5b9e\u4f8b\u7279\u5b9a\u65f6\u57df\uff0c\u901a\u8fc7\u521d\u59cb\u566a\u58f0\u6574\u5f62\u548c\u968f\u673a\u88c1\u526a\u5b50\u8f68\u8ff9\u8bad\u7ec3\u6765\u63a7\u5236\u8f68\u8ff9\u957f\u5ea6\uff0c\u65e0\u9700\u67b6\u6784\u4fee\u6539\u5373\u53ef\u4e0e\u73b0\u6709\u6269\u6563\u89c4\u5212\u5668\u517c\u5bb9\u3002", "result": "\u5728\u8ff7\u5bab\u5bfc\u822a\u548c\u673a\u68b0\u81c2\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u8def\u5f84\u6548\u7387\uff0c\u5bf9\u65f6\u57df\u4e0d\u5339\u914d\u548c\u672a\u89c1\u957f\u5ea6\u8868\u73b0\u51fa\u66f4\u5f3a\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u7b80\u5355\u4e14\u4ec5\u9700\u79bb\u7ebf\u8bad\u7ec3\u3002", "conclusion": "VHD\u6846\u67b6\u901a\u8fc7\u5c06\u65f6\u57df\u4f5c\u4e3a\u5b66\u4e60\u53d8\u91cf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fa\u5b9a\u65f6\u57df\u89c4\u5212\u5668\u7684\u957f\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6269\u6563\u89c4\u5212\u5668\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2509.11964", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.11964", "abs": "https://arxiv.org/abs/2509.11964", "authors": ["Junyoung Kim", "Minsik Jeon", "Jihong Min", "Kiho Kwak", "Junwon Seo"], "title": "E2-BKI: Evidential Ellipsoidal Bayesian Kernel Inference for Uncertainty-aware Gaussian Semantic Mapping", "comment": "Our project website can be found at\n  https://kjyoung.github.io/Homepage/#/Projects/E2-BKI", "summary": "Semantic mapping aims to construct a 3D semantic representation of the\nenvironment, providing essential knowledge for robots operating in complex\noutdoor settings. While Bayesian Kernel Inference (BKI) addresses\ndiscontinuities of map inference from sparse sensor data, existing semantic\nmapping methods suffer from various sources of uncertainties in challenging\noutdoor environments. To address these issues, we propose an uncertainty-aware\nsemantic mapping framework that handles multiple sources of uncertainties,\nwhich significantly degrade mapping performance. Our method estimates\nuncertainties in semantic predictions using Evidential Deep Learning and\nincorporates them into BKI for robust semantic inference. It further aggregates\nnoisy observations into coherent Gaussian representations to mitigate the\nimpact of unreliable points, while employing geometry-aligned kernels that\nadapt to complex scene structures. These Gaussian primitives effectively fuse\nlocal geometric and semantic information, enabling robust, uncertainty-aware\nmapping in complex outdoor scenarios. Comprehensive evaluation across diverse\noff-road and urban outdoor environments demonstrates consistent improvements in\nmapping quality, uncertainty calibration, representational flexibility, and\nrobustness, while maintaining real-time efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u8bed\u4e49\u5efa\u56fe\u6846\u67b6\uff0c\u901a\u8fc7\u8bc1\u636e\u6df1\u5ea6\u5b66\u4e60\u4f30\u8ba1\u8bed\u4e49\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u6574\u5408\u5230\u8d1d\u53f6\u65af\u6838\u63a8\u7406\u4e2d\uff0c\u5728\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u8bed\u4e49\u5efa\u56fe\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u5efa\u56fe\u65b9\u6cd5\u5728\u6311\u6218\u6027\u6237\u5916\u73af\u5883\u4e2d\u9762\u4e34\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\uff0c\u4e25\u91cd\u5f71\u54cd\u5efa\u56fe\u6027\u80fd\uff0c\u9700\u8981\u5904\u7406\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\u6765\u63d0\u5347\u5efa\u56fe\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u8bc1\u636e\u6df1\u5ea6\u5b66\u4e60\u4f30\u8ba1\u8bed\u4e49\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u5c06\u5176\u6574\u5408\u5230\u8d1d\u53f6\u65af\u6838\u63a8\u7406\u4e2d\uff1b\u5c06\u566a\u58f0\u89c2\u6d4b\u805a\u5408\u6210\u9ad8\u65af\u8868\u793a\u4ee5\u51cf\u8f7b\u4e0d\u53ef\u9760\u70b9\u7684\u5f71\u54cd\uff1b\u91c7\u7528\u51e0\u4f55\u5bf9\u9f50\u6838\u9002\u5e94\u590d\u6742\u573a\u666f\u7ed3\u6784\u3002", "result": "\u5728\u591a\u79cd\u8d8a\u91ce\u548c\u57ce\u5e02\u6237\u5916\u73af\u5883\u4e2d\u7684\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff0c\u5728\u5efa\u56fe\u8d28\u91cf\u3001\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u3001\u8868\u793a\u7075\u6d3b\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u6709\u4e00\u81f4\u6539\u8fdb\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u5904\u7406\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\uff0c\u5b9e\u73b0\u4e86\u5728\u590d\u6742\u6237\u5916\u573a\u666f\u4e2d\u9c81\u68d2\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u8bed\u4e49\u5efa\u56fe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5efa\u56fe\u6027\u80fd\u3002"}}
{"id": "2509.11971", "categories": ["cs.RO", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11971", "abs": "https://arxiv.org/abs/2509.11971", "authors": ["James C. Ward", "Alex Bott", "Connor York", "Edmund R. Hunt"], "title": "Time-Constrained Intelligent Adversaries for Automation Vulnerability Testing: A Multi-Robot Patrol Case Study", "comment": null, "summary": "Simulating hostile attacks of physical autonomous systems can be a useful\ntool to examine their robustness to attack and inform vulnerability-aware\ndesign. In this work, we examine this through the lens of multi-robot patrol,\nby presenting a machine learning-based adversary model that observes robot\npatrol behavior in order to attempt to gain undetected access to a secure\nenvironment within a limited time duration. Such a model allows for evaluation\nof a patrol system against a realistic potential adversary, offering insight\ninto future patrol strategy design. We show that our new model outperforms\nexisting baselines, thus providing a more stringent test, and examine its\nperformance against multiple leading decentralized multi-robot patrol\nstrategies.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5bf9\u6297\u6a21\u578b\u6765\u8bc4\u4f30\u591a\u673a\u5668\u4eba\u5de1\u903b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u89c2\u5bdf\u5de1\u903b\u884c\u4e3a\u6765\u5c1d\u8bd5\u5728\u9650\u5b9a\u65f6\u95f4\u5185\u65e0\u68c0\u6d4b\u5730\u8fdb\u5165\u5b89\u5168\u533a\u57df\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u901a\u8fc7\u6a21\u62df\u7269\u7406\u81ea\u4e3b\u7cfb\u7edf\u7684\u654c\u5bf9\u653b\u51fb\u6765\u68c0\u9a8c\u5176\u6297\u653b\u51fb\u9c81\u68d2\u6027\uff0c\u4e3a\u6f0f\u6d1e\u611f\u77e5\u8bbe\u8ba1\u63d0\u4f9b\u4fe1\u606f\uff0c\u7279\u522b\u662f\u5728\u591a\u673a\u5668\u4eba\u5de1\u903b\u573a\u666f\u4e2d\u8bc4\u4f30\u5de1\u903b\u7cfb\u7edf\u5bf9\u6297\u73b0\u5b9e\u6f5c\u5728\u5bf9\u624b\u7684\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u5bf9\u6297\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u89c2\u5bdf\u673a\u5668\u4eba\u5de1\u903b\u884c\u4e3a\uff0c\u5728\u6709\u9650\u65f6\u95f4\u7a97\u53e3\u5185\u5c1d\u8bd5\u65e0\u68c0\u6d4b\u5730\u6e17\u900f\u5b89\u5168\u73af\u5883\uff0c\u7528\u4e8e\u8bc4\u4f30\u5de1\u903b\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "result": "\u65b0\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u66f4\u4e25\u683c\u7684\u6d4b\u8bd5\u6807\u51c6\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u5148\u7684\u5206\u6563\u5f0f\u591a\u673a\u5668\u4eba\u5de1\u903b\u7b56\u7565\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u5bf9\u6297\u6a21\u578b\u4e3a\u591a\u673a\u5668\u4eba\u5de1\u903b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u5de5\u5177\uff0c\u80fd\u591f\u4e3a\u672a\u6765\u5de1\u903b\u7b56\u7565\u8bbe\u8ba1\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u7cfb\u7edf\u7684\u6297\u653b\u51fb\u80fd\u529b\u3002"}}
{"id": "2509.12008", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12008", "abs": "https://arxiv.org/abs/2509.12008", "authors": ["Yuqing Song", "Cesare Tonola", "Stefano Savazzi", "Sanaz Kianoush", "Nicola Pedrocchi", "Stephan Sigg"], "title": "Gesture-Based Robot Control Integrating Mm-wave Radar and Behavior Trees", "comment": null, "summary": "As robots become increasingly prevalent in both homes and industrial\nsettings, the demand for intuitive and efficient human-machine interaction\ncontinues to rise. Gesture recognition offers an intuitive control method that\ndoes not require physical contact with devices and can be implemented using\nvarious sensing technologies. Wireless solutions are particularly flexible and\nminimally invasive. While camera-based vision systems are commonly used, they\noften raise privacy concerns and can struggle in complex or poorly lit\nenvironments. In contrast, radar sensing preserves privacy, is robust to\nocclusions and lighting, and provides rich spatial data such as distance,\nrelative velocity, and angle. We present a gesture-controlled robotic arm using\nmm-wave radar for reliable, contactless motion recognition. Nine gestures are\nrecognized and mapped to real-time commands with precision. Case studies are\nconducted to demonstrate the system practicality, performance and reliability\nfor gesture-based robotic manipulation. Unlike prior work that treats gesture\nrecognition and robotic control separately, our system unifies both into a\nreal-time pipeline for seamless, contactless human-robot interaction.", "AI": {"tldr": "\u57fa\u4e8emm\u6ce2\u6fc0\u5149\u96f7\u8fbe\u7684\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\uff0c\u7528\u4e8e\u65e0\u63a5\u89e6\u63a7\u5236\u673a\u5668\u4eba\u624b\u81c2\uff0c\u652f\u63019\u79cd\u624b\u52bf\u5b9e\u65f6\u547d\u4ee4\u6620\u5c04", "motivation": "\u89e3\u51b3\u76f8\u673a\u89c6\u89c9\u7cfb\u7edf\u5b58\u5728\u7684\u9690\u79c1\u6cc4\u9732\u3001\u5149\u7167\u6761\u4ef6\u9650\u5236\u7b49\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u4fdd\u62a4\u9690\u79c1\u3001\u66f4\u7a33\u5b9a\u7684\u65e0\u63a5\u89e6\u4eba\u673a\u4ea4\u4e92\u65b9\u5f0f", "method": "\u91c7\u7528mm\u6ce2\u6fc0\u5149\u96f7\u8fbe\u611f\u77e5\u6280\u672f\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5b9e\u65f6\u5904\u7406\u6d41\u6c34\u7ebf\uff0c\u5c06\u624b\u52bf\u8bc6\u522b\u4e0e\u673a\u5668\u4eba\u63a7\u5236\u76f8\u7ed3\u5408", "result": "\u7cfb\u7edf\u80fd\u591f\u7cbe\u786e\u8bc6\u522b9\u79cd\u4e0d\u540c\u624b\u52bf\uff0c\u5e76\u5b9e\u65f6\u6620\u5c04\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u547d\u4ee4\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u3001\u6027\u80fd\u548c\u53ef\u9760\u6027", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u4fdd\u62a4\u9690\u79c1\u3001\u66f4\u7a33\u5b9a\u53ef\u9760\u7684\u65e0\u63a5\u89e6\u63a7\u5236\u65b9\u5f0f\uff0c\u5145\u5206\u5229\u7528\u4e86\u96f7\u8fbe\u611f\u77e5\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u4f18\u52bf"}}
{"id": "2509.12129", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12129", "abs": "https://arxiv.org/abs/2509.12129", "authors": ["Jiazhao Zhang", "Anqi Li", "Yunpeng Qi", "Minghan Li", "Jiahang Liu", "Shaoan Wang", "Haoran Liu", "Gengze Zhou", "Yuze Wu", "Xingxing Li", "Yuxin Fan", "Wenjun Li", "Zhibo Chen", "Fei Gao", "Qi Wu", "Zhizheng Zhang", "He Wang"], "title": "Embodied Navigation Foundation Model", "comment": "Project Page: https://pku-epic.github.io/NavFoM-Web/", "summary": "Navigation is a fundamental capability in embodied AI, representing the\nintelligence required to perceive and interact within physical environments\nfollowing language instructions. Despite significant progress in large\nVision-Language Models (VLMs), which exhibit remarkable zero-shot performance\non general vision-language tasks, their generalization ability in embodied\nnavigation remains largely confined to narrow task settings and\nembodiment-specific architectures. In this work, we introduce a\ncross-embodiment and cross-task Navigation Foundation Model (NavFoM), trained\non eight million navigation samples that encompass quadrupeds, drones, wheeled\nrobots, and vehicles, and spanning diverse tasks such as vision-and-language\nnavigation, object searching, target tracking, and autonomous driving. NavFoM\nemploys a unified architecture that processes multimodal navigation inputs from\nvarying camera configurations and navigation horizons. To accommodate diverse\ncamera setups and temporal horizons, NavFoM incorporates identifier tokens that\nembed camera view information of embodiments and the temporal context of tasks.\nFurthermore, to meet the demands of real-world deployment, NavFoM controls all\nobservation tokens using a dynamically adjusted sampling strategy under a\nlimited token length budget. Extensive evaluations on public benchmarks\ndemonstrate that our model achieves state-of-the-art or highly competitive\nperformance across multiple navigation tasks and embodiments without requiring\ntask-specific fine-tuning. Additional real-world experiments further confirm\nthe strong generalization capability and practical applicability of our\napproach.", "AI": {"tldr": "NavFoM\u662f\u4e00\u4e2a\u8de8\u5177\u8eab\u548c\u8de8\u4efb\u52a1\u7684\u5bfc\u822a\u57fa\u7840\u6a21\u578b\uff0c\u4f7f\u7528\u7edf\u4e00\u67b6\u6784\u5904\u7406\u591a\u6a21\u6001\u5bfc\u822a\u8f93\u5165\uff0c\u5728\u591a\u4e2a\u5bfc\u822a\u4efb\u52a1\u548c\u5177\u8eab\u5f62\u5f0f\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5177\u8eab\u5bfc\u822a\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u5c40\u9650\u4e8e\u72ed\u7a84\u7684\u4efb\u52a1\u8bbe\u7f6e\u548c\u7279\u5b9a\u5177\u8eab\u67b6\u6784\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u901a\u7528\u7684\u5bfc\u822a\u57fa\u7840\u6a21\u578b\u3002", "method": "\u4f7f\u7528800\u4e07\u5bfc\u822a\u6837\u672c\u8bad\u7ec3\uff0c\u6db5\u76d6\u56db\u8db3\u673a\u5668\u4eba\u3001\u65e0\u4eba\u673a\u3001\u8f6e\u5f0f\u673a\u5668\u4eba\u548c\u8f66\u8f86\u7b49\u591a\u79cd\u5177\u8eab\u5f62\u5f0f\u3002\u91c7\u7528\u7edf\u4e00\u67b6\u6784\u5904\u7406\u4e0d\u540c\u76f8\u673a\u914d\u7f6e\u548c\u5bfc\u822a\u89c6\u91ce\uff0c\u5f15\u5165\u6807\u8bc6\u7b26\u4ee4\u724c\u5d4c\u5165\u76f8\u673a\u89c6\u89d2\u4fe1\u606f\u548c\u4efb\u52a1\u65f6\u95f4\u4e0a\u4e0b\u6587\uff0c\u4f7f\u7528\u52a8\u6001\u8c03\u6574\u91c7\u6837\u7b56\u7565\u63a7\u5236\u89c2\u6d4b\u4ee4\u724c\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6216\u6781\u5177\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u3002\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u6027\u3002", "conclusion": "NavFoM\u5c55\u793a\u4e86\u8de8\u5177\u8eab\u548c\u8de8\u4efb\u52a1\u7684\u5f3a\u5927\u5bfc\u822a\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u7edf\u4e00\u5bfc\u822a\u57fa\u7840\u6a21\u578b\u7684\u53ef\u884c\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.12151", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12151", "abs": "https://arxiv.org/abs/2509.12151", "authors": ["Zongyao Yi", "Joachim Hertzberg", "Martin Atzmueller"], "title": "Learning Contact Dynamics for Control with Action-conditioned Face Interaction Graph Networks", "comment": null, "summary": "We present a learnable physics simulator that provides accurate motion and\nforce-torque prediction of robot end effectors in contact-rich manipulation.\nThe proposed model extends the state-of-the-art GNN-based simulator (FIGNet)\nwith novel node and edge types, enabling action-conditional predictions for\ncontrol and state estimation tasks. In simulation, the MPC agent using our\nmodel matches the performance of the same controller with the ground truth\ndynamics model in a challenging peg-in-hole task, while in the real-world\nexperiment, our model achieves a 50% improvement in motion prediction accuracy\nand 3$\\times$ increase in force-torque prediction precision over the baseline\nphysics simulator. Source code and data are publicly available.", "AI": {"tldr": "\u57fa\u4e8eGNN\u7684\u53ef\u5b66\u4e60\u7269\u7406\u6a21\u62df\u5668\uff0c\u901a\u8fc7\u65b0\u7684\u8282\u70b9\u548c\u8fb9\u7c7b\u578b\u6269\u5c55\uff0c\u5728\u63a2\u7d22\u6027\u64cd\u4f5c\u4efb\u52a1\u4e2d\u63d0\u4f9b\u51c6\u786e\u7684\u8fd0\u52a8\u548c\u529b\u77e9\u9884\u6d4b", "motivation": "\u9700\u8981\u4e00\u79cd\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u5728\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u4e2d\u7684\u8fd0\u52a8\u548c\u529b\u77e9\u7684\u7269\u7406\u6a21\u62df\u5668\uff0c\u4ee5\u652f\u6301\u63a7\u5236\u548c\u72b6\u6001\u4f30\u8ba1\u4efb\u52a1", "method": "\u6269\u5c55\u73b0\u6709\u7684FIGNet GNN\u6a21\u578b\uff0c\u5f15\u5165\u65b0\u7684\u8282\u70b9\u548c\u8fb9\u7c7b\u578b\uff0c\u652f\u6301\u52a8\u4f5c\u6761\u4ef6\u9884\u6d4b\uff0c\u5e76\u4f7f\u7528MPC\u63a7\u5236\u5668\u8fdb\u884c\u9a8c\u8bc1", "result": "\u5728\u6a21\u62df\u4e2d\uff0c\u4f7f\u7528\u8be5\u6a21\u578b\u7684MPC\u63a7\u5236\u5668\u5728\u9ad8\u96be\u5ea6\u680f\u585e\u63d2\u5165\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u4e0e\u771f\u5b9e\u52a8\u529b\u5b66\u6a21\u578b\u76f8\u540c\u7684\u6027\u80fd\uff1b\u5728\u5b9e\u9645\u5b9e\u9a8c\u4e2d\uff0c\u8fd0\u52a8\u9884\u6d4b\u51c6\u786e\u5ea6\u63d0\u534750%\uff0c\u529b\u77e9\u9884\u6d4b\u7cbe\u5ea6\u63d0\u9ad83\u500d", "conclusion": "\u8be5\u53ef\u5b66\u4e60\u7269\u7406\u6a21\u62df\u5668\u80fd\u591f\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u63d0\u4f9b\u51c6\u786e\u7684\u52a8\u529b\u5b66\u9884\u6d4b\uff0c\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u548c\u72b6\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
