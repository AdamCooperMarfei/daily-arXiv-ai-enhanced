{"id": "2507.08851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08851", "abs": "https://arxiv.org/abs/2507.08851", "authors": ["Simon Schwaiger", "Stefan Thalhammer", "Wilfried W\u00f6ber", "Gerald Steinbauer-Wagner"], "title": "OTAS: Open-vocabulary Token Alignment for Outdoor Segmentation", "comment": null, "summary": "Understanding open-world semantics is critical for robotic planning and\ncontrol, particularly in unstructured outdoor environments. Current\nvision-language mapping approaches rely on object-centric segmentation priors,\nwhich often fail outdoors due to semantic ambiguities and indistinct semantic\nclass boundaries. We propose OTAS - an Open-vocabulary Token Alignment method\nfor Outdoor Segmentation. OTAS overcomes the limitations of open-vocabulary\nsegmentation models by extracting semantic structure directly from the output\ntokens of pretrained vision models. By clustering semantically similar\nstructures across single and multiple views and grounding them in language,\nOTAS reconstructs a geometrically consistent feature field that supports\nopen-vocabulary segmentation queries. Our method operates zero-shot, without\nscene-specific fine-tuning, and runs at up to ~17 fps. OTAS provides a minor\nIoU improvement over fine-tuned and open-vocabulary 2D segmentation methods on\nthe Off-Road Freespace Detection dataset. Our model achieves up to a 151% IoU\nimprovement over open-vocabulary mapping methods in 3D segmentation on\nTartanAir. Real-world reconstructions demonstrate OTAS' applicability to\nrobotic applications. The code and ROS node will be made publicly available\nupon paper acceptance.", "AI": {"tldr": "OTAS\u662f\u4e00\u79cd\u5f00\u653e\u8bcd\u6c47\u6807\u8bb0\u5bf9\u9f50\u65b9\u6cd5\uff0c\u7528\u4e8e\u6237\u5916\u5206\u5272\uff0c\u901a\u8fc7\u4ece\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u8f93\u51fa\u6807\u8bb0\u4e2d\u63d0\u53d6\u8bed\u4e49\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7406\u89e3\u5f00\u653e\u4e16\u754c\u8bed\u4e49\u5bf9\u673a\u5668\u4eba\u89c4\u5212\u548c\u63a7\u5236\u5728\u975e\u7ed3\u6784\u5316\u6237\u5916\u73af\u5883\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5bf9\u8c61\u4e2d\u5fc3\u5206\u5272\u5148\u9a8c\uff0c\u5728\u6237\u5916\u73af\u5883\u4e2d\u5e38\u56e0\u8bed\u4e49\u6a21\u7cca\u548c\u8fb9\u754c\u4e0d\u6e05\u800c\u5931\u6548\u3002", "method": "OTAS\u901a\u8fc7\u805a\u7c7b\u5355\u89c6\u56fe\u548c\u591a\u89c6\u56fe\u4e2d\u7684\u8bed\u4e49\u76f8\u4f3c\u7ed3\u6784\uff0c\u5e76\u5c06\u5176\u4e0e\u8bed\u8a00\u5bf9\u9f50\uff0c\u6784\u5efa\u51e0\u4f55\u4e00\u81f4\u7684\u7279\u5f81\u573a\uff0c\u652f\u6301\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u67e5\u8be2\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u573a\u666f\u7279\u5b9a\u5fae\u8c03\uff0c\u96f6\u6837\u672c\u8fd0\u884c\uff0c\u901f\u5ea6\u8fbe17 fps\u3002", "result": "OTAS\u5728Off-Road Freespace Detection\u6570\u636e\u96c6\u4e0a\u7565\u4f18\u4e8e\u5fae\u8c03\u548c\u5f00\u653e\u8bcd\u6c472D\u5206\u5272\u65b9\u6cd5\uff0c\u5728TartanAir\u76843D\u5206\u5272\u4e2d\u6bd4\u5f00\u653e\u8bcd\u6c47\u6620\u5c04\u65b9\u6cd5\u63d0\u5347151% IoU\u3002", "conclusion": "OTAS\u9002\u7528\u4e8e\u673a\u5668\u4eba\u5e94\u7528\uff0c\u4ee3\u7801\u548cROS\u8282\u70b9\u5c06\u5728\u8bba\u6587\u63a5\u53d7\u540e\u516c\u5f00\u3002"}}
{"id": "2507.08885", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08885", "abs": "https://arxiv.org/abs/2507.08885", "authors": ["Baining Zhao", "Rongze Tang", "Mingyuan Jia", "Ziyou Wang", "Fanghang Man", "Xin Zhang", "Yu Shang", "Weichen Zhang", "Chen Gao", "Wei Wu", "Xin Wang", "Xinlei Chen", "Yong Li"], "title": "AirScape: An Aerial Generative World Model with Motion Controllability", "comment": null, "summary": "How to enable robots to predict the outcomes of their own motion intentions\nin three-dimensional space has been a fundamental problem in embodied\nintelligence. To explore more general spatial imagination capabilities, here we\npresent AirScape, the first world model designed for six-degree-of-freedom\naerial agents. AirScape predicts future observation sequences based on current\nvisual inputs and motion intentions. Specifically, we construct an dataset for\naerial world model training and testing, which consists of 11k video-intention\npairs. This dataset includes first-person-view videos capturing diverse drone\nactions across a wide range of scenarios, with over 1,000 hours spent\nannotating the corresponding motion intentions. Then we develop a two-phase\ntraining schedule to train a foundation model -- initially devoid of embodied\nspatial knowledge -- into a world model that is controllable by motion\nintentions and adheres to physical spatio-temporal constraints.", "AI": {"tldr": "AirScape\u662f\u9996\u4e2a\u4e3a\u516d\u81ea\u7531\u5ea6\u7a7a\u4e2d\u4ee3\u7406\u8bbe\u8ba1\u7684\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u89c6\u89c9\u8f93\u5165\u548c\u8fd0\u52a8\u610f\u56fe\u9884\u6d4b\u672a\u6765\u89c2\u5bdf\u5e8f\u5217\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u9884\u6d4b\u81ea\u8eab\u8fd0\u52a8\u610f\u56fe\u7ed3\u679c\u7684\u57fa\u672c\u95ee\u9898\uff0c\u63a2\u7d22\u66f4\u901a\u7528\u7684\u7a7a\u95f4\u60f3\u8c61\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b11k\u89c6\u9891-\u610f\u56fe\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8ba1\u5212\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u3002", "result": "\u5f00\u53d1\u51fa\u53ef\u63a7\u4e14\u7b26\u5408\u7269\u7406\u65f6\u7a7a\u7ea6\u675f\u7684\u4e16\u754c\u6a21\u578b\u3002", "conclusion": "AirScape\u4e3a\u7a7a\u4e2d\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7a7a\u95f4\u60f3\u8c61\u548c\u9884\u6d4b\u5de5\u5177\u3002"}}
{"id": "2507.08901", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08901", "abs": "https://arxiv.org/abs/2507.08901", "authors": ["Zebang Feng", "Miao Fan", "Bao Liu", "Shengtong Xu", "Haoyi Xiong"], "title": "End-to-End Generation of City-Scale Vectorized Maps by Crowdsourced Vehicles", "comment": "Accepted by ITSC'25", "summary": "High-precision vectorized maps are indispensable for autonomous driving, yet\ntraditional LiDAR-based creation is costly and slow, while single-vehicle\nperception methods lack accuracy and robustness, particularly in adverse\nconditions. This paper introduces EGC-VMAP, an end-to-end framework that\novercomes these limitations by generating accurate, city-scale vectorized maps\nthrough the aggregation of data from crowdsourced vehicles. Unlike prior\napproaches, EGC-VMAP directly fuses multi-vehicle, multi-temporal map elements\nperceived onboard vehicles using a novel Trip-Aware Transformer architecture\nwithin a unified learning process. Combined with hierarchical matching for\nefficient training and a multi-objective loss, our method significantly\nenhances map accuracy and structural robustness compared to single-vehicle\nbaselines. Validated on a large-scale, multi-city real-world dataset, EGC-VMAP\ndemonstrates superior performance, enabling a scalable, cost-effective solution\nfor city-wide mapping with a reported 90\\% reduction in manual annotation\ncosts.", "AI": {"tldr": "EGC-VMAP\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u4f17\u5305\u8f66\u8f86\u6570\u636e\u751f\u6210\u9ad8\u7cbe\u5ea6\u57ce\u5e02\u7ea7\u77e2\u91cf\u5316\u5730\u56fe\uff0c\u663e\u8457\u63d0\u5347\u5730\u56fe\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edfLiDAR\u5236\u56fe\u6210\u672c\u9ad8\u4e14\u6162\uff0c\u5355\u8f66\u611f\u77e5\u65b9\u6cd5\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u7f3a\u4e4f\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528Trip-Aware Transformer\u67b6\u6784\uff0c\u878d\u5408\u591a\u8f66\u3001\u591a\u65f6\u6001\u5730\u56fe\u5143\u7d20\uff0c\u7ed3\u5408\u5206\u5c42\u5339\u914d\u548c\u591a\u76ee\u6807\u635f\u5931\u3002", "result": "\u5728\u5927\u89c4\u6a21\u591a\u57ce\u5e02\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u5355\u8f66\u57fa\u7ebf\uff0c\u624b\u52a8\u6807\u6ce8\u6210\u672c\u964d\u4f4e90%\u3002", "conclusion": "EGC-VMAP\u4e3a\u57ce\u5e02\u7ea7\u5730\u56fe\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.08903", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08903", "abs": "https://arxiv.org/abs/2507.08903", "authors": ["Zhongzhang Chen", "Miao Fan", "Shengtong Xu", "Mengmeng Yang", "Kun Jiang", "Xiangzeng Liu", "Haoyi Xiong"], "title": "Multimodal HD Mapping for Intersections by Intelligent Roadside Units", "comment": "Accepted by ITSC'25", "summary": "High-definition (HD) semantic mapping of complex intersections poses\nsignificant challenges for traditional vehicle-based approaches due to\nocclusions and limited perspectives. This paper introduces a novel camera-LiDAR\nfusion framework that leverages elevated intelligent roadside units (IRUs).\nAdditionally, we present RS-seq, a comprehensive dataset developed through the\nsystematic enhancement and annotation of the V2X-Seq dataset. RS-seq includes\nprecisely labelled camera imagery and LiDAR point clouds collected from\nroadside installations, along with vectorized maps for seven intersections\nannotated with detailed features such as lane dividers, pedestrian crossings,\nand stop lines. This dataset facilitates the systematic investigation of\ncross-modal complementarity for HD map generation using IRU data. The proposed\nfusion framework employs a two-stage process that integrates modality-specific\nfeature extraction and cross-modal semantic integration, capitalizing on camera\nhigh-resolution texture and precise geometric data from LiDAR. Quantitative\nevaluations using the RS-seq dataset demonstrate that our multimodal approach\nconsistently surpasses unimodal methods. Specifically, compared to unimodal\nbaselines evaluated on the RS-seq dataset, the multimodal approach improves the\nmean Intersection-over-Union (mIoU) for semantic segmentation by 4\\% over the\nimage-only results and 18\\% over the point cloud-only results. This study\nestablishes a baseline methodology for IRU-based HD semantic mapping and\nprovides a valuable dataset for future research in infrastructure-assisted\nautonomous driving systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6444\u50cf\u5934-LiDAR\u878d\u5408\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u667a\u80fd\u8def\u8fb9\u5355\u5143\uff08IRUs\uff09\u751f\u6210\u9ad8\u7cbe\u5ea6\u8bed\u4e49\u5730\u56fe\uff0c\u5e76\u53d1\u5e03\u4e86RS-seq\u6570\u636e\u96c6\u3002\u8be5\u65b9\u6cd5\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u8f66\u8f86\u65b9\u6cd5\u5728\u590d\u6742\u4ea4\u53c9\u8def\u53e3\u7684\u9ad8\u6e05\u8bed\u4e49\u5730\u56fe\u751f\u6210\u4e2d\u5b58\u5728\u906e\u6321\u548c\u89c6\u89d2\u9650\u5236\u95ee\u9898\uff0c\u9700\u8981\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u878d\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u6444\u50cf\u5934\u7684\u9ad8\u5206\u8fa8\u7387\u7eb9\u7406\u548cLiDAR\u7684\u7cbe\u786e\u51e0\u4f55\u6570\u636e\uff0c\u901a\u8fc7\u6a21\u6001\u7279\u5b9a\u7279\u5f81\u63d0\u53d6\u548c\u8de8\u6a21\u6001\u8bed\u4e49\u6574\u5408\u5b9e\u73b0\u3002", "result": "\u591a\u6a21\u6001\u65b9\u6cd5\u5728RS-seq\u6570\u636e\u96c6\u4e0a\u7684mIoU\u6bd4\u56fe\u50cf\u5355\u6a21\u6001\u9ad84%\uff0c\u6bd4\u70b9\u4e91\u5355\u6a21\u6001\u9ad818%\u3002", "conclusion": "\u7814\u7a76\u4e3a\u57fa\u4e8eIRU\u7684\u9ad8\u6e05\u8bed\u4e49\u5730\u56fe\u751f\u6210\u63d0\u4f9b\u4e86\u57fa\u51c6\u65b9\u6cd5\uff0c\u5e76\u4e3a\u57fa\u7840\u8bbe\u65bd\u8f85\u52a9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2507.09117", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09117", "abs": "https://arxiv.org/abs/2507.09117", "authors": ["Gagan Khandate"], "title": "Towards Human-level Dexterity via Robot Learning", "comment": "PhD thesis", "summary": "Dexterous intelligence -- the ability to perform complex interactions with\nmulti-fingered hands -- is a pinnacle of human physical intelligence and\nemergent higher-order cognitive skills. However, contrary to Moravec's paradox,\ndexterous intelligence in humans appears simple only superficially. Many\nmillion years were spent co-evolving the human brain and hands including rich\ntactile sensing. Achieving human-level dexterity with robotic hands has long\nbeen a fundamental goal in robotics and represents a critical milestone toward\ngeneral embodied intelligence. In this pursuit, computational sensorimotor\nlearning has made significant progress, enabling feats such as arbitrary\nin-hand object reorientation. However, we observe that achieving higher levels\nof dexterity requires overcoming very fundamental limitations of computational\nsensorimotor learning.\n  I develop robot learning methods for highly dexterous multi-fingered\nmanipulation by directly addressing these limitations at their root cause.\nChiefly, through key studies, this disseration progressively builds an\neffective framework for reinforcement learning of dexterous multi-fingered\nmanipulation skills. These methods adopt structured exploration, effectively\novercoming the limitations of random exploration in reinforcement learning. The\ninsights gained culminate in a highly effective reinforcement learning that\nincorporates sampling-based planning for direct exploration. Additionally, this\nthesis explores a new paradigm of using visuo-tactile human demonstrations for\ndexterity, introducing corresponding imitation learning techniques.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u514b\u670d\u591a\u6307\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u57fa\u672c\u9650\u5236\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6c34\u5e73\u7684\u7075\u5de7\u6027\u3002", "motivation": "\u4eba\u7c7b\u7075\u5de7\u667a\u80fd\u7684\u590d\u6742\u6027\u53ca\u5176\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5b9e\u73b0\u662f\u4e00\u4e2a\u957f\u671f\u76ee\u6807\uff0c\u4f46\u73b0\u6709\u8ba1\u7b97\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u63a2\u7d22\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u91c7\u6837\u7684\u89c4\u5212\u548c\u89c6\u89c9\u89e6\u89c9\u4eba\u7c7b\u6f14\u793a\u7684\u6a21\u4eff\u5b66\u4e60\u6280\u672f\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u514b\u670d\u968f\u673a\u63a2\u7d22\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u6a21\u4eff\u5b66\u4e60\u8303\u5f0f\u3002", "conclusion": "\u901a\u8fc7\u76f4\u63a5\u89e3\u51b3\u8ba1\u7b97\u4f20\u611f\u5668\u8fd0\u52a8\u5b66\u4e60\u7684\u6839\u672c\u9650\u5236\uff0c\u8bba\u6587\u4e3a\u591a\u6307\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b66\u4e60\u65b9\u6cd5\u548c\u6846\u67b6\u3002"}}
{"id": "2507.09123", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09123", "abs": "https://arxiv.org/abs/2507.09123", "authors": ["Ziyan Gao", "Lijun Wang", "Yuntao Kong", "Nak Young Chong"], "title": "Online 3D Bin Packing with Fast Stability Validation and Stable Rearrangement Planning", "comment": null, "summary": "The Online Bin Packing Problem (OBPP) is a sequential decision-making task in\nwhich each item must be placed immediately upon arrival, with no knowledge of\nfuture arrivals. Although recent deep-reinforcement-learning methods achieve\nsuperior volume utilization compared with classical heuristics, the learned\npolicies cannot ensure the structural stability of the bin and lack mechanisms\nfor safely reconfiguring the bin when a new item cannot be placed directly. In\nthis work, we propose a novel framework that integrates packing policy with\nstructural stability validation and heuristic planning to overcome these\nlimitations. Specifically, we introduce the concept of Load Bearable Convex\nPolygon (LBCP), which provides a computationally efficient way to identify\nstable loading positions that guarantee no bin collapse. Additionally, we\npresent Stable Rearrangement Planning (SRP), a module that rearranges existing\nitems to accommodate new ones while maintaining overall stability. Extensive\nexperiments on standard OBPP benchmarks demonstrate the efficiency and\ngeneralizability of our LBCP-based stability validation, as well as the\nsuperiority of SRP in finding the effort-saving rearrangement plans. Our method\noffers a robust and practical solution for automated packing in real-world\nindustrial and logistics applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5305\u88c5\u7b56\u7565\u3001\u7ed3\u6784\u7a33\u5b9a\u6027\u9a8c\u8bc1\u548c\u542f\u53d1\u5f0f\u89c4\u5212\u7684\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5728\u7ebf\u88c5\u7bb1\u95ee\u9898\u4e2d\u7ed3\u6784\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u91cd\u65b0\u914d\u7f6e\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u867d\u80fd\u63d0\u9ad8\u5bb9\u79ef\u5229\u7528\u7387\uff0c\u4f46\u65e0\u6cd5\u786e\u4fdd\u7bb1\u4f53\u7ed3\u6784\u7a33\u5b9a\u6027\uff0c\u4e14\u7f3a\u4e4f\u5b89\u5168\u91cd\u65b0\u914d\u7f6e\u673a\u5236\u3002", "method": "\u5f15\u5165\u8d1f\u8f7d\u53ef\u627f\u8f7d\u51f8\u591a\u8fb9\u5f62\uff08LBCP\uff09\u8fdb\u884c\u7a33\u5b9a\u6027\u9a8c\u8bc1\uff0c\u5e76\u63d0\u51fa\u7a33\u5b9a\u91cd\u6392\u89c4\u5212\uff08SRP\uff09\u6a21\u5757\u4ee5\u5b89\u5168\u91cd\u6392\u7269\u54c1\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLBCP\u9a8c\u8bc1\u9ad8\u6548\u4e14\u901a\u7528\uff0cSRP\u5728\u8282\u7701\u91cd\u6392\u6210\u672c\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u9645\u5de5\u4e1a\u548c\u7269\u6d41\u5e94\u7528\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u5b9e\u7528\u7684\u81ea\u52a8\u5316\u88c5\u7bb1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09160", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09160", "abs": "https://arxiv.org/abs/2507.09160", "authors": ["Jialei Huang", "Shuo Wang", "Fanqi Lin", "Yihang Hu", "Chuan Wen", "Yang Gao"], "title": "Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization", "comment": null, "summary": "Vision-Language-Action (VLA) models have shown remarkable achievements,\ndriven by the rich implicit knowledge of their vision-language components.\nHowever, achieving generalist robotic agents demands precise grounding into\nphysical interactions, especially in contact-rich scenarios where fine-grained\nforce control is essential. We advance VLAs' implicit knowledge beyond\nidentifying what to do, towards guiding how to physically interact with real\nworld. This paper introduces Tactile-VLA, a novel framework that deeply fuses\nvision, language, action, and tactile sensing. This framework incorporates a\nhybrid position-force controller to translate the model's intentions into\nprecise physical actions and a reasoning module that allows the robot to adapt\nits strategy based on tactile feedback. Experiments demonstrate Tactile-VLA's\neffectiveness and generalizability in three key aspects: (1) enabling\ntactile-aware instruction following, (2) utilizing tactile-relevant\ncommonsense, and (3) facilitating adaptive tactile-involved reasoning. A key\nfinding is that the VLM's prior knowledge already contains semantic\nunderstanding of physical interaction; by connecting it to the robot's tactile\nsensors with only a few demonstrations, we can activate this prior knowledge to\nachieve zero-shot generalization in contact-rich tasks.", "AI": {"tldr": "Tactile-VLA\u6846\u67b6\u878d\u5408\u89c6\u89c9\u3001\u8bed\u8a00\u3001\u52a8\u4f5c\u548c\u89e6\u89c9\u611f\u77e5\uff0c\u901a\u8fc7\u6df7\u5408\u529b\u4f4d\u63a7\u5236\u5668\u548c\u63a8\u7406\u6a21\u5757\u5b9e\u73b0\u7cbe\u786e\u7269\u7406\u4ea4\u4e92\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u89e6\u89c9\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u7269\u7406\u4ea4\u4e92\u4e2d\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7cbe\u7ec6\u529b\u63a7\u5236\u7684\u63a5\u89e6\u4e30\u5bcc\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51faTactile-VLA\u6846\u67b6\uff0c\u7ed3\u5408\u6df7\u5408\u529b\u4f4d\u63a7\u5236\u5668\u548c\u89e6\u89c9\u53cd\u9988\u63a8\u7406\u6a21\u5757\uff0c\u901a\u8fc7\u5c11\u91cf\u6f14\u793a\u6fc0\u6d3bVLM\u7684\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u89e6\u89c9\u611f\u77e5\u6307\u4ee4\u8ddf\u968f\u3001\u89e6\u89c9\u5e38\u8bc6\u5229\u7528\u548c\u81ea\u9002\u5e94\u89e6\u89c9\u63a8\u7406\u4e09\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u3002", "conclusion": "Tactile-VLA\u901a\u8fc7\u89e6\u89c9\u53cd\u9988\u548cVLM\u5148\u9a8c\u77e5\u8bc6\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2507.09167", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09167", "abs": "https://arxiv.org/abs/2507.09167", "authors": ["Michal Vavrecka", "Radoslav Skoviera", "Gabriela Sejnova", "Karla Stepanova"], "title": "PRAG: Procedural Action Generator", "comment": null, "summary": "We present a novel approach for the procedural construction of multi-step\ncontact-rich manipulation tasks in robotics. Our generator takes as input\nuser-defined sets of atomic actions, objects, and spatial predicates and\noutputs solvable tasks of a given length for the selected robotic environment.\nThe generator produces solvable tasks by constraining all possible\n(nonsolvable) combinations by symbolic and physical validation. The symbolic\nvalidation checks each generated sequence for logical and operational\nconsistency, and also the suitability of object-predicate relations. Physical\nvalidation checks whether tasks can be solved in the selected robotic\nenvironment. Only the tasks that passed both validators are retained. The\noutput from the generator can be directly interfaced with any existing\nframework for training robotic manipulation tasks, or it can be stored as a\ndataset of curated robotic tasks with detailed information about each task.\nThis is beneficial for RL training as there are dense reward functions and\ninitial and goal states paired with each subgoal. It allows the user to measure\nthe semantic similarity of all generated tasks. We tested our generator on\nsequences of up to 15 actions resulting in millions of unique solvable\nmulti-step tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u591a\u6b65\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u4efb\u52a1\u7a0b\u5e8f\u5316\u6784\u5efa\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b26\u53f7\u548c\u7269\u7406\u9a8c\u8bc1\u751f\u6210\u53ef\u89e3\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u591a\u6b65\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u7684\u751f\u6210\u95ee\u9898\uff0c\u786e\u4fdd\u4efb\u52a1\u903b\u8f91\u548c\u7269\u7406\u53ef\u884c\u6027\u3002", "method": "\u8f93\u5165\u539f\u5b50\u52a8\u4f5c\u3001\u5bf9\u8c61\u548c\u7a7a\u95f4\u8c13\u8bcd\uff0c\u901a\u8fc7\u7b26\u53f7\u548c\u7269\u7406\u9a8c\u8bc1\u751f\u6210\u53ef\u89e3\u4efb\u52a1\u5e8f\u5217\u3002", "result": "\u751f\u6210\u4e86\u6570\u767e\u4e07\u4e2a\u72ec\u7279\u7684\u53ef\u89e3\u591a\u6b65\u4efb\u52a1\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u8bad\u7ec3\u6846\u67b6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u751f\u6210\u53ef\u89e3\u4efb\u52a1\uff0c\u652f\u6301\u673a\u5668\u4eba\u8bad\u7ec3\u548c\u4efb\u52a1\u8bed\u4e49\u76f8\u4f3c\u6027\u8bc4\u4f30\u3002"}}
{"id": "2507.09176", "categories": ["cs.RO", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09176", "abs": "https://arxiv.org/abs/2507.09176", "authors": ["Han Ye", "Yuqiang Jin", "Jinyuan Liu", "Tao Li", "Wen-An Zhang", "Minglei Fu"], "title": "DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA", "comment": "9 pages,14 figures", "summary": "Accurate extrinsic calibration of multiple LiDARs is crucial for improving\nthe foundational performance of three-dimensional (3D) map reconstruction\nsystems. This paper presents a novel targetless extrinsic calibration framework\nfor multi-LiDAR systems that does not rely on overlapping fields of view or\nprecise initial parameter estimates. Unlike conventional calibration methods\nthat require manual annotations or specific reference patterns, our approach\nintroduces a unified optimization framework by integrating LiDAR bundle\nadjustment (LBA) optimization with robust iterative refinement. The proposed\nmethod constructs an accurate reference point cloud map via continuous scanning\nfrom the target LiDAR and sliding-window LiDAR bundle adjustment, while\nformulating extrinsic calibration as a joint LBA optimization problem. This\nmethod effectively mitigates cumulative mapping errors and achieves\noutlier-resistant parameter estimation through an adaptive weighting mechanism.\nExtensive evaluations in both the CARLA simulation environment and real-world\nscenarios demonstrate that our method outperforms state-of-the-art calibration\ntechniques in both accuracy and robustness. Experimental results show that for\nnon-overlapping sensor configurations, our framework achieves an average\ntranslational error of 5 mm and a rotational error of 0.2{\\deg}, with an\ninitial error tolerance of up to 0.4 m/30{\\deg}. Moreover, the calibration\nprocess operates without specialized infrastructure or manual parameter tuning.\nThe code is open source and available on GitHub\n(\\underline{https://github.com/Silentbarber/DLBAcalib})", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u76ee\u6807\u7684\u591aLiDAR\u5916\u53c2\u6807\u5b9a\u6846\u67b6\uff0c\u901a\u8fc7LiDAR\u675f\u8c03\u6574\u548c\u81ea\u9002\u5e94\u52a0\u6743\u673a\u5236\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6807\u5b9a\u3002", "motivation": "\u591aLiDAR\u7cfb\u7edf\u7684\u5916\u53c2\u6807\u5b9a\u5bf93D\u5730\u56fe\u91cd\u5efa\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u91cd\u53e0\u89c6\u573a\u6216\u7cbe\u786e\u521d\u59cb\u53c2\u6570\uff0c\u9650\u5236\u4e86\u5b9e\u7528\u6027\u3002", "method": "\u7ed3\u5408LiDAR\u675f\u8c03\u6574\uff08LBA\uff09\u548c\u9c81\u68d2\u8fed\u4ee3\u4f18\u5316\uff0c\u6784\u5efa\u53c2\u8003\u70b9\u4e91\u56fe\u5e76\u901a\u8fc7\u8054\u5408LBA\u4f18\u5316\u5916\u53c2\u3002", "result": "\u5728CARLA\u4eff\u771f\u548c\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u5e73\u5747\u5e73\u79fb\u8bef\u5dee5mm\uff0c\u65cb\u8f6c\u8bef\u5dee0.2\u00b0\uff0c\u521d\u59cb\u8bef\u5dee\u5bb9\u5fcd\u5ea6\u8fbe0.4m/30\u00b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u65e0\u9700\u4e13\u7528\u57fa\u7840\u8bbe\u65bd\u6216\u624b\u52a8\u8c03\u53c2\uff0c\u4ee3\u7801\u5f00\u6e90\u3002"}}
{"id": "2507.09309", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09309", "abs": "https://arxiv.org/abs/2507.09309", "authors": ["Peng Xie", "Johannes Betz", "Amr Alanwar"], "title": "Informed Hybrid Zonotope-based Motion Planning Algorithm", "comment": null, "summary": "Optimal path planning in nonconvex free spaces is notoriously challenging, as\nformulating such problems as mixed-integer linear programs (MILPs) is NP-hard.\nWe propose HZ-MP, an informed Hybrid Zonotope-based Motion Planner, as an\nalternative approach that decomposes the obstacle-free space and performs\nlow-dimensional face sampling guided by an ellipsotope heuristic, enabling\nfocused exploration along promising transit regions. This structured\nexploration eliminates the excessive, unreachable sampling that degrades\nexisting informed planners such as AIT* and EIT* in narrow gaps or boxed-goal\nscenarios. We prove that HZ-MP is probabilistically complete and asymptotically\noptimal. It converges to near-optimal trajectories in finite time and scales to\nhigh-dimensional cluttered scenes.", "AI": {"tldr": "HZ-MP\u662f\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408Zonotope\u7684\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5206\u89e3\u65e0\u969c\u788d\u7a7a\u95f4\u548c\u4f4e\u7ef4\u9762\u91c7\u6837\uff0c\u89e3\u51b3\u4e86\u975e\u51f8\u81ea\u7531\u7a7a\u95f4\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u907f\u514d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8fc7\u5ea6\u91c7\u6837\u95ee\u9898\u3002", "motivation": "\u975e\u51f8\u81ea\u7531\u7a7a\u95f4\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\u7531\u4e8eNP\u96be\u7684\u7279\u6027\u800c\u6781\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u72ed\u7a84\u95f4\u9699\u6216\u76ee\u6807\u53d7\u9650\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "HZ-MP\u91c7\u7528\u6df7\u5408Zonotope\u5206\u89e3\u65e0\u969c\u788d\u7a7a\u95f4\uff0c\u7ed3\u5408\u4f4e\u7ef4\u9762\u91c7\u6837\u548c\u692d\u7403\u4f53\u542f\u53d1\u5f0f\u5f15\u5bfc\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\u3002", "result": "HZ-MP\u5728\u6709\u9650\u65f6\u95f4\u5185\u6536\u655b\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u8f68\u8ff9\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u590d\u6742\u573a\u666f\uff0c\u5e76\u5177\u6709\u6982\u7387\u5b8c\u5907\u6027\u548c\u6e10\u8fdb\u6700\u4f18\u6027\u3002", "conclusion": "HZ-MP\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.09340", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09340", "abs": "https://arxiv.org/abs/2507.09340", "authors": ["Hongyu Nie", "Xingyu Li", "Xu Liu", "Zhaotong Tan", "Sen Mei", "Wenbo Su"], "title": "Unified Linear Parametric Map Modeling and Perception-aware Trajectory Planning for Mobile Robotics", "comment": "Submitted to IEEE Transactions on Robotics (TRO) in July 2025", "summary": "Autonomous navigation in mobile robots, reliant on perception and planning,\nfaces major hurdles in large-scale, complex environments. These include heavy\ncomputational burdens for mapping, sensor occlusion failures for UAVs, and\ntraversal challenges on irregular terrain for UGVs, all compounded by a lack of\nperception-aware strategies. To address these challenges, we introduce Random\nMapping and Random Projection (RMRP). This method constructs a lightweight\nlinear parametric map by first mapping data to a high-dimensional space,\nfollowed by a sparse random projection for dimensionality reduction. Our novel\nResidual Energy Preservation Theorem provides theoretical guarantees for this\nprocess, ensuring critical geometric properties are preserved. Based on this\nmap, we propose the RPATR (Robust Perception-Aware Trajectory Planner)\nframework. For UAVs, our method unifies grid and Euclidean Signed Distance\nField (ESDF) maps. The front-end uses an analytical occupancy gradient to\nrefine initial paths for safety and smoothness, while the back-end uses a\nclosed-form ESDF for trajectory optimization. Leveraging the trained RMRP\nmodel's generalization, the planner predicts unobserved areas for proactive\nnavigation. For UGVs, the model characterizes terrain and provides closed-form\ngradients, enabling online planning to circumvent large holes. Validated in\ndiverse scenarios, our framework demonstrates superior mapping performance in\ntime, memory, and accuracy, and enables computationally efficient, safe\nnavigation for high-speed UAVs and UGVs. The code will be released to foster\ncommunity collaboration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRMRP\u7684\u8f7b\u91cf\u7ea7\u7ebf\u6027\u53c2\u6570\u5316\u5730\u56fe\u6784\u5efa\u65b9\u6cd5\uff0c\u7ed3\u5408RPATR\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u548c\u5730\u9762\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u95ee\u9898\u3002", "motivation": "\u5927\u89c4\u6a21\u590d\u6742\u73af\u5883\u4e2d\uff0c\u79fb\u52a8\u673a\u5668\u4eba\u7684\u81ea\u4e3b\u5bfc\u822a\u9762\u4e34\u8ba1\u7b97\u8d1f\u62c5\u91cd\u3001\u4f20\u611f\u5668\u906e\u6321\u548c\u5730\u5f62\u4e0d\u89c4\u5219\u7b49\u6311\u6218\uff0c\u7f3a\u4e4f\u611f\u77e5\u611f\u77e5\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u9ad8\u7ef4\u7a7a\u95f4\u6620\u5c04\u548c\u7a00\u758f\u968f\u673a\u6295\u5f71\u964d\u7ef4\u6784\u5efa\u8f7b\u91cf\u7ea7\u5730\u56fe\uff0c\u63d0\u51faRPATR\u6846\u67b6\uff0c\u7ed3\u5408ESDF\u548c\u68af\u5ea6\u4f18\u5316\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u5bfc\u822a\u3002", "result": "\u5728\u591a\u79cd\u573a\u666f\u4e2d\u9a8c\u8bc1\uff0cRMRP\u548cRPATR\u5728\u65f6\u95f4\u3001\u5185\u5b58\u548c\u7cbe\u5ea6\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u652f\u6301\u9ad8\u901f\u65e0\u4eba\u673a\u548c\u5730\u9762\u673a\u5668\u4eba\u7684\u9ad8\u6548\u5b89\u5168\u5bfc\u822a\u3002", "conclusion": "RMRP\u548cRPATR\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u969c\u548c\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u5408\u4f5c\u3002"}}
{"id": "2507.09344", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09344", "abs": "https://arxiv.org/abs/2507.09344", "authors": ["Daniel Engelsman", "Itzik Klein"], "title": "C-ZUPT: Stationarity-Aided Aerial Hovering", "comment": "14 Pages, 16 Figures, 9 Tables", "summary": "Autonomous systems across diverse domains have underscored the need for\ndrift-resilient state estimation. Although satellite-based positioning and\ncameras are widely used, they often suffer from limited availability in many\nenvironments. As a result, positioning must rely solely on inertial sensors,\nleading to rapid accuracy degradation over time due to sensor biases and noise.\nTo counteract this, alternative update sources-referred to as information\naiding-serve as anchors of certainty. Among these, the zero-velocity update\n(ZUPT) is particularly effective in providing accurate corrections during\nstationary intervals, though it is restricted to surface-bound platforms. This\nwork introduces a controlled ZUPT (C-ZUPT) approach for aerial navigation and\ncontrol, independent of surface contact. By defining an uncertainty threshold,\nC-ZUPT identifies quasi-static equilibria to deliver precise velocity updates\nto the estimation filter. Extensive validation confirms that these\nopportunistic, high-quality updates significantly reduce inertial drift and\ncontrol effort. As a result, C-ZUPT mitigates filter divergence and enhances\nnavigation stability, enabling more energy-efficient hovering and substantially\nextending sustained flight-key advantages for resource-constrained aerial\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7a7a\u4e2d\u5bfc\u822a\u7684\u53d7\u63a7\u96f6\u901f\u5ea6\u66f4\u65b0\uff08C-ZUPT\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u51c6\u9759\u6001\u5e73\u8861\u6765\u51cf\u5c11\u60ef\u6027\u6f02\u79fb\uff0c\u63d0\u9ad8\u5bfc\u822a\u7a33\u5b9a\u6027\u3002", "motivation": "\u536b\u661f\u548c\u6444\u50cf\u5934\u5b9a\u4f4d\u5728\u590d\u6742\u73af\u5883\u4e2d\u53d7\u9650\uff0c\u60ef\u6027\u4f20\u611f\u5668\u6613\u53d7\u504f\u5dee\u548c\u566a\u58f0\u5f71\u54cd\uff0c\u5bfc\u81f4\u7cbe\u5ea6\u5feb\u901f\u4e0b\u964d\u3002", "method": "\u5f15\u5165C-ZUPT\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u9608\u503c\u8bc6\u522b\u51c6\u9759\u6001\u5e73\u8861\uff0c\u63d0\u4f9b\u7cbe\u786e\u901f\u5ea6\u66f4\u65b0\u3002", "result": "C-ZUPT\u663e\u8457\u51cf\u5c11\u60ef\u6027\u6f02\u79fb\u548c\u63a7\u5236\u80fd\u8017\uff0c\u63d0\u5347\u5bfc\u822a\u7a33\u5b9a\u6027\uff0c\u5ef6\u957f\u98de\u884c\u65f6\u95f4\u3002", "conclusion": "C-ZUPT\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u7a7a\u4e2d\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09371", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09371", "abs": "https://arxiv.org/abs/2507.09371", "authors": ["Kehan Wen", "Chenhao Li", "Junzhe He", "Marco Hutter"], "title": "Constrained Style Learning from Imperfect Demonstrations under Task Optimality", "comment": "This paper is under review", "summary": "Learning from demonstration has proven effective in robotics for acquiring\nnatural behaviors, such as stylistic motions and lifelike agility, particularly\nwhen explicitly defining style-oriented reward functions is challenging.\nSynthesizing stylistic motions for real-world tasks usually requires balancing\ntask performance and imitation quality. Existing methods generally depend on\nexpert demonstrations closely aligned with task objectives. However, practical\ndemonstrations are often incomplete or unrealistic, causing current methods to\nboost style at the expense of task performance. To address this issue, we\npropose formulating the problem as a constrained Markov Decision Process\n(CMDP). Specifically, we optimize a style-imitation objective with constraints\nto maintain near-optimal task performance. We introduce an adaptively\nadjustable Lagrangian multiplier to guide the agent to imitate demonstrations\nselectively, capturing stylistic nuances without compromising task performance.\nWe validate our approach across multiple robotic platforms and tasks,\ndemonstrating both robust task performance and high-fidelity style learning. On\nANYmal-D hardware we show a 14.5% drop in mechanical energy and a more agile\ngait pattern, showcasing real-world benefits.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08CMDP\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5e73\u8861\u4efb\u52a1\u6027\u80fd\u548c\u98ce\u683c\u6a21\u4eff\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u4f9d\u8d56\u4e0d\u5b8c\u6574\u6216\u4e0d\u5207\u5b9e\u9645\u7684\u6f14\u793a\u800c\u727a\u7272\u4efb\u52a1\u6027\u80fd\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5408\u6210\u98ce\u683c\u5316\u52a8\u4f5c\u65f6\uff0c\u901a\u5e38\u4f9d\u8d56\u4e0e\u4efb\u52a1\u76ee\u6807\u7d27\u5bc6\u5bf9\u9f50\u7684\u4e13\u5bb6\u6f14\u793a\uff0c\u4f46\u5b9e\u9645\u6f14\u793a\u5f80\u5f80\u4e0d\u5b8c\u6574\u6216\u4e0d\u5207\u5b9e\u9645\uff0c\u5bfc\u81f4\u98ce\u683c\u6a21\u4eff\u4ee5\u727a\u7272\u4efb\u52a1\u6027\u80fd\u4e3a\u4ee3\u4ef7\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08CMDP\uff09\uff0c\u4f18\u5316\u98ce\u683c\u6a21\u4eff\u76ee\u6807\u7684\u540c\u65f6\u901a\u8fc7\u7ea6\u675f\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u7684\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u8c03\u6574\u7684\u62c9\u683c\u6717\u65e5\u4e58\u6570\u6765\u9009\u62e9\u6027\u6a21\u4eff\u6f14\u793a\u3002", "result": "\u5728\u591a\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u548c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u4efb\u52a1\u6027\u80fd\u548c\u9ad8\u4fdd\u771f\u7684\u98ce\u683c\u5b66\u4e60\uff0c\u5728ANYmal-D\u786c\u4ef6\u4e0a\u5c55\u793a\u4e8614.5%\u7684\u673a\u68b0\u80fd\u8017\u964d\u4f4e\u548c\u66f4\u654f\u6377\u7684\u6b65\u6001\u6a21\u5f0f\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7CMDP\u6846\u67b6\u548c\u81ea\u9002\u5e94\u62c9\u683c\u6717\u65e5\u4e58\u6570\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u98ce\u683c\u6a21\u4eff\u4e0e\u4efb\u52a1\u6027\u80fd\u7684\u5e73\u8861\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.09383", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09383", "abs": "https://arxiv.org/abs/2507.09383", "authors": ["Wondmgezahu Teshome", "Kian Behzad", "Octavia Camps", "Michael Everett", "Milad Siami", "Mario Sznaier"], "title": "Real-Time Adaptive Motion Planning via Point Cloud-Guided, Energy-Based Diffusion and Potential Fields", "comment": "Accepted to IEEE RA-L 2025", "summary": "Motivated by the problem of pursuit-evasion, we present a motion planning\nframework that combines energy-based diffusion models with artificial potential\nfields for robust real time trajectory generation in complex environments. Our\napproach processes obstacle information directly from point clouds, enabling\nefficient planning without requiring complete geometric representations. The\nframework employs classifier-free guidance training and integrates local\npotential fields during sampling to enhance obstacle avoidance. In dynamic\nscenarios, the system generates initial trajectories using the diffusion model\nand continuously refines them through potential field-based adaptation,\ndemonstrating effective performance in pursuit-evasion scenarios with partial\npursuer observability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u80fd\u91cf\u6269\u6563\u6a21\u578b\u548c\u4eba\u5de5\u52bf\u573a\u7684\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u8f68\u8ff9\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u8ffd\u9003\u95ee\u9898\uff0c\u5b9e\u73b0\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u65f6\u751f\u6210\u9c81\u68d2\u8f68\u8ff9\u7684\u9700\u6c42\u3002", "method": "\u5229\u7528\u70b9\u4e91\u76f4\u63a5\u5904\u7406\u969c\u788d\u7269\u4fe1\u606f\uff0c\u7ed3\u5408\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u8bad\u7ec3\u548c\u5c40\u90e8\u52bf\u573a\u91c7\u6837\u589e\u5f3a\u907f\u969c\u80fd\u529b\u3002", "result": "\u5728\u52a8\u6001\u573a\u666f\u4e2d\uff0c\u7cfb\u7edf\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u521d\u59cb\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u52bf\u573a\u9002\u5e94\u6301\u7eed\u4f18\u5316\uff0c\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u8ffd\u9003\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5b9e\u65f6\u8f68\u8ff9\u89c4\u5212\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u573a\u666f\u3002"}}
{"id": "2507.09463", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09463", "abs": "https://arxiv.org/abs/2507.09463", "authors": ["Anoop Kiran", "Nora Ayanian", "Kenneth Breuer"], "title": "Influence of Static and Dynamic Downwash Interactions on Multi-Quadrotor Systems", "comment": "Accepted for publication in Robotics: Science and Systems (RSS) 2025,\n  12 pages, 16 figures", "summary": "Flying multiple quadrotors in close proximity presents a significant\nchallenge due to complex aerodynamic interactions, particularly downwash\neffects that are known to destabilize vehicles and degrade performance.\nTraditionally, multi-quadrotor systems rely on conservative strategies, such as\ncollision avoidance zones around the robot volume, to circumvent this effect.\nThis restricts their capabilities by requiring a large volume for the operation\nof a multi-quadrotor system, limiting their applicability in dense\nenvironments. This work provides a comprehensive, data-driven analysis of the\ndownwash effect, with a focus on characterizing, analyzing, and understanding\nforces, moments, and velocities in both single and multi-quadrotor\nconfigurations. We use measurements of forces and torques to characterize\nvehicle interactions, and particle image velocimetry (PIV) to quantify the\nspatial features of the downwash wake for a single quadrotor and an interacting\npair of quadrotors. This data can be used to inform physics-based strategies\nfor coordination, leverage downwash for optimized formations, expand the\nenvelope of operation, and improve the robustness of multi-quadrotor control.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u5206\u6790\u4e86\u591a\u65cb\u7ffc\u65e0\u4eba\u673a\u95f4\u7684\u4e0b\u6d17\u6548\u5e94\uff0c\u63d0\u51fa\u4e86\u4f18\u5316\u7f16\u961f\u548c\u63a7\u5236\u7684\u7b56\u7565\u3002", "motivation": "\u591a\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u8fd1\u8ddd\u79bb\u98de\u884c\u65f6\u56e0\u4e0b\u6d17\u6548\u5e94\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u4f20\u7edf\u4fdd\u5b88\u7b56\u7565\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u4f7f\u7528\u529b\u548c\u529b\u77e9\u6d4b\u91cf\u4ee5\u53ca\u7c92\u5b50\u56fe\u50cf\u6d4b\u901f\u6280\u672f\uff08PIV\uff09\u91cf\u5316\u5355\u673a\u548c\u591a\u673a\u914d\u7f6e\u4e2d\u7684\u4e0b\u6d17\u6548\u5e94\u3002", "result": "\u6570\u636e\u53ef\u7528\u4e8e\u4f18\u5316\u7f16\u961f\u3001\u6269\u5c55\u64cd\u4f5c\u8303\u56f4\u5e76\u63d0\u9ad8\u591a\u673a\u63a7\u5236\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u591a\u65cb\u7ffc\u65e0\u4eba\u673a\u7684\u534f\u8c03\u63a7\u5236\u548c\u5bc6\u96c6\u73af\u5883\u5e94\u7528\u63d0\u4f9b\u4e86\u7269\u7406\u57fa\u7840\u3002"}}
{"id": "2507.09464", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09464", "abs": "https://arxiv.org/abs/2507.09464", "authors": ["Azfar Azdi Arfakhsyad", "Aufa Nasywa Rahman", "Larasati Kinanti", "Ahmad Ataka Awwalur Rizqi", "Hannan Nur Muhammad"], "title": "Unmanned Aerial Vehicle (UAV) Data-Driven Modeling Software with Integrated 9-Axis IMUGPS Sensor Fusion and Data Filtering Algorithm", "comment": "7 pages, 13 figures. Accepted to IEEE ICITEE 2023", "summary": "Unmanned Aerial Vehicles (UAV) have emerged as versatile platforms, driving\nthe demand for accurate modeling to support developmental testing. This paper\nproposes data-driven modeling software for UAV. Emphasizes the utilization of\ncost-effective sensors to obtain orientation and location data subsequently\nprocessed through the application of data filtering algorithms and sensor\nfusion techniques to improve the data quality to make a precise model\nvisualization on the software. UAV's orientation is obtained using processed\nInertial Measurement Unit (IMU) data and represented using Quaternion\nRepresentation to avoid the gimbal lock problem. The UAV's location is\ndetermined by combining data from the Global Positioning System (GPS), which\nprovides stable geographic coordinates but slower data update frequency, and\nthe accelerometer, which has higher data update frequency but integrating it to\nget position data is unstable due to its accumulative error. By combining data\nfrom these two sensors, the software is able to calculate and continuously\nupdate the UAV's real-time position during its flight operations. The result\nshows that the software effectively renders UAV orientation and position with\nhigh degree of accuracy and fluidity", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f4e\u6210\u672c\u4f20\u611f\u5668\u7684\u65e0\u4eba\u673a\u6570\u636e\u9a71\u52a8\u5efa\u6a21\u8f6f\u4ef6\uff0c\u901a\u8fc7\u4f20\u611f\u5668\u878d\u5408\u548c\u6ee4\u6ce2\u7b97\u6cd5\u63d0\u9ad8\u6570\u636e\u8d28\u91cf\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6a21\u578b\u53ef\u89c6\u5316\u3002", "motivation": "\u65e0\u4eba\u673a\u4f5c\u4e3a\u591a\u529f\u80fd\u5e73\u53f0\u9700\u8981\u7cbe\u786e\u5efa\u6a21\u652f\u6301\u5f00\u53d1\u6d4b\u8bd5\uff0c\u800c\u4f4e\u6210\u672c\u4f20\u611f\u5668\u548c\u9ad8\u6548\u6570\u636e\u5904\u7406\u662f\u5173\u952e\u3002", "method": "\u5229\u7528IMU\u6570\u636e\uff08\u56db\u5143\u6570\u8868\u793a\u907f\u514d\u4e07\u5411\u8282\u9501\uff09\u548cGPS\u4e0e\u52a0\u901f\u5ea6\u8ba1\u6570\u636e\u878d\u5408\uff08\u7ed3\u5408\u7a33\u5b9a\u5750\u6807\u4e0e\u9ad8\u9891\u66f4\u65b0\uff09\uff0c\u901a\u8fc7\u6ee4\u6ce2\u7b97\u6cd5\u548c\u4f20\u611f\u5668\u878d\u5408\u6280\u672f\u4f18\u5316\u6570\u636e\u3002", "result": "\u8f6f\u4ef6\u80fd\u9ad8\u7cbe\u5ea6\u3001\u6d41\u7545\u5730\u5b9e\u65f6\u6e32\u67d3\u65e0\u4eba\u673a\u7684\u65b9\u5411\u548c\u4f4d\u7f6e\u3002", "conclusion": "\u8be5\u8f6f\u4ef6\u901a\u8fc7\u4f4e\u6210\u672c\u4f20\u611f\u5668\u548c\u9ad8\u6548\u6570\u636e\u5904\u7406\uff0c\u5b9e\u73b0\u4e86\u65e0\u4eba\u673a\u5efa\u6a21\u7684\u9ad8\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u3002"}}
{"id": "2507.09469", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09469", "abs": "https://arxiv.org/abs/2507.09469", "authors": ["Haoyang Wang", "Jingao Xu", "Xinyu Luo", "Ting Zhang", "Xuecheng Chen", "Ruiyang Duan", "Jialong Chen", "Yunhao Liu", "Jianfeng Zheng", "Weijie Hong", "Xinlei Chen"], "title": "mmE-Loc: Facilitating Accurate Drone Landing with Ultra-High-Frequency Localization", "comment": "17 pages, 34 figures. arXiv admin note: substantial text overlap with\n  arXiv:2502.14992", "summary": "For precise, efficient, and safe drone landings, ground platforms should\nreal-time, accurately locate descending drones and guide them to designated\nspots. While mmWave sensing combined with cameras improves localization\naccuracy, lower sampling frequency of traditional frame cameras compared to\nmmWave radar creates bottlenecks in system throughput. In this work, we upgrade\ntraditional frame camera with event camera, a novel sensor that harmonizes in\nsampling frequency with mmWave radar within ground platform setup, and\nintroduce mmE-Loc, a high-precision, low-latency ground localization system\ndesigned for precise drone landings. To fully exploit the \\textit{temporal\nconsistency} and \\textit{spatial complementarity} between these two modalities,\nwe propose two innovative modules: \\textit{(i)} the Consistency-instructed\nCollaborative Tracking module, which further leverages the drone's physical\nknowledge of periodic micro-motions and structure for accurate measurements\nextraction, and \\textit{(ii)} the Graph-informed Adaptive Joint Optimization\nmodule, which integrates drone motion information for efficient sensor fusion\nand drone localization. Real-world experiments conducted in landing scenarios\nwith a drone delivery company demonstrate that mmE-Loc significantly\noutperforms state-of-the-art methods in both accuracy and latency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3ammE-Loc\u7684\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u5ef6\u8fdf\u5730\u9762\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u548c\u6beb\u7c73\u6ce2\u96f7\u8fbe\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u7cbe\u786e\u964d\u843d\u3002", "motivation": "\u4f20\u7edf\u5e27\u76f8\u673a\u91c7\u6837\u9891\u7387\u4f4e\uff0c\u4e0e\u6beb\u7c73\u6ce2\u96f7\u8fbe\u4e0d\u5339\u914d\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u91c7\u7528\u4e8b\u4ef6\u76f8\u673a\u4e0e\u6beb\u7c73\u6ce2\u96f7\u8fbe\u7ed3\u5408\uff0c\u63d0\u51fa\u4e24\u4e2a\u521b\u65b0\u6a21\u5757\uff1a\u4e00\u81f4\u6027\u534f\u4f5c\u8ddf\u8e2a\u548c\u56fe\u81ea\u9002\u5e94\u8054\u5408\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cmmE-Loc\u5728\u7cbe\u5ea6\u548c\u5ef6\u8fdf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "mmE-Loc\u7cfb\u7edf\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u4f18\u5316\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u65e0\u4eba\u673a\u7cbe\u786e\u964d\u843d\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09505", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09505", "abs": "https://arxiv.org/abs/2507.09505", "authors": ["Tenghui Xie", "Zhiying Song", "Fuxi Wen", "Jun Li", "Guangzhao Liu", "Zijian Zhao"], "title": "TruckV2X: A Truck-Centered Perception Dataset", "comment": null, "summary": "Autonomous trucking offers significant benefits, such as improved safety and\nreduced costs, but faces unique perception challenges due to trucks' large size\nand dynamic trailer movements. These challenges include extensive blind spots\nand occlusions that hinder the truck's perception and the capabilities of other\nroad users. To address these limitations, cooperative perception emerges as a\npromising solution. However, existing datasets predominantly feature light\nvehicle interactions or lack multi-agent configurations for heavy-duty vehicle\nscenarios. To bridge this gap, we introduce TruckV2X, the first large-scale\ntruck-centered cooperative perception dataset featuring multi-modal sensing\n(LiDAR and cameras) and multi-agent cooperation (tractors, trailers, CAVs, and\nRSUs). We further investigate how trucks influence collaborative perception\nneeds, establishing performance benchmarks while suggesting research priorities\nfor heavy vehicle perception. The dataset provides a foundation for developing\ncooperative perception systems with enhanced occlusion handling capabilities,\nand accelerates the deployment of multi-agent autonomous trucking systems. The\nTruckV2X dataset is available at\nhttps://huggingface.co/datasets/XieTenghu1/TruckV2X.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u4ee5\u5361\u8f66\u4e3a\u4e2d\u5fc3\u7684\u5927\u89c4\u6a21\u534f\u540c\u611f\u77e5\u6570\u636e\u96c6TruckV2X\uff0c\u65e8\u5728\u89e3\u51b3\u5361\u8f66\u611f\u77e5\u4e2d\u7684\u76f2\u533a\u548c\u906e\u6321\u95ee\u9898\uff0c\u5e76\u4fc3\u8fdb\u591a\u667a\u80fd\u4f53\u81ea\u52a8\u9a7e\u9a76\u5361\u8f66\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "motivation": "\u5361\u8f66\u81ea\u52a8\u9a7e\u9a76\u9762\u4e34\u72ec\u7279\u7684\u611f\u77e5\u6311\u6218\uff0c\u5982\u76f2\u533a\u548c\u52a8\u6001\u62d6\u8f66\u8fd0\u52a8\uff0c\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u9488\u5bf9\u91cd\u578b\u8f66\u8f86\u7684\u534f\u540c\u611f\u77e5\u914d\u7f6e\u3002", "method": "\u63d0\u51faTruckV2X\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6a21\u6001\u611f\u77e5\uff08LiDAR\u548c\u6444\u50cf\u5934\uff09\u548c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff08\u7275\u5f15\u8f66\u3001\u62d6\u8f66\u3001CAV\u548cRSU\uff09\u3002", "result": "\u6570\u636e\u96c6\u4e3a\u5f00\u53d1\u534f\u540c\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u589e\u5f3a\u4e86\u906e\u6321\u5904\u7406\u80fd\u529b\uff0c\u5e76\u52a0\u901f\u4e86\u591a\u667a\u80fd\u4f53\u81ea\u52a8\u9a7e\u9a76\u5361\u8f66\u7cfb\u7edf\u7684\u90e8\u7f72\u3002", "conclusion": "TruckV2X\u586b\u8865\u4e86\u91cd\u578b\u8f66\u8f86\u534f\u540c\u611f\u77e5\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u548c\u65b9\u5411\u3002"}}
{"id": "2507.09537", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09537", "abs": "https://arxiv.org/abs/2507.09537", "authors": ["Yangang Ren", "Guojian Zhan", "Chen Lv", "Jun Li", "Fenghua Liang", "Keqiang Li"], "title": "Self-supervised Pretraining for Integrated Prediction and Planning of Automated Vehicles", "comment": null, "summary": "Predicting the future of surrounding agents and accordingly planning a safe,\ngoal-directed trajectory are crucial for automated vehicles. Current methods\ntypically rely on imitation learning to optimize metrics against the ground\ntruth, often overlooking how scene understanding could enable more holistic\ntrajectories. In this paper, we propose Plan-MAE, a unified pretraining\nframework for prediction and planning that capitalizes on masked autoencoders.\nPlan-MAE fuses critical contextual understanding via three dedicated tasks:\nreconstructing masked road networks to learn spatial correlations, agent\ntrajectories to model social interactions, and navigation routes to capture\ndestination intents. To further align vehicle dynamics and safety constraints,\nwe incorporate a local sub-planning task predicting the ego-vehicle's near-term\ntrajectory segment conditioned on earlier segment. This pretrained model is\nsubsequently fine-tuned on downstream tasks to jointly generate the prediction\nand planning trajectories. Experiments on large-scale datasets demonstrate that\nPlan-MAE outperforms current methods on the planning metrics by a large margin\nand can serve as an important pre-training step for learning-based motion\nplanner.", "AI": {"tldr": "Plan-MAE\u662f\u4e00\u4e2a\u57fa\u4e8e\u63a9\u7801\u81ea\u7f16\u7801\u5668\u7684\u9884\u6d4b\u4e0e\u89c4\u5212\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u4efb\u52a1\u5b66\u4e60\u573a\u666f\u7406\u89e3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u8f68\u8ff9\u89c4\u5212\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u4f9d\u8d56\u6a21\u4eff\u5b66\u4e60\uff0c\u5ffd\u89c6\u4e86\u573a\u666f\u7406\u89e3\u5bf9\u751f\u6210\u66f4\u5168\u9762\u8f68\u8ff9\u7684\u6f5c\u529b\u3002Plan-MAE\u65e8\u5728\u901a\u8fc7\u9884\u8bad\u7ec3\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Plan-MAE\u901a\u8fc7\u4e09\u4e2a\u4efb\u52a1\uff08\u9053\u8def\u7f51\u7edc\u91cd\u5efa\u3001\u4ee3\u7406\u8f68\u8ff9\u5efa\u6a21\u3001\u5bfc\u822a\u8def\u7ebf\u6355\u6349\uff09\u5b66\u4e60\u573a\u666f\u7406\u89e3\uff0c\u5e76\u52a0\u5165\u5c40\u90e8\u5b50\u89c4\u5212\u4efb\u52a1\u5bf9\u9f50\u8f66\u8f86\u52a8\u6001\u3002\u9884\u8bad\u7ec3\u540e\u5fae\u8c03\u4e0b\u6e38\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPlan-MAE\u5728\u89c4\u5212\u6307\u6807\u4e0a\u5927\u5e45\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53ef\u4f5c\u4e3a\u5b66\u4e60\u578b\u8fd0\u52a8\u89c4\u5212\u5668\u7684\u91cd\u8981\u9884\u8bad\u7ec3\u6b65\u9aa4\u3002", "conclusion": "Plan-MAE\u901a\u8fc7\u573a\u666f\u7406\u89e3\u7684\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u9884\u6d4b\u4e0e\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2507.09538", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09538", "abs": "https://arxiv.org/abs/2507.09538", "authors": ["Zainab Ali", "Lujayn Al-Amir", "Ali Safa"], "title": "On the Importance of Neural Membrane Potential Leakage for LIDAR-based Robot Obstacle Avoidance using Spiking Neural Networks", "comment": null, "summary": "Using neuromorphic computing for robotics applications has gained much\nattention in recent year due to the remarkable ability of Spiking Neural\nNetworks (SNNs) for high-precision yet low memory and compute complexity\ninference when implemented in neuromorphic hardware. This ability makes SNNs\nwell-suited for autonomous robot applications (such as in drones and rovers)\nwhere battery resources and payload are typically limited. Within this context,\nthis paper studies the use of SNNs for performing direct robot navigation and\nobstacle avoidance from LIDAR data. A custom robot platform equipped with a\nLIDAR is set up for collecting a labeled dataset of LIDAR sensing data together\nwith the human-operated robot control commands used for obstacle avoidance.\nCrucially, this paper provides what is, to the best of our knowledge, a first\nfocused study about the importance of neuron membrane leakage on the SNN\nprecision when processing LIDAR data for obstacle avoidance. It is shown that\nby carefully tuning the membrane potential leakage constant of the spiking\nLeaky Integrate-and-Fire (LIF) neurons used within our SNN, it is possible to\nachieve on-par robot control precision compared to the use of a non-spiking\nConvolutional Neural Network (CNN). Finally, the LIDAR dataset collected during\nthis work is released as open-source with the hope of benefiting future\nresearch.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u7684\u673a\u5668\u4eba\u5bfc\u822a\u4e0e\u907f\u969c\uff0c\u91cd\u70b9\u63a2\u8ba8\u4e86\u795e\u7ecf\u5143\u819c\u6cc4\u6f0f\u5bf9SNN\u5904\u7406LIDAR\u6570\u636e\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4e0eCNN\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "motivation": "\u7531\u4e8eSNN\u5728\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4e2d\u5177\u6709\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u5185\u5b58\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u4f18\u52bf\uff0c\u9002\u5408\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u5e94\u7528\uff08\u5982\u65e0\u4eba\u673a\u548c\u6f2b\u6e38\u8f66\uff09\u3002", "method": "\u642d\u5efa\u4e86\u914d\u5907LIDAR\u7684\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u6536\u96c6\u5e26\u6807\u7b7e\u7684LIDAR\u6570\u636e\u53ca\u4eba\u5de5\u64cd\u4f5c\u7684\u63a7\u5236\u547d\u4ee4\uff0c\u7814\u7a76\u4e86SNN\u4e2d\u795e\u7ecf\u5143\u819c\u6cc4\u6f0f\u5bf9\u907f\u969c\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u901a\u8fc7\u8c03\u6574LIF\u795e\u7ecf\u5143\u7684\u819c\u7535\u4f4d\u6cc4\u6f0f\u5e38\u6570\uff0cSNN\u5728\u907f\u969c\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u4e0eCNN\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u795e\u7ecf\u5143\u819c\u6cc4\u6f0f\u5bf9SNN\u5904\u7406LIDAR\u6570\u636e\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5f00\u6e90\u4e86LIDAR\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2507.09714", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09714", "abs": "https://arxiv.org/abs/2507.09714", "authors": ["Yifan Zeng", "Yihan Li", "Suiyi He", "Koushil Sreenath", "Jun Zeng"], "title": "IteraOptiRacing: A Unified Planning-Control Framework for Real-time Autonomous Racing for Iterative Optimal Performance", "comment": null, "summary": "This paper presents a unified planning-control strategy for competing with\nother racing cars called IteraOptiRacing in autonomous racing environments.\nThis unified strategy is proposed based on Iterative Linear Quadratic Regulator\nfor Iterative Tasks (i2LQR), which can improve lap time performance in the\npresence of surrounding racing obstacles. By iteratively using the ego car's\nhistorical data, both obstacle avoidance for multiple moving cars and time cost\noptimization are considered in this unified strategy, resulting in\ncollision-free and time-optimal generated trajectories. The algorithm's\nconstant low computation burden and suitability for parallel computing enable\nreal-time operation in competitive racing scenarios. To validate its\nperformance, simulations in a high-fidelity simulator are conducted with\nmultiple randomly generated dynamic agents on the track. Results show that the\nproposed strategy outperforms existing methods across all randomly generated\nautonomous racing scenarios, enabling enhanced maneuvering for the ego racing\ncar.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ei2LQR\u7684\u7edf\u4e00\u89c4\u5212\u63a7\u5236\u7b56\u7565IteraOptiRacing\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u73af\u5883\u4e2d\u4e0e\u5176\u4ed6\u8d5b\u8f66\u7ade\u4e89\uff0c\u4f18\u5316\u5708\u901f\u5e76\u907f\u514d\u78b0\u649e\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u73af\u5883\u4e2d\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4f18\u5316\u5708\u901f\u53c8\u80fd\u907f\u514d\u4e0e\u52a8\u6001\u969c\u788d\u7269\u78b0\u649e\u7684\u7b56\u7565\u3002", "method": "\u57fa\u4e8ei2LQR\u7684\u7edf\u4e00\u7b56\u7565\uff0c\u5229\u7528\u5386\u53f2\u6570\u636e\u8fed\u4ee3\u4f18\u5316\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4f4e\u8ba1\u7b97\u8d1f\u62c5\u548c\u5e76\u884c\u8ba1\u7b97\u80fd\u529b\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b56\u7565\u5728\u968f\u673a\u751f\u6210\u7684\u52a8\u6001\u73af\u5883\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u65e0\u78b0\u649e\u4e14\u65f6\u95f4\u6700\u4f18\u7684\u8f68\u8ff9\u3002", "conclusion": "IteraOptiRacing\u7b56\u7565\u5728\u5b9e\u65f6\u6027\u548c\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u7ade\u4e89\u6027\u8d5b\u8f66\u573a\u666f\u3002"}}
{"id": "2507.09725", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09725", "abs": "https://arxiv.org/abs/2507.09725", "authors": ["Gabriel G. Gattaux", "Julien R. Serres", "Franck Ruffier", "Antoine Wystrach"], "title": "Visual Homing in Outdoor Robots Using Mushroom Body Circuits and Learning Walks", "comment": "Published by Springer Nature with the 14th bioinspired and biohybrid\n  systems conference in Sheffield, and presented at the conference in July 2025", "summary": "Ants achieve robust visual homing with minimal sensory input and only a few\nlearning walks, inspiring biomimetic solutions for autonomous navigation. While\nMushroom Body (MB) models have been used in robotic route following, they have\nnot yet been applied to visual homing. We present the first real-world\nimplementation of a lateralized MB architecture for visual homing onboard a\ncompact autonomous car-like robot. We test whether the sign of the angular path\nintegration (PI) signal can categorize panoramic views, acquired during\nlearning walks and encoded in the MB, into \"goal on the left\" and \"goal on the\nright\" memory banks, enabling robust homing in natural outdoor settings. We\nvalidate this approach through four incremental experiments: (1) simulation\nshowing attractor-like nest dynamics; (2) real-world homing after decoupled\nlearning walks, producing nest search behavior; (3) homing after random walks\nusing noisy PI emulated with GPS-RTK; and (4) precise stopping-at-the-goal\nbehavior enabled by a fifth MB Output Neuron (MBON) encoding goal-views to\ncontrol velocity. This mimics the accurate homing behavior of ants and\nfunctionally resembles waypoint-based position control in robotics, despite\nrelying solely on visual input. Operating at 8 Hz on a Raspberry Pi 4 with\n32x32 pixel views and a memory footprint under 9 kB, our system offers a\nbiologically grounded, resource-efficient solution for autonomous visual\nhoming.", "AI": {"tldr": "\u8682\u8681\u901a\u8fc7\u5c11\u91cf\u89c6\u89c9\u8f93\u5165\u548c\u5b66\u4e60\u884c\u8d70\u5b9e\u73b0\u7a33\u5065\u7684\u89c6\u89c9\u5f52\u5de2\uff0c\u542f\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u8611\u83c7\u4f53\uff08MB\uff09\u67b6\u6784\u7684\u81ea\u4e3b\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8682\u8681\u7684\u89c6\u89c9\u5f52\u5de2\u884c\u4e3a\u5177\u6709\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u751f\u7269\u542f\u53d1\u3002\u7814\u7a76\u65e8\u5728\u5c06\u8611\u83c7\u4f53\u6a21\u578b\u9996\u6b21\u5e94\u7528\u4e8e\u89c6\u89c9\u5f52\u5de2\uff0c\u9a8c\u8bc1\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u53ef\u884c\u6027\u3002", "method": "\u91c7\u7528\u4fa7\u5411\u5316\u8611\u83c7\u4f53\u67b6\u6784\uff0c\u901a\u8fc7\u89d2\u8def\u5f84\u79ef\u5206\u4fe1\u53f7\u5206\u7c7b\u5168\u666f\u89c6\u56fe\uff0c\u5206\u4e3a\u201c\u76ee\u6807\u5728\u5de6\u201d\u548c\u201c\u76ee\u6807\u5728\u53f3\u201d\u8bb0\u5fc6\u5e93\u3002\u901a\u8fc7\u56db\u4e2a\u5b9e\u9a8c\u9010\u6b65\u9a8c\u8bc1\uff1a\u4eff\u771f\u3001\u89e3\u8026\u5b66\u4e60\u884c\u8d70\u5f52\u5de2\u3001\u968f\u673a\u884c\u8d70\u5f52\u5de2\u548c\u7cbe\u786e\u505c\u6b62\u884c\u4e3a\u3002", "result": "\u7cfb\u7edf\u5728\u81ea\u7136\u6237\u5916\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u89c6\u89c9\u5f52\u5de2\uff0c\u5185\u5b58\u5360\u7528\u4f4e\u4e8e9 kB\uff0c\u8fd0\u884c\u9891\u7387\u4e3a8 Hz\uff0c\u529f\u80fd\u4e0a\u7c7b\u4f3c\u4e8e\u673a\u5668\u4eba\u4e2d\u7684\u57fa\u4e8e\u8def\u70b9\u7684\u4f4d\u7f6e\u63a7\u5236\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u7269\u5b66\u7684\u8d44\u6e90\u9ad8\u6548\u89c6\u89c9\u5f52\u5de2\u89e3\u51b3\u65b9\u6848\uff0c\u9a8c\u8bc1\u4e86\u8611\u83c7\u4f53\u67b6\u6784\u5728\u81ea\u4e3b\u5bfc\u822a\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.09822", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09822", "abs": "https://arxiv.org/abs/2507.09822", "authors": ["Darshan Gadginmath", "Farhad Nawaz", "Minjun Sung", "Faizan M Tariq", "Sangjae Bae", "David Isele", "Fabio Pasqualetti", "Jovin Dsa"], "title": "Active Probing with Multimodal Predictions for Motion Planning", "comment": "To appear at IROS '25. 8 pages. 3 tables. 6 figures", "summary": "Navigation in dynamic environments requires autonomous systems to reason\nabout uncertainties in the behavior of other agents. In this paper, we\nintroduce a unified framework that combines trajectory planning with multimodal\npredictions and active probing to enhance decision-making under uncertainty. We\ndevelop a novel risk metric that seamlessly integrates multimodal prediction\nuncertainties through mixture models. When these uncertainties follow a\nGaussian mixture distribution, we prove that our risk metric admits a\nclosed-form solution, and is always finite, thus ensuring analytical\ntractability. To reduce prediction ambiguity, we incorporate an active probing\nmechanism that strategically selects actions to improve its estimates of\nbehavioral parameters of other agents, while simultaneously handling multimodal\nuncertainties. We extensively evaluate our framework in autonomous navigation\nscenarios using the MetaDrive simulation environment. Results demonstrate that\nour active probing approach successfully navigates complex traffic scenarios\nwith uncertain predictions. Additionally, our framework shows robust\nperformance across diverse traffic agent behavior models, indicating its broad\napplicability to real-world autonomous navigation challenges. Code and videos\nare available at\nhttps://darshangm.github.io/papers/active-probing-multimodal-predictions/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8f68\u8ff9\u89c4\u5212\u3001\u591a\u6a21\u6001\u9884\u6d4b\u548c\u4e3b\u52a8\u63a2\u6d4b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u52a8\u6001\u73af\u5883\u4e2d\u5bfc\u822a\u9700\u8981\u5904\u7406\u5176\u4ed6\u4ee3\u7406\u884c\u4e3a\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u9884\u6d4b\u548c\u4e3b\u52a8\u63a2\u6d4b\u7684\u7ed3\u5408\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u98ce\u9669\u5ea6\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u6a21\u578b\u6574\u5408\u591a\u6a21\u6001\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5f15\u5165\u4e3b\u52a8\u63a2\u6d4b\u673a\u5236\u4ee5\u51cf\u5c11\u9884\u6d4b\u6a21\u7cca\u6027\u3002", "result": "\u5728MetaDrive\u4eff\u771f\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u6846\u67b6\u80fd\u6210\u529f\u5904\u7406\u590d\u6742\u4ea4\u901a\u573a\u666f\uff0c\u5e76\u5728\u591a\u79cd\u884c\u4e3a\u6a21\u578b\u4e0b\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u6311\u6218\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u9002\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09836", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09836", "abs": "https://arxiv.org/abs/2507.09836", "authors": ["Vindula Jayawardana", "Sirui Li", "Yashar Farid", "Cathy Wu"], "title": "Multi-residual Mixture of Experts Learning for Cooperative Control in Multi-vehicle Systems", "comment": null, "summary": "Autonomous vehicles (AVs) are becoming increasingly popular, with their\napplications now extending beyond just a mode of transportation to serving as\nmobile actuators of a traffic flow to control flow dynamics. This contrasts\nwith traditional fixed-location actuators, such as traffic signals, and is\nreferred to as Lagrangian traffic control. However, designing effective\nLagrangian traffic control policies for AVs that generalize across traffic\nscenarios introduces a major challenge. Real-world traffic environments are\nhighly diverse, and developing policies that perform robustly across such\ndiverse traffic scenarios is challenging. It is further compounded by the joint\ncomplexity of the multi-agent nature of traffic systems, mixed motives among\nparticipants, and conflicting optimization objectives subject to strict\nphysical and external constraints. To address these challenges, we introduce\nMulti-Residual Mixture of Expert Learning (MRMEL), a novel framework for\nLagrangian traffic control that augments a given suboptimal nominal policy with\na learned residual while explicitly accounting for the structure of the traffic\nscenario space. In particular, taking inspiration from residual reinforcement\nlearning, MRMEL augments a suboptimal nominal AV control policy by learning a\nresidual correction, but at the same time dynamically selects the most suitable\nnominal policy from a pool of nominal policies conditioned on the traffic\nscenarios and modeled as a mixture of experts. We validate MRMEL using a case\nstudy in cooperative eco-driving at signalized intersections in Atlanta, Dallas\nFort Worth, and Salt Lake City, with real-world data-driven traffic scenarios.\nThe results show that MRMEL consistently yields superior performance-achieving\nan additional 4%-9% reduction in aggregate vehicle emissions relative to the\nstrongest baseline in each setting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMRMEL\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u8ba1\u901a\u7528\u7684\u62c9\u683c\u6717\u65e5\u4ea4\u901a\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6700\u4f18\u7b56\u7565\u5e76\u5b66\u4e60\u6b8b\u5dee\u4fee\u6b63\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u591a\u6837\u5316\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4f5c\u4e3a\u79fb\u52a8\u6267\u884c\u5668\u5728\u4ea4\u901a\u6d41\u63a7\u5236\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4f20\u7edf\u56fa\u5b9a\u6267\u884c\u5668\uff08\u5982\u4ea4\u901a\u4fe1\u53f7\uff09\u65e0\u6cd5\u9002\u5e94\u591a\u6837\u5316\u4ea4\u901a\u573a\u666f\uff0c\u8bbe\u8ba1\u901a\u7528\u7684\u63a7\u5236\u7b56\u7565\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51faMRMEL\u6846\u67b6\uff0c\u7ed3\u5408\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u548c\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff0c\u52a8\u6001\u9009\u62e9\u6700\u4f18\u7b56\u7565\u5e76\u5b66\u4e60\u6b8b\u5dee\u4fee\u6b63\u3002", "result": "\u5728\u771f\u5b9e\u4ea4\u901a\u573a\u666f\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cMRMEL\u5b9e\u73b0\u4e86\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u989d\u59164%-9%\u7684\u8f66\u8f86\u6392\u653e\u51cf\u5c11\u3002", "conclusion": "MRMEL\u901a\u8fc7\u52a8\u6001\u7b56\u7565\u9009\u62e9\u548c\u6b8b\u5dee\u4fee\u6b63\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u591a\u6837\u5316\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u63a7\u5236\u6027\u80fd\u3002"}}
{"id": "2507.09857", "categories": ["cs.RO", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.09857", "abs": "https://arxiv.org/abs/2507.09857", "authors": ["Xiaofei Wang", "Mingliang Han", "Tianyu Hao", "Cegang Li", "Yunbo Zhao", "Keke Tang"], "title": "AdvGrasp: Adversarial Attacks on Robotic Grasping from a Physical Perspective", "comment": "IJCAI'2025", "summary": "Adversarial attacks on robotic grasping provide valuable insights into\nevaluating and improving the robustness of these systems. Unlike studies that\nfocus solely on neural network predictions while overlooking the physical\nprinciples of grasping, this paper introduces AdvGrasp, a framework for\nadversarial attacks on robotic grasping from a physical perspective.\nSpecifically, AdvGrasp targets two core aspects: lift capability, which\nevaluates the ability to lift objects against gravity, and grasp stability,\nwhich assesses resistance to external disturbances. By deforming the object's\nshape to increase gravitational torque and reduce stability margin in the\nwrench space, our method systematically degrades these two key grasping\nmetrics, generating adversarial objects that compromise grasp performance.\nExtensive experiments across diverse scenarios validate the effectiveness of\nAdvGrasp, while real-world validations demonstrate its robustness and practical\napplicability", "AI": {"tldr": "AdvGrasp\u6846\u67b6\u4ece\u7269\u7406\u89d2\u5ea6\u5bf9\u673a\u5668\u4eba\u6293\u53d6\u8fdb\u884c\u5bf9\u6297\u653b\u51fb\uff0c\u901a\u8fc7\u53d8\u5f62\u7269\u4f53\u5f62\u72b6\u964d\u4f4e\u6293\u53d6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u800c\u5ffd\u7565\u6293\u53d6\u7684\u7269\u7406\u539f\u7406\uff0cAdvGrasp\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u53d8\u5f62\u7269\u4f53\u5f62\u72b6\u589e\u52a0\u91cd\u529b\u626d\u77e9\u5e76\u964d\u4f4e\u7a33\u5b9a\u6027\uff0c\u7cfb\u7edf\u6027\u5730\u524a\u5f31\u6293\u53d6\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86AdvGrasp\u7684\u6709\u6548\u6027\uff0c\u5b9e\u9645\u5e94\u7528\u5c55\u793a\u4e86\u5176\u9c81\u68d2\u6027\u3002", "conclusion": "AdvGrasp\u4e3a\u673a\u5668\u4eba\u6293\u53d6\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.09858", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09858", "abs": "https://arxiv.org/abs/2507.09858", "authors": ["Shuaikang Wang", "Tiecheng Guo", "Meng Guo"], "title": "Customize Harmonic Potential Fields via Hybrid Optimization over Homotopic Paths", "comment": "accepted to IEEE RA-L", "summary": "Safe navigation within a workspace is a fundamental skill for autonomous\nrobots to accomplish more complex tasks. Harmonic potentials are artificial\npotential fields that are analytical, globally convergent and provably free of\nlocal minima. Thus, it has been widely used for generating safe and reliable\nrobot navigation control policies. However, most existing methods do not allow\ncustomization of the harmonic potential fields nor the resulting paths,\nparticularly regarding their topological properties. In this paper, we propose\na novel method that automatically finds homotopy classes of paths that can be\ngenerated by valid harmonic potential fields. The considered complex workspaces\ncan be as general as forest worlds consisting of numerous overlapping\nstar-obstacles. The method is based on a hybrid optimization algorithm that\nsearches over homotopy classes, selects the structure of each tree-of-stars\nwithin the forest, and optimizes over the continuous weight parameters for each\npurged tree via the projected gradient descent. The key insight is to transform\nthe forest world to the unbounded point world via proper diffeomorphic\ntransformations. It not only facilitates a simpler design of the\nmulti-directional D-signature between non-homotopic paths, but also retain the\nsafety and convergence properties. Extensive simulations and hardware\nexperiments are conducted for non-trivial scenarios, where the navigation\npotentials are customized for desired homotopic properties. Project page:\nhttps://shuaikang-wang.github.io/CustFields.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u4f18\u5316\u7b97\u6cd5\u81ea\u52a8\u751f\u6210\u5177\u6709\u7279\u5b9a\u62d3\u6251\u7279\u6027\u7684\u8c10\u6ce2\u52bf\u573a\u8def\u5f84\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5de5\u4f5c\u7a7a\u95f4\u3002", "motivation": "\u73b0\u6709\u8c10\u6ce2\u52bf\u573a\u65b9\u6cd5\u96be\u4ee5\u81ea\u5b9a\u4e49\u8def\u5f84\u7684\u62d3\u6251\u7279\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4f18\u5316\u7b97\u6cd5\uff0c\u641c\u7d22\u8def\u5f84\u7684\u540c\u4f26\u7c7b\uff0c\u5e76\u901a\u8fc7\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u6743\u91cd\u53c2\u6570\uff0c\u540c\u65f6\u5229\u7528\u5fae\u5206\u540c\u80da\u53d8\u6362\u7b80\u5316\u8bbe\u8ba1\u3002", "result": "\u65b9\u6cd5\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u80fd\u591f\u751f\u6210\u6ee1\u8db3\u7279\u5b9a\u540c\u4f26\u7279\u6027\u7684\u5bfc\u822a\u52bf\u573a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09985", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09985", "abs": "https://arxiv.org/abs/2507.09985", "authors": ["Samson Yu", "Kelvin Lin", "Harold Soh"], "title": "Demonstrating the Octopi-1.5 Visual-Tactile-Language Model", "comment": "Published at R:SS 2025", "summary": "Touch is recognized as a vital sense for humans and an equally important\nmodality for robots, especially for dexterous manipulation, material\nidentification, and scenarios involving visual occlusion. Building upon very\nrecent work in touch foundation models, this demonstration will feature\nOctopi-1.5, our latest visual-tactile-language model. Compared to its\npredecessor, Octopi-1.5 introduces the ability to process tactile signals from\nmultiple object parts and employs a simple retrieval-augmented generation (RAG)\nmodule to improve performance on tasks and potentially learn new objects\non-the-fly. The system can be experienced live through a new handheld\ntactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile\nsensors. This convenient and accessible setup allows users to interact with\nOctopi-1.5 without requiring a robot. During the demonstration, we will\nshowcase Octopi-1.5 solving tactile inference tasks by leveraging tactile\ninputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5\nwill identify objects being grasped and respond to follow-up queries about how\nto handle it (e.g., recommending careful handling for soft fruits). We also\nplan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items.\nWith live interactions, this demonstration aims to highlight both the progress\nand limitations of VTLMs such as Octopi-1.5 and to foster further interest in\nthis exciting field. Code for Octopi-1.5 and design files for the TMI gripper\nare available at https://github.com/clear-nus/octopi-1.5.", "AI": {"tldr": "Octopi-1.5\u662f\u4e00\u4e2a\u6700\u65b0\u7684\u89c6\u89c9-\u89e6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u5904\u7406\u591a\u90e8\u5206\u89e6\u89c9\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6a21\u5757\u63d0\u5347\u4efb\u52a1\u6027\u80fd\uff0c\u652f\u6301\u5b9e\u65f6\u5b66\u4e60\u65b0\u7269\u4f53\u3002", "motivation": "\u89e6\u89c9\u5bf9\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u5728\u7075\u5de7\u64cd\u4f5c\u3001\u6750\u6599\u8bc6\u522b\u548c\u89c6\u89c9\u906e\u6321\u573a\u666f\u4e2d\u3002Octopi-1.5\u65e8\u5728\u63d0\u5347\u89e6\u89c9\u63a8\u7406\u80fd\u529b\u5e76\u63a8\u52a8\u89c6\u89c9-\u89e6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VTLM\uff09\u7684\u53d1\u5c55\u3002", "method": "Octopi-1.5\u6269\u5c55\u4e86\u524d\u4ee3\u6a21\u578b\u529f\u80fd\uff0c\u652f\u6301\u591a\u90e8\u5206\u89e6\u89c9\u4fe1\u53f7\u5904\u7406\uff0c\u5e76\u5f15\u5165RAG\u6a21\u5757\u4f18\u5316\u4efb\u52a1\u6027\u80fd\u3002\u901a\u8fc7\u624b\u6301\u89e6\u89c9\u63a5\u53e3TMI\uff08\u914d\u5907GelSight\u548cTAC-02\u4f20\u611f\u5668\uff09\u5b9e\u73b0\u7528\u6237\u4ea4\u4e92\u3002", "result": "\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u89e6\u89c9\u8f93\u5165\u548c\u5e38\u8bc6\u77e5\u8bc6\u89e3\u51b3\u63a8\u7406\u4efb\u52a1\uff0c\u4f8b\u5982\u8bc6\u522b\u6293\u53d6\u7269\u4f53\u5e76\u63a8\u8350\u5904\u7406\u65b9\u5f0f\u3002RAG\u6a21\u5757\u8fd8\u652f\u6301\u5b9e\u65f6\u5b66\u4e60\u65b0\u7269\u4f53\u3002", "conclusion": "Octopi-1.5\u5c55\u793a\u4e86VTLM\u7684\u8fdb\u5c55\u4e0e\u6f5c\u529b\uff0c\u540c\u65f6\u901a\u8fc7\u5f00\u6e90\u4ee3\u7801\u548c\u8bbe\u8ba1\u6587\u4ef6\u4fc3\u8fdb\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2507.10003", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10003", "abs": "https://arxiv.org/abs/2507.10003", "authors": ["Mohit Singh", "Mihir Dharmadhikari", "Kostas Alexis"], "title": "Ariel Explores: Vision-based underwater exploration and inspection via generalist drone-level autonomy", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "This work presents a vision-based underwater exploration and inspection\nautonomy solution integrated into Ariel, a custom vision-driven underwater\nrobot. Ariel carries a $5$ camera and IMU based sensing suite, enabling a\nrefraction-aware multi-camera visual-inertial state estimation method aided by\na learning-based proprioceptive robot velocity prediction method that enhances\nrobustness against visual degradation. Furthermore, our previously developed\nand extensively field-verified autonomous exploration and general visual\ninspection solution is integrated on Ariel, providing aerial drone-level\nautonomy underwater. The proposed system is field-tested in a submarine dry\ndock in Trondheim under challenging visual conditions. The field demonstration\nshows the robustness of the state estimation solution and the generalizability\nof the path planning techniques across robot embodiments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u6c34\u4e0b\u63a2\u7d22\u548c\u68c0\u67e5\u81ea\u4e3b\u89e3\u51b3\u65b9\u6848\uff0c\u96c6\u6210\u5728Ariel\u6c34\u4e0b\u673a\u5668\u4eba\u4e2d\uff0c\u901a\u8fc7\u591a\u6444\u50cf\u5934\u89c6\u89c9-\u60ef\u6027\u72b6\u6001\u4f30\u8ba1\u548c\u5b66\u4e60\u578b\u673a\u5668\u4eba\u901f\u5ea6\u9884\u6d4b\u65b9\u6cd5\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u6f5c\u8247\u5e72\u8239\u575e\u4e2d\u8fdb\u884c\u4e86\u5b9e\u5730\u6d4b\u8bd5\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u89c6\u89c9\u6761\u4ef6\u6076\u52a3\u7684\u6c34\u4e0b\u73af\u5883\u4e2d\u7a33\u5b9a\u8fd0\u884c\u7684\u81ea\u4e3b\u63a2\u7d22\u548c\u68c0\u67e5\u7cfb\u7edf\uff0c\u4ee5\u63d0\u5347\u6c34\u4e0b\u673a\u5668\u4eba\u7684\u81ea\u4e3b\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u91c7\u7528\u591a\u6444\u50cf\u5934\u89c6\u89c9-\u60ef\u6027\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408\u5b66\u4e60\u578b\u673a\u5668\u4eba\u901f\u5ea6\u9884\u6d4b\u6280\u672f\uff0c\u589e\u5f3a\u7cfb\u7edf\u5bf9\u89c6\u89c9\u9000\u5316\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u5730\u6d4b\u8bd5\u8868\u660e\uff0c\u72b6\u6001\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\u5177\u6709\u9c81\u68d2\u6027\uff0c\u8def\u5f84\u89c4\u5212\u6280\u672f\u5728\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5177\u6709\u901a\u7528\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u6c34\u4e0b\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u6c34\u4e0b\u81ea\u4e3b\u63a2\u7d22\u548c\u68c0\u67e5\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2507.10030", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10030", "abs": "https://arxiv.org/abs/2507.10030", "authors": ["Marco Cal\u00ec", "Alberto Sinigaglia", "Niccol\u00f2 Turcato", "Ruggero Carli", "Gian Antonio Susto"], "title": "Finetuning Deep Reinforcement Learning Policies with Evolutionary Strategies for Control of Underactuated Robots", "comment": null, "summary": "Deep Reinforcement Learning (RL) has emerged as a powerful method for\naddressing complex control problems, particularly those involving underactuated\nrobotic systems. However, in some cases, policies may require refinement to\nachieve optimal performance and robustness aligned with specific task\nobjectives. In this paper, we propose an approach for fine-tuning Deep RL\npolicies using Evolutionary Strategies (ES) to enhance control performance for\nunderactuated robots. Our method involves initially training an RL agent with\nSoft-Actor Critic (SAC) using a surrogate reward function designed to\napproximate complex specific scoring metrics. We subsequently refine this\nlearned policy through a zero-order optimization step employing the Separable\nNatural Evolution Strategy (SNES), directly targeting the original score.\nExperimental evaluations conducted in the context of the 2nd AI Olympics with\nRealAIGym at IROS 2024 demonstrate that our evolutionary fine-tuning\nsignificantly improves agent performance while maintaining high robustness. The\nresulting controllers outperform established baselines, achieving competitive\nscores for the competition tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u8fdb\u5316\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u6b20\u9a71\u52a8\u673a\u5668\u4eba\u7684\u63a7\u5236\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u63a7\u5236\u95ee\u9898\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7b56\u7565\u53ef\u80fd\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u5b9e\u73b0\u7279\u5b9a\u4efb\u52a1\u76ee\u6807\u3002", "method": "\u5148\u7528SAC\u8bad\u7ec3\u4ee3\u7406\uff0c\u518d\u7528SNES\u8fdb\u5316\u7b56\u7565\u76f4\u63a5\u4f18\u5316\u539f\u59cb\u8bc4\u5206\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fdb\u5316\u5fae\u8c03\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u63a7\u5236\u5668\u4f18\u4e8e\u57fa\u7ebf\uff0c\u8fbe\u5230\u7ade\u8d5b\u4efb\u52a1\u7684\u9ad8\u5206\u3002", "conclusion": "\u7ed3\u5408RL\u548cES\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u6b20\u9a71\u52a8\u673a\u5668\u4eba\u63a7\u5236\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.10047", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10047", "abs": "https://arxiv.org/abs/2507.10047", "authors": ["Marc Kaufeld", "Mattia Piccinini", "Johannes Betz"], "title": "MP-RBFN: Learning-based Vehicle Motion Primitives using Radial Basis Function Networks", "comment": "8 pages, Submitted to the IEEE International Conference on\n  Intelligent Transportation Systems (ITSC 2025), Australia", "summary": "This research introduces MP-RBFN, a novel formulation leveraging Radial Basis\nFunction Networks for efficiently learning Motion Primitives derived from\noptimal control problems for autonomous driving. While traditional motion\nplanning approaches based on optimization are highly accurate, they are often\ncomputationally prohibitive. In contrast, sampling-based methods demonstrate\nhigh performance but impose constraints on the geometric shape of trajectories.\nMP-RBFN combines the strengths of both by coupling the high-fidelity trajectory\ngeneration of sampling-based methods with an accurate description of vehicle\ndynamics. Empirical results show compelling performance compared to previous\nmethods, achieving a precise description of motion primitives at low inference\ntimes. MP-RBFN yields a seven times higher accuracy in generating optimized\nmotion primitives compared to existing semi-analytic approaches. We demonstrate\nthe practical applicability of MP-RBFN for motion planning by integrating the\nmethod into a sampling-based trajectory planner. MP-RBFN is available as\nopen-source software at https://github.com/TUM-AVS/RBFN-Motion-Primitives.", "AI": {"tldr": "MP-RBFN\u662f\u4e00\u79cd\u57fa\u4e8e\u5f84\u5411\u57fa\u51fd\u6570\u7f51\u7edc\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u5b66\u4e60\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8fd0\u52a8\u57fa\u5143\uff0c\u7ed3\u5408\u4e86\u91c7\u6837\u65b9\u6cd5\u7684\u9ad8\u6027\u80fd\u548c\u4f18\u5316\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff08\u57fa\u4e8e\u4f18\u5316\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u57fa\u4e8e\u91c7\u6837\u7684\u65b9\u6cd5\u867d\u9ad8\u6548\u4f46\u5bf9\u8f68\u8ff9\u51e0\u4f55\u5f62\u72b6\u6709\u9650\u5236\u3002MP-RBFN\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "MP-RBFN\u5229\u7528\u5f84\u5411\u57fa\u51fd\u6570\u7f51\u7edc\uff0c\u5c06\u91c7\u6837\u65b9\u6cd5\u7684\u9ad8\u4fdd\u771f\u8f68\u8ff9\u751f\u6210\u4e0e\u8f66\u8f86\u52a8\u529b\u5b66\u7684\u7cbe\u786e\u63cf\u8ff0\u76f8\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMP-RBFN\u5728\u751f\u6210\u4f18\u5316\u8fd0\u52a8\u57fa\u5143\u65f6\u7684\u7cbe\u5ea6\u6bd4\u73b0\u6709\u534a\u89e3\u6790\u65b9\u6cd5\u9ad87\u500d\uff0c\u4e14\u63a8\u7406\u65f6\u95f4\u77ed\u3002", "conclusion": "MP-RBFN\u5728\u8fd0\u52a8\u89c4\u5212\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5df2\u5f00\u6e90\u5e76\u96c6\u6210\u5230\u91c7\u6837\u8f68\u8ff9\u89c4\u5212\u5668\u4e2d\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.10055", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10055", "abs": "https://arxiv.org/abs/2507.10055", "authors": ["Muhtadin", "I Wayan Agus Darmawan", "Muhammad Hilmi Rusydiansyah", "I Ketut Eddy Purnama", "Chastine Fatichah", "Mauridhi Hery Purnomo"], "title": "Hand Gesture Recognition for Collaborative Robots Using Lightweight Deep Learning in Real-Time Robotic Systems", "comment": null, "summary": "Direct and natural interaction is essential for intuitive human-robot\ncollaboration, eliminating the need for additional devices such as joysticks,\ntablets, or wearable sensors. In this paper, we present a lightweight deep\nlearning-based hand gesture recognition system that enables humans to control\ncollaborative robots naturally and efficiently. This model recognizes eight\ndistinct hand gestures with only 1,103 parameters and a compact size of 22 KB,\nachieving an accuracy of 93.5%. To further optimize the model for real-world\ndeployment on edge devices, we applied quantization and pruning using\nTensorFlow Lite, reducing the final model size to just 7 KB. The system was\nsuccessfully implemented and tested on a Universal Robot UR5 collaborative\nrobot within a real-time robotic framework based on ROS2. The results\ndemonstrate that even extremely lightweight models can deliver accurate and\nresponsive hand gesture-based control for collaborative robots, opening new\npossibilities for natural human-robot interaction in constrained environments.", "AI": {"tldr": "\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u7136\u63a7\u5236\u534f\u4f5c\u673a\u5668\u4eba\uff0c\u6a21\u578b\u4ec51,103\u53c2\u6570\uff0c22KB\u5927\u5c0f\uff0c\u51c6\u786e\u738793.5%\u3002\u901a\u8fc7\u91cf\u5316\u548c\u526a\u679d\u4f18\u5316\u81f37KB\uff0c\u6210\u529f\u5728UR5\u673a\u5668\u4eba\u4e0a\u5b9e\u65f6\u6d4b\u8bd5\u3002", "motivation": "\u5b9e\u73b0\u76f4\u63a5\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\uff0c\u907f\u514d\u4f7f\u7528\u989d\u5916\u8bbe\u5907\u5982\u64cd\u7eb5\u6746\u6216\u4f20\u611f\u5668\u3002", "method": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\uff0c\u91c7\u7528\u91cf\u5316\u548c\u526a\u679d\u6280\u672f\u4f18\u5316\u6a21\u578b\u3002", "result": "\u6a21\u578b\u51c6\u786e\u738793.5%\uff0c\u4f18\u5316\u540e\u4ec57KB\uff0c\u6210\u529f\u5728UR5\u673a\u5668\u4eba\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u6a21\u578b\u53ef\u5b9e\u73b0\u51c6\u786e\u54cd\u5e94\u624b\u52bf\u63a7\u5236\uff0c\u4e3a\u53d7\u9650\u73af\u5883\u4e2d\u81ea\u7136\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u65b0\u53ef\u80fd\u3002"}}
{"id": "2507.10075", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10075", "abs": "https://arxiv.org/abs/2507.10075", "authors": ["Jie Pan", "Tianyi Wang", "Yangyang Wang", "Junfeng Jiao", "Christian Claudel"], "title": "TGLD: A Trust-Aware Game-Theoretic Lane-Changing Decision Framework for Automated Vehicles in Heterogeneous Traffic", "comment": "6 pages, 7 figures, accepted for IEEE International Conference on\n  Intelligent Transportation Systems (ITSC) 2025", "summary": "Automated vehicles (AVs) face a critical need to adopt socially compatible\nbehaviors and cooperate effectively with human-driven vehicles (HVs) in\nheterogeneous traffic environment. However, most existing lane-changing\nframeworks overlook HVs' dynamic trust levels, limiting their ability to\naccurately predict human driver behaviors. To address this gap, this study\nproposes a trust-aware game-theoretic lane-changing decision (TGLD) framework.\nFirst, we formulate a multi-vehicle coalition game, incorporating fully\ncooperative interactions among AVs and partially cooperative behaviors from HVs\ninformed by real-time trust evaluations. Second, we develop an online trust\nevaluation method to dynamically estimate HVs' trust levels during\nlane-changing interactions, guiding AVs to select context-appropriate\ncooperative maneuvers. Lastly, social compatibility objectives are considered\nby minimizing disruption to surrounding vehicles and enhancing the\npredictability of AV behaviors, thereby ensuring human-friendly and\ncontext-adaptive lane-changing strategies. A human-in-the-loop experiment\nconducted in a highway on-ramp merging scenario validates our TGLD approach.\nResults show that AVs can effectively adjust strategies according to different\nHVs' trust levels and driving styles. Moreover, incorporating a trust mechanism\nsignificantly improves lane-changing efficiency, maintains safety, and\ncontributes to transparent and adaptive AV-HV interactions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4fe1\u4efb\u611f\u77e5\u7684\u535a\u5f08\u8bba\u6362\u9053\u51b3\u7b56\u6846\u67b6\uff08TGLD\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8bc4\u4f30\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\u7684\u4fe1\u4efb\u6c34\u5e73\uff0c\u4f18\u5316\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u6362\u9053\u7b56\u7565\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AVs\uff09\u9700\u8981\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\uff08HVs\uff09\u5728\u5f02\u6784\u4ea4\u901a\u73af\u5883\u4e2d\u6709\u6548\u534f\u4f5c\uff0c\u4f46\u73b0\u6709\u6362\u9053\u6846\u67b6\u5ffd\u89c6\u4e86HVs\u7684\u52a8\u6001\u4fe1\u4efb\u6c34\u5e73\uff0c\u5bfc\u81f4\u884c\u4e3a\u9884\u6d4b\u4e0d\u51c6\u786e\u3002", "method": "1. \u6784\u5efa\u591a\u8f66\u8f86\u8054\u76df\u535a\u5f08\u6a21\u578b\uff0c\u7ed3\u5408AVs\u7684\u5168\u534f\u4f5c\u548cHVs\u7684\u90e8\u5206\u534f\u4f5c\u884c\u4e3a\uff1b2. \u5f00\u53d1\u5728\u7ebf\u4fe1\u4efb\u8bc4\u4f30\u65b9\u6cd5\uff0c\u52a8\u6001\u4f30\u8ba1HVs\u7684\u4fe1\u4efb\u6c34\u5e73\uff1b3. \u8003\u8651\u793e\u4f1a\u517c\u5bb9\u6027\u76ee\u6807\uff0c\u6700\u5c0f\u5316\u5bf9\u5468\u56f4\u8f66\u8f86\u7684\u5e72\u6270\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cTGLD\u6846\u67b6\u80fd\u6839\u636eHVs\u7684\u4fe1\u4efb\u6c34\u5e73\u548c\u9a7e\u9a76\u98ce\u683c\u8c03\u6574\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u6362\u9053\u6548\u7387\u3001\u5b89\u5168\u6027\u53ca\u4ea4\u4e92\u900f\u660e\u5ea6\u3002", "conclusion": "TGLD\u6846\u67b6\u901a\u8fc7\u4fe1\u4efb\u673a\u5236\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u5b89\u5168\u548c\u9002\u5e94\u6027\u5f3a\u7684AV-HV\u4ea4\u4e92\uff0c\u4e3a\u5f02\u6784\u4ea4\u901a\u73af\u5883\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10082", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10082", "abs": "https://arxiv.org/abs/2507.10082", "authors": ["Amit Levy", "Itzik Klein"], "title": "Unscented Kalman Filter with a Nonlinear Propagation Model for Navigation Applications", "comment": "5 pages, 2 figures", "summary": "The unscented Kalman filter is a nonlinear estimation algorithm commonly used\nin navigation applications. The prediction of the mean and covariance matrix is\ncrucial to the stable behavior of the filter. This prediction is done by\npropagating the sigma points according to the dynamic model at hand. In this\npaper, we introduce an innovative method to propagate the sigma points\naccording to the nonlinear dynamic model of the navigation error state vector.\nThis improves the filter accuracy and navigation performance. We demonstrate\nthe benefits of our proposed approach using real sensor data recorded by an\nautonomous underwater vehicle during several scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\u4e2dsigma\u70b9\u4f20\u64ad\u7684\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6ee4\u6ce2\u7cbe\u5ea6\u548c\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\u5728\u5bfc\u822a\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176sigma\u70b9\u4f20\u64ad\u5bf9\u6ee4\u6ce2\u7a33\u5b9a\u6027\u81f3\u5173\u91cd\u8981\uff0c\u9700\u6539\u8fdb\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u3002", "method": "\u901a\u8fc7\u975e\u7ebf\u6027\u52a8\u6001\u6a21\u578b\u4f20\u64adsigma\u70b9\uff0c\u4f18\u5316\u5bfc\u822a\u8bef\u5dee\u72b6\u6001\u5411\u91cf\u7684\u9884\u6d4b\u3002", "result": "\u4f7f\u7528\u81ea\u4e3b\u6c34\u4e0b\u8f66\u8f86\u7684\u771f\u5b9e\u4f20\u611f\u5668\u6570\u636e\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63d0\u9ad8\u4e86\u5bfc\u822a\u6027\u80fd\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u7cbe\u5ea6\u548c\u5bfc\u822a\u8868\u73b0\u3002"}}
{"id": "2507.10087", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10087", "abs": "https://arxiv.org/abs/2507.10087", "authors": ["Muhammad Tayyab Khan", "Ammar Waheed"], "title": "Foundation Model Driven Robotics: A Comprehensive Review", "comment": null, "summary": "The rapid emergence of foundation models, particularly Large Language Models\n(LLMs) and Vision-Language Models (VLMs), has introduced a transformative\nparadigm in robotics. These models offer powerful capabilities in semantic\nunderstanding, high-level reasoning, and cross-modal generalization, enabling\nsignificant advances in perception, planning, control, and human-robot\ninteraction. This critical review provides a structured synthesis of recent\ndevelopments, categorizing applications across simulation-driven design,\nopen-world execution, sim-to-real transfer, and adaptable robotics. Unlike\nexisting surveys that emphasize isolated capabilities, this work highlights\nintegrated, system-level strategies and evaluates their practical feasibility\nin real-world environments. Key enabling trends such as procedural scene\ngeneration, policy generalization, and multimodal reasoning are discussed\nalongside core bottlenecks, including limited embodiment, lack of multimodal\ndata, safety risks, and computational constraints. Through this lens, this\npaper identifies both the architectural strengths and critical limitations of\nfoundation model-based robotics, highlighting open challenges in real-time\noperation, grounding, resilience, and trust. The review concludes with a\nroadmap for future research aimed at bridging semantic reasoning and physical\nintelligence through more robust, interpretable, and embodied models.", "AI": {"tldr": "\u7efc\u8ff0\u63a2\u8ba8\u4e86\u57fa\u7840\u6a21\u578b\uff08\u5982LLMs\u548cVLMs\uff09\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86\u5176\u4f18\u52bf\u3001\u5c40\u9650\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5b64\u7acb\u80fd\u529b\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u7ea7\u7b56\u7565\u548c\u5b9e\u9645\u53ef\u884c\u6027\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u7efc\u8ff0\uff0c\u5206\u7c7b\u5e94\u7528\u9886\u57df\uff08\u5982\u4eff\u771f\u9a71\u52a8\u8bbe\u8ba1\u3001\u5f00\u653e\u4e16\u754c\u6267\u884c\u7b49\uff09\uff0c\u5e76\u5206\u6790\u96c6\u6210\u7b56\u7565\u548c\u5b9e\u9645\u6311\u6218\u3002", "result": "\u57fa\u7840\u6a21\u578b\u5728\u8bed\u4e49\u7406\u89e3\u3001\u63a8\u7406\u548c\u591a\u6a21\u6001\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4ecd\u9762\u4e34\u5b9e\u65f6\u64cd\u4f5c\u3001\u5b89\u5168\u98ce\u9669\u548c\u8ba1\u7b97\u9650\u5236\u7b49\u74f6\u9888\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u9700\u5173\u6ce8\u66f4\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u548c\u5177\u8eab\u5316\u7684\u6a21\u578b\uff0c\u4ee5\u5f25\u5408\u8bed\u4e49\u63a8\u7406\u4e0e\u7269\u7406\u667a\u80fd\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2507.10105", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10105", "abs": "https://arxiv.org/abs/2507.10105", "authors": ["Ines Sorrentino", "Giulio Romualdi", "Lorenzo Moretti", "Silvio Traversaro", "Daniele Pucci"], "title": "Physics-Informed Neural Networks with Unscented Kalman Filter for Sensorless Joint Torque Estimation in Humanoid Robots", "comment": null, "summary": "This paper presents a novel framework for whole-body torque control of\nhumanoid robots without joint torque sensors, designed for systems with\nelectric motors and high-ratio harmonic drives. The approach integrates\nPhysics-Informed Neural Networks (PINNs) for friction modeling and Unscented\nKalman Filtering (UKF) for joint torque estimation, within a real-time torque\ncontrol architecture. PINNs estimate nonlinear static and dynamic friction from\njoint and motor velocity readings, capturing effects like motor actuation\nwithout joint movement. The UKF utilizes PINN-based friction estimates as\ndirect measurement inputs, improving torque estimation robustness. Experimental\nvalidation on the ergoCub humanoid robot demonstrates improved torque tracking\naccuracy, enhanced energy efficiency, and superior disturbance rejection\ncompared to the state-of-the-art Recursive Newton-Euler Algorithm (RNEA), using\na dynamic balancing experiment. The framework's scalability is shown by\nconsistent performance across robots with similar hardware but different\nfriction characteristics, without re-identification. Furthermore, a comparative\nanalysis with position control highlights the advantages of the proposed torque\ncontrol approach. The results establish the method as a scalable and practical\nsolution for sensorless torque control in humanoid robots, ensuring torque\ntracking, adaptability, and stability in dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5173\u8282\u626d\u77e9\u4f20\u611f\u5668\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u626d\u77e9\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u548c\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08UKF\uff09\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u626d\u77e9\u8ddf\u8e2a\u3001\u80fd\u6548\u548c\u6297\u5e72\u6270\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u5728\u65e0\u5173\u8282\u626d\u77e9\u4f20\u611f\u5668\u60c5\u51b5\u4e0b\u7684\u626d\u77e9\u63a7\u5236\u95ee\u9898\uff0c\u63d0\u9ad8\u626d\u77e9\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528PINNs\u5efa\u6a21\u975e\u7ebf\u6027\u6469\u64e6\uff0cUKF\u4f30\u8ba1\u5173\u8282\u626d\u77e9\uff0c\u5b9e\u65f6\u626d\u77e9\u63a7\u5236\u67b6\u6784\u7ed3\u5408\u4e24\u8005\u3002", "result": "\u5728ergoCub\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u4f18\u4e8e\u73b0\u6709RNEA\u65b9\u6cd5\uff0c\u626d\u77e9\u8ddf\u8e2a\u66f4\u51c6\u3001\u80fd\u6548\u66f4\u9ad8\u3001\u6297\u5e72\u6270\u66f4\u5f3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65e0\u4f20\u611f\u5668\u626d\u77e9\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u73af\u5883\u3002"}}
{"id": "2507.10121", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10121", "abs": "https://arxiv.org/abs/2507.10121", "authors": ["Seung Hyun Kim", "Jiamiao Guo", "Arman Tekinalp", "Heng-Sheng Chang", "Ugur Akcal", "Tixian Wang", "Darren Biskup", "Benjamin Walt", "Girish Chowdhary", "Girish Krishnan", "Prashant G. Mehta", "Mattia Gazzola"], "title": "Simulations and experiments with assemblies of fiber-reinforced soft actuators", "comment": "8 pages, 4 figures This work has been submitted to the IEEE for\n  possible publication", "summary": "Soft continuum arms (SCAs) promise versatile manipulation through mechanical\ncompliance, for assistive devices, agriculture, search applications, or\nsurgery. However, SCAs' real-world use is challenging, partly due to their\nhard-to-control non-linear behavior. Here, a simulation framework for SCAs\nmodularly assembled out of fiber reinforced elastomeric enclosures (FREEs) is\ndeveloped and integrated with a video-tracking system for experimental testing\nand control design.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e\u7ea4\u7ef4\u589e\u5f3a\u5f39\u6027\u4f53\u8f6f\u8fde\u7eed\u81c2\uff08SCAs\uff09\u7684\u4eff\u771f\u6846\u67b6\uff0c\u5e76\u7ed3\u5408\u89c6\u9891\u8ddf\u8e2a\u7cfb\u7edf\u8fdb\u884c\u5b9e\u9a8c\u6d4b\u8bd5\u548c\u63a7\u5236\u8bbe\u8ba1\u3002", "motivation": "\u8f6f\u8fde\u7eed\u81c2\uff08SCAs\uff09\u56e0\u5176\u673a\u68b0\u987a\u5e94\u6027\u5728\u591a\u4e2a\u9886\u57df\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u5176\u975e\u7ebf\u6027\u884c\u4e3a\u96be\u4ee5\u63a7\u5236\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u7ec4\u88c5\u7ea4\u7ef4\u589e\u5f3a\u5f39\u6027\u4f53\uff08FREEs\uff09\u7684\u4eff\u771f\u6846\u67b6\uff0c\u5e76\u96c6\u6210\u89c6\u9891\u8ddf\u8e2a\u7cfb\u7edf\u8fdb\u884c\u5b9e\u9a8c\u548c\u63a7\u5236\u8bbe\u8ba1\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5bf9SCAs\u975e\u7ebf\u6027\u884c\u4e3a\u7684\u63a7\u5236\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aSCAs\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u63a7\u5236\u548c\u6d4b\u8bd5\u5de5\u5177\u3002"}}
{"id": "2507.10131", "categories": ["cs.RO", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.10131", "abs": "https://arxiv.org/abs/2507.10131", "authors": ["Cesar Alan Contreras", "Manolis Chiou", "Alireza Rastegarpanah", "Michal Szulik", "Rustam Stolkin"], "title": "Probabilistic Human Intent Prediction for Mobile Manipulation: An Evaluation with Human-Inspired Constraints", "comment": "Submitted to Journal of Intelligent & Robotic Systems (Under Review)", "summary": "Accurate inference of human intent enables human-robot collaboration without\nconstraining human control or causing conflicts between humans and robots. We\npresent GUIDER (Global User Intent Dual-phase Estimation for Robots), a\nprobabilistic framework that enables a robot to estimate the intent of human\noperators. GUIDER maintains two coupled belief layers, one tracking navigation\ngoals and the other manipulation goals. In the Navigation phase, a Synergy Map\nblends controller velocity with an occupancy grid to rank interaction areas.\nUpon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.\nThe Manipulation phase combines U2Net saliency, FastSAM instance saliency, and\nthree geometric grasp-feasibility tests, with an end-effector kinematics-aware\nupdate rule that evolves object probabilities in real-time. GUIDER can\nrecognize areas and objects of intent without predefined goals. We evaluated\nGUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and\ncompared it with two baselines, one for navigation and one for manipulation.\nAcross the 25 trials, GUIDER achieved a median stability of 93-100% during\nnavigation, compared with 60-100% for the BOIR baseline, with an improvement of\n39.5% in a redirection scenario (T5). During manipulation, stability reached\n94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a\nredirection task (T3). In geometry-constrained trials (manipulation), GUIDER\nrecognized the object intent three times earlier than Trajectron (median\nremaining time to confident prediction 23.6 s vs 7.8 s). These results validate\nour dual-phase framework and show improvements in intent inference in both\nphases of mobile manipulation tasks.", "AI": {"tldr": "GUIDER\u662f\u4e00\u4e2a\u53cc\u9636\u6bb5\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u63a8\u65ad\u4eba\u7c7b\u610f\u56fe\uff0c\u5728\u5bfc\u822a\u548c\u64cd\u4f5c\u9636\u6bb5\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63d0\u9ad8\u4eba\u673a\u534f\u4f5c\u4e2d\u673a\u5668\u4eba\u5bf9\u4eba\u7c7b\u610f\u56fe\u7684\u51c6\u786e\u63a8\u65ad\u80fd\u529b\uff0c\u907f\u514d\u9650\u5236\u4eba\u7c7b\u63a7\u5236\u6216\u5f15\u53d1\u51b2\u7a81\u3002", "method": "GUIDER\u91c7\u7528\u53cc\u9636\u6bb5\u6846\u67b6\uff1a\u5bfc\u822a\u9636\u6bb5\u7ed3\u5408\u63a7\u5236\u5668\u901f\u5ea6\u548c\u5360\u7528\u7f51\u683c\u751f\u6210\u534f\u540c\u5730\u56fe\uff1b\u64cd\u4f5c\u9636\u6bb5\u878d\u5408U2Net\u3001FastSAM\u548c\u51e0\u4f55\u6293\u53d6\u53ef\u884c\u6027\u6d4b\u8bd5\uff0c\u5b9e\u65f6\u66f4\u65b0\u5bf9\u8c61\u6982\u7387\u3002", "result": "\u572825\u6b21\u8bd5\u9a8c\u4e2d\uff0cGUIDER\u5bfc\u822a\u7a33\u5b9a\u6027\u8fbe93-100%\uff0c\u64cd\u4f5c\u7a33\u5b9a\u6027\u8fbe94-100%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GUIDER\u9a8c\u8bc1\u4e86\u53cc\u9636\u6bb5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u610f\u56fe\u63a8\u65ad\u80fd\u529b\u3002"}}
{"id": "2507.10164", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10164", "abs": "https://arxiv.org/abs/2507.10164", "authors": ["Egor Maslennikov", "Eduard Zaliaev", "Nikita Dudorov", "Oleg Shamanin", "Karanov Dmitry", "Gleb Afanasev", "Alexey Burkov", "Egor Lygin", "Simeon Nedelchev", "Evgeny Ponomarev"], "title": "Robust RL Control for Bipedal Locomotion with Closed Kinematic Chains", "comment": null, "summary": "Developing robust locomotion controllers for bipedal robots with closed\nkinematic chains presents unique challenges, particularly since most\nreinforcement learning (RL) approaches simplify these parallel mechanisms into\nserial models during training. We demonstrate that this simplification\nsignificantly impairs sim-to-real transfer by failing to capture essential\naspects such as joint coupling, friction dynamics, and motor-space control\ncharacteristics. In this work, we present an RL framework that explicitly\nincorporates closed-chain dynamics and validate it on our custom-built robot\nTopA. Our approach enhances policy robustness through symmetry-aware loss\nfunctions, adversarial training, and targeted network regularization.\nExperimental results demonstrate that our integrated approach achieves stable\nlocomotion across diverse terrains, significantly outperforming methods based\non simplified kinematic models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u5f0f\u7ed3\u5408\u95ed\u94fe\u52a8\u529b\u5b66\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53cc\u8db3\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u63a7\u5236\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c06\u95ed\u94fe\u673a\u6784\u7b80\u5316\u4e3a\u4e32\u884c\u6a21\u578b\uff0c\u5bfc\u81f4\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u6027\u80fd\u4e0b\u964d\uff0c\u65e0\u6cd5\u6355\u6349\u5173\u8282\u8026\u5408\u7b49\u5173\u952e\u7279\u6027\u3002", "method": "\u91c7\u7528\u5bf9\u79f0\u611f\u77e5\u635f\u5931\u51fd\u6570\u3001\u5bf9\u6297\u8bad\u7ec3\u548c\u9488\u5bf9\u6027\u7f51\u7edc\u6b63\u5219\u5316\uff0c\u7ed3\u5408\u95ed\u94fe\u52a8\u529b\u5b66\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u81ea\u5b9a\u4e49\u673a\u5668\u4ebaTopA\u4e0a\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u8de8\u591a\u79cd\u5730\u5f62\u7684\u7a33\u5b9a\u8fd0\u52a8\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u7b80\u5316\u6a21\u578b\u65b9\u6cd5\u3002", "conclusion": "\u663e\u5f0f\u7ed3\u5408\u95ed\u94fe\u52a8\u529b\u5b66\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u53cc\u8db3\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u63a7\u5236\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.10204", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10204", "abs": "https://arxiv.org/abs/2507.10204", "authors": ["Abdelhakim Amer", "Mohit Mehindratta", "Yury Brodskiy", "Bilal Wehbe", "Erdal Kayacan"], "title": "REACT: Real-time Entanglement-Aware Coverage Path Planning for Tethered Underwater Vehicles", "comment": null, "summary": "Inspection of complex underwater structures with tethered underwater vehicles\nis often hindered by the risk of tether entanglement. We propose REACT\n(real-time entanglement-aware coverage path planning for tethered underwater\nvehicles), a framework designed to overcome this limitation. REACT comprises a\nfast geometry-based tether model using the signed distance field (SDF) map for\naccurate, real-time simulation of taut tether configurations around arbitrary\nstructures in 3D. This model enables an efficient online replanning strategy by\nenforcing a maximum tether length constraint, thereby actively preventing\nentanglement. By integrating REACT into a coverage path planning framework, we\nachieve safe and optimal inspection paths, previously challenging due to tether\nconstraints. The complete REACT framework's efficacy is validated in a pipe\ninspection scenario, demonstrating safe, entanglement-free navigation and\nfull-coverage inspection. Simulation results show that REACT achieves complete\ncoverage while maintaining tether constraints and completing the total mission\n20% faster than conventional planners, despite a longer inspection time due to\nproactive avoidance of entanglement that eliminates extensive post-mission\ndisentanglement. Real-world experiments confirm these benefits, where REACT\ncompletes the full mission, while the baseline planner fails due to physical\ntether entanglement.", "AI": {"tldr": "REACT\u6846\u67b6\u901a\u8fc7\u5b9e\u65f6\u51e0\u4f55\u6a21\u578b\u548c\u8def\u5f84\u89c4\u5212\uff0c\u89e3\u51b3\u4e86\u6c34\u4e0b\u673a\u5668\u4eba\u56e0\u7f06\u7ef3\u7f20\u7ed5\u5bfc\u81f4\u7684\u68c0\u67e5\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u5b8c\u6210\u901f\u5ea6\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u4f20\u7edf\u6c34\u4e0b\u673a\u5668\u4eba\u68c0\u67e5\u590d\u6742\u7ed3\u6784\u65f6\uff0c\u7f06\u7ef3\u7f20\u7ed5\u98ce\u9669\u9650\u5236\u4e86\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "method": "REACT\u7ed3\u5408\u4e86\u57fa\u4e8e\u51e0\u4f55\u7684\u7f06\u7ef3\u6a21\u578b\uff08SDF\u5730\u56fe\uff09\u548c\u5b9e\u65f6\u8def\u5f84\u91cd\u89c4\u5212\u7b56\u7565\uff0c\u4e3b\u52a8\u907f\u514d\u7f20\u7ed5\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u663e\u793a\uff0cREACT\u6bd4\u4f20\u7edf\u89c4\u5212\u5668\u5feb20%\uff0c\u4e14\u80fd\u5b8c\u6210\u5168\u90e8\u4efb\u52a1\uff0c\u800c\u57fa\u7ebf\u89c4\u5212\u5668\u56e0\u7f20\u7ed5\u5931\u8d25\u3002", "conclusion": "REACT\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u7f06\u7ef3\u7f20\u7ed5\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6c34\u4e0b\u68c0\u67e5\u7684\u6548\u7387\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2507.10284", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.10284", "abs": "https://arxiv.org/abs/2507.10284", "authors": ["Venkat Margapuri"], "title": "Prompt Informed Reinforcement Learning for Visual Coverage Path Planning", "comment": null, "summary": "Visual coverage path planning with unmanned aerial vehicles (UAVs) requires\nagents to strategically coordinate UAV motion and camera control to maximize\ncoverage, minimize redundancy, and maintain battery efficiency. Traditional\nreinforcement learning (RL) methods rely on environment-specific reward\nformulations that lack semantic adaptability. This study proposes\nPrompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates\nthe zero-shot reasoning ability and in-context learning capability of large\nlanguage models with curiosity-driven RL. PIRL leverages semantic feedback from\nan LLM, GPT-3.5, to dynamically shape the reward function of the Proximal\nPolicy Optimization (PPO) RL policy guiding the agent in position and camera\nadjustments for optimal visual coverage. The PIRL agent is trained using OpenAI\nGym and evaluated in various environments. Furthermore, the sim-to-real-like\nability and zero-shot generalization of the agent are tested by operating the\nagent in Webots simulator which introduces realistic physical dynamics. Results\nshow that PIRL outperforms multiple learning-based baselines such as PPO with\nstatic rewards, PPO with exploratory weight initialization, imitation learning,\nand an LLM-only controller. Across different environments, PIRL outperforms the\nbest-performing baseline by achieving up to 14% higher visual coverage in\nOpenAI Gym and 27% higher in Webots, up to 25% higher battery efficiency, and\nup to 18\\% lower redundancy, depending on the environment. The results\nhighlight the effectiveness of LLM-guided reward shaping in complex spatial\nexploration tasks and suggest a promising direction for integrating natural\nlanguage priors into RL for robotics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPIRL\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-3.5\uff09\u7684\u96f6\u6837\u672c\u63a8\u7406\u80fd\u529b\u548c\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u89c6\u89c9\u8986\u76d6\u8def\u5f84\u89c4\u5212\u3002PIRL\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5956\u52b1\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8986\u76d6\u7387\u548c\u7535\u6c60\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7279\u5b9a\u73af\u5883\u7684\u5956\u52b1\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u8bed\u4e49\u9002\u5e94\u6027\u3002PIRL\u65e8\u5728\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u53cd\u9988\uff0c\u52a8\u6001\u4f18\u5316\u5956\u52b1\u51fd\u6570\uff0c\u4ee5\u89e3\u51b3\u590d\u6742\u7a7a\u95f4\u63a2\u7d22\u4efb\u52a1\u4e2d\u7684\u8986\u76d6\u548c\u6548\u7387\u95ee\u9898\u3002", "method": "PIRL\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u63a8\u7406\u80fd\u529b\u548c\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\uff08PPO\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5956\u52b1\u51fd\u6570\u6765\u6307\u5bfc\u65e0\u4eba\u673a\u7684\u4f4d\u7f6e\u548c\u76f8\u673a\u8c03\u6574\u3002", "result": "PIRL\u5728OpenAI Gym\u548cWebots\u6a21\u62df\u5668\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u89c6\u89c9\u8986\u76d6\u7387\u5206\u522b\u63d0\u9ad8\u4e8614%\u548c27%\uff0c\u7535\u6c60\u6548\u7387\u63d0\u534725%\uff0c\u5197\u4f59\u964d\u4f4e18%\u3002", "conclusion": "PIRL\u5c55\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u81ea\u7136\u8bed\u8a00\u5148\u9a8c\u7684\u6574\u5408\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.10290", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10290", "abs": "https://arxiv.org/abs/2507.10290", "authors": ["Jiajun Yu", "Nanhe Chen", "Guodong Liu", "Chao Xu", "Fei Gao", "Yanjun Cao"], "title": "TOP: Trajectory Optimization via Parallel Optimization towards Constant Time Complexity", "comment": "8 pages, submitted to RA-L", "summary": "Optimization has been widely used to generate smooth trajectories for motion\nplanning. However, existing trajectory optimization methods show weakness when\ndealing with large-scale long trajectories. Recent advances in parallel\ncomputing have accelerated optimization in some fields, but how to efficiently\nsolve trajectory optimization via parallelism remains an open question. In this\npaper, we propose a novel trajectory optimization framework based on the\nConsensus Alternating Direction Method of Multipliers (CADMM) algorithm, which\ndecomposes the trajectory into multiple segments and solves the subproblems in\nparallel. The proposed framework reduces the time complexity to O(1) per\niteration to the number of segments, compared to O(N) of the state-of-the-art\n(SOTA) approaches. Furthermore, we introduce a closed-form solution that\nintegrates convex linear and quadratic constraints to speed up the\noptimization, and we also present numerical solutions for general inequality\nconstraints. A series of simulations and experiments demonstrate that our\napproach outperforms the SOTA approach in terms of efficiency and smoothness.\nEspecially for a large-scale trajectory, with one hundred segments, achieving\nover a tenfold speedup. To fully explore the potential of our algorithm on\nmodern parallel computing architectures, we deploy our framework on a GPU and\nshow high performance with thousands of segments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eCADMM\u7b97\u6cd5\u7684\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u8ba1\u7b97\u89e3\u51b3\u5927\u89c4\u6a21\u957f\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u957f\u8f68\u8ff9\u65f6\u6548\u7387\u4e0d\u8db3\uff0c\u5e76\u884c\u8ba1\u7b97\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "method": "\u4f7f\u7528CADMM\u7b97\u6cd5\u5c06\u8f68\u8ff9\u5206\u89e3\u4e3a\u591a\u6bb5\u5e76\u884c\u6c42\u89e3\uff0c\u5f15\u5165\u95ed\u5f0f\u89e3\u548c\u6570\u503c\u89e3\u5904\u7406\u7ea6\u675f\u3002", "result": "\u6846\u67b6\u5c06\u6bcf\u6b21\u8fed\u4ee3\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u964d\u81f3O(1)\uff0c\u5b9e\u9a8c\u663e\u793a\u6548\u7387\u548c\u5e73\u6ed1\u6027\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5927\u89c4\u6a21\u8f68\u8ff9\u63d0\u901f\u5341\u500d\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5e76\u884c\u8ba1\u7b97\u67b6\u6784\uff08\u5982GPU\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u8d85\u5927\u89c4\u6a21\u8f68\u8ff9\u4f18\u5316\u3002"}}
{"id": "2507.10310", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10310", "abs": "https://arxiv.org/abs/2507.10310", "authors": ["Michael Schr\u00f6der", "Eric Sch\u00f6neberg", "Daniel G\u00f6rges", "Hans D. Schotten"], "title": "Polygonal Obstacle Avoidance Combining Model Predictive Control and Fuzzy Logic", "comment": null, "summary": "In practice, navigation of mobile robots in confined environments is often\ndone using a spatially discrete cost-map to represent obstacles. Path following\nis a typical use case for model predictive control (MPC), but formulating\nconstraints for obstacle avoidance is challenging in this case. Typically the\ncost and constraints of an MPC problem are defined as closed-form functions and\ntypical solvers work best with continuously differentiable functions. This is\ncontrary to spatially discrete occupancy grid maps, in which a grid's value\ndefines the cost associated with occupancy. This paper presents a way to\novercome this compatibility issue by re-formulating occupancy grid maps to\ncontinuously differentiable functions to be embedded into the MPC scheme as\nconstraints. Each obstacle is defined as a polygon -- an intersection of\nhalf-spaces. Any half-space is a linear inequality representing one edge of a\npolygon. Using AND and OR operators, the combined set of all obstacles and\ntherefore the obstacle avoidance constraints can be described. The key\ncontribution of this paper is the use of fuzzy logic to re-formulate such\nconstraints that include logical operators as inequality constraints which are\ncompatible with standard MPC formulation. The resulting MPC-based trajectory\nplanner is successfully tested in simulation. This concept is also applicable\noutside of navigation tasks to implement logical or verbal constraints in MPC.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u79bb\u6563\u7684\u5360\u7528\u7f51\u683c\u5730\u56fe\u8f6c\u5316\u4e3a\u8fde\u7eed\u53ef\u5fae\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3MPC\u4e2d\u969c\u788d\u7269\u907f\u8ba9\u7ea6\u675f\u7684\u517c\u5bb9\u6027\u95ee\u9898\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u5728\u53d7\u9650\u73af\u5883\u4e2d\u5bfc\u822a\u65f6\uff0c\u901a\u5e38\u4f7f\u7528\u79bb\u6563\u7684\u5360\u7528\u7f51\u683c\u5730\u56fe\u8868\u793a\u969c\u788d\u7269\uff0c\u4f46MPC\u9700\u8981\u8fde\u7eed\u53ef\u5fae\u7684\u7ea6\u675f\u51fd\u6570\uff0c\u4e24\u8005\u4e0d\u517c\u5bb9\u3002", "method": "\u901a\u8fc7\u5c06\u969c\u788d\u7269\u5b9a\u4e49\u4e3a\u591a\u8fb9\u5f62\uff08\u534a\u7a7a\u95f4\u7684\u4ea4\u96c6\uff09\uff0c\u5e76\u4f7f\u7528\u6a21\u7cca\u903b\u8f91\u5c06\u903b\u8f91\u8fd0\u7b97\u7b26\u8f6c\u5316\u4e3a\u4e0d\u7b49\u5f0f\u7ea6\u675f\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u6807\u51c6MPC\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u6210\u529f\u6d4b\u8bd5\u4e86\u57fa\u4e8eMPC\u7684\u8f68\u8ff9\u89c4\u5212\u5668\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u5bfc\u822a\u4efb\u52a1\uff0c\u8fd8\u53ef\u7528\u4e8eMPC\u4e2d\u5b9e\u73b0\u903b\u8f91\u6216\u8bed\u8a00\u7ea6\u675f\u3002"}}
{"id": "2507.10376", "categories": ["cs.RO", "I.2"], "pdf": "https://arxiv.org/pdf/2507.10376", "abs": "https://arxiv.org/abs/2507.10376", "authors": ["Mohammadhossein Talebi", "Pragyan Dahal", "Davide Possenti", "Stefano Arrigoni", "Francesco Braghin"], "title": "Raci-Net: Ego-vehicle Odometry Estimation in Adverse Weather Conditions", "comment": "8 pages", "summary": "Autonomous driving systems are highly dependent on sensors like cameras,\nLiDAR, and inertial measurement units (IMU) to perceive the environment and\nestimate their motion. Among these sensors, perception-based sensors are not\nprotected from harsh weather and technical failures. Although existing methods\nshow robustness against common technical issues like rotational misalignment\nand disconnection, they often degrade when faced with dynamic environmental\nfactors like weather conditions. To address these problems, this research\nintroduces a novel deep learning-based motion estimator that integrates visual,\ninertial, and millimeter-wave radar data, utilizing each sensor strengths to\nimprove odometry estimation accuracy and reliability under adverse\nenvironmental conditions such as snow, rain, and varying light. The proposed\nmodel uses advanced sensor fusion techniques that dynamically adjust the\ncontributions of each sensor based on the current environmental condition, with\nradar compensating for visual sensor limitations in poor visibility. This work\nexplores recent advancements in radar-based odometry and highlights that radar\nrobustness in different weather conditions makes it a valuable component for\npose estimation systems, specifically when visual sensors are degraded.\nExperimental results, conducted on the Boreas dataset, showcase the robustness\nand effectiveness of the model in both clear and degraded environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8fd0\u52a8\u4f30\u8ba1\u6a21\u578b\uff0c\u878d\u5408\u89c6\u89c9\u3001\u60ef\u6027\u548c\u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\uff0c\u63d0\u5347\u6076\u52a3\u73af\u5883\u4e0b\u7684\u91cc\u7a0b\u4f30\u8ba1\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u56e0\u7d20\uff08\u5982\u5929\u6c14\uff09\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u6539\u8fdb\u4f20\u611f\u5668\u878d\u5408\u6280\u672f\u4ee5\u5e94\u5bf9\u6076\u52a3\u6761\u4ef6\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u52a8\u6001\u8c03\u6574\u5404\u4f20\u611f\u5668\u8d21\u732e\uff0c\u5229\u7528\u96f7\u8fbe\u5f25\u8865\u89c6\u89c9\u4f20\u611f\u5668\u5728\u4f4e\u80fd\u89c1\u5ea6\u4e0b\u7684\u4e0d\u8db3\u3002", "result": "\u5728Boreas\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u5728\u6e05\u6670\u548c\u6076\u52a3\u73af\u5883\u4e0b\u5747\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u96f7\u8fbe\u5728\u6076\u52a3\u5929\u6c14\u4e2d\u7684\u9c81\u68d2\u6027\u4f7f\u5176\u6210\u4e3a\u59ff\u6001\u4f30\u8ba1\u7cfb\u7edf\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u5c24\u5176\u5728\u89c6\u89c9\u4f20\u611f\u5668\u6027\u80fd\u4e0b\u964d\u65f6\u3002"}}
{"id": "2507.10500", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.10500", "abs": "https://arxiv.org/abs/2507.10500", "authors": ["Kyungtae Han", "Yitao Chen", "Rohit Gupta", "Onur Altintas"], "title": "Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance", "comment": null, "summary": "While autonomous driving technologies continue to advance, current Advanced\nDriver Assistance Systems (ADAS) remain limited in their ability to interpret\nscene context or engage with drivers through natural language. These systems\ntypically rely on predefined logic and lack support for dialogue-based\ninteraction, making them inflexible in dynamic environments or when adapting to\ndriver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a\nmodular framework that integrates Generative AI components including large\nlanguage models, vision-to-text interpretation, and structured function calling\nto enable real-time, interpretable, and adaptive driver assistance. SC-ADAS\nsupports multi-turn dialogue grounded in visual and sensor context, allowing\nnatural language recommendations and driver-confirmed ADAS control. Implemented\nin the CARLA simulator with cloud-based Generative AI, the system executes\nconfirmed user intents as structured ADAS commands without requiring model\nfine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and\nrevisited multi-turn interactions, highlighting trade-offs such as increased\nlatency from vision-based context retrieval and token growth from accumulated\ndialogue history. These results demonstrate the feasibility of combining\nconversational reasoning, scene perception, and modular ADAS control to support\nthe next generation of intelligent driver assistance.", "AI": {"tldr": "SC-ADAS\u662f\u4e00\u4e2a\u7ed3\u5408\u751f\u6210\u5f0fAI\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u548c\u573a\u666f\u611f\u77e5\u63d0\u5347ADAS\u7684\u4ea4\u4e92\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u5f53\u524dADAS\u7f3a\u4e4f\u573a\u666f\u7406\u89e3\u548c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7075\u6d3b\u6027\u3002", "method": "\u96c6\u6210\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u89c6\u89c9\u5230\u6587\u672c\u89e3\u91ca\u548c\u7ed3\u6784\u5316\u51fd\u6570\u8c03\u7528\uff0c\u5b9e\u73b0\u5b9e\u65f6\u3001\u53ef\u89e3\u91ca\u7684\u9a7e\u9a76\u5458\u8f85\u52a9\u3002", "result": "\u5728CARLA\u6a21\u62df\u5668\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u4f46\u5b58\u5728\u5ef6\u8fdf\u548c\u4ee4\u724c\u589e\u957f\u7b49\u6743\u8861\u3002", "conclusion": "SC-ADAS\u5c55\u793a\u4e86\u7ed3\u5408\u5bf9\u8bdd\u63a8\u7406\u3001\u573a\u666f\u611f\u77e5\u548c\u6a21\u5757\u5316ADAS\u63a7\u5236\u7684\u6f5c\u529b\uff0c\u652f\u6301\u4e0b\u4e00\u4ee3\u667a\u80fd\u9a7e\u9a76\u8f85\u52a9\u3002"}}
{"id": "2507.10543", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10543", "abs": "https://arxiv.org/abs/2507.10543", "authors": ["Juyi Sheng", "Ziyi Wang", "Peiming Li", "Mengyuan Liu"], "title": "MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation", "comment": null, "summary": "In robot manipulation, robot learning has become a prevailing approach.\nHowever, generative models within this field face a fundamental trade-off\nbetween the slow, iterative sampling of diffusion models and the architectural\nconstraints of faster Flow-based methods, which often rely on explicit\nconsistency losses. To address these limitations, we introduce MP1, which pairs\n3D point-cloud inputs with the MeanFlow paradigm to generate action\ntrajectories in one network function evaluation (1-NFE). By directly learning\nthe interval-averaged velocity via the MeanFlow Identity, our policy avoids any\nadditional consistency constraints. This formulation eliminates numerical\nODE-solver errors during inference, yielding more precise trajectories. MP1\nfurther incorporates CFG for improved trajectory controllability while\nretaining 1-NFE inference without reintroducing structural constraints. Because\nsubtle scene-context variations are critical for robot learning, especially in\nfew-shot learning, we introduce a lightweight Dispersive Loss that repels state\nembeddings during training, boosting generalization without slowing inference.\nWe validate our method on the Adroit and Meta-World benchmarks, as well as in\nreal-world scenarios. Experimental results show MP1 achieves superior average\ntask success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its\naverage inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster\nthan FlowPolicy. Our code is available at https://mp1-2254.github.io/.", "AI": {"tldr": "MP1\u662f\u4e00\u79cd\u65b0\u578b\u673a\u5668\u4eba\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7MeanFlow\u8303\u5f0f\u751f\u6210\u52a8\u4f5c\u8f68\u8ff9\uff0c\u907f\u514d\u4e86\u6269\u6563\u6a21\u578b\u7684\u6162\u91c7\u6837\u548cFlow\u65b9\u6cd5\u7684\u67b6\u6784\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u7cbe\u786e\u7684\u8f68\u8ff9\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u751f\u6210\u6a21\u578b\u5728\u91c7\u6837\u901f\u5ea6\u548c\u67b6\u6784\u9650\u5236\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u91c7\u7528MeanFlow Identity\u76f4\u63a5\u5b66\u4e60\u533a\u95f4\u5e73\u5747\u901f\u5ea6\uff0c\u7ed3\u5408CFG\u63d0\u5347\u8f68\u8ff9\u53ef\u63a7\u6027\uff0c\u5e76\u5f15\u5165Dispersive Loss\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728Adroit\u548cMeta-World\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMP1\u5e73\u5747\u4efb\u52a1\u6210\u529f\u7387\u6bd4DP3\u9ad810.2%\uff0c\u6bd4FlowPolicy\u9ad87.3%\uff0c\u63a8\u7406\u901f\u5ea6\u5feb19\u500d\u3002", "conclusion": "MP1\u5728\u4fdd\u6301\u5feb\u901f\u63a8\u7406\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u573a\u666f\u3002"}}
