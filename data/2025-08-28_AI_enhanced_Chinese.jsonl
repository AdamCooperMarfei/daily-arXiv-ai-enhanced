{"id": "2508.19367", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.19367", "abs": "https://arxiv.org/abs/2508.19367", "authors": ["Alex Cuellar", "Ho Chit Siu", "Julie A Shah"], "title": "Inference of Human-derived Specifications of Object Placement via Demonstration", "comment": null, "summary": "As robots' manipulation capabilities improve for pick-and-place tasks (e.g.,\nobject packing, sorting, and kitting), methods focused on understanding\nhuman-acceptable object configurations remain limited expressively with regard\nto capturing spatial relationships important to humans. To advance robotic\nunderstanding of human rules for object arrangement, we introduce\npositionally-augmented RCC (PARCC), a formal logic framework based on region\nconnection calculus (RCC) for describing the relative position of objects in\nspace. Additionally, we introduce an inference algorithm for learning PARCC\nspecifications via demonstrations. Finally, we present the results from a human\nstudy, which demonstrate our framework's ability to capture a human's intended\nspecification and the benefits of learning from demonstration approaches over\nhuman-provided specifications.", "AI": {"tldr": "\u63d0\u51fa\u4e86PARCC\u6846\u67b6\uff0c\u57fa\u4e8e\u533a\u57df\u8fde\u63a5\u6f14\u7b97\u7684\u5f62\u5f0f\u5316\u903b\u8f91\u7cfb\u7edf\uff0c\u7528\u4e8e\u63cf\u8ff0\u7269\u4f53\u95f4\u7684\u7a7a\u95f4\u76f8\u5bf9\u4f4d\u7f6e\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u6f14\u793a\u5b66\u4e60\u7b97\u6cd5\u6765\u63a8\u65ad\u4eba\u7c7b\u53ef\u63a5\u53d7\u7684\u7269\u4f53\u914d\u7f6e\u89c4\u5219\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u64cd\u4f5c\u80fd\u529b\u5728\u62fe\u53d6\u653e\u7f6e\u4efb\u52a1\u4e2d\u6709\u6240\u63d0\u5347\uff0c\u4f46\u7406\u89e3\u4eba\u7c7b\u53ef\u63a5\u53d7\u7684\u7269\u4f53\u914d\u7f6e\u65b9\u6cd5\u5728\u8868\u8fbe\u7a7a\u95f4\u5173\u7cfb\u65b9\u9762\u4ecd\u7136\u6709\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u5730\u6355\u6349\u5bf9\u4eba\u7c7b\u91cd\u8981\u7684\u7a7a\u95f4\u5173\u7cfb\u89c4\u5219\u3002", "method": "\u57fa\u4e8e\u533a\u57df\u8fde\u63a5\u6f14\u7b97(RCC)\u5f00\u53d1\u4e86\u4f4d\u7f6e\u589e\u5f3a\u7684PARCC\u5f62\u5f0f\u5316\u903b\u8f91\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u901a\u8fc7\u6f14\u793a\u5b66\u4e60\u7684\u63a8\u7406\u7b97\u6cd5\u6765\u5b66\u4e60\u7269\u4f53\u914d\u7f6e\u89c4\u8303\u3002", "result": "\u4eba\u7c7b\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u6355\u6349\u4eba\u7c7b\u7684\u610f\u56fe\u89c4\u8303\uff0c\u4e14\u901a\u8fc7\u6f14\u793a\u5b66\u4e60\u7684\u65b9\u6cd5\u4f18\u4e8e\u4eba\u7c7b\u76f4\u63a5\u63d0\u4f9b\u7684\u89c4\u8303\u8bf4\u660e\u3002", "conclusion": "PARCC\u6846\u67b6\u4e3a\u673a\u5668\u4eba\u7406\u89e3\u4eba\u7c7b\u7269\u4f53\u6392\u5217\u89c4\u5219\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5f62\u5f0f\u5316\u8868\u793a\u548c\u5b66\u4e60\u65b9\u6cd5\uff0c\u6f14\u793a\u5b66\u4e60\u65b9\u5f0f\u5728\u83b7\u53d6\u4eba\u7c7b\u7a7a\u95f4\u5173\u7cfb\u504f\u597d\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2508.19380", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19380", "abs": "https://arxiv.org/abs/2508.19380", "authors": ["Diancheng Li", "Nia Ralston", "Bastiaan Hagen", "Phoebe Tan", "Matthew A. Robertson"], "title": "FlipWalker: Jacob's Ladder toy-inspired robot for locomotion across diverse, complex terrain", "comment": "2025 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2025)", "summary": "This paper introduces FlipWalker, a novel underactuated robot locomotion\nsystem inspired by Jacob's Ladder illusion toy, designed to traverse\nchallenging terrains where wheeled robots often struggle. Like the Jacob's\nLadder toy, FlipWalker features two interconnected segments joined by flexible\ncables, enabling it to pivot and flip around singularities in a manner\nreminiscent of the toy's cascading motion. Actuation is provided by\nmotor-driven legs within each segment that push off either the ground or the\nopposing segment, depending on the robot's current configuration. A\nphysics-based model of the underactuated flipping dynamics is formulated to\nelucidate the critical design parameters governing forward motion and obstacle\nclearance or climbing. The untethered prototype weighs 0.78 kg, achieves a\nmaximum flipping speed of 0.2 body lengths per second. Experimental trials on\nartificial grass, river rocks, and snow demonstrate that FlipWalker's flipping\nstrategy, which relies on ground reaction forces applied normal to the surface,\noffers a promising alternative to traditional locomotion for navigating\nirregular outdoor terrain.", "AI": {"tldr": "FlipWalker\u662f\u4e00\u4e2a\u53d7\u96c5\u5404\u5e03\u5929\u68af\u73a9\u5177\u542f\u53d1\u7684\u6b20\u9a71\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ffb\u8f6c\u8fd0\u52a8\u5728\u590d\u6742\u5730\u5f62\u4e2d\u79fb\u52a8\uff0c\u6bd4\u8f6e\u5f0f\u673a\u5668\u4eba\u8868\u73b0\u66f4\u597d", "motivation": "\u8f6e\u5f0f\u673a\u5668\u4eba\u5728\u4e0d\u89c4\u5219\u6237\u5916\u5730\u5f62\u4e2d\u79fb\u52a8\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u8fd0\u52a8\u7b56\u7565\u6765\u5e94\u5bf9\u8349\u5730\u3001\u5ca9\u77f3\u548c\u96ea\u5730\u7b49\u6311\u6218\u6027\u73af\u5883", "method": "\u91c7\u7528\u4e24\u6bb5\u5f0f\u7ed3\u6784\u901a\u8fc7\u67d4\u6027\u7535\u7f06\u8fde\u63a5\uff0c\u6a21\u4eff\u96c5\u5404\u5e03\u5929\u68af\u73a9\u5177\u7684\u7ffb\u8f6c\u8fd0\u52a8\uff0c\u4f7f\u7528\u7535\u673a\u9a71\u52a8\u817f\u90e8\u5728\u914d\u7f6e\u53d8\u5316\u65f6\u63a8\u79bb\u5730\u9762\u6216\u76f8\u5bf9\u6bb5", "result": "\u539f\u578b\u673a\u91cd0.78kg\uff0c\u6700\u5927\u7ffb\u8f6c\u901f\u5ea6\u8fbe0.2\u4f53\u957f/\u79d2\uff0c\u5728\u4eba\u5de5\u8349\u5730\u3001\u6cb3\u77f3\u548c\u96ea\u5730\u7b49\u4e0d\u89c4\u5219\u5730\u5f62\u4e2d\u6210\u529f\u9a8c\u8bc1\u4e86\u8fd0\u52a8\u80fd\u529b", "conclusion": "\u57fa\u4e8e\u5730\u9762\u6cd5\u5411\u53cd\u4f5c\u7528\u529b\u7684\u7ffb\u8f6c\u7b56\u7565\u4e3a\u6237\u5916\u4e0d\u89c4\u5219\u5730\u5f62\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u4f20\u7edf\u8fd0\u52a8\u66ff\u4ee3\u65b9\u6848"}}
{"id": "2508.19391", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19391", "abs": "https://arxiv.org/abs/2508.19391", "authors": ["Chaoran Zhu", "Hengyi Wang", "Yik Lung Pang", "Changjae Oh"], "title": "LaVA-Man: Learning Visual Action Representations for Robot Manipulation", "comment": null, "summary": "Visual-textual understanding is essential for language-guided robot\nmanipulation. Recent works leverage pre-trained vision-language models to\nmeasure the similarity between encoded visual observations and textual\ninstructions, and then train a model to map this similarity to robot actions.\nHowever, this two-step approach limits the model to capture the relationship\nbetween visual observations and textual instructions, leading to reduced\nprecision in manipulation tasks. We propose to learn visual-textual\nassociations through a self-supervised pretext task: reconstructing a masked\ngoal image conditioned on an input image and textual instructions. This\nformulation allows the model to learn visual-action representations without\nrobot action supervision. The learned representations can then be fine-tuned\nfor manipulation tasks with only a few demonstrations. We also introduce the\n\\textit{Omni-Object Pick-and-Place} dataset, which consists of annotated robot\ntabletop manipulation episodes, including 180 object classes and 3,200\ninstances with corresponding textual instructions. This dataset enables the\nmodel to acquire diverse object priors and allows for a more comprehensive\nevaluation of its generalisation capability across object instances.\nExperimental results on the five benchmarks, including both simulated and\nreal-robot validations, demonstrate that our method outperforms prior art.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u4efb\u52a1\u5b66\u4e60\u89c6\u89c9-\u6587\u672c\u5173\u8054\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u673a\u5668\u4eba\u52a8\u4f5c\u76d1\u7763\u5373\u53ef\u5b66\u4e60\u89c6\u89c9-\u52a8\u4f5c\u8868\u793a\uff0c\u7136\u540e\u5728\u5c11\u91cf\u6f14\u793a\u6837\u672c\u4e0a\u5fae\u8c03\u7528\u4e8e\u64cd\u4f5c\u4efb\u52a1", "motivation": "\u73b0\u6709\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u9650\u5236\u4e86\u6a21\u578b\u6355\u6349\u89c6\u89c9\u89c2\u5bdf\u548c\u6587\u672c\u6307\u4ee4\u4e4b\u95f4\u5173\u7cfb\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u64cd\u4f5c\u4efb\u52a1\u7cbe\u5ea6\u964d\u4f4e", "method": "\u4f7f\u7528\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u4efb\u52a1\uff1a\u57fa\u4e8e\u8f93\u5165\u56fe\u50cf\u548c\u6587\u672c\u6307\u4ee4\u91cd\u6784\u88ab\u906e\u853d\u7684\u76ee\u6807\u56fe\u50cf\uff0c\u5b66\u4e60\u89c6\u89c9-\u6587\u672c\u5173\u8054\u3002\u8fd8\u5f15\u5165\u4e86\u5305\u542b180\u4e2a\u7269\u4f53\u7c7b\u522b\u548c3,200\u4e2a\u5b9e\u4f8b\u7684Omni-Object Pick-and-Place\u6570\u636e\u96c6", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u9a8c\u8bc1\uff09\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f", "conclusion": "\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5b66\u4e60\u89c6\u89c9-\u6587\u672c\u5173\u8054\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u8bed\u8a00\u5f15\u5bfc\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\uff0c\u4e14\u53ea\u9700\u5c11\u91cf\u6f14\u793a\u6837\u672c\u5373\u53ef\u5fae\u8c03"}}
{"id": "2508.19425", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19425", "abs": "https://arxiv.org/abs/2508.19425", "authors": ["John M. Scanlon", "Timothy L McMurry", "Yin-Hsiu Chen", "Kristofer D. Kusano", "Trent Victor"], "title": "From Stoplights to On-Ramps: A Comprehensive Set of Crash Rate Benchmarks for Freeway and Surface Street ADS Evaluation", "comment": null, "summary": "This paper presents crash rate benchmarks for evaluating US-based Automated\nDriving Systems (ADS) for multiple urban areas. The purpose of this study was\nto extend prior benchmarks focused only on surface streets to additionally\ncapture freeway crash risk for future ADS safety performance assessments. Using\npublicly available police-reported crash and vehicle miles traveled (VMT) data,\nthe methodology details the isolation of in-transport passenger vehicles, road\ntype classification, and crash typology. Key findings revealed that freeway\ncrash rates exhibit large geographic dependence variations with\nany-injury-reported crash rates being nearly 3.5 times higher in Atlanta (2.4\nIPMM; the highest) when compared to Phoenix (0.7 IPMM; the lowest). The results\nshow the critical need for location-specific benchmarks to avoid biased safety\nevaluations and provide insights into the vehicle miles traveled (VMT) required\nto achieve statistical significance for various safety impact levels. The\ndistribution of crash types depended on the outcome severity level. Higher\nseverity outcomes (e.g., fatal crashes) had a larger proportion of\nsingle-vehicle, vulnerable road users (VRU), and opposite-direction collisions\ncompared to lower severity (police-reported) crashes. Given heterogeneity in\ncrash types by severity, performance in low-severity scenarios may not be\npredictive of high-severity outcomes. These benchmarks are additionally used to\nquantify at the required mileage to show statistically significant deviations\nfrom human performance. This is the first paper to generate freeway-specific\nbenchmarks for ADS evaluation and provides a foundational framework for future\nADS benchmarking by evaluators and developers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7f8e\u56fd\u57fa\u5730\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf(ADS)\u7684\u51fa\u7a81\u7387\u57fa\u51c6\uff0c\u7279\u522b\u5173\u6ce8\u9ad8\u901f\u516c\u8def\u51fa\u7a81\u98ce\u9669\u8bc4\u4f30\uff0c\u5f3a\u8c03\u5730\u7406\u5dee\u5f02\u6027\u5bf9\u5b89\u5168\u6027\u8bc4\u4f30\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u6269\u5c55\u4ee5\u524d\u4ec5\u805a\u7126\u57fa\u7840\u9053\u8def\u7684\u51fa\u7a81\u7387\u57fa\u51c6\uff0c\u9700\u8981\u5305\u542b\u9ad8\u901f\u516c\u8def\u51fa\u7a81\u98ce\u9669\u8bc4\u4f30\uff0c\u4ee5\u652f\u6301\u672a\u6765ADS\u5b89\u5168\u6027\u80fd\u8bc4\u4f30\u3002", "method": "\u5229\u7528\u516c\u5f00\u7684\u8b66\u5bdf\u62a5\u544a\u51fa\u7a81\u548c\u8f66\u8f86\u884c\u9a76\u91cc\u7a0b(VMT)\u6570\u636e\uff0c\u8fdb\u884c\u5728\u8fd0\u5ba2\u8f66\u5206\u79bb\u3001\u9053\u8def\u7c7b\u578b\u5206\u7c7b\u548c\u51fa\u7a81\u7c7b\u578b\u5206\u6790\u3002", "result": "\u53d1\u73b0\u9ad8\u901f\u516c\u8def\u51fa\u7a81\u7387\u5b58\u5728\u663e\u8457\u5730\u7406\u5dee\u5f02\uff0c\u4e9a\u7279\u5170\u5927\u4f24\u5bb3\u51fa\u7a81\u7387(2.4 IPMM)\u6bd4\u51e4\u51f0\u57ce(0.7 IPMM)\u9ad83.5\u500d\u3002\u4e0d\u540c\u4e25\u91cd\u7a0b\u5ea6\u7684\u51fa\u7a81\u7c7b\u578b\u5206\u5e03\u5f02\u8d28\u6027\u5f3a\uff0c\u4f4e\u4e25\u91cd\u6027\u80fd\u65e0\u6cd5\u9884\u6d4b\u9ad8\u4e25\u91cd\u6027\u80fd\u3002", "conclusion": "\u9700\u8981\u57fa\u4e8e\u7279\u5b9a\u5730\u7406\u4f4d\u7f6e\u7684\u51fa\u7a81\u7387\u57fa\u51c6\u6765\u907f\u514d\u504f\u5dee\u7684\u5b89\u5168\u6027\u8bc4\u4f30\uff0c\u672c\u7814\u7a76\u9996\u6b21\u4e3aADS\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9ad8\u901f\u516c\u8def\u7279\u5b9a\u57fa\u51c6\uff0c\u4e3a\u672a\u6765\u8bc4\u4f30\u8005\u548c\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u57fa\u7840\u6846\u67b6\u3002"}}
{"id": "2508.19429", "categories": ["cs.RO", "cs.FL"], "pdf": "https://arxiv.org/pdf/2508.19429", "abs": "https://arxiv.org/abs/2508.19429", "authors": ["Gustavo A. Cardona", "Kaier Liang", "Cristian-Ioan Vasile"], "title": "An Iterative Approach for Heterogeneous Multi-Agent Route Planning with Resource Transportation Uncertainty and Temporal Logic Goals", "comment": null, "summary": "This paper presents an iterative approach for heterogeneous multi-agent route\nplanning in environments with unknown resource distributions. We focus on a\nteam of robots with diverse capabilities tasked with executing missions\nspecified using Capability Temporal Logic (CaTL), a formal framework built on\nSignal Temporal Logic to handle spatial, temporal, capability, and resource\nconstraints. The key challenge arises from the uncertainty in the initial\ndistribution and quantity of resources in the environment. To address this, we\nintroduce an iterative algorithm that dynamically balances exploration and task\nfulfillment. Robots are guided to explore the environment, identifying resource\nlocations and quantities while progressively refining their understanding of\nthe resource landscape. At the same time, they aim to maximally satisfy the\nmission objectives based on the current information, adapting their strategies\nas new data is uncovered. This approach provides a robust solution for planning\nin dynamic, resource-constrained environments, enabling efficient coordination\nof heterogeneous teams even under conditions of uncertainty. Our method's\neffectiveness and performance are demonstrated through simulated case studies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u7b97\u6cd5\u7528\u4e8e\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5728\u8d44\u6e90\u5206\u5e03\u672a\u77e5\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\uff0c\u901a\u8fc7\u52a8\u6001\u5e73\u8861\u63a2\u7d22\u548c\u4efb\u52a1\u6267\u884c\u6765\u5904\u7406\u8d44\u6e90\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u5728\u8d44\u6e90\u5206\u5e03\u672a\u77e5\u73af\u5883\u4e2d\u6267\u884c\u590d\u6742\u4efb\u52a1\u65f6\u7684\u89c4\u5212\u6311\u6218\uff0c\u7279\u522b\u662f\u5904\u7406\u8d44\u6e90\u521d\u59cb\u5206\u5e03\u548c\u6570\u91cf\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\u7684\u80fd\u529b\u65f6\u5e8f\u903b\u8f91(CaTL)\u6765\u89c4\u8303\u4efb\u52a1\uff0c\u5f15\u5165\u8fed\u4ee3\u7b97\u6cd5\u52a8\u6001\u5e73\u8861\u73af\u5883\u63a2\u7d22\u548c\u4efb\u52a1\u5b8c\u6210\uff0c\u673a\u5668\u4eba\u901a\u8fc7\u63a2\u7d22\u8bc6\u522b\u8d44\u6e90\u4f4d\u7f6e\u548c\u6570\u91cf\u5e76\u9010\u6b65\u5b8c\u5584\u5bf9\u8d44\u6e90\u73af\u5883\u7684\u7406\u89e3\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6027\u80fd\uff0c\u80fd\u591f\u5728\u52a8\u6001\u8d44\u6e90\u7ea6\u675f\u73af\u5883\u4e2d\u63d0\u4f9b\u9c81\u68d2\u7684\u89c4\u5212\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f02\u6784\u56e2\u961f\u5728\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u7684\u9ad8\u6548\u534f\u8c03\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u8d44\u6e90\u5206\u5e03\u672a\u77e5\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u3002"}}
{"id": "2508.19476", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19476", "abs": "https://arxiv.org/abs/2508.19476", "authors": ["Dane Brouwer", "Joshua Citron", "Heather Nolte", "Jeannette Bohg", "Mark Cutkosky"], "title": "Gentle Object Retraction in Dense Clutter Using Multimodal Force Sensing and Imitation Learning", "comment": "Submitted to IEEE Robotics and Automation Letters (RA-L)", "summary": "Dense collections of movable objects are common in everyday spaces -- from\ncabinets in a home to shelves in a warehouse. Safely retracting objects from\nsuch collections is difficult for robots, yet people do it easily, using\nnon-prehensile tactile sensing on the sides and backs of their hands and arms.\nWe investigate the role of such sensing for training robots to gently reach\ninto constrained clutter and extract objects. The available sensing modalities\nare (1) \"eye-in-hand\" vision, (2) proprioception, (3) non-prehensile triaxial\ntactile sensing, (4) contact wrenches estimated from joint torques, and (5) a\nmeasure of successful object acquisition obtained by monitoring the vacuum line\nof a suction cup. We use imitation learning to train policies from a set of\ndemonstrations on randomly generated scenes, then conduct an ablation study of\nwrench and tactile information. We evaluate each policy's performance across 40\nunseen environment configurations. Policies employing any force sensing show\nfewer excessive force failures, an increased overall success rate, and faster\ncompletion times. The best performance is achieved using both tactile and\nwrench information, producing an 80% improvement above the baseline without\nforce information.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u673a\u5668\u4eba\u5982\u4f55\u5229\u7528\u975e\u6293\u63e1\u89e6\u89c9\u4f20\u611f\u4ece\u5bc6\u96c6\u7269\u4f53\u96c6\u5408\u4e2d\u5b89\u5168\u63d0\u53d6\u7269\u4f53\uff0c\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u7b56\u7565\uff0c\u53d1\u73b0\u529b\u4f20\u611f\u80fd\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u3002", "motivation": "\u65e5\u5e38\u73af\u5883\u4e2d\u5b58\u5728\u5927\u91cf\u5bc6\u96c6\u6392\u5217\u7684\u53ef\u79fb\u52a8\u7269\u4f53\uff08\u5982\u6a71\u67dc\u3001\u8d27\u67b6\uff09\uff0c\u673a\u5668\u4eba\u5b89\u5168\u63d0\u53d6\u8fd9\u4e9b\u7269\u4f53\u5177\u6709\u6311\u6218\u6027\u3002\u4eba\u7c7b\u901a\u8fc7\u624b\u80cc\u548c\u624b\u81c2\u7684\u975e\u6293\u63e1\u89e6\u89c9\u4f20\u611f\u8f7b\u677e\u5b8c\u6210\u6b64\u7c7b\u4efb\u52a1\uff0c\u56e0\u6b64\u7814\u7a76\u8fd9\u79cd\u4f20\u611f\u65b9\u5f0f\u5bf9\u673a\u5668\u4eba\u8bad\u7ec3\u7684\u91cd\u8981\u6027\u3002", "method": "\u4f7f\u7528\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u968f\u673a\u751f\u6210\u7684\u573a\u666f\u4e0a\u8fdb\u884c\u6f14\u793a\u3002\u6bd4\u8f83\u4e94\u79cd\u4f20\u611f\u6a21\u6001\uff1a\u624b\u773c\u89c6\u89c9\u3001\u672c\u4f53\u611f\u89c9\u3001\u975e\u6293\u63e1\u4e09\u8f74\u89e6\u89c9\u4f20\u611f\u3001\u57fa\u4e8e\u5173\u8282\u626d\u77e9\u4f30\u8ba1\u7684\u63a5\u89e6\u529b\u77e9\u3001\u4ee5\u53ca\u5438\u76d8\u771f\u7a7a\u7ba1\u76d1\u6d4b\u7684\u6210\u529f\u83b7\u53d6\u5ea6\u91cf\u3002\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u5206\u6790\u529b\u77e9\u548c\u89e6\u89c9\u4fe1\u606f\u7684\u4f5c\u7528\u3002", "result": "\u572840\u4e2a\u672a\u89c1\u73af\u5883\u914d\u7f6e\u4e2d\u8bc4\u4f30\u7b56\u7565\u6027\u80fd\u3002\u4f7f\u7528\u4efb\u4f55\u529b\u4f20\u611f\u7684\u7b56\u7565\u90fd\u8868\u73b0\u51fa\u66f4\u5c11\u7684\u8fc7\u5ea6\u529b\u5931\u8d25\u3001\u66f4\u9ad8\u7684\u603b\u4f53\u6210\u529f\u7387\u548c\u66f4\u5feb\u7684\u5b8c\u6210\u65f6\u95f4\u3002\u540c\u65f6\u4f7f\u7528\u89e6\u89c9\u548c\u529b\u77e9\u4fe1\u606f\u65f6\u6027\u80fd\u6700\u4f73\uff0c\u76f8\u6bd4\u65e0\u529b\u4fe1\u606f\u57fa\u7ebf\u63d0\u534780%\u3002", "conclusion": "\u975e\u6293\u63e1\u89e6\u89c9\u4f20\u611f\u5bf9\u4e8e\u673a\u5668\u4eba\u5b89\u5168\u5730\u4ece\u7ea6\u675f\u6027\u6742\u4e71\u73af\u5883\u4e2d\u63d0\u53d6\u7269\u4f53\u81f3\u5173\u91cd\u8981\uff0c\u529b\u4f20\u611f\u4fe1\u606f\u80fd\u663e\u8457\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u548c\u6210\u529f\u7387\u3002"}}
{"id": "2508.19508", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19508", "abs": "https://arxiv.org/abs/2508.19508", "authors": ["Tian Qiu", "Alan Zoubi", "Yiyuan Lin", "Ruiming Du", "Lailiang Cheng", "Yu Jiang"], "title": "DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View", "comment": null, "summary": "Digital twin applications offered transformative potential by enabling\nreal-time monitoring and robotic simulation through accurate virtual replicas\nof physical assets. The key to these systems is 3D reconstruction with high\ngeometrical fidelity. However, existing methods struggled under field\nconditions, especially with sparse and occluded views. This study developed a\ntwo-stage framework (DATR) for the reconstruction of apple trees from sparse\nviews. The first stage leverages onboard sensors and foundation models to\nsemi-automatically generate tree masks from complex field images. Tree masks\nare used to filter out background information in multi-modal data for the\nsingle-image-to-3D reconstruction at the second stage. This stage consists of a\ndiffusion model and a large reconstruction model for respective multi view and\nimplicit neural field generation. The training of the diffusion model and LRM\nwas achieved by using realistic synthetic apple trees generated by a Real2Sim\ndata generator. The framework was evaluated on both field and synthetic\ndatasets. The field dataset includes six apple trees with field-measured ground\ntruth, while the synthetic dataset featured structurally diverse trees.\nEvaluation results showed that our DATR framework outperformed existing 3D\nreconstruction methods across both datasets and achieved domain-trait\nestimation comparable to industrial-grade stationary laser scanners while\nimproving the throughput by $\\sim$360 times, demonstrating strong potential for\nscalable agricultural digital twin systems.", "AI": {"tldr": "\u5f00\u53d1\u4e86DATR\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u89c6\u89d2\u91cd\u5efa\u82f9\u679c\u68113D\u6a21\u578b\uff0c\u5728\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5904\u7406\u901f\u5ea6\u6bd4\u5de5\u4e1a\u7ea7\u6fc0\u5149\u626b\u63cf\u4eea\u5feb\u7ea6360\u500d", "motivation": "\u6570\u5b57\u5b6a\u751f\u5e94\u7528\u9700\u8981\u9ad8\u7cbe\u5ea63D\u91cd\u5efa\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u91ce\u5916\u7a00\u758f\u548c\u906e\u6321\u89c6\u89d2\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u519c\u4e1a\u573a\u666f\u4e2d\u82f9\u679c\u6811\u7684\u91cd\u5efa\u9762\u4e34\u6311\u6218", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u673a\u8f7d\u4f20\u611f\u5668\u548c\u57fa\u7840\u6a21\u578b\u534a\u81ea\u52a8\u751f\u6210\u6811\u63a9\u7801\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u6269\u6563\u6a21\u578b\u548c\u5927\u91cd\u5efa\u6a21\u578b\u5206\u522b\u8fdb\u884c\u591a\u89c6\u89d2\u548c\u9690\u5f0f\u795e\u7ecf\u573a\u751f\u6210\uff0c\u901a\u8fc7Real2Sim\u6570\u636e\u751f\u6210\u5668\u8bad\u7ec3", "result": "\u5728\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u67093D\u91cd\u5efa\u65b9\u6cd5\uff0c\u57df\u7279\u5f81\u4f30\u8ba1\u4e0e\u5de5\u4e1a\u7ea7\u56fa\u5b9a\u6fc0\u5149\u626b\u63cf\u4eea\u76f8\u5f53\uff0c\u5904\u7406\u901f\u5ea6\u63d0\u5347\u7ea6360\u500d", "conclusion": "DATR\u6846\u67b6\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u519c\u4e1a\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u91ce\u5916\u6761\u4ef6\u4e0b\u7684\u7a00\u758f\u89c6\u89d2\u91cd\u5efa\u95ee\u9898"}}
{"id": "2508.19595", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19595", "abs": "https://arxiv.org/abs/2508.19595", "authors": ["Maryam Kazemi Eskeri", "Thomas Wiedemann", "Ville Kyrki", "Dominik Baumann", "Tomasz Piotr Kucner"], "title": "A Lightweight Crowd Model for Robot Social Navigation", "comment": "7 pages, 6 figures, accepted in ECMR 2025", "summary": "Robots operating in human-populated environments must navigate safely and\nefficiently while minimizing social disruption. Achieving this requires\nestimating crowd movement to avoid congested areas in real-time. Traditional\nmicroscopic models struggle to scale in dense crowds due to high computational\ncost, while existing macroscopic crowd prediction models tend to be either\noverly simplistic or computationally intensive. In this work, we propose a\nlightweight, real-time macroscopic crowd prediction model tailored for human\nmotion, which balances prediction accuracy and computational efficiency. Our\napproach simplifies both spatial and temporal processing based on the inherent\ncharacteristics of pedestrian flow, enabling robust generalization without the\noverhead of complex architectures. We demonstrate a 3.6 times reduction in\ninference time, while improving prediction accuracy by 3.1 %. Integrated into a\nsocially aware planning framework, the model enables efficient and socially\ncompliant robot navigation in dynamic environments. This work highlights that\nefficient human crowd modeling enables robots to navigate dense environments\nwithout costly computations.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5b9e\u65f6\u5b8f\u89c2\u6d41\u6d3b\u52a8\u9884\u6d4b\u6a21\u578b\uff0c\u4e13\u4e3a\u673a\u5668\u4eba\u5728\u4eba\u7fa4\u73af\u5883\u4e2d\u5b89\u5168\u9ad8\u6548\u5bfc\u822a\u800c\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u9884\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u7f29\u77ed\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u4f20\u7edf\u5fae\u89c2\u6a21\u578b\u5728\u5bc6\u96c6\u4eba\u7fa4\u4e2d\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u800c\u73b0\u6709\u5b8f\u89c2\u6a21\u578b\u8981\u4e48\u8fc7\u4e8e\u7b80\u5355\u8981\u4e48\u8ba1\u7b97\u8d39\u7528\u8fc7\u5927\uff0c\u65e0\u6cd5\u6ee1\u8db3\u673a\u5668\u4eba\u5b9e\u65f6\u5bfc\u822a\u7684\u9700\u6c42\u3002", "method": "\u57fa\u4e8e\u884c\u4eba\u6d41\u52a8\u7684\u5185\u5728\u7279\u5f81\uff0c\u7b80\u5316\u7a7a\u95f4\u548c\u65f6\u95f4\u5904\u7406\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5b9e\u65f6\u5b8f\u89c2\u6d41\u6d3b\u52a8\u9884\u6d4b\u6a21\u578b\uff0c\u4ee5\u5e73\u8861\u9884\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5b9e\u73b0\u4e86\u63a8\u7406\u65f6\u95f4\u7f29\u77ed3.6\u500d\uff0c\u540c\u65f6\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u9ad83.1%\uff0c\u96c6\u6210\u5230\u793e\u4ea4\u610f\u8bc6\u89c4\u5212\u6846\u67b6\u540e\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u4e14\u7b26\u5408\u793e\u4f1a\u89c4\u8303\u7684\u673a\u5668\u4eba\u5bfc\u822a\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bc1\u660e\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u4eba\u7fa4\u5efa\u6a21\u6280\u672f\uff0c\u673a\u5668\u4eba\u53ef\u4ee5\u5728\u4e0d\u9700\u8981\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u5b8c\u6210\u5bc6\u96c6\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u4efb\u52a1\u3002"}}
{"id": "2508.19607", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19607", "abs": "https://arxiv.org/abs/2508.19607", "authors": ["Amin Berjaoui Tahmaz", "Ravi Prakash", "Jens Kober"], "title": "Impedance Primitive-augmented Hierarchical Reinforcement Learning for Sequential Tasks", "comment": "This article is accepted for publication in IEEE International\n  Conference on Robotics and Automation (ICRA) 2025", "summary": "This paper presents an Impedance Primitive-augmented hierarchical\nreinforcement learning framework for efficient robotic manipulation in\nsequential contact tasks. We leverage this hierarchical structure to\nsequentially execute behavior primitives with variable stiffness control\ncapabilities for contact tasks. Our proposed approach relies on three key\ncomponents: an action space enabling variable stiffness control, an adaptive\nstiffness controller for dynamic stiffness adjustments during primitive\nexecution, and affordance coupling for efficient exploration while encouraging\ncompliance. Through comprehensive training and evaluation, our framework learns\nefficient stiffness control capabilities and demonstrates improvements in\nlearning efficiency, compositionality in primitive selection, and success rates\ncompared to the state-of-the-art. The training environments include block\nlifting, door opening, object pushing, and surface cleaning. Real world\nevaluations further confirm the framework's sim2real capability. This work lays\nthe foundation for more adaptive and versatile robotic manipulation systems,\nwith potential applications in more complex contact-based tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u963b\u6297\u539f\u8bed\u7684\u5c42\u6b21\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5e8f\u5217\u63a5\u89e6\u4efb\u52a1\u7684\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u901a\u8fc7\u53ef\u53d8\u521a\u5ea6\u63a7\u5236\u548c\u4eff\u5c04\u8026\u5408\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u548c\u6210\u529f\u7387", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u63a5\u89e6\u4efb\u52a1\u7684\u590d\u6742\u6027\u548c\u9002\u5e94\u6027\u9700\u6c42\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u52a8\u6001\u8c03\u6574\u521a\u5ea6\u5e76\u9ad8\u6548\u5b66\u4e60\u63a5\u89e6\u884c\u4e3a\u7684\u6846\u67b6", "method": "\u91c7\u7528\u5c42\u6b21\u5f3a\u5316\u5b66\u4e60\u7ed3\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u652f\u6301\u53ef\u53d8\u521a\u5ea6\u63a7\u5236\u7684\u52a8\u4f5c\u7a7a\u95f4\u3001\u81ea\u9002\u5e94\u521a\u5ea6\u63a7\u5236\u5668\u3001\u4ee5\u53ca\u7528\u4e8e\u9ad8\u6548\u63a2\u7d22\u7684\u4eff\u5c04\u8026\u5408\u673a\u5236", "result": "\u5728\u65b9\u5757\u642c\u8fd0\u3001\u5f00\u95e8\u3001\u7269\u4f53\u63a8\u52a8\u548c\u8868\u9762\u6e05\u6d01\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5b66\u4e60\u6548\u7387\u3001\u539f\u8bed\u7ec4\u5408\u80fd\u529b\u548c\u6210\u529f\u7387\uff0c\u5e76\u9a8c\u8bc1\u4e86sim2real\u80fd\u529b", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u66f4\u81ea\u9002\u5e94\u548c\u901a\u7528\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5728\u590d\u6742\u63a5\u89e6\u4efb\u52a1\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b"}}
{"id": "2508.19608", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19608", "abs": "https://arxiv.org/abs/2508.19608", "authors": ["Dongjae Lee", "Byeongjun Kim", "H. Jin Kim"], "title": "Autonomous Aerial Manipulation at Arbitrary Pose in SE(3) with Robust Control and Whole-body Planning", "comment": null, "summary": "Aerial manipulators based on conventional multirotors can conduct\nmanipulation only in small roll and pitch angles due to the underactuatedness\nof the multirotor base. If the multirotor base is capable of hovering at\narbitrary orientation, the robot can freely locate itself at any point in\n$\\mathsf{SE}(3)$, significantly extending its manipulation workspace and\nenabling a manipulation task that was originally not viable. In this work, we\npresent a geometric robust control and whole-body motion planning framework for\nan omnidirectional aerial manipulator (OAM). To maximize the strength of OAM,\nwe first propose a geometric robust controller for a floating base. Since the\nmotion of the robotic arm and the interaction forces during manipulation affect\nthe stability of the floating base, the base should be capable of mitigating\nthese adverse effects while controlling its 6D pose. We then design a two-step\noptimization-based whole-body motion planner, jointly considering the pose of\nthe floating base and the joint angles of the robotic arm to harness the entire\nconfiguration space. The devised two-step approach facilitates real-time\napplicability and enhances convergence of the optimization problem with\nnon-convex and non-Euclidean search space. The proposed approach enables the\nbase to be stationary at any 6D pose while autonomously carrying out\nsophisticated manipulation near obstacles without any collision. We demonstrate\nthe effectiveness of the proposed framework through experiments in which an OAM\nperforms grasping and pulling of an object in multiple scenarios, including\nnear $90^\\circ$ and even $180^\\circ$ pitch angles.", "AI": {"tldr": "\u57fa\u4e8e\u591a\u65cb\u7ffc\u7684\u7a7a\u4e2d\u64cd\u4f5c\u673a\u5668\u4eba\u53ea\u80fd\u5728\u5c0f\u89d2\u5ea6\u64cd\u4f5c\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u5411\u7a7a\u4e2d\u64cd\u4f5c\u673a\u5668\u4eba\u7684\u51e0\u4f55\u7a33\u5b9a\u63a7\u5236\u548c\u5168\u8eab\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4efb\u610f6D\u59ff\u6001\u4e0b\u7a33\u5b9a\u60ac\u505c\u5e76\u6267\u884c\u7cbe\u7ec6\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u591a\u65cb\u7ffc\u57fa\u5730\u56e0\u4e3a\u9aa8\u5e72\u4e0d\u8db3\uff0c\u53ea\u80fd\u5728\u5c0f\u89d2\u5ea6\u8303\u56f4\u5185\u64cd\u4f5c\uff0c\u9650\u5236\u4e86\u64cd\u4f5c\u5de5\u4f5c\u7a7a\u95f4\u3002\u5982\u679c\u80fd\u5728\u4efb\u610f\u65b9\u4f4d\u60ac\u505c\uff0c\u53ef\u663e\u8457\u6269\u5927\u64cd\u4f5c\u8303\u56f4\u5e76\u5b9e\u73b0\u539f\u672c\u4e0d\u53ef\u884c\u7684\u64cd\u4f5c\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u51e0\u4f55\u7a33\u5065\u63a7\u5236\u5668\u548c\u4e24\u6b65\u4f18\u5316\u57fa\u4e8e\u5168\u8eab\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u7edf\u7b79\u8003\u8651\u6d6e\u52a8\u57fa\u5ea7\u7684\u59ff\u6001\u548c\u673a\u68b0\u81c2\u5173\u8282\u89d2\u5ea6\uff0c\u4ee5\u5229\u7528\u6574\u4e2a\u914d\u7f6e\u7a7a\u95f4\u3002\u4e24\u6b65\u65b9\u6cd5\u6709\u52a9\u4e8e\u5b9e\u65f6\u5e94\u7528\u548c\u4f18\u5316\u95ee\u9898\u7684\u6536\u655b\u6027\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u4f7f\u57fa\u5ea7\u5728\u4efb\u610f6D\u59ff\u6001\u4e0b\u7a33\u5b9a\u60ac\u505c\uff0c\u540c\u65f6\u81ea\u4e3b\u6267\u884c\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u800c\u4e0d\u4f1a\u78b0\u649e\u969c\u788d\u7269\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u8fd190\u5ea6\u548c\u751a\u81f3180\u5ea6\u4fef\u4f0a\u89d2\u5ea6\u4e0b\u6267\u884c\u6293\u53d6\u548c\u62c9\u52d8\u7269\u4f53\u7684\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5168\u5411\u7a7a\u4e2d\u64cd\u4f5c\u673a\u5668\u4eba\u63a7\u5236\u548c\u89c4\u5212\u6846\u67b6\u6709\u6548\u6269\u5c55\u4e86\u7a7a\u4e2d\u64cd\u4f5c\u7684\u5de5\u4f5c\u7a7a\u95f4\uff0c\u80fd\u591f\u5728\u6781\u7aef\u59ff\u6001\u4e0b\u7a33\u5b9a\u6267\u884c\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\uff0c\u4e3a\u7a7a\u4e2d\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.19684", "categories": ["cs.RO", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.19684", "abs": "https://arxiv.org/abs/2508.19684", "authors": ["Ghadeer Elmkaiel", "Syn Schmitt", "Michael Muehlebach"], "title": "Embodied Intelligence for Sustainable Flight: A Soaring Robot with Active Morphological Control", "comment": null, "summary": "Achieving both agile maneuverability and high energy efficiency in aerial\nrobots, particularly in dynamic wind environments, remains challenging.\nConventional thruster-powered systems offer agility but suffer from high energy\nconsumption, while fixed-wing designs are efficient but lack hovering and\nmaneuvering capabilities. We present Floaty, a shape-changing robot that\novercomes these limitations by passively soaring, harnessing wind energy\nthrough intelligent morphological control inspired by birds. Floaty's design is\noptimized for passive stability, and its control policy is derived from an\nexperimentally learned aerodynamic model, enabling precise attitude and\nposition control without active propulsion. Wind tunnel experiments demonstrate\nFloaty's ability to hover, maneuver, and reject disturbances in vertical\nairflows up to 10 m/s. Crucially, Floaty achieves this with a specific power\nconsumption of 10 W/kg, an order of magnitude lower than thruster-powered\nsystems. This introduces a paradigm for energy-efficient aerial robotics,\nleveraging morphological intelligence and control to operate sustainably in\nchallenging wind conditions.", "AI": {"tldr": "Floaty\u662f\u4e00\u79cd\u5f62\u72b6\u53ef\u53d8\u7684\u7a7a\u4e2d\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u88ab\u52a8\u6ed1\u7fd4\u548c\u667a\u80fd\u5f62\u6001\u63a7\u5236\u6765\u5229\u7528\u98ce\u80fd\uff0c\u5b9e\u73b0\u4e86\u9ad8\u673a\u52a8\u6027\u548c\u6781\u4f4e\u80fd\u8017\uff0810W/kg\uff09\u7684\u5e73\u8861", "motivation": "\u89e3\u51b3\u4f20\u7edf\u63a8\u8fdb\u5668\u7cfb\u7edf\u80fd\u8017\u9ad8\u548c\u56fa\u5b9a\u7ffc\u8bbe\u8ba1\u7f3a\u4e4f\u60ac\u505c\u80fd\u529b\u7684\u95ee\u9898\uff0c\u5728\u52a8\u6001\u98ce\u73af\u5883\u4e2d\u5b9e\u73b0\u65e2\u654f\u6377\u53c8\u8282\u80fd\u7684\u7a7a\u4e2d\u673a\u5668\u4eba", "method": "\u91c7\u7528\u4eff\u751f\u9e1f\u7c7b\u667a\u80fd\u5f62\u6001\u63a7\u5236\uff0c\u4f18\u5316\u88ab\u52a8\u7a33\u5b9a\u6027\u8bbe\u8ba1\uff0c\u57fa\u4e8e\u5b9e\u9a8c\u5b66\u4e60\u7684\u7a7a\u6c14\u52a8\u529b\u5b66\u6a21\u578b\u5f00\u53d1\u63a7\u5236\u7b56\u7565\uff0c\u65e0\u9700\u4e3b\u52a8\u63a8\u8fdb\u5373\u53ef\u5b9e\u73b0\u7cbe\u786e\u59ff\u6001\u548c\u4f4d\u7f6e\u63a7\u5236", "result": "\u5728\u98ce\u901f\u8fbe10m/s\u7684\u5782\u76f4\u6c14\u6d41\u4e2d\u6210\u529f\u5b9e\u73b0\u60ac\u505c\u3001\u673a\u52a8\u548c\u6297\u5e72\u6270\uff0c\u80fd\u8017\u6bd4\u63a8\u8fdb\u5668\u7cfb\u7edf\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0810W/kg\uff09", "conclusion": "Floaty\u5f00\u521b\u4e86\u8282\u80fd\u7a7a\u4e2d\u673a\u5668\u4eba\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5f62\u6001\u667a\u80fd\u548c\u63a7\u5236\u5b9e\u73b0\u5728\u6311\u6218\u6027\u98ce\u73af\u5883\u4e2d\u7684\u53ef\u6301\u7eed\u8fd0\u884c"}}
{"id": "2508.19731", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19731", "abs": "https://arxiv.org/abs/2508.19731", "authors": ["Maryam Kazemi Eskeri", "Ville Kyrki", "Dominik Baumann", "Tomasz Piotr Kucner"], "title": "Efficient Human-Aware Task Allocation for Multi-Robot Systems in Shared Environments", "comment": "7 Pages, 4 Figures, Accepted in IROS2025", "summary": "Multi-robot systems are increasingly deployed in applications, such as\nintralogistics or autonomous delivery, where multiple robots collaborate to\ncomplete tasks efficiently. One of the key factors enabling their efficient\ncooperation is Multi-Robot Task Allocation (MRTA). Algorithms solving this\nproblem optimize task distribution among robots to minimize the overall\nexecution time. In shared environments, apart from the relative distance\nbetween the robots and the tasks, the execution time is also significantly\nimpacted by the delay caused by navigating around moving people. However, most\nexisting MRTA approaches are dynamics-agnostic, relying on static maps and\nneglecting human motion patterns, leading to inefficiencies and delays. In this\npaper, we introduce \\acrfull{method name}. This method leverages Maps of\nDynamics (MoDs), spatio-temporal queryable models designed to capture\nhistorical human movement patterns, to estimate the impact of humans on the\ntask execution time during deployment. \\acrshort{method name} utilizes a\nstochastic cost function that includes MoDs. Experimental results show that\nintegrating MoDs enhances task allocation performance, resulting in reduced\nmission completion times by up to $26\\%$ compared to the dynamics-agnostic\nmethod and up to $19\\%$ compared to the baseline. This work underscores the\nimportance of considering human dynamics in MRTA within shared environments and\npresents an efficient framework for deploying multi-robot systems in\nenvironments populated by humans.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u5730\u56fe(MoDs)\u7684\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u4eba\u7c7b\u8fd0\u52a8\u6a21\u5f0f\u6765\u4f18\u5316\u4efb\u52a1\u5206\u914d\uff0c\u663e\u8457\u51cf\u5c11\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709MRTA\u65b9\u6cd5\u5927\u591a\u5ffd\u7565\u4eba\u7c7b\u52a8\u6001\u8fd0\u52a8\u6a21\u5f0f\uff0c\u4ec5\u4f9d\u8d56\u9759\u6001\u5730\u56fe\uff0c\u5bfc\u81f4\u5728\u5171\u4eab\u73af\u5883\u4e2d\u6548\u7387\u4f4e\u4e0b\u548c\u5ef6\u8fdf\u3002\u4eba\u7c7b\u79fb\u52a8\u5bf9\u673a\u5668\u4eba\u6267\u884c\u65f6\u95f4\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9700\u8981\u52a8\u6001\u611f\u77e5\u7684\u5206\u914d\u7b97\u6cd5\u3002", "method": "\u5229\u7528\u52a8\u6001\u5730\u56fe(MoDs)\u8fd9\u4e00\u65f6\u7a7a\u53ef\u67e5\u8be2\u6a21\u578b\u6765\u6355\u6349\u5386\u53f2\u4eba\u7c7b\u8fd0\u52a8\u6a21\u5f0f\uff0c\u6784\u5efa\u5305\u542bMoDs\u7684\u968f\u673a\u6210\u672c\u51fd\u6570\u6765\u4f30\u8ba1\u4eba\u7c7b\u5bf9\u4efb\u52a1\u6267\u884c\u65f6\u95f4\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u96c6\u6210MoDs\u7684\u65b9\u6cd5\u6bd4\u52a8\u6001\u4e0d\u53ef\u77e5\u65b9\u6cd5\u51cf\u5c11\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u8fbe26%\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u51cf\u5c1119%\u3002", "conclusion": "\u5728\u5171\u4eab\u73af\u5883\u4e2d\u8003\u8651\u4eba\u7c7b\u52a8\u6001\u5bf9MRTA\u81f3\u5173\u91cd\u8981\uff0c\u8be5\u65b9\u6cd5\u4e3a\u5728\u4eba\u7c7b\u5bc6\u96c6\u73af\u5883\u4e2d\u90e8\u7f72\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u6846\u67b6\u3002"}}
{"id": "2508.19771", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19771", "abs": "https://arxiv.org/abs/2508.19771", "authors": ["Liding Zhang", "Zhenshan Bing", "Yu Zhang", "Kuanqi Cai", "Lingyun Chen", "Fan Wu", "Sami Haddadin", "Alois Knoll"], "title": "Elliptical K-Nearest Neighbors -- Path Optimization via Coulomb's Law and Invalid Vertices in C-space Obstacles", "comment": "2024 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)", "summary": "Path planning has long been an important and active research area in\nrobotics. To address challenges in high-dimensional motion planning, this study\nintroduces the Force Direction Informed Trees (FDIT*), a sampling-based planner\ndesigned to enhance speed and cost-effectiveness in pathfinding. FDIT* builds\nupon the state-of-the-art informed sampling planner, the Effort Informed Trees\n(EIT*), by capitalizing on often-overlooked information in invalid vertices. It\nincorporates principles of physical force, particularly Coulomb's law. This\napproach proposes the elliptical $k$-nearest neighbors search method, enabling\nfast convergence navigation and avoiding high solution cost or infeasible paths\nby exploring more problem-specific search-worthy areas. It demonstrates\nbenefits in search efficiency and cost reduction, particularly in confined,\nhigh-dimensional environments. It can be viewed as an extension of nearest\nneighbors search techniques. Fusing invalid vertex data with physical dynamics\nfacilitates force-direction-based search regions, resulting in an improved\nconvergence rate to the optimum. FDIT* outperforms existing single-query,\nsampling-based planners on the tested problems in R^4 to R^16 and has been\ndemonstrated on a real-world mobile manipulation task.", "AI": {"tldr": "FDIT*\u662f\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u65e0\u6548\u9876\u70b9\u4fe1\u606f\u548c\u5e93\u4ed1\u5b9a\u5f8b\u7269\u7406\u539f\u7406\uff0c\u5728EIT*\u57fa\u7840\u4e0a\u6539\u8fdb\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u4f4e\u7684\u8def\u5f84\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u6311\u6218\uff0c\u5229\u7528\u4f20\u7edf\u89c4\u5212\u5668\u4e2d\u5e38\u88ab\u5ffd\u89c6\u7684\u65e0\u6548\u9876\u70b9\u4fe1\u606f\uff0c\u7ed3\u5408\u7269\u7406\u529b\u539f\u7406\u6765\u63d0\u9ad8\u8def\u5f84\u89c4\u5212\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u3002", "method": "\u57fa\u4e8eEIT*\u7b97\u6cd5\uff0c\u5f15\u5165\u5e93\u4ed1\u5b9a\u5f8b\u7269\u7406\u539f\u7406\uff0c\u63d0\u51fa\u692d\u5706k\u8fd1\u90bb\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u529b\u65b9\u5411\u5f15\u5bfc\u641c\u7d22\u533a\u57df\uff0c\u63a2\u7d22\u66f4\u5177\u95ee\u9898\u7279\u6027\u7684\u641c\u7d22\u533a\u57df\u3002", "result": "\u5728R^4\u5230R^16\u7ef4\u5ea6\u7684\u95ee\u9898\u4e0a\u4f18\u4e8e\u73b0\u6709\u5355\u67e5\u8be2\u91c7\u6837\u89c4\u5212\u5668\uff0c\u5728\u53d7\u9650\u9ad8\u7ef4\u73af\u5883\u4e2d\u663e\u8457\u63d0\u9ad8\u641c\u7d22\u6548\u7387\u548c\u964d\u4f4e\u6210\u672c\uff0c\u5e76\u5728\u771f\u5b9e\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "FDIT*\u901a\u8fc7\u878d\u5408\u65e0\u6548\u9876\u70b9\u6570\u636e\u548c\u7269\u7406\u52a8\u529b\u5b66\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u529b\u65b9\u5411\u7684\u641c\u7d22\u533a\u57df\u4f18\u5316\uff0c\u63d0\u9ad8\u4e86\u5411\u6700\u4f18\u89e3\u7684\u6536\u655b\u901f\u7387\uff0c\u662f\u9ad8\u7ef4\u8def\u5f84\u89c4\u5212\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19776", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19776", "abs": "https://arxiv.org/abs/2508.19776", "authors": ["Liding Zhang", "Yao Ling", "Zhenshan Bing", "Fan Wu", "Sami Haddadin", "Alois Knoll"], "title": "Tree-Based Grafting Approach for Bidirectional Motion Planning with Local Subsets Optimization", "comment": "IEEE Robotics and Automation Letters (also presented at IEEE-IROS\n  2025)", "summary": "Bidirectional motion planning often reduces planning time compared to its\nunidirectional counterparts. It requires connecting the forward and reverse\nsearch trees to form a continuous path. However, this process could fail and\nrestart the asymmetric bidirectional search due to the limitations of\nlazy-reverse search. To address this challenge, we propose Greedy GuILD\nGrafting Trees (G3T*), a novel path planner that grafts invalid edge\nconnections at both ends to re-establish tree-based connectivity, enabling\nrapid path convergence. G3T* employs a greedy approach using the minimum\nLebesgue measure of guided incremental local densification (GuILD) subsets to\noptimize paths efficiently. Furthermore, G3T* dynamically adjusts the sampling\ndistribution between the informed set and GuILD subsets based on historical and\ncurrent cost improvements, ensuring asymptotic optimality. These features\nenhance the forward search's growth towards the reverse tree, achieving faster\nconvergence and lower solution costs. Benchmark experiments across dimensions\nfrom R^2 to R^8 and real-world robotic evaluations demonstrate G3T*'s superior\nperformance compared to existing single-query sampling-based planners. A video\nshowcasing our experimental results is available at:\nhttps://youtu.be/3mfCRL5SQIU", "AI": {"tldr": "G3T*\u662f\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u5411\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\uff0c\u901a\u8fc7\u8d2a\u5a6a\u5ac1\u63a5\u65e0\u6548\u8fb9\u8fde\u63a5\u6765\u91cd\u5efa\u6811\u8fde\u63a5\u6027\uff0c\u5b9e\u73b0\u5feb\u901f\u8def\u5f84\u6536\u655b\u548c\u6e10\u8fdb\u6700\u4f18\u6027", "motivation": "\u4f20\u7edf\u53cc\u5411\u8fd0\u52a8\u89c4\u5212\u4e2d\uff0c\u524d\u5411\u548c\u53cd\u5411\u641c\u7d22\u6811\u7684\u8fde\u63a5\u8fc7\u7a0b\u53ef\u80fd\u5931\u8d25\uff0c\u5bfc\u81f4\u9700\u8981\u91cd\u542f\u975e\u5bf9\u79f0\u53cc\u5411\u641c\u7d22\uff0c\u9650\u5236\u4e86\u89c4\u5212\u6548\u7387", "method": "\u91c7\u7528\u8d2a\u5a6a\u65b9\u6cd5\u4f7f\u7528GuILD\u5b50\u96c6\u7684\u6700\u5c0fLebesgue\u6d4b\u5ea6\u6765\u4f18\u5316\u8def\u5f84\uff0c\u52a8\u6001\u8c03\u6574\u91c7\u6837\u5206\u5e03\uff0c\u5ac1\u63a5\u65e0\u6548\u8fb9\u8fde\u63a5\u91cd\u5efa\u6811\u8fde\u63a5\u6027", "result": "\u5728R^2\u5230R^8\u7ef4\u5ea6\u7684\u57fa\u51c6\u5b9e\u9a8c\u548c\u771f\u5b9e\u673a\u5668\u4eba\u8bc4\u4f30\u4e2d\uff0cG3T*\u76f8\u6bd4\u73b0\u6709\u5355\u67e5\u8be2\u91c7\u6837\u89c4\u5212\u5668\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd", "conclusion": "G3T*\u901a\u8fc7\u521b\u65b0\u7684\u5ac1\u63a5\u673a\u5236\u548c\u52a8\u6001\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53cc\u5411\u8fd0\u52a8\u89c4\u5212\u7684\u6536\u655b\u901f\u5ea6\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf"}}
{"id": "2508.19788", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19788", "abs": "https://arxiv.org/abs/2508.19788", "authors": ["Sena Ishii", "Akash Chikhalikar", "Ankit A. Ravankar", "Jose Victorio Salazar Luces", "Yasuhisa Hirata"], "title": "Context-Aware Risk Estimation in Home Environments: A Probabilistic Framework for Service Robots", "comment": "8 pages, Accepted for IEEE RO-MAN 2025 Conference", "summary": "We present a novel framework for estimating accident-prone regions in\neveryday indoor scenes, aimed at improving real-time risk awareness in service\nrobots operating in human-centric environments. As robots become integrated\ninto daily life, particularly in homes, the ability to anticipate and respond\nto environmental hazards is crucial for ensuring user safety, trust, and\neffective human-robot interaction. Our approach models object-level risk and\ncontext through a semantic graph-based propagation algorithm. Each object is\nrepresented as a node with an associated risk score, and risk propagates\nasymmetrically from high-risk to low-risk objects based on spatial proximity\nand accident relationship. This enables the robot to infer potential hazards\neven when they are not explicitly visible or labeled. Designed for\ninterpretability and lightweight onboard deployment, our method is validated on\na dataset with human-annotated risk regions, achieving a binary risk detection\naccuracy of 75%. The system demonstrates strong alignment with human\nperception, particularly in scenes involving sharp or unstable objects. These\nresults underline the potential of context-aware risk reasoning to enhance\nrobotic scene understanding and proactive safety behaviors in shared\nhuman-robot spaces. This framework could serve as a foundation for future\nsystems that make context-driven safety decisions, provide real-time alerts, or\nautonomously assist users in avoiding or mitigating hazards within home\nenvironments.", "AI": {"tldr": "\u57fa\u4e8e\u8bed\u4e49\u56fe\u7b97\u6cd5\u7684\u5ba4\u5185\u573a\u666f\u98ce\u9669\u533a\u57df\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u4f53\u98ce\u9669\u5206\u6570\u4f20\u64ad\u9884\u6d4b\u610f\u5916\u98ce\u9669\uff0c\u63d0\u5347\u670d\u52a1\u673a\u5668\u4eba\u7684\u5b89\u5168\u610f\u8bc6", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u66f4\u591a\u8fdb\u5165\u5bb6\u5ead\u751f\u6d3b\uff0c\u9884\u6d4b\u548c\u5e94\u5bf9\u73af\u5883\u98ce\u9669\u5bf9\u4e8e\u786e\u4fdd\u7528\u6237\u5b89\u5168\u3001\u5efa\u7acb\u4fe1\u4efb\u548c\u6709\u6548\u4eba\u673a\u4ea4\u4e92\u81f3\u5173\u91cd\u8981", "method": "\u4f7f\u7528\u8bed\u4e49\u56fe\u57fa\u4e8e\u4f20\u64ad\u7b97\u6cd5\uff0c\u5c06\u6bcf\u4e2a\u7269\u4f53\u8868\u793a\u4e3a\u5173\u8054\u98ce\u9669\u5206\u6570\u7684\u8282\u70b9\uff0c\u6839\u636e\u7a7a\u95f4\u8ddd\u79bb\u548c\u610f\u5916\u5173\u7cfb\u4ece\u9ad8\u98ce\u9669\u7269\u4f53\u5411\u4f4e\u98ce\u9669\u7269\u4f53\u4f5c\u4e0d\u5bf9\u79f0\u98ce\u9669\u4f20\u64ad", "result": "\u5728\u4eba\u5de5\u6807\u6ce8\u98ce\u9669\u533a\u57df\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8675%\u7684\u4e8c\u5143\u98ce\u9669\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u5c24\u5176\u5728\u6d89\u53ca\u5c16\u5229\u6216\u4e0d\u7a33\u5b9a\u7269\u4f53\u7684\u573a\u666f\u4e2d\u4e0e\u4eba\u7c7b\u611f\u77e5\u5f3a\u4e00\u81f4", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e0a\u4f20\u90e8\u7f72\u800c\u8bbe\u8ba1\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u80fd\u591f\u63d0\u5347\u673a\u5668\u4eba\u7684\u573a\u666f\u7406\u89e3\u80fd\u529b\u548c\u4e3b\u52a8\u5b89\u5168\u884c\u4e3a\uff0c\u4e3a\u5b9e\u65f6\u98ce\u9669\u9884\u8b66\u548c\u81ea\u4e3b\u98ce\u9669\u907f\u514d\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840"}}
{"id": "2508.19790", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19790", "abs": "https://arxiv.org/abs/2508.19790", "authors": ["Liding Zhang", "Sicheng Wang", "Kuanqi Cai", "Zhenshan Bing", "Fan Wu", "Chaoqun Wang", "Sami Haddadin", "Alois Knoll"], "title": "APT*: Asymptotically Optimal Motion Planning via Adaptively Prolated Elliptical R-Nearest Neighbors", "comment": null, "summary": "Optimal path planning aims to determine a sequence of states from a start to\na goal while accounting for planning objectives. Popular methods often\nintegrate fixed batch sizes and neglect information on obstacles, which is not\nproblem-specific. This study introduces Adaptively Prolated Trees (APT*), a\nnovel sampling-based motion planner that extends based on Force Direction\nInformed Trees (FDIT*), integrating adaptive batch-sizing and elliptical\n$r$-nearest neighbor modules to dynamically modulate the path searching process\nbased on environmental feedback. APT* adjusts batch sizes based on the\nhypervolume of the informed sets and considers vertices as electric charges\nthat obey Coulomb's law to define virtual forces via neighbor samples, thereby\nrefining the prolate nearest neighbor selection. These modules employ\nnon-linear prolate methods to adaptively adjust the electric charges of\nvertices for force definition, thereby improving the convergence rate with\nlower solution costs. Comparative analyses show that APT* outperforms existing\nsingle-query sampling-based planners in dimensions from $\\mathbb{R}^4$ to\n$\\mathbb{R}^{16}$, and it was further validated through a real-world robot\nmanipulation task. A video showcasing our experimental results is available at:\nhttps://youtu.be/gCcUr8LiEw4", "AI": {"tldr": "APT*\u662f\u4e00\u79cd\u65b0\u578b\u91c7\u6837\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6279\u91cf\u5927\u5c0f\u548c\u692d\u5706\u5f62\u6700\u8fd1\u90bb\u6a21\u5757\uff0c\u57fa\u4e8e\u73af\u5883\u53cd\u9988\u52a8\u6001\u8c03\u8282\u8def\u5f84\u641c\u7d22\u8fc7\u7a0b\uff0c\u57284-16\u7ef4\u7a7a\u95f4\u4e2d\u4f18\u4e8e\u73b0\u6709\u5355\u67e5\u8be2\u91c7\u6837\u89c4\u5212\u5668\u3002", "motivation": "\u73b0\u6709\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u56fa\u5b9a\u6279\u91cf\u5927\u5c0f\u4e14\u5ffd\u7565\u969c\u788d\u7269\u4fe1\u606f\uff0c\u7f3a\u4e4f\u95ee\u9898\u7279\u5f02\u6027\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u81ea\u9002\u5e94\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u6269\u5c55FDIT*\u7b97\u6cd5\uff0c\u96c6\u6210\u81ea\u9002\u5e94\u6279\u91cf\u8c03\u6574\u548c\u692d\u5706\u5f62r-\u6700\u8fd1\u90bb\u6a21\u5757\uff0c\u5c06\u9876\u70b9\u89c6\u4e3a\u9075\u5faa\u5e93\u4ed1\u5b9a\u5f8b\u7684\u7535\u8377\u6765\u5b9a\u4e49\u865a\u62df\u529b\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u65b9\u6cd5\u81ea\u9002\u5e94\u8c03\u6574\u7535\u8377\u4ee5\u6539\u8fdb\u6536\u655b\u6027\u3002", "result": "\u5728\u211d\u2074\u5230\u211d\u00b9\u2076\u7ef4\u5ea6\u7a7a\u95f4\u4e2d\uff0cAPT*\u5728\u6536\u655b\u901f\u5ea6\u548c\u89e3\u51b3\u65b9\u6848\u6210\u672c\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u5355\u67e5\u8be2\u91c7\u6837\u89c4\u5212\u5668\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "APT*\u901a\u8fc7\u81ea\u9002\u5e94\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19816", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19816", "abs": "https://arxiv.org/abs/2508.19816", "authors": ["Ricardo J. Manr\u00edquez-Cisterna", "Ankit A. Ravankar", "Jose V. Salazar Luces", "Takuro Hatsukari", "Yasuhisa Hirata"], "title": "A Standing Support Mobility Robot for Enhancing Independence in Elderly Daily Living", "comment": "7 pages, accepted work for IEEE RO-MAN2025", "summary": "This paper presents a standing support mobility robot \"Moby\" developed to\nenhance independence and safety for elderly individuals during daily activities\nsuch as toilet transfers. Unlike conventional seated mobility aids, the robot\nmaintains users in an upright posture, reducing physical strain, supporting\nnatural social interaction at eye level, and fostering a greater sense of\nself-efficacy. Moby offers a novel alternative by functioning both passively\nand with mobility support, enabling users to perform daily tasks more\nindependently. Its main advantages include ease of use, lightweight design,\ncomfort, versatility, and effective sit-to-stand assistance. The robot\nleverages the Robot Operating System (ROS) for seamless control, featuring\nmanual and autonomous operation modes. A custom control system enables safe and\nintuitive interaction, while the integration with NAV2 and LiDAR allows for\nrobust navigation capabilities. This paper reviews existing mobility solutions\nand compares them to Moby, details the robot's design, and presents objective\nand subjective experimental results using the NASA-TLX method and time\ncomparisons to other methods to validate our design criteria and demonstrate\nthe advantages of our contribution.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u540d\u4e3aMoby\u7684\u7ad9\u7acb\u652f\u6491\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u65e8\u5728\u5e2e\u52a9\u8001\u5e74\u4eba\u4fdd\u6301\u76f4\u7acb\u59ff\u52bf\u8fdb\u884c\u65e5\u5e38\u6d3b\u52a8\uff0c\u51cf\u5c11\u8eab\u4f53\u8d1f\u62c5\u5e76\u589e\u5f3a\u72ec\u7acb\u6027", "motivation": "\u4e3a\u8001\u5e74\u4eba\u63d0\u4f9b\u66f4\u597d\u7684\u65e5\u5e38\u6d3b\u52a8\u652f\u6301\uff0c\u7279\u522b\u662f\u5982\u5395\u8f6c\u79fb\u7b49\u573a\u666f\uff0c\u89e3\u51b3\u4f20\u7edf\u5750\u5f0f\u79fb\u52a8\u8f85\u52a9\u8bbe\u5907\u65e0\u6cd5\u4fdd\u6301\u76f4\u7acb\u59ff\u52bf\u7684\u95ee\u9898", "method": "\u57fa\u4e8eROS\u7cfb\u7edf\u5f00\u53d1\uff0c\u5177\u6709\u624b\u52a8\u548c\u81ea\u4e3b\u64cd\u4f5c\u6a21\u5f0f\uff0c\u96c6\u6210NAV2\u548cLiDAR\u5b9e\u73b0\u7a33\u5065\u5bfc\u822a\uff0c\u91c7\u7528\u5b9a\u5236\u63a7\u5236\u7cfb\u7edf\u786e\u4fdd\u5b89\u5168\u76f4\u89c2\u7684\u4ea4\u4e92", "result": "\u901a\u8fc7NASA-TLX\u65b9\u6cd5\u548c\u65f6\u95f4\u5bf9\u6bd4\u5b9e\u9a8c\u9a8c\u8bc1\uff0cMoby\u5728\u6613\u7528\u6027\u3001\u8f7b\u91cf\u5316\u8bbe\u8ba1\u3001\u8212\u9002\u6027\u3001\u591a\u529f\u80fd\u6027\u548c\u7ad9\u8d77\u8f85\u52a9\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u52bf", "conclusion": "Moby\u673a\u5668\u4eba\u4f5c\u4e3a\u4e00\u79cd\u521b\u65b0\u7684\u7ad9\u7acb\u652f\u6491\u79fb\u52a8\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8001\u5e74\u4eba\u7684\u72ec\u7acb\u6027\u548c\u5b89\u5168\u6027\uff0c\u4e3a\u65e5\u5e38\u6d3b\u52a8\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u652f\u6301"}}
{"id": "2508.19926", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19926", "abs": "https://arxiv.org/abs/2508.19926", "authors": ["Tan Jing", "Shiting Chen", "Yangfan Li", "Weisheng Xu", "Renjing Xu"], "title": "FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for Physics-Based High-Dynamic Humanoid Control", "comment": null, "summary": "Unified physics-based humanoid controllers are pivotal for robotics and\ncharacter animation, yet models that excel on gentle, everyday motions still\nstumble on explosive actions, hampering real-world deployment. We bridge this\ngap with FARM (Frame-Accelerated Augmentation and Residual Mixture-of-Experts),\nan end-to-end framework composed of frame-accelerated augmentation, a robust\nbase controller, and a residual mixture-of-experts (MoE). Frame-accelerated\naugmentation exposes the model to high-velocity pose changes by widening\ninter-frame gaps. The base controller reliably tracks everyday low-dynamic\nmotions, while the residual MoE adaptively allocates additional network\ncapacity to handle challenging high-dynamic actions, significantly enhancing\ntracking accuracy. In the absence of a public benchmark, we curate the\nHigh-Dynamic Humanoid Motion (HDHM) dataset, comprising 3593 physically\nplausible clips. On HDHM, FARM reduces the tracking failure rate by 42.8\\% and\nlowers global mean per-joint position error by 14.6\\% relative to the baseline,\nwhile preserving near-perfect accuracy on low-dynamic motions. These results\nestablish FARM as a new baseline for high-dynamic humanoid control and\nintroduce the first open benchmark dedicated to this challenge. The code and\ndataset will be released at https://github.com/Colin-Jing/FARM.", "AI": {"tldr": "FARM\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u4eba\u5f62\u63a7\u5236\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u5e27\u52a0\u901f\u589e\u5f3a\u3001\u57fa\u7840\u63a7\u5236\u5668\u548c\u6b8b\u5dee\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u52a8\u6001\u52a8\u4f5c\u7684\u8ddf\u8e2a\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u52a8\u6001\u52a8\u4f5c\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u5f62\u63a7\u5236\u5668\u5728\u5904\u7406\u65e5\u5e38\u6e29\u548c\u52a8\u4f5c\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5904\u7406\u7206\u53d1\u6027\u9ad8\u52a8\u6001\u52a8\u4f5c\u65f6\u5bb9\u6613\u5931\u8d25\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u90e8\u7f72\u5e94\u7528\u3002", "method": "FARM\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u5e27\u52a0\u901f\u589e\u5f3a\u6280\u672f\uff0c\u901a\u8fc7\u6269\u5927\u5e27\u95f4\u95f4\u9694\u8ba9\u6a21\u578b\u63a5\u89e6\u9ad8\u901f\u59ff\u6001\u53d8\u5316\uff1b2\uff09\u7a33\u5065\u7684\u57fa\u7840\u63a7\u5236\u5668\u5904\u7406\u4f4e\u52a8\u6001\u52a8\u4f5c\uff1b3\uff09\u6b8b\u5dee\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\u81ea\u9002\u5e94\u5206\u914d\u989d\u5916\u7f51\u7edc\u5bb9\u91cf\u5904\u7406\u9ad8\u52a8\u6001\u52a8\u4f5c\u3002", "result": "\u5728HDHM\u6570\u636e\u96c6\u4e0a\uff0cFARM\u5c06\u8ddf\u8e2a\u5931\u8d25\u7387\u964d\u4f4e\u4e8642.8%\uff0c\u5168\u5c40\u5e73\u5747\u5173\u8282\u4f4d\u7f6e\u8bef\u5dee\u964d\u4f4e\u4e8614.6%\uff0c\u540c\u65f6\u5728\u4f4e\u52a8\u6001\u52a8\u4f5c\u4e0a\u4fdd\u6301\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u6027\u3002", "conclusion": "FARM\u4e3a\u9ad8\u52a8\u6001\u4eba\u5f62\u63a7\u5236\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u8fd9\u4e00\u6311\u6218\u7684\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u5f00\u6e90\u53d1\u5e03\u3002"}}
{"id": "2508.19953", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19953", "abs": "https://arxiv.org/abs/2508.19953", "authors": ["Rafael Cathomen", "Mayank Mittal", "Marin Vlastelica", "Marco Hutter"], "title": "Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors", "comment": "Accepted to CoRL 2025. For code and videos, please check:\n  https://leggedrobotics.github.io/d3-skill-discovery/", "summary": "Unsupervised Skill Discovery (USD) allows agents to autonomously learn\ndiverse behaviors without task-specific rewards. While recent USD methods have\nshown promise, their application to real-world robotics remains underexplored.\nIn this paper, we propose a modular USD framework to address the challenges in\nthe safety, interpretability, and deployability of the learned skills. Our\napproach employs user-defined factorization of the state space to learn\ndisentangled skill representations. It assigns different skill discovery\nalgorithms to each factor based on the desired intrinsic reward function. To\nencourage structured morphology-aware skills, we introduce symmetry-based\ninductive biases tailored to individual factors. We also incorporate a style\nfactor and regularization penalties to promote safe and robust behaviors. We\nevaluate our framework in simulation using a quadrupedal robot and demonstrate\nzero-shot transfer of the learned skills to real hardware. Our results show\nthat factorization and symmetry lead to the discovery of structured\nhuman-interpretable behaviors, while the style factor and penalties enhance\nsafety and diversity. Additionally, we show that the learned skills can be used\nfor downstream tasks and perform on par with oracle policies trained with\nhand-crafted rewards.", "AI": {"tldr": "\u63d0\u51fa\u6a21\u5757\u5316\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\u6846\u67b6\uff0c\u901a\u8fc7\u72b6\u6001\u7a7a\u95f4\u5206\u89e3\u3001\u5bf9\u79f0\u6027\u504f\u7f6e\u548c\u98ce\u683c\u56e0\u5b50\u6765\u63d0\u9ad8\u673a\u5668\u4eba\u6280\u80fd\u7684\u5b89\u5168\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u90e8\u7f72\u6027", "motivation": "\u89e3\u51b3\u73b0\u6709\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\u65b9\u6cd5\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u9762\u4e34\u7684\u5b89\u5168\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u90e8\u7f72\u6027\u6311\u6218", "method": "\u7528\u6237\u5b9a\u4e49\u72b6\u6001\u7a7a\u95f4\u5206\u89e3\uff0c\u4e3a\u4e0d\u540c\u56e0\u5b50\u5206\u914d\u6280\u80fd\u53d1\u73b0\u7b97\u6cd5\uff0c\u5f15\u5165\u5bf9\u79f0\u6027\u5f52\u7eb3\u504f\u7f6e\u548c\u98ce\u683c\u56e0\u5b50\uff0c\u52a0\u5165\u6b63\u5219\u5316\u60e9\u7f5a\u9879", "result": "\u5728\u56db\u8db3\u673a\u5668\u4eba\u4eff\u771f\u4e2d\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u771f\u5b9e\u786c\u4ef6\uff0c\u83b7\u5f97\u7ed3\u6784\u5316\u3001\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u884c\u4e3a\uff0c\u5b89\u5168\u6027\u548c\u591a\u6837\u6027\u5f97\u5230\u63d0\u5347", "conclusion": "\u5206\u89e3\u548c\u5bf9\u79f0\u6027\u6709\u52a9\u4e8e\u53d1\u73b0\u7ed3\u6784\u5316\u6280\u80fd\uff0c\u98ce\u683c\u56e0\u5b50\u548c\u60e9\u7f5a\u9879\u589e\u5f3a\u5b89\u5168\u6027\u548c\u591a\u6837\u6027\uff0c\u5b66\u4e60\u5230\u7684\u6280\u80fd\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0e\u624b\u5de5\u5956\u52b1\u8bad\u7ec3\u7684\u7b56\u7565\u76f8\u5f53"}}
{"id": "2508.19958", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19958", "abs": "https://arxiv.org/abs/2508.19958", "authors": ["Yiguo Fan", "Pengxiang Ding", "Shuanghao Bai", "Xinyang Tong", "Yuyang Zhu", "Hongchao Lu", "Fengqi Dai", "Wei Zhao", "Yang Liu", "Siteng Huang", "Zhaoxin Fan", "Badong Chen", "Donglin Wang"], "title": "Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation", "comment": "Accepted to CoRL 2025; Github Page: https://long-vla.github.io", "summary": "Vision-Language-Action (VLA) models have become a cornerstone in robotic\npolicy learning, leveraging large-scale multimodal data for robust and scalable\ncontrol. However, existing VLA frameworks primarily address short-horizon\ntasks, and their effectiveness on long-horizon, multi-step robotic manipulation\nremains limited due to challenges in skill chaining and subtask dependencies.\nIn this work, we introduce Long-VLA, the first end-to-end VLA model\nspecifically designed for long-horizon robotic tasks. Our approach features a\nnovel phase-aware input masking strategy that adaptively segments each subtask\ninto moving and interaction phases, enabling the model to focus on\nphase-relevant sensory cues and enhancing subtask compatibility. This unified\nstrategy preserves the scalability and data efficiency of VLA training, and our\narchitecture-agnostic module can be seamlessly integrated into existing VLA\nmodels. We further propose the L-CALVIN benchmark to systematically evaluate\nlong-horizon manipulation. Extensive experiments on both simulated and\nreal-world tasks demonstrate that Long-VLA significantly outperforms prior\nstate-of-the-art methods, establishing a new baseline for long-horizon robotic\ncontrol.", "AI": {"tldr": "Long-VLA\u662f\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u4efb\u52a1\u7684\u7aef\u5230\u7aef\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u76f8\u4f4d\u611f\u77e5\u8f93\u5165\u63a9\u7801\u7b56\u7565\u5c06\u5b50\u4efb\u52a1\u5206\u4e3a\u79fb\u52a8\u548c\u4ea4\u4e92\u9636\u6bb5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u65f6\u7a0b\u64cd\u4f5c\u6027\u80fd", "motivation": "\u73b0\u6709\u7684VLA\u6846\u67b6\u4e3b\u8981\u5904\u7406\u77ed\u65f6\u7a0b\u4efb\u52a1\uff0c\u5728\u957f\u65f6\u7a0b\u3001\u591a\u6b65\u9aa4\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7531\u4e8e\u6280\u80fd\u94fe\u548c\u5b50\u4efb\u52a1\u4f9d\u8d56\u6027\u7684\u6311\u6218\u800c\u6548\u679c\u6709\u9650", "method": "\u63d0\u51fa\u76f8\u4f4d\u611f\u77e5\u8f93\u5165\u63a9\u7801\u7b56\u7565\uff0c\u5c06\u6bcf\u4e2a\u5b50\u4efb\u52a1\u81ea\u9002\u5e94\u5206\u5272\u4e3a\u79fb\u52a8\u548c\u4ea4\u4e92\u9636\u6bb5\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5173\u6ce8\u9636\u6bb5\u76f8\u5173\u7684\u611f\u5b98\u7ebf\u7d22\uff1b\u63d0\u51faL-CALVIN\u57fa\u51c6\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLong-VLA\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e3a\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u63a7\u5236\u5efa\u7acb\u4e86\u65b0\u57fa\u51c6", "conclusion": "Long-VLA\u901a\u8fc7\u7edf\u4e00\u7684\u7b56\u7565\u4fdd\u6301\u4e86VLA\u8bad\u7ec3\u7684\u53ef\u6269\u5c55\u6027\u548c\u6570\u636e\u6548\u7387\uff0c\u5176\u67b6\u6784\u65e0\u5173\u7684\u6a21\u5757\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709VLA\u6a21\u578b\u4e2d"}}
{"id": "2508.20037", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.20037", "abs": "https://arxiv.org/abs/2508.20037", "authors": ["Henk H. A. Jekel", "Alejandro D\u00edaz Rosales", "Luka Peternel"], "title": "Visio-Verbal Teleimpedance Interface: Enabling Semi-Autonomous Control of Physical Interaction via Eye Tracking and Speech", "comment": null, "summary": "The paper presents a visio-verbal teleimpedance interface for commanding 3D\nstiffness ellipsoids to the remote robot with a combination of the operator's\ngaze and verbal interaction. The gaze is detected by an eye-tracker, allowing\nthe system to understand the context in terms of what the operator is currently\nlooking at in the scene. Along with verbal interaction, a Visual Language Model\n(VLM) processes this information, enabling the operator to communicate their\nintended action or provide corrections. Based on these inputs, the interface\ncan then generate appropriate stiffness matrices for different physical\ninteraction actions. To validate the proposed visio-verbal teleimpedance\ninterface, we conducted a series of experiments on a setup including a Force\nDimension Sigma.7 haptic device to control the motion of the remote Kuka LBR\niiwa robotic arm. The human operator's gaze is tracked by Tobii Pro Glasses 2,\nwhile human verbal commands are processed by a VLM using GPT-4o. The first\nexperiment explored the optimal prompt configuration for the interface. The\nsecond and third experiments demonstrated different functionalities of the\ninterface on a slide-in-the-groove task.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9-\u8bed\u97f3\u8fdc\u7a0b\u963b\u6297\u63a5\u53e3\uff0c\u901a\u8fc7\u7ecf\u9a8c\u7684\u89c6\u7ebf\u8ddf\u8e2a\u548c\u8bed\u97f3\u4ea4\u4e92\u6765\u63a7\u5236\u8fdc\u7a0b\u673a\u5668\u4eba\u76843D\u521a\u5ea6\u692d\u5706\u4f53\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u8fdc\u7a0b\u64cd\u4f5c\u7684\u76f4\u89c2\u6027\u548c\u6548\u7387\uff0c\u901a\u8fc7\u7ed3\u5408\u4eba\u7684\u89c6\u89c9\u548c\u8bed\u97f3\u8f93\u5165\u6765\u751f\u6210\u9002\u5f53\u7684\u673a\u5668\u4eba\u521a\u5ea6\u77e9\u9635\uff0c\u4ee5\u652f\u6301\u4e0d\u540c\u7684\u7269\u7406\u4ea4\u4e92\u52a8\u4f5c\u3002", "method": "\u4f7f\u7528Tobii Pro Glasses 2\u8fdb\u884c\u773c\u7403\u8ddf\u8e2a\u68c0\u6d4b\u64cd\u4f5c\u5458\u89c6\u7ebf\uff0c\u901a\u8fc7GPT-4o\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\u8bed\u97f3\u547d\u4ee4\uff0c\u4f7f\u7528Force Dimension Sigma.7\u529b\u89e6\u89c9\u8bbe\u5907\u63a7\u5236Kuka LBR iiwa\u673a\u5668\u624b\u81c2\u8fd0\u52a8\u3002", "result": "\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u5b9e\u9a8c\uff1a\u7b2c\u4e00\u4e2a\u5b9e\u9a8c\u63a2\u7d22\u4e86\u63a5\u53e3\u7684\u6700\u4f73\u63d0\u793a\u914d\u7f6e\uff0c\u7b2c\u4e8c\u548c\u7b2c\u4e09\u4e2a\u5b9e\u9a8c\u5728\u6ed1\u69fd\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u63a5\u53e3\u7684\u4e0d\u540c\u529f\u80fd\u6027\u3002", "conclusion": "\u8be5\u89c6\u89c9-\u8bed\u97f3\u8fdc\u7a0b\u963b\u6297\u63a5\u53e3\u80fd\u591f\u6709\u6548\u5730\u901a\u8fc7\u7ecf\u9a8c\u7684\u89c6\u7ebf\u8ddf\u8e2a\u548c\u8bed\u97f3\u4ea4\u4e92\u6765\u63a7\u5236\u673a\u5668\u4eba\u7684\u521a\u5ea6\u7279\u6027\uff0c\u4e3a\u8fdc\u7a0b\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u548c\u9ad8\u6548\u7684\u65b9\u5f0f\u3002"}}
{"id": "2508.20085", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.20085", "abs": "https://arxiv.org/abs/2508.20085", "authors": ["Zhecheng Yuan", "Tianming Wei", "Langzhe Gu", "Pu Hua", "Tianhai Liang", "Yuanpei Chen", "Huazhe Xu"], "title": "HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation", "comment": null, "summary": "Leveraging human motion data to impart robots with versatile manipulation\nskills has emerged as a promising paradigm in robotic manipulation.\nNevertheless, translating multi-source human hand motions into feasible robot\nbehaviors remains challenging, particularly for robots equipped with\nmulti-fingered dexterous hands characterized by complex, high-dimensional\naction spaces. Moreover, existing approaches often struggle to produce policies\ncapable of adapting to diverse environmental conditions. In this paper, we\nintroduce HERMES, a human-to-robot learning framework for mobile bimanual\ndexterous manipulation. First, HERMES formulates a unified reinforcement\nlearning approach capable of seamlessly transforming heterogeneous human hand\nmotions from multiple sources into physically plausible robotic behaviors.\nSubsequently, to mitigate the sim2real gap, we devise an end-to-end, depth\nimage-based sim2real transfer method for improved generalization to real-world\nscenarios. Furthermore, to enable autonomous operation in varied and\nunstructured environments, we augment the navigation foundation model with a\nclosed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise\nalignment of visual goals and effectively bridging autonomous navigation and\ndexterous manipulation. Extensive experimental results demonstrate that HERMES\nconsistently exhibits generalizable behaviors across diverse, in-the-wild\nscenarios, successfully performing numerous complex mobile bimanual dexterous\nmanipulation tasks. Project Page:https:/gemcollector.github.io/HERMES/.", "AI": {"tldr": "HERMES\u662f\u4e00\u4e2a\u4eba\u7c7b\u5230\u673a\u5668\u4eba\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u79fb\u52a8\u53cc\u624b\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c\uff0c\u80fd\u591f\u5c06\u591a\u6e90\u4eba\u7c7b\u624b\u90e8\u8fd0\u52a8\u8f6c\u5316\u4e3a\u53ef\u884c\u7684\u673a\u5668\u4eba\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u3001sim2real\u8fc1\u79fb\u548c\u5bfc\u822a\u589e\u5f3a\u5b9e\u73b0\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c06\u591a\u6e90\u4eba\u7c7b\u624b\u90e8\u8fd0\u52a8\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u53ef\u884c\u884c\u4e3a\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5177\u6709\u590d\u6742\u9ad8\u7ef4\u52a8\u4f5c\u7a7a\u95f4\u7684\u591a\u6307\u7075\u5de7\u624b\u673a\u5668\u4eba\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u73af\u5883\u6761\u4ef6\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8f6c\u6362\u4eba\u7c7b\u624b\u90e8\u8fd0\u52a8\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u6df1\u5ea6\u56fe\u50cf\u7684\u7aef\u5230\u7aefsim2real\u8fc1\u79fb\u65b9\u6cd5\uff0c\u5e76\u589e\u5f3a\u5bfc\u822a\u57fa\u7840\u6a21\u578b\u91c7\u7528\u95ed\u73afPnP\u5b9a\u4f4d\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eHERMES\u5728\u591a\u6837\u5316\u91ce\u5916\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u53ef\u6cdb\u5316\u7684\u884c\u4e3a\uff0c\u6210\u529f\u6267\u884c\u4f17\u591a\u590d\u6742\u7684\u79fb\u52a8\u53cc\u624b\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u3002", "conclusion": "HERMES\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4eba\u7c7b\u8fd0\u52a8\u5230\u673a\u5668\u4eba\u884c\u4e3a\u7684\u8f6c\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u7efc\u5408\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u79fb\u52a8\u7075\u5de7\u64cd\u4f5c\u80fd\u529b\u3002"}}
{"id": "2508.20095", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20095", "abs": "https://arxiv.org/abs/2508.20095", "authors": ["Jinhao Liang", "Sven Koenig", "Ferdinando Fioretto"], "title": "Discrete-Guided Diffusion for Scalable and Safe Multi-Robot Motion Planning", "comment": null, "summary": "Multi-Robot Motion Planning (MRMP) involves generating collision-free\ntrajectories for multiple robots operating in a shared continuous workspace.\nWhile discrete multi-agent path finding (MAPF) methods are broadly adopted due\nto their scalability, their coarse discretization severely limits trajectory\nquality. In contrast, continuous optimization-based planners offer\nhigher-quality paths but suffer from the curse of dimensionality, resulting in\npoor scalability with respect to the number of robots. This paper tackles the\nlimitations of these two approaches by introducing a novel framework that\nintegrates discrete MAPF solvers with constrained generative diffusion models.\nThe resulting framework, called Discrete-Guided Diffusion (DGD), has three key\ncharacteristics: (1) it decomposes the original nonconvex MRMP problem into\ntractable subproblems with convex configuration spaces, (2) it combines\ndiscrete MAPF solutions with constrained optimization techniques to guide\ndiffusion models capture complex spatiotemporal dependencies among robots, and\n(3) it incorporates a lightweight constraint repair mechanism to ensure\ntrajectory feasibility. The proposed method sets a new state-of-the-art\nperformance in large-scale, complex environments, scaling to 100 robots while\nachieving planning efficiency and high success rates.", "AI": {"tldr": "\u63d0\u51faDGD\u6846\u67b6\uff0c\u7ed3\u5408\u79bb\u6563MAPF\u6c42\u89e3\u5668\u548c\u751f\u6210\u6269\u6563\u6a21\u578b\uff0c\u89e3\u51b3\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u5728100\u4e2a\u673a\u5668\u4eba\u7684\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u89c4\u5212\u548c\u9ad8\u6210\u529f\u7387", "motivation": "\u79bb\u6563MAPF\u65b9\u6cd5\u53ef\u6269\u5c55\u4f46\u8f68\u8ff9\u8d28\u91cf\u4f4e\uff0c\u8fde\u7eed\u4f18\u5316\u65b9\u6cd5\u8d28\u91cf\u9ad8\u4f46\u7ef4\u5ea6\u707e\u96be\u96be\u4ee5\u6269\u5c55\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u65b0\u65b9\u6cd5", "method": "\u5c06\u975e\u51f8MRMP\u95ee\u9898\u5206\u89e3\u4e3a\u51f8\u914d\u7f6e\u7a7a\u95f4\u7684\u5b50\u95ee\u9898\uff0c\u7528\u79bb\u6563MAPF\u89e3\u6307\u5bfc\u6269\u6563\u6a21\u578b\u6355\u6349\u65f6\u7a7a\u4f9d\u8d56\uff0c\u52a0\u5165\u8f7b\u91cf\u7ea6\u675f\u4fee\u590d\u673a\u5236\u4fdd\u8bc1\u53ef\u884c\u6027", "result": "\u5728\u5927\u89c4\u6a21\u590d\u6742\u73af\u5883\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u53ef\u6269\u5c55\u5230100\u4e2a\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u9ad8\u6548\u89c4\u5212\u548c\u9ad8\u6210\u529f\u7387", "conclusion": "DGD\u6846\u67b6\u6210\u529f\u6574\u5408\u79bb\u6563\u548c\u8fde\u7eed\u65b9\u6cd5\u4f18\u52bf\uff0c\u4e3a\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9ad8\u8d28\u91cf\u89e3\u51b3\u65b9\u6848"}}
