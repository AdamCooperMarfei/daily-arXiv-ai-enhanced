<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 41]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Kilometer-Scale GNSS-Denied UAV Navigation via Heightmap Gradients: A Winning System from the SPRIN-D Challenge](https://arxiv.org/abs/2510.01348)
*Michal Werner,David Čapek,Tomáš Musil,Ondřej Franěk,Tomáš Báča,Martin Saska*

Main category: cs.RO

TL;DR: 提出一种完全机载的无人机系统，在GNSS拒止环境中通过激光雷达高度图匹配和粒子滤波实现长距离导航，显著减少里程计漂移。


<details>
  <summary>Details</summary>
Motivation: 解决无人机在GNSS拒止环境中长距离飞行的挑战，包括里程计漂移、新区域缺乏闭环检测以及嵌入式平台计算能力有限的问题。

Method: 集成感知、建图、规划和控制，采用轻量级漂移校正方法，通过梯度模板匹配将激光雷达局部高度图与先验地理数据高度图匹配，并使用聚类粒子滤波融合证据与里程计。

Result: 在竞赛中执行了公里级飞行，跨越城市、森林和开阔地带，相比原始里程计显著减少漂移，并在仅使用CPU的硬件上实时运行。

Conclusion: 系统展示了GNSS拒止环境下无人机自主飞行的可行性，为相关系统设计提供了实用的现场部署经验。

Abstract: Reliable long-range flight of unmanned aerial vehicles (UAVs) in GNSS-denied
environments is challenging: integrating odometry leads to drift, loop closures
are unavailable in previously unseen areas and embedded platforms provide
limited computational power. We present a fully onboard UAV system developed
for the SPRIN-D Funke Fully Autonomous Flight Challenge, which required 9 km
long-range waypoint navigation below 25 m AGL (Above Ground Level) without GNSS
or prior dense mapping. The system integrates perception, mapping, planning,
and control with a lightweight drift-correction method that matches
LiDAR-derived local heightmaps to a prior geo-data heightmap via
gradient-template matching and fuses the evidence with odometry in a clustered
particle filter. Deployed during the competition, the system executed
kilometer-scale flights across urban, forest, and open-field terrain and
reduced drift substantially relative to raw odometry, while running in real
time on CPU-only hardware. We describe the system architecture, the
localization pipeline, and the competition evaluation, and we report practical
insights from field deployment that inform the design of GNSS-denied UAV
autonomy.

</details>


### [2] [Safe Motion Planning and Control Using Predictive and Adaptive Barrier Methods for Autonomous Surface Vessels](https://arxiv.org/abs/2510.01357)
*Alejandro Gonzalez-Garcia,Wei Xiao,Wei Wang,Alejandro Astudillo,Wilm Decré,Jan Swevers,Carlo Ratti,Daniela Rus*

Main category: cs.RO

TL;DR: 提出一种结合MPC和CBF的安全运动规划策略，使用时变膨胀椭圆障碍物表示，通过自适应膨胀减少控制器的保守性，实现自主船舶在狭窄水域的实时安全导航。


<details>
  <summary>Details</summary>
Motivation: 传统运动规划方法在狭窄内河水域等挑战性空间中计算密集或过于保守，需要一种既能保证安全又能实时导航的解决方案。

Method: 结合模型预测控制(MPC)和控制屏障函数(CBFs)，使用时变膨胀椭圆障碍物表示，根据船舶与障碍物的相对位置和姿态自适应调整膨胀半径。

Result: 仿真和真实世界实验表明，该策略使全驱动自主机器人船舶能够在实时条件下安全通过狭窄空间，并解决潜在的僵局问题。

Conclusion: 所提出的自适应膨胀方法相比传统固定椭圆障碍物表述减少了保守性，同时确保了船舶的安全性，为自主船舶在狭窄水域的导航提供了有效的解决方案。

Abstract: Safe motion planning is essential for autonomous vessel operations,
especially in challenging spaces such as narrow inland waterways. However,
conventional motion planning approaches are often computationally intensive or
overly conservative. This paper proposes a safe motion planning strategy
combining Model Predictive Control (MPC) and Control Barrier Functions (CBFs).
We introduce a time-varying inflated ellipse obstacle representation, where the
inflation radius is adjusted depending on the relative position and attitude
between the vessel and the obstacle. The proposed adaptive inflation reduces
the conservativeness of the controller compared to traditional fixed-ellipsoid
obstacle formulations. The MPC solution provides an approximate motion plan,
and high-order CBFs ensure the vessel's safety using the varying inflation
radius. Simulation and real-world experiments demonstrate that the proposed
strategy enables the fully-actuated autonomous robot vessel to navigate through
narrow spaces in real time and resolve potential deadlocks, all while ensuring
safety.

</details>


### [3] [A Stochastic Framework for Continuous-Time State Estimation of Continuum Robots](https://arxiv.org/abs/2510.01381)
*Spencer Teetaert,Sven Lilge,Jessica Burgner-Kahrs,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: 提出了一种基于因子图优化的连续时间随机状态估计框架，用于连续体机器人的状态估计，能够适应未建模的外部力和数据丢失。


<details>
  <summary>Details</summary>
Motivation: 现有的连续体机器人状态估计技术通常使用计算复杂的动态模型、简化的形状近似或仅限于准静态方法，这些方法对未建模的扰动敏感。

Method: 基于连续时间运动学的因子图优化方法，使用简单的机器人模型配合高频率传感，通过高斯过程处理噪声。

Result: 能够连续估计机器人的位姿、速度和应变的均值与协方差，具有线性求解复杂度，并在实际系统中验证了其有效性。

Conclusion: 该方法为连续体机器人提供了一种灵活、高效的状态估计方案，能够处理实际系统中的不确定性和数据丢失问题。

Abstract: State estimation techniques for continuum robots (CRs) typically involve
using computationally complex dynamic models, simplistic shape approximations,
or are limited to quasi-static methods. These limitations can be sensitive to
unmodelled disturbances acting on the robot. Inspired by a factor-graph
optimization paradigm, this work introduces a continuous-time stochastic state
estimation framework for continuum robots. We introduce factors based on
continuous-time kinematics that are corrupted by a white noise Gaussian process
(GP). By using a simple robot model paired with high-rate sensing, we show
adaptability to unmodelled external forces and data dropout. The result
contains an estimate of the mean and covariance for the robot's pose, velocity,
and strain, each of which can be interpolated continuously in time or space.
This same interpolation scheme can be used during estimation, allowing for
inclusion of measurements on states that are not explicitly estimated. Our
method's inherent sparsity leads to a linear solve complexity with respect to
time and interpolation queries in constant time. We demonstrate our method on a
CR with gyroscope and pose sensors, highlighting its versatility in real-world
systems.

</details>


### [4] [VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation](https://arxiv.org/abs/2510.01388)
*Arthur Zhang,Xiangyun Meng,Luca Calliari,Dong-Ki Kim,Shayegan Omidshafiei,Joydeep Biswas,Ali Agha,Amirreza Shaban*

Main category: cs.RO

TL;DR: VENTURA是一个视觉语言导航系统，通过微调预训练的扩散模型进行路径规划，生成视觉路径掩码，再由轻量级策略执行轨迹，在真实环境中显著提升了导航性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉语言模型在机器人导航任务中因动作空间差异和预训练目标不匹配导致的迁移困难问题。

Method: 使用互联网预训练的扩散模型进行微调，生成图像空间中的路径掩码作为视觉规划，配合轻量级行为克隆策略执行轨迹。

Result: 在真实世界评估中，VENTURA在物体到达、避障和地形偏好任务上优于最先进的基础模型基线，成功率提高33%，碰撞减少54%，并能泛化到未见过的任务组合。

Conclusion: VENTURA展示了通过视觉路径规划实现自然语言导航的有效性，具有涌现的组合能力，为开放世界机器人导航提供了有前景的解决方案。

Abstract: Robots must adapt to diverse human instructions and operate safely in
unstructured, open-world environments. Recent Vision-Language models (VLMs)
offer strong priors for grounding language and perception, but remain difficult
to steer for navigation due to differences in action spaces and pretraining
objectives that hamper transferability to robotics tasks. Towards addressing
this, we introduce VENTURA, a vision-language navigation system that finetunes
internet-pretrained image diffusion models for path planning. Instead of
directly predicting low-level actions, VENTURA generates a path mask (i.e. a
visual plan) in image space that captures fine-grained, context-aware
navigation behaviors. A lightweight behavior-cloning policy grounds these
visual plans into executable trajectories, yielding an interface that follows
natural language instructions to generate diverse robot behaviors. To scale
training, we supervise on path masks derived from self-supervised tracking
models paired with VLM-augmented captions, avoiding manual pixel-level
annotation or highly engineered data collection setups. In extensive real-world
evaluations, VENTURA outperforms state-of-the-art foundation model baselines on
object reaching, obstacle avoidance, and terrain preference tasks, improving
success rates by 33% and reducing collisions by 54% across both seen and unseen
scenarios. Notably, we find that VENTURA generalizes to unseen combinations of
distinct tasks, revealing emergent compositional capabilities. Videos, code,
and additional materials: https://venturapath.github.io

</details>


### [5] [INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models](https://arxiv.org/abs/2510.01389)
*Ulas Berk Karli,Ziyao Shangguan,Tesca FItzgerald*

Main category: cs.RO

TL;DR: INSIGHT框架利用token级不确定性信号来预测VLA模型何时需要请求人类帮助，通过训练紧凑的transformer分类器实现失败预测。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型缺乏内省机制来预测失败并请求人类监督，需要开发能够主动识别不确定性并寻求帮助的机制。

Method: 使用π₀-FAST作为基础模型，提取token级的熵、对数概率、以及基于Dirichlet的偶然和认知不确定性估计，训练transformer分类器将这些序列映射到帮助触发信号。

Result: 强监督标签能捕捉细粒度不确定性动态以实现可靠的帮助检测，弱监督标签在训练和评估对齐时仍具有竞争力，且建模token级不确定性信号的时间演化比静态序列级分数具有更强的预测能力。

Conclusion: 这是对VLA中基于不确定性的内省机制的首个系统评估，为主动学习和通过选择性人类干预实现实时错误缓解开辟了新途径。

Abstract: Recent Vision-Language-Action (VLA) models show strong generalization
capabilities, yet they lack introspective mechanisms for anticipating failures
and requesting help from a human supervisor. We present \textbf{INSIGHT}, a
learning framework for leveraging token-level uncertainty signals to predict
when a VLA should request help. Using $\pi_0$-FAST as the underlying model, we
extract per-token \emph{entropy}, \emph{log-probability}, and Dirichlet-based
estimates of \emph{aleatoric and epistemic uncertainty}, and train compact
transformer classifiers to map these sequences to help triggers. We explore
supervision regimes for strong or weak supervision, and extensively compare
them across in-distribution and out-of-distribution tasks. Our results show a
trade-off: strong labels enable models to capture fine-grained uncertainty
dynamics for reliable help detection, while weak labels, though noisier, still
support competitive introspection when training and evaluation are aligned,
offering a scalable path when dense annotation is impractical. Crucially, we
find that modeling the temporal evolution of token-level uncertainty signals
with transformers provides far greater predictive power than static
sequence-level scores. This study provides the first systematic evaluation of
uncertainty-based introspection in VLAs, opening future avenues for active
learning and for real-time error mitigation through selective human
intervention.

</details>


### [6] [Beyond Collision Cones: Dynamic Obstacle Avoidance for Nonholonomic Robots via Dynamic Parabolic Control Barrier Functions](https://arxiv.org/abs/2510.01402)
*Hun Kuk Park,Taekyung Kim,Dimitra Panagou*

Main category: cs.RO

TL;DR: 提出动态抛物线控制屏障函数(DPCBF)来解决非完整机器人在密集动态环境中安全导航的问题，相比基于碰撞锥的方法显著提高了导航成功率和QP可行性。


<details>
  <summary>Details</summary>
Motivation: 现有基于碰撞锥或速度障碍物的CBF方法在密集动态环境中过于保守，容易导致二次规划不可行，限制了非完整机器人在复杂环境中的安全导航能力。

Method: 设计了动态抛物线控制屏障函数，其抛物线边界根据与障碍物的距离和相对速度大小动态调整顶点和曲率，形成限制性较小的安全约束。

Result: 在包含多达100个动态障碍物的密集环境中成功导航，相比基线方法显著提高了导航成功率和QP可行性。

Conclusion: DPCBF方法为非完整机器人在密集动态环境中的安全导航提供了有效的解决方案，克服了传统方法的保守性和可行性问题。

Abstract: Control Barrier Functions (CBFs) are a powerful tool for ensuring the safety
of autonomous systems, yet applying them to nonholonomic robots in cluttered,
dynamic environments remains an open challenge. State-of-the-art methods often
rely on collision-cone or velocity-obstacle constraints which, by only
considering the angle of the relative velocity, are inherently conservative and
can render the CBF-based quadratic program infeasible, particularly in dense
scenarios. To address this issue, we propose a Dynamic Parabolic Control
Barrier Function (DPCBF) that defines the safe set using a parabolic boundary.
The parabola's vertex and curvature dynamically adapt based on both the
distance to an obstacle and the magnitude of the relative velocity, creating a
less restrictive safety constraint. We prove that the proposed DPCBF is valid
for a kinematic bicycle model subject to input constraints. Extensive
comparative simulations demonstrate that our DPCBF-based controller
significantly enhances navigation success rates and QP feasibility compared to
baseline methods. Our approach successfully navigates through dense
environments with up to 100 dynamic obstacles, scenarios where collision
cone-based methods fail due to infeasibility.

</details>


### [7] [How Well do Diffusion Policies Learn Kinematic Constraint Manifolds?](https://arxiv.org/abs/2510.01404)
*Lexi Foland,Thomas Cohn,Adam Wei,Nicholas Pfaff,Boyuan Chen,Russ Tedrake*

Main category: cs.RO

TL;DR: 本文研究扩散策略在机器人模仿学习中学习运动学约束的能力，通过双手拾放任务分析数据集大小、质量和流形曲率对约束学习的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散策略在机器人模仿学习中表现出色，但任务性能不能可靠反映策略精确学习训练数据中约束的能力，需要研究扩散策略如何发现这些约束流形。

Method: 通过双手拾放任务的案例研究，分析三个因素对训练策略的影响：数据集大小、数据集质量和流形曲率。

Result: 扩散策略学习到约束流形的粗略近似，学习效果受数据集大小和质量下降的负面影响，而流形曲率与约束满足度和任务成功率的关联不明确。硬件评估验证了结果的现实适用性。

Conclusion: 扩散策略能够学习运动学约束，但学习效果受数据集特征影响，需要进一步研究如何提高约束学习的精确度。

Abstract: Diffusion policies have shown impressive results in robot imitation learning,
even for tasks that require satisfaction of kinematic equality constraints.
However, task performance alone is not a reliable indicator of the policy's
ability to precisely learn constraints in the training data. To investigate, we
analyze how well diffusion policies discover these manifolds with a case study
on a bimanual pick-and-place task that encourages fulfillment of a kinematic
constraint for success. We study how three factors affect trained policies:
dataset size, dataset quality, and manifold curvature. Our experiments show
diffusion policies learn a coarse approximation of the constraint manifold with
learning affected negatively by decreases in both dataset size and quality. On
the other hand, the curvature of the constraint manifold showed inconclusive
correlations with both constraint satisfaction and task success. A hardware
evaluation verifies the applicability of our results in the real world. Project
website with additional results and visuals:
https://diffusion-learns-kinematic.github.io

</details>


### [8] [AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation](https://arxiv.org/abs/2510.01433)
*Anukriti Singh,Kasra Torshizi,Khuzema Habib,Kelin Yu,Ruohan Gao,Pratap Tokekar*

Main category: cs.RO

TL;DR: AFFORD2ACT是一个基于视觉的机器人学习框架，通过文本提示和单张图像提取语义2D关键点，构建紧凑的38维状态策略，在15分钟内完成训练，在多样化真实世界操作任务中实现82%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于关键点的方法要么依赖手动启发式规则，要么受限于任务耦合选择，限制了可扩展性和语义理解能力。需要一种能够从文本提示中提取语义关键点的轻量级方法。

Method: 采用三阶段流程：功能过滤、类别级关键点构建、基于transformer的策略学习（带嵌入式门控机制），推理最相关的关键点。

Result: 在未见过的物体、新类别、背景和干扰物上实现82%的成功率，显著提高数据效率，无需本体感知或密集表示即可实时运行。

Conclusion: AFFORD2ACT提供了一种可扩展的语义关键点提取方法，在多样化真实世界操作任务中表现出色，平衡了计算效率和语义理解能力。

Abstract: Vision-based robot learning often relies on dense image or point-cloud
inputs, which are computationally heavy and entangle irrelevant background
features. Existing keypoint-based approaches can focus on manipulation-centric
features and be lightweight, but either depend on manual heuristics or
task-coupled selection, limiting scalability and semantic understanding. To
address this, we propose AFFORD2ACT, an affordance-guided framework that
distills a minimal set of semantic 2D keypoints from a text prompt and a single
image. AFFORD2ACT follows a three-stage pipeline: affordance filtering,
category-level keypoint construction, and transformer-based policy learning
with embedded gating to reason about the most relevant keypoints, yielding a
compact 38-dimensional state policy that can be trained in 15 minutes, which
performs well in real-time without proprioception or dense representations.
Across diverse real-world manipulation tasks, AFFORD2ACT consistently improves
data efficiency, achieving an 82% success rate on unseen objects, novel
categories, backgrounds, and distractors.

</details>


### [9] [Differentiable Skill Optimisation for Powder Manipulation in Laboratory Automation](https://arxiv.org/abs/2510.01438)
*Minglun Wei,Xintong Yang,Yu-Kun Lai,Ze Ji*

Main category: cs.RO

TL;DR: 提出了一种用于实验室粉末运输的轨迹优化框架，结合可微分物理模拟、低维技能空间参数化和课程学习策略，实现接触丰富的机器人轨迹端到端优化。


<details>
  <summary>Details</summary>
Motivation: 机器人自动化加速科学发现，但粉末精确操作仍具挑战性，特别是在需要精度和稳定性的运输任务中。

Method: 集成可微分物理模拟精确建模颗粒动力学，使用低维技能空间参数化降低优化复杂度，采用课程学习策略逐步提升长期任务能力。

Result: 实验结果表明，该方法相比强化学习基线实现了更高的任务成功率和稳定性。

Conclusion: 该框架能够在保持稳定性和收敛效率的同时，实现接触丰富机器人轨迹的端到端优化。

Abstract: Robotic automation is accelerating scientific discovery by reducing manual
effort in laboratory workflows. However, precise manipulation of powders
remains challenging, particularly in tasks such as transport that demand
accuracy and stability. We propose a trajectory optimisation framework for
powder transport in laboratory settings, which integrates differentiable
physics simulation for accurate modelling of granular dynamics, low-dimensional
skill-space parameterisation to reduce optimisation complexity, and a
curriculum-based strategy that progressively refines task competence over long
horizons. This formulation enables end-to-end optimisation of contact-rich
robot trajectories while maintaining stability and convergence efficiency.
Experimental results demonstrate that the proposed method achieves superior
task success rates and stability compared to the reinforcement learning
baseline.

</details>


### [10] [Touching the tumor boundary: A pilot study on ultrasound based virtual fixtures for breast-conserving surgery](https://arxiv.org/abs/2510.01452)
*Laura Connolly,Tamas Ungi,Adnan Munawar,Anton Deguet,Chris Yeung,Russell H. Taylor,Parvin Mousavi,Gabor Fichtinger Keyvan Hashtrudi-Zaad*

Main category: cs.RO

TL;DR: 开发了一种用于乳腺癌保乳手术的协作机器人引导系统，通过触觉反馈帮助定位肿瘤边界，在模拟实验中证明能改善切除边缘并降低手术难度。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌保乳手术中，由于肿瘤高度移动、不可触及且边界不规则，准确定位肿瘤边界具有挑战性。需要开发新技术来改善手术精度。

Method: 将小型触觉机器人改装为电灼刀，结合超声和电磁导航识别肿瘤边界，当手术工具碰撞肿瘤边界时施加禁止区域虚拟夹具。通过有/无触觉引导的模拟切除实验进行对比评估。

Result: 虚拟夹具引导改善了切除边缘，用户反馈使用触觉反馈时任务的心理需求、挫败感和努力程度都更低。同时发现了一些对手术流程的意外影响。

Conclusion: 虚拟夹具可在模拟保乳手术中帮助定位肿瘤边界。未来将进行更广泛的用户研究来验证结果并优化引导系统。

Abstract: Purpose: Delineating tumor boundaries during breast-conserving surgery is
challenging as tumors are often highly mobile, non-palpable, and have
irregularly shaped borders. To address these challenges, we introduce a
cooperative robotic guidance system that applies haptic feedback for tumor
localization. In this pilot study, we aim to assess if and how this system can
be successfully integrated into breast cancer care.
  Methods: A small haptic robot is retrofitted with an electrocautery blade to
operate as a cooperatively controlled surgical tool. Ultrasound and
electromagnetic navigation are used to identify the tumor boundaries and
position. A forbidden region virtual fixture is imposed when the surgical tool
collides with the tumor boundary. We conducted a study where users were asked
to resect tumors from breast simulants both with and without the haptic
guidance. We then assess the results of these simulated resections both
qualitatively and quantitatively.
  Results: Virtual fixture guidance is shown to improve resection margins. On
average, users find the task to be less mentally demanding, frustrating, and
effort intensive when haptic feedback is available. We also discovered some
unanticipated impacts on surgical workflow that will guide design adjustments
and training protocol moving forward.
  Conclusion: Our results suggest that virtual fixtures can help localize tumor
boundaries in simulated breast-conserving surgery. Future work will include an
extensive user study to further validate these results and fine-tune our
guidance system.

</details>


### [11] [VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs](https://arxiv.org/abs/2510.01483)
*Mohamad Al Mdfaa,Svetlana Lukina,Timur Akhtyamov,Arthur Nigmatzyanov,Dmitrii Nalberskii,Sergey Zagoruyko,Gonzalo Ferrer*

Main category: cs.RO

TL;DR: VL-KnG是一个视觉场景理解系统，通过构建时空知识图谱来解决视觉语言模型在机器人导航中的局限性，提供可解释的空间推理和实时计算效率。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在机器人导航中存在三个基本限制：缺乏持久场景记忆、空间推理能力有限、无法有效扩展视频时长以支持实时应用。

Method: 将视频序列分块处理，利用现代视觉语言模型构建持久知识图谱，保持对象身份随时间的一致性，并通过可查询的图结构实现可解释的空间推理。

Result: 在真实世界差分驱动机器人上的部署显示，该方法达到77.27%的成功率和76.92%的答案准确率，性能与Gemini 2.5 Pro相当，同时提供可解释的推理支持。

Conclusion: VL-KnG系统在保持计算效率的同时，实现了与最先进模型相当的性能，并为定位、导航和规划等任务提供了可解释的推理能力。

Abstract: Vision-language models (VLMs) have shown potential for robot navigation but
encounter fundamental limitations: they lack persistent scene memory, offer
limited spatial reasoning, and do not scale effectively with video duration for
real-time application. We present VL-KnG, a Visual Scene Understanding system
that tackles these challenges using spatiotemporal knowledge graph construction
and computationally efficient query processing for navigation goal
identification. Our approach processes video sequences in chunks utilizing
modern VLMs, creates persistent knowledge graphs that maintain object identity
over time, and enables explainable spatial reasoning through queryable graph
structures. We also introduce WalkieKnowledge, a new benchmark with about 200
manually annotated questions across 8 diverse trajectories spanning
approximately 100 minutes of video data, enabling fair comparison between
structured approaches and general-purpose VLMs. Real-world deployment on a
differential drive robot demonstrates practical applicability, with our method
achieving 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5
Pro performance while providing explainable reasoning supported by the
knowledge graph, computational efficiency for real-time deployment across
different tasks, such as localization, navigation and planning. Code and
dataset will be released after acceptance.

</details>


### [12] [Pose Estimation of a Thruster-Driven Bioinspired Multi-Link Robot](https://arxiv.org/abs/2510.01485)
*Nicholas B. Andrews,Yanhao Yang,Sofya Akhetova,Kristi A. Morgansen,Ross L. Hatton*

Main category: cs.RO

TL;DR: 该论文展示了如何通过无迹卡尔曼滤波结合高斯过程残差学习，对具有无驱动关节、最小感知能力的自由浮动仿生多连杆机器人进行姿态估计，并证明多步态训练数据可减少训练需求并提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究自由浮动、关节无驱动的仿生多连杆机器人的姿态估计问题，这类平台具有欠驱动和最小感知的特点，传统方法难以准确估计其姿态。

Method: 使用无迹卡尔曼滤波，并增强高斯过程残差学习来补偿非零均值、非高斯噪声。通过概念验证硬件实验和离线卡尔曼滤波分析进行验证。

Result: 实验表明机器人姿态可以可靠估计，且在多步态数据集上训练的滤波器与仅在前进步态数据集上训练的滤波器在前进步态测试轨迹上表现相当。

Conclusion: 步态输入空间存在重叠，可利用这一特性减少训练数据需求，同时提高滤波器在多个步态间的泛化能力。

Abstract: This work demonstrates pose (position and shape) estimation for a
free-floating, bioinspired multi-link robot with unactuated joints,
link-mounted thrusters for control, and a single gyroscope per link, resulting
in an underactuated, minimally sensed platform. Through a proof-of-concept
hardware experiment and offline Kalman filter analysis, we show that the
robot's pose can be reliably estimated. State estimation is performed using an
unscented Kalman filter augmented with Gaussian process residual learning to
compensate for non-zero-mean, non-Gaussian noise. We further show that a filter
trained on a multi-gait dataset (forward, backward, left, right, and turning)
performs comparably to one trained on a larger forward-gait-only dataset when
both are evaluated on the same forward-gait test trajectory. These results
reveal overlap in the gait input space, which can be exploited to reduce
training data requirements while enhancing the filter's generalizability across
multiple gaits.

</details>


### [13] [Online Hierarchical Policy Learning using Physics Priors for Robot Navigation in Unknown Environments](https://arxiv.org/abs/2510.01519)
*Wei Han Chen,Yuchen Liu,Alexiy Buynitsky,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: 提出分层规划框架解决机器人导航问题，高层用稀疏图捕捉全局连通性，低层用基于神经场的规划器解决Eikonal PDE，克服谱偏差和灾难性遗忘问题


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：传统采样方法难以控制分辨率和扩展性，模仿学习方法需要大量演示数据，ANTFields方法受谱偏差和灾难性遗忘影响，在复杂场景中效果不佳

Method: 分层规划结构：高层使用稀疏图表示环境全局连通性，低层使用基于神经场的规划器通过求解Eikonal PDE来导航局部障碍物，采用物理信息策略

Result: 在大规模环境中验证了框架的有效性，相比之前方法展现出更强的适应性和精度，能够实现平滑精确的成本景观表示

Conclusion: 该框架克服了谱偏差和神经场拟合困难等常见问题，在在线探索、建图和真实世界导航方面具有潜力

Abstract: Robot navigation in large, complex, and unknown indoor environments is a
challenging problem. The existing approaches, such as traditional
sampling-based methods, struggle with resolution control and scalability, while
imitation learning-based methods require a large amount of demonstration data.
Active Neural Time Fields (ANTFields) have recently emerged as a promising
solution by using local observations to learn cost-to-go functions without
relying on demonstrations. Despite their potential, these methods are hampered
by challenges such as spectral bias and catastrophic forgetting, which diminish
their effectiveness in complex scenarios. To address these issues, our approach
decomposes the planning problem into a hierarchical structure. At the high
level, a sparse graph captures the environment's global connectivity, while at
the low level, a planner based on neural fields navigates local obstacles by
solving the Eikonal PDE. This physics-informed strategy overcomes common
pitfalls like spectral bias and neural field fitting difficulties, resulting in
a smooth and precise representation of the cost landscape. We validate our
framework in large-scale environments, demonstrating its enhanced adaptability
and precision compared to previous methods, and highlighting its potential for
online exploration, mapping, and real-world navigation.

</details>


### [14] [Real-time Multi-Plane Segmentation Based on GPU Accelerated High-Resolution 3D Voxel Mapping for Legged Robot Locomotion](https://arxiv.org/abs/2510.01592)
*Shun Niijima,Ryoichi Tsuzaki,Noriaki Takasugi,Masaya Kinoshita*

Main category: cs.RO

TL;DR: 提出了一种基于GPU加速高分辨率3D体素映射的实时多平面分割方法，用于腿式机器人运动，能在30Hz更新率下实现快速准确的3D多平面分割。


<details>
  <summary>Details</summary>
Motivation: 现有在线平面映射方法难以平衡精度和计算效率：直接深度图像分割缺乏时间整合，高度图方法无法表示复杂3D结构，体素平面分割在实时应用中尚未探索。

Method: 开发了一个新颖框架，将基于顶点的连通组件标记与RANSAC平面检测和凸包相结合，利用GPU并行计算从高分辨率3D体素图中快速提取平面区域。

Result: 实验结果表明，即使在0.01米分辨率下，该方法也能以超过30Hz的更新率实现快速准确的3D多平面分割，使检测到的平面能够实时用于运动任务。

Conclusion: 在模拟环境和物理腿式机器人平台上的实验验证了该方法的有效性，确认了在考虑3D平面结构时的鲁棒运动性能。

Abstract: This paper proposes a real-time multi-plane segmentation method based on
GPU-accelerated high-resolution 3D voxel mapping for legged robot locomotion.
Existing online planar mapping approaches struggle to balance accuracy and
computational efficiency: direct depth image segmentation from specific sensors
suffers from poor temporal integration, height map-based methods cannot
represent complex 3D structures like overhangs, and voxel-based plane
segmentation remains unexplored for real-time applications. To address these
limitations, we develop a novel framework that integrates vertex-based
connected component labeling with random sample consensus based plane detection
and convex hull, leveraging GPU parallel computing to rapidly extract planar
regions from point clouds accumulated in high-resolution 3D voxel maps.
Experimental results demonstrate that the proposed method achieves fast and
accurate 3D multi-plane segmentation at over 30 Hz update rate even at a
resolution of 0.01 m, enabling the detected planes to be utilized in real time
for locomotion tasks. Furthermore, we validate the effectiveness of our
approach through experiments in both simulated environments and physical legged
robot platforms, confirming robust locomotion performance when considering 3D
planar structures.

</details>


### [15] [MiniBEE: A New Form Factor for Compact Bimanual Dexterity](https://arxiv.org/abs/2510.01603)
*Sharfin Islam,Zewen Chen,Zhanpeng He,Swapneel Bhatt,Andres Permuy,Brock Taylor,James Vickery,Pedro Piacenza,Cheng Zhang,Matei Ciocarlie*

Main category: cs.RO

TL;DR: 提出MiniBEE系统，将两个低自由度机械臂耦合为紧凑的双手机器人末端执行器，通过穿戴式演示训练模仿学习策略实现双手机器人操作


<details>
  <summary>Details</summary>
Motivation: 传统双手机器人系统复杂且仅利用部分工作空间进行灵巧操作，需要更紧凑高效的设计

Method: 设计耦合两个低自由度机械臂的紧凑系统，提出灵巧性度量指标，支持穿戴式数据收集和标准机械臂部署两种模式

Result: 开发出MiniBEE系统，能够通过穿戴式演示训练模仿学习策略，实现稳健的实时双手机器人操作

Conclusion: MiniBEE系统提供了一种紧凑高效的双手机器人解决方案，扩展了灵巧操作的工作空间范围

Abstract: Bimanual robot manipulators can achieve impressive dexterity, but typically
rely on two full six- or seven- degree-of-freedom arms so that paired grippers
can coordinate effectively. This traditional framework increases system
complexity while only exploiting a fraction of the overall workspace for
dexterous interaction. We introduce the MiniBEE (Miniature Bimanual
End-effector), a compact system in which two reduced-mobility arms (3+ DOF
each) are coupled into a kinematic chain that preserves full relative
positioning between grippers. To guide our design, we formulate a kinematic
dexterity metric that enlarges the dexterous workspace while keeping the
mechanism lightweight and wearable. The resulting system supports two
complementary modes: (i) wearable kinesthetic data collection with self-tracked
gripper poses, and (ii) deployment on a standard robot arm, extending dexterity
across its entire workspace. We present kinematic analysis and design
optimization methods for maximizing dexterous range, and demonstrate an
end-to-end pipeline in which wearable demonstrations train imitation learning
policies that perform robust, real-world bimanual manipulation.

</details>


### [16] [ActiveUMI: Robotic Manipulation with Active Perception from Robot-Free Human Demonstrations](https://arxiv.org/abs/2510.01607)
*Qiyuan Zeng,Chengmeng Li,Jude St. John,Zhongyi Zhou,Junjie Wen,Guorui Feng,Yichen Zhu,Yi Xu*

Main category: cs.RO

TL;DR: ActiveUMI是一个便携式数据收集系统，通过VR遥操作将人类演示转移到机器人，实现复杂的双手操作任务，并学习视觉注意力与操作的关联。


<details>
  <summary>Details</summary>
Motivation: 解决将真实世界中的人类演示有效转移到机器人进行复杂双手操作的问题，同时学习主动感知与操作之间的关键联系。

Method: 结合便携式VR遥操作套件和传感器化控制器，通过精确姿态对齐桥接人类-机器人运动学，包含沉浸式3D模型渲染、自包含可穿戴计算机和高效校准方法。

Result: 在六个挑战性双手任务上，仅使用ActiveUMI数据训练的策略在分布内任务上达到70%平均成功率，在新物体和新环境中保持56%的成功率，表现出强泛化能力。

Conclusion: 便携式数据收集系统与学习的主动感知相结合，为创建可泛化且高能力的真实世界机器人策略提供了有效且可扩展的途径。

Abstract: We present ActiveUMI, a framework for a data collection system that transfers
in-the-wild human demonstrations to robots capable of complex bimanual
manipulation. ActiveUMI couples a portable VR teleoperation kit with sensorized
controllers that mirror the robot's end-effectors, bridging human-robot
kinematics via precise pose alignment. To ensure mobility and data quality, we
introduce several key techniques, including immersive 3D model rendering, a
self-contained wearable computer, and efficient calibration methods.
ActiveUMI's defining feature is its capture of active, egocentric perception.
By recording an operator's deliberate head movements via a head-mounted
display, our system learns the crucial link between visual attention and
manipulation. We evaluate ActiveUMI on six challenging bimanual tasks. Policies
trained exclusively on ActiveUMI data achieve an average success rate of 70\%
on in-distribution tasks and demonstrate strong generalization, retaining a
56\% success rate when tested on novel objects and in new environments. Our
results demonstrate that portable data collection systems, when coupled with
learned active perception, provide an effective and scalable pathway toward
creating generalizable and highly capable real-world robot policies.

</details>


### [17] [FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models](https://arxiv.org/abs/2510.01642)
*Zijun Lin,Jiafei Duan,Haoquan Fang,Dieter Fox,Ranjay Krishna,Cheston Tan,Bihan Wen*

Main category: cs.RO

TL;DR: FailSafe是一个新颖的故障生成和恢复系统，能够自动产生多样化的故障案例并配以可执行的恢复动作，通过微调LLaVa-OneVision-7B构建FailSafe-VLM，显著提升VLA模型在机器人操作任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人操作数据集主要提供地面实况轨迹，缺乏故障恢复能力。少数处理故障检测的数据集通常只提供文本解释，难以直接用于VLA模型。

Method: 提出FailSafe系统，可无缝应用于任何模拟器中的任何操作任务，实现故障-动作数据的可扩展创建。通过微调LLaVa-OneVision-7B构建FailSafe-VLM。

Result: FailSafe-VLM成功帮助机械臂检测和恢复潜在故障，在Maniskill多个任务中将三个最先进VLA模型(pi0-FAST、OpenVLA、OpenVLA-OFT)的平均性能提升高达22.6%。

Conclusion: FailSafe-VLM能够泛化到不同的空间配置、相机视角和机器人实体，计划向社区发布FailSafe代码。

Abstract: Recent advances in robotic manipulation have integrated low-level robotic
control into Vision-Language Models (VLMs), extending them into
Vision-Language-Action (VLA) models. Although state-of-the-art VLAs achieve
strong performance in downstream robotic applications, supported by large-scale
crowd-sourced robot training data, they still inevitably encounter failures
during execution. Enabling robots to reason about and recover from
unpredictable and abrupt failures remains a critical challenge. Existing
robotic manipulation datasets, collected in either simulation or the real
world, primarily provide only ground-truth trajectories, leaving robots unable
to recover once failures occur. Moreover, the few datasets that address failure
detection typically offer only textual explanations, which are difficult to
utilize directly in VLA models. To address this gap, we introduce FailSafe, a
novel failure generation and recovery system that automatically produces
diverse failure cases paired with executable recovery actions. FailSafe can be
seamlessly applied to any manipulation task in any simulator, enabling scalable
creation of failure-action data. To demonstrate its effectiveness, we fine-tune
LLaVa-OneVision-7B (LLaVa-OV-7B) to build FailSafe-VLM. Experimental results
show that FailSafe-VLM successfully helps robotic arm detect and recover from
potential failures, improving the performance of three state-of-the-art VLA
models pi0-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across several
tasks in Maniskill. Furthermore, FailSafe-VLM could generalize across different
spatial configurations, camera viewpoints, and robotic embodiments. We plan to
release the FailSafe code to the community.

</details>


### [18] [Statistical Uncertainty Learning for Robust Visual-Inertial State Estimation](https://arxiv.org/abs/2510.01648)
*Seungwon Choi,Donggyu Park,Seo-Yeon Hwang,Tae-Wan Kim*

Main category: cs.RO

TL;DR: 提出了一种在线学习测量可靠性的统计框架，通过多视图几何一致性作为自监督，动态评估视觉惯性里程计中传感器测量的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设所有测量具有静态、均匀的不确定性，无法捕捉真实世界数据中的动态误差特性，限制了鲁棒性。

Method: 利用多视图几何一致性作为自监督，从传感器数据和优化结果中在线学习测量可靠性评估，推断地标不确定性并自适应加权视觉测量。

Result: 在EuRoC数据集上验证，相比固定不确定性参数的基线方法，平均减少24%的平移误差和42%的旋转误差。

Conclusion: 该框架能够实时运行，在精度和鲁棒性方面均有提升，源代码将公开以促进可复现性和进一步研究。

Abstract: A fundamental challenge in robust visual-inertial odometry (VIO) is to
dynamically assess the reliability of sensor measurements. This assessment is
crucial for properly weighting the contribution of each measurement to the
state estimate. Conventional methods often simplify this by assuming a static,
uniform uncertainty for all measurements. This heuristic, however, may be
limited in its ability to capture the dynamic error characteristics inherent in
real-world data. To improve this limitation, we present a statistical framework
that learns measurement reliability assessment online, directly from sensor
data and optimization results. Our approach leverages multi-view geometric
consistency as a form of self-supervision. This enables the system to infer
landmark uncertainty and adaptively weight visual measurements during
optimization. We evaluated our method on the public EuRoC dataset,
demonstrating improvements in tracking accuracy with average reductions of
approximately 24\% in translation error and 42\% in rotation error compared to
baseline methods with fixed uncertainty parameters. The resulting framework
operates in real time while showing enhanced accuracy and robustness. To
facilitate reproducibility and encourage further research, the source code will
be made publicly available.

</details>


### [19] [Symskill: Symbol and Skill Co-Invention for Data-Efficient and Real-Time Long-Horizon Manipulation](https://arxiv.org/abs/2510.01661)
*Yifei Simon Shao,Yuchen Zheng,Sunan Sun,Pratik Chaudhari,Vijay Kumar,Nadia Figueroa*

Main category: cs.RO

TL;DR: SymSkill是一个结合模仿学习和任务运动规划优势的统一学习框架，能够在实时环境中实现组合泛化和故障恢复。


<details>
  <summary>Details</summary>
Motivation: 解决多步操作在动态环境中的挑战：模仿学习缺乏组合泛化能力，而任务运动规划存在规划延迟问题，无法实时恢复故障。

Method: 从无标签、无分割的演示中联合学习谓词、操作符和技能；执行时使用符号规划器组合和重新排序技能以实现符号目标，并在运动层和符号层进行实时恢复。

Result: 在RoboCasa模拟中执行12个单步任务成功率85%，无需额外数据即可组合成最多需要6次技能重组的多步计划；在真实机器人上从5分钟无标签数据学习后能执行多个任务。

Conclusion: SymSkill框架成功结合了IL和TAMP的优势，实现了组合泛化和实时故障恢复，在模拟和真实环境中都表现出色。

Abstract: Multi-step manipulation in dynamic environments remains challenging. Two
major families of methods fail in distinct ways: (i) imitation learning (IL) is
reactive but lacks compositional generalization, as monolithic policies do not
decide which skill to reuse when scenes change; (ii) classical task-and-motion
planning (TAMP) offers compositionality but has prohibitive planning latency,
preventing real-time failure recovery. We introduce SymSkill, a unified
learning framework that combines the benefits of IL and TAMP, allowing
compositional generalization and failure recovery in real-time. Offline,
SymSkill jointly learns predicates, operators, and skills directly from
unlabeled and unsegmented demonstrations. At execution time, upon specifying a
conjunction of one or more learned predicates, SymSkill uses a symbolic planner
to compose and reorder learned skills to achieve the symbolic goals, while
performing recovery at both the motion and symbolic levels in real time.
Coupled with a compliant controller, SymSkill enables safe and uninterrupted
execution under human and environmental disturbances. In RoboCasa simulation,
SymSkill can execute 12 single-step tasks with 85% success rate. Without
additional data, it composes these skills into multi-step plans requiring up to
6 skill recompositions, recovering robustly from execution failures. On a real
Franka robot, we demonstrate SymSkill, learning from 5 minutes of unsegmented
and unlabeled play data, is capable of performing multiple tasks simply by goal
specifications. The source code and additional analysis can be found on
https://sites.google.com/view/symskill.

</details>


### [20] [Geometric Backstepping Control of Omnidirectional Tiltrotors Incorporating Servo-Rotor Dynamics for Robustness against Sudden Disturbances](https://arxiv.org/abs/2510.01675)
*Jaewoo Lee,Dongjae Lee,Jinwoo Lee,Hyungyu Lee,Yeonjoon Kim,H. Jin Kim*

Main category: cs.RO

TL;DR: 提出了一种考虑伺服和转子动力学的几何反步控制器，用于可变倾角全向多旋翼飞行器，相比不考虑执行器动力学的方法在快速跟踪和抗干扰方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有执行器感知控制方法依赖于执行器输入与力矩之间的线性关系，无法捕捉可变倾角引起的非线性效应，这限制了在激进飞行或抗干扰时的性能。

Method: 利用多旋翼刚体动力学与非线性执行器动力学之间的级联结构，设计几何反步控制器，并建立整个系统的指数稳定性。

Result: 在快速平移跟踪、快速旋转跟踪和抗突然干扰三个实验场景中，所提控制器相比基线方法始终获得更好的跟踪性能，在基线方法发散坠毁的情况下仍能保持稳定完成任务。

Conclusion: 所提出的控制器能有效处理执行器非线性动力学，对模型参数不确定性具有鲁棒性，显著提升了可变倾角多旋翼飞行器在激进飞行和抗干扰场景下的性能。

Abstract: This work presents a geometric backstepping controller for a variable-tilt
omnidirectional multirotor that explicitly accounts for both servo and rotor
dynamics. Considering actuator dynamics is essential for more effective and
reliable operation, particularly during aggressive flight maneuvers or recovery
from sudden disturbances. While prior studies have investigated actuator-aware
control for conventional and fixed-tilt multirotors, these approaches rely on
linear relationships between actuator input and wrench, which cannot capture
the nonlinearities induced by variable tilt angles. In this work, we exploit
the cascade structure between the rigid-body dynamics of the multirotor and its
nonlinear actuator dynamics to design the proposed backstepping controller and
establish exponential stability of the overall system. Furthermore, we reveal
parametric uncertainty in the actuator model through experiments, and we
demonstrate that the proposed controller remains robust against such
uncertainty. The controller was compared against a baseline that does not
account for actuator dynamics across three experimental scenarios: fast
translational tracking, rapid rotational tracking, and recovery from sudden
disturbance. The proposed method consistently achieved better tracking
performance, and notably, while the baseline diverged and crashed during the
fastest translational trajectory tracking and the recovery experiment, the
proposed controller maintained stability and successfully completed the tasks,
thereby demonstrating its effectiveness.

</details>


### [21] [PolySim: Bridging the Sim-to-Real Gap for Humanoid Control via Multi-Simulator Dynamics Randomization](https://arxiv.org/abs/2510.01708)
*Zixing Lei,Zibo Zhou,Sheng Yin,Yueru Chen,Qingyao Xu,Weixin Li,Yunhong Wang,Bowei Tang,Wei Jing,Siheng Chen*

Main category: cs.RO

TL;DR: PolySim是一个多仿真器训练平台，通过同时使用多个异构仿真器训练人形机器人全身控制策略，减少仿真器归纳偏差，实现更好的仿真到真实世界的迁移。


<details>
  <summary>Details</summary>
Motivation: 解决单仿真器训练中的仿真器归纳偏差问题，这种偏差导致仿真到真实世界和跨仿真器之间的显著差异。

Method: 开发PolySim平台，在单个训练运行中同时启动来自不同引擎的并行环境，实现动态层面的领域随机化。

Result: 在仿真到仿真评估中显著减少运动跟踪误差，在MuJoCo上比IsaacSim基线提高执行成功率52.8%；实现零样本部署到真实Unitree G1机器人。

Conclusion: 多仿真器训练能有效减少仿真器归纳偏差，实现更好的仿真到真实世界迁移，无需额外微调。

Abstract: Humanoid whole-body control (WBC) policies trained in simulation often suffer
from the sim-to-real gap, which fundamentally arises from simulator inductive
bias, the inherent assumptions and limitations of any single simulator. These
biases lead to nontrivial discrepancies both across simulators and between
simulation and the real world. To mitigate the effect of simulator inductive
bias, the key idea is to train policies jointly across multiple simulators,
encouraging the learned controller to capture dynamics that generalize beyond
any single simulator's assumptions. We thus introduce PolySim, a WBC training
platform that integrates multiple heterogeneous simulators. PolySim can launch
parallel environments from different engines simultaneously within a single
training run, thereby realizing dynamics-level domain randomization.
Theoretically, we show that PolySim yields a tighter upper bound on simulator
inductive bias than single-simulator training. In experiments, PolySim
substantially reduces motion-tracking error in sim-to-sim evaluations; for
example, on MuJoCo, it improves execution success by 52.8 over an IsaacSim
baseline. PolySim further enables zero-shot deployment on a real Unitree G1
without additional fine-tuning, showing effective transfer from simulation to
the real world. We will release the PolySim code upon acceptance of this work.

</details>


### [22] [Contrastive Representation Regularization for Vision-Language-Action Models](https://arxiv.org/abs/2510.01711)
*Taeyoung Kim,Jimin Lee,Myungkyu Koo,Dongyoung Kim,Kyungmin Lee,Changyeon Kim,Younggyo Seo,Jinwoo Shin*

Main category: cs.RO

TL;DR: 提出Robot State-aware Contrastive Loss (RS-CL)，一种简单有效的VLA模型表示正则化方法，通过机器人本体感知状态的相对距离作为软监督，提升机器人操作性能


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在机器人操作中表现良好，但其表示对机器人信号（如控制动作和本体感知状态）不够敏感，存在优化空间

Method: 引入RS-CL对比损失，将VLM表示与机器人本体感知状态对齐，使用状态间相对距离作为软监督，与原始动作预测目标互补

Result: 在RoboCasa-Kitchen的拾放任务中，将成功率从30.8%提升至41.5%；在真实机器人操作任务中，成功率从45.0%提升至58.3%

Conclusion: RS-CL能有效增强VLA模型的控制相关表示学习，显著提升机器人操作性能，且轻量级、与标准VLA训练流程完全兼容

Abstract: Vision-Language-Action (VLA) models have shown its capabilities in robot
manipulation by leveraging rich representations from pre-trained
Vision-Language Models (VLMs). However, their representations arguably remain
suboptimal, lacking sensitivity to robotic signals such as control actions and
proprioceptive states. To address the issue, we introduce Robot State-aware
Contrastive Loss (RS-CL), a simple and effective representation regularization
for VLA models, designed to bridge the gap between VLM representations and
robotic signals. In particular, RS-CL aligns the representations more closely
with the robot's proprioceptive states, by using relative distances between the
states as soft supervision. Complementing the original action prediction
objective, RS-CL effectively enhances control-relevant representation learning,
while being lightweight and fully compatible with standard VLA training
pipeline. Our empirical results demonstrate that RS-CL substantially improves
the manipulation performance of state-of-the-art VLA models; it pushes the
prior art from 30.8% to 41.5% on pick-and-place tasks in RoboCasa-Kitchen,
through more accurate positioning during grasping and placing, and boosts
success rates from 45.0% to 58.3% on challenging real-robot manipulation tasks.

</details>


### [23] [Dual-Mode Magnetic Continuum Robot for Targeted Drug Delivery](https://arxiv.org/abs/2510.01761)
*Wendu Zhang,Heng Wang,Shuangyi Wang,Yuanrui Huang*

Main category: cs.RO

TL;DR: 提出了一种新型磁控连续机器人设计，通过在导管壁内径向嵌入永磁体，实现单个外部永磁体独立控制弯曲或扭转运动，并开发了基于扭转剪切的双层阻塞机制实现按需药物释放。


<details>
  <summary>Details</summary>
Motivation: 传统轴向磁化设计的磁控连续机器人主要限于弯曲运动，为了扩展变形能力，需要开发能够独立控制弯曲和扭转的新型设计。

Method: 采用径向嵌入永磁体的简单装配结构，结合基于物理的建模和有限元分析建立驱动原理，并通过实验验证解耦模式控制。开发了由外部凹槽和内部板组成的双层阻塞机制。

Result: 实验验证了在实际磁场下的解耦模式控制，并在体模干预实验中展示了完整的端到端操作：通过弯曲进行目标接近，然后通过扭转激活在目标位置释放药物。

Conclusion: 这种紧凑、无缆的平台结合了多功能变形和精确有效载荷输送，显示出在下一代靶向治疗中的强大潜力。

Abstract: Magnetic continuum robots (MCRs) enable minimally invasive navigation through
tortuous anatomical channels, yet axially magnetized designs have largely been
limited to bending-only motion. To expand deformation capabilities, this paper
presents a simple assembly that embeds permanent magnets radially within the
catheter wall, allowing a single externally steered permanent magnet to
independently induce either bending or torsion. A physics-based formulation
together with finite-element analysis establishes the actuation principles, and
benchtop experiments validate decoupled mode control under practical fields.
Building on this, a dual-layer blockage mechanism consisting of outer grooves
and inner plates leverages torsional shear to achieve on-demand drug release.
Finally, an in-phantom intervention experiment demonstrates end-to-end
operation: lumen following by bending for target approach, followed by
twist-activated release at the site. The resulting compact, cable-free platform
combines versatile deformation with precise payload delivery, indicating strong
potential for next-generation, site-specific therapies.

</details>


### [24] [An Anytime, Scalable and Complete Algorithm for Embedding a Manufacturing Procedure in a Smart Factory](https://arxiv.org/abs/2510.01770)
*Christopher Leet,Aidan Sciortino,Sven Koenig*

Main category: cs.RO

TL;DR: 提出了TS-ACES，首个可扩展的智能工厂嵌入解决方案，能够处理包含数百台机器的真实工业场景


<details>
  <summary>Details</summary>
Motivation: 现代智能工厂可能包含数百台机器，但现有SFE求解器只能扩展到几十台机器，存在可扩展性不足的问题

Method: 开发了基于交通系统的随时循环嵌入求解器TS-ACES

Result: TS-ACES具有完备性，能够扩展到基于真实工业场景的SFE实例，处理超过一百台机器

Conclusion: TS-ACES填补了现有SFE求解器无法处理大规模智能工厂的空白

Abstract: Modern automated factories increasingly run manufacturing procedures using a
matrix of programmable machines, such as 3D printers, interconnected by a
programmable transport system, such as a fleet of tabletop robots. To embed a
manufacturing procedure into a smart factory, an operator must: (a) assign each
of its processes to a machine and (b) specify how agents should transport parts
between machines. The problem of embedding a manufacturing process into a smart
factory is termed the Smart Factory Embedding (SFE) problem. State-of-the-art
SFE solvers can only scale to factories containing a couple dozen machines.
Modern smart factories, however, may contain hundreds of machines. We fill this
hole by introducing the first highly scalable solution to the SFE, TS-ACES, the
Traffic System based Anytime Cyclic Embedding Solver. We show that TS-ACES is
complete and can scale to SFE instances based on real industrial scenarios with
more than a hundred machines.

</details>


### [25] [Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2510.01795)
*Haibo Hu,Lianming Huang,Xinyu Wang,Yufei Cui,Nan Guan,Chun Jason Xue*

Main category: cs.RO

TL;DR: Nav-EE是一个导航引导的早期退出框架，通过利用自动驾驶导航系统的先验知识预计算任务特定的退出层，在保持精度的同时显著降低视觉语言模型的推理延迟。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在自动驾驶中应用广泛，但高推理延迟阻碍实时部署。传统早期退出方法因任务依赖性难以适应多样化场景，而自动驾驶导航系统能够预知即将到来的场景上下文，这为优化推理提供了机会。

Method: 提出Nav-EE框架：离线预计算任务特定退出层，在线基于导航先验动态应用这些层。利用导航系统对即将场景（如交叉口、交通灯）的预知来确定所需任务。

Result: 在CODA、Waymo和BOSCH数据集上，Nav-EE在保持与完整推理相当精度的同时，将延迟降低高达63.9%。实车集成测试中推理延迟从600ms降至300ms。

Conclusion: 将导航预见性与早期退出相结合，为大模型在自动驾驶系统中的高效部署提供了可行路径。

Abstract: Vision-Language Models (VLMs) are increasingly applied in autonomous driving
for unified perception and reasoning, but high inference latency hinders
real-time deployment. Early-exit reduces latency by terminating inference at
intermediate layers, yet its task-dependent nature limits generalization across
diverse scenarios. We observe that this limitation aligns with autonomous
driving: navigation systems can anticipate upcoming contexts (e.g.,
intersections, traffic lights), indicating which tasks will be required. We
propose Nav-EE, a navigation-guided early-exit framework that precomputes
task-specific exit layers offline and dynamically applies them online based on
navigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE
achieves accuracy comparable to full inference while reducing latency by up to
63.9%. Real-vehicle integration with Autoware Universe further demonstrates
reduced inference latency (600ms to 300ms), supporting faster decision-making
in complex scenarios. These results suggest that coupling navigation foresight
with early-exit offers a viable path toward efficient deployment of large
models in autonomous systems. Code and data are available at our anonymous
repository: https://anonymous.4open.science/r/Nav-EE-BBC4

</details>


### [26] [What Matters in RL-Based Methods for Object-Goal Navigation? An Empirical Study and A Unified Framework](https://arxiv.org/abs/2510.01830)
*Hongze Wang,Boyang Sun,Jiaxu Xing,Fan Yang,Marco Hutter,Dhruv Shah,Davide Scaramuzza,Marc Pollefeys*

Main category: cs.RO

TL;DR: 本文对基于强化学习的物体目标导航系统进行了大规模实证研究，分解为感知、策略和测试时增强三个关键组件，发现感知质量和测试时策略是性能的主要驱动因素，而策略改进仅带来边际收益。


<details>
  <summary>Details</summary>
Motivation: 物体目标导航是移动机器人在非受控环境中部署的关键能力，但现有方法缺乏对哪些组件真正驱动性能的统一分析。

Method: 通过大规模受控实验，将模块化RL系统分解为感知、策略和测试时增强三个组件，并隔离每个组件的贡献。

Result: 提出的增强模块化系统在SPL指标上超过现有最优方法6.6%，成功率提高2.7%。人类专家在相同条件下达到98%的平均成功率。

Conclusion: 感知质量和测试时策略是物体目标导航性能的关键驱动因素，为未来ObjectNav开发和评估提供了原则性指导。

Abstract: Object-Goal Navigation (ObjectNav) is a critical component toward deploying
mobile robots in everyday, uncontrolled environments such as homes, schools,
and workplaces. In this context, a robot must locate target objects in
previously unseen environments using only its onboard perception. Success
requires the integration of semantic understanding, spatial reasoning, and
long-horizon planning, which is a combination that remains extremely
challenging. While reinforcement learning (RL) has become the dominant
paradigm, progress has spanned a wide range of design choices, yet the field
still lacks a unifying analysis to determine which components truly drive
performance. In this work, we conduct a large-scale empirical study of modular
RL-based ObjectNav systems, decomposing them into three key components:
perception, policy, and test-time enhancement. Through extensive controlled
experiments, we isolate the contribution of each and uncover clear trends:
perception quality and test-time strategies are decisive drivers of
performance, whereas policy improvements with current methods yield only
marginal gains. Building on these insights, we propose practical design
guidelines and demonstrate an enhanced modular system that surpasses
State-of-the-Art (SotA) methods by 6.6% on SPL and by a 2.7% success rate. We
also introduce a human baseline under identical conditions, where experts
achieve an average 98% success, underscoring the gap between RL agents and
human-level navigation. Our study not only sets the SotA performance but also
provides principled guidance for future ObjectNav development and evaluation.

</details>


### [27] [Like Playing a Video Game: Spatial-Temporal Optimization of Foot Trajectories for Controlled Football Kicking in Bipedal Robots](https://arxiv.org/abs/2510.01843)
*Wanyue Li,Ji Ma,Minghao Lu,Peng Lu*

Main category: cs.RO

TL;DR: 该研究将无人机中成功的时空轨迹规划方法创新性地应用于双足机器人系统，自主生成满足目标踢球位置、速度和加速度约束的足部轨迹，同时优化摆动阶段持续时间。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人足球中在激烈踢球动作时保持系统稳定性并实现精确球轨迹控制的挑战。现有方法（传统位置控制或强化学习）存在显著局限性，MPC方法在腿摆动过程中过度简化，限制了足部环境交互能力。

Method: 将无人机应用中成功的时空轨迹规划方法适配到双足机器人系统，自主生成满足约束的足部轨迹，同时优化摆动阶段持续时间。

Result: 优化后的轨迹紧密模仿人类踢球行为，具有后摆动作。轨迹规划时间低于1毫秒，在-90度到90度范围内的足球目标实现近100%的任务完成准确率。

Conclusion: 该方法在人形机器人足球应用中表现出高效性和可靠性，能够生成类人踢球轨迹并实现高精度的球控制。

Abstract: Humanoid robot soccer presents several challenges, particularly in
maintaining system stability during aggressive kicking motions while achieving
precise ball trajectory control. Current solutions, whether traditional
position-based control methods or reinforcement learning (RL) approaches,
exhibit significant limitations. Model predictive control (MPC) is a prevalent
approach for ordinary quadruped and biped robots. While MPC has demonstrated
advantages in legged robots, existing studies often oversimplify the leg swing
progress, relying merely on simple trajectory interpolation methods. This
severely constrains the foot's environmental interaction capability, hindering
tasks such as ball kicking. This study innovatively adapts the spatial-temporal
trajectory planning method, which has been successful in drone applications, to
bipedal robotic systems. The proposed approach autonomously generates foot
trajectories that satisfy constraints on target kicking position, velocity, and
acceleration while simultaneously optimizing swing phase duration. Experimental
results demonstrate that the optimized trajectories closely mimic human kicking
behavior, featuring a backswing motion. Simulation and hardware experiments
confirm the algorithm's efficiency, with trajectory planning times under 1 ms,
and its reliability, achieving nearly 100 % task completion accuracy when the
soccer goal is within the range of -90{\deg} to 90{\deg}.

</details>


### [28] [GreenhouseSplat: A Dataset of Photorealistic Greenhouse Simulations for Mobile Robotics](https://arxiv.org/abs/2510.01848)
*Diram Tabaa,Gianni Di Caro*

Main category: cs.RO

TL;DR: GreenhouseSplat：一个从低成本RGB图像生成逼真温室资产的框架和数据集，用于农业机器人仿真评估


<details>
  <summary>Details</summary>
Motivation: 现有温室环境仿真方法依赖简单或合成资产，限制了仿真到现实的迁移。辐射场方法虽然能实现逼真重建，但此前仅限于单个植物或受控实验室条件。

Method: 提出GreenhouseSplat框架，从低成本RGB图像生成逼真温室资产，集成到ROS仿真环境中，支持相机和LiDAR渲染，包含82个黄瓜植物的数据集。

Result: 创建了包含多种行配置的黄瓜植物数据集，展示了在机器人定位等任务中的实用性。

Conclusion: 这是迈向温室尺度辐射场仿真的第一步，为农业机器人研究提供了基础。

Abstract: Simulating greenhouse environments is critical for developing and evaluating
robotic systems for agriculture, yet existing approaches rely on simplistic or
synthetic assets that limit simulation-to-real transfer. Recent advances in
radiance field methods, such as Gaussian splatting, enable photorealistic
reconstruction but have so far been restricted to individual plants or
controlled laboratory conditions. In this work, we introduce GreenhouseSplat, a
framework and dataset for generating photorealistic greenhouse assets directly
from inexpensive RGB images. The resulting assets are integrated into a
ROS-based simulation with support for camera and LiDAR rendering, enabling
tasks such as localization with fiducial markers. We provide a dataset of 82
cucumber plants across multiple row configurations and demonstrate its utility
for robotics evaluation. GreenhouseSplat represents the first step toward
greenhouse-scale radiance-field simulation and offers a foundation for future
research in agricultural robotics.

</details>


### [29] [TACOS: Task Agnostic COordinator of a multi-drone System](https://arxiv.org/abs/2510.01869)
*Alessandro Nazzari,Roberto Rubinacci,Marco Lovera*

Main category: cs.RO

TL;DR: TACOS是一个基于大语言模型的多无人机系统统一框架，通过自然语言接口实现高级任务控制，集成了语言交互、智能协调和自主执行三大功能。


<details>
  <summary>Details</summary>
Motivation: 单个飞行员管理多无人机系统需要灵活的自洽度控制，从直接控制到群体协调再到完全自主的群体行为。需要支持多种共享自洽模式的框架，利用语言模型的推理和规划能力减少飞行员工作负担。

Method: TACOS框架整合了三个关键能力：一对多自然语言接口、智能协调器（将用户意图转化为结构化任务计划）、自主代理（与现实世界交互执行计划）。LLM与可执行API库交互，连接语义推理与实时多机器人协调。

Result: 在真实多无人机系统中进行了演示，并通过消融研究评估了每个模块的贡献。

Conclusion: TACOS提供了一个统一的框架，通过大语言模型实现多无人机系统的高级自然语言控制，有效连接语义推理与实时多机器人协调。

Abstract: When a single pilot is responsible for managing a multi-drone system, the
task demands varying levels of autonomy, from direct control of individual
UAVs, to group-level coordination, to fully autonomous swarm behaviors for
accomplishing high-level tasks. Enabling such flexible interaction requires a
framework that supports multiple modes of shared autonomy. As language models
continue to improve in reasoning and planning, they provide a natural
foundation for such systems, reducing pilot workload by enabling high-level
task delegation through intuitive, language-based interfaces. In this paper we
present TACOS (Task-Agnostic COordinator of a multi-drone System), a unified
framework that enables high-level natural language control of multi-UAV systems
through Large Language Models (LLMs). TACOS integrates three key capabilities
into a single architecture: a one-to-many natural language interface for
intuitive user interaction, an intelligent coordinator for translating user
intent into structured task plans, and an autonomous agent that executes plans
interacting with the real-world. TACOS allows a LLM to interact with a library
of executable APIs, bridging semantic reasoning with real-time multi-robot
coordination. We demonstrate the system in real-world multi-drone system and
conduct an ablation study to assess the contribution of each module.

</details>


### [30] [SPARC: Spine with Prismatic and Revolute Compliance for Quadruped Robot](https://arxiv.org/abs/2510.01984)
*Yue Wang*

Main category: cs.RO

TL;DR: SPARC是一个紧凑的开源3自由度脊柱模块，结合了旋转和轴向运动，具有可编程任务空间阻抗，用于四足机器人。


<details>
  <summary>Details</summary>
Motivation: 为四足机器人提供系统化的脊柱柔顺性研究平台，实现闭环刚度和阻尼控制。

Method: 集成三个扭矩控制执行器、定制1kHz控制板和受保护电源单元，采用基于RNEA的计算加速度控制器和Stribeck摩擦补偿。

Result: 实验验证了线性力-位移特性，水平刚度300-700N/m，相对误差≤1.5%；动态测试确认了质量-弹簧-阻尼器响应。

Conclusion: SPARC为腿部运动中的脊柱柔顺性研究提供了便携平台，并将发布完整的硬件和固件资源。

Abstract: We present SPARC, a compact, open-source 3-DoF sagittal-plane spine module
that combines revolute (pitch) and prismatic (axial) motion with programmable
task-space impedance for quadruped robots. The system integrates three
torque-controlled actuators, a custom 1 kHz control board, and a protected
power unit in a 1.26 kg package, enabling closed-loop stiffness and damping
shaping along x, z, and theta. We develop an RNEA-based computed-acceleration
controller with smooth Stribeck friction compensation to render spring-damper
behavior without explicit inertia shaping. Bench experiments validate the
approach. Quasi-static push-pull tests show linear force-displacement
characteristics with commanded horizontal stiffness spanning 300-700 N/m and <=
1.5% relative error (R^2 >= 0.992, narrow 95% CIs). Dynamic
displace-and-release trials confirm mass-spring-damper responses over multiple
damping settings, with small, interpretable phase deviations due to
configuration-dependent inertia and low-speed friction effects. A task-space PD
controller produces roughly linear stiffness but with greater variability and
coupling sensitivity. SPARC provides a portable platform for systematic studies
of spine compliance in legged locomotion and will be released with complete
hardware and firmware resources.

</details>


### [31] [Reducing Discomfort in Driving Simulators: Motion Cueing for Motion Sickness Mitigation](https://arxiv.org/abs/2510.01986)
*Varun Kotian,Vishrut Jain,Andrea Michelle Rios Lazcano,Daan Marinus Pool,Riender Happee,Barys Shyrokau*

Main category: cs.RO

TL;DR: 提出了一种基于模型预测控制的运动提示算法，通过惩罚感官冲突和特定力误差来联合优化驾驶模拟器的保真度和舒适度，实验显示该算法能将晕动症减少50%以上。


<details>
  <summary>Details</summary>
Motivation: 驾驶模拟器在研发中应用日益广泛，但由于运动缩放和视觉不匹配常导致晕动症，需要开发能同时优化保真度和舒适度的运动提示算法。

Method: 使用模型预测控制开发运动提示算法，在成本函数中同时惩罚感官冲突和特定力误差；进行人机环实验比较四种运动设置：两种MPC算法变体（纯特定力跟踪、折衷方案）、自适应洗出算法和无运动情况。

Result: 实验结果显示无运动条件晕动症最低但保真度评分也最低；折衷方案相比自适应洗出和纯特定力跟踪算法，晕动症减少超过50%（平均MISC水平从3降至1.5），且保真度评分无显著下降。

Conclusion: 提出的方法综合考虑模拟器动力学和晕动症时间演化，在实现晕动症控制和特定力重现的最优平衡方面取得重要进展，有助于推动模拟器的更广泛应用。

Abstract: Driving simulators are increasingly used in research and development.
However, simulators often cause motion sickness due to downscaled motion and
unscaled veridical visuals. In this paper, a motion cueing algorithm is
proposed that reduces motion sickness as predicted by the subjective vertical
conflict (SVC) model using model predictive control (MPC). Both sensory
conflict and specific force errors are penalised in the cost function, allowing
the algorithm to jointly optimise fidelity and comfort.
  Human-in-the-loop experiments were conducted to compare four simulator motion
settings: two variations of our MPC-based algorithm, one focused on pure
specific force tracking and the second compromising specific force tracking and
motion sickness minimisation, as well as reference adaptive washout and no
motion cases. The experiments were performed on a hexapod driving simulator
with participants exposed to passive driving.
  Experimental motion sickness results closely matched the sickness model
predictions. As predicted by the model, the no motion condition yielded the
lowest sickness levels. However, it was rated lowest in terms of fidelity. The
compromise solution reduced sickness by over 50% (average MISC level 3 to 1.5)
compared to adaptive washout and the algorithm focusing on specific force
tracking, without any significant reduction in fidelity rating.
  The proposed approach for developing MCA that takes into account both the
simulator dynamics and time evolution of motion sickness offers a significant
advancement in achieving an optimal control of motion sickness and specific
force recreation in driving simulators, supporting broader simulator use.

</details>


### [32] [EC3R-SLAM: Efficient and Consistent Monocular Dense SLAM with Feed-Forward 3D Reconstruction](https://arxiv.org/abs/2510.02080)
*Lingxiang Hu,Naima Ait Oufroukh,Fabien Bonardi,Raymond Ghandour*

Main category: cs.RO

TL;DR: EC3R-SLAM是一种无需相机标定的单目稠密SLAM框架，通过结合稀疏特征点跟踪和基于前馈3D重建模型的映射模块，实现了高精度定位与建图、低延迟和低GPU内存消耗。


<details>
  <summary>Details</summary>
Motivation: 传统单目稠密SLAM存在高延迟、大GPU内存消耗和依赖相机标定的问题，限制了实际应用。

Method: 结合跟踪模块（维护稀疏特征点地图）和映射模块（基于前馈3D重建模型同时估计相机内参），并融入局部和全局闭环检测以确保数据关联和多视角一致性。

Result: 在多个基准测试中达到与最先进方法相竞争的性能，同时更快、更节省内存，可在笔记本电脑和Jetson Orin NX等资源受限平台上有效运行。

Conclusion: EC3R-SLAM框架在保持高精度的同时显著提升了效率和实用性，具有在真实世界机器人应用中推广的潜力。

Abstract: The application of monocular dense Simultaneous Localization and Mapping
(SLAM) is often hindered by high latency, large GPU memory consumption, and
reliance on camera calibration. To relax this constraint, we propose EC3R-SLAM,
a novel calibration-free monocular dense SLAM framework that jointly achieves
high localization and mapping accuracy, low latency, and low GPU memory
consumption. This enables the framework to achieve efficiency through the
coupling of a tracking module, which maintains a sparse map of feature points,
and a mapping module based on a feed-forward 3D reconstruction model that
simultaneously estimates camera intrinsics. In addition, both local and global
loop closures are incorporated to ensure mid-term and long-term data
association, enforcing multi-view consistency and thereby enhancing the overall
accuracy and robustness of the system. Experiments across multiple benchmarks
show that EC3R-SLAM achieves competitive performance compared to
state-of-the-art methods, while being faster and more memory-efficient.
Moreover, it runs effectively even on resource-constrained platforms such as
laptops and Jetson Orin NX, highlighting its potential for real-world robotics
applications.

</details>


### [33] [LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot Grasping with Ambiguous Instructions](https://arxiv.org/abs/2510.02104)
*Yunhan Lin,Wenqi Wu,Zhijie Zhang,Huasong Min*

Main category: cs.RO

TL;DR: LangGrasp是一个语言交互的机器人抓取框架，通过微调的大语言模型解决模糊指令中的隐含意图问题，并实现从对象级到部件级的精细抓取操作。


<details>
  <summary>Details</summary>
Motivation: 现有的语言驱动抓取方法难以处理包含隐含意图的模糊指令，限制了机器人在非结构化环境中的适应性和任务执行效率。

Method: 集成微调的大语言模型进行常识理解和环境感知，结合2D部件分割引导的点云定位模块，实现从对象级到部件级的精细抓取。

Result: 实验表明LangGrasp能准确解析模糊指令中的隐含意图，识别关键操作和目标信息，并动态选择最优抓取姿态，显著提升机器人适应性。

Conclusion: 该框架通过语言交互和精细抓取能力，显著增强了机器人在非结构化环境中的任务执行效率和适应性。

Abstract: The existing language-driven grasping methods struggle to fully handle
ambiguous instructions containing implicit intents. To tackle this challenge,
we propose LangGrasp, a novel language-interactive robotic grasping framework.
The framework integrates fine-tuned large language models (LLMs) to leverage
their robust commonsense understanding and environmental perception
capabilities, thereby deducing implicit intents from linguistic instructions
and clarifying task requirements along with target manipulation objects.
Furthermore, our designed point cloud localization module, guided by 2D part
segmentation, enables partial point cloud localization in scenes, thereby
extending grasping operations from coarse-grained object-level to fine-grained
part-level manipulation. Experimental results show that the LangGrasp framework
accurately resolves implicit intents in ambiguous instructions, identifying
critical operations and target information that are unstated yet essential for
task completion. Additionally, it dynamically selects optimal grasping poses by
integrating environmental information. This enables high-precision grasping
from object-level to part-level manipulation, significantly enhancing the
adaptability and task execution efficiency of robots in unstructured
environments. More information and code are available here:
https://github.com/wu467/LangGrasp.

</details>


### [34] [Stand Up, NAO! Increasing the Reliability of Stand-Up Motions Through Error Compensation in Position Control](https://arxiv.org/abs/2510.02129)
*Philip Reichenberg,Tim Laue*

Main category: cs.RO

TL;DR: 本文介绍了NAO机器人从2019年开始开发并持续优化的站立动作系统，通过解决关节位置误差问题显著提高了站立成功率，该方法已被多个机器人足球团队采用。


<details>
  <summary>Details</summary>
Motivation: 站立动作是人形机器人足球比赛中不可或缺的部分，无法自主站立的机器人会被罚下场一段时间，因此需要开发可靠的站立动作系统。

Method: 通过执行特殊动作来释放卡住的肢体（如手臂），或通过其他关节补偿大误差，来解决关节位置执行误差问题。

Result: 显著提高了站立动作的整体成功率，该方法已被标准平台联盟中的多个团队采用，并在多个锦标赛的视频分析中显示出相似的性能。

Conclusion: 通过解决关节位置误差问题，开发出了可靠的站立动作系统，该方法经过6年评估和扩展，在实践中证明有效并被广泛采用。

Abstract: Stand-up motions are an indispensable part of humanoid robot soccer. A robot
incapable of standing up by itself is removed from the game for some time. In
this paper, we present our stand-up motions for the NAO robot. Our approach
dates back to 2019 and has been evaluated and slightly expanded over the past
six years. We claim that the main reason for failed stand-up attempts are large
errors in the executed joint positions. By addressing such problems by either
executing special motions to free up stuck limbs such as the arms, or by
compensating large errors with other joints, we significantly increased the
overall success rate of our stand-up routine. The motions presented in this
paper are also used by several other teams in the Standard Platform League,
which thereby achieve similar success rates, as shown in an analysis of videos
from multiple tournaments.

</details>


### [35] [SCANS: A Soft Gripper with Curvature and Spectroscopy Sensors for In-Hand Material Differentiation](https://arxiv.org/abs/2510.02164)
*Nathaniel Hanson,Austin Allison,Charles DiMarzio,Taşkın Padır,Kristen L. Dorsey*

Main category: cs.RO

TL;DR: SCANS系统是一种无电子元件的流体驱动软体机械手，能够通过预接触包覆或手持方式评估物体的光谱特性，具有比以往软体机器人更宽的光谱感知能力。


<details>
  <summary>Details</summary>
Motivation: 开发一种多功能的光学感知软体机器人平台，通过光谱分析来识别不同材料类型的物体，特别是在近红外波长下区分视觉相似物体。

Method: 使用流体驱动的软体机械手结构，进行材料分析以优化光谱传感的软基板，评估预接触和手持性能，并通过线性判别分析验证光谱分离效果。

Result: 实验证明该系统能够对不同类别和尺寸的物体（金属、木材、塑料、有机物、纸张、泡沫）进行可解释的统计分离，物体间存在大的光谱角度差异。

Conclusion: 近红外波长的敏感性对于区分视觉相似物体至关重要，这些能力推进了光学作为软体机器人多功能感知模式的潜力。

Abstract: We introduce the soft curvature and spectroscopy (SCANS) system: a versatile,
electronics-free, fluidically actuated soft manipulator capable of assessing
the spectral properties of objects either in hand or through pre-touch caging.
This platform offers a wider spectral sensing capability than previous soft
robotic counterparts. We perform a material analysis to explore optimal soft
substrates for spectral sensing, and evaluate both pre-touch and in-hand
performance. Experiments demonstrate explainable, statistical separation across
diverse object classes and sizes (metal, wood, plastic, organic, paper, foam),
with large spectral angle differences between items. Through linear
discriminant analysis, we show that sensitivity in the near-infrared
wavelengths is critical to distinguishing visually similar objects. These
capabilities advance the potential of optics as a multi-functional sensory
modality for soft robots. The complete parts list, assembly guidelines, and
processing code for the SCANS gripper are accessible at:
https://parses-lab.github.io/scans/.

</details>


### [36] [Product Digital Twin Supporting End-of-life Phase of Electric Vehicle Batteries Utilizing Product-Process-Resource Asset Network](https://arxiv.org/abs/2510.02167)
*Sara Strakosova,Petr Novak,Petr Kadera*

Main category: cs.RO

TL;DR: 本文提出使用数字孪生技术优化电动汽车电池的拆解过程，通过扩展产品-过程-资源资产网络（PAN）为双向PAN（Bi-PAN），覆盖制造和再制造/回收两个阶段，以提高可持续性和减少生态影响。


<details>
  <summary>Details</summary>
Motivation: 在循环经济背景下，制造商往往不共享相关数据，导致产品生命周期末端的再制造和回收过程得不到足够支持，影响可持续性和环境保护。

Method: 采用数字孪生技术，基于扩展的产品-过程-资源资产网络（Bi-PAN）表示法，该网络能够建模产品、生产资源、制造过程以及特定生产操作之间的关系，并覆盖制造和再制造/回收两个阶段。

Result: 通过电动汽车电池拆解用例证明，产品数字孪生能够灵活高效地解决不同类型电池的拆解挑战。

Conclusion: 数字孪生技术和Bi-PAN表示法为优化产品拆解过程提供了有效解决方案，有助于实现循环经济和可持续发展目标。

Abstract: In the context of the circular economy, products in their end-of-life phase
should be either remanufactured or recycled. Both of these processes are
crucial for sustainability and environmental conservation. However,
manufacturers often do not support these processes enough by not sharing
relevant data. This paper proposes use of a digital twin technology, which is
capable to help optimizing the disassembly processes to reduce ecological
impact and enhance sustainability. The proposed approach is demonstrated
through a disassembly use-case of the product digital twin of an electric
vehicle battery. By utilizing product digital twins, challenges associated with
the disassembly of electric vehicle batteries can be solved flexibly and
efficiently for various battery types. As a backbone for the product digital
twin representation, the paper uses the paradigm of product-process-resource
asset networks (PAN). Such networks enable to model relevant relationships
across products, production resources, manufacturing processes, and specific
production operations that have to be done in the manufacturing phase of a
product. This paper introduces a Bi-Flow Product-Process-Resource Asset Network
(Bi-PAN) representation, which extends the PAN paradigm to cover not only the
manufacturing, but also the remanufacturing/recycling phase.

</details>


### [37] [DisCo-Layout: Disentangling and Coordinating Semantic and Physical Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis](https://arxiv.org/abs/2510.02178)
*Jialin Gao,Donghao Zhou,Mingjian Liang,Lihao Liu,Chi-Wing Fu,Xiaowei Hu,Pheng-Ann Heng*

Main category: cs.RO

TL;DR: DisCo-Layout是一个用于3D室内布局合成的框架，通过解耦物理和语义细化来解决传统方法泛化性差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法因固定数据集而泛化性差，而基于LLM和VLM的方法虽然语义丰富但缺乏鲁棒和灵活的细化能力，导致布局不理想。

Method: 开发了语义细化工具(SRT)修正抽象对象关系，物理细化工具(PRT)通过网格匹配算法解决具体空间问题，并采用多智能体框架协调这些工具。

Result: 实验证明DisCo-Layout达到了最先进的性能，能生成真实、连贯且可泛化的3D室内布局。

Conclusion: DisCo-Layout通过解耦和协调物理与语义细化，有效解决了3D室内布局合成中的泛化和细化问题。

Abstract: 3D indoor layout synthesis is crucial for creating virtual environments.
Traditional methods struggle with generalization due to fixed datasets. While
recent LLM and VLM-based approaches offer improved semantic richness, they
often lack robust and flexible refinement, resulting in suboptimal layouts. We
develop DisCo-Layout, a novel framework that disentangles and coordinates
physical and semantic refinement. For independent refinement, our Semantic
Refinement Tool (SRT) corrects abstract object relationships, while the
Physical Refinement Tool (PRT) resolves concrete spatial issues via a
grid-matching algorithm. For collaborative refinement, a multi-agent framework
intelligently orchestrates these tools, featuring a planner for placement
rules, a designer for initial layouts, and an evaluator for assessment.
Experiments demonstrate DisCo-Layout's state-of-the-art performance, generating
realistic, coherent, and generalizable 3D indoor layouts. Our code will be
publicly available.

</details>


### [38] [Performance-Guided Refinement for Visual Aerial Navigation using Editable Gaussian Splatting in FalconGym 2.0](https://arxiv.org/abs/2510.02248)
*Yan Miao,Ege Yuceel,Georgios Fainekos,Bardh Hoxha,Hideki Okamoto,Sayan Mitra*

Main category: cs.RO

TL;DR: 开发了FalconGym 2.0仿真框架和性能引导精炼算法，训练出的视觉策略在无人机导航中表现出优异的泛化能力和鲁棒性，并能实现零样本的仿真到真实环境迁移。


<details>
  <summary>Details</summary>
Motivation: 现有视觉策略在无人机导航中容易过拟合到单一赛道，当赛道几何形状变化时性能会下降，需要提高视觉策略的泛化能力和鲁棒性。

Method: 基于高斯泼溅技术构建FalconGym 2.0仿真框架，提供编辑API快速生成多样化赛道；提出性能引导精炼算法，在训练过程中专注于挑战性赛道并迭代提升性能。

Result: 在固定翼无人机和四旋翼无人机两个案例中，使用PGR训练的单一视觉策略在泛化性和鲁棒性上均优于现有方法：在三个未见赛道上达到100%成功率，无需逐赛道重新训练；在门位姿扰动下保持更高成功率；在四旋翼硬件上实现零样本迁移，达到98.6%成功率。

Conclusion: FalconGym 2.0框架结合PGR算法能够有效训练出具有强大泛化能力和鲁棒性的视觉策略，并成功实现仿真到真实环境的零样本迁移。

Abstract: Visual policy design is crucial for aerial navigation. However,
state-of-the-art visual policies often overfit to a single track and their
performance degrades when track geometry changes. We develop FalconGym 2.0, a
photorealistic simulation framework built on Gaussian Splatting (GSplat) with
an Edit API that programmatically generates diverse static and dynamic tracks
in milliseconds. Leveraging FalconGym 2.0's editability, we propose a
Performance-Guided Refinement (PGR) algorithm, which concentrates visual
policy's training on challenging tracks while iteratively improving its
performance. Across two case studies (fixed-wing UAVs and quadrotors) with
distinct dynamics and environments, we show that a single visual policy trained
with PGR in FalconGym 2.0 outperforms state-of-the-art baselines in
generalization and robustness: it generalizes to three unseen tracks with 100%
success without per-track retraining and maintains higher success rates under
gate-pose perturbations. Finally, we demonstrate that the visual policy trained
with PGR in FalconGym 2.0 can be zero-shot sim-to-real transferred to a
quadrotor hardware, achieving a 98.6% success rate (69 / 70 gates) over 30
trials spanning two three-gate tracks and a moving-gate track.

</details>


### [39] [Retargeting Matters: General Motion Retargeting for Humanoid Motion Tracking](https://arxiv.org/abs/2510.02252)
*Joao Pedro Araujo,Yanjie Ze,Pei Xu,Jiajun Wu,C. Karen Liu*

Main category: cs.RO

TL;DR: 本文提出了一种新的运动重定向方法GMR，解决了人形机器人运动跟踪中由于重定向伪影导致的问题，在减少奖励工程的情况下显著提升了策略的鲁棒性和跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人运动跟踪策略面临人体与机器人之间的具身差距问题，现有重定向方法会在参考轨迹中留下伪影（如脚滑、自穿透等），需要强化学习策略来纠正，这通常需要大量的奖励工程和领域随机化。

Method: 提出了通用运动重定向(GMR)方法，并与PHC、ProtoMotions等开源重定向器以及Unitree的闭源数据集进行比较，使用BeyondMimic进行策略训练以隔离重定向效果。

Result: 实验表明，虽然大多数运动可以被跟踪，但重定向数据中的伪影显著降低了策略鲁棒性，特别是在动态或长序列中。GMR在跟踪性能和运动保真度方面始终优于现有开源方法，接近闭源基线的性能。

Conclusion: 重定向质量对策略性能有重要影响，GMR方法能够有效减少重定向伪影，提高运动跟踪的鲁棒性和保真度。

Abstract: Humanoid motion tracking policies are central to building teleoperation
pipelines and hierarchical controllers, yet they face a fundamental challenge:
the embodiment gap between humans and humanoid robots. Current approaches
address this gap by retargeting human motion data to humanoid embodiments and
then training reinforcement learning (RL) policies to imitate these reference
trajectories. However, artifacts introduced during retargeting, such as foot
sliding, self-penetration, and physically infeasible motion are often left in
the reference trajectories for the RL policy to correct. While prior work has
demonstrated motion tracking abilities, they often require extensive reward
engineering and domain randomization to succeed. In this paper, we
systematically evaluate how retargeting quality affects policy performance when
excessive reward tuning is suppressed. To address issues that we identify with
existing retargeting methods, we propose a new retargeting method, General
Motion Retargeting (GMR). We evaluate GMR alongside two open-source
retargeters, PHC and ProtoMotions, as well as with a high-quality closed-source
dataset from Unitree. Using BeyondMimic for policy training, we isolate
retargeting effects without reward tuning. Our experiments on a diverse subset
of the LAFAN1 dataset reveal that while most motions can be tracked, artifacts
in retargeted data significantly reduce policy robustness, particularly for
dynamic or long sequences. GMR consistently outperforms existing open-source
methods in both tracking performance and faithfulness to the source motion,
achieving perceptual fidelity and policy success rates close to the
closed-source baseline. Website:
https://jaraujo98.github.io/retargeting_matters. Code:
https://github.com/YanjieZe/GMR.

</details>


### [40] [Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning](https://arxiv.org/abs/2510.02268)
*Tianchong Jiang,Jingtian Ji,Xiangshan Tan,Jiading Fang,Anand Bhattad,Vitor Guizilini,Matthew R. Walter*

Main category: cs.RO

TL;DR: 该论文研究通过显式地将策略与相机外参条件化来实现视角不变模仿学习，使用Plucker嵌入表示像素光线，在RoboSuite和ManiSkill中引入六个操作任务来评估策略在视角变化下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习策略在固定场景中可能通过静态背景推断相机姿态，但当工作空间几何或相机位置变化时这种捷径会失效，需要开发对视角变化鲁棒的控制策略。

Method: 使用Plucker嵌入表示像素光线，将相机外参显式地条件化到策略中，在ACT、Diffusion Policy和SmolVLA等行为克隆策略上进行测试，并设计了固定和随机化场景变体来解耦背景线索和相机姿态。

Result: 条件化相机外参显著提高了策略在不同视角下的泛化能力，恢复了性能并实现了无需深度的鲁棒RGB控制，而不使用外参的策略在视角变化时性能下降。

Conclusion: 显式条件化相机外参是实现视角不变模仿学习的有效方法，能够提高策略在真实世界视角变化下的鲁棒性。

Abstract: We study view-invariant imitation learning by explicitly conditioning
policies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we
show that conditioning on extrinsics significantly improves generalization
across viewpoints for standard behavior cloning policies, including ACT,
Diffusion Policy, and SmolVLA. To evaluate policy robustness under realistic
viewpoint shifts, we introduce six manipulation tasks in RoboSuite and
ManiSkill that pair "fixed" and "randomized" scene variants, decoupling
background cues from camera pose. Our analysis reveals that policies without
extrinsics often infer camera pose using visual cues from static backgrounds in
fixed scenes; this shortcut collapses when workspace geometry or camera
placement shifts. Conditioning on extrinsics restores performance and yields
robust RGB-only control without depth. We release the tasks, demonstrations,
and code at https://ripl.github.io/know_your_camera/ .

</details>


### [41] [ARMADA: Autonomous Online Failure Detection and Human Shared Control Empower Scalable Real-world Deployment and Adaptation](https://arxiv.org/abs/2510.02298)
*Wenye Yu,Jun Lv,Zixi Ying,Yang Jin,Chuan Wen,Cewu Lu*

Main category: cs.RO

TL;DR: ARMADA是一个多机器人部署和适应系统，通过FLOAT自主故障检测方法减少对人类监督的依赖，实现并行策略执行和高效领域数据采集。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在大规模真实数据集上表现良好，但预训练策略在缺乏领域数据时性能较差，且人工收集演示数据成本高、质量不均。现有方法需要人类全程监控策略执行。

Method: 开发ARMADA系统，包含FLOAT自主在线故障检测方法，实现多机器人并行策略执行，仅在必要时请求人工干预。

Result: FLOAT故障检测准确率平均达到95%，比现有方法提高20%以上；ARMADA在多次策略执行和后训练中，成功率提高4倍以上，人工干预率减少2倍以上。

Conclusion: ARMADA系统通过减少对人类监督的依赖，实现了更可扩展的部署和更快的新场景适应，显著提升了模仿学习的实际应用效果。

Abstract: Imitation learning has shown promise in learning from large-scale real-world
datasets. However, pretrained policies usually perform poorly without
sufficient in-domain data. Besides, human-collected demonstrations entail
substantial labour and tend to encompass mixed-quality data and redundant
information. As a workaround, human-in-the-loop systems gather domain-specific
data for policy post-training, and exploit closed-loop policy feedback to offer
informative guidance, but usually require full-time human surveillance during
policy rollout. In this work, we devise ARMADA, a multi-robot deployment and
adaptation system with human-in-the-loop shared control, featuring an
autonomous online failure detection method named FLOAT. Thanks to FLOAT, ARMADA
enables paralleled policy rollout and requests human intervention only when
necessary, significantly reducing reliance on human supervision. Hence, ARMADA
enables efficient acquisition of in-domain data, and leads to more scalable
deployment and faster adaptation to new scenarios. We evaluate the performance
of ARMADA on four real-world tasks. FLOAT achieves nearly 95% accuracy on
average, surpassing prior state-of-the-art failure detection approaches by over
20%. Besides, ARMADA manifests more than 4$\times$ increase in success rate and
greater than 2$\times$ reduction in human intervention rate over multiple
rounds of policy rollout and post-training, compared to previous
human-in-the-loop learning methods.

</details>
