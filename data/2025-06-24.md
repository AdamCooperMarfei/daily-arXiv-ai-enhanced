<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 51]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Reflective VLM Planning for Dual-Arm Desktop Cleaning: Bridging Open-Vocabulary Perception and Precise Manipulation](https://arxiv.org/abs/2506.17328)
*Yufan Liu,Yi Wu,Gweneth Ge,Haoliang Cheng,Rui Liu*

Main category: cs.RO

TL;DR: 提出了一种结合视觉语言模型（VLM）规划和双臂执行的层次化框架，用于桌面清理任务，显著提高了任务完成率。


<details>
  <summary>Details</summary>
Motivation: 解决桌面清理中开放词汇识别和精确操作异质碎片的需求。

Method: 使用Grounded-SAM2进行开放词汇检测，并通过记忆增强的VLM生成、评估和修订操作序列，转换为参数化轨迹由双Franka臂执行。

Result: 在模拟场景中，系统任务完成率达到87.2%，比静态VLM提高28.8%，比单臂基线提高36.2%。

Conclusion: 结构化记忆集成对鲁棒、可泛化的操作至关重要，同时保持实时控制性能。

Abstract: Desktop cleaning demands open-vocabulary recognition and precise manipulation
for heterogeneous debris. We propose a hierarchical framework integrating
reflective Vision-Language Model (VLM) planning with dual-arm execution via
structured scene representation. Grounded-SAM2 facilitates open-vocabulary
detection, while a memory-augmented VLM generates, critiques, and revises
manipulation sequences. These sequences are converted into parametric
trajectories for five primitives executed by coordinated Franka arms. Evaluated
in simulated scenarios, our system achieving 87.2% task completion, a 28.8%
improvement over static VLM and 36.2% over single-arm baselines. Structured
memory integration proves crucial for robust, generalizable manipulation while
maintaining real-time control performance.

</details>


### [2] [A workflow for generating synthetic LiDAR datasets in simulation environments](https://arxiv.org/abs/2506.17378)
*Abhishek Phadke,Shakib Mahmud Dipto,Pratip Rana*

Main category: cs.RO

TL;DR: 提出了一种生成合成LiDAR数据的仿真工作流，用于支持自动驾驶感知、机器人研究和传感器安全分析。


<details>
  <summary>Details</summary>
Motivation: 为自动驾驶系统和机器人研究提供高质量、多模态的合成数据，同时分析LiDAR数据的安全漏洞。

Method: 利用CoppeliaSim仿真环境和Python API，集成LiDAR、图像传感器和二维扫描仪，自动化数据捕获、存储和标注。

Result: 生成了大规模点云及同步的RGB和深度图像，验证了工作流的有效性，并展示了合成数据在安全漏洞评估中的应用。

Conclusion: 该工作流为生成高保真合成LiDAR数据提供了灵活、可复现的框架，未来可扩展至天气效果和真实地形模型。

Abstract: This paper presents a simulation workflow for generating synthetic LiDAR
datasets to support autonomous vehicle perception, robotics research, and
sensor security analysis. Leveraging the CoppeliaSim simulation environment and
its Python API, we integrate time-of-flight LiDAR, image sensors, and two
dimensional scanners onto a simulated vehicle platform operating within an
urban scenario. The workflow automates data capture, storage, and annotation
across multiple formats (PCD, PLY, CSV), producing synchronized multimodal
datasets with ground truth pose information. We validate the pipeline by
generating large-scale point clouds and corresponding RGB and depth imagery.
The study examines potential security vulnerabilities in LiDAR data, such as
adversarial point injection and spoofing attacks, and demonstrates how
synthetic datasets can facilitate the evaluation of defense strategies.
Finally, limitations related to environmental realism, sensor noise modeling,
and computational scalability are discussed, and future research directions,
such as incorporating weather effects, real-world terrain models, and advanced
scanner configurations, are proposed. The workflow provides a versatile,
reproducible framework for generating high-fidelity synthetic LiDAR datasets to
advance perception research and strengthen sensor security in autonomous
systems. Documentation and examples accompany this framework; samples of
animated cloud returns and image sensor data can be found at this Link.

</details>


### [3] [Kinematic Model Optimization via Differentiable Contact Manifold for In-Space Manipulation](https://arxiv.org/abs/2506.17458)
*Abhay Negi,Omey M. Manyar,Satyandra K. Gupta*

Main category: cs.RO

TL;DR: 提出了一种仅需编码器测量和接触检测的机器人运动学参数估计新方法，适用于太空环境中的精确操作。


<details>
  <summary>Details</summary>
Motivation: 太空机器人操作（如碎片清除和制造）需要高精度接触操作，但热变形和编码器偏差导致运动学参数误差，传统校准方法在太空环境中不可行。

Method: 利用接触流形信息，提出基于学习的可微分接触流形模型和优化算法，通过编码器测量和接触实例估计参数。

Result: 该方法仅需编码器数据和接触检测即可估计参数，为太空环境中的安全、精确操作提供了高效解决方案。

Conclusion: 新方法在太空环境中具有鲁棒性、可解释性和数据效率，适用于动态和不确定的操作场景。

Abstract: Robotic manipulation in space is essential for emerging applications such as
debris removal and in-space servicing, assembly, and manufacturing (ISAM). A
key requirement for these tasks is the ability to perform precise, contact-rich
manipulation under significant uncertainty. In particular, thermal-induced
deformation of manipulator links and temperature-dependent encoder bias
introduce kinematic parameter errors that significantly degrade end-effector
accuracy. Traditional calibration techniques rely on external sensors or
dedicated calibration procedures, which can be infeasible or risky in dynamic,
space-based operational scenarios.
  This paper proposes a novel method for kinematic parameter estimation that
only requires encoder measurements and binary contact detection. The approach
focuses on estimating link thermal deformation strain and joint encoder biases
by leveraging information of the contact manifold - the set of relative SE(3)
poses at which contact between the manipulator and environment occurs. We
present two core contributions: (1) a differentiable, learning-based model of
the contact manifold, and (2) an optimization-based algorithm for estimating
kinematic parameters from encoder measurements at contact instances. By
enabling parameter estimation using only encoder measurements and contact
detection, this method provides a robust, interpretable, and data-efficient
solution for safe and accurate manipulation in the challenging conditions of
space.

</details>


### [4] [General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting](https://arxiv.org/abs/2506.17462)
*Bernard Lange,Anil Yildiz,Mansur Arief,Shehryar Khattak,Mykel Kochenderfer,Georgios Georgakis*

Main category: cs.RO

TL;DR: ARNA是一个基于LVLM的通用导航框架，通过自主定义任务工作流，实现未知环境中的鲁棒导航和推理。


<details>
  <summary>Details</summary>
Motivation: 现有导航系统依赖任务特定神经网络和固定数据流，泛化能力有限。LVLM提供人类知识嵌入，但现有集成依赖预映射空间和硬编码表示。

Method: ARNA结合感知、推理和导航工具库，运行时自主定义任务工作流，迭代查询机器人模块并选择导航动作。

Result: 在HM-EQA基准测试中，ARNA实现最先进性能，无需手工计划或预建地图。

Conclusion: ARNA为机器人栈设计提供新视角，展示在未知环境中的高效探索和导航能力。

Abstract: Developing general-purpose navigation policies for unknown environments
remains a core challenge in robotics. Most existing systems rely on
task-specific neural networks and fixed data flows, limiting generalizability.
Large Vision-Language Models (LVLMs) offer a promising alternative by embedding
human-like knowledge suitable for reasoning and planning. Yet, prior LVLM-robot
integrations typically depend on pre-mapped spaces, hard-coded representations,
and myopic exploration. We introduce the Agentic Robotic Navigation
Architecture (ARNA), a general-purpose navigation framework that equips an
LVLM-based agent with a library of perception, reasoning, and navigation tools
available within modern robotic stacks. At runtime, the agent autonomously
defines and executes task-specific workflows that iteratively query the robotic
modules, reason over multimodal inputs, and select appropriate navigation
actions. This approach enables robust navigation and reasoning in previously
unmapped environments, providing a new perspective on robotic stack design.
Evaluated in Habitat Lab on the HM-EQA benchmark, ARNA achieves
state-of-the-art performance, demonstrating effective exploration, navigation,
and embodied question answering without relying on handcrafted plans, fixed
input representations, or pre-existing maps.

</details>


### [5] [DiLQR: Differentiable Iterative Linear Quadratic Regulator via Implicit Differentiation](https://arxiv.org/abs/2506.17473)
*Shuyuan Wang,Philip D. Loewen,Michael Forbes,Bhushan Gopaluni,Wei Pan*

Main category: cs.RO

TL;DR: 论文提出DiLQR框架，使iLQR成为可微分的控制器模块，通过隐式微分提供梯度解析解，显著提升计算和学习性能。


<details>
  <summary>Details</summary>
Motivation: iLQR作为可微分组件的潜力未被充分挖掘，扩展性和迭代梯度计算是主要挑战。

Method: 引入DiLQR框架，通过隐式微分提供iLQR控制器的梯度解析解，确保恒定反向计算成本。

Result: 在模仿任务中，DiLQR计算速度提升21x-128x，学习性能优于传统神经网络策略和缺乏精确梯度的控制器。

Conclusion: DiLQR成功将iLQR转化为高效可微分模块，适用于高维端到端任务。

Abstract: While differentiable control has emerged as a powerful paradigm combining
model-free flexibility with model-based efficiency, the iterative Linear
Quadratic Regulator (iLQR) remains underexplored as a differentiable component.
The scalability of differentiating through extended iterations and horizons
poses significant challenges, hindering iLQR from being an effective
differentiable controller. This paper introduces DiLQR, a framework that
facilitates differentiation through iLQR, allowing it to serve as a trainable
and differentiable module, either as or within a neural network. A novel aspect
of this framework is the analytical solution that it provides for the gradient
of an iLQR controller through implicit differentiation, which ensures a
constant backward cost regardless of iteration, while producing an accurate
gradient. We evaluate our framework on imitation tasks on famous control
benchmarks. Our analytical method demonstrates superior computational
performance, achieving up to 128x speedup and a minimum of 21x speedup compared
to automatic differentiation. Our method also demonstrates superior learning
performance ($10^6$x) compared to traditional neural network policies and
better model loss with differentiable controllers that lack exact analytical
gradients. Furthermore, we integrate our module into a larger network with
visual inputs to demonstrate the capacity of our method for high-dimensional,
fully end-to-end tasks. Codes can be found on the project homepage
https://sites.google.com/view/dilqr/.

</details>


### [6] [Distilling On-device Language Models for Robot Planning with Minimal Human Intervention](https://arxiv.org/abs/2506.17486)
*Zachary Ravichandran,Ignacio Hounie,Fernando Cladera,Alejandro Ribeiro,George J. Pappas,Vijay Kumar*

Main category: cs.RO

TL;DR: PRISM框架通过自动合成任务和环境，从大型语言模型（LLM）中提取计划，并蒸馏出小型语言模型（SLM），以在设备上运行，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前依赖云端LLM的机器人在通信不可靠的环境中（如户外或工业环境）实用性受限，需要一种能在本地运行的高效解决方案。

Method: PRISM从现有LLM规划器中自动合成任务和环境，提取计划，并利用合成数据蒸馏出紧凑的SLM作为替代。

Result: PRISM将Llama-3.2-3B的性能从GPT-4o的10-20%提升至93%以上，且蒸馏后的规划器能在不同机器人平台和环境中泛化。

Conclusion: PRISM提供了一种高效、本地化的机器人规划解决方案，显著提升了性能并具备广泛适用性。

Abstract: Large language models (LLMs) provide robots with powerful contextual
reasoning abilities and a natural human interface. Yet, current LLM-enabled
robots typically depend on cloud-hosted models, limiting their usability in
environments with unreliable communication infrastructure, such as outdoor or
industrial settings. We present PRISM, a framework for distilling small
language model (SLM)-enabled robot planners that run on-device with minimal
human supervision. Starting from an existing LLM-enabled planner, PRISM
automatically synthesizes diverse tasks and environments, elicits plans from
the LLM, and uses this synthetic dataset to distill a compact SLM as a drop-in
replacement of the source model. We apply PRISM to three LLM-enabled planners
for mapping and exploration, manipulation, and household assistance, and we
demonstrate that PRISM improves the performance of Llama-3.2-3B from 10-20% of
GPT-4o's performance to over 93% - using only synthetic data. We further
demonstrate that the distilled planners generalize across heterogeneous robotic
platforms (ground and aerial) and diverse environments (indoor and outdoor). We
release all software, trained models, and datasets at
https://zacravichandran.github.io/PRISM.

</details>


### [7] [Online Adaptation for Flying Quadrotors in Tight Formations](https://arxiv.org/abs/2506.17488)
*Pei-An Hsieh,Kong Yao Chee,M. Ani Hsieh*

Main category: cs.RO

TL;DR: L1 KNODE-DW MPC框架通过自适应学习和混合专家控制，解决了多旋翼无人机在紧密编队飞行中的气动干扰问题，显著提升了轨迹跟踪和稳定性。


<details>
  <summary>Details</summary>
Motivation: 紧密编队飞行中复杂的气动干扰会导致无人机不稳定，且这些干扰难以建模和预测。

Method: 提出L1 KNODE-DW MPC框架，结合自适应学习和模型预测控制，以应对时变气动干扰。

Result: 在三无人机编队实验中，该框架优于多个MPC基线，能保持垂直对齐和近距离飞行。

Conclusion: L1自适应模块与精确动力学模型结合，能有效补偿未建模干扰，提升编队飞行性能。

Abstract: The task of flying in tight formations is challenging for teams of quadrotors
because the complex aerodynamic wake interactions can destabilize individual
team members as well as the team. Furthermore, these aerodynamic effects are
highly nonlinear and fast-paced, making them difficult to model and predict. To
overcome these challenges, we present L1 KNODE-DW MPC, an adaptive, mixed
expert learning based control framework that allows individual quadrotors to
accurately track trajectories while adapting to time-varying aerodynamic
interactions during formation flights. We evaluate L1 KNODE-DW MPC in two
different three-quadrotor formations and show that it outperforms several MPC
baselines. Our results show that the proposed framework is capable of enabling
the three-quadrotor team to remain vertically aligned in close proximity
throughout the flight. These findings show that the L1 adaptive module
compensates for unmodeled disturbances most effectively when paired with an
accurate dynamics model. A video showcasing our framework and the physical
experiments is available here: https://youtu.be/9QX1Q5Ut9Rs

</details>


### [8] [EASE: Embodied Active Event Perception via Self-Supervised Energy Minimization](https://arxiv.org/abs/2506.17516)
*Zhou Chen,Sanjoy Kundu,Harsimran S. Baweja,Sathyanarayanan N. Aakur*

Main category: cs.RO

TL;DR: EASE是一个自监督框架，通过自由能最小化统一时空表示学习和具身控制，无需标注或外部奖励，实现动态事件感知。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预定义动作空间和标注数据，限制了在动态现实场景中的适应性和可扩展性。

Method: 结合生成感知模型和动作驱动控制策略，利用预测误差和熵作为内在信号，动态对齐预测与观察。

Result: 在仿真和现实环境中验证了EASE的隐私保护和可扩展事件感知能力。

Conclusion: EASE为无脚本动态任务中的具身系统提供了鲁棒基础。

Abstract: Active event perception, the ability to dynamically detect, track, and
summarize events in real time, is essential for embodied intelligence in tasks
such as human-AI collaboration, assistive robotics, and autonomous navigation.
However, existing approaches often depend on predefined action spaces,
annotated datasets, and extrinsic rewards, limiting their adaptability and
scalability in dynamic, real-world scenarios. Inspired by cognitive theories of
event perception and predictive coding, we propose EASE, a self-supervised
framework that unifies spatiotemporal representation learning and embodied
control through free energy minimization. EASE leverages prediction errors and
entropy as intrinsic signals to segment events, summarize observations, and
actively track salient actors, operating without explicit annotations or
external rewards. By coupling a generative perception model with an
action-driven control policy, EASE dynamically aligns predictions with
observations, enabling emergent behaviors such as implicit memory, target
continuity, and adaptability to novel environments. Extensive evaluations in
simulation and real-world settings demonstrate EASE's ability to achieve
privacy-preserving and scalable event perception, providing a robust foundation
for embodied systems in unscripted, dynamic tasks.

</details>


### [9] [Risk-Guided Diffusion: Toward Deploying Robot Foundation Models in Space, Where Failure Is Not An Option](https://arxiv.org/abs/2506.17601)
*Rohan Thakker,Adarsh Patnaik,Vince Kurtz,Jonas Frey,Jonathan Becktor,Sangwoo Moon,Rob Royce,Marcel Kaufmann,Georgios Georgakis,Pascal Roth,Joel Burdick,Marco Hutter,Shehryar Khattak*

Main category: cs.RO

TL;DR: 提出了一种结合快速学习系统与慢速物理系统的风险引导扩散框架，显著降低失败率并保持目标达成性能。


<details>
  <summary>Details</summary>
Motivation: 为未来太空探索任务提供安全可靠的导航，解决现有生成AI方法安全性不足的问题。

Method: 融合快速学习的“系统1”与慢速物理的“系统2”，通过风险引导扩散框架共享计算。

Result: 在NASA JPL的Mars Yard实验中，失败率降低4倍，目标达成性能与学习模型相当。

Conclusion: 该方法在无需额外训练的情况下，通过推理计算实现了安全性与适应性的平衡。

Abstract: Safe, reliable navigation in extreme, unfamiliar terrain is required for
future robotic space exploration missions. Recent generative-AI methods learn
semantically aware navigation policies from large, cross-embodiment datasets,
but offer limited safety guarantees. Inspired by human cognitive science, we
propose a risk-guided diffusion framework that fuses a fast, learned "System-1"
with a slow, physics-based "System-2", sharing computation at both training and
inference to couple adaptability with formal safety. Hardware experiments
conducted at the NASA JPL's Mars-analog facility, Mars Yard, show that our
approach reduces failure rates by up to $4\times$ while matching the
goal-reaching performance of learning-based robotic models by leveraging
inference-time compute without any additional training.

</details>


### [10] [Imitation Learning for Active Neck Motion Enabling Robot Manipulation beyond the Field of View](https://arxiv.org/abs/2506.17624)
*Koki Nakagawa,Yoshiyuki Ohmura,Yasuo Kuniyoshi*

Main category: cs.RO

TL;DR: 提出了一种结合颈部运动的机器人模仿学习系统，通过动态视角数据集和新网络模型，显著提升了任务成功率和复杂场景适应性。


<details>
  <summary>Details</summary>
Motivation: 固定视角限制了模仿学习的任务范围，而主动颈部运动可以扩展任务多样性和表现力。

Method: 开发了包含颈部运动的数据集收集系统和新型网络模型，用于学习包含颈部运动的操作任务。

Result: 模型在动态视角干扰下仍能保持约90%的成功率，且在传统模型难以处理的边缘或视野外物体场景中表现优异。

Conclusion: 该方法提高了数据集收集效率，并将模仿学习扩展到更复杂和动态的场景。

Abstract: Most prior research in deep imitation learning has predominantly utilized
fixed cameras for image input, which constrains task performance to the
predefined field of view. However, enabling a robot to actively maneuver its
neck can significantly expand the scope of imitation learning to encompass a
wider variety of tasks and expressive actions such as neck gestures. To
facilitate imitation learning in robots capable of neck movement while
simultaneously performing object manipulation, we propose a teaching system
that systematically collects datasets incorporating neck movements while
minimizing discomfort caused by dynamic viewpoints during teleoperation. In
addition, we present a novel network model for learning manipulation tasks
including active neck motion. Experimental results showed that our model can
achieve a high success rate of around 90\%, regardless of the distraction from
the viewpoint variations by active neck motion. Moreover, the proposed model
proved particularly effective in challenging scenarios, such as when objects
were situated at the periphery or beyond the standard field of view, where
traditional models struggled. The proposed approach contributes to the
efficiency of dataset collection and extends the applicability of imitation
learning to more complex and dynamic scenarios.

</details>


### [11] [RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models](https://arxiv.org/abs/2506.17639)
*Yuxuan Chen,Xiao Li*

Main category: cs.RO

TL;DR: 论文提出了一种名为RLRC的三阶段恢复方法，用于压缩视觉-语言-动作模型（VLA），显著减少了内存使用并提高了推理速度，同时保持了任务成功率。


<details>
  <summary>Details</summary>
Motivation: VLA模型在复杂机器人操作任务中表现出色，但其大参数规模和高推理延迟限制了在资源受限平台上的实际部署。

Method: RLRC方法包括结构化剪枝、基于SFT和RL的性能恢复以及进一步量化三个阶段。

Result: RLRC实现了内存使用减少8倍，推理吞吐量提高2.3倍，同时任务成功率保持或超过原始VLA。

Conclusion: RLRC在压缩VLA方面优于现有基线方法，展示了在设备端部署的潜力。

Abstract: Vision-Language-Action models (VLA) have demonstrated remarkable capabilities
and promising potential in solving complex robotic manipulation tasks. However,
their substantial parameter sizes and high inference latency pose significant
challenges for real-world deployment, particularly on resource-constrained
robotic platforms. To address this issue, we begin by conducting an extensive
empirical study to explore the effectiveness of model compression techniques
when applied to VLAs. Building on the insights gained from these preliminary
experiments, we propose RLRC, a three-stage recovery method for compressed
VLAs, including structured pruning, performance recovery based on SFT and RL,
and further quantization. RLRC achieves up to an 8x reduction in memory usage
and a 2.3x improvement in inference throughput, while maintaining or even
surpassing the original VLA's task success rate. Extensive experiments show
that RLRC consistently outperforms existing compression baselines,
demonstrating strong potential for on-device deployment of VLAs. Project
website: https://rlrc-vla.github.io

</details>


### [12] [Optimizing Exploration with a New Uncertainty Framework for Active SLAM Systems](https://arxiv.org/abs/2506.17775)
*Sebastian Sansoni,Javier Gimenez,Gastón Castro,Santiago Tosetti,Flavio Craparo*

Main category: cs.RO

TL;DR: 提出了一种基于不确定性地图（UM）的主动SLAM新方法，通过概率分布建模地图不确定性，并引入Signed Relative Entropy（SiREn）平衡探索与利用。


<details>
  <summary>Details</summary>
Motivation: 解决SLAM系统中环境重建精度受轨迹影响的问题，提出通用且兼容多传感器的地图不确定性建模方法。

Method: 使用UM捕捉地图不确定性，定义Uncertainty Frontiers（UF）作为目标，并引入SiREn衡量覆盖与不确定性。

Result: 方法适用于多种传感器，解决了探索规划和停止条件问题，并实现自主探索开放空间。

Conclusion: 该方法通用性强，代码和数据公开，便于推广和验证。

Abstract: Accurate reconstruction of the environment is a central goal of Simultaneous
Localization and Mapping (SLAM) systems. However, the agent's trajectory can
significantly affect estimation accuracy. This paper presents a new method to
model map uncertainty in Active SLAM systems using an Uncertainty Map (UM). The
UM uses probability distributions to capture where the map is uncertain,
allowing Uncertainty Frontiers (UF) to be defined as key
exploration-exploitation objectives and potential stopping criteria. In
addition, the method introduces the Signed Relative Entropy (SiREn), based on
the Kullback-Leibler divergence, to measure both coverage and uncertainty
together. This helps balance exploration and exploitation through an
easy-to-understand parameter. Unlike methods that depend on particular SLAM
setups, the proposed approach is compatible with different types of sensors,
such as cameras, LiDARs, and multi-sensor fusion. It also addresses common
problems in exploration planning and stopping conditions. Furthermore,
integrating this map modeling approach with a UF-based planning system enables
the agent to autonomously explore open spaces, a behavior not previously
observed in the Active SLAM literature. Code and implementation details are
available as a ROS node, and all generated data are openly available for public
use, facilitating broader adoption and validation of the proposed approach.

</details>


### [13] [RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models](https://arxiv.org/abs/2506.17811)
*Jacky Kwok,Christopher Agia,Rohan Sinha,Matt Foutter,Shulu Li,Ion Stoica,Azalia Mirhoseini,Marco Pavone*

Main category: cs.RO

TL;DR: 本文提出RoboMonkey框架，通过采样、扰动和验证增强Vision-Language-Action模型的鲁棒性和泛化能力，显著提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 尽管VLA模型在视觉运动控制中表现优异，但在非结构化真实环境中的鲁棒性仍面临挑战。本文旨在通过测试时缩放提升其性能。

Method: 提出RoboMonkey框架，包括采样、高斯扰动、多数投票构建动作分布，以及基于VLM的验证器选择最优动作。使用合成数据训练验证器。

Result: 实验表明，RoboMonkey显著提升性能，OOD任务提升25%，ID任务提升8%。新机器人设置下，联合微调VLA和验证器性能提升7%。

Conclusion: RoboMonkey通过测试时缩放和验证机制有效增强VLA模型的鲁棒性和适应性，为实际部署提供了可靠解决方案。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities
in visuomotor control, yet ensuring their robustness in unstructured real-world
environments remains a persistent challenge. In this paper, we investigate
test-time scaling through the lens of sampling and verification as means to
enhance the robustness and generalization of VLAs. We first demonstrate that
the relationship between action error and the number of generated samples
follows an exponentiated power law across a range of VLAs, indicating the
existence of inference-time scaling laws. Building on these insights, we
introduce RoboMonkey, a test-time scaling framework for VLAs. At deployment,
RoboMonkey samples a small set of actions from a VLA, applies Gaussian
perturbation and majority voting to construct an action proposal distribution,
and then uses a Vision Language Model (VLM)-based verifier to select the
optimal action. We propose a synthetic data generation pipeline for training
such VLM-based action verifiers, and demonstrate that scaling the synthetic
dataset consistently improves verification and downstream accuracy. Through
extensive simulated and hardware experiments, we show that pairing existing
VLAs with RoboMonkey yields significant performance gains, achieving a 25%
absolute improvement on out-of-distribution tasks and 8% on in-distribution
tasks. Additionally, when adapting to new robot setups, we show that
fine-tuning both VLAs and action verifiers yields a 7% performance increase
compared to fine-tuning VLAs alone.

</details>


### [14] [Learning to Dock: A Simulation-based Study on Closing the Sim2Real Gap in Autonomous Underwater Docking](https://arxiv.org/abs/2506.17823)
*Kevin Chang,Rakesh Vivekanandan,Noah Pragin,Sean Bullock,Geoffrey Hollinger*

Main category: cs.RO

TL;DR: 研究通过强化学习减少AUV在动态环境中自主对接的模拟与现实差距，探索随机化技术和历史条件控制器等方法。


<details>
  <summary>Details</summary>
Motivation: 解决AUV在动态和不确定环境中自主对接的挑战，尤其是模拟与现实性能差距问题。

Method: 通过训练多种控制器并在真实扰动下评估，探索随机化技术和历史条件控制器。

Result: 研究发现可减少模拟与现实差距的方法，并指出未来研究方向。

Conclusion: 研究为海洋机器人社区提供了减少模拟与现实差距的见解和未来研究方向。

Abstract: Autonomous Underwater Vehicle (AUV) docking in dynamic and uncertain
environments is a critical challenge for underwater robotics. Reinforcement
learning is a promising method for developing robust controllers, but the
disparity between training simulations and the real world, or the sim2real gap,
often leads to a significant deterioration in performance. In this work, we
perform a simulation study on reducing the sim2real gap in autonomous docking
through training various controllers and then evaluating them under realistic
disturbances. In particular, we focus on the real-world challenge of docking
under different payloads that are potentially outside the original training
distribution. We explore existing methods for improving robustness including
randomization techniques and history-conditioned controllers. Our findings
provide insights into mitigating the sim2real gap when training docking
controllers. Furthermore, our work indicates areas of future research that may
be beneficial to the marine robotics community.

</details>


### [15] [Engagement and Disclosures in LLM-Powered Cognitive Behavioral Therapy Exercises: A Factorial Design Comparing the Influence of a Robot vs. Chatbot Over Time](https://arxiv.org/abs/2506.17831)
*Mina Kian,Mingyu Zong,Katrin Fischer,Anna-Maria Velentza,Abhyuday Singh,Kaleen Shrestha,Pau Sang,Shriya Upadhyay,Wallace Browning,Misha Arif Faruki,Sébastien M. R. Arnold,Bhaskar Krishnamachari,Maja Matarić*

Main category: cs.RO

TL;DR: 研究探讨了具身化（SAR）与非具身化（聊天机器人）对心理治疗参与度和亲密感的影响，发现具身化随时间提升效果，而非具身化则相反。


<details>
  <summary>Details</summary>
Motivation: 解决全球心理健康危机，探索具身化技术（如SAR）与聊天机器人在心理治疗中的长期效果差异。

Method: 采用因子设计，26名大学生在两周内每天使用SAR或聊天机器人完成CBT练习，评估参与度和亲密感的变化。

Result: 具身化（SAR）随时间显著提升参与度和亲密感，而聊天机器人则效果下降。

Conclusion: 具身化设计对心理治疗的长期效果更优，支持SAR在心理健康应用中的潜力。

Abstract: Many researchers are working to address the worldwide mental health crisis by
developing therapeutic technologies that increase the accessibility of care,
including leveraging large language model (LLM) capabilities in chatbots and
socially assistive robots (SARs) used for therapeutic applications. Yet, the
effects of these technologies over time remain unexplored. In this study, we
use a factorial design to assess the impact of embodiment and time spent
engaging in therapeutic exercises on participant disclosures. We assessed
transcripts gathered from a two-week study in which 26 university student
participants completed daily interactive Cognitive Behavioral Therapy (CBT)
exercises in their residences using either an LLM-powered SAR or a disembodied
chatbot. We evaluated the levels of active engagement and high intimacy of
their disclosures (opinions, judgments, and emotions) during each session and
over time. Our findings show significant interactions between time and
embodiment for both outcome measures: participant engagement and intimacy
increased over time in the physical robot condition, while both measures
decreased in the chatbot condition.

</details>


### [16] [Leveling the Playing Field: Carefully Comparing Classical and Learned Controllers for Quadrotor Trajectory Tracking](https://arxiv.org/abs/2506.17832)
*Pratik Kunapuli,Jake Welde,Dinesh Jayaraman,Vijay Kumar*

Main category: cs.RO

TL;DR: 论文探讨了基于学习的控制器（如强化学习）与传统几何控制器在四旋翼轨迹跟踪任务中的性能比较，提出了对称比较的最佳实践，并发现两者在不同场景下各有优劣。


<details>
  <summary>Details</summary>
Motivation: 现有研究在比较学习控制器与传统控制器时存在不对称性，可能导致误导性结论，因此需要更公平的比较方法。

Method: 通过案例研究（四旋翼固定臂的敏捷跟踪任务），开发了一套最佳实践，用于合成和比较强化学习与几何控制器。

Result: 对称比较后，几何控制器在稳态误差上表现更好，而强化学习在瞬态性能上更优。

Conclusion: 改进的实验协议对公平比较至关重要，未来研究应避免不对称性，并开源了控制器实现。

Abstract: Learning-based control approaches like reinforcement learning (RL) have
recently produced a slew of impressive results for tasks like quadrotor
trajectory tracking and drone racing. Naturally, it is common to demonstrate
the advantages of these new controllers against established methods like
analytical controllers. We observe, however, that reliably comparing the
performance of such very different classes of controllers is more complicated
than might appear at first sight. As a case study, we take up the problem of
agile tracking of an end-effector for a quadrotor with a fixed arm. We develop
a set of best practices for synthesizing the best-in-class RL and geometric
controllers (GC) for benchmarking. In the process, we resolve widespread
RL-favoring biases in prior studies that provide asymmetric access to: (1) the
task definition, in the form of an objective function, (2) representative
datasets, for parameter optimization, and (3) feedforward information,
describing the desired future trajectory. The resulting findings are the
following: our improvements to the experimental protocol for comparing learned
and classical controllers are critical, and each of the above asymmetries can
yield misleading conclusions. Prior works have claimed that RL outperforms GC,
but we find the gaps between the two controller classes are much smaller than
previously published when accounting for symmetric comparisons. Geometric
control achieves lower steady-state error than RL, while RL has better
transient performance, resulting in GC performing better in relatively slow or
less agile tasks, but RL performing better when greater agility is required.
Finally, we open-source implementations of geometric and RL controllers for
these aerial vehicles, implementing best practices for future development.
Website and code is available at https://pratikkunapuli.github.io/rl-vs-gc/

</details>


### [17] [Generative Grasp Detection and Estimation with Concept Learning-based Safety Criteria](https://arxiv.org/abs/2506.17842)
*Al-Harith Farhad,Khalil Abuibaid,Christiane Plociennik,Achim Wagner,Martin Ruskowski*

Main category: cs.RO

TL;DR: 提出了一种用于协作机器人抓取算法的透明化流程，结合可解释AI方法提高安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络作为黑盒模型在安全关键应用中的复杂性和不透明性问题。

Method: 集成可解释AI方法，提取学习特征并与输入类别关联，作为额外安全标准。

Result: 在工业环境中测试，验证了方法的稳定性和抓取位置的改进。

Conclusion: 该方法提高了协作机器人抓取算法的透明度和可靠性，适用于工业环境。

Abstract: Neural networks are often regarded as universal equations that can estimate
any function. This flexibility, however, comes with the drawback of high
complexity, rendering these networks into black box models, which is especially
relevant in safety-centric applications. To that end, we propose a pipeline for
a collaborative robot (Cobot) grasping algorithm that detects relevant tools
and generates the optimal grasp. To increase the transparency and reliability
of this approach, we integrate an explainable AI method that provides an
explanation for the underlying prediction of a model by extracting the learned
features and correlating them to corresponding classes from the input. These
concepts are then used as additional criteria to ensure the safe handling of
work tools. In this paper, we show the consistency of this approach and the
criterion for improving the handover position. This approach was tested in an
industrial environment, where a camera system was set up to enable a robot to
pick up certain tools and objects.

</details>


### [18] [Geometric Contact Flows: Contactomorphisms for Dynamics and Control](https://arxiv.org/abs/2506.17868)
*Andrea Testa,Søren Hauberg,Tamim Asfour,Leonel Rozo*

Main category: cs.RO

TL;DR: 论文提出了一种名为Geometric Contact Flows（GFC）的新框架，利用黎曼几何和接触几何作为归纳偏置，学习复杂动力系统，并在物理系统和机器人控制任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 准确建模和预测涉及力交换和耗散的复杂动力系统对流体动力学和机器人学等应用至关重要，但由于几何约束和能量传递的复杂相互作用，存在显著挑战。

Method: GFC构建了一个潜在接触哈密顿模型，编码稳定性或能量守恒等理想特性，并通过接触同胚的集合将该模型适应于目标动力学，同时保留这些特性。

Result: 实验表明，GFC在学习物理系统动力学和控制机器人交互任务方面具有有效性。

Conclusion: GFC通过几何方法实现了对复杂动力系统的鲁棒建模和预测，具有广泛的应用潜力。

Abstract: Accurately modeling and predicting complex dynamical systems, particularly
those involving force exchange and dissipation, is crucial for applications
ranging from fluid dynamics to robotics, but presents significant challenges
due to the intricate interplay of geometric constraints and energy transfer.
This paper introduces Geometric Contact Flows (GFC), a novel framework
leveraging Riemannian and Contact geometry as inductive biases to learn such
systems. GCF constructs a latent contact Hamiltonian model encoding desirable
properties like stability or energy conservation. An ensemble of
contactomorphisms then adapts this model to the target dynamics while
preserving these properties. This ensemble allows for uncertainty-aware
geodesics that attract the system's behavior toward the data support, enabling
robust generalization and adaptation to unseen scenarios. Experiments on
learning dynamics for physical systems and for controlling robots on
interaction tasks demonstrate the effectiveness of our approach.

</details>


### [19] [Embedded Flexible Circumferential Sensing for Real-Time Intraoperative Environmental Perception in Continuum Robots](https://arxiv.org/abs/2506.17902)
*Peiyu Luo,Shilong Yao,Yuhan Chen,Max Q. -H. Meng*

Main category: cs.RO

TL;DR: 提出了一种集成在连续体机器人椎间盘周围的柔性环形传感器结构，用于实时环境映射，提高手术安全性。


<details>
  <summary>Details</summary>
Motivation: 连续体机器人在狭窄腔道中缺乏环境感知能力，可能导致意外组织接触和手术风险。

Method: 设计了一种基于柔性印刷电路技术的环形传感器，用于实时估计机器人与周围组织的距离。

Result: 实验证明传感器在障碍物检测中的精度可达0.19毫米，具有模块化、低成本和小尺寸的特点。

Conclusion: 该传感器设计为增强手术机器人术中感知和控制提供了有前景的解决方案。

Abstract: Continuum robots have been widely adopted in robot-assisted minimally
invasive surgery (RMIS) because of their compact size and high flexibility.
However, their proprioceptive capabilities remain limited, particularly in
narrow lumens, where lack of environmental awareness can lead to unintended
tissue contact and surgical risks. To address this challenge, this work
proposes a flexible annular sensor structure integrated around the vertebral
disks of continuum robots. The proposed design enables real-time environmental
mapping by estimating the distance between the robotic disks and the
surrounding tissue, thereby facilitating safer operation through advanced
control strategies. The experiment has proven that its accuracy in obstacle
detection can reach 0.19 mm. Fabricated using flexible printed circuit (FPC)
technology, the sensor demonstrates a modular and cost-effective design with
compact dimensions and low noise interference. Its adaptable parameters allow
compatibility with various continuum robot architectures, offering a promising
solution for enhancing intraoperative perception and control in surgical
robotics.

</details>


### [20] [GeNIE: A Generalizable Navigation System for In-the-Wild Environments](https://arxiv.org/abs/2506.17960)
*Jiaming Wang,Diwen Liu,Jizhuo Chen,Jiaxuan Da,Nuowen Qian,Tram Minh Man,Harold Soh*

Main category: cs.RO

TL;DR: GeNIE是一个通用导航系统，通过结合可泛化的可通行性预测模型和路径融合策略，在复杂环境中实现稳定导航，并在ICRA 2025比赛中取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 解决非结构化真实环境中导航的挑战，特别是在多样化地形、天气和传感器配置下的可靠性问题。

Method: 整合基于SAM2的可泛化可通行性预测模型和新型路径融合策略，提升规划稳定性。

Result: 在ICRA 2025比赛中获得第一名，成绩超出第二名17%，全程无需人工干预。

Conclusion: GeNIE为户外机器人导航设立了新标杆，并将开源代码和数据集以支持未来研究。

Abstract: Reliable navigation in unstructured, real-world environments remains a
significant challenge for embodied agents, especially when operating across
diverse terrains, weather conditions, and sensor configurations. In this paper,
we introduce GeNIE (Generalizable Navigation System for In-the-Wild
Environments), a robust navigation framework designed for global deployment.
GeNIE integrates a generalizable traversability prediction model built on SAM2
with a novel path fusion strategy that enhances planning stability in noisy and
ambiguous settings. We deployed GeNIE in the Earth Rover Challenge (ERC) at
ICRA 2025, where it was evaluated across six countries spanning three
continents. GeNIE took first place and achieved 79% of the maximum possible
score, outperforming the second-best team by 17%, and completed the entire
competition without a single human intervention. These results set a new
benchmark for robust, generalizable outdoor robot navigation. We will release
the codebase, pretrained model weights, and newly curated datasets to support
future research in real-world navigation.

</details>


### [21] [Newtonian and Lagrangian Neural Networks: A Comparison Towards Efficient Inverse Dynamics Identification](https://arxiv.org/abs/2506.17994)
*Minh Trinh,Andreas René Geist,Josefine Monnet,Stefan Vilceanu,Sebastian Trimpe,Christian Brecher*

Main category: cs.RO

TL;DR: 比较牛顿神经网络和拉格朗日神经网络在工业机器人逆动力学建模中的表现，发现牛顿神经网络在估计电机扭矩时更有效。


<details>
  <summary>Details</summary>
Motivation: 工业机器人控制需要精确的逆动力学模型，但目前缺乏关于选择拉格朗日或牛顿神经网络的指导。

Method: 结合神经网络回归与牛顿-欧拉和欧拉-拉格朗日运动方程的逆动力学公式，比较两种模型在MABI MAX 100工业机器人数据上的表现。

Result: 拉格朗日神经网络在估计电机扭矩时效果较差，因其未显式建模耗散扭矩。

Conclusion: 牛顿神经网络在电机扭矩估计中表现更优，适用于工业机器人控制。

Abstract: Accurate inverse dynamics models are essential tools for controlling
industrial robots. Recent research combines neural network regression with
inverse dynamics formulations of the Newton-Euler and the Euler-Lagrange
equations of motion, resulting in so-called Newtonian neural networks and
Lagrangian neural networks, respectively. These physics-informed models seek to
identify unknowns in the analytical equations from data. Despite their
potential, current literature lacks guidance on choosing between Lagrangian and
Newtonian networks. In this study, we show that when motor torques are
estimated instead of directly measuring joint torques, Lagrangian networks
prove less effective compared to Newtonian networks as they do not explicitly
model dissipative torques. The performance of these models is compared to
neural network regression on data of a MABI MAX 100 industrial robot.

</details>


### [22] [ADA-DPM: A Neural Descriptors-based Adaptive Noise Point Filtering Strategy for SLAM](https://arxiv.org/abs/2506.18016)
*Yongxin Shao,Binrui Wang,Aihong Tan*

Main category: cs.RO

TL;DR: 提出了一种自适应噪声过滤SLAM策略ADA-DPM，通过动态分割头和全局重要性评分头优化特征点选择，结合GLI-GCN模块提升特征判别能力，在动态干扰和噪声环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR SLAM方法在动态物体干扰、点云噪声和非结构化环境中需权衡定位精度与系统鲁棒性。

Method: 设计动态分割头预测动态特征点类别并消除；全局重要性评分头自适应选择高贡献特征点；构建GLI-GCN模块融合多尺度邻域结构。

Result: 在多个公开数据集上测试，取得了出色的结果。

Conclusion: ADA-DPM在定位精度和系统鲁棒性方面均表现出色。

Abstract: LiDAR SLAM has demonstrated significant application value in various fields,
including mobile robot navigation and high-precision map construction. However,
existing methods often need to make a trade-off between positioning accuracy
and system robustness when faced with dynamic object interference, point cloud
noise, and unstructured environments. To address this challenge, we propose an
adaptive noise filtering SLAM strategy-ADA-DPM, achieving excellent preference
in both aspects. We design the Dynamic Segmentation Head to predict the
category of feature points belonging to dynamic points, to eliminate dynamic
feature points; design the Global Importance Scoring Head to adaptively select
feature points with higher contribution and features while suppressing noise
interference; and construct the Cross Layer Intra-Graph Convolution Module
(GLI-GCN) to fuse multi-scale neighborhood structures, thereby enhancing the
discriminative ability of overlapping features. Finally, to further validate
the effectiveness of our method, we tested it on several publicly available
datasets and achieved outstanding results.

</details>


### [23] [StereoTacTip: Vision-based Tactile Sensing with Biomimetic Skin-Marker Arrangements](https://arxiv.org/abs/2506.18040)
*Chenghua Lu,Kailuan Tang,Xueming Hui,Haoran Li,Saekwang Nam,Nathan F. Lepora*

Main category: cs.RO

TL;DR: 本文研究了基于标记的视觉触觉传感器（VBTS）中皮肤形态对立体视觉触觉感知的影响，提出了一种名为StereoTacTip的新型VBTS，并通过多种算法和模型实现了精确的几何重建。


<details>
  <summary>Details</summary>
Motivation: 许多基于标记的VBTS使用复杂的仿生皮肤标记排列，导致从标记重建皮肤表面几何形状时出现问题。本文旨在探索标记皮肤形态如何影响立体视觉触觉感知。

Method: 提出了一种新型VBTS（StereoTacTip），并引入以下方法：(i) 使用Delaunay三角环编码算法进行立体标记匹配与跟踪；(ii) 提出折射深度校正模型以修正内部介质折射引起的深度失真；(iii) 基于标记位置的皮肤表面校正模型；(iv) 多接触点的几何重建方法。

Result: 通过在大规模3D地图上重建地形，验证了所提方法的有效性。贡献(i)和(ii)虽然针对仿生标记设计，但应能提升所有基于标记的VBTS性能。

Conclusion: 研究表明，深入理解和评估形态复杂的皮肤及基于标记的触觉传感器原理，对于获取精确几何信息至关重要。

Abstract: Vision-Based Tactile Sensors (VBTSs) stand out for their superior performance
due to their high-information content output. Recently, marker-based VBTSs have
been shown to give accurate geometry reconstruction when using stereo cameras.
\uhl{However, many marker-based VBTSs use complex biomimetic skin-marker
arrangements, which presents issues for the geometric reconstruction of the
skin surface from the markers}. Here we investigate how the marker-based skin
morphology affects stereo vision-based tactile sensing, using a novel VBTS
called the StereoTacTip. To achieve accurate geometry reconstruction, we
introduce: (i) stereo marker matching and tracking using a novel
Delaunay-Triangulation-Ring-Coding algorithm; (ii) a refractive depth
correction model that corrects the depth distortion caused by refraction in the
internal media; (iii) a skin surface correction model from the marker
positions, relying on an inverse calculation of normals to the skin surface;
and (iv)~methods for geometry reconstruction over multiple contacts. To
demonstrate these findings, we reconstruct topographic terrains on a large 3D
map. Even though contributions (i) and (ii) were developed for biomimetic
markers, they should improve the performance of all marker-based VBTSs.
Overall, this work illustrates that a thorough understanding and evaluation of
the morphologically-complex skin and marker-based tactile sensor principles are
crucial for obtaining accurate geometric information.

</details>


### [24] [RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation](https://arxiv.org/abs/2506.18088)
*Tianxing Chen,Zanxin Chen,Baijun Chen,Zijian Cai,Yibin Liu,Qiwei Liang,Zixuan Li,Xianliang Lin,Yiheng Ge,Zhenyu Gu,Weiliang Deng,Yubin Guo,Tian Nian,Xuanbing Xie,Qiangyu Chen,Kailun Su,Tianling Xu,Guodong Liu,Mengkang Hu,Huan-ang Gao,Kaixuan Wang,Zhixuan Liang,Yusen Qin,Xiaokang Yang,Ping Luo,Yao Mu*

Main category: cs.RO

TL;DR: RoboTwin 2.0是一个可扩展的仿真框架，用于生成多样且真实的双机械臂操作数据，通过结合多模态大语言模型和仿真优化，显著提升了代码生成成功率和现实场景泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有合成数据集在双机械臂操作中效率低和仿真环境过于简化的问题。

Method: 构建大规模物体库，结合MLLMs和仿真优化生成任务级代码，并通过结构化域随机化提升数据多样性。

Result: 代码生成成功率提升10.9%，在未见过的现实任务中泛化能力显著增强（最高367%相对提升）。

Conclusion: RoboTwin 2.0为双机械臂操作的稳健研究提供了可扩展的数据生成和评估工具。

Abstract: Simulation-based data synthesis has emerged as a powerful paradigm for
enhancing real-world robotic manipulation. However, existing synthetic datasets
remain insufficient for robust bimanual manipulation due to two challenges: (1)
the lack of an efficient, scalable data generation method for novel tasks, and
(2) oversimplified simulation environments that fail to capture real-world
complexity. We present RoboTwin 2.0, a scalable simulation framework that
enables automated, large-scale generation of diverse and realistic data, along
with unified evaluation protocols for dual-arm manipulation. We first construct
RoboTwin-OD, a large-scale object library comprising 731 instances across 147
categories, each annotated with semantic and manipulation-relevant labels.
Building on this foundation, we develop an expert data synthesis pipeline that
combines multimodal large language models (MLLMs) with simulation-in-the-loop
refinement to generate task-level execution code automatically. To improve
sim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization
along five axes: clutter, lighting, background, tabletop height and language
instructions, thereby enhancing data diversity and policy robustness. We
instantiate this framework across 50 dual-arm tasks spanning five robot
embodiments, and pre-collect over 100,000 domain-randomized expert
trajectories. Empirical results show a 10.9% gain in code generation success
and improved generalization to novel real-world scenarios. A VLA model
fine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)
on unseen scene real-world tasks, while zero-shot models trained solely on our
synthetic data achieve a 228% relative gain, highlighting strong generalization
without real-world supervision. We release the data generator, benchmark,
dataset, and code to support scalable research in robust bimanual manipulation.

</details>


### [25] [RoboArena: Distributed Real-World Evaluation of Generalist Robot Policies](https://arxiv.org/abs/2506.18123)
*Pranav Atreya,Karl Pertsch,Tony Lee,Moo Jin Kim,Arhan Jain,Artur Kuramshin,Clemens Eppner,Cyrus Neary,Edward Hu,Fabio Ramos,Jonathan Tremblay,Kanav Arora,Kirsty Ellis,Luca Macesanu,Matthew Leonard,Meedeum Cho,Ozgur Aslan,Shivin Dass,Jie Wang,Xingfang Yuan,Xuning Yang,Abhishek Gupta,Dinesh Jayaraman,Glen Berseth,Kostas Daniilidis,Roberto Martin-Martin,Youngwoon Lee,Percy Liang,Chelsea Finn,Sergey Levine*

Main category: cs.RO

TL;DR: 提出RoboArena，一种通过众包评估通用机器人策略的新方法，取代传统固定任务评估，提高多样性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人评估方法依赖固定任务或集中式挑战，难以扩展和评估通用策略的多样性。

Method: 通过分布式网络众包评估，评估者自由选择任务和环境，进行双盲策略对比，汇总偏好反馈生成策略排名。

Result: 在7个学术机构的600多次评估中，该方法比传统方法更准确、可扩展、可靠。

Conclusion: RoboArena为通用机器人策略提供了更灵活、可信的评估框架，并开放网络以促进社区合作。

Abstract: Comprehensive, unbiased, and comparable evaluation of modern generalist
policies is uniquely challenging: existing approaches for robot benchmarking
typically rely on heavy standardization, either by specifying fixed evaluation
tasks and environments, or by hosting centralized ''robot challenges'', and do
not readily scale to evaluating generalist policies across a broad range of
tasks and environments. In this work, we propose RoboArena, a new approach for
scalable evaluation of generalist robot policies in the real world. Instead of
standardizing evaluations around fixed tasks, environments, or locations, we
propose to crowd-source evaluations across a distributed network of evaluators.
Importantly, evaluators can freely choose the tasks and environments they
evaluate on, enabling easy scaling of diversity, but they are required to
perform double-blind evaluations over pairs of policies. Then, by aggregating
preference feedback from pairwise comparisons across diverse tasks and
environments, we can derive a ranking of policies. We instantiate our approach
across a network of evaluators at seven academic institutions using the DROID
robot platform. Through more than 600 pairwise real-robot evaluation episodes
across seven generalist policies, we demonstrate that our crowd-sourced
approach can more accurately rank the performance of existing generalist
policies than conventional, centralized evaluation approaches, while being more
scalable, resilient, and trustworthy. We open our evaluation network to the
community and hope that it can enable more accessible comparisons of generalist
robot policies.

</details>


### [26] [Automated Plan Refinement for Improving Efficiency of Robotic Layup of Composite Sheets](https://arxiv.org/abs/2506.18160)
*Rutvik Patel,Alec Kanyuck,Zachary McNulty,Zeren Yu,Lisa Carlson,Vann Heng,Brice Johnson,Satyandra K. Gupta*

Main category: cs.RO

TL;DR: 提出了一种结合人类专业知识和数据驱动决策的框架，用于优化复合材料铺层的机器人执行计划，减少未压实区域并提高时间效率。


<details>
  <summary>Details</summary>
Motivation: 复合材料铺层自动化需求增加，但现有铺层计划在不同条件下表现不稳定，需要优化以适应多样化的生产环境。

Method: 整合人类专业知识与数据驱动决策，通过实验验证、动作有效性建模和基于搜索的优化来改进铺层计划。

Result: 实验显示，该方法显著减少了纠正路径数量，提高了时间效率，优于专家初始设计的计划。

Conclusion: 该框架有效优化了机器人铺层过程，推动了复合材料制造自动化的技术进步。

Abstract: The automation of composite sheet layup is essential to meet the increasing
demand for composite materials in various industries. However, draping plans
for the robotic layup of composite sheets are not robust. A plan that works
well under a certain condition does not work well in a different condition.
Changes in operating conditions due to either changes in material properties or
working environment may lead a draping plan to exhibit suboptimal performance.
In this paper, we present a comprehensive framework aimed at refining plans
based on the observed execution performance. Our framework prioritizes the
minimization of uncompacted regions while simultaneously improving time
efficiency. To achieve this, we integrate human expertise with data-driven
decision-making to refine expert-crafted plans for diverse production
environments. We conduct experiments to validate the effectiveness of our
approach, revealing significant reductions in the number of corrective paths
required compared to initial expert-crafted plans. Through a combination of
empirical data analysis, action-effectiveness modeling, and search-based
refinement, our system achieves superior time efficiency in robotic layup.
Experimental results demonstrate the efficacy of our approach in optimizing the
layup process, thereby advancing the state-of-the-art in composite
manufacturing automation.

</details>


### [27] [Integrating LLMs and Digital Twins for Adaptive Multi-Robot Task Allocation in Construction](https://arxiv.org/abs/2506.18178)
*Min Deng,Bo Fu,Lingyao Li,Xi Wang*

Main category: cs.RO

TL;DR: 提出了一种结合数字孪生、整数规划和大型语言模型的自适应任务分配框架，用于解决多机器人系统在动态环境中的协调问题。


<details>
  <summary>Details</summary>
Motivation: 工业领域对多机器人系统的需求增加，但在动态和不确定环境（如建筑工地）中协调机器人仍具挑战性。

Method: 使用整数规划模型解决任务分配问题，结合大型语言模型解析自然语言输入并动态更新约束，通过数字孪生实现实时同步。

Result: 案例研究表明优化算法高效，大型语言模型在约束和参数提取中准确率超过97%。

Conclusion: 该方法具有实用性、适应性和跨领域适用性。

Abstract: Multi-robot systems are emerging as a promising solution to the growing
demand for productivity, safety, and adaptability across industrial sectors.
However, effectively coordinating multiple robots in dynamic and uncertain
environments, such as construction sites, remains a challenge, particularly due
to unpredictable factors like material delays, unexpected site conditions, and
weather-induced disruptions. To address these challenges, this study proposes
an adaptive task allocation framework that strategically leverages the
synergistic potential of Digital Twins, Integer Programming (IP), and Large
Language Models (LLMs). The multi-robot task allocation problem is formally
defined and solved using an IP model that accounts for task dependencies, robot
heterogeneity, scheduling constraints, and re-planning requirements. A
mechanism for narrative-driven schedule adaptation is introduced, in which
unstructured natural language inputs are interpreted by an LLM, and
optimization constraints are autonomously updated, enabling human-in-the-loop
flexibility without manual coding. A digital twin-based system has been
developed to enable real-time synchronization between physical operations and
their digital representations. This closed-loop feedback framework ensures that
the system remains dynamic and responsive to ongoing changes on site. A case
study demonstrates both the computational efficiency of the optimization
algorithm and the reasoning performance of several LLMs, with top-performing
models achieving over 97% accuracy in constraint and parameter extraction. The
results confirm the practicality, adaptability, and cross-domain applicability
of the proposed methods.

</details>


### [28] [Haptic-ACT -- Pseudo Oocyte Manipulation by a Robot Using Multimodal Information and Action Chunking with Transformers](https://arxiv.org/abs/2506.18212)
*Pedro Miguel Uriguen Eljuri,Hironobu Shibata,Maeyama Katsuyoshi,Yuanyuan Jia,Tadahiro Taniguchi*

Main category: cs.RO

TL;DR: Haptic-ACT是一种结合触觉反馈和ACT的先进机器人系统，用于伪卵母细胞操作，提高了任务成功率、鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统卵母细胞转移自动化方法依赖视觉感知，常需人工监督，Haptic-ACT通过触觉反馈增强ACT，解决生物变异和环境干扰问题。

Method: 结合触觉反馈和ACT，引入3D打印TPU软夹持器，实现实时抓取失败检测和自适应校正。

Result: 实验表明Haptic-ACT在动态环境中优于传统ACT，提高了任务成功率、鲁棒性和适应性。

Conclusion: 多模态学习在生物医学自动化机器人中具有潜力。

Abstract: In this paper we introduce Haptic-ACT, an advanced robotic system for pseudo
oocyte manipulation, integrating multimodal information and Action Chunking
with Transformers (ACT). Traditional automation methods for oocyte transfer
rely heavily on visual perception, often requiring human supervision due to
biological variability and environmental disturbances. Haptic-ACT enhances ACT
by incorporating haptic feedback, enabling real-time grasp failure detection
and adaptive correction. Additionally, we introduce a 3D-printed TPU soft
gripper to facilitate delicate manipulations. Experimental results demonstrate
that Haptic-ACT improves the task success rate, robustness, and adaptability
compared to conventional ACT, particularly in dynamic environments. These
findings highlight the potential of multimodal learning in robotics for
biomedical automation.

</details>


### [29] [Robot Tactile Gesture Recognition Based on Full-body Modular E-skin](https://arxiv.org/abs/2506.18256)
*Shuo Jiang,Boce Hu,Linfeng Zhao,Lawson L. S. Wong*

Main category: cs.RO

TL;DR: 本文探讨了机器人电子皮肤技术如何通过AI增强的触觉传感器识别触觉手势并将其转化为人类指令。


<details>
  <summary>Details</summary>
Motivation: 随着机器人电子皮肤技术的发展，如何利用触觉传感器实现更直观的人机交互成为研究重点。

Method: 开发了一种模块化机器人电子皮肤，由多个不规则形状的皮肤补丁组成，能够实时捕捉压力和姿态数据。采用等变图神经网络分类器高效准确地识别触觉手势。

Result: 系统成功分类了多种触觉手势（如戳、抓、抚摸、双击），并将其映射为预定义的机器人动作。

Conclusion: 通过触觉输入实现了直观的人机交互，展示了电子皮肤技术在机器人感知领域的潜力。

Abstract: With the development of robot electronic skin technology, various tactile
sensors, enhanced by AI, are unlocking a new dimension of perception for
robots. In this work, we explore how robots equipped with electronic skin can
recognize tactile gestures and interpret them as human commands. We developed a
modular robot E-skin, composed of multiple irregularly shaped skin patches,
which can be assembled to cover the robot's body while capturing real-time
pressure and pose data from thousands of sensing points. To process this
information, we propose an equivariant graph neural network-based recognizer
that efficiently and accurately classifies diverse tactile gestures, including
poke, grab, stroke, and double-pat. By mapping the recognized gestures to
predefined robot actions, we enable intuitive human-robot interaction purely
through tactile input.

</details>


### [30] [Learning Approach to Efficient Vision-based Active Tracking of a Flying Target by an Unmanned Aerial Vehicle](https://arxiv.org/abs/2506.18264)
*Jagadeswara PKV Pothuri,Aditya Bhatt,Prajit KrisshnaKumar,Manaswin Oddiraju,Souma Chowdhury*

Main category: cs.RO

TL;DR: 论文提出了一种结合深度学习与KCF的高效目标检测方法，并利用强化学习训练神经控制器解决无人机视觉跟踪问题，实验验证其优于传统PID控制。


<details>
  <summary>Details</summary>
Motivation: 解决地面基础设施跟踪的局限性，以及传统方法在复杂环境中的不足，实现无人机对空中目标的视觉主动跟踪。

Method: 1) 结合深度学习与KCF实现高效目标检测；2) 使用强化学习训练神经控制器进行机动决策。

Result: 实验表明，提出的感知框架高效且准确，神经控制器在复杂目标机动下优于PID控制。

Conclusion: 该方法为无人机视觉跟踪提供了高效解决方案，适用于复杂环境和动态目标。

Abstract: Autonomous tracking of flying aerial objects has important civilian and
defense applications, ranging from search and rescue to counter-unmanned aerial
systems (counter-UAS). Ground based tracking requires setting up
infrastructure, could be range limited, and may not be feasible in remote
areas, crowded cities or in dense vegetation areas. Vision based active
tracking of aerial objects from another airborne vehicle, e.g., a chaser
unmanned aerial vehicle (UAV), promises to fill this important gap, along with
serving aerial coordination use cases. Vision-based active tracking by a UAV
entails solving two coupled problems: 1) compute-efficient and accurate
(target) object detection and target state estimation; and 2) maneuver
decisions to ensure that the target remains in the field of view in the future
time-steps and favorably positioned for continued detection. As a solution to
the first problem, this paper presents a novel integration of standard deep
learning based architectures with Kernelized Correlation Filter (KCF) to
achieve compute-efficient object detection without compromising accuracy,
unlike standalone learning or filtering approaches. The proposed perception
framework is validated using a lab-scale setup. For the second problem, to
obviate the linearity assumptions and background variations limiting
effectiveness of the traditional controllers, we present the use of
reinforcement learning to train a neuro-controller for fast computation of
velocity maneuvers. New state space, action space and reward formulations are
developed for this purpose, and training is performed in simulation using
AirSim. The trained model is also tested in AirSim with respect to complex
target maneuvers, and is found to outperform a baseline PID control in terms of
tracking up-time and average distance maintained (from the target) during
tracking.

</details>


### [31] [Improvement on LiDAR-Camera Calibration Using Square Targets](https://arxiv.org/abs/2506.18294)
*Zhongyuan Li,Honggang Gou,Ping Li,Jiaotong Guo,Mao Ye*

Main category: cs.RO

TL;DR: 提出了一种基于目标的快速、易部署且对传感器噪声鲁棒的全自动LiDAR-相机外参标定算法。


<details>
  <summary>Details</summary>
Motivation: 精确的传感器标定对自动驾驶车辆至关重要，现有方法未充分考虑工厂制造或售后服务场景的挑战。

Method: 包括自动多阶段LiDAR板检测、快速粗外参搜索机制和直接优化算法。

Result: 在真实场景数据中验证了方法的有效性。

Conclusion: 该方法快速、鲁棒且易于部署，适用于实际应用场景。

Abstract: Precise sensor calibration is critical for autonomous vehicles as a
prerequisite for perception algorithms to function properly. Rotation error of
one degree can translate to position error of meters in target object detection
at large distance, leading to improper reaction of the system or even safety
related issues. Many methods for multi-sensor calibration have been proposed.
However, there are very few work that comprehensively consider the challenges
of the calibration procedure when applied to factory manufacturing pipeline or
after-sales service scenarios. In this work, we introduce a fully automatic
LiDAR-camera extrinsic calibration algorithm based on targets that is fast,
easy to deploy and robust to sensor noises such as missing data. The core of
the method include: (1) an automatic multi-stage LiDAR board detection pipeline
using only geometry information with no specific material requirement; (2) a
fast coarse extrinsic parameter search mechanism that is robust to initial
extrinsic errors; (3) a direct optimization algorithm that is robust to sensor
noises. We validate the effectiveness of our methods through experiments on
data captured in real world scenarios.

</details>


### [32] [TritonZ: A Remotely Operated Underwater Rover with Manipulator Arm for Exploration and Rescue Operations](https://arxiv.org/abs/2506.18343)
*Kawser Ahmed,Mir Shahriar Fardin,Md Arif Faysal Nayem,Fahim Hafiz,Swakkhar Shatabda*

Main category: cs.RO

TL;DR: 论文介绍了一种半无线水下车辆'TritonZ'，配备机械臂和多种传感器，用于水下探索和救援任务。实验证明其高效性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 水下探索和救援任务的需求增加，推动了半无线水下车辆的发展，以应对复杂水下环境的挑战。

Method: 设计并实现了一种紧凑的半无线水下车辆，配备机械臂和传感器（如Pi-Camera、温湿度传感器），通过定制遥控器控制，实现实时数据传输和任务执行。

Result: 实验结果显示，TritonZ能以平均13.5cm/s的速度运行，延迟仅2-3秒，并能稳定应对水下波浪。

Conclusion: TritonZ是一种高效且稳定的水下探索和救援工具，实验验证了其性能，项目代码已开源。

Abstract: The increasing demand for underwater exploration and rescue operations
enforces the development of advanced wireless or semi-wireless underwater
vessels equipped with manipulator arms. This paper presents the implementation
of a semi-wireless underwater vehicle, "TritonZ" equipped with a manipulator
arm, tailored for effective underwater exploration and rescue operations. The
vehicle's compact design enables deployment in different submarine
surroundings, addressing the need for wireless systems capable of navigating
challenging underwater terrains. The manipulator arm can interact with the
environment, allowing the robot to perform sophisticated tasks during
exploration and rescue missions in emergency situations. TritonZ is equipped
with various sensors such as Pi-Camera, Humidity, and Temperature sensors to
send real-time environmental data. Our underwater vehicle controlled using a
customized remote controller can navigate efficiently in the water where
Pi-Camera enables live streaming of the surroundings. Motion control and video
capture are performed simultaneously using this camera. The manipulator arm is
designed to perform various tasks, similar to grasping, manipulating, and
collecting underwater objects. Experimental results shows the efficacy of the
proposed remotely operated vehicle in performing a variety of underwater
exploration and rescue tasks. Additionally, the results show that TritonZ can
maintain an average of 13.5cm/s with a minimal delay of 2-3 seconds.
Furthermore, the vehicle can sustain waves underwater by maintaining its
position as well as average velocity. The full project details and source code
can be accessed at this link: https://github.com/kawser-ahmed-byte/TritonZ

</details>


### [33] [Robotic Manipulation of a Rotating Chain with Bottom End Fixed](https://arxiv.org/abs/2506.18355)
*Qi Jing Chen,Shilin Shan,Quang-Cuong Pham*

Main category: cs.RO

TL;DR: 本文研究了机器人手臂如何操纵底部固定的均匀旋转链条，提出了一种稳定且一致的形状转换策略，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究探讨了理想旋转形状的实际应用，但未涉及如何通过操纵规划实现这些形状。本文旨在填补这一空白。

Method: 利用链条构型空间与三维立方体同胚的性质，提出了一种考虑稳定性和可行性的操纵策略。

Result: 在物理实验中成功实现了从静止状态到前两种旋转模式的转换。

Conclusion: 该策略在钻柱和纱线纺丝操作中具有重要的安全性和效率应用价值。

Abstract: This paper studies the problem of using a robot arm to manipulate a uniformly
rotating chain with its bottom end fixed. Existing studies have investigated
ideal rotational shapes for practical applications, yet they do not discuss how
these shapes can be consistently achieved through manipulation planning. Our
work presents a manipulation strategy for stable and consistent shape
transitions. We find that the configuration space of such a chain is
homeomorphic to a three-dimensional cube. Using this property, we suggest a
strategy to manipulate the chain into different configurations, specifically
from one rotation mode to another, while taking stability and feasibility into
consideration. We demonstrate the effectiveness of our strategy in physical
experiments by successfully transitioning from rest to the first two rotation
modes. The concepts explored in our work has critical applications in ensuring
safety and efficiency of drill string and yarn spinning operations.

</details>


### [34] [Robots and Children that Learn Together : Improving Knowledge Retention by Teaching Peer-Like Interactive Robots](https://arxiv.org/abs/2506.18365)
*Imene Tarakli,Samuele Vinanzi,Richard Moore,Alessandro Di Nuovo*

Main category: cs.RO

TL;DR: 研究探索了在真实教室中使用自主社交机器人实现“教中学”（LbT）的可行性，通过交互式强化学习（RL）模型，发现学生通过教机器人显著提高了学习效果，尤其是语法任务和低基础学生。


<details>
  <summary>Details</summary>
Motivation: 尽管对“教中学”模式兴趣增长，但少有研究在真实教室中探索自主社交机器人的应用，现有研究多依赖脚本或Wizard-of-Oz方法，限制了实时交互学习的理解。

Method: 采用交互式强化学习（RL）作为可教社交机器人的认知模型，进行两项实验，58名小学生分别教机器人或独立练习法语词汇和语法。

Result: LbT组学生在语法任务中表现显著优于自主练习组，低基础学生受益最大；行为数据显示学生调整教学策略并更深入参与推理任务。

Conclusion: 交互式RL是教育机器人有效且可扩展的模型，首次证明在真实教室中同时部署多个自主机器人的可行性，机器人可作为适应性伙伴提升元认知参与和长期学习效果。

Abstract: Despite growing interest in Learning-by-Teaching (LbT), few studies have
explored how this paradigm can be implemented with autonomous, peer-like social
robots in real classrooms. Most prior work has relied on scripted or
Wizard-of-Oz behaviors, limiting our understanding of how real-time,
interactive learning can be supported by artificial agents. This study
addresses this gap by introducing Interactive Reinforcement Learning (RL) as a
cognitive model for teachable social robots. We conducted two between-subject
experiments with 58 primary school children, who either taught a robot or
practiced independently on a tablet while learning French vocabulary
(memorization) and grammatical rules (inference). The robot, powered by
Interactive RL, learned from the child's evaluative feedback. Children in the
LbT condition achieved significantly higher retention gains compared to those
in the self-practice condition, especially on the grammar task. Learners with
lower prior knowledge benefited most from teaching the robot. Behavioural
metrics revealed that children adapted their teaching strategies over time and
engaged more deeply during inference tasks. This work makes two contributions:
(1) it introduces Interactive RL as a pedagogically effective and scalable
model for peer-robot learning, and (2) it demonstrates, for the first time, the
feasibility of deploying multiple autonomous robots simultaneously in real
classrooms. These findings extend theoretical understanding of LbT by showing
that social robots can function not only as passive tutees but as adaptive
partners that enhance meta-cognitive engagement and long-term learning
outcomes.

</details>


### [35] [Integrating Maneuverable Planning and Adaptive Control for Robot Cart-Pushing under Disturbances](https://arxiv.org/abs/2506.18410)
*Zhe Zhang,Peijia Xie,Zhirui Sun,Bingyi Xia,Bi-Ke Zhu,Jiankun Wang*

Main category: cs.RO

TL;DR: 提出了一种用于移动机器人精确灵活推车的规划和控制框架，通过局部坐标表示和新型运动学模型优化运动规划，并采用抗干扰控制方法提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 推车任务中的运动约束、机器人冗余以及动态负载和干扰导致复杂的运动规划和动力学问题，需要灵活且鲁棒的解决方案。

Method: 采用局部坐标表示和新型运动学模型进行非线性优化规划，结合抗干扰控制方法处理复杂控制问题。

Result: 通过仿真和实际实验验证了方法的优越性，首次系统评估了推车方法的灵活性和鲁棒性。

Conclusion: 提出的框架在推车任务中表现出更高的灵活性和鲁棒性，优于现有方法。

Abstract: Precise and flexible cart-pushing is a challenging task for mobile robots.
The motion constraints during cart-pushing and the robot's redundancy lead to
complex motion planning problems, while variable payloads and disturbances
present complicated dynamics. In this work, we propose a novel planning and
control framework for flexible whole-body coordination and robust adaptive
control. Our motion planning method employs a local coordinate representation
and a novel kinematic model to solve a nonlinear optimization problem, thereby
enhancing motion maneuverability by generating feasible and flexible push
poses. Furthermore, we present a disturbance rejection control method to resist
disturbances and reduce control errors for the complex control problem without
requiring an accurate dynamic model. We validate our method through extensive
experiments in simulation and real-world settings, demonstrating its
superiority over existing approaches. To the best of our knowledge, this is the
first work to systematically evaluate the flexibility and robustness of
cart-pushing methods in experiments. The video supplement is available at
https://sites.google.com/view/mpac-pushing/.

</details>


### [36] [Radar and Event Camera Fusion for Agile Robot Ego-Motion Estimation](https://arxiv.org/abs/2506.18443)
*Yang Lyu,Zhenghao Zou,Yanfeng Li,Chunhui Zhao,Quan Pan*

Main category: cs.RO

TL;DR: 提出了一种基于事件相机和毫米波雷达的无IMU和无特征关联的机器人运动速度估计框架，适用于高动态场景。


<details>
  <summary>Details</summary>
Motivation: 高动态机器人运动时，传统传感器因测量模糊、失真和延迟难以实现可靠的运动估计。

Method: 结合事件相机和毫米波雷达，直接利用原始事件和多普勒测量推导速度，后端采用连续时间状态空间模型进行融合。

Result: 在自采集数据集上验证，框架在挑战性环境中实现了可靠且高效的速度输出。

Conclusion: 该框架为高动态场景下的机器人运动估计提供了一种无IMU和无特征关联的解决方案。

Abstract: Achieving reliable ego motion estimation for agile robots, e.g., aerobatic
aircraft, remains challenging because most robot sensors fail to respond timely
and clearly to highly dynamic robot motions, often resulting in measurement
blurring, distortion, and delays. In this paper, we propose an IMU-free and
feature-association-free framework to achieve aggressive ego-motion velocity
estimation of a robot platform in highly dynamic scenarios by combining two
types of exteroceptive sensors, an event camera and a millimeter wave radar,
First, we used instantaneous raw events and Doppler measurements to derive
rotational and translational velocities directly. Without a sophisticated
association process between measurement frames, the proposed method is more
robust in texture-less and structureless environments and is more
computationally efficient for edge computing devices. Then, in the back-end, we
propose a continuous-time state-space model to fuse the hybrid time-based and
event-based measurements to estimate the ego-motion velocity in a fixed-lagged
smoother fashion. In the end, we validate our velometer framework extensively
in self-collected experiment datasets. The results indicate that our IMU-free
and association-free ego motion estimation framework can achieve reliable and
efficient velocity output in challenging environments. The source code,
illustrative video and dataset are available at
https://github.com/ZzhYgwh/TwistEstimator.

</details>


### [37] [GraspMAS: Zero-Shot Language-driven Grasp Detection with Multi-Agent System](https://arxiv.org/abs/2506.18448)
*Quang Nguyen,Tri Le,Huy Nguyen,Thieu Vo,Tung D. Ta,Baoru Huang,Minh N. Vu,Anh Nguyen*

Main category: cs.RO

TL;DR: GraspMAS是一种多智能体系统框架，用于语言驱动的抓取检测，通过三个智能体（Planner、Coder、Observer）解决复杂指令解释和密集环境操作问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有语言驱动抓取检测方法难以处理复杂指令和密集环境，且需额外训练适应新领域，限制了实际应用。

Method: 提出GraspMAS框架，包含Planner（策略规划）、Coder（代码生成与执行）、Observer（结果评估与反馈）三个智能体。

Result: 在两个大规模数据集上显著优于基线方法，仿真和真实机器人实验验证了其有效性。

Conclusion: GraspMAS通过多智能体协作解决了语言驱动抓取检测的关键挑战，具有实际应用潜力。

Abstract: Language-driven grasp detection has the potential to revolutionize
human-robot interaction by allowing robots to understand and execute grasping
tasks based on natural language commands. However, existing approaches face two
key challenges. First, they often struggle to interpret complex text
instructions or operate ineffectively in densely cluttered environments.
Second, most methods require a training or finetuning step to adapt to new
domains, limiting their generation in real-world applications. In this paper,
we introduce GraspMAS, a new multi-agent system framework for language-driven
grasp detection. GraspMAS is designed to reason through ambiguities and improve
decision-making in real-world scenarios. Our framework consists of three
specialized agents: Planner, responsible for strategizing complex queries;
Coder, which generates and executes source code; and Observer, which evaluates
the outcomes and provides feedback. Intensive experiments on two large-scale
datasets demonstrate that our GraspMAS significantly outperforms existing
baselines. Additionally, robot experiments conducted in both simulation and
real-world settings further validate the effectiveness of our approach.

</details>


### [38] [A Motivational Architecture for Open-Ended Learning Challenges in Robots](https://arxiv.org/abs/2506.18454)
*Alejandro Romero,Gianluca Baldassarre,Richard J. Duro,Vieri Giuliano Santucci*

Main category: cs.RO

TL;DR: H-GRAIL是一种分层架构，通过内在动机和互联学习机制，自主发现目标、学习技能并适应动态环境。


<details>
  <summary>Details</summary>
Motivation: 开发能在动态环境中自主交互的智能体，解决开放学习中的目标生成、技能获取和环境适应问题。

Method: 提出H-GRAIL分层架构，结合多种内在动机和互联学习机制。

Result: 在真实机器人场景中验证了H-GRAIL的有效性。

Conclusion: H-GRAIL为开放学习中的多挑战提供了集成解决方案。

Abstract: Developing agents capable of autonomously interacting with complex and
dynamic environments, where task structures may change over time and prior
knowledge cannot be relied upon, is a key prerequisite for deploying artificial
systems in real-world settings. The open-ended learning framework identifies
the core challenges for creating such agents, including the ability to
autonomously generate new goals, acquire the necessary skills (or curricula of
skills) to achieve them, and adapt to non-stationary environments. While many
existing works tackles various aspects of these challenges in isolation, few
propose integrated solutions that address them simultaneously. In this paper,
we introduce H-GRAIL, a hierarchical architecture that, through the use of
different typologies of intrinsic motivations and interconnected learning
mechanisms, autonomously discovers new goals, learns the required skills for
their achievement, generates skill sequences for tackling interdependent tasks,
and adapts to non-stationary environments. We tested H-GRAIL in a real robotic
scenario, demonstrating how the proposed solutions effectively address the
various challenges of open-ended learning.

</details>


### [39] [Mirror Eyes: Explainable Human-Robot Interaction at a Glance](https://arxiv.org/abs/2506.18466)
*Matti Krüger,Daniel Tanneberg,Chao Wang,Stephan Hasler,Michael Gienger*

Main category: cs.RO

TL;DR: 研究探讨了机器人通过模拟人类凝视行为（在眼睛上显示注视区域的镜像）如何提升人机协作体验。实验表明，这种方法能增强用户对机器人信息处理的感知，并提高任务执行中的错误检测速度。


<details>
  <summary>Details</summary>
Motivation: 探索机器人模拟人类凝视行为是否能提升人机协作的直观性和效率。

Method: 开发了一个带屏幕眼睛模型的机器人头部系统，可显示注视区域的镜像。通过用户研究（33人）评估其效果。

Result: 用户在使用带镜像眼睛的机器人时，对机器人信息处理的感知更强，错误检测更快，用户体验评分更高。

Conclusion: 模拟人类凝视行为的机器人眼睛设计在人机协作中具有直观且有益的潜力。

Abstract: The gaze of a person tends to reflect their interest. This work explores what
happens when this statement is taken literally and applied to robots. Here we
present a robot system that employs a moving robot head with a screen-based eye
model that can direct the robot's gaze to points in physical space and present
a reflection-like mirror image of the attended region on top of each eye. We
conducted a user study with 33 participants, who were asked to instruct the
robot to perform pick-and-place tasks, monitor the robot's task execution, and
interrupt it in case of erroneous actions. Despite a deliberate lack of
instructions about the role of the eyes and a very brief system exposure,
participants felt more aware about the robot's information processing, detected
erroneous actions earlier, and rated the user experience higher when eye-based
mirroring was enabled compared to non-reflective eyes. These results suggest a
beneficial and intuitive utilization of the introduced method in cooperative
human-robot interaction.

</details>


### [40] [Design, fabrication and control of a cable-driven parallel robot](https://arxiv.org/abs/2506.18526)
*Dhruv Sorathiya,Sarthak Sahoo,Vivek Natarajan*

Main category: cs.RO

TL;DR: 本文介绍了电缆驱动并联机器人（CDPRs）的实验装置设计与验证，展示了其复杂动力学现象（如电缆振动）的复现能力，并计划用于未来高级控制算法的验证。


<details>
  <summary>Details</summary>
Motivation: CDPRs因其高灵活性和低功耗潜力巨大，但其复杂动力学特性需要先进的建模和控制方法，并通过实验验证。

Method: 设计并搭建了一个三电缆CDPR实验装置，验证了基本的开环运动规划算法。

Result: 实验装置成功复现了大型CDPR中的电缆横向振动等复杂现象。

Conclusion: 该实验装置为未来复杂动力学建模、控制算法验证及高级运动规划研究提供了基础。

Abstract: In cable driven parallel robots (CDPRs), the payload is suspended using a
network of cables whose length can be controlled to maneuver the payload within
the workspace. Compared to rigid link robots, CDPRs provide better
maneuverability due to the flexibility of the cables and consume lesser power
due to the high strength-to-weight ratio of the cables. However, amongst other
things, the flexibility of the cables and the fact that they can only pull (and
not push) render the dynamics of CDPRs complex. Hence advanced modelling
paradigms and control algorithms must be developed to fully utilize the
potential of CDPRs. Furthermore, given the complex dynamics of CDPRs, the
models and control algorithms proposed for them must be validated on
experimental setups to ascertain their efficacy in practice. We have recently
developed an elaborate experimental setup for a CDPR with three cables and
validated elementary open-loop motion planning algorithms on it. In this paper,
we describe several aspects of the design and fabrication of our setup,
including component selection and assembly, and present our experimental
results. Our setup can reproduce complex phenomenon such as the transverse
vibration of the cables seen in large CDPRs and will in the future be used to
model and control such phenomenon and also to validate more sophisticated
motion planning algorithms.

</details>


### [41] [Learning Point Correspondences In Radar 3D Point Clouds For Radar-Inertial Odometry](https://arxiv.org/abs/2506.18580)
*Jan Michalczyk,Stephan Weiss,Jan Steinbrener*

Main category: cs.RO

TL;DR: 提出了一种基于Transformer的学习框架，用于在低质量3D点云中预测鲁棒的点对应关系，显著提升了雷达惯性里程计的精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在点云质量下降时表现不佳，特别是在低成本雷达传感器产生的稀疏、噪声数据中。

Method: 使用Transformer架构和自监督学习，通过多标签分类交叉熵损失训练网络，避免手动标注。

Result: 在真实无人机飞行和公开数据集上测试，位置估计精度分别提高了14%和19%。

Conclusion: 该方法有效解决了低质量点云中的对应关系问题，为低成本雷达传感器提供了实用解决方案。

Abstract: Using 3D point clouds in odometry estimation in robotics often requires
finding a set of correspondences between points in subsequent scans. While
there are established methods for point clouds of sufficient quality,
state-of-the-art still struggles when this quality drops. Thus, this paper
presents a novel learning-based framework for predicting robust point
correspondences between pairs of noisy, sparse and unstructured 3D point clouds
from a light-weight, low-power, inexpensive, consumer-grade System-on-Chip
(SoC) Frequency Modulated Continuous Wave (FMCW) radar sensor. Our network is
based on the transformer architecture which allows leveraging the attention
mechanism to discover pairs of points in consecutive scans with the greatest
mutual affinity. The proposed network is trained in a self-supervised way using
set-based multi-label classification cross-entropy loss, where the ground-truth
set of matches is found by solving the Linear Sum Assignment (LSA) optimization
problem, which avoids tedious hand annotation of the training data.
Additionally, posing the loss calculation as multi-label classification permits
supervising on point correspondences directly instead of on odometry error,
which is not feasible for sparse and noisy data from the SoC radar we use. We
evaluate our method with an open-source state-of-the-art Radar-Inertial
Odometry (RIO) framework in real-world Unmanned Aerial Vehicle (UAV) flights
and with the widely used public Coloradar dataset. Evaluation shows that the
proposed method improves the position estimation accuracy by over 14 % and 19 %
on average, respectively. The open source code and datasets can be found here:
https://github.com/aau-cns/radar_transformer.

</details>


### [42] [PG-LIO: Photometric-Geometric fusion for Robust LiDAR-Inertial Odometry](https://arxiv.org/abs/2506.18583)
*Nikhil Khedekar,Kostas Alexis*

Main category: cs.RO

TL;DR: PG-LIO是一种融合LiDAR的光度和几何信息以及IMU惯性约束的实时LIO方法，显著提高了在几何结构缺失情况下的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统LIO方法在几何结构缺失时容易失效，限制了其在更广泛场景中的应用。

Method: PG-LIO通过将多模态信息（光度、几何和IMU）集成到滑动窗口因子图中进行实时优化。

Result: 在几何结构良好和自相似场景中，PG-LIO表现优异，尤其在退化情况下显著优于现有方法。

Conclusion: PG-LIO在几何结构缺失情况下具有更高的鲁棒性和准确性，适用于更广泛的自主机器人应用。

Abstract: LiDAR-Inertial Odometry (LIO) is widely used for accurate state estimation
and mapping which is an essential requirement for autonomous robots.
Conventional LIO methods typically rely on formulating constraints from the
geometric structure sampled by the LiDAR. Hence, in the lack of geometric
structure, these tend to become ill-conditioned (degenerate) and fail.
Robustness of LIO to such conditions is a necessity for its broader deployment.
To address this, we propose PG-LIO, a real-time LIO method that fuses
photometric and geometric information sampled by the LiDAR along with inertial
constraints from an Inertial Measurement Unit (IMU). This multi-modal
information is integrated into a factor graph optimized over a sliding window
for real-time operation. We evaluate PG-LIO on multiple datasets that include
both geometrically well-conditioned as well as self-similar scenarios. Our
method achieves accuracy on par with state-of-the-art LIO in geometrically
well-structured settings while significantly improving accuracy in degenerate
cases including against methods that also fuse intensity. Notably, we
demonstrate only 1 m drift over a 1 km manually piloted aerial trajectory
through a geometrically self-similar tunnel at an average speed of 7.5m/s (max
speed 10.8 m/s). For the benefit of the community, we shall also release our
source code https://github.com/ntnu-arl/mimosa

</details>


### [43] [NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed Target Tracking in Unstructured GPS-Denied Environments](https://arxiv.org/abs/2506.18689)
*Alessandro Saviolo,Giuseppe Loianno*

Main category: cs.RO

TL;DR: NOVA是一个基于立体相机和IMU的自主空中目标跟踪框架，无需全局地图或绝对定位，实现了高速、鲁棒的目标跟踪和避障。


<details>
  <summary>Details</summary>
Motivation: 解决在无结构和GPS缺失环境中自主空中目标跟踪的挑战，避免依赖运动捕捉系统或预建地图。

Method: 结合轻量级目标检测、立体深度补全、直方图滤波、视觉惯性状态估计和非线性模型预测控制（NMPC），实现目标参考框架下的感知、估计和控制。

Result: 在复杂场景（如城市迷宫、森林小径）中实现超过50 km/h的高速目标跟踪，表现出鲁棒性和可靠性。

Conclusion: NOVA证明了仅依靠机载传感器即可实现高速视觉目标跟踪，无需外部定位或环境假设。

Abstract: Autonomous aerial target tracking in unstructured and GPS-denied environments
remains a fundamental challenge in robotics. Many existing methods rely on
motion capture systems, pre-mapped scenes, or feature-based localization to
ensure safety and control, limiting their deployment in real-world conditions.
We introduce NOVA, a fully onboard, object-centric framework that enables
robust target tracking and collision-aware navigation using only a stereo
camera and an IMU. Rather than constructing a global map or relying on absolute
localization, NOVA formulates perception, estimation, and control entirely in
the target's reference frame. A tightly integrated stack combines a lightweight
object detector with stereo depth completion, followed by histogram-based
filtering to infer robust target distances under occlusion and noise. These
measurements feed a visual-inertial state estimator that recovers the full
6-DoF pose of the robot relative to the target. A nonlinear model predictive
controller (NMPC) plans dynamically feasible trajectories in the target frame.
To ensure safety, high-order control barrier functions are constructed online
from a compact set of high-risk collision points extracted from depth, enabling
real-time obstacle avoidance without maps or dense representations. We validate
NOVA across challenging real-world scenarios, including urban mazes, forest
trails, and repeated transitions through buildings with intermittent GPS loss
and severe lighting changes that disrupt feature-based localization. Each
experiment is repeated multiple times under similar conditions to assess
resilience, showing consistent and reliable performance. NOVA achieves agile
target following at speeds exceeding 50 km/h. These results show that
high-speed vision-based tracking is possible in the wild using only onboard
sensing, with no reliance on external localization or environment assumptions.

</details>


### [44] [Safety-Aware Optimal Scheduling for Autonomous Masonry Construction using Collaborative Heterogeneous Aerial Robots](https://arxiv.org/abs/2506.18697)
*Marios-Nektarios Stamatopoulos,Shridhar Velhal,Avijit Banerjee,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 提出了一种用于自主砌体施工的高层任务规划和优化协调框架，利用异构空中机器人团队解决砖块放置和砂浆应用的调度与协调挑战。


<details>
  <summary>Details</summary>
Motivation: 解决砂浆固化期限和无人机并行操作的安全约束带来的调度与协调新挑战。

Method: 自动化管道生成墙体施工计划，优化无人机任务分配和执行时间，结合动态耦合的优先期限约束和静态结构依赖约束。

Result: 通过Gazebo模拟验证，框架能够优化无人机操作，确保施工过程中的结构完整性和安全性。

Conclusion: 该框架有效实现了协调且高效的空中砌体施工，满足了结构粘合和安全操作的需求。

Abstract: This paper presents a novel high-level task planning and optimal coordination
framework for autonomous masonry construction, using a team of heterogeneous
aerial robotic workers, consisting of agents with separate skills for brick
placement and mortar application. This introduces new challenges in scheduling
and coordination, particularly due to the mortar curing deadline required for
structural bonding and ensuring the safety constraints among UAVs operating in
parallel. To address this, an automated pipeline generates the wall
construction plan based on the available bricks while identifying static
structural dependencies and potential conflicts for safe operation. The
proposed framework optimizes UAV task allocation and execution timing by
incorporating dynamically coupled precedence deadline constraints that account
for the curing process and static structural dependency constraints, while
enforcing spatio-temporal constraints to prevent collisions and ensure safety.
The primary objective of the scheduler is to minimize the overall construction
makespan while minimizing logistics, traveling time between tasks, and the
curing time to maintain both adhesion quality and safe workspace separation.
The effectiveness of the proposed method in achieving coordinated and
time-efficient aerial masonry construction is extensively validated through
Gazebo simulated missions. The results demonstrate the framework's capability
to streamline UAV operations, ensuring both structural integrity and safety
during the construction process.

</details>


### [45] [TDACloud: Point Cloud Recognition Using Topological Data Analysis](https://arxiv.org/abs/2506.18725)
*Anirban Ghosh,Ian Dahlin,Ayan Dutta*

Main category: cs.RO

TL;DR: 提出了一种名为TDACloud的新方法，利用拓扑数据分析（TDA）从点云中提取局部描述符，无需GPU密集型训练，并在噪声和变换条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 点云识别在自动驾驶等应用中具有重要意义，但现有方法在噪声和变换条件下表现不佳，且依赖资源密集型训练。

Method: 使用ATOL向量化方法生成点云的固定大小TDA描述符向量，直接处理原始点云。

Result: 在真实和模拟数据集上测试，TDACloud在噪声和变换条件下识别准确率高，比基线方法提升约14%。

Conclusion: TDACloud是一种高效且鲁棒的点云识别方法，适用于复杂场景。

Abstract: Point cloud-based object/place recognition remains a problem of interest in
applications such as autonomous driving, scene reconstruction, and
localization. Extracting meaningful local descriptors from a query point cloud
that can be matched with the descriptors of the collected point clouds is a
challenging problem. Furthermore, when the query point cloud is noisy or has
been transformed (e.g., rotated), it adds to the complexity. To this end, we
propose a novel methodology, named TDACloud, using Topological Data Analysis
(TDA) for local descriptor extraction from a point cloud, which does not need
resource-intensive GPU-based machine learning training. More specifically, we
used the ATOL vectorization method to generate vectors for point clouds. Unlike
voxelization, our proposed technique can take raw point clouds as inputs and
outputs a fixed-size TDA-descriptor vector. To test the quality of the proposed
TDACloud technique, we have implemented it on multiple real-world (e.g., Oxford
RobotCar, KITTI-360) and realistic (e.g., ShapeNet) point cloud datasets for
object and place recognition. We have also tested TDACloud on noisy and
transformed test cases where the query point cloud has been scaled, translated,
or rotated. Our results demonstrate high recognition accuracies in noisy
conditions and large-scale real-world place recognition while outperforming the
baselines by up to approximately 14%.

</details>


### [46] [DefFusionNet: Learning Multimodal Goal Shapes for Deformable Object Manipulation via a Diffusion-based Probabilistic Model](https://arxiv.org/abs/2506.18779)
*Bao Thach,Siyeon Kim,Britton Jordan,Mohanraj Shanthi,Tanner Watts,Shing-Hei Ho,James M. Ferguson,Tucker Hermans,Alan Kuntz*

Main category: cs.RO

TL;DR: DefFusionNet提出了一种基于扩散概率模型的神经网络，用于生成多样化的可变形物体目标形状，解决了DefGoalNet在多模态场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如DefGoalNet）在多模态场景中无法生成多样化的目标形状，导致实用性受限。

Method: 利用扩散概率模型学习所有有效目标形状的分布，而非单一确定性结果。

Result: 在仿真和实际机器人任务中验证了方法的有效性，能够生成多样化的目标形状。

Conclusion: DefFusionNet是首个能够为实际机器人应用生成多样化、多模态目标形状的生成模型。

Abstract: Deformable object manipulation is critical to many real-world robotic
applications, ranging from surgical robotics and soft material handling in
manufacturing to household tasks like laundry folding. At the core of this
important robotic field is shape servoing, a task focused on controlling
deformable objects into desired shapes. The shape servoing formulation requires
the specification of a goal shape. However, most prior works in shape servoing
rely on impractical goal shape acquisition methods, such as laborious
domain-knowledge engineering or manual manipulation. DefGoalNet previously
posed the current state-of-the-art solution to this problem, which learns
deformable object goal shapes directly from a small number of human
demonstrations. However, it significantly struggles in multi-modal settings,
where multiple distinct goal shapes can all lead to successful task completion.
As a deterministic model, DefGoalNet collapses these possibilities into a
single averaged solution, often resulting in an unusable goal. In this paper,
we address this problem by developing DefFusionNet, a novel neural network that
leverages the diffusion probabilistic model to learn a distribution over all
valid goal shapes rather than predicting a single deterministic outcome. This
enables the generation of diverse goal shapes and avoids the averaging
artifacts. We demonstrate our method's effectiveness on robotic tasks inspired
by both manufacturing and surgical applications, both in simulation and on a
physical robot. Our work is the first generative model capable of producing a
diverse, multi-modal set of deformable object goals for real-world robotic
applications.

</details>


### [47] [Learning Physical Systems: Symplectification via Gauge Fixing in Dirac Structures](https://arxiv.org/abs/2506.18812)
*Aristotelis Papatheodorou,Pranav Vaidhyanathan,Natalia Ares,Ioannis Havoutis*

Main category: cs.RO

TL;DR: 提出了Presymplectification Networks (PSNs)，通过Dirac结构学习辛提升，解决含耗散和约束系统中辛形式退化的问题，结合SympNet实现约束轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 在含耗散和约束的系统中，传统辛形式退化，导致稳定性与长期预测能力下降。

Method: 引入PSNs框架，通过Dirac结构学习辛提升，结合SympNet预测约束轨迹。

Result: 在ANYmal四足机器人动力学中验证了方法的有效性。

Conclusion: PSNs填补了约束耗散系统与辛学习之间的空白，为几何机器学习模型开辟了新方向。

Abstract: Physics-informed deep learning has achieved remarkable progress by embedding
geometric priors, such as Hamiltonian symmetries and variational principles,
into neural networks, enabling structure-preserving models that extrapolate
with high accuracy. However, in systems with dissipation and holonomic
constraints, ubiquitous in legged locomotion and multibody robotics, the
canonical symplectic form becomes degenerate, undermining the very invariants
that guarantee stability and long-term prediction. In this work, we tackle this
foundational limitation by introducing Presymplectification Networks (PSNs),
the first framework to learn the symplectification lift via Dirac structures,
restoring a non-degenerate symplectic geometry by embedding constrained systems
into a higher-dimensional manifold. Our architecture combines a recurrent
encoder with a flow-matching objective to learn the augmented phase-space
dynamics end-to-end. We then attach a lightweight Symplectic Network (SympNet)
to forecast constrained trajectories while preserving energy, momentum, and
constraint satisfaction. We demonstrate our method on the dynamics of the
ANYmal quadruped robot, a challenging contact-rich, multibody system. To the
best of our knowledge, this is the first framework that effectively bridges the
gap between constrained, dissipative mechanical systems and symplectic
learning, unlocking a whole new class of geometric machine learning models,
grounded in first principles yet adaptable from data.

</details>


### [48] [SViP: Sequencing Bimanual Visuomotor Policies with Object-Centric Motion Primitives](https://arxiv.org/abs/2506.18825)
*Yizhou Chen,Hang Xu,Dongjie Yu,Zeqing Zhang,Yi Ren,Jia Pan*

Main category: cs.RO

TL;DR: SViP框架通过结合视觉运动策略与任务和运动规划（TAMP），提升了小样本模仿学习在复杂双手操作任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决视觉运动策略在小样本数据下泛化能力不足及长时任务中误差累积的问题。

Method: SViP利用语义场景图分割演示，训练切换条件生成器，结合TAMP生成参数化脚本原语。

Result: 仅需20个真实演示，SViP在未见初始条件和任务中表现优异，优于现有生成模仿学习方法。

Conclusion: SViP为复杂任务提供了更广泛的应用潜力，尤其在双手操作领域。

Abstract: Imitation learning (IL), particularly when leveraging high-dimensional visual
inputs for policy training, has proven intuitive and effective in complex
bimanual manipulation tasks. Nonetheless, the generalization capability of
visuomotor policies remains limited, especially when small demonstration
datasets are available. Accumulated errors in visuomotor policies significantly
hinder their ability to complete long-horizon tasks. To address these
limitations, we propose SViP, a framework that seamlessly integrates visuomotor
policies into task and motion planning (TAMP). SViP partitions human
demonstrations into bimanual and unimanual operations using a semantic scene
graph monitor. Continuous decision variables from the key scene graph are
employed to train a switching condition generator. This generator produces
parameterized scripted primitives that ensure reliable performance even when
encountering out-of-the-distribution observations. Using only 20 real-world
demonstrations, we show that SViP enables visuomotor policies to generalize
across out-of-distribution initial conditions without requiring object pose
estimators. For previously unseen tasks, SViP automatically discovers effective
solutions to achieve the goal, leveraging constraint modeling in TAMP
formulism. In real-world experiments, SViP outperforms state-of-the-art
generative IL methods, indicating wider applicability for more complex tasks.
Project website: https://sites.google.com/view/svip-bimanual

</details>


### [49] [Reproducible Evaluation of Camera Auto-Exposure Methods in the Field: Platform, Benchmark and Lessons Learned](https://arxiv.org/abs/2506.18844)
*Olivier Gamache,Jean-Michel Fortin,Matěj Boxan,François Pomerleau,Philippe Giguère*

Main category: cs.RO

TL;DR: 提出了一种利用模拟器生成不同曝光时间图像的方法，基于BorealHDR数据集，评估了八种自动曝光方法，发现传统方法性能最佳。


<details>
  <summary>Details</summary>
Motivation: 标准数据集因输入传感器固定，难以比较动态调整传感器参数的方法，尤其是自动曝光（AE）方法。现有在线评估方式不可复现。

Method: 利用BorealHDR多曝光立体数据集及其扩展，通过模拟器生成不同曝光时间的图像，离线评估AE方法。

Result: 模拟图像与真实图像的RMSE低于1.78%，传统AE方法表现最佳。

Conclusion: 离线方法提高了实验复现性，传统AE方法仍最优。数据集和代码公开，支持进一步研究。

Abstract: Standard datasets often present limitations, particularly due to the fixed
nature of input data sensors, which makes it difficult to compare methods that
actively adjust sensor parameters to suit environmental conditions. This is the
case with Automatic-Exposure (AE) methods, which rely on environmental factors
to influence the image acquisition process. As a result, AE methods have
traditionally been benchmarked in an online manner, rendering experiments
non-reproducible. Building on our prior work, we propose a methodology that
utilizes an emulator capable of generating images at any exposure time. This
approach leverages BorealHDR, a unique multi-exposure stereo dataset, along
with its new extension, in which data was acquired along a repeated trajectory
at different times of the day to assess the impact of changing illumination. In
total, BorealHDR covers 13.4 km over 59 trajectories in challenging lighting
conditions. The dataset also includes lidar-inertial-odometry-based maps with
pose estimation for each image frame, as well as Global Navigation Satellite
System (GNSS) data for comparison. We demonstrate that by using images acquired
at various exposure times, we can emulate realistic images with a
Root-Mean-Square Error (RMSE) below 1.78% compared to ground truth images.
Using this offline approach, we benchmarked eight AE methods, concluding that
the classical AE method remains the field's best performer. To further support
reproducibility, we provide in-depth details on the development of our backpack
acquisition platform, including hardware, electrical components, and
performance specifications. Additionally, we share valuable lessons learned
from deploying the backpack over more than 25 km across various environments.
Our code and dataset are available online at this link:
https://github.com/norlab-ulaval/TFR24 BorealHDR

</details>


### [50] [GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM](https://arxiv.org/abs/2506.18885)
*Annika Thomas,Aneesa Sonawalla,Alex Rose,Jonathan P. How*

Main category: cs.RO

TL;DR: GRAND-SLAM是一种多智能体协作的3D高斯泼溅SLAM方法，适用于大规模户外环境，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅SLAM方法局限于小规模室内环境，无法满足大规模多智能体户外场景的需求。

Method: GRAND-SLAM结合了基于子图的隐式跟踪模块和多机器人闭环检测方法，并集成到位姿图优化框架中。

Result: 在Replica室内数据集上，GRAND-SLAM的跟踪性能最优，PSNR提升28%；在Kimera-Multi户外数据集上，多智能体跟踪误差降低91%，渲染效果更优。

Conclusion: GRAND-SLAM为大规模多智能体户外环境提供了一种高效且性能优越的SLAM解决方案。

Abstract: 3D Gaussian splatting has emerged as an expressive scene representation for
RGB-D visual SLAM, but its application to large-scale, multi-agent outdoor
environments remains unexplored. Multi-agent Gaussian SLAM is a promising
approach to rapid exploration and reconstruction of environments, offering
scalable environment representations, but existing approaches are limited to
small-scale, indoor environments. To that end, we propose Gaussian
Reconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative
Gaussian splatting SLAM method that integrates i) an implicit tracking module
based on local optimization over submaps and ii) an approach to inter- and
intra-robot loop closure integrated into a pose-graph optimization framework.
Experiments show that GRAND-SLAM provides state-of-the-art tracking performance
and 28% higher PSNR than existing methods on the Replica indoor dataset, as
well as 91% lower multi-agent tracking error and improved rendering over
existing multi-agent methods on the large-scale, outdoor Kimera-Multi dataset.

</details>


### [51] [MinD: Unified Visual Imagination and Control via Hierarchical World Models](https://arxiv.org/abs/2506.18897)
*Xiaowei Chi,Kuangzhi Ge,Jiaming Liu,Siyuan Zhou,Peidong Jia,Zichen He,Yuzhen Liu,Tingguang Li,Lei Han,Sirui Han,Shanghang Zhang,Yike Guo*

Main category: cs.RO

TL;DR: MinD提出了一种分层扩散框架，通过双系统设计和DiffMatcher模块，解决了视频生成模型在机器人中的实时性和一致性挑战，实现了低延迟闭环控制和世界模拟功能。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型（VGMs）在机器人统一世界建模中潜力巨大，但受限于生成速度慢和视频与动作一致性差的问题。

Method: MinD采用分层扩散框架，结合低频VGMs和高频扩散策略，引入DiffMatcher模块及协同训练策略，优化视频与动作的协调。

Result: MinD在RL-Bench中实现了63%+的先进操作性能，并展示了任务可行性预评估和风险缓解能力。

Conclusion: MinD通过创新设计提升了VGMs的实用性，推动了机器人统一世界建模的发展。

Abstract: Video generation models (VGMs) offer a promising pathway for unified world
modeling in robotics by integrating simulation, prediction, and manipulation.
However, their practical application remains limited due to (1) slowgeneration
speed, which limits real-time interaction, and (2) poor consistency between
imagined videos and executable actions. To address these challenges, we propose
Manipulate in Dream (MinD), a hierarchical diffusion-based world model
framework that employs a dual-system design for vision-language manipulation.
MinD executes VGM at low frequencies to extract video prediction features,
while leveraging a high-frequency diffusion policy for real-time interaction.
This architecture enables low-latency, closed-loop control in manipulation with
coherent visual guidance. To better coordinate the two systems, we introduce a
video-action diffusion matching module (DiffMatcher), with a novel co-training
strategy that uses separate schedulers for each diffusion model. Specifically,
we introduce a diffusion-forcing mechanism to DiffMatcher that aligns their
intermediate representations during training, helping the fast action model
better understand video-based predictions. Beyond manipulation, MinD also
functions as a world simulator, reliably predicting task success or failure in
latent space before execution. Trustworthy analysis further shows that VGMs can
preemptively evaluate task feasibility and mitigate risks. Extensive
experiments across multiple benchmarks demonstrate that MinD achieves
state-of-the-art manipulation (63%+) in RL-Bench, advancing the frontier of
unified world modeling in robotics.

</details>
