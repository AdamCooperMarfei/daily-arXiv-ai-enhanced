{"id": "2506.20801", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20801", "abs": "https://arxiv.org/abs/2506.20801", "authors": ["Francesco Tassi", "Jianzhuang Zhao", "Gustavo J. G. Lahr", "Luna Gava", "Marco Monforte", "Arren Glover", "Chiara Bartolozzi", "Arash Ajoudani"], "title": "IMA-Catcher: An IMpact-Aware Nonprehensile Catching Framework based on Combined Optimization and Learning", "comment": "25 pages, 17 figures, accepted by International Journal of Robotics\n  Research (IJRR)", "summary": "Robotic catching of flying objects typically generates high impact forces\nthat might lead to task failure and potential hardware damages. This is\naccentuated when the object mass to robot payload ratio increases, given the\nstrong inertial components characterizing this task. This paper aims to address\nthis problem by proposing an implicitly impact-aware framework that\naccomplishes the catching task in both pre- and post-catching phases. In the\nfirst phase, a motion planner generates optimal trajectories that minimize\ncatching forces, while in the second, the object's energy is dissipated\nsmoothly, minimizing bouncing. In particular, in the pre-catching phase, a\nreal-time optimal planner is responsible for generating trajectories of the\nend-effector that minimize the velocity difference between the robot and the\nobject to reduce impact forces during catching. In the post-catching phase, the\nrobot's position, velocity, and stiffness trajectories are generated based on\nhuman demonstrations when catching a series of free-falling objects with\nunknown masses. A hierarchical quadratic programming-based controller is used\nto enforce the robot's constraints (i.e., joint and torque limits) and create a\nstack of tasks that minimizes the reflected mass at the end-effector as a\nsecondary objective. The initial experiments isolate the problem along one\ndimension to accurately study the effects of each contribution on the metrics\nproposed. We show how the same task, without velocity matching, would be\ninfeasible due to excessive joint torques resulting from the impact. The\naddition of reflected mass minimization is then investigated, and the catching\nheight is increased to evaluate the method's robustness. Finally, the setup is\nextended to catching along multiple Cartesian axes, to prove its generalization\nin space.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u5f0f\u51b2\u51fb\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u5728\u6355\u83b7\u98de\u884c\u7269\u4f53\u65f6\u51cf\u5c11\u51b2\u51fb\u529b\u548c\u53cd\u5f39\uff0c\u5206\u4e3a\u9884\u6355\u83b7\u548c\u540e\u6355\u83b7\u4e24\u4e2a\u9636\u6bb5\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u6355\u83b7\u98de\u884c\u7269\u4f53\u65f6\u56e0\u9ad8\u51b2\u51fb\u529b\u5bfc\u81f4\u7684\u4efb\u52a1\u5931\u8d25\u548c\u786c\u4ef6\u635f\u574f\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5f53\u7269\u4f53\u8d28\u91cf\u4e0e\u673a\u5668\u4eba\u8d1f\u8f7d\u6bd4\u589e\u52a0\u65f6\u3002", "method": "\u9884\u6355\u83b7\u9636\u6bb5\u901a\u8fc7\u5b9e\u65f6\u6700\u4f18\u89c4\u5212\u5668\u751f\u6210\u8f68\u8ff9\u4ee5\u51cf\u5c11\u51b2\u51fb\u529b\uff1b\u540e\u6355\u83b7\u9636\u6bb5\u57fa\u4e8e\u4eba\u7c7b\u6f14\u793a\u751f\u6210\u8f68\u8ff9\u4ee5\u5e73\u6ed1\u6d88\u6563\u7269\u4f53\u80fd\u91cf\u3002\u4f7f\u7528\u5206\u5c42\u4e8c\u6b21\u89c4\u5212\u63a7\u5236\u5668\u6ee1\u8db3\u7ea6\u675f\u5e76\u6700\u5c0f\u5316\u53cd\u5c04\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65e0\u901f\u5ea6\u5339\u914d\u7684\u4efb\u52a1\u56e0\u51b2\u51fb\u529b\u8fc7\u5927\u800c\u4e0d\u53ef\u884c\uff1b\u53cd\u5c04\u8d28\u91cf\u6700\u5c0f\u5316\u589e\u52a0\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\uff1b\u591a\u8f74\u6355\u83b7\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u51cf\u5c11\u4e86\u6355\u83b7\u8fc7\u7a0b\u4e2d\u7684\u51b2\u51fb\u529b\u548c\u53cd\u5f39\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u53ef\u884c\u6027\u548c\u786c\u4ef6\u5b89\u5168\u6027\u3002"}}
{"id": "2506.20804", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20804", "abs": "https://arxiv.org/abs/2506.20804", "authors": ["Ritvik Agarwal", "Behnoushsadat Hatami", "Alvika Gautam", "Parikshit Maini"], "title": "Online Planning for Cooperative Air-Ground Robot Systems with Unknown Fuel Requirements", "comment": "Submitted to RSS (MRS Workshop)", "summary": "We consider an online variant of the fuel-constrained UAV routing problem\nwith a ground-based mobile refueling station (FCURP-MRS), where targets incur\nunknown fuel costs. We develop a two-phase solution: an offline heuristic-based\nplanner computes initial UAV and UGV paths, and a novel online planning\nalgorithm that dynamically adjusts rendezvous points based on real-time fuel\nconsumption during target processing. Preliminary Gazebo simulations\ndemonstrate the feasibility of our approach in maintaining UAV-UGV path\nvalidity, ensuring mission completion. Link to video:\nhttps://youtu.be/EmpVj-fjqNY", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebf\u71c3\u6599\u53d7\u9650\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u7ed3\u5408\u5730\u9762\u79fb\u52a8\u52a0\u6cb9\u7ad9\u7684\u52a8\u6001\u8c03\u6574\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u672a\u77e5\u71c3\u6599\u6d88\u8017\u60c5\u51b5\u4e0b\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u786e\u4fdd\u4efb\u52a1\u5b8c\u6210\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u89e3\u51b3\u65b9\u6848\uff1a\u79bb\u7ebf\u542f\u53d1\u5f0f\u89c4\u5212\u521d\u59cb\u8def\u5f84\uff0c\u5728\u7ebf\u52a8\u6001\u8c03\u6574\u4f1a\u5408\u70b9\u3002", "result": "Gazebo\u6a21\u62df\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u786e\u4fdd\u65e0\u4eba\u673a\u4e0e\u5730\u9762\u8f66\u8f86\u8def\u5f84\u6709\u6548\u6027\u3002", "conclusion": "\u65b9\u6cd5\u80fd\u6709\u6548\u5e94\u5bf9\u672a\u77e5\u71c3\u6599\u6d88\u8017\uff0c\u4fdd\u969c\u4efb\u52a1\u987a\u5229\u5b8c\u6210\u3002"}}
{"id": "2506.20812", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20812", "abs": "https://arxiv.org/abs/2506.20812", "authors": ["Alexandre Girard", "Steven A. Parkison", "Philippe Hamelin"], "title": "Model-Based Real-Time Pose and Sag Estimation of Overhead Power Lines Using LiDAR for Drone Inspection", "comment": "Submitted to IEEE case 2025", "summary": "Drones can inspect overhead power lines while they remain energized,\nsignificantly simplifying the inspection process. However, localizing a drone\nrelative to all conductors using an onboard LiDAR sensor presents several\nchallenges: (1) conductors provide minimal surface for LiDAR beams limiting the\nnumber of conductor points in a scan, (2) not all conductors are consistently\ndetected, and (3) distinguishing LiDAR points corresponding to conductors from\nother objects, such as trees and pylons, is difficult. This paper proposes an\nestimation approach that minimizes the error between LiDAR measurements and a\nsingle geometric model representing the entire conductor array, rather than\ntracking individual conductors separately. Experimental results, using data\nfrom a power line drone inspection, demonstrate that this method achieves\naccurate tracking, with a solver converging under 50 ms per frame, even in the\npresence of partial observations, noise, and outliers. A sensitivity analysis\nshows that the estimation approach can tolerate up to twice as many outlier\npoints as valid conductors measurements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLiDAR\u7684\u65e0\u4eba\u673a\u7535\u529b\u7ebf\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316LiDAR\u6d4b\u91cf\u503c\u4e0e\u6574\u4f53\u51e0\u4f55\u6a21\u578b\u7684\u8bef\u5dee\uff0c\u89e3\u51b3\u4e86\u5bfc\u4f53\u70b9\u5c11\u3001\u68c0\u6d4b\u4e0d\u4e00\u81f4\u548c\u5e72\u6270\u7269\u533a\u5206\u7684\u95ee\u9898\u3002", "motivation": "\u65e0\u4eba\u673a\u68c0\u6d4b\u7535\u529b\u7ebf\u65f6\uff0cLiDAR\u4f20\u611f\u5668\u9762\u4e34\u5bfc\u4f53\u70b9\u5c11\u3001\u68c0\u6d4b\u4e0d\u4e00\u81f4\u548c\u5e72\u6270\u7269\u533a\u5206\u56f0\u96be\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4f30\u8ba1\u65b9\u6cd5\uff0c\u6700\u5c0f\u5316LiDAR\u6d4b\u91cf\u503c\u4e0e\u6574\u4f53\u5bfc\u4f53\u51e0\u4f55\u6a21\u578b\u7684\u8bef\u5dee\uff0c\u800c\u975e\u5355\u72ec\u8ddf\u8e2a\u6bcf\u6839\u5bfc\u4f53\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u90e8\u5206\u89c2\u6d4b\u3001\u566a\u58f0\u548c\u5f02\u5e38\u503c\u60c5\u51b5\u4e0b\u4ecd\u80fd\u51c6\u786e\u8ddf\u8e2a\uff0c\u6c42\u89e3\u5668\u6bcf\u5e27\u6536\u655b\u65f6\u95f4\u5c0f\u4e8e50\u6beb\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5bf9\u5f02\u5e38\u70b9\u7684\u5bb9\u5fcd\u5ea6\u662f\u6709\u6548\u5bfc\u4f53\u6d4b\u91cf\u70b9\u7684\u4e24\u500d\uff0c\u5177\u6709\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.20954", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20954", "abs": "https://arxiv.org/abs/2506.20954", "authors": ["Xueming Liu", "Lin Li", "Xiang Zhou", "Qingrui Zhang", "Tianjiang Hu"], "title": "Cooperative Circumnavigation for Multi-Quadrotor Systems via Onboard Sensing", "comment": "8 Pages, 7 figures. Accepted by RA-L", "summary": "A cooperative circumnavigation framework is proposed for multi-quadrotor\nsystems to enclose and track a moving target without reliance on external\nlocalization systems. The distinct relationships between quadrotor-quadrotor\nand quadrotor-target interactions are evaluated using a heterogeneous\nperception strategy and corresponding state estimation algorithms. A modified\nKalman filter is developed to fuse visual-inertial odometry with range\nmeasurements to enhance the accuracy of inter-quadrotor relative localization.\nAn event-triggered distributed Kalman filter is designed to achieve robust\ntarget state estimation under visual occlusion by incorporating neighbor\nmeasurements and estimated inter-quadrotor relative positions. Using the\nestimation results, a cooperative circumnavigation controller is constructed,\nleveraging an oscillator-based autonomous formation flight strategy. We conduct\nextensive indoor and outdoor experiments to validate the efficiency of the\nproposed circumnavigation framework in occluded environments. Furthermore, a\nquadrotor failure experiment highlights the inherent fault tolerance property\nof the proposed framework, underscoring its potential for deployment in\nsearch-and-rescue operations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u56db\u65cb\u7ffc\u7cfb\u7edf\u7684\u534f\u4f5c\u73af\u7ed5\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4e0d\u4f9d\u8d56\u5916\u90e8\u5b9a\u4f4d\u7cfb\u7edf\u7684\u60c5\u51b5\u4e0b\u5305\u56f4\u548c\u8ddf\u8e2a\u79fb\u52a8\u76ee\u6807\u3002", "motivation": "\u89e3\u51b3\u591a\u56db\u65cb\u7ffc\u7cfb\u7edf\u5728\u89c6\u89c9\u906e\u6321\u73af\u5883\u4e0b\u5bf9\u79fb\u52a8\u76ee\u6807\u7684\u534f\u4f5c\u8ddf\u8e2a\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u5b9a\u4f4d\u548c\u72b6\u6001\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u5f02\u6784\u611f\u77e5\u7b56\u7565\u548c\u72b6\u6001\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5f00\u53d1\u6539\u8fdb\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u878d\u5408\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u548c\u8ddd\u79bb\u6d4b\u91cf\uff0c\u8bbe\u8ba1\u4e8b\u4ef6\u89e6\u53d1\u7684\u5206\u5e03\u5f0f\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5b9e\u73b0\u76ee\u6807\u72b6\u6001\u4f30\u8ba1\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5ba4\u5185\u5916\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u906e\u6321\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u56fa\u6709\u7684\u5bb9\u9519\u7279\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u641c\u7d22\u548c\u6551\u63f4\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u5728\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2506.20966", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20966", "abs": "https://arxiv.org/abs/2506.20966", "authors": ["Tian-Yu Xiang", "Ao-Qun Jin", "Xiao-Hu Zhou", "Mei-Jiang Gui", "Xiao-Liang Xie", "Shi-Qi Liu", "Shuang-Yi Wang", "Sheng-Bin Duan", "Fu-Chao Xie", "Wen-Kai Wang", "Si-Cheng Wang", "Ling-Yun Li", "Tian Tu", "Zeng-Guang Hou"], "title": "Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends", "comment": null, "summary": "Vision-language-action (VLA) models extend vision-language models (VLM) by\nintegrating action generation modules for robotic manipulation. Leveraging\nstrengths of VLM in vision perception and instruction understanding, VLA models\nexhibit promising generalization across diverse manipulation tasks. However,\napplications demanding high precision and accuracy reveal performance gaps\nwithout further adaptation. Evidence from multiple domains highlights the\ncritical role of post-training to align foundational models with downstream\napplications, spurring extensive research on post-training VLA models. VLA\nmodel post-training aims to address the challenge of improving an embodiment's\nability to interact with the environment for the given tasks, analogous to the\nprocess of humans motor skills acquisition. Accordingly, this paper reviews\npost-training strategies for VLA models through the lens of human motor\nlearning, focusing on three dimensions: environments, embodiments, and tasks. A\nstructured taxonomy is introduced aligned with human learning mechanisms: (1)\nenhancing environmental perception, (2) improving embodiment awareness, (3)\ndeepening task comprehension, and (4) multi-component integration. Finally, key\nchallenges and trends in post-training VLA models are identified, establishing\na conceptual framework to guide future research. This work delivers both a\ncomprehensive overview of current VLA model post-training methods from a human\nmotor learning perspective and practical insights for VLA model development.\n(Project website: https://github.com/AoqunJin/Awesome-VLA-Post-Training)", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u4ece\u4eba\u7c7b\u8fd0\u52a8\u5b66\u4e60\u7684\u89d2\u5ea6\u63d0\u51fa\u4e86\u56db\u4e2a\u7ef4\u5ea6\u7684\u5206\u7c7b\uff0c\u5e76\u603b\u7ed3\u4e86\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u9ad8\u7cbe\u5ea6\u9700\u6c42\u4efb\u52a1\u4e2d\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff0c\u9700\u901a\u8fc7\u540e\u8bad\u7ec3\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "method": "\u901a\u8fc7\u4eba\u7c7b\u8fd0\u52a8\u5b66\u4e60\u7684\u89c6\u89d2\uff0c\u63d0\u51fa\u73af\u5883\u3001\u4f53\u73b0\u548c\u4efb\u52a1\u4e09\u4e2a\u7ef4\u5ea6\u7684\u540e\u8bad\u7ec3\u7b56\u7565\u5206\u7c7b\uff0c\u5305\u62ec\u73af\u5883\u611f\u77e5\u589e\u5f3a\u3001\u4f53\u73b0\u610f\u8bc6\u63d0\u5347\u3001\u4efb\u52a1\u7406\u89e3\u6df1\u5316\u548c\u591a\u7ec4\u4ef6\u6574\u5408\u3002", "result": "\u5efa\u7acb\u4e86VLA\u6a21\u578b\u540e\u8bad\u7ec3\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u603b\u7ed3\u4e86\u5f53\u524d\u65b9\u6cd5\u548c\u672a\u6765\u8d8b\u52bf\u3002", "conclusion": "\u672c\u6587\u4e3aVLA\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u7efc\u8ff0\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.20969", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20969", "abs": "https://arxiv.org/abs/2506.20969", "authors": ["Shruti Bansal", "Wenshan Wang", "Yifei Liu", "Parv Maheshwari"], "title": "ThermalDiffusion: Visual-to-Thermal Image-to-Image Translation for Autonomous Navigation", "comment": "Accepted at Thermal Infrared in Robotics (TIRO) Workshop, ICRA 2025", "summary": "Autonomous systems rely on sensors to estimate the environment around them.\nHowever, cameras, LiDARs, and RADARs have their own limitations. In nighttime\nor degraded environments such as fog, mist, or dust, thermal cameras can\nprovide valuable information regarding the presence of objects of interest due\nto their heat signature. They make it easy to identify humans and vehicles that\nare usually at higher temperatures compared to their surroundings. In this\npaper, we focus on the adaptation of thermal cameras for robotics and\nautomation, where the biggest hurdle is the lack of data. Several multi-modal\ndatasets are available for driving robotics research in tasks such as scene\nsegmentation, object detection, and depth estimation, which are the cornerstone\nof autonomous systems. However, they are found to be lacking in thermal\nimagery. Our paper proposes a solution to augment these datasets with synthetic\nthermal data to enable widespread and rapid adaptation of thermal cameras. We\nexplore the use of conditional diffusion models to convert existing RGB images\nto thermal images using self-attention to learn the thermal properties of\nreal-world objects.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5c06RGB\u56fe\u50cf\u8f6c\u6362\u4e3a\u70ed\u6210\u50cf\u56fe\u50cf\uff0c\u4ee5\u89e3\u51b3\u70ed\u6210\u50cf\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4fc3\u8fdb\u70ed\u50cf\u4eea\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "motivation": "\u70ed\u50cf\u4eea\u5728\u591c\u95f4\u6216\u6076\u52a3\u73af\u5883\u4e0b\u80fd\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u4fe1\u606f\uff0c\u4f46\u7f3a\u4e4f\u76f8\u5173\u6570\u636e\u9650\u5236\u4e86\u5176\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u771f\u5b9e\u7269\u4f53\u7684\u70ed\u7279\u6027\uff0c\u5c06RGB\u56fe\u50cf\u8f6c\u6362\u4e3a\u70ed\u6210\u50cf\u56fe\u50cf\u3002", "result": "\u901a\u8fc7\u5408\u6210\u70ed\u6210\u50cf\u6570\u636e\uff0c\u6269\u5145\u4e86\u73b0\u6709\u6570\u636e\u96c6\uff0c\u652f\u6301\u70ed\u50cf\u4eea\u5728\u573a\u666f\u5206\u5272\u3001\u76ee\u6807\u68c0\u6d4b\u548c\u6df1\u5ea6\u4f30\u8ba1\u7b49\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u70ed\u50cf\u4eea\u7684\u5feb\u901f\u9002\u5e94\u548c\u5e7f\u6cdb\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21016", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.21016", "abs": "https://arxiv.org/abs/2506.21016", "authors": ["B. Chidambaram", "A. Hilbert", "M. Silva"], "title": "Fault-Tolerant Spacecraft Attitude Determination using State Estimation Techniques", "comment": "8 pages, 19 figures", "summary": "The extended and unscented Kalman filter, and the particle filter provide a\nrobust framework for fault-tolerant attitude estimation on spacecraft. This\npaper explores how each filter performs for a large satellite in a low earth\norbit. Additionally, various techniques, built on these filters, for fault\ndetection, isolation and recovery from erroneous sensor measurements, are\nanalyzed. Key results from this analysis include filter performance for various\nfault modes.", "AI": {"tldr": "\u6bd4\u8f83\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u3001\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u7c92\u5b50\u6ee4\u6ce2\u5728\u4f4e\u5730\u7403\u8f68\u9053\u5927\u578b\u536b\u661f\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u5206\u6790\u57fa\u4e8e\u8fd9\u4e9b\u6ee4\u6ce2\u5668\u7684\u6545\u969c\u68c0\u6d4b\u3001\u9694\u79bb\u548c\u6062\u590d\u6280\u672f\u3002", "motivation": "\u7814\u7a76\u4e0d\u540c\u6ee4\u6ce2\u65b9\u6cd5\u5728\u5927\u578b\u536b\u661f\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u53ca\u5982\u4f55\u5e94\u5bf9\u4f20\u611f\u5668\u6d4b\u91cf\u4e2d\u7684\u6545\u969c\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u3001\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u7c92\u5b50\u6ee4\u6ce2\u8fdb\u884c\u59ff\u6001\u4f30\u8ba1\uff0c\u5e76\u5206\u6790\u57fa\u4e8e\u8fd9\u4e9b\u6ee4\u6ce2\u5668\u7684\u6545\u969c\u68c0\u6d4b\u3001\u9694\u79bb\u548c\u6062\u590d\u6280\u672f\u3002", "result": "\u63d0\u4f9b\u4e86\u4e0d\u540c\u6ee4\u6ce2\u65b9\u6cd5\u5728\u591a\u79cd\u6545\u969c\u6a21\u5f0f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u3001\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u7c92\u5b50\u6ee4\u6ce2\u4e3a\u536b\u661f\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u9c81\u68d2\u6846\u67b6\uff0c\u6545\u969c\u68c0\u6d4b\u548c\u6062\u590d\u6280\u672f\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2506.21030", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21030", "abs": "https://arxiv.org/abs/2506.21030", "authors": ["Zhou Tianxing", "Wang Zhirui", "Ao Haojia", "Chen Guangyan", "Xing Boyang", "Cheng Jingwen", "Yang Yi", "Yue Yufeng"], "title": "STEP Planner: Constructing cross-hierarchical subgoal tree as an embodied long-horizon task planner", "comment": null, "summary": "The ability to perform reliable long-horizon task planning is crucial for\ndeploying robots in real-world environments. However, directly employing Large\nLanguage Models (LLMs) as action sequence generators often results in low\nsuccess rates due to their limited reasoning ability for long-horizon embodied\ntasks. In the STEP framework, we construct a subgoal tree through a pair of\nclosed-loop models: a subgoal decomposition model and a leaf node termination\nmodel. Within this framework, we develop a hierarchical tree structure that\nspans from coarse to fine resolutions. The subgoal decomposition model\nleverages a foundation LLM to break down complex goals into manageable\nsubgoals, thereby spanning the subgoal tree. The leaf node termination model\nprovides real-time feedback based on environmental states, determining when to\nterminate the tree spanning and ensuring each leaf node can be directly\nconverted into a primitive action. Experiments conducted in both the\nVirtualHome WAH-NL benchmark and on real robots demonstrate that STEP achieves\nlong-horizon embodied task completion with success rates up to 34% (WAH-NL) and\n25% (real robot) outperforming SOTA methods.", "AI": {"tldr": "STEP\u6846\u67b6\u901a\u8fc7\u5b50\u76ee\u6807\u5206\u89e3\u6a21\u578b\u548c\u53f6\u8282\u70b9\u7ec8\u6b62\u6a21\u578b\u6784\u5efa\u5b50\u76ee\u6807\u6811\uff0c\u63d0\u5347\u957f\u65f6\u7a0b\u4efb\u52a1\u89c4\u5212\u7684\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u89c4\u5212\u4e2d\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u5bfc\u81f4\u4f4e\u6210\u529f\u7387\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6811\u7ed3\u6784\uff0c\u5229\u7528\u57fa\u7840LLM\u5206\u89e3\u590d\u6742\u76ee\u6807\u4e3a\u5b50\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u5b9e\u65f6\u53cd\u9988\u7ec8\u6b62\u6811\u6269\u5c55\u3002", "result": "\u5728VirtualHome WAH-NL\u57fa\u51c6\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0c\u6210\u529f\u7387\u5206\u522b\u8fbe\u523034%\u548c25%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "STEP\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u957f\u65f6\u7a0b\u4efb\u52a1\u89c4\u5212\u7684\u6210\u529f\u7387\uff0c\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.21041", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21041", "abs": "https://arxiv.org/abs/2506.21041", "authors": ["Junwei You", "Pei Li", "Zhuoyu Jiang", "Zilin Huang", "Rui Gan", "Haotian Shi", "Bin Ran"], "title": "V2X-REALM: Vision-Language Model-Based Robust End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling", "comment": null, "summary": "Ensuring robust planning and decision-making under rare, diverse, and\nvisually degraded long-tail scenarios remains a fundamental challenge for\nautonomous driving in urban environments. This issue becomes more critical in\ncooperative settings, where vehicles and infrastructure jointly perceive and\nreason across complex environments. To address this challenge, we propose\nV2X-REALM, a vision-language model (VLM)-based framework with adaptive\nmultimodal learning for robust cooperative autonomous driving under long-tail\nscenarios. V2X-REALM introduces three core innovations: (i) a prompt-driven\nlong-tail scenario generation and evaluation pipeline that leverages foundation\nmodels to synthesize realistic long-tail conditions such as snow and fog across\nvehicle- and infrastructure-side views, enriching training diversity\nefficiently; (ii) a gated multi-scenario adaptive attention module that\nmodulates the visual stream using scenario priors to recalibrate ambiguous or\ncorrupted features; and (iii) a multi-task scenario-aware contrastive learning\nobjective that improves multimodal alignment and promotes cross-scenario\nfeature separability. Extensive experiments demonstrate that V2X-REALM\nsignificantly outperforms existing baselines in robustness, semantic reasoning,\nsafety, and planning accuracy under complex, challenging driving conditions,\nadvancing the scalability of end-to-end cooperative autonomous driving.", "AI": {"tldr": "V2X-REALM\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u5728\u957f\u5c3e\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5b66\u4e60\u548c\u521b\u65b0\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u5728\u7f55\u89c1\u3001\u591a\u6837\u4e14\u89c6\u89c9\u9000\u5316\u7684\u957f\u5c3e\u573a\u666f\u4e2d\u7684\u89c4\u5212\u548c\u51b3\u7b56\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u534f\u540c\u73af\u5883\u4e2d\u3002", "method": "\u63d0\u51faV2X-REALM\u6846\u67b6\uff0c\u5305\u62ec\u957f\u5c3e\u573a\u666f\u751f\u6210\u4e0e\u8bc4\u4f30\u3001\u95e8\u63a7\u591a\u573a\u666f\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u6a21\u5757\u548c\u591a\u4efb\u52a1\u573a\u666f\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cV2X-REALM\u5728\u9c81\u68d2\u6027\u3001\u8bed\u4e49\u63a8\u7406\u3001\u5b89\u5168\u6027\u548c\u89c4\u5212\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "V2X-REALM\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u534f\u540c\u81ea\u52a8\u9a7e\u9a76\u7684\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u6311\u6218\u6027\u9a7e\u9a76\u6761\u4ef6\u3002"}}
{"id": "2506.21057", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21057", "abs": "https://arxiv.org/abs/2506.21057", "authors": ["Zhuochen Miao", "Jun Lv", "Hongjie Fang", "Yang Jin", "Cewu Lu"], "title": "Knowledge-Driven Imitation Learning: Enabling Generalization Across Diverse Conditions", "comment": "IROS 2025", "summary": "Imitation learning has emerged as a powerful paradigm in robot manipulation,\nyet its generalization capability remains constrained by object-specific\ndependencies in limited expert demonstrations. To address this challenge, we\npropose knowledge-driven imitation learning, a framework that leverages\nexternal structural semantic knowledge to abstract object representations\nwithin the same category. We introduce a novel semantic keypoint graph as a\nknowledge template and develop a coarse-to-fine template-matching algorithm\nthat optimizes both structural consistency and semantic similarity. Evaluated\non three real-world robotic manipulation tasks, our method achieves superior\nperformance, surpassing image-based diffusion policies with only one-quarter of\nthe expert demonstrations. Extensive experiments further demonstrate its\nrobustness across novel objects, backgrounds, and lighting conditions. This\nwork pioneers a knowledge-driven approach to data-efficient robotic learning in\nreal-world settings. Code and more materials are available on\nhttps://knowledge-driven.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u77e5\u8bc6\u9a71\u52a8\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u5916\u90e8\u8bed\u4e49\u77e5\u8bc6\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u5173\u952e\u70b9\u56fe\u548c\u6a21\u677f\u5339\u914d\u7b97\u6cd5\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u5f3a\u5927\uff0c\u4f46\u5176\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u4e8e\u4e13\u5bb6\u6f14\u793a\u4e2d\u7684\u5bf9\u8c61\u7279\u5b9a\u4f9d\u8d56\u3002", "method": "\u5f15\u5165\u8bed\u4e49\u5173\u952e\u70b9\u56fe\u4f5c\u4e3a\u77e5\u8bc6\u6a21\u677f\uff0c\u5f00\u53d1\u7c97\u5230\u7ec6\u7684\u6a21\u677f\u5339\u914d\u7b97\u6cd5\uff0c\u4f18\u5316\u7ed3\u6784\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u9700\u56db\u5206\u4e4b\u4e00\u7684\u4e13\u5bb6\u6f14\u793a\u5373\u8d85\u8d8a\u57fa\u4e8e\u56fe\u50cf\u7684\u6269\u6563\u7b56\u7565\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5f00\u521b\u4e86\u77e5\u8bc6\u9a71\u52a8\u7684\u6570\u636e\u9ad8\u6548\u673a\u5668\u4eba\u5b66\u4e60\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u573a\u666f\u3002"}}
{"id": "2506.21063", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21063", "abs": "https://arxiv.org/abs/2506.21063", "authors": ["Lin Hong", "Lu Liu", "Zhouhua Peng", "Fumin Zhang"], "title": "Control of Marine Robots in the Era of Data-Driven Intelligence", "comment": null, "summary": "The control of marine robots has long relied on model-based methods grounded\nin classical and modern control theory. However, the nonlinearity and\nuncertainties inherent in robot dynamics, coupled with the complexity of marine\nenvironments, have revealed the limitations of conventional control methods.\nThe rapid evolution of machine learning has opened new avenues for\nincorporating data-driven intelligence into control strategies, prompting a\nparadigm shift in the control of marine robots. This paper provides a review of\nrecent progress in marine robot control through the lens of this emerging\nparadigm. The review covers both individual and cooperative marine robotic\nsystems, highlighting notable achievements in data-driven control of marine\nrobots and summarizing open-source resources that support the development and\nvalidation of advanced control methods. Finally, several future perspectives\nare outlined to guide research toward achieving high-level autonomy for marine\nrobots in real-world applications. This paper aims to serve as a roadmap toward\nthe next-generation control framework of marine robots in the era of\ndata-driven intelligence.", "AI": {"tldr": "\u7efc\u8ff0\u4e86\u6570\u636e\u9a71\u52a8\u667a\u80fd\u5728\u6d77\u6d0b\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u603b\u7ed3\u4e86\u5f00\u6e90\u8d44\u6e90\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\u65b9\u6cd5\u5728\u975e\u7ebf\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u590d\u6742\u6d77\u6d0b\u73af\u5883\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u673a\u5668\u5b66\u4e60\u7684\u53d1\u5c55\u4e3a\u6d77\u6d0b\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u6790\u6570\u636e\u9a71\u52a8\u63a7\u5236\u5728\u4e2a\u4f53\u548c\u534f\u4f5c\u6d77\u6d0b\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u6210\u5c31\u548c\u5f00\u6e90\u8d44\u6e90\u3002", "result": "\u603b\u7ed3\u4e86\u6570\u636e\u9a71\u52a8\u63a7\u5236\u5728\u6d77\u6d0b\u673a\u5668\u4eba\u4e2d\u7684\u663e\u8457\u6210\u679c\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5173\u5f00\u6e90\u8d44\u6e90\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u81f4\u529b\u4e8e\u5b9e\u73b0\u6d77\u6d0b\u673a\u5668\u4eba\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9ad8\u7ea7\u81ea\u4e3b\u6027\uff0c\u63a8\u52a8\u6570\u636e\u9a71\u52a8\u667a\u80fd\u63a7\u5236\u6846\u67b6\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.21077", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21077", "abs": "https://arxiv.org/abs/2506.21077", "authors": ["Kaicheng Zhang", "Shida Xu", "Yining Ding", "Xianwen Kong", "Sen Wang"], "title": "CURL-SLAM: Continuous and Compact LiDAR Mapping", "comment": null, "summary": "This paper studies 3D LiDAR mapping with a focus on developing an updatable\nand localizable map representation that enables continuity, compactness and\nconsistency in 3D maps. Traditional LiDAR Simultaneous Localization and Mapping\n(SLAM) systems often rely on 3D point cloud maps, which typically require\nextensive storage to preserve structural details in large-scale environments.\nIn this paper, we propose a novel paradigm for LiDAR SLAM by leveraging the\nContinuous and Ultra-compact Representation of LiDAR (CURL) introduced in [1].\nOur proposed LiDAR mapping approach, CURL-SLAM, produces compact 3D maps\ncapable of continuous reconstruction at variable densities using CURL's\nspherical harmonics implicit encoding, and achieves global map consistency\nafter loop closure. Unlike popular Iterative Closest Point (ICP)-based LiDAR\nodometry techniques, CURL-SLAM formulates LiDAR pose estimation as a unique\noptimization problem tailored for CURL and extends it to local Bundle\nAdjustment (BA), enabling simultaneous pose refinement and map correction.\nExperimental results demonstrate that CURL-SLAM achieves state-of-the-art 3D\nmapping quality and competitive LiDAR trajectory accuracy, delivering\nsensor-rate real-time performance (10 Hz) on a CPU. We will release the\nCURL-SLAM implementation to the community.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCURL\u7684\u65b0\u578bLiDAR SLAM\u65b9\u6cd5CURL-SLAM\uff0c\u901a\u8fc7\u7403\u5f62\u8c10\u6ce2\u9690\u5f0f\u7f16\u7801\u5b9e\u73b0\u7d27\u51d1\u3001\u8fde\u7eed\u4e14\u4e00\u81f4\u76843D\u5730\u56fe\uff0c\u5e76\u5728CPU\u4e0a\u8fbe\u5230\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfLiDAR SLAM\u4f9d\u8d56\u70b9\u4e91\u5730\u56fe\uff0c\u5b58\u50a8\u9700\u6c42\u5927\u4e14\u96be\u4ee5\u4fdd\u6301\u4e00\u81f4\u6027\uff0cCURL-SLAM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5229\u7528CURL\u7684\u7403\u5f62\u8c10\u6ce2\u9690\u5f0f\u7f16\u7801\uff0c\u5c06LiDAR\u4f4d\u59ff\u4f30\u8ba1\u4f18\u5316\u95ee\u9898\u6269\u5c55\u4e3a\u5c40\u90e8Bundle Adjustment\uff0c\u5b9e\u73b0\u5730\u56fe\u4fee\u6b63\u4e0e\u4f4d\u59ff\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCURL-SLAM\u57283D\u5730\u56fe\u8d28\u91cf\u548c\u8f68\u8ff9\u7cbe\u5ea6\u4e0a\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\uff0cCPU\u4e0a\u5b9e\u73b010Hz\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "CURL-SLAM\u4e3aLiDAR SLAM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7d27\u51d1\u4e14\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u5b9e\u73b0\u3002"}}
{"id": "2506.21178", "categories": ["cs.RO", "68T40", "I.2.9; I.6.3"], "pdf": "https://arxiv.org/pdf/2506.21178", "abs": "https://arxiv.org/abs/2506.21178", "authors": ["Johnata Brayan", "Armando Alves Neto", "Pavel Petrovi\u010d", "Gustavo M Freitas", "Vinicius Mariano Gon\u00e7alves"], "title": "UAIbot: Beginner-friendly web-based simulator for interactive robotics learning and research", "comment": "12 pages, 8 figures, submitted to Springer proceedings", "summary": "This paper presents UAIbot, a free and open-source web-based robotics\nsimulator designed to address the educational and research challenges\nconventional simulation platforms generally face. The Python and JavaScript\ninterfaces of UAIbot enable accessible hands-on learning experiences without\ncumbersome installations. By allowing users to explore fundamental mathematical\nand physical principles interactively, ranging from manipulator kinematics to\npedestrian flow dynamics, UAIbot provides an effective tool for deepening\nstudent understanding, facilitating rapid experimentation, and enhancing\nresearch dissemination.", "AI": {"tldr": "UAIbot\u662f\u4e00\u4e2a\u514d\u8d39\u5f00\u6e90\u7684\u57fa\u4e8e\u7f51\u7edc\u7684\u673a\u5668\u4eba\u6a21\u62df\u5668\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u6a21\u62df\u5e73\u53f0\u5728\u6559\u80b2\u4e0e\u7814\u7a76\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6a21\u62df\u5e73\u53f0\u5728\u6559\u80b2\u4e0e\u7814\u7a76\u4e2d\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u66f4\u4fbf\u6377\u7684\u5b66\u4e60\u4e0e\u7814\u7a76\u5de5\u5177\u3002", "method": "\u901a\u8fc7Python\u548cJavaScript\u63a5\u53e3\u5b9e\u73b0\u65e0\u9700\u7e41\u7410\u5b89\u88c5\u7684\u4ea4\u4e92\u5f0f\u5b66\u4e60\u4f53\u9a8c\uff0c\u6db5\u76d6\u4ece\u673a\u68b0\u81c2\u8fd0\u52a8\u5b66\u5230\u884c\u4eba\u6d41\u52a8\u529b\u5b66\u7684\u57fa\u672c\u539f\u7406\u3002", "result": "UAIbot\u6210\u4e3a\u6df1\u5316\u5b66\u751f\u7406\u89e3\u3001\u4fc3\u8fdb\u5feb\u901f\u5b9e\u9a8c\u548c\u589e\u5f3a\u7814\u7a76\u4f20\u64ad\u7684\u6709\u6548\u5de5\u5177\u3002", "conclusion": "UAIbot\u4e3a\u6559\u80b2\u548c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u6613\u7528\u7684\u6a21\u62df\u5e73\u53f0\u3002"}}
{"id": "2506.21205", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21205", "abs": "https://arxiv.org/abs/2506.21205", "authors": ["Elia Trevisan", "Khaled A. Mustafa", "Godert Notten", "Xinwei Wang", "Javier Alonso-Mora"], "title": "Dynamic Risk-Aware MPPI for Mobile Robots in Crowds via Efficient Monte Carlo Approximations", "comment": "Accepted for presentation at IROS 2025. Submitted Version", "summary": "Deploying mobile robots safely among humans requires the motion planner to\naccount for the uncertainty in the other agents' predicted trajectories. This\nremains challenging in traditional approaches, especially with arbitrarily\nshaped predictions and real-time constraints. To address these challenges, we\npropose a Dynamic Risk-Aware Model Predictive Path Integral control (DRA-MPPI),\na motion planner that incorporates uncertain future motions modelled with\npotentially non-Gaussian stochastic predictions. By leveraging MPPI's\ngradient-free nature, we propose a method that efficiently approximates the\njoint Collision Probability (CP) among multiple dynamic obstacles for several\nhundred sampled trajectories in real-time via a Monte Carlo (MC) approach. This\nenables the rejection of samples exceeding a predefined CP threshold or the\nintegration of CP as a weighted objective within the navigation cost function.\nConsequently, DRA-MPPI mitigates the freezing robot problem while enhancing\nsafety. Real-world and simulated experiments with multiple dynamic obstacles\ndemonstrate DRA-MPPI's superior performance compared to state-of-the-art\napproaches, including Scenario-based Model Predictive Control (S-MPC), Frenet\nplanner, and vanilla MPPI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u98ce\u9669\u611f\u77e5\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206\u63a7\u5236\uff08DRA-MPPI\uff09\uff0c\u7528\u4e8e\u5b9e\u65f6\u5904\u7406\u52a8\u6001\u969c\u788d\u7269\u7684\u4e0d\u786e\u5b9a\u6027\u8f68\u8ff9\u89c4\u5212\uff0c\u907f\u514d\u673a\u5668\u4eba\u51bb\u7ed3\u95ee\u9898\u5e76\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u52a8\u6001\u969c\u788d\u7269\u7684\u975e\u9ad8\u65af\u9884\u6d4b\u8f68\u8ff9\u548c\u5b9e\u65f6\u7ea6\u675f\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u5229\u7528MPPI\u7684\u65e0\u68af\u5ea6\u7279\u6027\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u5b9e\u65f6\u8fd1\u4f3c\u591a\u52a8\u6001\u969c\u788d\u7269\u7684\u8054\u5408\u78b0\u649e\u6982\u7387\uff08CP\uff09\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u5bfc\u822a\u6210\u672c\u51fd\u6570\u7684\u52a0\u6743\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDRA-MPPI\u5728\u771f\u5b9e\u548c\u6a21\u62df\u73af\u5883\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982S-MPC\u3001Frenet\u89c4\u5212\u548c\u539f\u59cbMPPI\uff09\u3002", "conclusion": "DRA-MPPI\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u969c\u788d\u7269\u8f68\u8ff9\u89c4\u5212\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5b89\u5168\u6027\u548c\u5b9e\u65f6\u6027\u80fd\u3002"}}
{"id": "2506.21250", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21250", "abs": "https://arxiv.org/abs/2506.21250", "authors": ["Jing Bi", "Lianggong Bruce Wen", "Zhang Liu", "Chenliang Xu"], "title": "ACTLLM: Action Consistency Tuned Large Language Model", "comment": null, "summary": "This paper introduces ACTLLM (Action Consistency Tuned Large Language Model),\na novel approach for robot manipulation in dynamic environments. Traditional\nvision-based systems often struggle to learn visual representations that excel\nin both task execution and spatial reasoning, thereby limiting their\nadaptability in dynamic environments. ACTLLM addresses these challenges by\nharnessing language to craft structured scene descriptors, providing a uniform\ninterface for both spatial understanding and task performance through flexible\nlanguage instructions. Moreover, we introduce a novel action consistency\nconstraint that aligns visual perception with corresponding actions, thereby\nenhancing the learning of actionable visual representations. Additionally, we\nhave reformulated the Markov decision process for manipulation tasks into a\nmulti-turn visual dialogue framework. This approach enables the modeling of\nlong-term task execution with enhanced contextual relevance derived from the\nhistory of task execution. During our evaluation, ACTLLM excels in diverse\nscenarios, proving its effectiveness on challenging vision-based robot\nmanipulation tasks.", "AI": {"tldr": "ACTLLM\u662f\u4e00\u79cd\u901a\u8fc7\u8bed\u8a00\u6307\u4ee4\u589e\u5f3a\u673a\u5668\u4eba\u52a8\u6001\u73af\u5883\u64cd\u4f5c\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c6\u89c9\u611f\u77e5\u4e0e\u52a8\u4f5c\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u63d0\u5347\u4efb\u52a1\u6267\u884c\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u96be\u4ee5\u540c\u65f6\u4f18\u5316\u4efb\u52a1\u6267\u884c\u548c\u7a7a\u95f4\u63a8\u7406\uff0cACTLLM\u65e8\u5728\u901a\u8fc7\u8bed\u8a00\u63a5\u53e3\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u8bed\u8a00\u751f\u6210\u7ed3\u6784\u5316\u573a\u666f\u63cf\u8ff0\uff0c\u5f15\u5165\u52a8\u4f5c\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u5e76\u5c06\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u91cd\u6784\u4e3a\u591a\u8f6e\u89c6\u89c9\u5bf9\u8bdd\u6846\u67b6\u3002", "result": "ACTLLM\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "ACTLLM\u901a\u8fc7\u8bed\u8a00\u548c\u89c6\u89c9\u7684\u878d\u5408\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21265", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.21265", "abs": "https://arxiv.org/abs/2506.21265", "authors": ["Jelmer van der Saag", "Elia Trevisan", "Wouter Falkena", "Javier Alonso-Mora"], "title": "Active Disturbance Rejection Control for Trajectory Tracking of a Seagoing USV: Design, Simulation, and Field Experiments", "comment": "Accepted for presentation at IROS 2025. Submitted version", "summary": "Unmanned Surface Vessels (USVs) face significant control challenges due to\nuncertain environmental disturbances like waves and currents. This paper\nproposes a trajectory tracking controller based on Active Disturbance Rejection\nControl (ADRC) implemented on the DUS V2500. A custom simulation incorporating\nrealistic waves and current disturbances is developed to validate the\ncontroller's performance, supported by further validation through field tests\nin the harbour of Scheveningen, the Netherlands, and at sea. Simulation results\ndemonstrate that ADRC significantly reduces cross-track error across all tested\nconditions compared to a baseline PID controller but increases control effort\nand energy consumption. Field trials confirm these findings while revealing a\nfurther increase in energy consumption during sea trials compared to the\nbaseline.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eADRC\u7684\u8f68\u8ff9\u8ddf\u8e2a\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u89e3\u51b3USV\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u6270\u52a8\u4e0b\u7684\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u5730\u6d4b\u8bd5\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "USV\u5728\u6ce2\u6d6a\u548c\u6d0b\u6d41\u7b49\u73af\u5883\u6270\u52a8\u4e0b\u9762\u4e34\u63a7\u5236\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u63a7\u5236\u5668\u3002", "method": "\u91c7\u7528ADRC\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u5305\u542b\u771f\u5b9e\u6ce2\u6d6a\u548c\u6d0b\u6d41\u6270\u52a8\u7684\u4eff\u771f\u6a21\u578b\uff0c\u5e76\u5728\u8377\u5170Scheveningen\u6e2f\u548c\u6d77\u4e0a\u8fdb\u884c\u4e86\u5b9e\u5730\u6d4b\u8bd5\u3002", "result": "ADRC\u663e\u8457\u51cf\u5c11\u4e86\u8f68\u8ff9\u8ddf\u8e2a\u8bef\u5dee\uff0c\u4f46\u589e\u52a0\u4e86\u63a7\u5236\u52aa\u529b\u548c\u80fd\u8017\uff0c\u6d77\u4e0a\u8bd5\u9a8c\u4e2d\u80fd\u8017\u8fdb\u4e00\u6b65\u589e\u52a0\u3002", "conclusion": "ADRC\u5728\u8f68\u8ff9\u8ddf\u8e2a\u6027\u80fd\u4e0a\u4f18\u4e8ePID\uff0c\u4f46\u9700\u6743\u8861\u80fd\u8017\u95ee\u9898\u3002"}}
{"id": "2506.21347", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21347", "abs": "https://arxiv.org/abs/2506.21347", "authors": ["Edwina Lewis", "Aditya Parameshwaran", "Laura Redmond", "Yue Wang"], "title": "Real-time Terrain Analysis for Off-road Autonomous Vehicles", "comment": null, "summary": "This research addresses critical autonomous vehicle control challenges\narising from road roughness variation, which induces course deviations and\npotential loss of road contact during steering operations. We present a novel\nreal-time road roughness estimation system employing Bayesian calibration\nmethodology that processes axle accelerations to predict terrain roughness with\nquantifiable confidence measures. The technical framework integrates a Gaussian\nprocess surrogate model with a simulated half-vehicle model, systematically\nprocessing vehicle velocity and road surface roughness parameters to generate\ncorresponding axle acceleration responses. The Bayesian calibration routine\nperforms inverse estimation of road roughness from observed accelerations and\nvelocities, yielding posterior distributions that quantify prediction\nuncertainty for adaptive risk management. Training data generation utilizes\nLatin Hypercube sampling across comprehensive velocity and roughness parameter\nspaces, while the calibrated model integrates seamlessly with a Simplex\ncontroller architecture to dynamically adjust velocity limits based on\nreal-time roughness predictions. Experimental validation on stochastically\ngenerated surfaces featuring varying roughness regions demonstrates robust\nreal-time characterization capabilities, with the integrated Simplex control\nstrategy effectively enhancing autonomous vehicle operational safety through\nproactive surface condition response. This innovative Bayesian framework\nestablishes a comprehensive foundation for mitigating roughness-related\noperational risks while simultaneously improving efficiency and safety margins\nin autonomous vehicle systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u6821\u51c6\u7684\u5b9e\u65f6\u9053\u8def\u7c97\u7cd9\u5ea6\u4f30\u8ba1\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u56e0\u9053\u8def\u7c97\u7cd9\u5ea6\u53d8\u5316\u5bfc\u81f4\u7684\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u9053\u8def\u7c97\u7cd9\u5ea6\u7684\u53d8\u5316\u4f1a\u5bfc\u81f4\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u504f\u79bb\u8def\u7ebf\u6216\u5931\u53bb\u8def\u9762\u63a5\u89e6\uff0c\u5f71\u54cd\u63a7\u5236\u548c\u5b89\u5168\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5b9e\u65f6\u4f30\u8ba1\u7c97\u7cd9\u5ea6\u6765\u4f18\u5316\u8f66\u8f86\u63a7\u5236\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u6821\u51c6\u65b9\u6cd5\uff0c\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u4ee3\u7406\u6a21\u578b\u548c\u534a\u8f66\u8f86\u6a21\u578b\uff0c\u901a\u8fc7\u5904\u7406\u8f66\u8f86\u901f\u5ea6\u548c\u8def\u9762\u7c97\u7cd9\u5ea6\u53c2\u6570\u751f\u6210\u8f74\u52a0\u901f\u5ea6\u54cd\u5e94\uff0c\u5e76\u5229\u7528\u62c9\u4e01\u8d85\u7acb\u65b9\u91c7\u6837\u751f\u6210\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u5b9e\u65f6\u51c6\u786e\u9884\u6d4b\u9053\u8def\u7c97\u7cd9\u5ea6\uff0c\u5e76\u901a\u8fc7\u96c6\u6210\u7684Simplex\u63a7\u5236\u5668\u52a8\u6001\u8c03\u6574\u8f66\u901f\u9650\u5236\uff0c\u63d0\u5347\u8f66\u8f86\u64cd\u4f5c\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u8d1d\u53f6\u65af\u6846\u67b6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63d0\u4f9b\u4e86\u51cf\u5c11\u7c97\u7cd9\u5ea6\u76f8\u5173\u98ce\u9669\u7684\u57fa\u7840\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2506.21539", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21539", "abs": "https://arxiv.org/abs/2506.21539", "authors": ["Jun Cen", "Chaohui Yu", "Hangjie Yuan", "Yuming Jiang", "Siteng Huang", "Jiayan Guo", "Xin Li", "Yibing Song", "Hao Luo", "Fan Wang", "Deli Zhao", "Hao Chen"], "title": "WorldVLA: Towards Autoregressive Action World Model", "comment": "Code: https://github.com/alibaba-damo-academy/WorldVLA", "summary": "We present WorldVLA, an autoregressive action world model that unifies action\nand image understanding and generation. Our WorldVLA intergrates\nVision-Language-Action (VLA) model and world model in one single framework. The\nworld model predicts future images by leveraging both action and image\nunderstanding, with the purpose of learning the underlying physics of the\nenvironment to improve action generation. Meanwhile, the action model generates\nthe subsequent actions based on image observations, aiding in visual\nunderstanding and in turn helps visual generation of the world model. We\ndemonstrate that WorldVLA outperforms standalone action and world models,\nhighlighting the mutual enhancement between the world model and the action\nmodel. In addition, we find that the performance of the action model\ndeteriorates when generating sequences of actions in an autoregressive manner.\nThis phenomenon can be attributed to the model's limited generalization\ncapability for action prediction, leading to the propagation of errors from\nearlier actions to subsequent ones. To address this issue, we propose an\nattention mask strategy that selectively masks prior actions during the\ngeneration of the current action, which shows significant performance\nimprovement in the action chunk generation task.", "AI": {"tldr": "WorldVLA\u662f\u4e00\u4e2a\u81ea\u56de\u5f52\u52a8\u4f5c\u4e16\u754c\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u548c\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u4f5c\u548c\u56fe\u50cf\u7406\u89e3\u9884\u6d4b\u672a\u6765\u56fe\u50cf\uff0c\u540c\u65f6\u751f\u6210\u540e\u7eed\u52a8\u4f5c\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u72ec\u7acb\u6a21\u578b\uff0c\u4f46\u81ea\u56de\u5f52\u52a8\u4f5c\u751f\u6210\u6027\u80fd\u4e0b\u964d\uff0c\u63d0\u51fa\u6ce8\u610f\u529b\u63a9\u7801\u7b56\u7565\u4ee5\u6539\u5584\u3002", "motivation": "\u7edf\u4e00\u52a8\u4f5c\u548c\u56fe\u50cf\u7684\u7406\u89e3\u4e0e\u751f\u6210\uff0c\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u5b66\u4e60\u73af\u5883\u7269\u7406\u7279\u6027\u4ee5\u6539\u8fdb\u52a8\u4f5c\u751f\u6210\uff0c\u540c\u65f6\u5229\u7528\u52a8\u4f5c\u6a21\u578b\u589e\u5f3a\u89c6\u89c9\u7406\u89e3\u3002", "method": "\u6574\u5408VLA\u6a21\u578b\u548c\u4e16\u754c\u6a21\u578b\uff0c\u5229\u7528\u52a8\u4f5c\u548c\u56fe\u50cf\u7406\u89e3\u9884\u6d4b\u672a\u6765\u56fe\u50cf\uff0c\u751f\u6210\u540e\u7eed\u52a8\u4f5c\uff1b\u63d0\u51fa\u6ce8\u610f\u529b\u63a9\u7801\u7b56\u7565\u4ee5\u51cf\u5c11\u81ea\u56de\u5f52\u52a8\u4f5c\u751f\u6210\u7684\u8bef\u5dee\u4f20\u64ad\u3002", "result": "WorldVLA\u4f18\u4e8e\u72ec\u7acb\u6a21\u578b\uff0c\u4f46\u81ea\u56de\u5f52\u52a8\u4f5c\u751f\u6210\u6027\u80fd\u4e0b\u964d\uff1b\u6ce8\u610f\u529b\u63a9\u7801\u7b56\u7565\u663e\u8457\u6539\u5584\u4e86\u52a8\u4f5c\u5757\u751f\u6210\u4efb\u52a1\u7684\u8868\u73b0\u3002", "conclusion": "WorldVLA\u5c55\u793a\u4e86\u4e16\u754c\u6a21\u578b\u4e0e\u52a8\u4f5c\u6a21\u578b\u7684\u76f8\u4e92\u589e\u5f3a\uff0c\u6ce8\u610f\u529b\u63a9\u7801\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u52a8\u4f5c\u751f\u6210\u7684\u8bef\u5dee\u4f20\u64ad\u95ee\u9898\u3002"}}
