<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 37]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [COSMO-Bench: A Benchmark for Collaborative SLAM Optimization](https://arxiv.org/abs/2508.16731)
*Daniel McGann,Easton R. Potokar,Michael Kaess*

Main category: cs.RO

TL;DR: 提出了COSMO-Bench基准数据集套件，包含24个基于真实LiDAR数据和先进C-SLAM前端的数据集，用于解决多机器人协同SLAM领域缺乏标准基准的问题


<details>
  <summary>Details</summary>
Motivation: 多机器人协同SLAM研究因缺乏标准基准数据集而困难，单机器人SLAM领域已证明基准数据集的有效性，多机器人领域亟需专用基准

Method: 设计和发布基于真实LiDAR数据和先进C-SLAM前端的24个数据集套件

Result: 成功创建并开源了COSMO-Bench基准数据集，为多机器人优化研究提供标准化评估工具

Conclusion: COSMO-Bench填补了多机器人协同SLAM领域基准数据集的空白，将促进该领域研究的标准化和可比性

Abstract: Recent years have seen a focus on research into distributed optimization
algorithms for multi-robot Collaborative Simultaneous Localization and Mapping
(C-SLAM). Research in this domain, however, is made difficult by a lack of
standard benchmark datasets. Such datasets have been used to great effect in
the field of single-robot SLAM, and researchers focused on multi-robot problems
would benefit greatly from dedicated benchmark datasets. To address this gap,
we design and release the Collaborative Open-Source Multi-robot Optimization
Benchmark (COSMO-Bench) -- a suite of 24 datasets derived from a
state-of-the-art C-SLAM front-end and real-world LiDAR data. Data DOI:
https://doi.org/10.1184/R1/29652158

</details>


### [2] [A Dataset and Benchmark for Robotic Cloth Unfolding Grasp Selection: The ICRA 2024 Cloth Competition](https://arxiv.org/abs/2508.16749)
*Victor-Louis De Gusseme,Thomas Lips,Remko Proesmans,Julius Hietala,Giwan Lee,Jiyoung Choi,Jeongil Choi,Geon Kim,Phayuth Yonrith,Domen Tabernik,Andrej Gams,Peter Nimac,Matej Urbas,Jon Muhovič,Danijel Skočaj,Matija Mavsar,Hyojeong Yu,Minseo Kwon,Young J. Kim,Yang Cong,Ronghan Chen,Yu Ren,Supeng Diao,Jiawei Weng,Jiayue Liu,Haoran Sun,Linhan Yang,Zeqing Zhang,Ning Guo,Lei Yang,Fang Wan,Chaoyang Song,Jia Pan,Yixiang Jin,Yong A,Jun Shi,Dingzhe Li,Yong Yang,Kakeru Yamasaki,Takumi Kajiwara,Yuki Nakadera,Krati Saxena,Tomohiro Shibata,Chongkun Xia,Kai Mo,Yanzhao Yu,Qihao Lin,Binqiang Ma,Uihun Sagong,JungHyun Choi,JeongHyun Park,Dongwoo Lee,Yeongmin Kim,Myun Joong Hwang,Yusuke Kuribayashi,Naoki Hiratsuka,Daisuke Tanaka,Solvi Arnold,Kimitoshi Yamazaki,Carlos Mateo-Agullo,Andreas Verleysen,Francis Wyffels*

Main category: cs.RO

TL;DR: 本文提出了一个机器人布料操作的标准化基准和数据集，组织了ICRA 2024布料竞赛，通过11个团队的参与验证了不同方法的性能，并揭示了工程化方法在抓取成功率和覆盖率之间的权衡优势。


<details>
  <summary>Details</summary>
Motivation: 机器人布料操作领域缺乏标准化的基准测试和共享数据集，这阻碍了不同方法的比较和评估。为了解决这个问题，作者创建了一个基准并组织了竞赛。

Method: 创建了公开的真实世界机器人布料展开数据集，组织了11个团队参与的竞赛，使用各种方法设计展开策略，并收集了679个展开演示数据。

Result: 竞赛结果显示工程化方法表现强劲，抓取成功率和覆盖率之间存在权衡关系，竞赛性能与先前工作存在显著差异，强调了独立实验室外评估的重要性。

Conclusion: 该基准、数据集和竞赛结果为未来基准测试奠定了基础，推动了数据驱动机器人布料操作的进一步发展，数据集对基于学习的方法特别有价值。

Abstract: Robotic cloth manipulation suffers from a lack of standardized benchmarks and
shared datasets for evaluating and comparing different approaches. To address
this, we created a benchmark and organized the ICRA 2024 Cloth Competition, a
unique head-to-head evaluation focused on grasp pose selection for in-air
robotic cloth unfolding. Eleven diverse teams participated in the competition,
utilizing our publicly released dataset of real-world robotic cloth unfolding
attempts and a variety of methods to design their unfolding approaches.
Afterwards, we also expanded our dataset with 176 competition evaluation
trials, resulting in a dataset of 679 unfolding demonstrations across 34
garments. Analysis of the competition results revealed insights about the
trade-off between grasp success and coverage, the surprisingly strong
achievements of hand-engineered methods and a significant discrepancy between
competition performance and prior work, underscoring the importance of
independent, out-of-the-lab evaluation in robotic cloth manipulation. The
associated dataset is a valuable resource for developing and evaluating grasp
selection methods, particularly for learning-based approaches. We hope that our
benchmark, dataset and competition results can serve as a foundation for future
benchmarks and drive further progress in data-driven robotic cloth
manipulation. The dataset and benchmarking code are available at
https://airo.ugent.be/cloth_competition.

</details>


### [3] [Autonomous UAV Flight Navigation in Confined Spaces: A Reinforcement Learning Approach](https://arxiv.org/abs/2508.16807)
*Marco S. Tayar,Lucas K. de Oliveira,Juliano D. Negri,Thiago H. Segreto,Ricardo V. Godoy,Marcelo Becker*

Main category: cs.RO

TL;DR: 比较PPO和SAC两种深度强化学习算法在GPS拒止环境下无人机管道导航的性能，发现PPO表现更稳定可靠


<details>
  <summary>Details</summary>
Motivation: 工业管道等受限空间的人工检测危险且低效，无人机是理想替代方案，但需要鲁棒的控制策略来避免碰撞

Method: 使用Genesis仿真环境生成程序化管道环境，设计奖励函数引导无人机通过航点同时惩罚碰撞，比较PPO和SAC两种DRL算法

Result: PPO学会了稳定策略，完成所有评估任务无碰撞，轨迹平滑；SAC则收敛到次优行为，仅能通过初始段后失败

Conclusion: 在危险密集的导航任务中，on-policy方法（PPO）的训练稳定性优于off-policy方法（SAC）的样本效率；程序化高保真仿真是开发和评估导航策略的有效平台

Abstract: Inspecting confined industrial infrastructure, such as ventilation shafts, is
a hazardous and inefficient task for humans. Unmanned Aerial Vehicles (UAVs)
offer a promising alternative, but GPS-denied environments require robust
control policies to prevent collisions. Deep Reinforcement Learning (DRL) has
emerged as a powerful framework for developing such policies, and this paper
provides a comparative study of two leading DRL algorithms for this task: the
on-policy Proximal Policy Optimization (PPO) and the off-policy Soft
Actor-Critic (SAC). The training was conducted with procedurally generated duct
environments in Genesis simulation environment. A reward function was designed
to guide a drone through a series of waypoints while applying a significant
penalty for collisions. PPO learned a stable policy that completed all
evaluation episodes without collision, producing smooth trajectories. By
contrast, SAC consistently converged to a suboptimal behavior that traversed
only the initial segments before failure. These results suggest that, in
hazard-dense navigation, the training stability of on-policy methods can
outweigh the nominal sample efficiency of off-policy algorithms. More broadly,
the study provides evidence that procedurally generated, high-fidelity
simulations are effective testbeds for developing and benchmarking robust
navigation policies.

</details>


### [4] [A Workflow for Map Creation in Autonomous Vehicle Simulations](https://arxiv.org/abs/2508.16856)
*Zubair Islam,Ahmaad Ansari,George Daoud,Mohamed El-Darieby*

Main category: cs.RO

TL;DR: 本文提出了一个简化自动驾驶车辆开发中地图创建的自定义工作流，通过生成安大略理工大学停车场的3D地图进行演示，旨在解决现有方法计算资源需求大和模拟器兼容性有限的问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶技术的快速发展需要大量仿真测试，而精确且适应性强的地图对于车辆定位、路径规划和场景测试至关重要。现有地图创建工作流通常计算资源密集且受限于特定模拟器，限制了开发者的灵活性。

Method: 开发了一个自定义工作流来简化地图创建过程，具体通过生成安大略理工大学停车场的3D地图来演示该方法。

Result: 成功创建了停车场3D地图，证明了该工作流在自动驾驶开发中的实用性。

Conclusion: 该工作流为自动驾驶开发提供了更高效的地图创建解决方案，未来工作将重点整合SLAM技术、优化工作流以支持更广泛的模拟器兼容性，并改进经纬度值处理以提高地图生成精度。

Abstract: The fast development of technology and artificial intelligence has
significantly advanced Autonomous Vehicle (AV) research, emphasizing the need
for extensive simulation testing. Accurate and adaptable maps are critical in
AV development, serving as the foundation for localization, path planning, and
scenario testing. However, creating simulation-ready maps is often difficult
and resource-intensive, especially with simulators like CARLA (CAR Learning to
Act). Many existing workflows require significant computational resources or
rely on specific simulators, limiting flexibility for developers. This paper
presents a custom workflow to streamline map creation for AV development,
demonstrated through the generation of a 3D map of a parking lot at Ontario
Tech University. Future work will focus on incorporating SLAM technologies,
optimizing the workflow for broader simulator compatibility, and exploring more
flexible handling of latitude and longitude values to enhance map generation
accuracy.

</details>


### [5] [Relative Navigation and Dynamic Target Tracking for Autonomous Underwater Proximity Operations](https://arxiv.org/abs/2508.16901)
*David Baxter,Aldo Terán Espinoza,Antonio Terán Espinoza,Amy Loutfi,John Folkesson,Peter Sigray,Stephanie Lowry,Jakob Kuttenkeuler*

Main category: cs.RO

TL;DR: 提出了一种基于李群切空间的广义恒定扭转运动先验，用于水下近距离操作中目标6自由度运动估计，通过SE(3)表示耦合平移和旋转，解决了因子图估计的欠约束问题。


<details>
  <summary>Details</summary>
Motivation: 水下近距离操作中，追踪器缺乏目标侧的本体感知，可用的相对观测稀疏、噪声大且通常不完整（如USBL位置）。没有运动先验时，因子图最大后验估计是欠约束的：连续目标状态关联弱，方向容易漂移。

Method: 提出了基于李群切空间的广义恒定扭转运动先验，在SE(3)中耦合了体坐标系下的平移和旋转。设计了三元因子并推导了基于标准李群操作的闭式雅可比矩阵，可在任意李群轨迹上使用。评估了两种部署模式：纯SE(3)表示和带边界因子的混合表示模式。

Result: 在真实世界动态对接场景数据集上的验证表明，该方法能够通过USBL-only和光学相对测量段实现一致的自-目标轨迹估计，相比噪声测量提高了相对跟踪精度。

Conclusion: 该方法依赖于标准李群原语构建，具有跨状态流形和感知模态的可移植性，为水下6-DoF运动估计提供了有效的解决方案。

Abstract: Estimating a target's 6-DoF motion in underwater proximity operations is
difficult because the chaser lacks target-side proprioception and the available
relative observations are sparse, noisy, and often partial (e.g., Ultra-Short
Baseline (USBL) positions). Without a motion prior, factor-graph maximum a
posteriori estimation is underconstrained: consecutive target states are weakly
linked and orientation can drift. We propose a generalized constant-twist
motion prior defined on the tangent space of Lie groups that enforces
temporally consistent trajectories across all degrees of freedom; in SE(3) it
couples translation and rotation in the body frame. We present a ternary factor
and derive its closed-form Jacobians based on standard Lie group operations,
enabling drop-in use for trajectories on arbitrary Lie groups. We evaluate two
deployment modes: (A) an SE(3)-only representation that regularizes orientation
even when only position is measured, and (B) a mode with boundary factors that
switches the target representation between SE(3) and 3D position while applying
the same generalized constant-twist prior across representation changes.
Validation on a real-world dynamic docking scenario dataset shows consistent
ego-target trajectory estimation through USBL-only and optical relative
measurement segments with an improved relative tracking accuracy compared to
the noisy measurements to the target. Because the construction relies on
standard Lie group primitives, it is portable across state manifolds and
sensing modalities.

</details>


### [6] [HumanoidVerse: A Versatile Humanoid for Vision-Language Guided Multi-Object Rearrangement](https://arxiv.org/abs/2508.16943)
*Haozhuo Zhang,Jingkai Sun,Michele Caprio,Jian Tang,Shanghang Zhang,Qiang Zhang,Wei Pan*

Main category: cs.RO

TL;DR: HumanoidVerse是一个新颖的视觉语言引导人形机器人控制框架，能够在多样化场景中执行长时程、多目标重排任务，仅使用自然语言指令和第一视角RGB观察。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常在固定设置中进行单目标交互，无法处理现实世界中复杂的多目标连续操作任务。需要开发能够在多样化环境中执行长时程、多目标操作的通用人形机器人控制系统。

Method: 采用多阶段课程学习和双教师蒸馏管道进行训练，构建了包含350个多目标任务的大规模数据集，涵盖四种房间布局。在Isaac Gym模拟器中进行物理仿真。

Result: 在任务成功率和空间精度方面显著优于现有最先进方法，能够很好地泛化到未见过的环境和指令。

Conclusion: 这项工作代表了向能够在现实世界感官约束下执行复杂顺序任务的鲁棒通用人形代理迈出的关键一步。

Abstract: We introduce HumanoidVerse, a novel framework for vision-language guided
humanoid control that enables a single physically simulated robot to perform
long-horizon, multi-object rearrangement tasks across diverse scenes. Unlike
prior methods that operate in fixed settings with single-object interactions,
our approach supports consecutive manipulation of multiple objects, guided only
by natural language instructions and egocentric camera RGB observations.
HumanoidVerse is trained via a multi-stage curriculum using a dual-teacher
distillation pipeline, enabling fluid transitions between sub-tasks without
requiring environment resets. To support this, we construct a large-scale
dataset comprising 350 multi-object tasks spanning four room layouts. Extensive
experiments in the Isaac Gym simulator demonstrate that our method
significantly outperforms prior state-of-the-art in both task success rate and
spatial precision, and generalizes well to unseen environments and
instructions. Our work represents a key step toward robust, general-purpose
humanoid agents capable of executing complex, sequential tasks under real-world
sensory constraints. The video visualization results can be found on the
project page: https://haozhuo-zhang.github.io/HumanoidVerse-project-page/.

</details>


### [7] [Drive As You Like: Strategy-Level Motion Planning Based on A Multi-Head Diffusion Model](https://arxiv.org/abs/2508.16947)
*Fan Ding,Xuewen Luo,Hwa Hui Tew,Ruturaj Reddy,Xikun Wang,Junn Yong Loo*

Main category: cs.RO

TL;DR: 提出基于扩散模型的多头轨迹规划器(M-diffusion planner)，通过GRPO微调实现多样化策略行为，结合LLM进行动态指令感知规划，在nuPlan基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶运动规划模型在监督训练后策略固定，导致驾驶行为刚性，无法反映人类偏好或适应动态指令驱动的需求。

Method: 使用扩散模型构建多头轨迹规划器，早期训练阶段共享权重学习高质量轨迹生成，然后应用GRPO进行策略特定行为的微调，推理时结合LLM指导策略选择。

Result: 闭环仿真显示规划器保持强规划能力，在nuPlan val14基准上达到SOTA性能；开环结果显示生成轨迹具有明显多样性，有效满足多模态驾驶行为需求。

Conclusion: 该方法成功实现了在保持规划质量的同时，提供多样化、可适应动态指令的自动驾驶轨迹规划能力。

Abstract: Recent advances in motion planning for autonomous driving have led to models
capable of generating high-quality trajectories. However, most existing
planners tend to fix their policy after supervised training, leading to
consistent but rigid driving behaviors. This limits their ability to reflect
human preferences or adapt to dynamic, instruction-driven demands. In this
work, we propose a diffusion-based multi-head trajectory planner(M-diffusion
planner). During the early training stage, all output heads share weights to
learn to generate high-quality trajectories. Leveraging the probabilistic
nature of diffusion models, we then apply Group Relative Policy Optimization
(GRPO) to fine-tune the pre-trained model for diverse policy-specific
behaviors. At inference time, we incorporate a large language model (LLM) to
guide strategy selection, enabling dynamic, instruction-aware planning without
switching models. Closed-loop simulation demonstrates that our post-trained
planner retains strong planning capability while achieving state-of-the-art
(SOTA) performance on the nuPlan val14 benchmark. Open-loop results further
show that the generated trajectories exhibit clear diversity, effectively
satisfying multi-modal driving behavior requirements. The code and related
experiments will be released upon acceptance of the paper.

</details>


### [8] [LLM-based Human-like Traffic Simulation for Self-driving Tests](https://arxiv.org/abs/2508.16962)
*Wendi Li,Hao Wu,Han Gao,Bing Mao,Fengyuan Xu,Sheng Zhong*

Main category: cs.RO

TL;DR: HDSim是一个结合认知理论和大型语言模型的交通生成框架，通过分层驾驶员模型和感知介导的行为影响策略，在仿真平台中生成可扩展且真实的交通场景。


<details>
  <summary>Details</summary>
Motivation: 现有仿真平台依赖手工启发式或狭窄的数据驱动模型，只能捕捉真实驾驶行为的片段，驾驶风格多样性和可解释性有限，需要更真实的交通动态来评估自动驾驶系统的可靠性。

Method: 引入分层驾驶员模型表示多样驾驶风格特征，开发感知介导的行为影响策略，利用LLM引导感知来间接塑造驾驶员行为。

Result: 实验表明，将HDSim嵌入仿真可将自动驾驶系统安全关键故障的检测率提高高达68%，并产生符合真实感的事故可解释性。

Conclusion: HDSim框架通过结合认知理论和LLM辅助，能够生成更真实、多样化的交通场景，显著提升自动驾驶系统测试的有效性和可解释性。

Abstract: Ensuring realistic traffic dynamics is a prerequisite for simulation
platforms to evaluate the reliability of self-driving systems before deployment
in the real world. Because most road users are human drivers, reproducing their
diverse behaviors within simulators is vital. Existing solutions, however,
typically rely on either handcrafted heuristics or narrow data-driven models,
which capture only fragments of real driving behaviors and offer limited
driving style diversity and interpretability. To address this gap, we introduce
HDSim, an HD traffic generation framework that combines cognitive theory with
large language model (LLM) assistance to produce scalable and realistic traffic
scenarios within simulation platforms. The framework advances the state of the
art in two ways: (i) it introduces a hierarchical driver model that represents
diverse driving style traits, and (ii) it develops a Perception-Mediated
Behavior Influence strategy, where LLMs guide perception to indirectly shape
driver actions. Experiments reveal that embedding HDSim into simulation
improves detection of safety-critical failures in self-driving systems by up to
68% and yields realism-consistent accident interpretability.

</details>


### [9] [DualReg: Dual-Space Filtering and Reinforcement for Rigid Registration](https://arxiv.org/abs/2508.17034)
*Jiayi Li,Yuxin Yao,Qiuhang Lu,Juyong Zhang*

Main category: cs.RO

TL;DR: 这篇论文提出了一种新的双空间范式来解决刚体注册问题，结合了特征基和局部几何方法的优势，实现了高效的准确对齐。


<details>
  <summary>Details</summary>
Motivation: 解决在噪声、部分重叠数据和实时处理要求下的刚体注册挑战，充分利用特征基匹配和局部几何匹配的各自优势。

Method: 首先使用计算轻量的单点RANSAC算法和精细模块过滤不可靠的特征对应关系，然后将筛选后的对应点作为锚点提取几何代理，构建有效的目标函数并使用定制求解器估计变换。

Result: 在KITTI数据集上实现了较MAC方法达到32倍的CPU时间加速，同时保持相当的准确性。

Conclusion: 该双空间范式有效地结合了特征基和几何基方法的优势，为刚体注册问题提供了一种高效准确的解决方案。

Abstract: Rigid registration, aiming to estimate a rigid transformation to align source
and target data, play a crucial role in applications such as SLAM and 3D
reconstruction. However, noisy, partially overlapping data and the need for
real-time processing pose major challenges for rigid registration. Considering
that feature-based matching can handle large transformation differences but
suffers from limited accuracy, while local geometry-based matching can achieve
fine-grained local alignment but relies heavily on a good initial
transformation, we propose a novel dual-space paradigm to fully leverage the
strengths of both approaches. First, we introduce an efficient filtering
mechanism that incorporates a computationally lightweight single-point RANSAC
algorithm followed by a refinement module to eliminate unreliable feature-based
correspondences. Subsequently, we treat filtered correspondences as anchor
points, extract geometric proxies, and formulates an effective objective
function with a tailored solver to estimate the transformation. Experiments
verify our method's effectiveness, as shown by achieving up to a 32x CPU-time
speedup over MAC on KITTI with comparable accuracy.

</details>


### [10] [A Rapid Iterative Trajectory Planning Method for Automated Parking through Differential Flatness](https://arxiv.org/abs/2508.17038)
*Zhouheng Li,Lei Xie,Cheng Hu,Hongye Su*

Main category: cs.RO

TL;DR: 提出基于路径速度分解的快速迭代轨迹规划方法，解决自动泊车中的快速精确无碰撞轨迹规划与控制可行性问题


<details>
  <summary>Details</summary>
Motivation: 自动泊车中路径速度分解轨迹规划面临快速精确无碰撞规划与控制可行性的双重挑战，特别是在换挡点处

Method: 采用新颖的碰撞避免框架平衡时间效率与精确避碰，结合车辆运动学模型和终端平滑约束确保路径符合运动学特性并保持曲率连续性

Result: 仿真结果显示相比其他方法具有更优的时间效率和跟踪误差，实车实验验证了该方法在真实车辆上的适用性

Conclusion: 所提出的RITP方法有效解决了自动泊车轨迹规划的关键挑战，实现了快速精确的无碰撞规划并提高了控制可行性

Abstract: As autonomous driving continues to advance, automated parking is becoming
increasingly essential. However, significant challenges arise when implementing
path velocity decomposition (PVD) trajectory planning for automated parking.
The primary challenge is ensuring rapid and precise collision-free trajectory
planning, which is often in conflict. The secondary challenge involves
maintaining sufficient control feasibility of the planned trajectory,
particularly at gear shifting points (GSP). This paper proposes a PVD-based
rapid iterative trajectory planning (RITP) method to solve the above
challenges. The proposed method effectively balances the necessity for time
efficiency and precise collision avoidance through a novel collision avoidance
framework. Moreover, it enhances the overall control feasibility of the planned
trajectory by incorporating the vehicle kinematics model and including terminal
smoothing constraints (TSC) at GSP during path planning. Specifically, the
proposed method leverages differential flatness to ensure the planned path
adheres to the vehicle kinematic model. Additionally, it utilizes TSC to
maintain curvature continuity at GSP, thereby enhancing the control feasibility
of the overall trajectory. The simulation results demonstrate superior time
efficiency and tracking errors compared to model-integrated and other
iteration-based trajectory planning methods. In the real-world experiment, the
proposed method was implemented and validated on a ROS-based vehicle,
demonstrating the applicability of the RITP method for real vehicles.

</details>


### [11] [LaGarNet: Goal-Conditioned Recurrent State-Space Models for Pick-and-Place Garment Flattening](https://arxiv.org/abs/2508.17070)
*Halid Abdulrahim Kadi,Kasim Terzić*

Main category: cs.RO

TL;DR: 提出了GC-RSSM模型LaGarNet，用于学习衣物抓取放置操作的潜在动态，在真实和仿真环境中实现了四种不同类型衣物的平整操作，性能达到基于网格方法的先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决衣物操作任务中复杂动态建模的挑战，减少先前方法中的归纳偏置，首次成功将状态空间模型应用于复杂衣物操作。

Method: 使用GC-RSSM（目标条件循环状态空间）模型，基于覆盖对齐奖励进行训练，数据集通过随机策略和从少量人类演示学习的扩散策略收集。

Result: LaGarNet在真实世界和仿真环境中成功实现了四种不同类型衣物的平整操作，性能与基于网格的方法相当。

Conclusion: 该方法显著减少了归纳偏置，证明了状态空间模型在复杂衣物操作任务中的有效性，为衣物操作提供了新的解决方案。

Abstract: We present a novel goal-conditioned recurrent state space (GC-RSSM) model
capable of learning latent dynamics of pick-and-place garment manipulation. Our
proposed method LaGarNet matches the state-of-the-art performance of mesh-based
methods, marking the first successful application of state-space models on
complex garments. LaGarNet trains on a coverage-alignment reward and a dataset
collected through a general procedure supported by a random policy and a
diffusion policy learned from few human demonstrations; it substantially
reduces the inductive biases introduced in the previous similar methods. We
demonstrate that a single-policy LaGarNet achieves flattening on four different
types of garments in both real-world and simulation settings.

</details>


### [12] [OVITA: Open-Vocabulary Interpretable Trajectory Adaptations](https://arxiv.org/abs/2508.17260)
*Anurag Maurya,Tashmoy Ghosh,Anh Nguyen,Ravi Prakash*

Main category: cs.RO

TL;DR: OVITA是一个基于大语言模型的机器人轨迹自适应框架，通过自然语言指令实现动态环境下的轨迹调整，支持多种机器人平台。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，非专业用户需要通过自然语言交互来适应动态情况和用户偏好，调整机器人轨迹。

Method: 利用多个预训练大语言模型，通过生成代码作为适应策略来调整轨迹路径点，并使用另一个LLM作为代码解释器实现直观交互。

Result: 在多种机器人平台（KUKA机械臂、地面机器人、无人机）上进行了广泛仿真和真实环境测试，证明了框架的有效性。

Conclusion: OVITA提供了一个可解释、开放词汇的语言驱动框架，能够有效适应动态和新颖情况下的机器人轨迹调整需求。

Abstract: Adapting trajectories to dynamic situations and user preferences is crucial
for robot operation in unstructured environments with non-expert users. Natural
language enables users to express these adjustments in an interactive manner.
We introduce OVITA, an interpretable, open-vocabulary, language-driven
framework designed for adapting robot trajectories in dynamic and novel
situations based on human instructions. OVITA leverages multiple pre-trained
Large Language Models (LLMs) to integrate user commands into trajectories
generated by motion planners or those learned through demonstrations. OVITA
employs code as an adaptation policy generated by an LLM, enabling users to
adjust individual waypoints, thus providing flexible control. Another LLM,
which acts as a code explainer, removes the need for expert users, enabling
intuitive interactions. The efficacy and significance of the proposed OVITA
framework is demonstrated through extensive simulations and real-world
environments with diverse tasks involving spatiotemporal variations on
heterogeneous robotic platforms such as a KUKA IIWA robot manipulator,
Clearpath Jackal ground robot, and CrazyFlie drone.

</details>


### [13] [Robotic Manipulation via Imitation Learning: Taxonomy, Evolution, Benchmark, and Challenges](https://arxiv.org/abs/2508.17449)
*Zezeng Li,Alexandre Chapin,Enda Xiang,Rui Yang,Bruno Machado,Na Lei,Emmanuel Dellandrea,Di Huang,Liming Chen*

Main category: cs.RO

TL;DR: 这篇综述论文系统回顾了基于模仿学习的机器人操作方法，分析了该领域最具影响力的研究，提供了结构化总结、技术发展时间线和定量评估。


<details>
  <summary>Details</summary>
Motivation: 机器人操作是自主机器人的核心技术，模仿学习能让机器人通过模仿人类演示来学习复杂操作技能，需要对该领域进行系统性总结以推动技术发展。

Method: 通过识别和筛选基于社区影响力和内在质量的最具影响力研究，为每篇论文提供结构化总结，包括研究目的、技术实现、层次分类、输入格式、关键先验、优劣势和引用指标，并建立技术发展时间线。

Result: 提供了机器人操作中模仿学习技术的全面综述，包括关键技术进展的时间线、现有方法的基准结果比较和定量评估，为研究者和实践者提供了综合资源。

Conclusion: 该综述不仅展示了机器人操作中模仿学习的最新技术水平，还指出了该领域未来面临的挑战，为该领域的进一步发展提供了重要参考。

Abstract: Robotic Manipulation (RM) is central to the advancement of autonomous robots,
enabling them to interact with and manipulate objects in real-world
environments. This survey focuses on RM methodologies that leverage imitation
learning, a powerful technique that allows robots to learn complex manipulation
skills by mimicking human demonstrations. We identify and analyze the most
influential studies in this domain, selected based on community impact and
intrinsic quality. For each paper, we provide a structured summary, covering
the research purpose, technical implementation, hierarchical classification,
input formats, key priors, strengths and limitations, and citation metrics.
Additionally, we trace the chronological development of imitation learning
techniques within RM policy (RMP), offering a timeline of key technological
advancements. Where available, we report benchmark results and perform
quantitative evaluations to compare existing methods. By synthesizing these
insights, this review provides a comprehensive resource for researchers and
practitioners, highlighting both the state of the art and the challenges that
lie ahead in the field of robotic manipulation through imitation learning.

</details>


### [14] [Evolutionary Brain-Body Co-Optimization Consistently Fails to Select for Morphological Potential](https://arxiv.org/abs/2508.17464)
*Alican Mertan,Nick Cheney*

Main category: cs.RO

TL;DR: 该研究通过详尽映射130万种形态的形态-适应度景观，揭示了进化脑体协同优化算法的局限性，发现算法经常低估新突变个体的适应度，无法有效追踪梯度，难以找到接近最优解。


<details>
  <summary>Details</summary>
Motivation: 脑体协同优化是一个具有挑战性的问题，研究团队希望通过详尽映射形态-适应度景观来深入理解这一问题的本质和现有算法的局限性。

Method: 在包含1,305,840种不同形态的设计空间中，为每个可行形态训练控制器，并在此空间上详尽映射形态-适应度景观，分析进化脑体协同优化算法的工作原理。

Result: 研究发现现有算法无法一致找到接近最优解，搜索过程经常陷入局部最优，算法会低估新突变形态的适应度，从而在整个进化过程中淘汰有前景的形态。

Conclusion: 这项工作最具体地展示了进化脑体协同优化的挑战，为文献中的趋势提供了实证基础，并为未来研究提供了有价值的见解。

Abstract: Brain-body co-optimization remains a challenging problem, despite increasing
interest from the community in recent years. To understand and overcome the
challenges, we propose exhaustively mapping a morphology-fitness landscape to
study it. To this end, we train controllers for each feasible morphology in a
design space of 1,305,840 distinct morphologies, constrained by a computational
budget. First, we show that this design space constitutes a good model for
studying the brain-body co-optimization problem, and our attempt to
exhaustively map it roughly captures the landscape. We then proceed to analyze
how evolutionary brain-body co-optimization algorithms work in this design
space. The complete knowledge of the morphology-fitness landscape facilitates a
better understanding of the results of evolutionary brain-body co-optimization
algorithms and how they unfold over evolutionary time in the morphology space.
This investigation shows that the experimented algorithms cannot consistently
find near-optimal solutions. The search, at times, gets stuck on morphologies
that are sometimes one mutation away from better morphologies, and the
algorithms cannot efficiently track the fitness gradient in the
morphology-fitness landscape. We provide evidence that experimented algorithms
regularly undervalue the fitness of individuals with newly mutated bodies and,
as a result, eliminate promising morphologies throughout evolution. Our work
provides the most concrete demonstration of the challenges of evolutionary
brain-body co-optimization. Our findings ground the trends in the literature
and provide valuable insights for future work.

</details>


### [15] [Optimizing Grasping in Legged Robots: A Deep Learning Approach to Loco-Manipulation](https://arxiv.org/abs/2508.17466)
*Dilermando Almeida,Guilherme Lazzarini,Juliano Negri,Thiago H. Segreto,Ricardo V. Godoy,Marcelo Becker*

Main category: cs.RO

TL;DR: 基于深度学习的模型通过模拟到实际的方法，提升四足机器人搭载操作臂的抓取精度和适应性，实现了全自主的移动操作任务。


<details>
  <summary>Details</summary>
Motivation: 四足机器人在复杂环境中具有优勃移动能力，加装操作臂后可执行移动操作任务。但在动态场景中实现精确和适应性抓取仍面临挑战，需要大量实际数据量定和预编程抓取配置。

Method: 采用模拟到实际的方法，在Genesis模拟环境中生成合成数据集，包含各种视角的抓取尝试。训练了一个基于U-Net结构的CNN模型，处理RGB图像、深度图、分割遮罩和表面法线图等多模态输入，输出抓取质量热力图。

Result: 在四足机器人上验证了完整框架，成功执行了全自主移动操作任务：自主导航到目标物体、使用传感器感知、预测最佳抓取姿势并执行精确抓取。

Conclusion: 证明了利用模拟训练结合先进感知技术可以为物体处理提供可扩展的高效解决方案。

Abstract: Quadruped robots have emerged as highly efficient and versatile platforms,
excelling in navigating complex and unstructured terrains where traditional
wheeled robots might fail. Equipping these robots with manipulator arms unlocks
the advanced capability of loco-manipulation to perform complex physical
interaction tasks in areas ranging from industrial automation to
search-and-rescue missions. However, achieving precise and adaptable grasping
in such dynamic scenarios remains a significant challenge, often hindered by
the need for extensive real-world calibration and pre-programmed grasp
configurations. This paper introduces a deep learning framework designed to
enhance the grasping capabilities of quadrupeds equipped with arms, focusing on
improved precision and adaptability. Our approach centers on a sim-to-real
methodology that minimizes reliance on physical data collection. We developed a
pipeline within the Genesis simulation environment to generate a synthetic
dataset of grasp attempts on common objects. By simulating thousands of
interactions from various perspectives, we created pixel-wise annotated
grasp-quality maps to serve as the ground truth for our model. This dataset was
used to train a custom CNN with a U-Net-like architecture that processes
multi-modal input from an onboard RGB and depth cameras, including RGB images,
depth maps, segmentation masks, and surface normal maps. The trained model
outputs a grasp-quality heatmap to identify the optimal grasp point. We
validated the complete framework on a four-legged robot. The system
successfully executed a full loco-manipulation task: autonomously navigating to
a target object, perceiving it with its sensors, predicting the optimal grasp
pose using our model, and performing a precise grasp. This work proves that
leveraging simulated training with advanced sensing offers a scalable and
effective solution for object handling.

</details>


### [16] [Morphological Cognition: Classifying MNIST Digits Through Morphological Computation Alone](https://arxiv.org/abs/2508.17469)
*Alican Mertan,Nick Cheney*

Main category: cs.RO

TL;DR: 本文提出了一种"形态认知"概念，通过模拟物理体素构建机器人，无需神经网络即可实现MNIST数字图像分类的认知行为


<details>
  <summary>Details</summary>
Motivation: 当前深度学习主导AI研究，但自然界存在多种智能机制未被探索。本文关注体现性在智能行为中的作用，研究如何通过简单物理部件的固定行为涌现出认知能力

Method: 使用具有固定行为的模拟体素构建机器人，当呈现MNIST数字0时向左移动，数字1时向右移动，完全通过形态结构实现图像分类

Result: 成功展示了无需神经电路的高层次心智能力（图像分类），这是首次证明机器人可以通过纯粹形态过程执行认知行为

Conclusion: 这项工作作为概念验证，展示了形态认知的可能性，希望促进对智能不同模型的研究，突破神经网络主导的AI范式

Abstract: With the rise of modern deep learning, neural networks have become an
essential part of virtually every artificial intelligence system, making it
difficult even to imagine different models for intelligent behavior. In
contrast, nature provides us with many different mechanisms for intelligent
behavior, most of which we have yet to replicate. One of such underinvestigated
aspects of intelligence is embodiment and the role it plays in intelligent
behavior. In this work, we focus on how the simple and fixed behavior of
constituent parts of a simulated physical body can result in an emergent
behavior that can be classified as cognitive by an outside observer.
Specifically, we show how simulated voxels with fixed behaviors can be combined
to create a robot such that, when presented with an image of an MNIST digit
zero, it moves towards the left; and when it is presented with an image of an
MNIST digit one, it moves towards the right. Such robots possess what we refer
to as ``morphological cognition'' -- the ability to perform cognitive behavior
as a result of morphological processes. To the best of our knowledge, this is
the first demonstration of a high-level mental faculty such as image
classification performed by a robot without any neural circuitry. We hope that
this work serves as a proof-of-concept and fosters further research into
different models of intelligence.

</details>


### [17] [Variational Shape Inference for Grasp Diffusion on SE(3)](https://arxiv.org/abs/2508.17482)
*S. Talha Bukhari,Kaivalya Agrawal,Zachary Kingston,Aniket Bera*

Main category: cs.RO

TL;DR: 提出了一种基于变分形状推断和扩散模型的多模态抓取合成框架，通过隐式神经表示学习几何特征，在SE(3)流形上生成多样化的稳定抓取，在ACRONYM数据集上性能提升6.3%，并实现了零样本迁移到真实世界物体抓取。


<details>
  <summary>Details</summary>
Motivation: 多模态抓取合成需要生成多样化的稳定抓取方案，但现有方法对形状噪声和点云稀疏性不够鲁棒。需要一种能够有效学习几何特征并增强对形状变化鲁棒性的方法。

Method: 使用变分自编码器进行形状推断学习隐式神经表示，然后利用学习到的几何特征指导扩散模型在SE(3)流形上进行抓取合成，并引入测试时抓取优化技术。

Result: 在ACRONYM数据集上比最先进方法性能提升6.3%，对点云密度退化具有鲁棒性，在真实世界家庭物体抓取中零样本迁移成功率比基线高34%。

Conclusion: 该框架通过变分形状推断和扩散模型的结合，有效提升了多模态抓取合成的性能和鲁棒性，实现了从仿真到真实世界的成功迁移。

Abstract: Grasp synthesis is a fundamental task in robotic manipulation which usually
has multiple feasible solutions. Multimodal grasp synthesis seeks to generate
diverse sets of stable grasps conditioned on object geometry, making the robust
learning of geometric features crucial for success. To address this challenge,
we propose a framework for learning multimodal grasp distributions that
leverages variational shape inference to enhance robustness against shape noise
and measurement sparsity. Our approach first trains a variational autoencoder
for shape inference using implicit neural representations, and then uses these
learned geometric features to guide a diffusion model for grasp synthesis on
the SE(3) manifold. Additionally, we introduce a test-time grasp optimization
technique that can be integrated as a plugin to further enhance grasping
performance. Experimental results demonstrate that our shape inference for
grasp synthesis formulation outperforms state-of-the-art multimodal grasp
synthesis methods on the ACRONYM dataset by 6.3%, while demonstrating
robustness to deterioration in point cloud density compared to other
approaches. Furthermore, our trained model achieves zero-shot transfer to
real-world manipulation of household objects, generating 34% more successful
grasps than baselines despite measurement noise and point cloud calibration
errors.

</details>


### [18] [LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations](https://arxiv.org/abs/2508.17547)
*Weikang Wan,Jiawei Fu,Xiaodi Yuan,Yifeng Zhu,Hao Su*

Main category: cs.RO

TL;DR: LodeStar是一个机器人学习框架，利用基础模型自动分解任务演示为语义技能，通过强化学习生成合成演示数据集，显著提升长时程灵巧操作任务的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 开发能够稳健执行长时程灵巧操作任务的机器人系统具有挑战性，需要物理灵巧性和技能无缝衔接，而模仿学习需要大量数据集，获取成本高。

Method: 使用现成基础模型自动分解任务演示为语义技能，通过强化学习从少量人类演示生成多样化合成数据集，采用Skill Routing Transformer策略链式组合学习到的技能。

Result: 在三个具有挑战性的真实世界长时程灵巧操作任务上的实验评估表明，该方法相比先前基线显著提高了任务性能和鲁棒性。

Conclusion: LodeStar框架通过自动技能分解和合成数据生成，有效解决了长时程灵巧操作任务的学习问题，为机器人系统提供了更强大的能力。

Abstract: Developing robotic systems capable of robustly executing long-horizon
manipulation tasks with human-level dexterity is challenging, as such tasks
require both physical dexterity and seamless sequencing of manipulation skills
while robustly handling environment variations. While imitation learning offers
a promising approach, acquiring comprehensive datasets is resource-intensive.
In this work, we propose a learning framework and system LodeStar that
automatically decomposes task demonstrations into semantically meaningful
skills using off-the-shelf foundation models, and generates diverse synthetic
demonstration datasets from a few human demos through reinforcement learning.
These sim-augmented datasets enable robust skill training, with a Skill Routing
Transformer (SRT) policy effectively chaining the learned skills together to
execute complex long-horizon manipulation tasks. Experimental evaluations on
three challenging real-world long-horizon dexterous manipulation tasks
demonstrate that our approach significantly improves task performance and
robustness compared to previous baselines. Videos are available at
lodestar-robot.github.io.

</details>


### [19] [GWM: Towards Scalable Gaussian World Models for Robotic Manipulation](https://arxiv.org/abs/2508.17600)
*Guanxing Lu,Baoxiong Jia,Puhao Li,Yixin Chen,Ziwei Wang,Yansong Tang,Siyuan Huang*

Main category: cs.RO

TL;DR: 提出了基于高斯原语传播的3D世界模型GWM，通过扩散变换器和3D变分自编码器实现精细的未来场景重建，显著提升了机器人操作策略的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图像的世界模型缺乏稳健的几何信息，无法提供对三维世界的一致空间和物理理解，即使在大规模视频数据上预训练也存在局限性。

Method: 提出高斯世界模型(GWM)，通过推断高斯原语在机器人动作作用下的传播来重建未来状态。核心是结合3D变分自编码器的潜在扩散变换器(DiT)，利用高斯泼溅实现精细的场景级未来状态重建。

Result: 模拟和真实世界实验表明，GWM能够精确预测不同机器人动作条件下的未来场景，训练出的策略性能显著超越现有最佳方法，展示了3D世界模型的数据扩展潜力。

Conclusion: GWM不仅可以通过自监督未来预测训练增强模仿学习代理的视觉表示，还能作为支持基于模型强化学习的神经模拟器，为机器人操作任务提供了有效的3D世界建模解决方案。

Abstract: Training robot policies within a learned world model is trending due to the
inefficiency of real-world interactions. The established image-based world
models and policies have shown prior success, but lack robust geometric
information that requires consistent spatial and physical understanding of the
three-dimensional world, even pre-trained on internet-scale video sources. To
this end, we propose a novel branch of world model named Gaussian World Model
(GWM) for robotic manipulation, which reconstructs the future state by
inferring the propagation of Gaussian primitives under the effect of robot
actions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D
variational autoencoder, enabling fine-grained scene-level future state
reconstruction with Gaussian Splatting. GWM can not only enhance the visual
representation for imitation learning agent by self-supervised future
prediction training, but can serve as a neural simulator that supports
model-based reinforcement learning. Both simulated and real-world experiments
depict that GWM can precisely predict future scenes conditioned on diverse
robot actions, and can be further utilized to train policies that outperform
the state-of-the-art by impressive margins, showcasing the initial data scaling
potential of 3D world model.

</details>


### [20] [SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation](https://arxiv.org/abs/2508.17643)
*Krishna Vinod,Prithvi Jai Ramesh,Pavan Kumar B N,Bharatesh Chakravarthi*

Main category: cs.RO

TL;DR: 开发了一个开源ROS包，用于在Gazebo模拟器中从RGB相机生成事件流，并研究了基于事件的机器人策略在导航和操作任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有微秒级延迟、高动态范围和低功耗等优势，适合在运动模糊、遮挡和光照变化等挑战性条件下进行实时机器人感知，但主流机器人模拟器中缺乏合成事件视觉的仿真设置。

Method: 提出了一个用户友好的v2e ROS包，用于Gazebo模拟，能够从RGB相机无缝生成事件流。通过行为克隆训练基于Transformer的事件机器人策略，并在移动机器人目标跟随和机械臂目标检测抓取两个代表性场景中与RGB基准进行比较。

Result: 实验结果表明，事件引导的策略在各种操作条件下始终提供竞争优势，事件驱动的感知能够改善实时机器人导航和操作性能。

Conclusion: 这项工作为事件相机更广泛地集成到机器人策略学习中奠定了基础，展示了事件驱动感知在机器人应用中的潜力。

Abstract: Event cameras offer microsecond latency, high dynamic range, and low power
consumption, making them ideal for real-time robotic perception under
challenging conditions such as motion blur, occlusion, and illumination
changes. However, despite their advantages, synthetic event-based vision
remains largely unexplored in mainstream robotics simulators. This lack of
simulation setup hinders the evaluation of event-driven approaches for robotic
manipulation and navigation tasks. This work presents an open-source,
user-friendly v2e robotics operating system (ROS) package for Gazebo simulation
that enables seamless event stream generation from RGB camera feeds. The
package is used to investigate event-based robotic policies (ERP) for real-time
navigation and manipulation. Two representative scenarios are evaluated: (1)
object following with a mobile robot and (2) object detection and grasping with
a robotic manipulator. Transformer-based ERPs are trained by behavior cloning
and compared to RGB-based counterparts under various operating conditions.
Experimental results show that event-guided policies consistently deliver
competitive advantages. The results highlight the potential of event-driven
perception to improve real-time robotic navigation and manipulation, providing
a foundation for broader integration of event cameras into robotic policy
learning. The GitHub repo for the dataset and code:
https://eventbasedvision.github.io/SEBVS/

</details>


### [21] [MEVITA: Open-Source Bipedal Robot Assembled from E-Commerce Components via Sheet Metal Welding](https://arxiv.org/abs/2508.17684)
*Kento Kawaharazuka,Shogo Sawaguchi,Ayumu Iwata,Keita Yoneda,Temma Suzuki,Kei Okada*

Main category: cs.RO

TL;DR: 开发了MEVITA开源双足机器人，使用电商平台可获取的零件和钣金焊接技术，简化组装过程，通过强化学习实现稳健行走


<details>
  <summary>Details</summary>
Motivation: 现有开源双足机器人大多依赖3D打印，尺寸扩展性有限且结构脆弱；金属基机器人零件数量多、组装困难且不易获取。需要开发易于组装、零件易得的开源双足机器人平台

Method: 采用钣金焊接技术将复杂几何结构集成到单个零件中，显著减少组件数量；通过仿真中的强化学习和Sim-to-Real迁移技术实现行走控制

Result: 成功开发出MEVITA机器人，在各种环境中表现出稳健的行走行为，验证了方法的有效性

Conclusion: MEVITA提供了一个可行的开源双足机器人解决方案，硬件、软件和训练环境全部开源，促进了双足机器人技术的普及和发展

Abstract: Various bipedal robots have been developed to date, and in recent years,
there has been a growing trend toward releasing these robots as open-source
platforms. This shift is fostering an environment in which anyone can freely
develop bipedal robots and share their knowledge, rather than relying solely on
commercial products. However, most existing open-source bipedal robots are
designed to be fabricated using 3D printers, which limits their scalability in
size and often results in fragile structures. On the other hand, some
metal-based bipedal robots have been developed, but they typically involve a
large number of components, making assembly difficult, and in some cases, the
parts themselves are not readily available through e-commerce platforms. To
address these issues, we developed MEVITA, an open-source bipedal robot that
can be built entirely from components available via e-commerce. Aiming for the
minimal viable configuration for a bipedal robot, we utilized sheet metal
welding to integrate complex geometries into single parts, thereby
significantly reducing the number of components and enabling easy assembly for
anyone. Through reinforcement learning in simulation and Sim-to-Real transfer,
we demonstrated robust walking behaviors across various environments,
confirming the effectiveness of our approach. All hardware, software, and
training environments can be obtained from https://github.com/haraduka/mevita .

</details>


### [22] [Talking to Robots: A Practical Examination of Speech Foundation Models for HRI Applications](https://arxiv.org/abs/2508.17753)
*Theresa Pekarek Rosin,Julia Gachot,Henri-Leon Kordt,Matthias Kerzel,Stefan Wermter*

Main category: cs.RO

TL;DR: 这篇论文评估了4种现最先进的语音识别系统在人机交互环境中的表现，发现它们在应对各种实际困难情况时存在显著性能差异、幻觉偏向和偏见问题


<details>
  <summary>Details</summary>
Motivation: 实际世界中的语音识别系统需要处理不完美音频和多样化用户群体，而在人机交互中，识别错误可能影响任务执行、用户信任和安全

Method: 使用8个公开数据集评估4种现最先进ASR系统，这些数据集涵盖了6个难度维度：领域特定、口音、噪声、年龄变化、语音障碍和自发性语音

Result: 分析显示了在标准测试中表现相似的系统之间存在显著的性能差异、幻觉偏向和内在偏见

Conclusion: 这些限制对人机交互有严重影响，因为识别错误可能干扰任务执行、破坏用户信任并带来安全风险

Abstract: Automatic Speech Recognition (ASR) systems in real-world settings need to
handle imperfect audio, often degraded by hardware limitations or environmental
noise, while accommodating diverse user groups. In human-robot interaction
(HRI), these challenges intersect to create a uniquely challenging recognition
environment. We evaluate four state-of-the-art ASR systems on eight publicly
available datasets that capture six dimensions of difficulty: domain-specific,
accented, noisy, age-variant, impaired, and spontaneous speech. Our analysis
demonstrates significant variations in performance, hallucination tendencies,
and inherent biases, despite similar scores on standard benchmarks. These
limitations have serious implications for HRI, where recognition errors can
interfere with task performance, user trust, and safety.

</details>


### [23] [Adaptive Output Steps: FlexiSteps Network for Dynamic Trajectory Prediction](https://arxiv.org/abs/2508.17797)
*Yunxiang Liu,Hongkuo Niu,Jianlin Zhu*

Main category: cs.RO

TL;DR: 提出FlexiSteps Network (FSN)框架，动态调整轨迹预测的时间步长，通过自适应预测模块和动态解码器实现灵活准确的预测。


<details>
  <summary>Details</summary>
Motivation: 传统轨迹预测模型使用固定长度输出，无法适应动态现实场景的需求，需要开发能够根据上下文条件动态调整预测步长的解决方案。

Method: 设计FSN框架，包含预训练的自适应预测模块(APM)评估和动态调整输出步长，动态解码器(DD)确保即插即用，以及结合Fréchet距离和预测步长长度的评分机制。

Result: 在Argoverse和INTERACTION等基准数据集上的大量实验证明了FSN框架的有效性和灵活性。

Conclusion: FSN框架能够根据上下文条件动态调整预测时间步长，在保持预测准确性的同时提高了轨迹预测的适应性和效率。

Abstract: Accurate trajectory prediction is vital for autonomous driving, robotics, and
intelligent decision-making systems, yet traditional models typically rely on
fixed-length output predictions, limiting their adaptability to dynamic
real-world scenarios. In this paper, we introduce the FlexiSteps Network (FSN),
a novel framework that dynamically adjusts prediction output time steps based
on varying contextual conditions. Inspired by recent advancements addressing
observation length discrepancies and dynamic feature extraction, FSN
incorporates an pre-trained Adaptive Prediction Module (APM) to evaluate and
adjust the output steps dynamically, ensuring optimal prediction accuracy and
efficiency. To guarantee the plug-and-play of our FSN, we also design a Dynamic
Decoder(DD). Additionally, to balance the prediction time steps and prediction
accuracy, we design a scoring mechanism, which not only introduces the
Fr\'echet distance to evaluate the geometric similarity between the predicted
trajectories and the ground truth trajectories but the length of predicted
steps is also considered. Extensive experiments conducted on benchmark datasets
including Argoverse and INTERACTION demonstrate the effectiveness and
flexibility of our proposed FSN framework.

</details>


### [24] [Effect of Performance Feedback Timing on Motor Learning for a Surgical Training Task](https://arxiv.org/abs/2508.17830)
*Mary Kate Gale,Kailana Baker-Matsuoka,Ilana Nisky,Allison Okamura*

Main category: cs.RO

TL;DR: 实时多感官错误反馈相比任务回放反馈或无反馈，能更好地提高机器人辅助微创手术训练中的学习效果，特别是在环的方向控制和弯曲路径段的定位精度方面。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助微创手术已成为多种手术的金标准，但最佳的训练方法尚不明确。研究假设实时错误反馈比任务后反馈能更好地提高学习速度和减少错误。

Method: 42名手术新手学习虚拟环线任务，分为三组：(1)实时错误反馈（触觉和视觉）(2)任务回放错误反馈(3)无错误反馈，评估环的位置和方向准确性。

Result: 实时反馈组在环的方向控制上表现最佳；回放反馈组在长直路径段的方向控制上优于无反馈组；实时反馈组在紧密弯曲路径段的定位精度上表现最好；各组在总体环位置准确性上无显著差异。

Conclusion: 实时触觉和视觉错误反馈相比回放反馈或无反馈，能显著改善虚拟手术任务的学习效果，这种新型训练方法可能使手术学员以更快速度和更高准确性发展技能。

Abstract: Objective: Robot-assisted minimally invasive surgery (RMIS) has become the
gold standard for a variety of surgical procedures, but the optimal method of
training surgeons for RMIS is unknown. We hypothesized that real-time, rather
than post-task, error feedback would better increase learning speed and reduce
errors. Methods: Forty-two surgical novices learned a virtual version of the
ring-on-wire task, a canonical task in RMIS training. We investigated the
impact of feedback timing with multi-sensory (haptic and visual) cues in three
groups: (1) real-time error feedback, (2) trial replay with error feedback, and
(3) no error feedback. Results: Participant performance was evaluated based on
the accuracy of ring position and orientation during the task. Participants who
received real-time feedback outperformed other groups in ring orientation.
Additionally, participants who received feedback in replay outperformed
participants who did not receive any error feedback on ring orientation during
long, straight path sections. There were no significant differences between
groups for ring position overall, but participants who received real-time
feedback outperformed the other groups in positional accuracy on tightly curved
path sections. Conclusion: The addition of real-time haptic and visual error
feedback improves learning outcomes in a virtual surgical task over error
feedback in replay or no error feedback at all. Significance: This work
demonstrates that multi-sensory error feedback delivered in real time leads to
better training outcomes as compared to the same feedback delivered after task
completion. This novel method of training may enable surgical trainees to
develop skills with greater speed and accuracy.

</details>


### [25] [CubeDN: Real-time Drone Detection in 3D Space from Dual mmWave Radar Cubes](https://arxiv.org/abs/2508.17831)
*Yuan Fang,Fangzhan Shi,Xijia Wei,Qingchao Chen,Kevin Chetty,Simon Julier*

Main category: cs.RO

TL;DR: CubeDN是一个专门为无人机3D检测设计的单阶段端到端雷达目标检测网络，通过双雷达配置和深度学习管道解决毫米波雷达在高度测量方面的限制，实现了分米级精度的无人机检测和跟踪。


<details>
  <summary>Details</summary>
Motivation: 随着无人机的广泛使用，需要确保安全和安全性。光学传感器在恶劣光照和环境条件下性能下降，而现有的毫米波雷达系统主要专注于2D道路用户检测，缺乏对无人机3D检测所需的高度测量能力。

Method: 提出CubeDN单阶段端到端雷达目标检测网络，采用双雷达配置来克服高度分辨率差的问题，并设计了新颖的深度学习管道，能够同时检测、定位和分类两种尺寸的无人机。

Result: 在较近距离实现了分米级跟踪精度，整体达到95%的平均精度(AP)和85%的平均召回率(AR)，数据处理和推理速度为10Hz，适合实际应用。

Conclusion: CubeDN通过创新的双雷达配置和深度学习架构，成功解决了毫米波雷达在无人机3D检测中的高度测量挑战，为无人机安全监控提供了可靠的技术解决方案。

Abstract: As drone use has become more widespread, there is a critical need to ensure
safety and security. A key element of this is robust and accurate drone
detection and localization. While cameras and other optical sensors like LiDAR
are commonly used for object detection, their performance degrades under
adverse lighting and environmental conditions. Therefore, this has generated
interest in finding more reliable alternatives, such as millimeter-wave
(mmWave) radar. Recent research on mmWave radar object detection has
predominantly focused on 2D detection of road users. Although these systems
demonstrate excellent performance for 2D problems, they lack the sensing
capability to measure elevation, which is essential for 3D drone detection. To
address this gap, we propose CubeDN, a single-stage end-to-end radar object
detection network specifically designed for flying drones. CubeDN overcomes
challenges such as poor elevation resolution by utilizing a dual radar
configuration and a novel deep learning pipeline. It simultaneously detects,
localizes, and classifies drones of two sizes, achieving decimeter-level
tracking accuracy at closer ranges with overall $95\%$ average precision (AP)
and $85\%$ average recall (AR). Furthermore, CubeDN completes data processing
and inference at 10Hz, making it highly suitable for practical applications.

</details>


### [26] [Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Model](https://arxiv.org/abs/2508.17922)
*Bokai Ji,Jie Gu,Xiaokang Ma,Chu Tang,Jingmin Chen,Guangxia Li*

Main category: cs.RO

TL;DR: 本文提出任务/指令相关的affordance概念，构建了包含1.5万个物体-指令-功能三元组的数据集，并开发了基于大语言模型的"搜索验证"推理流程来预测affordance。


<details>
  <summary>Details</summary>
Motivation: 现有affordance研究大多忽视了任务/指令依赖性，即同一物体在不同指令下会产生不同的操作区域和方向，这限制了智能机器人的物体操作能力。

Method: 构建了包含1.5万个egocentric视角的三元组数据集，提出"search against verifiers"流程：让大语言模型逐步预测affordance，并在迭代过程中通过自我验证来模仿推理过程。

Result: 实验表明该方法不仅解锁了指令导向的affordance预测能力，而且在广泛任务上取得了优异性能。

Conclusion: 任务/指令相关的affordance概念对于智能机器人物体操作至关重要，基于大语言模型的推理流程能够有效预测不同指令下的affordance信息。

Abstract: Affordance is crucial for intelligent robots in the context of object
manipulation. In this paper, we argue that affordance should be
task-/instruction-dependent, which is overlooked by many previous works. That
is, different instructions can lead to different manipulation regions and
directions even for the same object. According to this observation, we present
a new dataset comprising fifteen thousand object-instruction-affordance
triplets. All scenes in the dataset are from an egocentric viewpoint, designed
to approximate the perspective of a human-like robot. Furthermore, we
investigate how to enable large multimodal models (LMMs) to serve as affordance
predictors by implementing a ``search against verifiers'' pipeline. An LMM is
asked to progressively predict affordances, with the output at each step being
verified by itself during the iterative process, imitating a reasoning process.
Experiments show that our method not only unlocks new instruction-oriented
affordance prediction capabilities, but also achieves outstanding performance
broadly.

</details>


### [27] [A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm](https://arxiv.org/abs/2508.17969)
*Alexandros Gkillas,Christos Anagnostopoulos,Nikos Piperigkos,Dimitris Tsiktsiris,Theofilos Christodoulou,Theofanis Siamatras,Dimitrios Triantafyllou,Christos Basdekis,Theoktisti Marinopoulou,Panagiotis Lepentsiotis,Elefterios Blitsis,Aggeliki Zacharaki,Nearchos Stylianidis,Leonidas Katelaris,Lamberto Salvan,Aris S. Lalos,Christos Laoudias,Antonios Lalas,Konstantinos Votis*

Main category: cs.RO

TL;DR: 本文提出了一个用于自动驾驶车辆内外监控的整体感知系统，结合AI技术优化车载感知和体验，包括基于多摄像头的驾驶员行为识别和基于LiDAR的外部环境语义分割。


<details>
  <summary>Details</summary>
Motivation: 开发一个全面的自动驾驶车辆监控系统，通过AI技术提升车辆内外环境的感知能力，优化驾驶体验和安全性。

Method: 采用多摄像头系统进行驾驶员和乘员行为识别，利用大语言模型作为虚拟助手；内部监控还包括空气质量检测和热舒适度分析；外部监控使用基于LiDAR的成本效益语义分割方法，对低质量3D点云进行超分辨率处理。

Result: 系统在欧盟AutoTRUST项目中开发并集成到实际电动车辆中，在意大利Ispra的联合研究中心进行实验验证，显示出模块化感知架构的性能和效率提升。

Conclusion: 提出的整体感知框架成功展示了AI驱动的自适应车辆技术，在内外环境监控方面均表现出优异的性能，为自动驾驶车辆提供了全面的感知解决方案。

Abstract: This paper introduces a holistic perception system for internal and external
monitoring of autonomous vehicles, with the aim of demonstrating a novel
AI-leveraged self-adaptive framework of advanced vehicle technologies and
solutions that optimize perception and experience on-board. Internal monitoring
system relies on a multi-camera setup designed for predicting and identifying
driver and occupant behavior through facial recognition, exploiting in addition
a large language model as virtual assistant. Moreover, the in-cabin monitoring
system includes AI-empowered smart sensors that measure air-quality and perform
thermal comfort analysis for efficient on and off-boarding. On the other hand,
external monitoring system perceives the surrounding environment of vehicle,
through a LiDAR-based cost-efficient semantic segmentation approach, that
performs highly accurate and efficient super-resolution on low-quality raw 3D
point clouds. The holistic perception framework is developed in the context of
EU's Horizon Europe programm AutoTRUST, and has been integrated and deployed on
a real electric vehicle provided by ALKE. Experimental validation and
evaluation at the integration site of Joint Research Centre at Ispra, Italy,
highlights increased performance and efficiency of the modular blocks of the
proposed perception architecture.

</details>


### [28] [Integration of Computer Vision with Adaptive Control for Autonomous Driving Using ADORE](https://arxiv.org/abs/2508.17985)
*Abu Shad Ahammed,Md Shahi Amran Hossain,Sayeri Mukherjee,Roman Obermaisser,Md. Ziaur Rahman*

Main category: cs.RO

TL;DR: 本文提出了一种结合上下文感知计算机视觉模型和ADORE自适应控制框架的自动驾驶系统，在CARLA模拟器中验证了其在恶劣天气条件下的鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在天气变化或未知物体导致的漂移场景中，传统计算机视觉模型的检测性能会下降，需要更强大的感知与决策集成来确保安全。

Method: 将CARLA模拟器通过ROS桥接与ADORE框架集成，构建了包含感知、决策和控制模块的实时通信系统，在清晰和恶劣天气条件下进行模拟测试。

Result: 感知模型在恶劣天气条件下仍保持鲁棒检测性能，ADORE框架成功适应车速限制和障碍物，响应延迟较低。

Conclusion: 深度学习感知与基于规则的自适应决策相结合，能够显著提升汽车安全关键系统的性能。

Abstract: Ensuring safety in autonomous driving requires a seamless integration of
perception and decision making under uncertain conditions. Although computer
vision (CV) models such as YOLO achieve high accuracy in detecting traffic
signs and obstacles, their performance degrades in drift scenarios caused by
weather variations or unseen objects. This work presents a simulated autonomous
driving system that combines a context aware CV model with adaptive control
using the ADORE framework. The CARLA simulator was integrated with ADORE via
the ROS bridge, allowing real-time communication between perception, decision,
and control modules. A simulated test case was designed in both clear and drift
weather conditions to demonstrate the robust detection performance of the
perception model while ADORE successfully adapted vehicle behavior to speed
limits and obstacles with low response latency. The findings highlight the
potential of coupling deep learning-based perception with rule-based adaptive
decision making to improve automotive safety critical system.

</details>


### [29] [No Need to Look! Locating and Grasping Objects by a Robot Arm Covered with Sensitive Skin](https://arxiv.org/abs/2508.17986)
*Karel Bartunek,Lukas Rustler,Matej Hoffmann*

Main category: cs.RO

TL;DR: 本文提出了一种在完全无视觉输入情况下，仅依靠触觉反馈来搜索和抓取物体的方法，使用覆盖敏感皮肤的机器人全身表面接触来定位物体


<details>
  <summary>Details</summary>
Motivation: 传统机器人物体定位和抓取主要依赖视觉传感器，触觉反馈通常只是辅助作用。本研究探索在完全无视觉输入的情况下，仅通过触觉反馈来搜索和抓取物体的极端情况

Method: 将搜索分为两个阶段：(1) 使用完整机器人表面进行粗略的工作空间探索，(2) 使用配备力/力矩传感器的末端执行器进行精确定位。在仿真和真实机器人上系统评估该方法

Result: 真实机器人上单个物体的总体成功率为85.7%，失败主要发生在抓取特定物体时。使用全身接触的方法比仅使用末端执行器触觉反馈的基线方法快6倍

Conclusion: 该方法不限于特定设置，可部署在任何具有全身表面接触感知能力的平台上，在视觉感知具有挑战性的领域（如农业中在枝叶中定位和采摘水果蔬菜）具有广泛应用前景

Abstract: Locating and grasping of objects by robots is typically performed using
visual sensors. Haptic feedback from contacts with the environment is only
secondary if present at all. In this work, we explored an extreme case of
searching for and grasping objects in complete absence of visual input, relying
on haptic feedback only. The main novelty lies in the use of contacts over the
complete surface of a robot manipulator covered with sensitive skin. The search
is divided into two phases: (1) coarse workspace exploration with the complete
robot surface, followed by (2) precise localization using the end-effector
equipped with a force/torque sensor. We systematically evaluated this method in
simulation and on the real robot, demonstrating that diverse objects can be
located, grasped, and put in a basket. The overall success rate on the real
robot for one object was 85.7\% with failures mainly while grasping specific
objects. The method using whole-body contacts is six times faster compared to a
baseline that uses haptic feedback only on the end-effector. We also show
locating and grasping multiple objects on the table. This method is not
restricted to our specific setup and can be deployed on any platform with the
ability of sensing contacts over the entire body surface. This work holds
promise for diverse applications in areas with challenging visual perception
(due to lighting, dust, smoke, occlusion) such as in agriculture when fruits or
vegetables need to be located inside foliage and picked.

</details>


### [30] [Modeling and Control Framework for Autonomous Space Manipulator Handover Operations](https://arxiv.org/abs/2508.18039)
*Diego Quevedo,Sarah Hudson,Donghoon Kim*

Main category: cs.RO

TL;DR: 这篇论文研究太空机器人双臂系统的动态模型和跟踪控制算法，以支持自主机器人间重要物体交接的太空服务、组装和制造任务。


<details>
  <summary>Details</summary>
Motivation: 自主太空机器人技术在未来太空任务中将发挥关键作用，特别是在太空服务、组装和制造(ISAM)领域。机器人之间(R2R)的重要物体交接是这类任务的核心能力。

Method: 开发了协作机器人动态模型，并对多种跟踪控制算法进行了比较分析。

Result: 建立了双臂太空操纵器系统的动态模型，并完成了各种控制算法的比较研究。

Conclusion: 该研究为支持ISAM场景下自主R2R交接的协作机器人动态模型和控制算法提供了重要技术基础。

Abstract: Autonomous space robotics is poised to play a vital role in future space
missions, particularly for In-space Servicing, Assembly, and Manufacturing
(ISAM). A key capability in such missions is the Robot-to-Robot (R2R) handover
of mission-critical objects. This work presents a dynamic model of a dual-arm
space manipulator system and compares various tracking control laws. The key
contributions of this work are the development of a cooperative manipulator
dynamic model and the comparative analysis of control laws to support
autonomous R2R handovers in ISAM scenarios.

</details>


### [31] [Arnold: a generalist muscle transformer policy](https://arxiv.org/abs/2508.18066)
*Alberto Silvio Chiappa,Boshi An,Merkourios Simos,Chengkun Li,Alexander Mathis*

Main category: cs.RO

TL;DR: 开发了Arnold通用策略，通过行为克隆和PPO微调在14个任务中实现专家级性能，使用传感器运动词汇和Transformer架构处理多任务多体现学习


<details>
  <summary>Details</summary>
Motivation: 解决高维非线性人体肌肉骨骼模型的控制挑战，现有方法只能处理单一技能，需要开发能够掌握多个任务和体现的通用策略

Method: 结合行为克隆和PPO微调，使用创新的传感器运动词汇（异构感官模态、目标和执行器的组合表示），采用Transformer架构处理可变观测和动作空间

Result: 在14个挑战性控制任务中达到专家或超专家性能，包括灵巧物体操作和运动任务，支持高效多任务多体现学习并快速适应新任务

Conclusion: 为生物运动控制提供新见解，证实了肌肉协同作用在跨任务中可转移性有限的最新发现，建立了通用的多任务控制框架

Abstract: Controlling high-dimensional and nonlinear musculoskeletal models of the
human body is a foundational scientific challenge. Recent machine learning
breakthroughs have heralded policies that master individual skills like
reaching, object manipulation and locomotion in musculoskeletal systems with
many degrees of freedom. However, these agents are merely "specialists",
achieving high performance for a single skill. In this work, we develop Arnold,
a generalist policy that masters multiple tasks and embodiments. Arnold
combines behavior cloning and fine-tuning with PPO to achieve expert or
super-expert performance in 14 challenging control tasks from dexterous object
manipulation to locomotion. A key innovation is Arnold's sensorimotor
vocabulary, a compositional representation of the semantics of heterogeneous
sensory modalities, objectives, and actuators. Arnold leverages this vocabulary
via a transformer architecture to deal with the variable observation and action
spaces of each task. This framework supports efficient multi-task,
multi-embodiment learning and facilitates rapid adaptation to novel tasks.
Finally, we analyze Arnold to provide insights into biological motor control,
corroborating recent findings on the limited transferability of muscle
synergies across tasks.

</details>


### [32] [The Effects of Communication Delay on Human Performance and Neurocognitive Responses in Mobile Robot Teleoperation](https://arxiv.org/abs/2508.18074)
*Zhaokun Chen,Wenshuo Wang,Wenzhuo Liu,Yichen Liu,Junqiang Xi*

Main category: cs.RO

TL;DR: 本研究通过脑电图和行为数据分析，首次系统揭示了移动机器人遥操作中通信延迟对人类操作性能和神经认知的影响，发现了200-300ms的性能下降阈值和100-200ms的早期感知窗口。


<details>
  <summary>Details</summary>
Motivation: 移动机器人遥操作中的通信延迟严重影响人机协作，但此前缺乏对延迟如何影响人类操作性能和神经认知的系统研究，需要填补这一空白。

Method: 采用人在环实验设计，招募10名参与者，在0-500ms延迟范围内以100ms为增量进行测试，同时采集脑电图(EEG)数据和机器人行为数据进行分析。

Result: 行为分析显示200-300ms延迟时性能显著下降；EEG分析发现前额θ/β波段和顶叶α波段功率具有显著延迟依赖性；识别出100-200ms的早期感知窗口；400ms以上延迟时所有特征趋于平稳。

Conclusion: 研究首次提供了人类在遥操作任务中感知和认知延迟阈值的证据，为延迟补偿策略的设计提供了关键的神经认知学见解。

Abstract: Communication delays in mobile robot teleoperation adversely affect
human-machine collaboration. Understanding delay effects on human operational
performance and neurocognition is essential for resolving this issue. However,
no previous research has explored this. To fill this gap, we conduct a
human-in-the-loop experiment involving 10 participants, integrating
electroencephalography (EEG) and robot behavior data under varying delays
(0-500 ms in 100 ms increments) to systematically investigate these effects.
Behavior analysis reveals significant performance degradation at 200-300 ms
delays, affecting both task efficiency and accuracy. EEG analysis discovers
features with significant delay dependence: frontal $\theta/\beta$-band and
parietal $\alpha$-band power. We also identify a threshold window (100-200 ms)
for early perception of delay in humans, during which these EEG features first
exhibit significant differences. When delay exceeds 400 ms, all features
plateau, indicating saturation of cognitive resource allocation at
physiological limits. These findings provide the first evidence of perceptual
and cognitive delay thresholds during teleoperation tasks in humans, offering
critical neurocognitive insights for the design of delay compensation
strategies.

</details>


### [33] [Analysis of Harpy's Constrained Trotting and Jumping Maneuver](https://arxiv.org/abs/2508.18139)
*Prathima Ananda Kumar*

Main category: cs.RO

TL;DR: 对Harpy推力辅助双足机器人实验数据的分析显示，通过腿部和推力器的战略协同，机器人实现了稳定的混合运动，具有有界轨迹和精确足部放置。


<details>
  <summary>Details</summary>
Motivation: 研究混合腿-推力器运动的基本原理，了解推力辅助双足机器人在不同运动模式下的稳定性和控制特性。

Method: 分析Harpy机器人在小跑和跳跃实验中的数据集，通过多运动模式数据分析来理解腿-推力器协同作用。

Result: 机器人实现了低扭矩的受控关节行为、精确的足部放置、欠驱动自由度的稳定性，腿部提供主要推进力，推力器实现额外的空中相位控制。

Conclusion: 混合驱动方法具有鲁棒性，识别出空中相位需要特定相位控制策略的关键体-腿耦合动力学，实验的一致可重复性和对称性验证了该方法的有效性。

Abstract: This study presents an analysis of experimental data from Harpy, a
thruster-assisted bipedal robot developed at Northeastern University. The study
examines data sets from trotting and jumping experiments to understand the
fundamental principles governing hybrid leg-thruster locomotion. Through data
analysis across multiple locomotion modes, this research reveals that Harpy
achieves stable locomotion with bounded trajectories and consistent foot
placement through strategic leg-thruster synergy. The results demonstrate
controlled joint behavior with low torques and symmetric tracking, accurate
foot placement within kinematic constraints despite phase-transition
perturbations, and underactuated degree-of-freedom stability without
divergence. Energy level analysis reveals that legs provide primary propulsion,
while the thrusters enable additional aerial phase control. The analysis
identifies critical body-leg coupling dynamics during aerial phases that
require phase-specific control strategies. Consistent repeatability and
symmetry across experiments validate the robustness of the hybrid actuation
approach.

</details>


### [34] [DANCeRS: A Distributed Algorithm for Negotiating Consensus in Robot Swarms with Gaussian Belief Propagation](https://arxiv.org/abs/2508.18153)
*Aalok Patwardhan,Andrew J. Davison*

Main category: cs.RO

TL;DR: DANCeRS是一个统一的分布式算法，使用高斯置信传播在离散和连续决策空间中实现群体共识，通过因子图表示群体，支持纯对等消息传递，在动态环境中具有可扩展性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将离散和连续决策空间中的共识视为不同问题，需要一种统一的分布式算法来解决群体机器人在这两个领域的共识需求。

Method: 使用高斯置信传播(GBP)和因子图表示群体，通过纯对等消息传递实现分布式共识，支持路径规划、碰撞避免和离散决策形成。

Result: 实验结果表明该方法在可扩展性和效率方面优于现有方法，成功应用于形状形成和离散决策共识两个场景。

Conclusion: DANCeRS为需要分布式共识的多机器人系统提供了一个有前景的解决方案，能够统一处理离散和连续决策空间的共识问题。

Abstract: Robot swarms require cohesive collective behaviour to address diverse
challenges, including shape formation and decision-making. Existing approaches
often treat consensus in discrete and continuous decision spaces as distinct
problems. We present DANCeRS, a unified, distributed algorithm leveraging
Gaussian Belief Propagation (GBP) to achieve consensus in both domains. By
representing a swarm as a factor graph our method ensures scalability and
robustness in dynamic environments, relying on purely peer-to-peer message
passing. We demonstrate the effectiveness of our general framework through two
applications where agents in a swarm must achieve consensus on global behaviour
whilst relying on local communication. In the first, robots must perform path
planning and collision avoidance to create shape formations. In the second, we
show how the same framework can be used by a group of robots to form a
consensus over a set of discrete decisions. Experimental results highlight our
method's scalability and efficiency compared to recent approaches to these
problems making it a promising solution for multi-robot systems requiring
distributed consensus. We encourage the reader to see the supplementary video
demo.

</details>


### [35] [Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework](https://arxiv.org/abs/2508.18249)
*Zipeng Fang,Yanbo Wang,Lei Zhao,Weidong Chen*

Main category: cs.RO

TL;DR: 提出了一种多模态自监督框架，通过整合足印、LiDAR和相机数据生成可通行性标签，并训练双流网络进行多模态学习，在多种环境中实现了88%的IoU和1.6-3.5%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督学习方法往往无法充分捕捉不可通行区域的特征，且大多数工作专注于单一模态，忽略了多模态传感器融合的优势。

Method: 1) 整合足印、LiDAR和相机数据作为视觉基础模型的提示，生成考虑语义和几何线索的可通行性标签；2) 训练解耦的双流网络进行多模态联合学习；3) 引入稀疏LiDAR监督来减轻伪标签噪声。

Result: 在城市场景、越野环境和校园环境中进行了广泛实验，自动标注方法在不同数据集上达到约88%的IoU，多模态可通行性估计网络相比现有最先进方法在所有评估数据集上IoU提高了1.6-3.5%。

Conclusion: 该多模态自监督框架通过有效整合异构传感器模态，显著提升了可通行性估计的准确性和鲁棒性，为机器人导航提供了更可靠的环境感知能力。

Abstract: Traversability estimation is critical for enabling robots to navigate across
diverse terrains and environments. While recent self-supervised learning
methods achieve promising results, they often fail to capture the
characteristics of non-traversable regions. Moreover, most prior works
concentrate on a single modality, overlooking the complementary strengths
offered by integrating heterogeneous sensory modalities for more robust
traversability estimation. To address these limitations, we propose a
multimodal self-supervised framework for traversability labeling and
estimation. First, our annotation pipeline integrates footprint, LiDAR, and
camera data as prompts for a vision foundation model, generating traversability
labels that account for both semantic and geometric cues. Then, leveraging
these labels, we train a dual-stream network that jointly learns from different
modalities in a decoupled manner, enhancing its capacity to recognize diverse
traversability patterns. In addition, we incorporate sparse LiDAR-based
supervision to mitigate the noise introduced by pseudo labels. Finally,
extensive experiments conducted across urban, off-road, and campus environments
demonstrate the effectiveness of our approach. The proposed automatic labeling
method consistently achieves around 88% IoU across diverse datasets. Compared
to existing self-supervised state-of-the-art methods, our multimodal
traversability estimation network yields consistently higher IoU, improving by
1.6-3.5% on all evaluated datasets.

</details>


### [36] [SafeBimanual: Diffusion-based Trajectory Optimization for Safe Bimanual Manipulation](https://arxiv.org/abs/2508.18268)
*Haoyuan Deng,Wenkai Guo,Qianzhun Wang,Zhenyu Wu,Ziwei Wang*

Main category: cs.RO

TL;DR: 基于潜变模型的双手操控策略存在安全风险，SafeBimanual框架通过轨迹优化和引导采样加强安全约束，提升任务成功率和安全性


<details>
  <summary>Details</summary>
Motivation: 现有潜变模型基于的双手操控策略忽视了物理安全约束，导致可能损坏机器人和物体的危险行为

Method: 设计了多种安全约束成本函数（避免拉裂物体、碰撞等），通过引导采样优化轨迹，并使用VLM动态调度约束函数

Result: 在模拟任务中成功率提升13.7%，不安全交互减少18.8%；实际任务中成功率提升32.5%

Conclusion: SafeBimanual框架能够有效提升任务成功率和安全性，为双手操控提供了可靠的安全保障

Abstract: Bimanual manipulation has been widely applied in household services and
manufacturing, which enables the complex task completion with coordination
requirements. Recent diffusion-based policy learning approaches have achieved
promising performance in modeling action distributions for bimanual
manipulation. However, they ignored the physical safety constraints of bimanual
manipulation, which leads to the dangerous behaviors with damage to robots and
objects. To this end, we propose a test-time trajectory optimization framework
named SafeBimanual for any pre-trained diffusion-based bimanual manipulation
policies, which imposes the safety constraints on bimanual actions to avoid
dangerous robot behaviors with improved success rate. Specifically, we design
diverse cost functions for safety constraints in different dual-arm cooperation
patterns including avoidance of tearing objects and collision between arms and
objects, which optimizes the manipulator trajectories with guided sampling of
diffusion denoising process. Moreover, we employ a vision-language model (VLM)
to schedule the cost functions by specifying keypoints and corresponding
pairwise relationship, so that the optimal safety constraint is dynamically
generated in the entire bimanual manipulation process. SafeBimanual
demonstrates superiority on 8 simulated tasks in RoboTwin with a 13.7% increase
in success rate and a 18.8% reduction in unsafe interactions over
state-of-the-art diffusion-based methods. Extensive experiments on 4 real-world
tasks further verify its practical value by improving the success rate by
32.5%.

</details>


### [37] [FlowVLA: Thinking in Motion with a Visual Chain of Thought](https://arxiv.org/abs/2508.18269)
*Zhide Zhong,Haodong Yan,Junfeng Li,Xiangchen Liu,Xin Gong,Wenxuan Song,Jiayi Chen,Haoang Li*

Main category: cs.RO

TL;DR: FlowVLA提出了一种新的视觉思维链预训练框架，通过先预测光流再预测帧序列的方式，解决了传统VLA模型在物理推理中的局限性，实现了更好的视觉预测和策略学习效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于下一帧预测的视觉-语言-动作模型在物理推理方面存在困难，因为它们将静态外观与动态运动纠缠在一起，导致不合理的视觉预测和低效的策略学习。

Method: 提出了Visual CoT框架，通过"当前帧→光流→下一帧"的推理过程，在单个自回归Transformer中实现。FlowVLA先生成中间光流表示来编码运动动态，然后再预测未来帧。

Result: 在具有挑战性的机器人操作基准测试中实现了最先进的性能，样本效率显著提高，产生了连贯的视觉预测。

Conclusion: 该方法为世界建模提供了更原则性的基础，通过解耦动态学习改善了VLA模型的物理推理能力。

Abstract: Many Vision-Language-Action (VLA) models rely on an internal world model
trained via next-frame prediction. This approach, however, struggles with
physical reasoning as it entangles static appearance with dynamic motion, often
resulting in implausible visual forecasts and inefficient policy learning. To
address these limitations, we introduce the Visual Chain of Thought (Visual
CoT): a pre-training framework that encourages a model to reason about how a
scene evolves before predicting what it will look like. We instantiate this
principle in FlowVLA, which predicts a future frame ($v_{t+1}$) only after
generating an intermediate optical flow representation ($f_t$) that encodes
motion dynamics. This ``$v_t \rightarrow f_t \rightarrow v_{t+1}$'' reasoning
process is implemented within a single autoregressive Transformer, guiding the
model to learn disentangled dynamics. As a result, FlowVLA produces coherent
visual predictions and facilitates more efficient policy learning. Experiments
on challenging robotics manipulation benchmarks demonstrate state-of-the-art
performance with substantially improved sample efficiency, pointing toward a
more principled foundation for world modeling. Project page:
https://irpn-lab.github.io/FlowVLA/

</details>
