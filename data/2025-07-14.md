<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 12]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Noise-Enabled Goal Attainment in Crowded Collectives](https://arxiv.org/abs/2507.08100)
*Lucy Liu,Justin Werfel,Federico Toschi,L. Mahadevan*

Main category: cs.RO

TL;DR: 研究通过模拟、理论和机器人实验，探讨噪声运动如何打破交通堵塞并优化群体目标达成率，发现简单反应性导航在中等密度下表现优异。


<details>
  <summary>Details</summary>
Motivation: 在拥挤环境中，个体需绕过他人到达目的地，理解并控制交通流对机器人群体协调和基础设施设计至关重要。

Method: 结合模拟、理论和机器人实验，分析噪声水平对交通流的影响，并求解最优噪声水平和代理密度。

Result: 高于临界噪声水平时，大堵塞不会持续；简单反应性导航在中等密度下表现高效。

Conclusion: 简单反应性导航在中等密度下优于复杂中央规划器，为实际应用提供启示。

Abstract: In crowded environments, individuals must navigate around other occupants to
reach their destinations. Understanding and controlling traffic flows in these
spaces is relevant to coordinating robot swarms and designing infrastructure
for dense populations. Here, we combine simulations, theory, and robotic
experiments to study how noisy motion can disrupt traffic jams and enable flow
as agents travel to individual goals. Above a critical noise level, large jams
do not persist. From this observation, we analytically approximate the goal
attainment rate as a function of the noise level, then solve for the optimal
agent density and noise level that maximize the swarm's goal attainment rate.
We perform robotic experiments to corroborate our simulated and theoretical
results. Finally, we compare simple, local navigation approaches with a
sophisticated but computationally costly central planner. A simple reactive
scheme performs well up to moderate densities and is far more computationally
efficient than a planner, suggesting lessons for real-world problems.

</details>


### [2] [Imitation Learning for Obstacle Avoidance Using End-to-End CNN-Based Sensor Fusion](https://arxiv.org/abs/2507.08112)
*Lamiaa H. Zain,Hossam H. Ammar,Raafat E. Shalaby*

Main category: cs.RO

TL;DR: 研究设计并测试了两种自定义卷积神经网络（CNN），用于移动机器人在已知和未知环境中的避障导航。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在复杂环境中的避障导航是一个关键问题，需要高效的视觉感知和决策能力。

Method: 使用深度相机的彩色和深度图像作为输入，通过传感器融合生成机器人的角速度作为转向命令。

Result: 在多样化环境中收集了新数据集，并通过多种评估指标比较了两种网络的性能。

Conclusion: 研究明确了哪种网络更适合实际应用，为移动机器人避障提供了有效解决方案。

Abstract: Obstacle avoidance is crucial for mobile robots' navigation in both known and
unknown environments. This research designs, trains, and tests two custom
Convolutional Neural Networks (CNNs), using color and depth images from a depth
camera as inputs. Both networks adopt sensor fusion to produce an output: the
mobile robot's angular velocity, which serves as the robot's steering command.
A newly obtained visual dataset for navigation was collected in diverse
environments with varying lighting conditions and dynamic obstacles. During
data collection, a communication link was established over Wi-Fi between a
remote server and the robot, using Robot Operating System (ROS) topics.
Velocity commands were transmitted from the server to the robot, enabling
synchronized recording of visual data and the corresponding steering commands.
Various evaluation metrics, such as Mean Squared Error, Variance Score, and
Feed-Forward time, provided a clear comparison between the two networks and
clarified which one to use for the application.

</details>


### [3] [Making VLMs More Robot-Friendly: Self-Critical Distillation of Low-Level Procedural Reasoning](https://arxiv.org/abs/2507.08224)
*Chan Young Park,Jillian Fisher,Marius Memmel,Dipika Khullar,Andy Yun,Abhishek Gupta,Yejin Choi*

Main category: cs.RO

TL;DR: SelfReVision是一个轻量级、可扩展的自改进框架，用于提升视觉语言模型在机器人程序规划中的表现，无需外部监督。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）和视觉语言模型（VLMs）在机器人程序规划中缺乏低层次细节，或依赖昂贵的大模型。SelfReVision旨在解决这一问题。

Method: 通过自我批评、修订和验证的循环（受链式思维提示和自我指导范式启发），小型VLMs能生成更高质量、可执行的计划。

Result: 实验表明，SelfReVision显著提升了小型VLMs的性能，甚至优于100倍大小的模型，并改善了下游具体任务的控制能力。

Conclusion: SelfReVision为轻量级VLMs提供了一种高效的自改进方法，适用于机器人程序规划。

Abstract: Large language models (LLMs) have shown promise in robotic procedural
planning, yet their human-centric reasoning often omits the low-level, grounded
details needed for robotic execution. Vision-language models (VLMs) offer a
path toward more perceptually grounded plans, but current methods either rely
on expensive, large-scale models or are constrained to narrow simulation
settings. We introduce SelfReVision, a lightweight and scalable
self-improvement framework for vision-language procedural planning.
SelfReVision enables small VLMs to iteratively critique, revise, and verify
their own plans-without external supervision or teacher models-drawing
inspiration from chain-of-thought prompting and self-instruct paradigms.
Through this self-distillation loop, models generate higher-quality,
execution-ready plans that can be used both at inference and for continued
fine-tuning. Using models varying from 3B to 72B, our results show that
SelfReVision not only boosts performance over weak base VLMs but also
outperforms models 100X the size, yielding improved control in downstream
embodied tasks.

</details>


### [4] [CL3R: 3D Reconstruction and Contrastive Learning for Enhanced Robotic Manipulation Representations](https://arxiv.org/abs/2507.08262)
*Wenbo Cui,Chengyang Zhao,Yuhui Chen,Haoran Li,Zhizheng Zhang,Dongbin Zhao,He Wang*

Main category: cs.RO

TL;DR: 论文提出CL3R框架，结合3D空间感知与语义理解，通过点云掩码自编码器和对比学习提升机器人操作策略的感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练的2D基础模型，但缺乏3D空间信息且难以泛化到不同视角，限制了机器人精细操作的效果。

Method: CL3R使用点云掩码自编码器学习3D表示，并通过对比学习结合2D基础模型的语义知识；统一坐标系并融合多视角点云以提升泛化能力。

Result: 实验表明CL3R在仿真和现实中均优于现有方法，显著提升机器人操作的感知能力。

Conclusion: CL3R通过3D预训练框架有效解决了视角泛化和空间感知问题，为机器人操作策略提供了鲁棒的感知模块。

Abstract: Building a robust perception module is crucial for visuomotor policy
learning. While recent methods incorporate pre-trained 2D foundation models
into robotic perception modules to leverage their strong semantic
understanding, they struggle to capture 3D spatial information and generalize
across diverse camera viewpoints. These limitations hinder the policy's
effectiveness, especially in fine-grained robotic manipulation scenarios. To
address these challenges, we propose CL3R, a novel 3D pre-training framework
designed to enhance robotic manipulation policies. Our method integrates both
spatial awareness and semantic understanding by employing a point cloud Masked
Autoencoder to learn rich 3D representations while leveraging pre-trained 2D
foundation models through contrastive learning for efficient semantic knowledge
transfer. Additionally, we propose a 3D visual representation pre-training
framework for robotic tasks. By unifying coordinate systems across datasets and
introducing random fusion of multi-view point clouds, we mitigate camera view
ambiguity and improve generalization, enabling robust perception from novel
viewpoints at test time. Extensive experiments in both simulation and the real
world demonstrate the superiority of our method, highlighting its effectiveness
in visuomotor policy learning for robotic manipulation.

</details>


### [5] [Learning Robust Motion Skills via Critical Adversarial Attacks for Humanoid Robots](https://arxiv.org/abs/2507.08303)
*Yang Zhang,Zhanxiang Cao,Buqing Nie,Haoyang Li,Yue Gao*

Main category: cs.RO

TL;DR: 提出了一种新颖的对抗训练范式，通过可学习的对抗攻击网络增强人形机器人运动策略的鲁棒性，实验验证了其在真实环境中的有效性。


<details>
  <summary>Details</summary>
Motivation: 强化学习运动策略因仿真与现实的动力学差异导致鲁棒性下降，影响机器人在真实环境中的敏捷性。

Method: 引入可学习的对抗攻击网络，精准识别运动策略的脆弱点并施加针对性扰动，通过动态对抗训练提升鲁棒性。

Result: 在Unitree G1人形机器人上的实验表明，该方法显著提升了运动鲁棒性，成功应对复杂地形和高敏捷性全身轨迹跟踪任务。

Conclusion: 提出的对抗训练范式有效增强了人形机器人在真实环境中的运动鲁棒性和敏捷性。

Abstract: Humanoid robots show significant potential in daily tasks. However,
reinforcement learning-based motion policies often suffer from robustness
degradation due to the sim-to-real dynamics gap, thereby affecting the agility
of real robots. In this work, we propose a novel robust adversarial training
paradigm designed to enhance the robustness of humanoid motion policies in real
worlds. The paradigm introduces a learnable adversarial attack network that
precisely identifies vulnerabilities in motion policies and applies targeted
perturbations, forcing the motion policy to enhance its robustness against
perturbations through dynamic adversarial training. We conduct experiments on
the Unitree G1 humanoid robot for both perceptive locomotion and whole-body
control tasks. The results demonstrate that our proposed method significantly
enhances the robot's motion robustness in real world environments, enabling
successful traversal of challenging terrains and highly agile whole-body
trajectory tracking.

</details>


### [6] [Joint Optimization-based Targetless Extrinsic Calibration for Multiple LiDARs and GNSS-Aided INS of Ground Vehicles](https://arxiv.org/abs/2507.08349)
*Junhui Wang,Yan Qiao,Chao Gao,Naiqi Wu*

Main category: cs.RO

TL;DR: 提出了一种无需外部目标或重叠视场的多LiDAR与GINS外参标定方法，通过已知高度约束和联合优化框架解决平面运动下的可观测性问题。


<details>
  <summary>Details</summary>
Motivation: 智能采矿环境中，多LiDAR与GINS的精确外参标定对传感器融合至关重要，但现有方法依赖人工目标或重叠视场，实际中难以满足。

Method: 基于GINS安装高度的观测模型约束不可观测参数，结合几何对应和运动一致性的联合优化框架。

Result: 模拟和真实数据集验证了方法在不同传感器配置下的准确性、鲁棒性和实用性。

Conclusion: 该方法适用于异构LiDAR配置，解决了平面运动下的标定问题，具有实际应用价值。

Abstract: Accurate extrinsic calibration between multiple LiDAR sensors and a
GNSS-aided inertial navigation system (GINS) is essential for achieving
reliable sensor fusion in intelligent mining environments. Such calibration
enables vehicle-road collaboration by aligning perception data from
vehicle-mounted sensors to a unified global reference frame. However, existing
methods often depend on artificial targets, overlapping fields of view, or
precise trajectory estimation, which are assumptions that may not hold in
practice. Moreover, the planar motion of mining vehicles leads to observability
issues that degrade calibration performance. This paper presents a targetless
extrinsic calibration method that aligns multiple onboard LiDAR sensors to the
GINS coordinate system without requiring overlapping sensor views or external
targets. The proposed approach introduces an observation model based on the
known installation height of the GINS unit to constrain unobservable
calibration parameters under planar motion. A joint optimization framework is
developed to refine both the extrinsic parameters and GINS trajectory by
integrating multiple constraints derived from geometric correspondences and
motion consistency. The proposed method is applicable to heterogeneous LiDAR
configurations, including both mechanical and solid-state sensors. Extensive
experiments on simulated and real-world datasets demonstrate the accuracy,
robustness, and practical applicability of the approach under diverse sensor
setups.

</details>


### [7] [Towards Robust Sensor-Fusion Ground SLAM: A Comprehensive Benchmark and A Resilient Framework](https://arxiv.org/abs/2507.08364)
*Deteng Zhang,Junjie Zhang,Yan Sun,Tao Li,Hao Yin,Hongzhao Xie,Jie Yin*

Main category: cs.RO

TL;DR: 论文提出了M3DGR数据集和Ground-Fusion++框架，填补了SLAM在复杂环境下评估和自适应传感器融合的空白。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM方法在结构化环境中表现良好，但在极端情况下鲁棒性不足，且缺乏标准化评估和多传感器自适应融合策略。

Method: 引入M3DGR数据集，评估40种SLAM系统，并开发Ground-Fusion++框架，融合多种传感器。

Result: M3DGR数据集提供了多样化退化场景，Ground-Fusion++在复杂条件下表现鲁棒。

Conclusion: M3DGR和Ground-Fusion++为SLAM研究提供了新工具和方向，提升了多传感器融合的适应性。

Abstract: Considerable advancements have been achieved in SLAM methods tailored for
structured environments, yet their robustness under challenging corner cases
remains a critical limitation. Although multi-sensor fusion approaches
integrating diverse sensors have shown promising performance improvements, the
research community faces two key barriers: On one hand, the lack of
standardized and configurable benchmarks that systematically evaluate SLAM
algorithms under diverse degradation scenarios hinders comprehensive
performance assessment. While on the other hand, existing SLAM frameworks
primarily focus on fusing a limited set of sensor types, without effectively
addressing adaptive sensor selection strategies for varying environmental
conditions.
  To bridge these gaps, we make three key contributions: First, we introduce
M3DGR dataset: a sensor-rich benchmark with systematically induced degradation
patterns including visual challenge, LiDAR degeneracy, wheel slippage and GNSS
denial. Second, we conduct a comprehensive evaluation of forty SLAM systems on
M3DGR, providing critical insights into their robustness and limitations under
challenging real-world conditions. Third, we develop a resilient modular
multi-sensor fusion framework named Ground-Fusion++, which demonstrates robust
performance by coupling GNSS, RGB-D, LiDAR, IMU (Inertial Measurement Unit) and
wheel odometry. Codes and datasets are publicly available.

</details>


### [8] [Intelligent Control of Spacecraft Reaction Wheel Attitude Using Deep Reinforcement Learning](https://arxiv.org/abs/2507.08366)
*Ghaith El-Dalahmeh,Mohammad Reza Jabbarpour,Bao Quoc Vo,Ryszard Kowalczyk*

Main category: cs.RO

TL;DR: 论文提出了一种基于深度强化学习（DRL）的控制策略TD3-HD，用于提升卫星在反应轮故障条件下的适应性和稳定性，显著优于传统PD控制器和其他DRL算法。


<details>
  <summary>Details</summary>
Motivation: 卫星在动态和不确定环境中自主运行时，传统控制方法和现有DRL算法在实时适应性和容错性方面表现不足，需要更可靠的控制策略。

Method: 结合Twin Delayed Deep Deterministic Policy Gradient（TD3）、Hindsight Experience Replay（HER）和Dimension Wise Clipping（DWC），提出TD3-HD方法，以提升稀疏奖励环境中的学习效果和故障条件下的稳定性。

Result: 实验结果显示，TD3-HD在姿态误差、角速度调节和故障条件下的稳定性方面显著优于PD控制器和其他DRL算法。

Conclusion: TD3-HD是一种强大且容错的机载AI解决方案，适用于自主卫星姿态控制。

Abstract: Reliable satellite attitude control is essential for the success of space
missions, particularly as satellites increasingly operate autonomously in
dynamic and uncertain environments. Reaction wheels (RWs) play a pivotal role
in attitude control, and maintaining control resilience during RW faults is
critical to preserving mission objectives and system stability. However,
traditional Proportional Derivative (PD) controllers and existing deep
reinforcement learning (DRL) algorithms such as TD3, PPO, and A2C often fall
short in providing the real time adaptability and fault tolerance required for
autonomous satellite operations. This study introduces a DRL-based control
strategy designed to improve satellite resilience and adaptability under fault
conditions. Specifically, the proposed method integrates Twin Delayed Deep
Deterministic Policy Gradient (TD3) with Hindsight Experience Replay (HER) and
Dimension Wise Clipping (DWC) referred to as TD3-HD to enhance learning in
sparse reward environments and maintain satellite stability during RW failures.
The proposed approach is benchmarked against PD control and leading DRL
algorithms. Experimental results show that TD3-HD achieves significantly lower
attitude error, improved angular velocity regulation, and enhanced stability
under fault conditions. These findings underscore the proposed method potential
as a powerful, fault tolerant, onboard AI solution for autonomous satellite
attitude control.

</details>


### [9] [LiDAR, GNSS and IMU Sensor Alignment through Dynamic Time Warping to Construct 3D City Maps](https://arxiv.org/abs/2507.08420)
*Haitian Wang,Hezam Albaqami,Xinyu Wang,Muhammad Ibrahim,Zainy M. Malakan,Abdullah M. Algamdi,Mohammed H. Alghamdi,Ajmal Mian*

Main category: cs.RO

TL;DR: 提出了一种融合LiDAR、GNSS和IMU数据的统一框架，用于解决GNSS受限环境下的3D地图全局对齐问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: LiDAR-based 3D mapping在GNSS受限环境中存在累积漂移问题，导致全局对齐误差。

Method: 结合Dynamic Time Warping进行速度对齐，通过扩展卡尔曼滤波优化GNSS和IMU数据，使用NDT配准和位姿图优化构建局部地图，并通过GNSS约束锚点和重叠段精细配准保证全局一致性。

Result: 全局对齐误差从3.32米降至1.24米，提升了61.4%。

Conclusion: 该方法在GNSS受限环境下显著提升了3D城市地图的精度，并为智能城市规划等应用提供了高保真地图。数据集和代码将公开。

Abstract: LiDAR-based 3D mapping suffers from cumulative drift causing global
misalignment, particularly in GNSS-constrained environments. To address this,
we propose a unified framework that fuses LiDAR, GNSS, and IMU data for
high-resolution city-scale mapping. The method performs velocity-based temporal
alignment using Dynamic Time Warping and refines GNSS and IMU signals via
extended Kalman filtering. Local maps are built using Normal Distributions
Transform-based registration and pose graph optimization with loop closure
detection, while global consistency is enforced using GNSS-constrained anchors
followed by fine registration of overlapping segments. We also introduce a
large-scale multimodal dataset captured in Perth, Western Australia to
facilitate future research in this direction. Our dataset comprises 144{,}000
frames acquired with a 128-channel Ouster LiDAR, synchronized RTK-GNSS
trajectories, and MEMS-IMU measurements across 21 urban loops. To assess
geometric consistency, we evaluated our method using alignment metrics based on
road centerlines and intersections to capture both global and local accuracy.
Our method reduces the average global alignment error from 3.32\,m to 1.24\,m,
achieving a 61.4\% improvement. The constructed high-fidelity map supports a
wide range of applications, including smart city planning, geospatial data
integration, infrastructure monitoring, and GPS-free navigation. Our method,
and dataset together establish a new benchmark for evaluating 3D city mapping
in GNSS-constrained environments. The dataset and code will be released
publicly.

</details>


### [10] [Robotic Calibration Based on Haptic Feedback Improves Sim-to-Real Transfer](https://arxiv.org/abs/2507.08572)
*Juraj Gavura,Michal Vavrecka,Igor Farkas,Connor Gade*

Main category: cs.RO

TL;DR: 提出了一种基于触觉反馈校准的新方法，通过触摸屏获取真实环境中末端执行器位置信息，显著减少定位误差。


<details>
  <summary>Details</summary>
Motivation: 解决仿真与现实中机器人末端执行器位置不一致的问题，尤其是在仅有仿真中末端执行器位置信息的情况下。

Method: 使用触摸屏校准机器人末端执行器位置，构建基于线性变换和神经网络的转换函数，填补缺失变量。

Result: 非线性神经网络模型表现最佳，显著降低了定位误差。

Conclusion: 通过触觉反馈校准和神经网络模型，有效解决了仿真到现实转移中的末端执行器位置不一致问题。

Abstract: When inverse kinematics (IK) is adopted to control robotic arms in
manipulation tasks, there is often a discrepancy between the end effector (EE)
position of the robot model in the simulator and the physical EE in reality. In
most robotic scenarios with sim-to-real transfer, we have information about
joint positions in both simulation and reality, but the EE position is only
available in simulation. We developed a novel method to overcome this
difficulty based on haptic feedback calibration, using a touchscreen in front
of the robot that provides information on the EE position in the real
environment. During the calibration procedure, the robot touches specific
points on the screen, and the information is stored. In the next stage, we
build a transformation function from the data based on linear transformation
and neural networks that is capable of outputting all missing variables from
any partial input (simulated/real joint/EE position). Our results demonstrate
that a fully nonlinear neural network model performs best, significantly
reducing positioning errors.

</details>


### [11] [Multi-critic Learning for Whole-body End-effector Twist Tracking](https://arxiv.org/abs/2507.08656)
*Aravind Elanjimattathil Vijayan,Andrei Cramariuc,Mattia Risiglione,Christian Gehring,Marco Hutter*

Main category: cs.RO

TL;DR: 论文提出了一种基于强化学习的框架，用于动态、速度感知的全身末端执行器控制，解决了运动与手臂动作的冲突目标问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法在同时控制机器人运动和末端执行器动作时存在冲突目标（如基座倾斜与水平运动），且缺乏直接控制末端执行器速度的能力。

Method: 采用多评论家演员架构，解耦运动和操纵的奖励信号，并设计基于扭转的末端执行器任务公式。

Result: 在四足机器人上的仿真和硬件实验表明，控制器能同时行走和移动末端执行器，并表现出全身协作行为。

Conclusion: 该方法有效解决了任务冲突，实现了动态、速度感知的全身控制。

Abstract: Learning whole-body control for locomotion and arm motions in a single policy
has challenges, as the two tasks have conflicting goals. For instance,
efficient locomotion typically favors a horizontal base orientation, while
end-effector tracking may benefit from base tilting to extend reachability.
Additionally, current Reinforcement Learning (RL) approaches using a pose-based
task specification lack the ability to directly control the end-effector
velocity, making smoothly executing trajectories very challenging. To address
these limitations, we propose an RL-based framework that allows for dynamic,
velocity-aware whole-body end-effector control. Our method introduces a
multi-critic actor architecture that decouples the reward signals for
locomotion and manipulation, simplifying reward tuning and allowing the policy
to resolve task conflicts more effectively. Furthermore, we design a
twist-based end-effector task formulation that can track both discrete poses
and motion trajectories. We validate our approach through a set of simulation
and hardware experiments using a quadruped robot equipped with a robotic arm.
The resulting controller can simultaneously walk and move its end-effector and
shows emergent whole-body behaviors, where the base assists the arm in
extending the workspace, despite a lack of explicit formulations.

</details>


### [12] [Learning human-to-robot handovers through 3D scene reconstruction](https://arxiv.org/abs/2507.08726)
*Yuekun Wu,Yik Lung Pang,Andrea Cavallaro,Changjae Oh*

Main category: cs.RO

TL;DR: 提出了一种基于稀疏视图高斯泼溅重建的方法（H2RH-SGS），仅从RGB图像学习机器人交接策略，无需真实机器人训练或数据收集。


<details>
  <summary>Details</summary>
Motivation: 解决从仿真到真实环境的视觉域差距问题，减少对真实机器人试验的依赖。

Method: 利用稀疏视图高斯泼溅重建人类-机器人交接场景，生成机器人演示数据，训练策略并直接部署到真实环境。

Result: 在16种家庭物品上的实验表明，H2RH-SGS是一种有效的机器人交接任务表示方法。

Conclusion: H2RH-SGS为机器人交接任务提供了一种新的高效解决方案。

Abstract: Learning robot manipulation policies from raw, real-world image data requires
a large number of robot-action trials in the physical environment. Although
training using simulations offers a cost-effective alternative, the visual
domain gap between simulation and robot workspace remains a major limitation.
Gaussian Splatting visual reconstruction methods have recently provided new
directions for robot manipulation by generating realistic environments. In this
paper, we propose the first method for learning supervised-based robot
handovers solely from RGB images without the need of real-robot training or
real-robot data collection. The proposed policy learner, Human-to-Robot
Handover using Sparse-View Gaussian Splatting (H2RH-SGS), leverages sparse-view
Gaussian Splatting reconstruction of human-to-robot handover scenes to generate
robot demonstrations containing image-action pairs captured with a camera
mounted on the robot gripper. As a result, the simulated camera pose changes in
the reconstructed scene can be directly translated into gripper pose changes.
We train a robot policy on demonstrations collected with 16 household objects
and {\em directly} deploy this policy in the real environment. Experiments in
both Gaussian Splatting reconstructed scene and real-world human-to-robot
handover experiments demonstrate that H2RH-SGS serves as a new and effective
representation for the human-to-robot handover task.

</details>
