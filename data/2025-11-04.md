<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 62]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Gen AI in Automotive: Applications, Challenges, and Opportunities with a Case study on In-Vehicle Experience](https://arxiv.org/abs/2511.00026)
*Chaitanya Shinde,Divya Garikapati*

Main category: cs.RO

TL;DR: 本文综述了生成式AI在汽车行业的应用现状，重点分析了GAN和VAE等使能技术，探讨了在自动驾驶验证、部件设计和人机交互等方面的机遇，以及技术、伦理和安全挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在成为汽车行业的变革力量，但现有综述主要关注感知或制造领域，本文旨在填补生成式AI在语音人机交互方面的研究空白，连接安全性和用户体验视角。

Method: 采用综合文献综述方法，结合梅赛德斯-奔驰MBUX虚拟助手的案例研究，分析生成式AI在汽车领域的应用现状和挑战。

Result: 研究发现生成式AI能够通过合成数据加速自动驾驶验证、优化部件设计，并实现更自然、主动和个性化的车内交互体验，但面临计算需求、偏见、知识产权和对抗鲁棒性等挑战。

Conclusion: 生成式AI在汽车行业具有巨大潜力，但需要解决技术、伦理和安全挑战才能实现负责任部署，未来研究应致力于实现更安全、高效和以用户为中心的移动出行。

Abstract: Generative Artificial Intelligence is emerging as a transformative force in
the automotive industry, enabling novel applications across vehicle design,
manufacturing, autonomous driving, predictive maintenance, and in vehicle user
experience. This paper provides a comprehensive review of the current state of
GenAI in automotive, highlighting enabling technologies such as Generative
Adversarial Networks and Variational Autoencoders. Key opportunities include
accelerating autonomous driving validation through synthetic data generation,
optimizing component design, and enhancing human machine interaction via
personalized and adaptive interfaces. At the same time, the paper identifies
significant technical, ethical, and safety challenges, including computational
demands, bias, intellectual property concerns, and adversarial robustness, that
must be addressed for responsible deployment. A case study on Mercedes Benzs
MBUX Virtual Assistant illustrates how GenAI powered voice systems deliver more
natural, proactive, and personalized in car interactions compared to legacy
rule based assistants. Through this review and case study, the paper outlines
both the promise and limitations of GenAI integration in the automotive sector
and presents directions for future research and development aimed at achieving
safer, more efficient, and user centric mobility. Unlike prior reviews that
focus solely on perception or manufacturing, this paper emphasizes generative
AI in voice based HMI, bridging safety and user experience perspectives.

</details>


### [2] [STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization](https://arxiv.org/abs/2511.00033)
*Diqi He,Xuehao Gao,Hao Li,Junwei Han,Dingwen Zhang*

Main category: cs.RO

TL;DR: 提出STRIDER框架，通过整合空间布局先验和动态任务反馈来优化智能体在零样本视觉语言导航中的决策空间，显著提升了导航成功率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在零样本视觉语言导航中因缺乏结构化决策和先前动作反馈集成不足而导致的导航不鲁棒问题。

Method: 提出STRIDER框架，包含结构化航点生成器（约束动作空间）和任务对齐调节器（基于任务进度调整行为），确保导航过程中的语义对齐。

Result: 在R2R-CE和RxR-CE基准测试中显著超越现有SOTA方法，成功率从29%提升至35%，相对增益达20.7%。

Conclusion: 空间约束决策和反馈引导执行对于提升零样本VLN-CE导航保真度至关重要。

Abstract: The Zero-shot Vision-and-Language Navigation in Continuous Environments
(VLN-CE) task requires agents to navigate previously unseen 3D environments
using natural language instructions, without any scene-specific training. A
critical challenge in this setting lies in ensuring agents' actions align with
both spatial structure and task intent over long-horizon execution. Existing
methods often fail to achieve robust navigation due to a lack of structured
decision-making and insufficient integration of feedback from previous actions.
To address these challenges, we propose STRIDER (Instruction-Aligned Structural
Decision Space Optimization), a novel framework that systematically optimizes
the agent's decision space by integrating spatial layout priors and dynamic
task feedback. Our approach introduces two key innovations: 1) a Structured
Waypoint Generator that constrains the action space through spatial structure,
and 2) a Task-Alignment Regulator that adjusts behavior based on task progress,
ensuring semantic alignment throughout navigation. Extensive experiments on the
R2R-CE and RxR-CE benchmarks demonstrate that STRIDER significantly outperforms
strong SOTA across key metrics; in particular, it improves Success Rate (SR)
from 29% to 35%, a relative gain of 20.7%. Such results highlight the
importance of spatially constrained decision-making and feedback-guided
execution in improving navigation fidelity for zero-shot VLN-CE.

</details>


### [3] [Endowing GPT-4 with a Humanoid Body: Building the Bridge Between Off-the-Shelf VLMs and the Physical World](https://arxiv.org/abs/2511.00041)
*Yingzhao Jian,Zhongan Wang,Yi Yang,Hehe Fan*

Main category: cs.RO

TL;DR: BiBo系统利用现成的视觉语言模型控制人形智能体，通过指令编译器和运动执行器实现开放环境中的多样化交互，无需大量数据收集。


<details>
  <summary>Details</summary>
Motivation: 解决人形智能体在开放环境中处理灵活多样交互的困难，避免昂贵的大规模数据收集需求。

Method: 包含两个关键组件：1) 具身指令编译器，将高级用户指令转换为低级原始命令；2) 基于扩散的运动执行器，从命令生成类人运动并适应环境物理反馈。

Result: 在开放环境中实现90.2%的交互任务成功率，文本引导运动执行精度比先前方法提高16.3%。

Conclusion: BiBo系统成功利用现成VLM的强大开放世界泛化能力，有效处理复杂运动交互，无需大量数据收集。

Abstract: Humanoid agents often struggle to handle flexible and diverse interactions in
open environments. A common solution is to collect massive datasets to train a
highly capable model, but this approach can be prohibitively expensive. In this
paper, we explore an alternative solution: empowering off-the-shelf
Vision-Language Models (VLMs, such as GPT-4) to control humanoid agents,
thereby leveraging their strong open-world generalization to mitigate the need
for extensive data collection. To this end, we present \textbf{BiBo}
(\textbf{B}uilding humano\textbf{I}d agent \textbf{B}y \textbf{O}ff-the-shelf
VLMs). It consists of two key components: (1) an \textbf{embodied instruction
compiler}, which enables the VLM to perceive the environment and precisely
translate high-level user instructions (e.g., {\small\itshape ``have a rest''})
into low-level primitive commands with control parameters (e.g.,
{\small\itshape ``sit casually, location: (1, 2), facing: 90$^\circ$''}); and
(2) a diffusion-based \textbf{motion executor}, which generates human-like
motions from these commands, while dynamically adapting to physical feedback
from the environment. In this way, BiBo is capable of handling not only basic
interactions but also diverse and complex motions. Experiments demonstrate that
BiBo achieves an interaction task success rate of 90.2\% in open environments,
and improves the precision of text-guided motion execution by 16.3\% over prior
methods. The code will be made publicly available.

</details>


### [4] [Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail](https://arxiv.org/abs/2511.00088)
*NVIDIA,:,Yan Wang,Wenjie Luo,Junjie Bai,Yulong Cao,Tong Che,Ke Chen,Yuxiao Chen,Jenna Diamond,Yifan Ding,Wenhao Ding,Liang Feng,Greg Heinrich,Jack Huang,Peter Karkus,Boyi Li,Pinyi Li,Tsung-Yi Lin,Dongran Liu,Ming-Yu Liu,Langechuan Liu,Zhijian Liu,Jason Lu,Yunxiang Mao,Pavlo Molchanov,Lindsey Pavao,Zhenghao Peng,Mike Ranzinger,Ed Schmerling,Shida Shen,Yunfei Shi,Sarah Tariq,Ran Tian,Tilman Wekel,Xinshuo Weng,Tianjun Xiao,Eric Yang,Xiaodong Yang,Yurong You,Xiaohui Zeng,Wenyuan Zhang,Boris Ivanovic,Marco Pavone*

Main category: cs.RO

TL;DR: AR1是一个集成因果链推理与轨迹规划的视觉-语言-动作模型，通过多阶段训练策略提升自动驾驶决策能力，在复杂场景中显著改善规划准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端模仿学习架构在安全关键的长尾场景中表现脆弱，监督稀疏且因果理解有限，需要增强决策的因果推理能力。

Method: 提出三个关键创新：1) 因果链数据集构建；2) 模块化VLA架构结合视觉语言模型和扩散轨迹解码器；3) 多阶段训练策略（监督微调+强化学习）。

Result: 相比轨迹基线，规划准确性提升12%，脱轨率降低35%，近距离接触率降低25%。强化学习后训练使推理质量提升45%，推理-行动一致性提升37%。

Conclusion: AR1通过将可解释推理与精确控制相结合，为L4级自动驾驶提供了实用路径，实车测试证实了实时性能和城市部署能力。

Abstract: End-to-end architectures trained via imitation learning have advanced
autonomous driving by scaling model size and data, yet performance remains
brittle in safety-critical long-tail scenarios where supervision is sparse and
causal understanding is limited. To address this, we introduce Alpamayo-R1
(AR1), a vision-language-action model (VLA) that integrates Chain of Causation
reasoning with trajectory planning to enhance decision-making in complex
driving scenarios. Our approach features three key innovations: (1) the Chain
of Causation (CoC) dataset, built through a hybrid auto-labeling and
human-in-the-loop pipeline producing decision-grounded, causally linked
reasoning traces aligned with driving behaviors; (2) a modular VLA architecture
combining Cosmos-Reason, a Vision-Language Model pre-trained for Physical AI
applications, with a diffusion-based trajectory decoder that generates
dynamically feasible plans in real time; (3) a multi-stage training strategy
using supervised fine-tuning to elicit reasoning and reinforcement learning
(RL) to optimize reasoning quality via large reasoning model feedback and
enforce reasoning-action consistency. Evaluation shows AR1 achieves up to a 12%
improvement in planning accuracy on challenging cases compared to a
trajectory-only baseline, with a 35% reduction in off-road rate and 25%
reduction in close encounter rate in closed-loop simulation. RL post-training
improves reasoning quality by 45% as measured by a large reasoning model critic
and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B
parameters shows consistent improvements. On-vehicle road tests confirm
real-time performance (99 ms latency) and successful urban deployment. By
bridging interpretable reasoning with precise control, AR1 demonstrates a
practical path towards Level 4 autonomous driving. We plan to release AR1
models and a subset of the CoC in a future update.

</details>


### [5] [Digital Twin based Automatic Reconfiguration of Robotic Systems in Smart Environments](https://arxiv.org/abs/2511.00094)
*Angelos Alexopoulos,Agorakis Bompotas,Nikitas Rigas Kalogeropoulos,Panagiotis Kechagias,Athanasios P. Kalogeras,Christos Alexakos*

Main category: cs.RO

TL;DR: 提出了一种基于数字孪生技术的机器人控制器自主动态重构框架，通过虚拟环境模拟优化运动轨迹，实现机器人在动态环境中的快速自适应。


<details>
  <summary>Details</summary>
Motivation: 传统控制系统在动态环境（如智慧城市、精准农业）中难以快速适应不断变化的地形和环境条件，导致效率低下或操作失败。

Method: 利用数字孪生技术创建机器人操作环境的虚拟副本，模拟和优化运动轨迹，根据实时变化重新计算路径和控制参数，并将更新后的代码部署到物理机器人。

Result: 实现了无需人工干预的快速可靠自适应，提升了机器人在动态环境中的自主性。

Conclusion: 该工作推进了数字孪生在机器人领域的集成，为智能动态环境中的自主性增强提供了可扩展的解决方案。

Abstract: Robotic systems have become integral to smart environments, enabling
applications ranging from urban surveillance and automated agriculture to
industrial automation. However, their effective operation in dynamic settings -
such as smart cities and precision farming - is challenged by continuously
evolving topographies and environmental conditions. Traditional control systems
often struggle to adapt quickly, leading to inefficiencies or operational
failures. To address this limitation, we propose a novel framework for
autonomous and dynamic reconfiguration of robotic controllers using Digital
Twin technology. Our approach leverages a virtual replica of the robot's
operational environment to simulate and optimize movement trajectories in
response to real-world changes. By recalculating paths and control parameters
in the Digital Twin and deploying the updated code to the physical robot, our
method ensures rapid and reliable adaptation without manual intervention. This
work advances the integration of Digital Twins in robotics, offering a scalable
solution for enhancing autonomy in smart, dynamic environments.

</details>


### [6] [Real-DRL: Teach and Learn in Reality](https://arxiv.org/abs/2511.00112)
*Yanbing Mao,Yihao Cai,Lui Sha*

Main category: cs.RO

TL;DR: Real-DRL框架是一个用于安全关键自主系统的运行时学习框架，通过DRL-Student、PHY-Teacher和Trigger三个交互组件，在真实物理系统中实现安全优先的深度强化学习。


<details>
  <summary>Details</summary>
Motivation: 解决安全关键自主系统中由于未知未知和Sim2Real差距带来的安全挑战，确保在真实物理系统中运行时既能保证安全又能实现高性能控制。

Method: 采用三组件交互架构：DRL-Student负责双自学习和教中学范式及安全信息批量采样；PHY-Teacher基于物理模型专注于安全关键功能；Trigger管理两者交互。

Result: 在真实四足机器人、NVIDIA Isaac Gym中的四足机器人和倒立摆系统上的实验验证了框架的有效性，实现了保证安全、自动层次学习（安全优先学习后高性能学习）和安全信息批量采样。

Conclusion: Real-DRL框架成功解决了安全关键自主系统中的核心挑战，提供了安全保证的学习机制，能够有效应对未知情况和Sim2Real差距问题。

Abstract: This paper introduces the Real-DRL framework for safety-critical autonomous
systems, enabling runtime learning of a deep reinforcement learning (DRL) agent
to develop safe and high-performance action policies in real plants (i.e., real
physical systems to be controlled), while prioritizing safety! The Real-DRL
consists of three interactive components: a DRL-Student, a PHY-Teacher, and a
Trigger. The DRL-Student is a DRL agent that innovates in the dual
self-learning and teaching-to-learn paradigm and the real-time safety-informed
batch sampling. On the other hand, PHY-Teacher is a physics-model-based design
of action policies that focuses solely on safety-critical functions.
PHY-Teacher is novel in its real-time patch for two key missions: i) fostering
the teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of
real plants. The Trigger manages the interaction between the DRL-Student and
the PHY-Teacher. Powered by the three interactive components, the Real-DRL can
effectively address safety challenges that arise from the unknown unknowns and
the Sim2Real gap. Additionally, Real-DRL notably features i) assured safety,
ii) automatic hierarchy learning (i.e., safety-first learning and then
high-performance learning), and iii) safety-informed batch sampling to address
the learning experience imbalance caused by corner cases. Experiments with a
real quadruped robot, a quadruped robot in NVIDIA Isaac Gym, and a cart-pole
system, along with comparisons and ablation studies, demonstrate the Real-DRL's
effectiveness and unique features.

</details>


### [7] [End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection](https://arxiv.org/abs/2511.00139)
*Yu Cui,Yujian Zhang,Lina Tao,Yang Li,Xinyu Yi,Zhibin Li*

Main category: cs.RO

TL;DR: 提出了一种共享自主性框架，通过VR遥操作和自主手部控制相结合的方式，高效收集高质量的手臂-手部协调演示数据，训练出具有90%成功率的灵巧操作VLA策略。


<details>
  <summary>Details</summary>
Motivation: 解决灵巧操作中高质量训练数据稀缺的问题，现有方法存在人工遥操作负担重、自动规划动作不自然的限制。

Method: 共享自主性框架：人类操作员通过VR控制手臂宏观运动，DexGrasp-VLA策略基于触觉和视觉反馈控制手部精细动作；使用Arm-Hand特征增强模块训练端到端VLA策略；通过纠正性遥操作实现持续改进。

Result: 以最少人力生成高质量数据，在包括未见物体在内的多样化物体上达到90%的成功率。

Conclusion: 该框架有效解决了灵巧操作中的数据收集挑战，显著提升了机器人的灵巧操作能力。

Abstract: Achieving human-like dexterous manipulation remains a major challenge for
general-purpose robots. While Vision-Language-Action (VLA) models show
potential in learning skills from demonstrations, their scalability is limited
by scarce high-quality training data. Existing data collection methods face
inherent constraints: manual teleoperation overloads human operators, while
automated planning often produces unnatural motions. We propose a Shared
Autonomy framework that divides control between macro and micro motions. A
human operator guides the robot's arm pose through intuitive VR teleoperation,
while an autonomous DexGrasp-VLA policy handles fine-grained hand control using
real-time tactile and visual feedback. This division significantly reduces
cognitive load and enables efficient collection of high-quality coordinated
arm-hand demonstrations. Using this data, we train an end-to-end VLA policy
enhanced with our novel Arm-Hand Feature Enhancement module, which captures
both distinct and shared representations of macro and micro movements for more
natural coordination. Our Corrective Teleoperation system enables continuous
policy improvement through human-in-the-loop failure recovery. Experiments
demonstrate that our framework generates high-quality data with minimal
manpower and achieves a 90% success rate across diverse objects, including
unseen instances. Comprehensive evaluations validate the system's effectiveness
in developing dexterous manipulation capabilities.

</details>


### [8] [EgoMI: Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations](https://arxiv.org/abs/2511.00153)
*Justin Yu,Yide Shentu,Di Wu,Pieter Abbeel,Ken Goldberg,Philipp Wu*

Main category: cs.RO

TL;DR: 提出了EgoMI框架，通过捕捉同步的末端执行器和主动头部轨迹来解决模仿学习中的人机体现差距问题，并引入记忆增强策略来处理快速变化的头部视角。


<details>
  <summary>Details</summary>
Motivation: 人类演示的模仿学习存在体现差距问题，人类在操作时会主动协调头部和手部运动，产生动态的头部运动，而静态机器人感知系统无法复制这些行为，导致策略性能下降。

Method: 开发了EgoMI框架，捕捉同步的末端执行器和主动头部轨迹；引入记忆增强策略，选择性整合历史观察来处理快速变化的头部视角。

Result: 在配备驱动相机头的双手机器人上评估，显示具有显式头部运动建模的策略始终优于基线方法。

Conclusion: EgoMI通过协调的手眼学习有效弥合了人机体现差距，为半人形机器人的稳健模仿学习提供了解决方案。

Abstract: Imitation learning from human demonstrations offers a promising approach for
robot skill acquisition, but egocentric human data introduces fundamental
challenges due to the embodiment gap. During manipulation, humans actively
coordinate head and hand movements, continuously reposition their viewpoint and
use pre-action visual fixation search strategies to locate relevant objects.
These behaviors create dynamic, task-driven head motions that static robot
sensing systems cannot replicate, leading to a significant distribution shift
that degrades policy performance. We present EgoMI (Egocentric Manipulation
Interface), a framework that captures synchronized end-effector and active head
trajectories during manipulation tasks, resulting in data that can be
retargeted to compatible semi-humanoid robot embodiments. To handle rapid and
wide-spanning head viewpoint changes, we introduce a memory-augmented policy
that selectively incorporates historical observations. We evaluate our approach
on a bimanual robot equipped with an actuated camera head and find that
policies with explicit head-motion modeling consistently outperform baseline
methods. Results suggest that coordinated hand-eye learning with EgoMI
effectively bridges the human-robot embodiment gap for robust imitation
learning on semi-humanoid embodiments. Project page:
https://egocentric-manipulation-interface.github.io

</details>


### [9] [Reducing Robotic Upper-Limb Assessment Time While Maintaining Precision: A Time Series Foundation Model Approach](https://arxiv.org/abs/2511.00193)
*Faranak Akbarifar,Nooshin Maghsoodi,Sean P Dukelow,Stephen Scott,Parvin Mousavi*

Main category: cs.RO

TL;DR: 使用时间序列基础模型预测未记录的伸手试验，可在仅使用8次实际试验加预测试验的情况下，达到与24-28次完整试验相当的可靠性，显著缩短Kinarm机器人视觉引导伸手评估时间。


<details>
  <summary>Details</summary>
Motivation: Kinarm机器人的视觉引导伸手评估需要40-64次伸手试验，给患者带来时间和疲劳负担，需要找到缩短评估时间的方法。

Method: 分析461名中风患者和599名对照者的VGR速度信号，仅使用前8或16次试验，通过ARIMA、MOMENT和Chronos模型预测合成试验，重新计算运动学特征并与完整试验参考值比较。

Result: Chronos模型预测在所有参数上恢复了ICC≥0.90的可靠性，仅需8次实际试验加预测试验即可达到24-28次完整试验的可靠性水平。

Conclusion: 基础模型预测可显著缩短Kinarm VGR评估时间，对最严重的中风患者，评估时间从4-5分钟降至约1分钟，同时保持运动学精度，为中风后运动障碍评估提供了高效的机器人评估范式。

Abstract: Purpose: Visually Guided Reaching (VGR) on the Kinarm robot yields sensitive
kinematic biomarkers but requires 40-64 reaches, imposing time and fatigue
burdens. We evaluate whether time-series foundation models can replace
unrecorded trials from an early subset of reaches while preserving the
reliability of standard Kinarm parameters.
  Methods: We analyzed VGR speed signals from 461 stroke and 599 control
participants across 4- and 8-target reaching protocols. We withheld all but the
first 8 or 16 reaching trials and used ARIMA, MOMENT, and Chronos models,
fine-tuned on 70 percent of subjects, to forecast synthetic trials. We
recomputed four kinematic features of reaching (reaction time, movement time,
posture speed, maximum speed) on combined recorded plus forecasted trials and
compared them to full-length references using ICC(2,1).
  Results: Chronos forecasts restored ICC >= 0.90 for all parameters with only
8 recorded trials plus forecasts, matching the reliability of 24-28 recorded
reaches (Delta ICC <= 0.07). MOMENT yielded intermediate gains, while ARIMA
improvements were minimal. Across cohorts and protocols, synthetic trials
replaced reaches without materially compromising feature reliability.
  Conclusion: Foundation-model forecasting can greatly shorten Kinarm VGR
assessment time. For the most impaired stroke survivors, sessions drop from 4-5
minutes to about 1 minute while preserving kinematic precision. This
forecast-augmented paradigm promises efficient robotic evaluations for
assessing motor impairments following stroke.

</details>


### [10] [Tailored robotic training improves hand function and proprioceptive processing in stroke survivors with proprioceptive deficits: A randomized controlled trial](https://arxiv.org/abs/2511.00259)
*Andria J. Farrens,Luis Garcia-Fernandez,Raymond Diaz Rojas,Jillian Obeso Estrada,Dylan Reinsdorf,Vicky Chan,Disha Gupta,Joel Perry,Eric Wolbrecht,An Do,Steven C. Cramer,David J. Reinkensmeyer*

Main category: cs.RO

TL;DR: 本研究测试了两种本体感觉定制的机器人训练方法对中风幸存者手部功能和神经处理的影响，发现针对本体感觉缺陷的定制训练能显著改善手部功能并增强神经敏感性。


<details>
  <summary>Details</summary>
Motivation: 精准康复旨在通过定制化运动训练改善康复效果。本研究探索本体感觉定制的机器人训练是否能改善中风幸存者的手部功能和神经处理能力。

Method: 使用机器人手指外骨骼，测试两种本体感觉定制方法：Propriopixel训练（通过机器人辅助的游戏化运动增强本体感觉处理）和虚拟辅助训练（减少机器人辅助以增加对自我生成反馈的依赖）。46名慢性中风幸存者随机接受标准训练、Propriopixel训练或虚拟训练，共完成9次2小时的训练。

Result: 在有本体感觉缺陷的参与者中，Propriopixel训练（Box and Block Test: 7±4.2, p=0.002）和虚拟辅助训练（4.5±4.4, p=0.068）比标准训练（0.8±2.3）带来更大的手部功能改善。本体感觉改善与手部功能进步相关。定制训练增强了神经对本体感觉线索的敏感性，通过新型EEG生物标志物（本体感觉相关负变化）得到证实。

Conclusion: 本体感觉定制的训练是精准神经康复的有效途径，能够改善手部功能并增强神经处理能力。

Abstract: Precision rehabilitation aims to tailor movement training to improve
outcomes. We tested whether proprioceptively-tailored robotic training improves
hand function and neural processing in stroke survivors. Using a robotic finger
exoskeleton, we tested two proprioceptively-tailored approaches: Propriopixel
Training, which uses robot-facilitated, gamified movements to enhance
proprioceptive processing, and Virtual Assistance Training, which reduces
robotic aid to increase reliance on self-generated feedback. In a randomized
controlled trial, forty-six chronic stroke survivors completed nine 2-hour
sessions of Standard, Propriopixel or Virtual training. Among participants with
proprioceptive deficits, Propriopixel ((Box and Block Test: 7 +/- 4.2, p=0.002)
and Virtual Assistance (4.5 +/- 4.4 , p=0.068) yielded greater gains in hand
function (Standard: 0.8 +/- 2.3 blocks). Proprioceptive gains correlated with
improvements in hand function. Tailored training enhanced neural sensitivity to
proprioceptive cues, evidenced by a novel EEG biomarker, the proprioceptive
Contingent Negative Variation. These findings support proprioceptively-tailored
training as a pathway to precision neurorehabilitation.

</details>


### [11] [FGO MythBusters: Explaining how Kalman Filter variants achieve the same performance as FGO in navigation applications](https://arxiv.org/abs/2511.00306)
*Baoshan Song,Ruijie Xu,Li-Ta Hsu*

Main category: cs.RO

TL;DR: 本文揭示了滑动窗口因子图优化(SW-FGO)与卡尔曼滤波器变体(KFV)之间的理论联系，提出了递归FGO(Re-FGO)框架，在特定条件下Re-FGO可精确再生为EKF/IEKF/REKF/RIEKF，同时阐明了SW-FGO在非线性非高斯场景中的优势。


<details>
  <summary>Details</summary>
Motivation: 虽然SW-FGO在导航研究中因对非高斯噪声和非线性测量模型的鲁棒性而受到关注，但其与EKF等卡尔曼滤波器变体的理论关系仍不明确，需要建立两者之间的理论连接。

Method: 提出了递归FGO(Re-FGO)框架来表示KFV在SW-FGO公式下的形式，基于马尔可夫假设、高斯噪声和L2损失等明确条件，在单状态窗口下建立连接。

Result: 在特定条件下，Re-FGO可以精确再生为EKF/IEKF/REKF/RIEKF，同时SW-FGO在非线性非高斯场景中展现出可衡量的优势，计算成本可预测。

Conclusion: 澄清了SW-FGO与KFV之间的关系，突出了SW-FGO在实际应用中的独特优势，特别是在数值估计和深度学习集成方面。

Abstract: Sliding window-factor graph optimization (SW-FGO) has gained more and more
attention in navigation research due to its robust approximation to
non-Gaussian noises and nonlinearity of measuring models. There are lots of
works focusing on its application performance compared to extended Kalman
filter (EKF) but there is still a myth at the theoretical relationship between
the SW-FGO and EKF. In this paper, we find the necessarily fair condition to
connect SW-FGO and Kalman filter variants (KFV) (e.g., EKF, iterative EKF
(IEKF), robust EKF (REKF) and robust iterative EKF (RIEKF)). Based on the
conditions, we propose a recursive FGO (Re-FGO) framework to represent KFV
under SW-FGO formulation. Under explicit conditions (Markov assumption,
Gaussian noise with L2 loss, and a one-state window), Re-FGO regenerates
exactly to EKF/IEKF/REKF/RIEKF, while SW-FGO shows measurable benefits in
nonlinear, non-Gaussian regimes at a predictable compute cost. Finally, after
clarifying the connection between them, we highlight the unique advantages of
SW-FGO in practical phases, especially on numerical estimation and deep
learning integration. The code and data used in this work is open sourced at
https://github.com/Baoshan-Song/KFV-FGO-Comparison.

</details>


### [12] [SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping](https://arxiv.org/abs/2511.00392)
*Lingpeng Chen,Jiakun Tang,Apple Pui-Yi Chui,Ziyang Hong,Junfeng Wu*

Main category: cs.RO

TL;DR: SonarSweep是一个端到端的深度学习框架，通过改进平面扫描算法实现声纳和视觉数据的跨模态融合，在视觉退化的水下环境中生成密集准确的深度图。


<details>
  <summary>Details</summary>
Motivation: 解决水下环境中3D重建的挑战：视觉方法因能见度差和几何约束失效，声纳方法存在高程模糊和低分辨率问题，现有融合技术依赖启发式和有缺陷的几何假设。

Method: 基于平面扫描算法原理，开发端到端深度学习框架进行声纳和视觉数据的跨模态融合。

Result: 在高保真仿真和真实环境中的广泛实验表明，SonarSweep能持续生成密集准确的深度图，在挑战性条件下（特别是高浑浊度）显著优于最先进方法。

Conclusion: SonarSweep克服了现有方法的限制，将公开代码和首个同步立体相机与声纳数据的新型数据集，以促进进一步研究。

Abstract: Accurate 3D reconstruction in visually-degraded underwater environments
remains a formidable challenge. Single-modality approaches are insufficient:
vision-based methods fail due to poor visibility and geometric constraints,
while sonar is crippled by inherent elevation ambiguity and low resolution.
Consequently, prior fusion technique relies on heuristics and flawed geometric
assumptions, leading to significant artifacts and an inability to model complex
scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep
learning framework that overcomes these limitations by adapting the principled
plane sweep algorithm for cross-modal fusion between sonar and visual data.
Extensive experiments in both high-fidelity simulation and real-world
environments demonstrate that SonarSweep consistently generates dense and
accurate depth maps, significantly outperforming state-of-the-art methods
across challenging conditions, particularly in high turbidity. To foster
further research, we will publicly release our code and a novel dataset
featuring synchronized stereo-camera and sonar data, the first of its kind.

</details>


### [13] [Runge-Kutta Approximations for Direct Coning Compensation Applying Lie Theory](https://arxiv.org/abs/2511.00412)
*John A. Christian,Michael R. Walker II,Wyatt Bridgman,Michael J. Sparapany*

Main category: cs.RO

TL;DR: 本文提出了一种基于经典龙格-库塔积分方法的新型圆锥误差补偿算法，展示了如何从标准积分方法推导出圆锥补偿算法，并提供了生成高阶算法的清晰步骤。


<details>
  <summary>Details</summary>
Motivation: 现代导航系统需要精确的陀螺仪积分，而圆锥误差补偿对于旋转传感器在积分过程中的运动至关重要。现有算法虽然多样，但需要更系统化的方法来生成高阶补偿算法。

Method: 直接从经典的龙格-库塔积分方法构建新的圆锥误差补偿算法类别，通过简单案例展示其可退化为最流行的圆锥算法，并提供了生成高阶算法的明确流程。

Result: 成功开发了一类新的圆锥校正算法，证明了该方法能够产生有效的圆锥补偿方案，并且可以系统性地扩展到更高阶算法。

Conclusion: 基于龙格-库塔积分方法构建圆锥补偿算法是可行的，该方法为导航系统中陀螺仪积分的高精度圆锥误差补偿提供了系统化的解决方案。

Abstract: The integration of gyroscope measurements is an essential task for most
navigation systems. Modern vehicles typically use strapdown systems, such that
gyro integration requires coning compensation to account for the sensor's
rotation during the integration. Many coning compensation algorithms have been
developed and a few are reviewed. This work introduces a new class of coning
correction algorithm built directly from the classical Runge-Kutta integration
routines. A simple case is shown to collapse to one of the most popular coning
algorithms and a clear procedure for generating higher-order algorithms is
presented.

</details>


### [14] [Design and Development of a Modular Bucket Drum Excavator for Lunar ISRU](https://arxiv.org/abs/2511.00492)
*Simon Giel,James Hurrell,Shreya Santra,Ashutosh Mishra,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 开发用于月球机器人系统MoonBot的挖掘工具原型，在沙盒测试中实现连续挖掘率777.54 kg/h和批量挖掘率172.02 kg/h


<details>
  <summary>Details</summary>
Motivation: 月球原位资源利用(ISRU)是实现月球可持续开发的关键技术，挖掘月壤是使月球资源可用的第一步

Method: 为日本Moonshot计划的模块化机器人系统MoonBot开发桶式挖掘工具，制造3D打印PLA原型，通过沙盒测试评估效率

Result: 工具重4.8kg，容积14.06L，连续挖掘率777.54 kg/h，能耗0.022 Wh/kg；批量挖掘率172.02 kg/h，能耗0.86 Wh/kg

Conclusion: 概念成功实现，工具与模块化MoonBot平台兼容，支持灵活高效的任务规划，未来可集成传感器和自主控制系统

Abstract: In-Situ Resource Utilization (ISRU) is one of the key technologies for
enabling sustainable access to the Moon. The ability to excavate lunar regolith
is the first step in making lunar resources accessible and usable. This work
presents the development of a bucket drum for the modular robotic system
MoonBot, as part of the Japanese Moonshot program. A 3D-printed prototype made
of PLA was manufactured to evaluate its efficiency through a series of sandbox
tests. The resulting tool weighs 4.8 kg and has a volume of 14.06 L. It is
capable of continuous excavation at a rate of 777.54 kg/h with a normalized
energy consumption of 0.022 Wh/kg. In batch operation, the excavation rate is
172.02 kg/h with a normalized energy consumption of 0.86 Wh per kilogram of
excavated material. The obtained results demonstrate the successful
implementation of the concept. A key advantage of the developed tool is its
compatibility with the modular MoonBot robotic platform, which enables flexible
and efficient mission planning. Further improvements may include the
integration of sensors and an autonomous control system to enhance the
excavation process.

</details>


### [15] [Descriptive Model-based Learning and Control for Bipedal Locomotion](https://arxiv.org/abs/2511.00512)
*Suraj Kumar,Andy Ruina*

Main category: cs.RO

TL;DR: 提出了一种新的双足平衡控制方法，避免将低维模型强加于完整模型，而是使用描述性模型约束最小必要的自由度来维持平衡，让其余自由度在高维空间中自由演化，从而实现高效的人形步态和更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统双足机器人平衡控制方法依赖低维模型进行运动规划和反应控制，这限制了完整机器人的行为，导致低效的弯曲膝盖行走模式。研究发现双足平衡本质上是低维的，可以用简单的状态和动作描述符在低维状态空间中有效描述。

Method: 提出控制框架使用描述性模型，仅约束维持平衡所需的最小自由度，允许其余自由度在高维空间中自由演化，避免将低维模型强加于完整模型。

Result: 该方法实现了高效的人形行走步态，并提高了系统的鲁棒性。

Conclusion: 通过仅约束维持平衡所需的最小自由度，允许其余自由度自由演化，可以实现更自然高效的双足行走，同时保持平衡能力。

Abstract: Bipedal balance is challenging due to its multi-phase, hybrid nature and
high-dimensional state space. Traditional balance control approaches for
bipedal robots rely on low-dimensional models for locomotion planning and
reactive control, constraining the full robot to behave like these simplified
models. This involves tracking preset reference paths for the Center of Mass
and upper body obtained through low-dimensional models, often resulting in
inefficient walking patterns with bent knees. However, we observe that bipedal
balance is inherently low-dimensional and can be effectively described with
simple state and action descriptors in a low-dimensional state space. This
allows the robot's motion to evolve freely in its high-dimensional state space,
only constraining its projection in the low-dimensional state space. In this
work, we propose a novel control approach that avoids prescribing a
low-dimensional model to the full model. Instead, our control framework uses a
descriptive model with the minimum degrees of freedom necessary to maintain
balance, allowing the remaining degrees of freedom to evolve freely in the
high-dimensional space. This results in an efficient human-like walking gait
and improved robustness.

</details>


### [16] [Adaptive and Multi-object Grasping via Deformable Origami Modules](https://arxiv.org/abs/2511.00516)
*Peiyi Wang,Paul A. M. Lefeuvre,Shangwei Zou,Zhenwei Ni,Daniela Rus,Cecilia Laschi*

Main category: cs.RO

TL;DR: 提出一种多指混合夹爪，采用被动可变形的折纸模块产生恒定力和扭矩输出，无需主动传感或反馈控制即可实现稳定抓取，并能同时抓取多个物体


<details>
  <summary>Details</summary>
Motivation: 现有软体机器人夹爪通常依赖笨重的执行器、复杂控制策略或先进触觉传感来实现稳定抓取，需要更简单可靠的解决方案

Method: 设计多指混合夹爪，每个手指由并行折纸模块组成，通过单自由度执行机构驱动，实现被动形状适应和稳定抓取力

Result: 夹爪能够同时抓取形状和尺寸各异的堆叠物体，在不同状态下独立拾取、运输和放置，显著提高操作效率

Conclusion: 折纸基柔性结构作为可扩展模块，在家庭和工业拾放场景中具有实现自适应、稳定和高效多物体操作的潜力

Abstract: Soft robotics gripper have shown great promise in handling fragile and
geometrically complex objects. However, most existing solutions rely on bulky
actuators, complex control strategies, or advanced tactile sensing to achieve
stable and reliable grasping performance. In this work, we present a
multi-finger hybrid gripper featuring passively deformable origami modules that
generate constant force and torque output. Each finger composed of parallel
origami modules is driven by a 1-DoF actuator mechanism, enabling passive shape
adaptability and stable grasping force without active sensing or feedback
control. More importantly, we demonstrate an interesting capability in
simultaneous multi-object grasping, which allows stacked objects of varied
shape and size to be picked, transported and placed independently at different
states, significantly improving manipulation efficiency compared to
single-object grasping. These results highlight the potential of origami-based
compliant structures as scalable modules for adaptive, stable and efficient
multi-object manipulation in domestic and industrial pick-and-place scenarios.

</details>


### [17] [Improving Robustness to Out-of-Distribution States in Imitation Learning via Deep Koopman-Boosted Diffusion Policy](https://arxiv.org/abs/2511.00555)
*Dianye Huang,Nassir Navab,Zhongliang Jiang*

Main category: cs.RO

TL;DR: 提出D3P算法，通过双分支架构解耦不同感官模态，结合深度Koopman算子增强视觉表征学习，在机器人操作任务中显著优于现有扩散策略。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的策略在捕捉多步骤间强时间依赖性方面存在困难，特别是结合本体感觉输入时容易过拟合，导致任务失败。

Method: 采用双分支架构：视觉分支编码任务进展，融合分支整合视觉和本体感觉输入；加入深度Koopman算子模块捕捉视觉输入的时序动态；使用生成模型的测试时损失作为置信度信号指导动作块聚合。

Result: 在6个RLBench桌面任务中平均优于最先进扩散策略14.6%；在3个真实世界机器人操作任务中提升15.0%。

Conclusion: D3P通过解耦感官模态和增强视觉表征学习，有效解决了现有方法在时序依赖性和模态过拟合方面的问题，显著提升了机器人模仿学习的性能。

Abstract: Integrating generative models with action chunking has shown significant
promise in imitation learning for robotic manipulation. However, the existing
diffusion-based paradigm often struggles to capture strong temporal
dependencies across multiple steps, particularly when incorporating
proprioceptive input. This limitation can lead to task failures, where the
policy overfits to proprioceptive cues at the expense of capturing the visually
derived features of the task. To overcome this challenge, we propose the Deep
Koopman-boosted Dual-branch Diffusion Policy (D3P) algorithm. D3P introduces a
dual-branch architecture to decouple the roles of different sensory modality
combinations. The visual branch encodes the visual observations to indicate
task progression, while the fused branch integrates both visual and
proprioceptive inputs for precise manipulation. Within this architecture, when
the robot fails to accomplish intermediate goals, such as grasping a drawer
handle, the policy can dynamically switch to execute action chunks generated by
the visual branch, allowing recovery to previously observed states and
facilitating retrial of the task. To further enhance visual representation
learning, we incorporate a Deep Koopman Operator module that captures
structured temporal dynamics from visual inputs. During inference, we use the
test-time loss of the generative model as a confidence signal to guide the
aggregation of the temporally overlapping predicted action chunks, thereby
enhancing the reliability of policy execution. In simulation experiments across
six RLBench tabletop tasks, D3P outperforms the state-of-the-art diffusion
policy by an average of 14.6\%. On three real-world robotic manipulation tasks,
it achieves a 15.0\% improvement. Code: https://github.com/dianyeHuang/D3P.

</details>


### [18] [Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles](https://arxiv.org/abs/2511.00635)
*Hyungtae Lim,Daebeom Kim,Hyun Myung*

Main category: cs.RO

TL;DR: 提出Multi-Mapcher框架，通过大规模地图到地图配准实现多会话LiDAR建图的初始对齐，替代传统依赖闭环检测的方法，提高了异构传感器场景下的建图性能。


<details>
  <summary>Details</summary>
Motivation: 现有MSS方法过度依赖闭环检测，但异构LiDAR传感器在点云密度和视场角上的差异会降低闭环检测性能，因此需要新的初始对齐方法。

Method: 采用大规模地图到地图配准进行会话间初始对齐，然后基于半径搜索发现会话间闭环，最后使用锚节点优化的位姿图构建全局一致地图。

Result: 实验表明该方法在各种LiDAR传感器上表现出更好的MSS性能，且比现有方法更快。

Conclusion: Multi-Mapcher框架通过地图到地图配准有效解决了异构LiDAR多会话建图的初始对齐问题，提升了建图精度和效率。

Abstract: As various 3D light detection and ranging (LiDAR) sensors have been
introduced to the market, research on multi-session simultaneous localization
and mapping (MSS) using heterogeneous LiDAR sensors has been actively
conducted. Existing MSS methods mostly rely on loop closure detection for
inter-session alignment; however, the performance of loop closure detection can
be potentially degraded owing to the differences in the density and field of
view (FoV) of the sensors used in different sessions. In this study, we
challenge the existing paradigm that relies heavily on loop detection modules
and propose a novel MSS framework, called Multi-Mapcher, that employs
large-scale map-to-map registration to perform inter-session initial alignment,
which is commonly assumed to be infeasible, by leveraging outlier-robust 3D
point cloud registration. Next, after finding inter-session loops by radius
search based on the assumption that the inter-session initial alignment is
sufficiently precise, anchor node-based robust pose graph optimization is
employed to build a consistent global map. As demonstrated in our experiments,
our approach shows substantially better MSS performance for various LiDAR
sensors used to capture the sessions and is faster than state-of-the-art
approaches. Our code is available at
https://github.com/url-kaist/multi-mapcher.

</details>


### [19] [When Semantics Connect the Swarm: LLM-Driven Fuzzy Control for Cooperative Multi-Robot Underwater Coverage](https://arxiv.org/abs/2511.00783)
*Jingzehua Xu,Weihang Zhang,Yangyang Li,Hongmiaoyi Zhang,Guanwen Xie,Jiwei Tang,Shuai Zhang,Yi Li*

Main category: cs.RO

TL;DR: 提出了一种语义引导的模糊控制框架，将大语言模型与可解释控制和轻量级协调相结合，解决水下多机器人协同覆盖问题。


<details>
  <summary>Details</summary>
Motivation: 解决水下多机器人协同覆盖面临的挑战：部分可观测性、有限通信、环境不确定性以及缺乏全局定位能力。

Method: 使用LLM将原始多模态观测压缩为紧凑的语义标记，通过模糊推理系统映射为平滑稳定的转向和步态命令，并引入语义通信实现多机器人协调。

Result: 在未知珊瑚礁环境中的广泛仿真表明，该框架在有限感知和通信条件下实现了稳健的OOI导向导航和协同覆盖，提高了效率和适应性。

Conclusion: 该框架缩小了语义认知与分布式水下控制之间的差距，适用于GPS缺失、无地图条件下的水下作业。

Abstract: Underwater multi-robot cooperative coverage remains challenging due to
partial observability, limited communication, environmental uncertainty, and
the lack of access to global localization. To address these issues, this paper
presents a semantics-guided fuzzy control framework that couples Large Language
Models (LLMs) with interpretable control and lightweight coordination. Raw
multimodal observations are compressed by the LLM into compact,
human-interpretable semantic tokens that summarize obstacles, unexplored
regions, and Objects Of Interest (OOIs) under uncertain perception. A fuzzy
inference system with pre-defined membership functions then maps these tokens
into smooth and stable steering and gait commands, enabling reliable navigation
without relying on global positioning. Then, we further coordinate multiple
robots by introducing semantic communication that shares intent and local
context in linguistic form, enabling agreement on who explores where while
avoiding redundant revisits. Extensive simulations in unknown reef-like
environments show that, under limited sensing and communication, the proposed
framework achieves robust OOI-oriented navigation and cooperative coverage with
improved efficiency and adaptability, narrowing the gap between semantic
cognition and distributed underwater control in GPS-denied, map-free
conditions.

</details>


### [20] [Real-Time Learning of Predictive Dynamic Obstacle Models for Robotic Motion Planning](https://arxiv.org/abs/2511.00814)
*Stella Kombo,Masih Haseli,Skylar Wei,Joel W. Burdick*

Main category: cs.RO

TL;DR: 提出了一种实时学习非线性运动预测模型的在线框架，使用改进的滑动窗口Hankel动态模态分解进行去噪和预测。


<details>
  <summary>Details</summary>
Motivation: 自主系统需要从部分噪声数据中预测附近智能体的运动，但现有方法难以在实时条件下学习非线性预测模型。

Method: 使用Hankel矩阵嵌入部分噪声测量，通过Page矩阵进行奇异值硬阈值处理估计有效秩，Cadzow投影确保结构化低秩一致性，构建时变Hankel-DMD提升线性预测器进行多步预测。

Result: 在模拟和动态起重机实验平台上验证，方法实现了稳定的方差感知去噪和适合实时控制集成的短时域预测。

Conclusion: 该方法能够有效处理高斯和重尾噪声，为下游估计器和风险感知规划提供方差跟踪信号。

Abstract: Autonomous systems often must predict the motions of nearby agents from
partial and noisy data. This paper asks and answers the question: "can we
learn, in real-time, a nonlinear predictive model of another agent's motions?"
Our online framework denoises and forecasts such dynamics using a modified
sliding-window Hankel Dynamic Mode Decomposition (Hankel-DMD). Partial noisy
measurements are embedded into a Hankel matrix, while an associated Page matrix
enables singular-value hard thresholding (SVHT) to estimate the effective rank.
A Cadzow projection enforces structured low-rank consistency, yielding a
denoised trajectory and local noise variance estimates. From this
representation, a time-varying Hankel-DMD lifted linear predictor is
constructed for multi-step forecasts. The residual analysis provides
variance-tracking signals that can support downstream estimators and risk-aware
planning. We validate the approach in simulation under Gaussian and
heavy-tailed noise, and experimentally on a dynamic crane testbed. Results show
that the method achieves stable variance-aware denoising and short-horizon
prediction suitable for integration into real-time control frameworks.

</details>


### [21] [Heuristic Step Planning for Learning Dynamic Bipedal Locomotion: A Comparative Study of Model-Based and Model-Free Approaches](https://arxiv.org/abs/2511.00840)
*William Suliman,Ekaterina Chaikovskaia,Egor Davydenko,Roman Gorbachev*

Main category: cs.RO

TL;DR: 提出了一种结合启发式步态规划策略的学习型双足机器人运动框架，通过期望躯干速度跟踪实现精确环境交互，在保持目标速度、不平坦地形鲁棒性和能效方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统基于完整或简化动力学的方法需要复杂的步态规划器和解析模型，作者希望开发一种避免这些复杂组件但仍能实现稳定鲁棒双足行走的方法。

Method: 使用启发式命令驱动步态规划，结合Raibert型控制器根据期望与实际躯干速度误差调节足部放置长度，与基于线性倒立摆模型(LIPM)的控制方法进行对比。

Result: 实验显示该方法在保持目标速度方面达到80%的精度，在不平坦地形上的鲁棒性提高50%以上，并具有更好的能效表现。

Conclusion: 研究表明在训练架构中融入复杂的解析模型组件对于实现稳定鲁棒的双足行走可能是不必要的，即使在非结构化环境中也是如此。

Abstract: This work presents an extended framework for learning-based bipedal
locomotion that incorporates a heuristic step-planning strategy guided by
desired torso velocity tracking. The framework enables precise interaction
between a humanoid robot and its environment, supporting tasks such as crossing
gaps and accurately approaching target objects. Unlike approaches based on full
or simplified dynamics, the proposed method avoids complex step planners and
analytical models. Step planning is primarily driven by heuristic commands,
while a Raibert-type controller modulates the foot placement length based on
the error between desired and actual torso velocity. We compare our method with
a model-based step-planning approach -- the Linear Inverted Pendulum Model
(LIPM) controller. Experimental results demonstrate that our approach attains
comparable or superior accuracy in maintaining target velocity (up to 80%),
significantly greater robustness on uneven terrain (over 50% improvement), and
improved energy efficiency. These results suggest that incorporating complex
analytical, model-based components into the training architecture may be
unnecessary for achieving stable and robust bipedal walking, even in
unstructured environments.

</details>


### [22] [Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots](https://arxiv.org/abs/2511.00917)
*Junyao Shi,Rujia Yang,Kaitian Chao,Selina Bingqing Wan,Yifei Shao,Jiahui Lei,Jianing Qian,Long Le,Pratik Chaudhari,Kostas Daniilidis,Chuan Wen,Dinesh Jayaraman*

Main category: cs.RO

TL;DR: Maestro是一个基于视觉语言模型的机器人系统，通过动态组合感知、规划和控制模块来构建通用策略，在零样本操作任务上超越现有VLA模型。


<details>
  <summary>Details</summary>
Motivation: 探索不同于传统大规模机器人数据集训练的路径，直接围绕视觉语言模型构建通用机器人策略，利用其通用能力结合特定机器人模块。

Method: 使用VLM编码代理动态组合感知、规划和控制模块为程序化策略，具有简化的闭环接口和多样化工具库。

Result: 在挑战性操作技能上显著超越现有VLA模型的零样本性能，易于扩展新模块，适应新机器人形态，并能通过本地代码编辑从少量真实世界经验中快速适应。

Conclusion: Maestro展示了基于VLM构建通用机器人策略的可行性，提供了一种灵活、可扩展且易于适应的替代方案。

Abstract: Today's best-explored routes towards generalist robots center on collecting
ever larger "observations-in actions-out" robotics datasets to train large
end-to-end models, copying a recipe that has worked for vision-language models
(VLMs). We pursue a road less traveled: building generalist policies directly
around VLMs by augmenting their general capabilities with specific robot
capabilities encapsulated in a carefully curated set of perception, planning,
and control modules. In Maestro, a VLM coding agent dynamically composes these
modules into a programmatic policy for the current task and scenario. Maestro's
architecture benefits from a streamlined closed-loop interface without many
manually imposed structural constraints, and a comprehensive and diverse tool
repertoire. As a result, it largely surpasses today's VLA models for zero-shot
performance on challenging manipulation skills. Further, Maestro is easily
extensible to incorporate new modules, easily editable to suit new embodiments
such as a quadruped-mounted arm, and even easily adapts from minimal real-world
experiences through local code edits.

</details>


### [23] [Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2511.00933)
*Xiangyu Shi,Zerui Li,Yanyuan Qiao,Qi Wu*

Main category: cs.RO

TL;DR: Fast-SmartWay是一个端到端的零样本视觉语言导航框架，仅使用三个前视RGB-D图像和自然语言指令，无需全景视图和路径点预测器，显著降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖全景观测和两阶段流水线，导致显著延迟并限制实际应用。

Method: 使用三个前视RGB-D图像结合语言指令，让MLLM直接预测动作；引入不确定性感知推理模块，包括消歧模块和未来-过去双向推理机制。

Result: 在模拟和真实机器人环境中显著降低每步延迟，同时达到或超过全景视图基线的性能。

Conclusion: Fast-SmartWay展示了实际可行性和有效性，适用于真实世界的零样本具身导航。

Abstract: Recent advances in Vision-and-Language Navigation in Continuous Environments
(VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve
zero-shot navigation. However, existing methods often rely on panoramic
observations and two-stage pipelines involving waypoint predictors, which
introduce significant latency and limit real-world applicability. In this work,
we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that
eliminates the need for panoramic views and waypoint predictors. Our approach
uses only three frontal RGB-D images combined with natural language
instructions, enabling MLLMs to directly predict actions. To enhance decision
robustness, we introduce an Uncertainty-Aware Reasoning module that integrates
(i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past
Bidirectional Reasoning mechanism for globally coherent planning. Experiments
on both simulated and real-robot environments demonstrate that our method
significantly reduces per-step latency while achieving competitive or superior
performance compared to panoramic-view baselines. These results demonstrate the
practicality and effectiveness of Fast-SmartWay for real-world zero-shot
embodied navigation.

</details>


### [24] [URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model](https://arxiv.org/abs/2511.00940)
*Zhe Li,Xiang Bai,Jieyu Zhang,Zhuangzhe Wu,Che Xu,Ying Li,Chengkai Hou,Shanghang Zhang*

Main category: cs.RO

TL;DR: URDF-Anything是一个基于3D多模态大语言模型的端到端自动重建框架，用于构建铰接物体的数字孪生，通过联合优化几何分割和运动学参数预测，显著提升了分割精度和物理可执行性。


<details>
  <summary>Details</summary>
Motivation: 构建精确的铰接物体数字孪生对机器人仿真训练和具身AI世界模型构建至关重要，但传统方法需要繁琐的手动建模或多阶段流程，因此需要开发端到端的自动重建方法。

Method: 采用基于点云和文本多模态输入的自回归预测框架，实现几何分割和运动学参数预测的联合优化，并设计了专门的[SEG]令牌机制与点云特征直接交互，实现细粒度部件级分割。

Result: 在仿真和真实数据集上的实验表明，该方法在几何分割（mIoU提升17%）、运动学参数预测（平均误差降低29%）和物理可执行性（超越基线50%）方面显著优于现有方法，且在训练集外物体上表现出优秀的泛化能力。

Conclusion: 该工作为机器人仿真构建数字孪生提供了高效解决方案，显著增强了从仿真到现实的迁移能力。

Abstract: Constructing accurate digital twins of articulated objects is essential for
robotic simulation training and embodied AI world model building, yet
historically requires painstaking manual modeling or multi-stage pipelines. In
this work, we propose \textbf{URDF-Anything}, an end-to-end automatic
reconstruction framework based on a 3D multimodal large language model (MLLM).
URDF-Anything utilizes an autoregressive prediction framework based on
point-cloud and text multimodal input to jointly optimize geometric
segmentation and kinematic parameter prediction. It implements a specialized
$[SEG]$ token mechanism that interacts directly with point cloud features,
enabling fine-grained part-level segmentation while maintaining consistency
with the kinematic parameter predictions. Experiments on both simulated and
real-world datasets demonstrate that our method significantly outperforms
existing approaches regarding geometric segmentation (mIoU 17\% improvement),
kinematic parameter prediction (average error reduction of 29\%), and physical
executability (surpassing baselines by 50\%). Notably, our method exhibits
excellent generalization ability, performing well even on objects outside the
training set. This work provides an efficient solution for constructing digital
twins for robotic simulation, significantly enhancing the sim-to-real transfer
capability.

</details>


### [25] [Breaking the Latency Barrier: Synergistic Perception and Control for High-Frequency 3D Ultrasound Servoing](https://arxiv.org/abs/2511.00983)
*Yizhao Qian,Yujie Zhu,Jiayuan Luo,Li Liu,Yixuan Yuan,Guochen Ning,Hongen Liao*

Main category: cs.RO

TL;DR: 提出了一种用于机器人超声系统的实时动态目标跟踪框架，通过感知与控制的协同设计，实现了超过60Hz的闭环控制频率，在复杂3D轨迹跟踪中平均误差低于6.5mm。


<details>
  <summary>Details</summary>
Motivation: 解决机器人超声系统中大规模高频干扰下动态目标实时跟踪的关键挑战，主要由于现有系统的端到端延迟问题。

Method: 包含两个紧密耦合的贡献：(1)解耦双流感知网络，从2D图像高频估计3D平移状态；(2)单步流策略，在一次推理中生成完整动作序列，绕过传统策略的迭代瓶颈。

Result: 在动态体模上，系统不仅能以低于6.5mm的平均误差跟踪复杂3D轨迹，还能从超过170mm的位移中稳健重新获取目标，以102mm/s速度跟踪目标时终端误差低于1.7mm。人体活体实验验证了框架的有效性和稳健性。

Conclusion: 该工作提出了一个整体架构的机器人超声系统，将高带宽跟踪与大规模重新定位统一起来，是迈向动态临床环境中稳健自主性的关键一步。

Abstract: Real-time tracking of dynamic targets amidst large-scale, high-frequency
disturbances remains a critical unsolved challenge in Robotic Ultrasound
Systems (RUSS), primarily due to the end-to-end latency of existing systems.
This paper argues that breaking this latency barrier requires a fundamental
shift towards the synergistic co-design of perception and control. We realize
it in a novel framework with two tightly-coupled contributions: (1) a Decoupled
Dual-Stream Perception Network that robustly estimates 3D translational state
from 2D images at high frequency, and (2) a Single-Step Flow Policy that
generates entire action sequences in one inference pass, bypassing the
iterative bottleneck of conventional policies. This synergy enables a
closed-loop control frequency exceeding 60Hz. On a dynamic phantom, our system
not only tracks complex 3D trajectories with a mean error below 6.5mm but also
demonstrates robust re-acquisition from over 170mm displacement. Furthermore,
it can track targets at speeds of 102mm/s, achieving a terminal error below
1.7mm. Moreover, in-vivo experiments on a human volunteer validate the
framework's effectiveness and robustness in a realistic clinical setting. Our
work presents a RUSS holistically architected to unify high-bandwidth tracking
with large-scale repositioning, a critical step towards robust autonomy in
dynamic clinical environments.

</details>


### [26] [GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies](https://arxiv.org/abs/2511.00998)
*Ziye Wang,Li Kang,Yiran Qin,Jiahua Ma,Zhanglin Peng,Lei Bai,Ruimao Zhang*

Main category: cs.RO

TL;DR: GauDP是一种用于多智能体协作系统的Gaussian-image协同表示方法，通过构建全局一致的3D高斯场并动态重分配属性到各智能体局部视角，实现可扩展的感知感知模仿学习。


<details>
  <summary>Details</summary>
Motivation: 解决具身多智能体系统中有效协调的挑战，特别是平衡个体视角与全局环境感知的问题。现有方法难以兼顾细粒度局部控制和全面场景理解，导致可扩展性有限和协作质量下降。

Method: 从分散的RGB观测构建全局一致的3D高斯场，然后动态地将3D高斯属性重新分配到每个智能体的局部视角，使所有智能体能够从共享场景表示中自适应查询任务关键特征。

Result: 在RoboFactory基准测试中，GauDP优于现有的基于图像的方法，接近点云驱动方法的有效性，同时在智能体数量增加时保持强大的可扩展性。

Conclusion: GauDP实现了细粒度控制和全局一致行为，无需额外的感知模态（如3D点云），为多智能体协作系统提供了一种有效的解决方案。

Abstract: Recently, effective coordination in embodied multi-agent systems has remained
a fundamental challenge, particularly in scenarios where agents must balance
individual perspectives with global environmental awareness. Existing
approaches often struggle to balance fine-grained local control with
comprehensive scene understanding, resulting in limited scalability and
compromised collaboration quality. In this paper, we present GauDP, a novel
Gaussian-image synergistic representation that facilitates scalable,
perception-aware imitation learning in multi-agent collaborative systems.
Specifically, GauDP constructs a globally consistent 3D Gaussian field from
decentralized RGB observations, then dynamically redistributes 3D Gaussian
attributes to each agent's local perspective. This enables all agents to
adaptively query task-critical features from the shared scene representation
while maintaining their individual viewpoints. This design facilitates both
fine-grained control and globally coherent behavior without requiring
additional sensing modalities (e.g., 3D point cloud). We evaluate GauDP on the
RoboFactory benchmark, which includes diverse multi-arm manipulation tasks. Our
method achieves superior performance over existing image-based methods and
approaches the effectiveness of point-cloud-driven methods, while maintaining
strong scalability as the number of agents increases.

</details>


### [27] [AquaROM: shape optimization pipeline for soft swimmers using parametric reduced order models](https://arxiv.org/abs/2511.01031)
*Mathieu Dubied,Paolo Tiso,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 提出基于张量参数化降阶模型(PROM)的优化算法，用于高效解决软体机器人非线性约束优化问题，显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 软体结构在复杂非线性力作用下的高效优化是机器人领域的关键挑战，传统有限元模拟计算成本高昂。

Method: 利用张量参数化降阶模型，结合维度缩减和求解近似技术，在特定降阶基(ROB)中使用解析梯度。

Result: 成功应用于软体游泳机器人形状优化，能够处理内外非线性力，实现快速准确计算。

Conclusion: 该方法不仅降低计算复杂度，还为软体机器人复杂非线性系统优化开辟了新途径，推动更高效的设计与控制。

Abstract: The efficient optimization of actuated soft structures, particularly under
complex nonlinear forces, remains a critical challenge in advancing robotics.
Simulations of nonlinear structures, such as soft-bodied robots modeled using
the finite element method (FEM), often demand substantial computational
resources, especially during optimization. To address this challenge, we
propose a novel optimization algorithm based on a tensorial parametric reduced
order model (PROM). Our algorithm leverages dimensionality reduction and
solution approximation techniques to facilitate efficient solving of nonlinear
constrained optimization problems. The well-structured tensorial approach
enables the use of analytical gradients within a specifically chosen reduced
order basis (ROB), significantly enhancing computational efficiency. To
showcase the performance of our method, we apply it to optimizing soft robotic
swimmer shapes. These actuated soft robots experience hydrodynamic forces,
subjecting them to both internal and external nonlinear forces, which are
incorporated into our optimization process using a data-free ROB for fast and
accurate computations. This approach not only reduces computational complexity
but also unlocks new opportunities to optimize complex nonlinear systems in
soft robotics, paving the way for more efficient design and control.

</details>


### [28] [Deployable Vision-driven UAV River Navigation via Human-in-the-loop Preference Alignment](https://arxiv.org/abs/2511.01083)
*Zihan Wang,Jianwen Li,Li-Fan Wu,Nina Mahmoudian*

Main category: cs.RO

TL;DR: SPAR-H是一种人机协同学习方法，通过融合直接偏好优化和基于奖励的路径，在无人机河流跟踪任务中实现高效在线适应，仅需5次人工干预就能获得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 无人机在河流环境监测中面临仿真训练到实际部署的分布偏移和安全风险，需要从有限人工干预中实现高效适应。

Method: 提出SPAR-H方法，结合直接偏好优化和基于奖励的路径，训练即时奖励估计器，并使用信任域代理更新策略。

Result: 仅用5次人工干预，SPAR-H在测试方法中获得最高最终回合奖励和最低方差，奖励模型与人类偏好一致。

Conclusion: 双重状态偏好为河流导航中的数据高效在线适应提供了实用途径。

Abstract: Rivers are critical corridors for environmental monitoring and disaster
response, where Unmanned Aerial Vehicles (UAVs) guided by vision-driven
policies can provide fast, low-cost coverage. However, deployment exposes
simulation-trained policies with distribution shift and safety risks and
requires efficient adaptation from limited human interventions. We study
human-in-the-loop (HITL) learning with a conservative overseer who vetoes
unsafe or inefficient actions and provides statewise preferences by comparing
the agent's proposal with a corrective override. We introduce Statewise Hybrid
Preference Alignment for Robotics (SPAR-H), which fuses direct preference
optimization on policy logits with a reward-based pathway that trains an
immediate-reward estimator from the same preferences and updates the policy
using a trust-region surrogate. With five HITL rollouts collected from a fixed
novice policy, SPAR-H achieves the highest final episodic reward and the lowest
variance across initial conditions among tested methods. The learned reward
model aligns with human-preferred actions and elevates nearby non-intervened
choices, supporting stable propagation of improvements. We benchmark SPAR-H
against imitation learning (IL), direct preference variants, and evaluative
reinforcement learning (RL) in the HITL setting, and demonstrate real-world
feasibility of continual preference alignment for UAV river following. Overall,
dual statewise preferences empirically provide a practical route to
data-efficient online adaptation in riverine navigation.

</details>


### [29] [SLAP: Shortcut Learning for Abstract Planning](https://arxiv.org/abs/2511.01107)
*Y. Isabel Liu,Bowen Li,Benjamin Eysenbach,Tom Silver*

Main category: cs.RO

TL;DR: SLAP方法通过结合任务和运动规划与无模型强化学习，自动发现新的抽象动作选项，显著缩短规划路径长度并提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决传统任务和运动规划中手动定义抽象动作的局限性，让智能体能够自动发现更有效的行为策略，而不仅限于人类工程师已知的可编程行为。

Method: 利用现有TAMP选项，在抽象规划图中使用无模型强化学习学习捷径，自动发现新的抽象动作选项。

Result: 在四个模拟机器人环境中，SLAP能够解决和泛化到广泛任务，将总体规划长度减少50%以上，在规划成功率和效率上持续优于规划和强化学习基线方法。

Conclusion: SLAP方法成功地将规划与学习相结合，通过自动发现抽象动作选项，显著提升了长时程决策的性能，并发现了与手动定义动作显著不同的动态物理即兴行为。

Abstract: Long-horizon decision-making with sparse rewards and continuous states and
actions remains a fundamental challenge in AI and robotics. Task and motion
planning (TAMP) is a model-based framework that addresses this challenge by
planning hierarchically with abstract actions (options). These options are
manually defined, limiting the agent to behaviors that we as human engineers
know how to program (pick, place, move). In this work, we propose Shortcut
Learning for Abstract Planning (SLAP), a method that leverages existing TAMP
options to automatically discover new ones. Our key idea is to use model-free
reinforcement learning (RL) to learn shortcuts in the abstract planning graph
induced by the existing options in TAMP. Without any additional assumptions or
inputs, shortcut learning leads to shorter solutions than pure planning, and
higher task success rates than flat and hierarchical RL. Qualitatively, SLAP
discovers dynamic physical improvisations (e.g., slap, wiggle, wipe) that
differ significantly from the manually-defined ones. In experiments in four
simulated robotic environments, we show that SLAP solves and generalizes to a
wide range of tasks, reducing overall plan lengths by over 50% and consistently
outperforming planning and RL baselines.

</details>


### [30] [An Enhanced Proprioceptive Method for Soft Robots Integrating Bend Sensors and IMUs](https://arxiv.org/abs/2511.01165)
*Dong Heon Han,Mayank Mehta,Runze Zuo,Zachary Wanger,Daniel Bruder*

Main category: cs.RO

TL;DR: 提出一种使用现成传感器的软体机器人形状估计方法，通过IMU和弯曲传感器融合来减少IMU漂移，实现长期可靠的形状感知。


<details>
  <summary>Details</summary>
Motivation: 解决软体机器人形状估计中IMU传感器漂移问题，开发成本效益高且易于应用的长期形状感知方法。

Method: 使用IMU和互补弯曲传感器，通过卡尔曼滤波器融合两种传感器的尖端方向数据，采用分段恒定曲率模型从融合的方向数据估计尖端位置并重建机器人变形。

Result: 在45分钟连续运行中，无负载、外部力和被动障碍物交互条件下，均方根误差为16.96毫米（总长度的2.91%），相比仅使用IMU的方法减少了56%。

Conclusion: 该方法不仅实现了软体机器人的长期形状感知，而且在各种条件下保持了高精度和鲁棒性。

Abstract: This study presents an enhanced proprioceptive method for accurate shape
estimation of soft robots using only off-the-shelf sensors, ensuring
cost-effectiveness and easy applicability. By integrating inertial measurement
units (IMUs) with complementary bend sensors, IMU drift is mitigated, enabling
reliable long-term proprioception. A Kalman filter fuses segment tip
orientations from both sensors in a mutually compensatory manner, improving
shape estimation over single-sensor methods. A piecewise constant curvature
model estimates the tip location from the fused orientation data and
reconstructs the robot's deformation. Experiments under no loading, external
forces, and passive obstacle interactions during 45 minutes of continuous
operation showed a root mean square error of 16.96 mm (2.91% of total length),
a 56% reduction compared to IMU-only benchmarks. These results demonstrate that
our approach not only enables long-duration proprioception in soft robots but
also maintains high accuracy and robustness across these diverse conditions.

</details>


### [31] [Scaling Cross-Embodiment World Models for Dexterous Manipulation](https://arxiv.org/abs/2511.01177)
*Zihao He,Bo Ai,Tongzhou Mu,Yulin Liu,Weikang Wan,Jiawei Fu,Yilun Du,Henrik I. Christensen,Hao Su*

Main category: cs.RO

TL;DR: 该论文提出了一种跨具身学习方法，通过3D粒子表示和基于图的世界模型来统一不同形态机器人的动作表示，利用环境动态的具身不变性实现策略迁移。


<details>
  <summary>Details</summary>
Motivation: 解决不同形态机器人之间由于动作空间和运动学差异导致的数据共享和策略迁移困难问题，探索是否存在跨具身的动作迁移不变性。

Method: 将不同具身表示为3D粒子集合，定义动作为粒子位移，构建共享表示；训练基于图的世界模型，整合模型规划用于新硬件部署。

Result: 实验表明：(1)增加训练具身数量能改善对未见具身的泛化；(2)模拟和真实数据联合训练优于单独训练；(3)学习模型能在不同自由度机器人上实现有效控制。

Conclusion: 世界模型是跨具身灵巧操作的有前景的统一接口，环境动态具有具身不变性，为构建通用机器人提供了可行路径。

Abstract: Cross-embodiment learning seeks to build generalist robots that operate
across diverse morphologies, but differences in action spaces and kinematics
hinder data sharing and policy transfer. This raises a central question: Is
there any invariance that allows actions to transfer across embodiments? We
conjecture that environment dynamics are embodiment-invariant, and that world
models capturing these dynamics can provide a unified interface across
embodiments. To learn such a unified world model, the crucial step is to design
state and action representations that abstract away embodiment-specific details
while preserving control relevance. To this end, we represent different
embodiments (e.g., human hands and robot hands) as sets of 3D particles and
define actions as particle displacements, creating a shared representation for
heterogeneous data and control problems. A graph-based world model is then
trained on exploration data from diverse simulated robot hands and real human
hands, and integrated with model-based planning for deployment on novel
hardware. Experiments on rigid and deformable manipulation tasks reveal three
findings: (i) scaling to more training embodiments improves generalization to
unseen ones, (ii) co-training on both simulated and real data outperforms
training on either alone, and (iii) the learned models enable effective control
on robots with varied degrees of freedom. These results establish world models
as a promising interface for cross-embodiment dexterous manipulation.

</details>


### [32] [LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping](https://arxiv.org/abs/2511.01186)
*Lijie Wang,Lianjie Guo,Ziyi Xu,Qianhao Wang,Fei Gao,Xieyuanli Chen*

Main category: cs.RO

TL;DR: 提出LiDAR-VGGT框架，通过两阶段粗到精融合管道紧密耦合LiDAR惯性里程计与VGGT模型，解决VGGT在大规模环境中可扩展性差和缺乏度量尺度的问题，实现稠密、全局一致的彩色点云重建。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR惯性视觉里程计(LIVO)对外部标定高度敏感，而3D视觉基础模型VGGT在大规模环境中可扩展性有限且缺乏度量尺度，需要克服这些限制。

Method: 两阶段粗到精融合管道：预融合模块通过鲁棒初始化细化高效估计VGGT位姿和点云；后融合模块使用基于边界框的正则化增强跨模态3D相似变换，减少LiDAR和相机传感器间FOV不一致导致的尺度失真。

Result: 在多个数据集上的广泛实验表明，LiDAR-VGGT实现了稠密、全局一致的彩色点云，性能优于VGGT方法和LIVO基线。

Conclusion: 提出的LiDAR-VGGT框架有效解决了VGGT在大规模环境中的可扩展性和度量尺度问题，实现了优越的彩色点云重建性能，并将发布开源的颜色点云评估工具包。

Abstract: Reconstructing large-scale colored point clouds is an important task in
robotics, supporting perception, navigation, and scene understanding. Despite
advances in LiDAR inertial visual odometry (LIVO), its performance remains
highly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation
models, such as VGGT, suffer from limited scalability in large environments and
inherently lack metric scale. To overcome these limitations, we propose
LiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with
the state-of-the-art VGGT model through a two-stage coarse- to-fine fusion
pipeline: First, a pre-fusion module with robust initialization refinement
efficiently estimates VGGT poses and point clouds with coarse metric scale
within each session. Then, a post-fusion module enhances cross-modal 3D
similarity transformation, using bounding-box-based regularization to reduce
scale distortions caused by inconsistent FOVs between LiDAR and camera sensors.
Extensive experiments across multiple datasets demonstrate that LiDAR-VGGT
achieves dense, globally consistent colored point clouds and outperforms both
VGGT-based methods and LIVO baselines. The implementation of our proposed novel
color point cloud evaluation toolkit will be released as open source.

</details>


### [33] [Closed-loop Control of Steerable Balloon Endoscopes for Robot-assisted Transcatheter Intracardiac Procedures](https://arxiv.org/abs/2511.01199)
*Max McCandless,Jonathan Hamid,Sammy Elmariah,Nathaniel Langer,Pierre E. Dupont*

Main category: cs.RO

TL;DR: 提出了一种可操纵球囊式心脏镜，通过单一输入（球囊充气压力）独立控制球囊直径和弯曲角度，用于心脏内可视化及工具输送。


<details>
  <summary>Details</summary>
Motivation: 为了从开胸手术转向更安全的经导管手术，需要改进成像技术和机器人解决方案，以实现简单准确的手术工具导航。

Method: 设计可操纵球囊式心脏镜，通过精确设计球囊壁厚度，利用单一充气压力独立控制球囊直径和弯曲角度，集成工作通道用于工具输送。

Result: 开发出可调节的心脏镜技术，适用于多种心内任务，并展示了用于主动脉瓣叶撕裂的特定设计，实现了基于图像的闭环弯曲角度控制。

Conclusion: 该球囊技术可定制化设计用于不同心内任务，通过闭环控制实现稳定的方向控制，为经导管手术提供改进的成像和导航能力。

Abstract: To move away from open-heart surgery towards safer transcatheter procedures,
there is a growing need for improved imaging techniques and robotic solutions
to enable simple, accurate tool navigation. Common imaging modalities, such as
fluoroscopy and ultrasound, have limitations that can be overcome using
cardioscopy, i.e., direct optical visualization inside the beating heart. We
present a cardioscope designed as a steerable balloon. As a balloon, it can be
collapsed to pass through the vasculature and subsequently inflated inside the
heart for visualization and tool delivery through an integrated working
channel. Through careful design of balloon wall thickness, a single input,
balloon inflation pressure, is used to independently control two outputs,
balloon diameter (corresponding to field of view diameter) and balloon bending
angle (enabling precise working channel positioning). This balloon technology
can be tuned to produce cardioscopes designed for a range of intracardiac
tasks. To illustrate this approach, a balloon design is presented for the
specific task of aortic leaflet laceration. Image-based closed-loop control of
bending angle is also demonstrated as a means of enabling stable orientation
control during tool insertion and removal.

</details>


### [34] [Tackling the Kidnapped Robot Problem via Sparse Feasible Hypothesis Sampling and Reliable Batched Multi-Stage Inference](https://arxiv.org/abs/2511.01219)
*Muhua Zhang,Lei Ma,Ying Wu,Kai Shen,Deqing Huang,Henry Leung*

Main category: cs.RO

TL;DR: 提出一个被动2D全局重定位框架，解决机器人绑架问题，通过单次LiDAR扫描和占据栅格地图高效可靠地估计全局位姿，采用多假设方案平衡完整性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决机器人绑架问题(KRP)，即在已知地图中重新定位机器人而无需先验位姿估计，这对于SLAM初始化或定位丢失时增强移动机器人的长期自主性至关重要。

Method: 使用多假设方案，通过RRT在可达空间中生成稀疏均匀的位置假设，用SMAD指标初步排序假设并实现早期终止，提出TAM指标进行可靠的方向选择和最终位姿评估。

Result: 在资源受限的移动机器人上进行真实世界实验，证明该框架在全局重定位成功率和计算效率方面均优于现有方法。

Conclusion: 该被动2D全局重定位框架能够高效可靠地解决机器人绑架问题，显著提升了移动机器人的长期自主能力。

Abstract: This paper addresses the Kidnapped Robot Problem (KRP), a core localization
challenge of relocalizing a robot in a known map without prior pose estimate
when localization loss or at SLAM initialization. For this purpose, a passive
2-D global relocalization framework is proposed. It estimates the global pose
efficiently and reliably from a single LiDAR scan and an occupancy grid map
while the robot remains stationary, thereby enhancing the long-term autonomy of
mobile robots. The proposed framework casts global relocalization as a
non-convex problem and solves it via the multi-hypothesis scheme with batched
multi-stage inference and early termination, balancing completeness and
efficiency. The Rapidly-exploring Random Tree (RRT), under traversability
constraints, asymptotically covers the reachable space to generate sparse,
uniformly distributed feasible positional hypotheses, fundamentally reducing
the sampling space. The hypotheses are preliminarily ordered by the proposed
Scan Mean Absolute Difference (SMAD), a coarse beam-error level metric that
facilitates the early termination by prioritizing high-likelihood candidates.
The SMAD computation is optimized for non-panoramic scans. And the
Translation-Affinity Scan-to-Map Alignment Metric (TAM) is proposed for
reliable orientation selection at hypothesized positions and accurate final
pose evaluation to mitigate degradation in conventional likelihood-field
metrics under translational uncertainty induced by sparse hypotheses, as well
as non-panoramic LiDAR scan and environmental changes. Real-world experiments
on a resource-constrained mobile robot with non-panoramic LiDAR scan
demonstrate that the proposed framework outperforms existing methods in both
global relocalization success rate and computational efficiency.

</details>


### [35] [Embodiment Transfer Learning for Vision-Language-Action Models](https://arxiv.org/abs/2511.01224)
*Chengmeng Li,Yaxin Peng*

Main category: cs.RO

TL;DR: ET-VLA框架通过合成持续预训练和具身思维图技术，有效解决多机器人协作问题，在真实任务中性能提升超过53.2%


<details>
  <summary>Details</summary>
Motivation: 现有的自回归VLA模型在多机器人协作方面表现不佳，需要一种能够高效迁移预训练模型到多机器人系统的框架

Method: 提出ET-VLA框架，包含合成持续预训练(SCP)来预热模型学习新具身形态，以及具身思维图技术来区分不同具身形态的功能角色

Result: 在三个不同双手机器人具身形态的仿真基准和真实机器人上验证有效性，在六个真实世界任务中比OpenVLA性能提升53.2%

Conclusion: ET-VLA框架能够高效地将预训练VLA模型迁移到多机器人系统，显著提升多机器人协作性能

Abstract: Vision-language-action (VLA) models have significantly advanced robotic
learning, enabling training on large-scale, cross-embodiment data and
fine-tuning for specific robots. However, state-of-the-art autoregressive VLAs
struggle with multi-robot collaboration. We introduce embodiment transfer
learning, denoted as ET-VLA, a novel framework for efficient and effective
transfer of pre-trained VLAs to multi-robot. ET-VLA's core is Synthetic
Continued Pretraining (SCP), which uses synthetically generated data to warm up
the model for the new embodiment, bypassing the need for real human
demonstrations and reducing data collection costs. SCP enables the model to
learn correct actions and precise action token numbers. Following SCP, the
model is fine-tuned on target embodiment data. To further enhance the model
performance on multi-embodiment, we present the Embodied Graph-of-Thought
technique, a novel approach that formulates each sub-task as a node, that
allows the VLA model to distinguish the functionalities and roles of each
embodiment during task execution. Our work considers bimanual robots, a simple
version of multi-robot to verify our approaches. We validate the effectiveness
of our method on both simulation benchmarks and real robots covering three
different bimanual embodiments. In particular, our proposed ET-VLA \space can
outperform OpenVLA on six real-world tasks over 53.2%. We will open-source all
codes to support the community in advancing VLA models for robot learning.

</details>


### [36] [High-Precision Surgical Robotic System for Intraocular Procedures](https://arxiv.org/abs/2511.01232)
*Yu-Ting Lai,Jacob Rosen,Yasamin Foroutani,Ji Ma,Wen-Cheng Wu,Jean-Pierre Hubschman,Tsu-Chin Tsao*

Main category: cs.RO

TL;DR: 开发了一种新型机器人系统，用于提高眼科手术中的工具尖端精度、跟踪性能和平滑工具交换机制，在OCT引导下实现了高精度的白内障晶状体摘除手术。


<details>
  <summary>Details</summary>
Motivation: 现有白内障和玻璃体视网膜手术机器人系统在精度、自由度和工具交换方面存在不足，需要开发更准确、更灵活的手术机器人。

Method: 设计制造了新型机器人系统，通过机器人校准和精确坐标配准，使用OCT系统评估工具尖端精度和远程运动中心保持小切口的能力，结合深度学习术前解剖建模和实时监督。

Result: 工具尖端定位精度达到0.053±0.031毫米，成功演示了OCT引导的自动化白内障晶状体摘除手术。

Conclusion: 该机器人系统显著提高了眼科手术的精度和自动化水平，为复杂眼科手术提供了可靠的技术支持。

Abstract: Despite the extensive demonstration of robotic systems for both cataract and
vitreoretinal procedures, existing technologies or mechanisms still possess
insufficient accuracy, precision, and degrees of freedom for instrument
manipulation or potentially automated tool exchange during surgical procedures.
A new robotic system that focuses on improving tooltip accuracy, tracking
performance, and smooth instrument exchange mechanism is therefore designed and
manufactured. Its tooltip accuracy, precision, and mechanical capability of
maintaining small incision through remote center of motion were externally
evaluated using an optical coherence tomography (OCT) system. Through robot
calibration and precise coordinate registration, the accuracy of tooltip
positioning was measured to be 0.053$\pm$0.031 mm, and the overall performance
was demonstrated on an OCT-guided automated cataract lens extraction procedure
with deep learning-based pre-operative anatomical modeling and real-time
supervision.

</details>


### [37] [Don't Just Search, Understand: Semantic Path Planning Agent for Spherical Tensegrity Robots in Unknown Environments](https://arxiv.org/abs/2511.01236)
*Junwen Zhang,Changyue Liu,Pengqi Fu,Xiang Guo,Ye Shi,Xudong Liang,Zhijian Wang,Hanzhi Ma*

Main category: cs.RO

TL;DR: SATPlanner：基于大语言模型的球形张拉整体机器人语义路径规划器，通过自适应观测窗口机制实现高效探索和鲁棒规划，在未知环境中搜索空间比A*算法减少37.2%，成功率100%。


<details>
  <summary>Details</summary>
Motivation: 传统路径规划器将环境视为几何网格，在复杂场景中容易失败且缺乏语义理解。球形张拉整体机器人在未知环境中的路径规划需要平衡高效探索和鲁棒规划。

Method: 将路径规划重构为语义推理任务，提出SATPlanner框架，采用基于LLM的自适应观测窗口机制，动态调整感知范围：在开阔空间缩小窗口快速穿越，在复杂障碍配置时扩大窗口进行推理。

Result: 在1000次仿真试验中达到100%成功率，搜索空间比A*算法减少37.2%，同时保持接近最优的路径长度。在物理原型机器人上验证了可行性。

Conclusion: SATPlanner通过语义理解和自适应观测机制，有效解决了球形张拉整体机器人在未知环境中的路径规划问题，实现了线性增长的搜索空间和高质量的路径规划。

Abstract: Endowed with inherent dynamical properties that grant them remarkable
ruggedness and adaptability, spherical tensegrity robots stand as prototypical
examples of hybrid softrigid designs and excellent mobile platforms. However,
path planning for these robots in unknown environments presents a significant
challenge, requiring a delicate balance between efficient exploration and
robust planning. Traditional path planners, which treat the environment as a
geometric grid, often suffer from redundant searches and are prone to failure
in complex scenarios due to their lack of semantic understanding. To overcome
these limitations, we reframe path planning in unknown environments as a
semantic reasoning task. We introduce a Semantic Agent for Tensegrity robots
(SATPlanner) driven by a Large Language Model (LLM). SATPlanner leverages
high-level environmental comprehension to generate efficient and reliable
planning strategies.At the core of SATPlanner is an Adaptive Observation Window
mechanism, inspired by the "fast" and "slow" thinking paradigms of LLMs. This
mechanism dynamically adjusts the perceptual field of the agent: it narrows for
rapid traversal of open spaces and expands to reason about complex obstacle
configurations. This allows the agent to construct a semantic belief of the
environment, enabling the search space to grow only linearly with the path
length (O(L)) while maintaining path quality. We extensively evaluate
SATPlanner in 1,000 simulation trials, where it achieves a 100% success rate,
outperforming other real-time planning algorithms. Critically, SATPlanner
reduces the search space by 37.2% compared to the A* algorithm while achieving
comparable, near-optimal path lengths. Finally, the practical feasibility of
SATPlanner is validated on a physical spherical tensegrity robot prototype.

</details>


### [38] [Improving Needle Penetration via Precise Rotational Insertion Using Iterative Learning Control](https://arxiv.org/abs/2511.01256)
*Yasamin Foroutani,Yasamin Mousavi-Motlagh,Aya Barzelay,Tsu-Chin Tsao*

Main category: cs.RO

TL;DR: 提出一种迭代学习控制策略，用于机器人手术中工具的精确旋转插入，相比直线插入提高了穿透效果和安全性


<details>
  <summary>Details</summary>
Motivation: 机器人工具路径的精确控制面临系统错位、未建模动力学和执行不准确性等挑战，特别是在视网膜下注射等精细手术中

Method: 使用4自由度机器人操纵器，通过前向运动学校准提高精度，然后基于OCT体积扫描的反馈进行迭代学习控制来调整关节指令

Result: 在离体猪眼视网膜下注射实验中，优化后的轨迹相比直线插入在组织穿透和视网膜下注射方面获得了更高的成功率

Conclusion: ILC方法能有效克服错位挑战，在需要精确控制插入的其他高精度机器人任务中具有应用潜力

Abstract: Achieving precise control of robotic tool paths is often challenged by
inherent system misalignments, unmodeled dynamics, and actuation inaccuracies.
This work introduces an Iterative Learning Control (ILC) strategy to enable
precise rotational insertion of a tool during robotic surgery, improving
penetration efficacy and safety compared to straight insertion tested in
subretinal injection. A 4 degree of freedom (DOF) robot manipulator is used,
where misalignment of the fourth joint complicates the simple application of
needle rotation, motivating an ILC approach that iteratively adjusts joint
commands based on positional feedback. The process begins with calibrating the
forward kinematics for the chosen surgical tool to achieve higher accuracy,
followed by successive ILC iterations guided by Optical Coherence Tomography
(OCT) volume scans to measure the error and refine control inputs. Experimental
results, tested on subretinal injection tasks on ex vivo pig eyes, show that
the optimized trajectory resulted in higher success rates in tissue penetration
and subretinal injection compared to straight insertion, demonstrating the
effectiveness of ILC in overcoming misalignment challenges. This approach
offers potential applications for other high precision robot tasks requiring
controlled insertions as well.

</details>


### [39] [Design and Fabrication of Origami-Inspired Knitted Fabrics for Soft Robotics](https://arxiv.org/abs/2511.01272)
*Sehui Jeong,Magaly C. Aviles,Athena X. Naylor,Cynthia Sung,Allison M. Okamura*

Main category: cs.RO

TL;DR: 提出了一种将折纸结构与针织织物相结合的新方法，通过编程针迹和材料图案来制造可穿戴软机器人，实现了结构完整性和舒适性的平衡。


<details>
  <summary>Details</summary>
Motivation: 软机器人使用柔性材料为可穿戴设备提供了舒适性和安全性，但如何在保持结构完整性的同时确保舒适性仍是一个挑战。

Method: 结合折纸结构的优势与针织织物的可编程性和可穿戴性，通过选择性加入热熔纱线在柔性折痕周围创建刚性面板，编程针迹和材料图案来控制折叠方向。

Result: 成功复制了复杂的折纸镶嵌图案（Miura-ori、Yoshimura、Kresling），并制造出能够运动的可穿戴针织万花筒循环机器人，实验量化了折叠力矩。

Conclusion: 针织折纸结合了结构可重构性、材料可编程性和制造可扩展性，是下一代可穿戴机器人技术的有前景平台。

Abstract: Soft robots employing compliant materials and deformable structures offer
great potential for wearable devices that are comfortable and safe for human
interaction. However, achieving both structural integrity and compliance for
comfort remains a significant challenge. In this study, we present a novel
fabrication and design method that combines the advantages of origami
structures with the material programmability and wearability of knitted
fabrics. We introduce a general design method that translates origami patterns
into knit designs by programming both stitch and material patterns. The method
creates folds in preferred directions while suppressing unintended buckling and
bending by selectively incorporating heat fusible yarn to create rigid panels
around compliant creases. We experimentally quantify folding moments and show
that stitch patterning enhances folding directionality while the heat fusible
yarn (1) keeps geometry consistent by reducing edge curl and (2) prevents
out-of-plane deformations by stiffening panels. We demonstrate the framework
through the successful reproduction of complex origami tessellations, including
Miura-ori, Yoshimura, and Kresling patterns, and present a wearable knitted
Kaleidocycle robot capable of locomotion. The combination of structural
reconfigurability, material programmability, and potential for manufacturing
scalability highlights knitted origami as a promising platform for
next-generation wearable robotics.

</details>


### [40] [Contact Map Transfer with Conditional Diffusion Model for Generalizable Dexterous Grasp Generation](https://arxiv.org/abs/2511.01276)
*Yiyao Ma,Kai Chen,Kexin Zheng,Qi Dou*

Main category: cs.RO

TL;DR: 提出了一种基于条件扩散模型的灵巧抓取生成框架，通过将高质量抓取从形状模板转移到同类新物体，解决了抓取稳定性和任务适应性的挑战。


<details>
  <summary>Details</summary>
Motivation: 灵巧抓取生成需要平衡抓取稳定性和任务适应性。分析方法稳定但效率低且缺乏任务适应性，生成方法效率高但泛化能力差。需要一种能结合两者优势的方法。

Method: 使用条件扩散模型将抓取转移问题重新定义为物体接触图生成问题。引入双映射机制处理复杂形状变化，并开发级联条件扩散模型框架联合转移三个地图（接触图、部件图、方向图）。

Result: 实验证明该方法在抓取质量、生成效率和泛化性能方面表现优越，有效平衡了各种任务需求。

Conclusion: 该方法成功解决了灵巧抓取生成中的稳定性和适应性平衡问题，为机器人抓取提供了高效且泛化能力强的解决方案。

Abstract: Dexterous grasp generation is a fundamental challenge in robotics, requiring
both grasp stability and adaptability across diverse objects and tasks.
Analytical methods ensure stable grasps but are inefficient and lack task
adaptability, while generative approaches improve efficiency and task
integration but generalize poorly to unseen objects and tasks due to data
limitations. In this paper, we propose a transfer-based framework for dexterous
grasp generation, leveraging a conditional diffusion model to transfer
high-quality grasps from shape templates to novel objects within the same
category. Specifically, we reformulate the grasp transfer problem as the
generation of an object contact map, incorporating object shape similarity and
task specifications into the diffusion process. To handle complex shape
variations, we introduce a dual mapping mechanism, capturing intricate
geometric relationship between shape templates and novel objects. Beyond the
contact map, we derive two additional object-centric maps, the part map and
direction map, to encode finer contact details for more stable grasps. We then
develop a cascaded conditional diffusion model framework to jointly transfer
these three maps, ensuring their intra-consistency. Finally, we introduce a
robust grasp recovery mechanism, identifying reliable contact points and
optimizing grasp configurations efficiently. Extensive experiments demonstrate
the superiority of our proposed method. Our approach effectively balances grasp
quality, generation efficiency, and generalization performance across various
tasks. Project homepage: https://cmtdiffusion.github.io/

</details>


### [41] [A High-Speed Capable Spherical Robot](https://arxiv.org/abs/2511.01288)
*Bixuan Zhang,Fengqi Zhang,Haojie Chen,You Wang,Jie Hao,Zhiyuan Luo,Guang Li*

Main category: cs.RO

TL;DR: 设计了一种新型球形机器人结构，通过增加与二级摆对齐的动量轮，实现了高达10m/s的高速稳定运动，并显著提升了越障能力和地形适应性。


<details>
  <summary>Details</summary>
Motivation: 基于单摆驱动球形机器人，旨在突破原有结构无法实现稳定高速运动的限制，提升机器人的运动性能和实用性。

Method: 在单摆驱动球形机器人基础上，增加一个与二级摆轴线对齐的动量轮，形成新型球形机器人结构，采用简单的解耦控制方法。

Result: 物理样机实验证明，该新型球形机器人能够实现稳定高速运动（最高10m/s），同时显著提升了越障性能和地形鲁棒性。

Conclusion: 提出的新型球形机器人结构成功解决了高速稳定运动的难题，通过简单的控制方法实现了性能的全面提升，为球形机器人的实际应用开辟了新途径。

Abstract: This paper designs a new spherical robot structure capable of supporting
high-speed motion at up to 10 m/s. Building upon a single-pendulum-driven
spherical robot, the design incorporates a momentum wheel with an axis aligned
with the secondary pendulum, creating a novel spherical robot structure.
Practical experiments with the physical prototype have demonstrated that this
new spherical robot can achieve stable high-speed motion through simple
decoupled control, which was unattainable with the original structure. The
spherical robot designed for high-speed motion not only increases speed but
also significantly enhances obstacle-crossing performance and terrain
robustness.

</details>


### [42] [Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects](https://arxiv.org/abs/2511.01294)
*Jiawei Wang,Dingyou Wang,Jiaming Hu,Qixuan Zhang,Jingyi Yu,Lan Xu*

Main category: cs.RO

TL;DR: Kinematify是一个从RGB图像或文本提示自动合成关节物体的框架，解决了高自由度物体的运动学拓扑推断和关节参数估计的挑战。


<details>
  <summary>Details</summary>
Motivation: 关节物体建模对于机器人操作和物理仿真至关重要，但现有方法依赖运动序列或手工数据集，难以扩展到复杂系统。

Method: 结合MCTS搜索进行结构推断和几何驱动的优化进行关节推理，生成物理一致且功能有效的描述。

Result: 在合成和真实环境中的多样化输入上评估，显示在配准和运动学拓扑准确性方面优于先前工作。

Conclusion: Kinematify提供了一种可扩展的方法，能够从静态图像或文本自动生成关节物体模型。

Abstract: A deep understanding of kinematic structures and movable components is
essential for enabling robots to manipulate objects and model their own
articulated forms. Such understanding is captured through articulated objects,
which are essential for tasks such as physical simulation, motion planning, and
policy learning. However, creating these models, particularly for complex
systems like robots or objects with high degrees of freedom (DoF), remains a
significant challenge. Existing methods typically rely on motion sequences or
strong assumptions from hand-curated datasets, which hinders scalability. In
this paper, we introduce Kinematify, an automated framework that synthesizes
articulated objects directly from arbitrary RGB images or text prompts. Our
method addresses two core challenges: (i) inferring kinematic topologies for
high-DoF objects and (ii) estimating joint parameters from static geometry. To
achieve this, we combine MCTS search for structural inference with
geometry-driven optimization for joint reasoning, producing physically
consistent and functionally valid descriptions. We evaluate Kinematify on
diverse inputs from both synthetic and real-world environments, demonstrating
improvements in registration and kinematic topology accuracy over prior work.

</details>


### [43] [RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models](https://arxiv.org/abs/2511.01331)
*Hongyin Zhang,Shuo Zhang,Junxi Jin,Qixin Zeng,Runze Li,Donglin Wang*

Main category: cs.RO

TL;DR: RobustVLA是一种轻量级在线RL后训练方法，通过雅可比正则化和平滑正则化增强VLA模型在观测噪声和动作扰动下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在分布外部署时对观测噪声、传感器误差和动作扰动等环境不确定性缺乏鲁棒性，而现有的RL后训练方法主要关注奖励最大化，忽视了环境不确定性的影响。

Method: 提出RobustVLA方法，包含两个关键正则化：雅可比正则化降低对观测噪声的敏感性，平滑正则化在动作扰动下稳定策略。

Result: 在多种机器人环境中的实验表明，RobustVLA在鲁棒性和可靠性方面显著优于现有最先进方法。

Conclusion: 基于原则的鲁棒性感知RL后训练是提高VLA模型可靠性和鲁棒性的关键步骤。

Abstract: Vision-Language-Action (VLA) models have recently emerged as powerful
general-purpose policies for robotic manipulation, benefiting from large-scale
multi-modal pre-training. However, they often fail to generalize reliably in
out-of-distribution deployments, where unavoidable disturbances such as
observation noise, sensor errors, or actuation perturbations become prevalent.
While recent Reinforcement Learning (RL)-based post-training provides a
practical means to adapt pre-trained VLA models, existing methods mainly
emphasize reward maximization and overlook robustness to environmental
uncertainty. In this work, we introduce RobustVLA, a lightweight online RL
post-training method designed to explicitly enhance the resilience of VLA
models. Through a systematic robustness analysis, we identify two key
regularizations: Jacobian regularization, which mitigates sensitivity to
observation noise, and smoothness regularization, which stabilizes policies
under action perturbations. Extensive experiments across diverse robotic
environments demonstrate that RobustVLA significantly outperforms prior
state-of-the-art methods in robustness and reliability. Our results highlight
the importance of principled robustness-aware RL post-training as a key step
toward improving the reliability and robustness of VLA models.

</details>


### [44] [Embodied Cognition Augmented End2End Autonomous Driving](https://arxiv.org/abs/2511.01334)
*Ling Niu,Xiaoji Zheng,Han Wang,Chen Zheng,Ziyuan Yang,Bokui Chen,Jiangtao Gong*

Main category: cs.RO

TL;DR: 提出E³AD新范式，通过视觉特征提取网络与EEG大模型对比学习，学习人类驾驶认知来增强端到端规划性能


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法依赖标签监督训练的视觉特征提取网络，这种有限监督框架限制了驾驶模型的通用性和适用性

Method: 收集认知数据集进行对比学习，研究利用人类驾驶认知增强端到端规划的方法和机制，在公开自动驾驶数据集上使用流行驾驶模型作为基线

Result: 实验结果表明E³AD范式显著提升了基线模型的端到端规划性能，消融研究验证了驾驶认知的贡献和对比学习过程的有效性

Conclusion: 这是首个将人类驾驶认知整合到端到端自动驾驶规划中的工作，为未来脑启发自动驾驶系统提供了宝贵见解

Abstract: In recent years, vision-based end-to-end autonomous driving has emerged as a
new paradigm. However, popular end-to-end approaches typically rely on visual
feature extraction networks trained under label supervision. This limited
supervision framework restricts the generality and applicability of driving
models. In this paper, we propose a novel paradigm termed $E^{3}AD$, which
advocates for comparative learning between visual feature extraction networks
and the general EEG large model, in order to learn latent human driving
cognition for enhancing end-to-end planning. In this work, we collected a
cognitive dataset for the mentioned contrastive learning process. Subsequently,
we investigated the methods and potential mechanisms for enhancing end-to-end
planning with human driving cognition, using popular driving models as
baselines on publicly available autonomous driving datasets. Both open-loop and
closed-loop tests are conducted for a comprehensive evaluation of planning
performance. Experimental results demonstrate that the $E^{3}AD$ paradigm
significantly enhances the end-to-end planning performance of baseline models.
Ablation studies further validate the contribution of driving cognition and the
effectiveness of comparative learning process. To the best of our knowledge,
this is the first work to integrate human driving cognition for improving
end-to-end autonomous driving planning. It represents an initial attempt to
incorporate embodied cognitive data into end-to-end autonomous driving,
providing valuable insights for future brain-inspired autonomous driving
systems. Our code will be made available at Github

</details>


### [45] [Thermo-responsive closing and reopening artificial Venus Flytrap utilizing shape memory elastomers](https://arxiv.org/abs/2511.01346)
*Shun Yoshida,Qingchuan Song,Bastian E. Rapp,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: 开发了一种能够自主闭合和重新张开的人造捕蝇草系统，使用热响应形状记忆材料实现双向运动


<details>
  <summary>Details</summary>
Motivation: 虽然捕蝇草的快速闭合运动已被广泛研究，但将其闭合和重新张开的双向运动转化为自主的植物启发软机器尚未实现

Method: 使用新型热响应UV固化形状记忆材料构建软机器人系统，通过形状记忆聚合物实现闭合，形状记忆弹性体条带作为拮抗执行器实现重新张开

Result: 制造出与真实尺寸相当的捕蝇草，在38°C时闭合，45°C时重新张开，在自然温度范围内实现程序化顺序运动

Conclusion: 这是首个展示热响应闭合和重新张开的人造捕蝇草系统，代表了向自主双向移动软机器发展的下一步

Abstract: Despite their often perceived static and slow nature, some plants can move
faster than the blink of an eye. The rapid snap closure motion of the Venus
flytrap (Dionaea muscipula) has long captivated the interest of researchers and
engineers alike, serving as a model for plant-inspired soft machines and
robots. The translation of the fast snapping closure has inspired the
development of various artificial Venus flytrap (AVF) systems. However,
translating both the closing and reopening motion of D. muscipula into an
autonomous plant inspired soft machine has yet to be achieved. In this study,
we present an AVF that autonomously closes and reopens, utilizing novel
thermo-responsive UV-curable shape memory materials for soft robotic systems.
The life-sized thermo-responsive AVF exhibits closing and reopening motions
triggered in a naturally occurring temperature range. The doubly curved trap
lobes, built from shape memory polymers, close at 38{\deg}C, while reopening
initiates around 45{\deg}C, employing shape memory elastomer strips as
antagonistic actuators to facilitate lobe reopening. This work represents the
first demonstration of thermo-responsive closing and reopening in an AVF with
programmed sequential motion in response to increasing temperature. This
approach marks the next step toward autonomously bidirectional moving soft
machines/robots.

</details>


### [46] [Design and development of an electronics-free earthworm robot](https://arxiv.org/abs/2511.01347)
*Riddhi Das,Joscha Teichmann,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: 提出了一种基于改进气动逻辑门(PLG)设计的无电子元件、仿蚯蚓气动机器人，实现了无需外部电子控制的蠕动运动


<details>
  <summary>Details</summary>
Motivation: 现有仿蚯蚓机器人主要依赖气动驱动，但通常需要笨重、高功耗的电子控制单元，限制了实际应用。本研究旨在开发无电子控制的气动机器人系统

Method: 将预配置的PLG单元与波纹管执行器集成，构建即插即用式模块化系统，通过改进的PLG设计实现蠕动波传播

Result: 改进的PLG控制系统有效生成蠕动波传播，实现自主运动且偏差最小。波纹管执行器在不同工况下进行了表征测试

Conclusion: 该研究为开发无电子元件的蠕动软体机器人提供了概念验证，在危险环境中具有应用潜力，未来将优化设计并探索使用机载压缩空气源的无缆操作

Abstract: Soft robotic systems have gained widespread attention due to their inherent
flexibility, adaptability, and safety, making them well-suited for varied
applications. Among bioinspired designs, earthworm locomotion has been
extensively studied for its efficient peristaltic motion, enabling movement in
confined and unstructured environments. Existing earthworm-inspired robots
primarily utilize pneumatic actuation due to its high force-to-weight ratio and
ease of implementation. However, these systems often rely on bulky,
power-intensive electronic control units, limiting their practicality. In this
work, we present an electronics-free, earthworm-inspired pneumatic robot
utilizing a modified Pneumatic Logic Gate (PLG) design. By integrating
preconfigured PLG units with bellow actuators, we achieved a plug-and-play
style modular system capable of peristaltic locomotion without external
electronic components. The proposed design reduces system complexity while
maintaining efficient actuation. We characterize the bellow actuators under
different operating conditions and evaluate the robots locomotion performance.
Our findings demonstrate that the modified PLG-based control system effectively
generates peristaltic wave propagation, achieving autonomous motion with
minimal deviation. This study serves as a proof of concept for the development
of electronics-free, peristaltic soft robots. The proposed system has potential
for applications in hazardous environments, where untethered, adaptable
locomotion is critical. Future work will focus on further optimizing the robot
design and exploring untethered operation using onboard compressed air sources.

</details>


### [47] [Model to Model: Understanding the Venus Flytrap Snapping Mechanism and Transferring it to a 3D-printed Bistable Soft Robotic Demonstrator](https://arxiv.org/abs/2511.01350)
*Maartje H. M. Wermelink,Renate Sachse,Sebastian Kruppert,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: 该研究分析了捕蝇草的快速闭合机制，并基于其几何特征和双稳态特性设计制造了人工双稳态执行器，模拟了捕蝇叶片的机械行为。


<details>
  <summary>Details</summary>
Motivation: 深入理解捕蝇草的运动力学原理，并将其应用于人工双稳态执行器的设计，开发能够模拟生物模型机械行为的软快速抓取器。

Method: 识别捕蝇草叶片的几何特征（尺寸比例、厚度梯度），将其转化为两种3D打印的双稳态执行器模型：一种模拟捕蝇草叶片几何形状，另一种使用CAD设计的叶片模型。

Result: 两种模型都表现出凹-凸双稳态特性并能快速闭合，这是开发人工捕蝇草的第一步。

Conclusion: 成功将捕蝇草的双稳态机制应用于人工执行器设计，为开发软快速抓取器奠定了基础。

Abstract: The Venus flytrap (Dionaea muscipula) does not only serve as the textbook
model for a carnivorous plant, but also has long intrigued both botanists and
engineers with its rapidly closing leaf trap. The trap closure is triggered by
two consecutive touches of a potential prey, after which the lobes rapidly
switch from their concave open-state to their convex close-state and catch the
prey within 100-500 ms after being triggered. This transformation from concave
to convex is initiated by changes in turgor pressure and the release of stored
elastic energy from prestresses in the concave state, which accelerate this
movement, leading to inversion of the lobes bi-axial curvature. Possessing two
low-energy states, the leaves can be characterized as bistable systems. With
our research, we seek to deepen the understanding of Venus flytrap motion
mechanics and apply its principles to the design of an artificial bistable lobe
actuator. We identified geometrical characteristics, such as dimensional ratios
and the thickness gradient in the lobe, and transferred these to two 3D-printed
bistable actuator models. One actuator parallels the simulated geometry of a
Venus flytrap leaf, the other is a lobe model designed with CAD. Both models
display concave-convex bi-stability and snap close. These demonstrators are the
first step in the development of an artificial Venus flytrap that mimics the
mechanical behavior of the biological model and can be used as a soft fast
gripper.

</details>


### [48] [Lateral Velocity Model for Vehicle Parking Applications](https://arxiv.org/abs/2511.01369)
*Luis Diener,Jens Kalkkuhl,Markus Enzweiler*

Main category: cs.RO

TL;DR: 提出了一种改进的横向速度模型，用于解决自动泊车中横向速度估计的挑战，该模型仅需两个参数，更适合消费级车辆应用。


<details>
  <summary>Details</summary>
Motivation: 自动泊车需要精确的定位，但消费级车辆缺乏专门的横向速度传感器，现有方法基于零滑移假设在低速驾驶时存在系统偏差。

Method: 分析真实泊车场景数据，识别零滑移假设的系统偏差，提出仅需两个参数的横向速度模型来更好地捕捉泊车过程中的横向动力学。

Result: 该模型提高了估计精度，同时仅依赖两个参数，适合集成到消费级应用中。

Conclusion: 提出的横向速度模型能更准确地捕捉车辆在泊车过程中的横向动力学，解决了现有零滑移假设的局限性。

Abstract: Automated parking requires accurate localization for quick and precise
maneuvering in tight spaces. While the longitudinal velocity can be measured
using wheel encoders, the estimation of the lateral velocity remains a key
challenge due to the absence of dedicated sensors in consumer-grade vehicles.
Existing approaches often rely on simplified vehicle models, such as the
zero-slip model, which assumes no lateral velocity at the rear axle. It is well
established that this assumption does not hold during low-speed driving and
researchers thus introduce additional heuristics to account for differences. In
this work, we analyze real-world data from parking scenarios and identify a
systematic deviation from the zero-slip assumption. We provide explanations for
the observed effects and then propose a lateral velocity model that better
captures the lateral dynamics of the vehicle during parking. The model improves
estimation accuracy, while relying on only two parameters, making it
well-suited for integration into consumer-grade applications.

</details>


### [49] [CM-LIUW-Odometry: Robust and High-Precision LiDAR-Inertial-UWB-Wheel Odometry for Extreme Degradation Coal Mine Tunnels](https://arxiv.org/abs/2511.01379)
*Kun Hu,Menggang Li,Zhiwen Jin,Chaoquan Tang,Eryi Hu,Gongbo Zhou*

Main category: cs.RO

TL;DR: 提出CM-LIUW-Odometry多模态SLAM框架，融合LiDAR、IMU、UWB和轮式里程计，解决煤矿地下环境中GPS缺失、地形恶劣和特征贫乏的定位挑战。


<details>
  <summary>Details</summary>
Motivation: 解决煤矿地下环境中SLAM面临的三大挑战：GPS不可用导致全局定位困难、恶劣地形降低轮式里程计精度、长隧道特征贫乏影响LiDAR性能。

Method: 基于迭代误差状态卡尔曼滤波(IESKF)，紧密融合LiDAR-IMU里程计与UWB绝对定位约束，集成轮式里程计并增强非完整约束和车辆杠杆臂补偿，采用自适应运动模式切换机制。

Result: 在真实煤矿地下场景中验证了方法的优越精度和鲁棒性，超越了现有最先进方法。

Conclusion: 提出的多模态SLAM框架在恶劣煤矿环境中表现出色，代码已开源供机器人社区使用。

Abstract: Simultaneous Localization and Mapping (SLAM) in large-scale, complex, and
GPS-denied underground coal mine environments presents significant challenges.
Sensors must contend with abnormal operating conditions: GPS unavailability
impedes scene reconstruction and absolute geographic referencing, uneven or
slippery terrain degrades wheel odometer accuracy, and long, feature-poor
tunnels reduce LiDAR effectiveness. To address these issues, we propose
CoalMine-LiDAR-IMU-UWB-Wheel-Odometry (CM-LIUW-Odometry), a multimodal SLAM
framework based on the Iterated Error-State Kalman Filter (IESKF). First,
LiDAR-inertial odometry is tightly fused with UWB absolute positioning
constraints to align the SLAM system with a global coordinate. Next, wheel
odometer is integrated through tight coupling, enhanced by nonholonomic
constraints (NHC) and vehicle lever arm compensation, to address performance
degradation in areas beyond UWB measurement range. Finally, an adaptive motion
mode switching mechanism dynamically adjusts the robot's motion mode based on
UWB measurement range and environmental degradation levels. Experimental
results validate that our method achieves superior accuracy and robustness in
real-world underground coal mine scenarios, outperforming state-of-the-art
approaches. We open source our code of this work on Github to benefit the
robotics community.

</details>


### [50] [CaRLi-V: Camera-RADAR-LiDAR Point-Wise 3D Velocity Estimation](https://arxiv.org/abs/2511.01383)
*Landson Guo,Andres M. Diaz Aguilar,William Talbot,Turcan Tuna,Marco Hutter,Cesar Cadena*

Main category: cs.RO

TL;DR: 提出CaRLi-V，一种融合RADAR、LiDAR和相机的新型3D点速度估计方法，通过速度立方体、光流和LiDAR测量实现密集点云3D速度估计。


<details>
  <summary>Details</summary>
Motivation: 精确的3D点速度估计对于机器人与非刚性动态物体（如人类）的交互至关重要，能提升动态环境中的路径规划、避障和物体操作性能。

Method: 结合RADAR原始测量创建速度立方体表示径向速度，使用光流估计切向速度，LiDAR提供点云距离测量，通过闭式解融合三种传感器数据。

Result: 在自定义数据集上测试显示相对于真实值的低速度误差，能够为密集点云提供3D速度估计。

Conclusion: CaRLi-V作为开源ROS2包，能够有效实现点级3D速度估计，适用于机器人应用。

Abstract: Accurate point-wise velocity estimation in 3D is crucial for robot
interaction with non-rigid, dynamic agents, such as humans, enabling robust
performance in path planning, collision avoidance, and object manipulation in
dynamic environments. To this end, this paper proposes a novel RADAR, LiDAR,
and camera fusion pipeline for point-wise 3D velocity estimation named CaRLi-V.
This pipeline leverages raw RADAR measurements to create a novel RADAR
representation, the velocity cube, which densely represents radial velocities
within the RADAR's field-of-view. By combining the velocity cube for radial
velocity extraction, optical flow for tangential velocity estimation, and LiDAR
for point-wise range measurements through a closed-form solution, our approach
can produce 3D velocity estimates for a dense array of points. Developed as an
open-source ROS2 package, CaRLi-V has been field-tested against a custom
dataset and proven to produce low velocity error metrics relative to ground
truth, enabling point-wise velocity estimation for robotic applications.

</details>


### [51] [FoldPath: End-to-End Object-Centric Motion Generation via Modulated Implicit Paths](https://arxiv.org/abs/2511.01407)
*Paolo Rabino,Gabriele Tiboni,Tatiana Tommasi*

Main category: cs.RO

TL;DR: FoldPath是一种新颖的端到端神经场方法，用于对象中心运动生成(OCMG)，通过连续函数学习机器人运动，无需后处理步骤，在工业环境中仅需70个专家样本即可实现泛化。


<details>
  <summary>Details</summary>
Motivation: 当前OCMG技术要么基于临时启发式方法，要么采用仍依赖敏感后处理步骤的学习管道来生成可执行路径，需要更鲁棒的算法来生成复杂3D几何上的扩展、对象感知轨迹。

Method: FoldPath是一种基于神经场的端到端方法，将机器人运动作为连续函数学习，隐式编码平滑输出路径，而不是预测离散的末端执行器路径点序列。

Result: 该方法在预测性能上优于最近提出的基于学习的方法，在真实工业环境中仅使用70个专家样本就实现了泛化能力，并通过真实仿真环境中的综合实验进行了验证。

Conclusion: FoldPath通过消除脆弱的后处理步骤，将OCMG任务推向实际成熟，并引入了新的严格指标来全面评估长时程机器人路径。

Abstract: Object-Centric Motion Generation (OCMG) is instrumental in advancing
automated manufacturing processes, particularly in domains requiring
high-precision expert robotic motions, such as spray painting and welding. To
realize effective automation, robust algorithms are essential for generating
extended, object-aware trajectories across intricate 3D geometries. However,
contemporary OCMG techniques are either based on ad-hoc heuristics or employ
learning-based pipelines that are still reliant on sensitive post-processing
steps to generate executable paths. We introduce FoldPath, a novel, end-to-end,
neural field based method for OCMG. Unlike prior deep learning approaches that
predict discrete sequences of end-effector waypoints, FoldPath learns the robot
motion as a continuous function, thus implicitly encoding smooth output paths.
This paradigm shift eliminates the need for brittle post-processing steps that
concatenate and order the predicted discrete waypoints. Particularly, our
approach demonstrates superior predictive performance compared to recently
proposed learning-based methods, and attains generalization capabilities even
in real industrial settings, where only a limited amount of 70 expert samples
are provided. We validate FoldPath through comprehensive experiments in a
realistic simulation environment and introduce new, rigorous metrics designed
to comprehensively evaluate long-horizon robotic paths, thus advancing the OCMG
task towards practical maturity.

</details>


### [52] [Designing for Distributed Heterogeneous Modularity: On Software Architecture and Deployment of MoonBots](https://arxiv.org/abs/2511.01437)
*Elian Neppel,Shamistan Karimov,Ashutosh Mishra,Gustavo Hernan Diaz Huenupan,Hazal Gozbasi,Kentaro Uno,Shreya Santra,Kazuya Yoshida*

Main category: cs.RO

TL;DR: MoonBot平台是一个模块化空间机器人系统，采用分布式异构架构，通过ROS2和Zenoh实现数据导向通信，支持动态重构和去中心化控制，已在自组装机器人和远程操作中验证。


<details>
  <summary>Details</summary>
Motivation: 解决模块化机器人系统在集成和维护方面的重大挑战，为跨时间、硬件、团队和操作环境的机器人系统设计提供可推广的模式。

Method: 采用组件化设计、基于ROS2和Zenoh的数据导向通信模型，以及能够管理复杂多模块组件的部署编排器。

Result: 系统经过数月现场部署验证，支持自组装机器人、机器人间协作和远程操作，显著降低了集成和维护开销。

Conclusion: 该架构虽然针对太空应用设计，但提出了可推广的机器人系统设计模式，具有可扩展性和鲁棒性。

Abstract: This paper presents the software architecture and deployment strategy behind
the MoonBot platform: a modular space robotic system composed of heterogeneous
components distributed across multiple computers, networks and ultimately
celestial bodies. We introduce a principled approach to distributed,
heterogeneous modularity, extending modular robotics beyond physical
reconfiguration to software, communication and orchestration. We detail the
architecture of our system that integrates component-based design, a
data-oriented communication model using ROS2 and Zenoh, and a deployment
orchestrator capable of managing complex multi-module assemblies. These
abstractions enable dynamic reconfiguration, decentralized control, and
seamless collaboration between numerous operators and modules. At the heart of
this system lies our open-source Motion Stack software, validated by months of
field deployment with self-assembling robots, inter-robot cooperation, and
remote operation. Our architecture tackles the significant hurdles of modular
robotics by significantly reducing integration and maintenance overhead, while
remaining scalable and robust. Although tested with space in mind, we propose
generalizable patterns for designing robotic systems that must scale across
time, hardware, teams and operational environments.

</details>


### [53] [AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models](https://arxiv.org/abs/2511.01472)
*Sarthak Mishra,Rishabh Dev Yadav,Avirup Das,Saksham Gupta,Wei Pan,Spandan Roy*

Main category: cs.RO

TL;DR: AERMANI-VLM是首个将预训练视觉语言模型适配到空中机械臂控制的框架，通过分离高层推理和低层控制，无需任务特定微调即可确保安全可靠的操作。


<details>
  <summary>Details</summary>
Motivation: 直接部署视觉语言模型驱动的策略到空中机械臂上存在安全问题，因为生成的动作往往不一致、易产生幻觉且飞行动态不可行。

Method: 使用结构化提示编码自然语言指令、任务上下文和安全约束，引导模型生成逐步推理轨迹，然后从预定义的安全技能库中选择动作。

Result: 在仿真和硬件实验中验证了框架在多样化多步骤拾放任务中的有效性，展示了对未见指令、物体和环境的强泛化能力。

Conclusion: 通过解耦符号推理和物理动作，AERMANI-VLM能够减轻幻觉命令并防止不安全行为，实现鲁棒的任务完成。

Abstract: The rapid progress of vision--language models (VLMs) has sparked growing
interest in robotic control, where natural language can express the operation
goals while visual feedback links perception to action. However, directly
deploying VLM-driven policies on aerial manipulators remains unsafe and
unreliable since the generated actions are often inconsistent,
hallucination-prone, and dynamically infeasible for flight. In this work, we
present AERMANI-VLM, the first framework to adapt pretrained VLMs for aerial
manipulation by separating high-level reasoning from low-level control, without
any task-specific fine-tuning. Our framework encodes natural language
instructions, task context, and safety constraints into a structured prompt
that guides the model to generate a step-by-step reasoning trace in natural
language. This reasoning output is used to select from a predefined library of
discrete, flight-safe skills, ensuring interpretable and temporally consistent
execution. By decoupling symbolic reasoning from physical action, AERMANI-VLM
mitigates hallucinated commands and prevents unsafe behavior, enabling robust
task completion. We validate the framework in both simulation and hardware on
diverse multi-step pick-and-place tasks, demonstrating strong generalization to
previously unseen commands, objects, and environments.

</details>


### [54] [MO-SeGMan: Rearrangement Planning Framework for Multi Objective Sequential and Guided Manipulation in Constrained Environments](https://arxiv.org/abs/2511.01476)
*Cankut Bora Tuncer,Marc Toussaint,Ozgur S. Oguz*

Main category: cs.RO

TL;DR: MO-SeGMan是一个多目标序列化引导操作规划器，用于解决高度受限的重排问题。它通过最小化重规划次数和机器人移动距离，同时保持关键依赖结构，在复杂场景中高效生成可行的运动规划方案。


<details>
  <summary>Details</summary>
Motivation: 解决高度受限、非单调的重排规划问题，传统方法在处理复杂依赖关系和密集环境时效率低下，需要开发能够同时优化多个目标的高效规划器。

Method: 提出选择性引导前向搜索(SGFS)方法，仅重定位关键障碍物到可行位置；采用自适应子目标选择的细化方法消除不必要的拾放动作；使用惰性评估方法保持关键依赖结构。

Result: 在9个基准重排任务上的广泛评估表明，MO-SeGMan在所有情况下都能生成可行的运动规划，相比基线方法获得更快的求解时间和更优的解决方案质量。

Conclusion: MO-SeGMan框架在复杂重排规划问题中展现出强大的鲁棒性和可扩展性，为高度受限环境下的操作规划提供了有效解决方案。

Abstract: In this work, we introduce MO-SeGMan, a Multi-Objective Sequential and Guided
Manipulation planner for highly constrained rearrangement problems. MO-SeGMan
generates object placement sequences that minimize both replanning per object
and robot travel distance while preserving critical dependency structures with
a lazy evaluation method. To address highly cluttered, non-monotone scenarios,
we propose a Selective Guided Forward Search (SGFS) that efficiently relocates
only critical obstacles and to feasible relocation points. Furthermore, we
adopt a refinement method for adaptive subgoal selection to eliminate
unnecessary pick-and-place actions, thereby improving overall solution quality.
Extensive evaluations on nine benchmark rearrangement tasks demonstrate that
MO-SeGMan generates feasible motion plans in all cases, consistently achieving
faster solution times and superior solution quality compared to the baselines.
These results highlight the robustness and scalability of the proposed
framework for complex rearrangement planning problems.

</details>


### [55] [Floor Plan-Guided Visual Navigation Incorporating Depth and Directional Cues](https://arxiv.org/abs/2511.01493)
*Wei Huang,Jiaxin Li,Zang Wan,Huijun Di,Wei Liang,Zhu Yang*

Main category: cs.RO

TL;DR: 提出GlocDiff，一种基于扩散的导航策略，结合楼层平面图的全局路径规划和RGB观测的局部深度特征，解决室内导航中视觉与空间信息的整合问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在室内导航中存在两个挑战：1）第一人称RGB观测与楼层平面图之间的模态差异阻碍了视觉和空间信息的整合；2）在未见环境中由于RGB输入与楼层平面图缺乏显式几何对齐，精确定位仍然困难。

Method: GlocDiff整合楼层平面图的全局路径规划和RGB观测的局部深度感知特征，通过扩散模型预测最优导航方向。在训练中引入噪声扰动增强对姿态估计误差的鲁棒性，在推理时结合相对稳定的视觉里程计模块。

Result: 在FloNa基准测试上的广泛实验表明，GlocDiff实现了优越的导航性能，真实世界部署的成功也突显了其广泛实际应用的潜力。

Conclusion: GlocDiff通过结合全局楼层平面图指导和局部深度特征，有效解决了室内导航中的模态差异和定位挑战，展示了出色的导航性能和实际应用价值。

Abstract: Guiding an agent to a specific target in indoor environments based solely on
RGB inputs and a floor plan is a promising yet challenging problem. Although
existing methods have made significant progress, two challenges remain
unresolved. First, the modality gap between egocentric RGB observations and the
floor plan hinders the integration of visual and spatial information for both
local obstacle avoidance and global planning. Second, accurate localization is
critical for navigation performance, but remains challenging at deployment in
unseen environments due to the lack of explicit geometric alignment between RGB
inputs and floor plans. We propose a novel diffusion-based policy, denoted as
GlocDiff, which integrates global path planning from the floor plan with local
depth-aware features derived from RGB observations. The floor plan offers
explicit global guidance, while the depth features provide implicit geometric
cues, collectively enabling precise prediction of optimal navigation directions
and robust obstacle avoidance. Moreover, GlocDiff introduces noise perturbation
during training to enhance robustness against pose estimation errors, and we
find that combining this with a relatively stable VO module during inference
results in significantly improved navigation performance. Extensive experiments
on the FloNa benchmark demonstrate GlocDiff's efficiency and effectiveness in
achieving superior navigation performance, and the success of real-world
deployments also highlights its potential for widespread practical
applications.

</details>


### [56] [Phy-Tac: Toward Human-Like Grasping via Physics-Conditioned Tactile Goals](https://arxiv.org/abs/2511.01520)
*Shipeng Lyu,Lijie Sheng,Fangyuan Wang,Wenyao Zhang,Weiwei Lin,Zhenzhong Jia,David Navarro-Alarcon,Guodong Guo*

Main category: cs.RO

TL;DR: 提出Phy-Tac方法，通过物理条件触觉技术实现力最优稳定抓取，结合姿态选择、触觉预测和力调节，使机器人能像人类一样用最小必要力稳定抓取物体。


<details>
  <summary>Details</summary>
Motivation: 解决机器人抓取时过度用力的问题，缩小与人类自然抓取（仅用维持稳定所需的最小力）之间的差距。

Method: 1) 基于物理的姿态选择器识别最优力分布的接触区域；2) 物理条件潜在扩散模型预测目标触觉印记；3) 潜在空间LQR控制器驱动夹爪以最小作动达到目标触觉。

Result: 在多样化物体和接触条件下，Phy-LDM实现优越的触觉预测精度，Phy-Tac在抓取稳定性和力效率方面优于固定力和GraspNet基线方法。

Conclusion: 该方法在经典机器人平台上展示了力高效和自适应操作，缩小了机器人与人类抓取之间的差距。

Abstract: Humans naturally grasp objects with minimal level required force for
stability, whereas robots often rely on rigid, over-squeezing control. To
narrow this gap, we propose a human-inspired physics-conditioned tactile method
(Phy-Tac) for force-optimal stable grasping (FOSG) that unifies pose selection,
tactile prediction, and force regulation. A physics-based pose selector first
identifies feasible contact regions with optimal force distribution based on
surface geometry. Then, a physics-conditioned latent diffusion model (Phy-LDM)
predicts the tactile imprint under FOSG target. Last, a latent-space LQR
controller drives the gripper toward this tactile imprint with minimal
actuation, preventing unnecessary compression. Trained on a physics-conditioned
tactile dataset covering diverse objects and contact conditions, the proposed
Phy-LDM achieves superior tactile prediction accuracy, while the Phy-Tac
outperforms fixed-force and GraspNet-based baselines in grasp stability and
force efficiency. Experiments on classical robotic platforms demonstrate
force-efficient and adaptive manipulation that bridges the gap between robotic
and human grasping.

</details>


### [57] [MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence](https://arxiv.org/abs/2511.01594)
*Renjun Gao,Peiyan Zhong*

Main category: cs.RO

TL;DR: MARS是一个基于多模态大语言模型的多智能体机器人系统，专为智能家居机器人设计，旨在为残障人士提供风险感知、个性化的辅助服务。


<details>
  <summary>Details</summary>
Motivation: 现有系统在风险感知规划、用户个性化以及将语言计划转化为可执行技能方面存在困难，特别是在杂乱的家庭环境中。

Method: 系统集成四个智能体：视觉感知智能体提取环境语义和空间特征，风险评估智能体识别和优先处理危险，规划智能体生成可执行动作序列，评估智能体进行迭代优化。

Result: 在多个数据集上的实验表明，该系统在风险感知规划和协调多智能体执行方面优于最先进的多模态模型。

Conclusion: 该方法展示了协作AI在实际辅助场景中的潜力，并为在现实环境中部署基于MLLM的多智能体系统提供了可推广的方法论。

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities
in cross-modal understanding and reasoning, offering new opportunities for
intelligent assistive systems, yet existing systems still struggle with
risk-aware planning, user personalization, and grounding language plans into
executable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic
System powered by MLLMs for assistive intelligence and designed for smart home
robots supporting people with disabilities. The system integrates four agents:
a visual perception agent for extracting semantic and spatial features from
environment images, a risk assessment agent for identifying and prioritizing
hazards, a planning agent for generating executable action sequences, and an
evaluation agent for iterative optimization. By combining multimodal perception
with hierarchical multi-agent decision-making, the framework enables adaptive,
risk-aware, and personalized assistance in dynamic indoor environments.
Experiments on multiple datasets demonstrate the superior overall performance
of the proposed system in risk-aware planning and coordinated multi-agent
execution compared with state-of-the-art multimodal models. The proposed
approach also highlights the potential of collaborative AI for practical
assistive scenarios and provides a generalizable methodology for deploying
MLLM-enabled multi-agent systems in real-world environments.

</details>


### [58] [Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process](https://arxiv.org/abs/2511.01718)
*Jiayi Chen,Wenxuan Song,Pengxiang Ding,Ziyang Zhou,Han Zhao,Feilong Tang,Donglin Wang,Haoang Li*

Main category: cs.RO

TL;DR: 提出了统一扩散VLA模型和联合离散去噪扩散过程(JD3P)，通过同步去噪过程联合优化图像生成和动作预测，在CALVIN等基准测试中达到最先进性能，推理速度比自回归方法快4倍。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型要么依赖外部专家进行模态统一，要么将图像生成和动作预测作为独立过程处理，限制了这些任务之间的直接协同效益。

Method: 采用统一扩散VLA模型和联合离散去噪扩散过程(JD3P)，将所有模态集成到单一去噪轨迹中，使用统一标记化空间和混合注意力机制，并提出两阶段训练流程和推理时优化技术。

Result: 在CALVIN、LIBERO和SimplerEnv等基准测试中达到最先进性能，推理速度比自回归方法快4倍，并通过深入分析和真实世界评估验证了有效性。

Conclusion: 通过同步去噪过程联合优化生成和动作，实现了理解、生成和行动之间的内在协同，为视觉-语言-动作模型提供了更高效统一的解决方案。

Abstract: Vision-language-action (VLA) models aim to understand natural language
instructions and visual observations and to execute corresponding actions as an
embodied agent. Recent work integrates future images into the
understanding-acting loop, yielding unified VLAs that jointly understand,
generate, and act -- reading text and images and producing future images and
actions. However, these models either rely on external experts for modality
unification or treat image generation and action prediction as separate
processes, limiting the benefits of direct synergy between these tasks. Our
core philosophy is to optimize generation and action jointly through a
synchronous denoising process, where the iterative refinement enables actions
to evolve from initialization, under constant and sufficient visual guidance.
We ground this philosophy in our proposed Unified Diffusion VLA and Joint
Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process
that integrates multiple modalities into a single denoising trajectory to serve
as the key mechanism enabling understanding, generation, and acting to be
intrinsically synergistic. Our model and theory are built on a unified
tokenized space of all modalities and a hybrid attention mechanism. We further
propose a two-stage training pipeline and several inference-time techniques
that optimize performance and efficiency. Our approach achieves
state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and
SimplerEnv with 4$\times$ faster inference than autoregressive methods, and we
demonstrate its effectiveness through in-depth analysis and real-world
evaluations. Our project page is available at
https://irpn-eai.github.io/UD-VLA.github.io/.

</details>


### [59] [Lightweight Learning from Actuation-Space Demonstrations via Flow Matching for Whole-Body Soft Robotic Grasping](https://arxiv.org/abs/2511.01770)
*Liudi Yang,Yang Bai,Yuhao Wang,Ibrahim Alsarraj,Gitta Kutyniok,Zhanchi Wang,Ke Wu*

Main category: cs.RO

TL;DR: 提出了一种基于驱动空间学习的轻量级框架，利用软体机器人的被动柔顺性实现自适应抓取，仅需少量演示就能在不确定环境下实现高成功率抓取。


<details>
  <summary>Details</summary>
Motivation: 解决刚性机器人抓取在不确定接触环境下面临的挑战，利用软体机器人的机械智能特性来简化控制复杂度。

Method: 使用流匹配模型从确定性演示中学习分布式的控制表示，无需密集传感或复杂控制回路。

Result: 仅用30个演示（不到可达工作空间的8%）就实现了97.5%的抓取成功率，能泛化到±33%的物体尺寸变化，并在执行时间缩放20%-200%时保持稳定性能。

Conclusion: 驱动空间学习通过利用软体机器人的被动冗余自由度和柔顺性，将机械特性转化为功能控制智能，显著减轻了中央控制器的负担。

Abstract: Robotic grasping under uncertainty remains a fundamental challenge due to its
uncertain and contact-rich nature. Traditional rigid robotic hands, with
limited degrees of freedom and compliance, rely on complex model-based and
heavy feedback controllers to manage such interactions. Soft robots, by
contrast, exhibit embodied mechanical intelligence: their underactuated
structures and passive flexibility of their whole body, naturally accommodate
uncertain contacts and enable adaptive behaviors. To harness this capability,
we propose a lightweight actuation-space learning framework that infers
distributional control representations for whole-body soft robotic grasping,
directly from deterministic demonstrations using a flow matching model
(Rectified Flow),without requiring dense sensing or heavy control loops. Using
only 30 demonstrations (less than 8% of the reachable workspace), the learned
policy achieves a 97.5% grasp success rate across the whole workspace,
generalizes to grasped-object size variations of +-33%, and maintains stable
performance when the robot's dynamic response is directly adjusted by scaling
the execution time from 20% to 200%. These results demonstrate that
actuation-space learning, by leveraging its passive redundant DOFs and
flexibility, converts the body's mechanics into functional control intelligence
and substantially reduces the burden on central controllers for this
uncertain-rich task.

</details>


### [60] [MOBIUS: A Multi-Modal Bipedal Robot that can Walk, Crawl, Climb, and Roll](https://arxiv.org/abs/2511.01774)
*Alexander Schperberg,Yusuke Tanaka,Stefano Di Cairano,Dennis Hong*

Main category: cs.RO

TL;DR: MOBIUS是一种多模态双足机器人，能够行走、爬行、攀爬和滚动，通过混合控制架构和高级规划器实现不同运动模式间的平滑转换。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够在多样化地形上无缝转换运动模式的机器人，扩展机器人的交互能力、工作空间和穿越能力。

Method: 采用四肢体设计（两个6自由度手臂和两个4自由度腿部），结合强化学习运动控制与基于模型的预测和导纳控制，使用MIQCP规划器自主选择运动模式。

Result: 硬件实验展示了稳健的步态转换、动态攀爬和通过夹持抓握实现的全身体重支撑。

Conclusion: MOBIUS证明了形态学、高级规划和控制之间的紧密集成对于实现移动定位操作和抓取的重要性，显著扩展了机器人的交互能力。

Abstract: This article presents a Multi-Modal Bipedal Intelligent Urban Scout robot
(MOBIUS) capable of walking, crawling, climbing, and rolling. MOBIUS features
four limbs--two 6-DoF arms with two-finger grippers for manipulation and
climbing, and two 4-DoF legs for locomotion--enabling smooth transitions across
diverse terrains without reconfiguration. A hybrid control architecture
combines reinforcement learning-based locomotion with model-based predictive
and admittance control enhanced for safety by a Reference Governor toward
compliant contact interactions. A high-level MIQCP planner autonomously selects
locomotion modes to balance stability and energy efficiency. Hardware
experiments demonstrate robust gait transitions, dynamic climbing, and
full-body load support via pinch grasp. Overall, MOBIUS demonstrates the
importance of tight integration between morphology, high-level planning, and
control to enable mobile loco-manipulation and grasping, substantially
expanding its interaction capabilities, workspace, and traversability.

</details>


### [61] [GenDexHand: Generative Simulation for Dexterous Hands](https://arxiv.org/abs/2511.01791)
*Feng Chen,Zhuxiu Xu,Tianzhe Chu,Xunzhe Zhou,Li Sun,Zewen Wu,Shenghua Gao,Zhongyu Li,Yanchao Yang,Yi Ma*

Main category: cs.RO

TL;DR: GenDexHand是一个生成式仿真流水线，用于自动生成灵巧操作任务的多样化环境和任务，通过视觉语言模型的反馈闭环优化环境质量，并将任务分解为子任务以支持顺序强化学习。


<details>
  <summary>Details</summary>
Motivation: 解决灵巧操作任务中数据稀缺的根本瓶颈，现有基于大语言模型的抓取仿真生成方法在灵巧操作中迁移效果差，且灵巧操作由于自由度更高而更困难。

Method: 引入闭环精炼过程，基于视觉语言模型反馈调整物体位置和尺寸；将任务分解为子任务以支持顺序强化学习；提供生成式仿真流水线自动生成多样化任务和环境。

Result: 显著提高了生成环境的平均质量，减少了训练时间并提高了成功率。

Conclusion: 为具身智能中多样化灵巧手行为的可扩展训练提供了可行的仿真解决方案，解决了合成数据生成的挑战。

Abstract: Data scarcity remains a fundamental bottleneck for embodied intelligence.
Existing approaches use large language models (LLMs) to automate gripper-based
simulation generation, but they transfer poorly to dexterous manipulation,
which demands more specialized environment design. Meanwhile, dexterous
manipulation tasks are inherently more difficult due to their higher degrees of
freedom. Massively generating feasible and trainable dexterous hand tasks
remains an open challenge. To this end, we present GenDexHand, a generative
simulation pipeline that autonomously produces diverse robotic tasks and
environments for dexterous manipulation. GenDexHand introduces a closed-loop
refinement process that adjusts object placements and scales based on
vision-language model (VLM) feedback, substantially improving the average
quality of generated environments. Each task is further decomposed into
sub-tasks to enable sequential reinforcement learning, reducing training time
and increasing success rates. Our work provides a viable path toward scalable
training of diverse dexterous hand behaviors in embodied intelligence by
offering a simulation-based solution to synthetic data generation. Our website:
https://winniechen2002.github.io/GenDexHand/.

</details>


### [62] [Hybrid Neural Network-Based Indoor Localisation System for Mobile Robots Using CSI Data in a Robotics Simulator](https://arxiv.org/abs/2511.01797)
*Javier Ballesteros-Jerez,Jesus Martínez-Gómez,Ismael García-Varea,Luis Orozco-Barbosa,Manuel Castillo-Cara*

Main category: cs.RO

TL;DR: 提出了一种基于混合神经网络(HyNN)的移动机器人定位方法，利用大规模MIMO系统的CSI数据，通过CNN和MLP结合实现2D位置估计。


<details>
  <summary>Details</summary>
Motivation: 解决复杂室内环境中移动机器人的精确定位和导航问题，利用现有的CSI数据集开发通用性强的定位方案。

Method: 使用TINTO工具将CSI读数转换为合成图像，构建CNN与MLP结合的混合神经网络模型，并与机器人仿真器和ROS系统集成。

Result: 实现了移动机器人在复杂环境中的精确室内定位和导航，展示了HyNN模型的有效性。

Conclusion: 该方法提供了一个可推广的通用流程，适用于不同场景和数据集，展示了在复杂环境中实现精确机器人定位的潜力。

Abstract: We present a hybrid neural network model for inferring the position of mobile
robots using Channel State Information (CSI) data from a Massive MIMO system.
By leveraging an existing CSI dataset, our approach integrates a Convolutional
Neural Network (CNN) with a Multilayer Perceptron (MLP) to form a Hybrid Neural
Network (HyNN) that estimates 2D robot positions. CSI readings are converted
into synthetic images using the TINTO tool. The localisation solution is
integrated with a robotics simulator, and the Robot Operating System (ROS),
which facilitates its evaluation through heterogeneous test cases, and the
adoption of state estimators like Kalman filters. Our contributions illustrate
the potential of our HyNN model in achieving precise indoor localisation and
navigation for mobile robots in complex environments. The study follows, and
proposes, a generalisable procedure applicable beyond the specific use case
studied, making it adaptable to different scenarios and datasets.

</details>
