<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 28]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [A*-based Temporal Logic Path Planning with User Preferences on Relaxed Task Satisfaction](https://arxiv.org/abs/2511.16844)
*Disha Kamale,Xi Yu,Cristian-Ioan Vasile*

Main category: cs.RO

TL;DR: 提出了一种基于A*的规划框架，用于在大型机器人环境中处理时序逻辑任务，当无法完全满足任务要求时，通过整合用户偏好来获得最佳可能的任务满意度。


<details>
  <summary>Details</summary>
Motivation: 在大型机器人环境中，完全满足时序逻辑任务要求往往不可行，需要一种能够考虑用户偏好的规划方法，在任务无法完全遵守时获得最优的妥协方案。

Method: 使用时序逻辑目标和用户偏好的自动机表示，提出基于A*的规划框架，并设计了一个简单高效的可采纳启发式函数，显著减少了规划时间和搜索内存需求。

Result: 该方法能够有效处理大规模问题，生成接近最优的高层轨迹，相比无信息搜索算法，规划时间和搜索内存大幅减少。案例研究验证了方法的可扩展性、运行时分析和启发式函数的次优性界限。

Conclusion: 所提出的规划框架和启发式函数能够高效处理大型机器人环境中的时序逻辑任务规划问题，在任务无法完全满足时通过用户偏好获得最优妥协方案。

Abstract: In this work, we consider the problem of planning for temporal logic tasks in large robot environments. When full task compliance is unattainable, we aim to achieve the best possible task satisfaction by integrating user preferences for relaxation into the planning process. Utilizing the automata-based representations for temporal logic goals and user preferences, we propose an A*-based planning framework. This approach effectively tackles large-scale problems while generating near-optimal high-level trajectories. To facilitate this, we propose a simple, efficient heuristic that allows for planning over large robot environments in a fraction of time and search memory as compared to uninformed search algorithms. We present extensive case studies to demonstrate the scalability, runtime analysis as well as empirical bounds on the suboptimality of the proposed heuristic.

</details>


### [2] [Single-Pixel Tactile Skin via Compressive Sampling](https://arxiv.org/abs/2511.16898)
*Ariel Slepyan,Laura Xing,Rudy Zhang,Nitish Thakor*

Main category: cs.RO

TL;DR: SPTS是一种单像素触觉皮肤系统，通过压缩采样技术从整个传感器阵列中重建丰富的触觉信息，仅使用单个输出通道，显著简化了布线复杂度并提高了数据效率。


<details>
  <summary>Details</summary>
Motivation: 开发大面积、高速电子皮肤面临布线复杂性和数据瓶颈的根本限制，这在机器人、假肢和人机接口领域是一个重大挑战。

Method: 采用压缩采样方法，每个传感元件配备微型微控制器，向全局总和贡献动态加权的模拟信号，在硬件层面实现分布式压缩感知。采用柔性、可菊花链连接的设计，将布线简化为几条输入线和一条输出线。

Result: 实现了3500 FPS的有效物体分类，捕获了瞬态动态，将8毫秒的弹丸撞击解析为23帧。支持自适应重建，仅使用7%的总数据即可快速定位接触，然后逐步细化到高保真图像。

Conclusion: 这项工作为机器人和人机接口的大规模触觉智能提供了一条高效途径，解决了传统方法中的布线复杂性和数据瓶颈问题。

Abstract: Development of large-area, high-speed electronic skins is a grand challenge for robotics, prosthetics, and human-machine interfaces, but is fundamentally limited by wiring complexity and data bottlenecks. Here, we introduce Single-Pixel Tactile Skin (SPTS), a paradigm that uses compressive sampling to reconstruct rich tactile information from an entire sensor array via a single output channel. This is achieved through a direct circuit-level implementation where each sensing element, equipped with a miniature microcontroller, contributes a dynamically weighted analog signal to a global sum, performing distributed compressed sensing in hardware. Our flexible, daisy-chainable design simplifies wiring to a few input lines and one output, and significantly reduces measurement requirements compared to raster scanning methods. We demonstrate the system's performance by achieving object classification at an effective 3500 FPS and by capturing transient dynamics, resolving an 8 ms projectile impact into 23 frames. A key feature is the support for adaptive reconstruction, where sensing fidelity scales with measurement time. This allows for rapid contact localization using as little as 7% of total data, followed by progressive refinement to a high-fidelity image - a capability critical for responsive robotic systems. This work offers an efficient pathway towards large-scale tactile intelligence for robotics and human-machine interfaces.

</details>


### [3] [Multi-UAV Swarm Obstacle Avoidance Based on Potential Field Optimization](https://arxiv.org/abs/2511.16911)
*Yendo Hu,Yiliang Wu,Weican Chen*

Main category: cs.RO

TL;DR: 提出了一种结合改进多机器人编队避障算法和增强人工势场法的混合算法，解决多无人机场景中传统APF方法路径冗余、航向突变和碰撞风险高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统人工势场法在多无人机场景中会导致飞行路径冗余、航向频繁突变，且在避障过程中容易发生无人机间碰撞，需要改进避障路径规划的合理性。

Method: 结合改进的MRF IAPF算法和优化的单无人机路径规划APF，集成三种相互作用力（障碍排斥力、无人机间作用力、目标吸引力），并引入碰撞风险评估和辅助子目标策略，在碰撞威胁高时生成临时航点引导避障。

Result: 仿真结果表明，相比传统APF编队算法，该算法在路径长度优化和航向稳定性方面有显著提升，能有效避障并快速恢复编队构型。

Conclusion: 验证了该算法在未知障碍物静态环境中的适用性和有效性。

Abstract: In multi UAV scenarios,the traditional Artificial Potential Field (APF) method often leads to redundant flight paths and frequent abrupt heading changes due to unreasonable obstacle avoidance path planning,and is highly prone to inter UAV collisions during the obstacle avoidance process.To address these issues,this study proposes a novel hybrid algorithm that combines the improved Multi-Robot Formation Obstacle Avoidance (MRF IAPF) algorithm with an enhanced APF optimized for single UAV path planning.Its core ideas are as follows:first,integrating three types of interaction forces from MRF IAPF obstacle repulsion force,inter UAV interaction force,and target attraction force;second,incorporating a refined single UAV path optimization mechanism,including collision risk assessment and an auxiliary sub goal strategy.When a UAV faces a high collision threat,temporary waypoints are generated to guide obstacle avoidance,ensuring eventual precise arrival at the actual target.Simulation results demonstrate that compared with traditional APF based formation algorithms,the proposed algorithm achieves significant improvements in path length optimization and heading stability,can effectively avoid obstacles and quickly restore the formation configuration,thus verifying its applicability and effectiveness in static environments with unknown obstacles.

</details>


### [4] [MobileOcc: A Human-Aware Semantic Occupancy Dataset for Mobile Robots](https://arxiv.org/abs/2511.16949)
*Junseo Kim,Guido Dumont,Xinyu Gao,Gang Chen,Holger Caesar,Javier Alonso-Mora*

Main category: cs.RO

TL;DR: 提出了MobileOcc数据集，用于移动机器人在拥挤人类环境中的语义占据感知，包含静态物体占据标注和专门用于人类占据建模的网格优化框架，并建立了占据预测和行人速度预测的基准。


<details>
  <summary>Details</summary>
Motivation: 密集3D语义占据感知在移动机器人领域研究不足，特别是在行人密集环境中，需要专门的数据集来推动该领域发展。

Method: 开发了包含静态物体占据标注和新型网格优化框架的标注流程，从2D图像重建可变形人体几何，并使用LiDAR点云数据进行优化。

Result: 建立了占据预测和行人速度预测的基准，评估了单目、立体和全景占据等方法，并在3D人体姿态估计数据集上验证了标注方法的鲁棒性。

Conclusion: MobileOcc数据集填补了移动机器人语义占据感知的空白，提出的标注方法在不同数据集上表现出鲁棒性能。

Abstract: Dense 3D semantic occupancy perception is critical for mobile robots operating in pedestrian-rich environments, yet it remains underexplored compared to its application in autonomous driving. To address this gap, we present MobileOcc, a semantic occupancy dataset for mobile robots operating in crowded human environments. Our dataset is built using an annotation pipeline that incorporates static object occupancy annotations and a novel mesh optimization framework explicitly designed for human occupancy modeling. It reconstructs deformable human geometry from 2D images and subsequently refines and optimizes it using associated LiDAR point data. Using MobileOcc, we establish benchmarks for two tasks, i) Occupancy prediction and ii) Pedestrian velocity prediction, using different methods including monocular, stereo, and panoptic occupancy, with metrics and baseline implementations for reproducible comparison. Beyond occupancy prediction, we further assess our annotation method on 3D human pose estimation datasets. Results demonstrate that our method exhibits robust performance across different datasets.

</details>


### [5] [Stable Offline Hand-Eye Calibration for any Robot with Just One Mark](https://arxiv.org/abs/2511.17001)
*Sicheng Xie,Lingchen Meng,Zhiying Du,Shuyuan Tu,Haidong Cao,Jiaqi Leng,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.RO

TL;DR: CalibAll是一种简单有效的相机外参标定方法，只需在机器人末端执行器上标记一个点，通过从粗到精的标定流程，无需训练即可在各种机器人和数据集上实现稳定准确的相机外参估计。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在机器人任务中取得了显著成功，但相机外参信息常常不可得，而现有的估计方法存在局部最小值和泛化能力差的问题。

Method: 在末端执行器上标记一个点，利用视觉基础模型的对应能力自动定位标记点，结合点跟踪和3D末端轨迹通过时序PnP获得粗略相机外参，再通过基于渲染的优化对齐渲染和真实掩码进行精炼。

Result: 实验结果表明该方法优于现有最先进方法，在三个机器人平台上表现出强大的鲁棒性和通用有效性，还能生成深度图、链接级掩码和末端执行器2D轨迹等有用辅助标注。

Conclusion: CalibAll提供了一种训练免费、稳定准确的相机外参估计解决方案，能够支持下游任务。

Abstract: Imitation learning has achieved remarkable success in a variety of robotic tasks by learning a mapping function from camera-space observations to robot-space actions. Recent work indicates that the use of robot-to-camera transformation information ({\ie}, camera extrinsics) benefits the learning process and produces better results. However, camera extrinsics are oftentimes unavailable and estimation methods usually suffer from local minima and poor generalizations. In this paper, we present CalibAll, a simple yet effective method that \textbf{requires only a single mark} and performs training-free, stable, and accurate camera extrinsic estimation across diverse robots and datasets through a coarse-to-fine calibration pipeline. In particular, we annotate a single mark on an end-effector (EEF), and leverage the correspondence ability emerged from vision foundation models (VFM) to automatically localize the corresponding mark across robots in diverse datasets. Using this mark, together with point tracking and the 3D EEF trajectory, we obtain a coarse camera extrinsic via temporal Perspective-n-Point (PnP). This estimate is further refined through a rendering-based optimization that aligns rendered and ground-true masks, yielding accurate and stable camera extrinsic. Experimental results demonstrate that our method outperforms state-of-the-art approaches, showing strong robustness and general effectiveness across three robot platforms. It also produces useful auxiliary annotations such as depth maps, link-wise masks, and end-effector 2D trajectories, which can further support downstream tasks.

</details>


### [6] [MfNeuPAN: Proactive End-to-End Navigation in Dynamic Environments via Direct Multi-Frame Point Constraints](https://arxiv.org/abs/2511.17013)
*Yiwen Ying,Hanjing Ye,Senzi Luo,Luyao Liu,Yu Zhan,Li He,Hong Zhang*

Main category: cs.RO

TL;DR: 提出了一种利用多帧点约束的主动端到端导航框架，通过预测模块预测移动障碍物的未来路径，使机器人能够主动规避潜在危险。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设静态环境无法适应实时变化，而基于学习的方法依赖单帧观测来估计运动约束，限制了在高度动态场景中的适应性。

Method: 使用多帧点约束（包括当前帧和预测模块预测的未来帧），结合预测模块基于多帧观测预测移动障碍物的未来路径，实现主动端到端导航。

Result: 仿真和真实世界实验验证了该方法的有效性，显著提高了在未知动态环境中的导航鲁棒性和效率。

Conclusion: 所提出的框架通过多帧约束和主动预测，成功解决了复杂动态环境中的实时机器人导航挑战。

Abstract: Obstacle avoidance in complex and dynamic environments is a critical challenge for real-time robot navigation. Model-based and learning-based methods often fail in highly dynamic scenarios because traditional methods assume a static environment and cannot adapt to real-time changes, while learning-based methods rely on single-frame observations for motion constraint estimation, limiting their adaptability. To overcome these limitations, this paper proposes a novel framework that leverages multi-frame point constraints, including current and future frames predicted by a dedicated module, to enable proactive end-to-end navigation. By incorporating a prediction module that forecasts the future path of moving obstacles based on multi-frame observations, our method allows the robot to proactively anticipate and avoid potential dangers. This proactive planning capability significantly enhances navigation robustness and efficiency in unknown dynamic environments. Simulations and real-world experiments validate the effectiveness of our approach.

</details>


### [7] [H-GAR: A Hierarchical Interaction Framework via Goal-Driven Observation-Action Refinement for Robotic Manipulation](https://arxiv.org/abs/2511.17079)
*Yijie Zhu,Rui Shao,Ziyang Liu,Jie He,Jizhihui Liu,Jiuru Wang,Zitong Yu*

Main category: cs.RO

TL;DR: H-GAR是一个用于机器人操作的分层交互框架，通过目标驱动的观察-动作精炼实现更准确的视频和动作预测。


<details>
  <summary>Details</summary>
Motivation: 现有的视频和动作预测方法通常以整体和无目标的方式进行，导致语义不对齐的预测和不连贯的行为。需要一种能够将预测锚定到任务目标，并实现观察与动作之间显式交互的方法。

Method: 提出H-GAR分层框架：首先生成目标观察和粗略动作草图；然后通过两个协同模块进行精炼：(1)目标条件观察合成器基于粗略动作和目标观察合成中间观察；(2)交互感知动作精炼器利用中间观察反馈和历史动作记忆库将粗略动作精炼为细粒度、目标一致的动作。

Result: 在仿真和真实世界机器人操作任务上的广泛实验表明，H-GAR实现了最先进的性能。

Conclusion: 通过将目标锚定与显式的动作-观察交互以从粗到细的方式集成，H-GAR能够实现更准确的机器人操作。

Abstract: Unified video and action prediction models hold great potential for robotic manipulation, as future observations offer contextual cues for planning, while actions reveal how interactions shape the environment. However, most existing approaches treat observation and action generation in a monolithic and goal-agnostic manner, often leading to semantically misaligned predictions and incoherent behaviors. To this end, we propose H-GAR, a Hierarchical interaction framework via Goal-driven observation-Action Refinement.To anchor prediction to the task objective, H-GAR first produces a goal observation and a coarse action sketch that outline a high-level route toward the goal. To enable explicit interaction between observation and action under the guidance of the goal observation for more coherent decision-making, we devise two synergistic modules. (1) Goal-Conditioned Observation Synthesizer (GOS) synthesizes intermediate observations based on the coarse-grained actions and the predicted goal observation. (2) Interaction-Aware Action Refiner (IAAR) refines coarse actions into fine-grained, goal-consistent actions by leveraging feedback from the intermediate observations and a Historical Action Memory Bank that encodes prior actions to ensure temporal consistency. By integrating goal grounding with explicit action-observation interaction in a coarse-to-fine manner, H-GAR enables more accurate manipulation. Extensive experiments on both simulation and real-world robotic manipulation tasks demonstrate that H-GAR achieves state-of-the-art performance.

</details>


### [8] [Progress-Think: Semantic Progress Reasoning for Vision-Language Navigation](https://arxiv.org/abs/2511.17097)
*Shuo Wang,Yucheng Wang,Guoxin Lian,Yongcai Wang,Maiyue Chen,Kaihui Wang,Bo Zhang,Zhizhong Su,Yutian Zhou,Wanting Li,Deying Li,Zhaoxin Fan*

Main category: cs.RO

TL;DR: Progress-Think通过语义进度推理来改进视觉语言导航，提出三阶段框架：自对齐进度预训练、进度引导策略预训练、进度策略联合微调，在R2R-CE和RxR-CE上实现最先进的导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言导航方法要么直接预测动作，要么预测数值进度，都忽略了观察序列和指令序列之间的单调共进特性，导致导航一致性不足。

Method: 提出三阶段框架：1）自对齐进度预训练通过视觉历史和指令前缀的微分对齐引导推理模块；2）进度引导策略预训练将学习到的进度状态注入导航上下文；3）进度策略联合微调使用进度感知强化目标联合优化两个模块。

Result: 在R2R-CE和RxR-CE数据集上的实验表明，该方法在导航成功率和效率方面达到最先进水平，语义进度提供了更一致的导航进展表示。

Conclusion: 语义进度推理能够更准确地表示导航进展，通过三阶段框架有效提升了视觉语言导航的一致性和性能。

Abstract: Vision-Language Navigation requires agents to act coherently over long horizons by understanding not only local visual context but also how far they have advanced within a multi-step instruction. However, recent Vision-Language-Action models focus on direct action prediction and earlier progress methods predict numeric achievements; both overlook the monotonic co-progression property of the observation and instruction sequences. Building on this insight, Progress-Think introduces semantic progress reasoning, predicting instruction-style progress from visual observations to enable more accurate navigation. To achieve this without expensive annotations, we propose a three-stage framework. In the initial stage, Self-Aligned Progress Pretraining bootstraps a reasoning module via a novel differentiable alignment between visual history and instruction prefixes. Then, Progress-Guided Policy Pretraining injects learned progress states into the navigation context, guiding the policy toward consistent actions. Finally, Progress-Policy Co-Finetuning jointly optimizes both modules with tailored progress-aware reinforcement objectives. Experiments on R2R-CE and RxR-CE show state-of-the-art success and efficiency, demonstrating that semantic progress yields a more consistent representation of navigation advancement.

</details>


### [9] [Reflection-Based Relative Localization for Cooperative UAV Teams Using Active Markers](https://arxiv.org/abs/2511.17166)
*Tim Lakemann,Daniel Bonilla Licea,Viktor Walter,Martin Saska*

Main category: cs.RO

TL;DR: 提出一种利用主动标记反射进行多机器人相对定位的新方法，无需机器人尺寸或标记配置的先验知识，特别适用于未知环境中的异构微型空中集群。


<details>
  <summary>Details</summary>
Motivation: 环境中主动标记的反射通常会导致视觉相对定位的模糊性，但本研究旨在利用这些通常被视为干扰的反射来实现更可靠的相对定位。

Method: 开发了一种基于反射的定位系统，能够处理非平面表面（特别是动态水面）引起的不确定性，无需机器人尺寸或预定义标记配置的先验知识。

Result: 在室内外实验中验证了该方法，证明其比现有方法具有更大的有效范围（超过30米）和更高的精度，且无需团队成员尺寸的先验知识。

Conclusion: 反射式定位系统在未知环境中可靠运行，特别适用于海洋部署等动态水面环境，为异构微型空中集群协作提供了有效的相对定位解决方案。

Abstract: Reflections of active markers in the environment are a common source of ambiguity in onboard visual relative localization. This work presents a novel approach for onboard relative localization in multi-robot teams that exploits these typically unwanted reflections of active markers in the environment. It operates without prior knowledge of robot size or predefined marker configurations and remains independent of surface properties, an essential feature for heterogeneous micro-aerial swarms cooperating in unknown environments. It explicitly accounts for uncertainties caused by non-flat surfaces, with a particular focus on dynamic water surfaces, which are especially relevant for marine deployments. We validated the approach in both indoor and outdoor experiments, demonstrating that the proposed reflection-based localization system operates reliably without prior knowledge of team member size and achieves greater effective range (above 30 m) and accuracy than state-of-the-art methods. The video and source code of this work will be made publicly available after publication.

</details>


### [10] [Efficient Robot Design with Multi-Objective Black-Box Optimization and Large Language Models](https://arxiv.org/abs/2511.17178)
*Kento Kawaharazuka,Yoshiki Obinata,Naoaki Kanazawa,Haoyu Jia,Kei Okada*

Main category: cs.RO

TL;DR: 提出了一种利用大型语言模型增强机器人身体设计黑盒优化效率的方法，通过并行采样提高探索效率


<details>
  <summary>Details</summary>
Motivation: 传统数值优化不适用于复杂结构或离散值场景，而黑盒优化采样效率低、需要大量迭代才能获得良好解

Method: 在黑盒优化采样过程中并行使用LLM进行采样，为LLM提供问题设置和广泛反馈

Result: 该方法能够更高效地探索设计解决方案

Conclusion: 讨论了该方法的特性和局限性

Abstract: Various methods for robot design optimization have been developed so far. These methods are diverse, ranging from numerical optimization to black-box optimization. While numerical optimization is fast, it is not suitable for cases involving complex structures or discrete values, leading to frequent use of black-box optimization instead. However, black-box optimization suffers from low sampling efficiency and takes considerable sampling iterations to obtain good solutions. In this study, we propose a method to enhance the efficiency of robot body design based on black-box optimization by utilizing large language models (LLMs). In parallel with the sampling process based on black-box optimization, sampling is performed using LLMs, which are provided with problem settings and extensive feedback. We demonstrate that this method enables more efficient exploration of design solutions and discuss its characteristics and limitations.

</details>


### [11] [TP-MDDN: Task-Preferenced Multi-Demand-Driven Navigation with Autonomous Decision-Making](https://arxiv.org/abs/2511.17225)
*Shanshan Li,Da Huang,Yu He,Yanwei Fu,Yu-Gang Jiang,Xiangyang Xue*

Main category: cs.RO

TL;DR: 提出了TP-MDDN新基准和AWMSystem自主决策系统，用于解决多需求驱动的长程导航任务，结合了指令分解、目标选择和任务监控模块，在感知精度和导航鲁棒性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统单需求导航无法反映现实世界中涉及多个需求和个性化选择的复杂任务，需要开发能够处理多需求导航的新方法。

Method: AWMSystem包含BreakLLM（指令分解）、LocateLLM（目标选择）、StatusMLLM（任务监控）三个模块；使用MASMap进行空间记忆管理；采用双节奏动作生成框架和自适应错误校正器。

Result: 实验表明该方法在感知精度和导航鲁棒性方面优于最先进的基线方法。

Conclusion: 提出的TP-MDDN基准和AWMSystem系统有效解决了多需求驱动的长程导航问题，为复杂现实任务提供了可行的解决方案。

Abstract: In daily life, people often move through spaces to find objects that meet their needs, posing a key challenge in embodied AI. Traditional Demand-Driven Navigation (DDN) handles one need at a time but does not reflect the complexity of real-world tasks involving multiple needs and personal choices. To bridge this gap, we introduce Task-Preferenced Multi-Demand-Driven Navigation (TP-MDDN), a new benchmark for long-horizon navigation involving multiple sub-demands with explicit task preferences. To solve TP-MDDN, we propose AWMSystem, an autonomous decision-making system composed of three key modules: BreakLLM (instruction decomposition), LocateLLM (goal selection), and StatusMLLM (task monitoring). For spatial memory, we design MASMap, which combines 3D point cloud accumulation with 2D semantic mapping for accurate and efficient environmental understanding. Our Dual-Tempo action generation framework integrates zero-shot planning with policy-based fine control, and is further supported by an Adaptive Error Corrector that handles failure cases in real time. Experiments demonstrate that our approach outperforms state-of-the-art baselines in both perception accuracy and navigation robustness.

</details>


### [12] [A ROS2 Interface for Universal Robots Collaborative Manipulators Based on ur_rtde](https://arxiv.org/abs/2511.17237)
*Alessio Saccuti,Riccardo Monica,Jacopo Aleotti*

Main category: cs.RO

TL;DR: 提出基于ur_rtde C++库的新型ROS2驱动，用于UR机器人操作臂，具有灵活性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 为Universal Robots机器人提供一个灵活的ROS2驱动解决方案，适应各种应用场景。

Method: 基于ur_rtde C++库开发，通过插件系统支持自定义命令，暴露URScripts的高级命令。

Result: 实现了多种命令功能，包括基于路径点的运动执行，并作为开源项目发布。

Conclusion: 该ROS2驱动为UR机器人提供了一个灵活、可扩展的控制解决方案。

Abstract: In this paper a novel ROS2 driver for UR robot manipulators is presented, based on the ur_rtde C++ library. The proposed driver aims to be a flexible solution, adaptable to a wide range of applications. The driver exposes the high-level commands of Universal Robots URScripts, and custom commands can be added using a plugin system. Several commands have been implemented, including motion execution along a waypoint-based path. The driver is published as open source.

</details>


### [13] [Simulation of Active Soft Nets for Capture of Space Debris](https://arxiv.org/abs/2511.17266)
*Leone Costi,Dario Izzo*

Main category: cs.RO

TL;DR: 开发基于MuJoCo的软体机器人网络模拟器，用于太空碎片自主清除，重点研究不同机械模型和控制策略对捕获Envisat卫星碎片的效果。


<details>
  <summary>Details</summary>
Motivation: 解决太空碎片清除问题，特别是针对大型ESA卫星Envisat的捕获，需要开发能够模拟网络动力学、接触力学和轨道力学的仿真工具。

Method: 使用MuJoCo物理引擎构建模拟器，包含网络动力学、接触检测、自接触、轨道力学和控制器，研究不同柔顺度网络模型和滑模控制策略。

Result: 更柔顺的网络在捕获Envisat时表现更好，结合滑模控制器时软网络在100%测试案例中成功捕获，具有更大的有效接触面积和更多接触点。

Conclusion: 软体机器人网络结合滑模控制是实现太空碎片高效捕获的有效方法，柔顺性对捕获性能有显著积极影响。

Abstract: In this work, we propose a simulator, based on the open-source physics engine MuJoCo, for the design and control of soft robotic nets for the autonomous removal of space debris. The proposed simulator includes net dynamics, contact between the net and the debris, self-contact of the net, orbital mechanics, and a controller that can actuate thrusters on the four satellites at the corners of the net. It showcases the case of capturing Envisat, a large ESA satellite that remains in orbit as space debris following the end of its mission. This work investigates different mechanical models, which can be used to simulate the net dynamics, simulating various degrees of compliance, and different control strategies to achieve the capture of the debris, depending on the relative position of the net and the target. Unlike previous works on this topic, we do not assume that the net has been previously ballistically thrown toward the target, and we start from a relatively static configuration. The results show that a more compliant net achieves higher performance when attempting the capture of Envisat. Moreover, when paired with a sliding mode controller, soft nets are able to achieve successful capture in 100% of the tested cases, whilst also showcasing a higher effective area at contact and a higher number of contact points between net and Envisat.

</details>


### [14] [Leveraging CVAE for Joint Configuration Estimation of Multifingered Grippers from Point Cloud Data](https://arxiv.org/abs/2511.17276)
*Julien Merand,Boris Meden,Mathieu Grossard*

Main category: cs.RO

TL;DR: 提出一种基于条件变分自编码器(CVAE)的方法，仅从多指抓手的点云数据中高效确定关节配置，无需传统逆运动学求解。


<details>
  <summary>Details</summary>
Motivation: 传统逆运动学方法需要基于指尖位姿进行数学求解，往往需要后处理决策或数值近似，而本文旨在通过机器学习隐式解决这些挑战。

Method: 使用条件变分自编码器(CVAE)，以关键结构元素的点云数据作为输入，重建相应的关节配置。

Result: 在MultiDex抓取数据集上使用Allegro Hand进行验证，运行时间在0.05毫秒内，准确性与最先进方法相当。

Conclusion: 该方法在AI驱动的抓取规划技术中展示了关节配置估计的有效性。

Abstract: This paper presents an efficient approach for determining the joint configuration of a multifingered gripper solely from the point cloud data of its poly-articulated chain, as generated by visual sensors, simulations or even generative neural networks. Well-known inverse kinematics (IK) techniques can provide mathematically exact solutions (when they exist) for joint configuration determination based solely on the fingertip pose, but often require post-hoc decision-making by considering the positions of all intermediate phalanges in the gripper's fingers, or rely on algorithms to numerically approximate solutions for more complex kinematics. In contrast, our method leverages machine learning to implicitly overcome these challenges. This is achieved through a Conditional Variational Auto-Encoder (CVAE), which takes point cloud data of key structural elements as input and reconstructs the corresponding joint configurations. We validate our approach on the MultiDex grasping dataset using the Allegro Hand, operating within 0.05 milliseconds and achieving accuracy comparable to state-of-the-art methods. This highlights the effectiveness of our pipeline for joint configuration estimation within the broader context of AI-driven techniques for grasp planning.

</details>


### [15] [MonoSpheres: Large-Scale Monocular SLAM-Based UAV Exploration through Perception-Coupled Mapping and Planning](https://arxiv.org/abs/2511.17299)
*Tomáš Musil,Matěj Petrlík,Martin Saska*

Main category: cs.RO

TL;DR: 提出了一种基于单目视觉的自主探索方法，能够在没有密集距离传感器的情况下，仅使用单目相机安全地探索大规模非结构化室内外3D环境。


<details>
  <summary>Details</summary>
Motivation: 解决仅配备单目相机且无密集距离传感器的移动机器人在未知环境中的自主探索问题，这是目前尚未完全解决的关键能力。

Method: 通过显式考虑稀疏单目SLAM前端的特性，在映射和规划两个模块中分别处理：映射模块解决稀疏深度数据、自由空间间隙和大深度不确定性问题；规划模块通过快速重新规划和感知感知的航向控制来处理增加的自由空间不确定性。

Result: 在多样化的真实世界和模拟环境中进行了广泛评估，包括消融研究。据作者所知，这是首个在真实世界非结构化室外环境中实现3D单目探索的方法。

Conclusion: 证明了基于稀疏单目深度数据的边界探索是可行的，前提是考虑视差要求和可能存在无纹理表面的情况。开源了实现以支持未来研究。

Abstract: Autonomous exploration of unknown environments is a key capability for mobile robots, but it is largely unsolved for robots equipped with only a single monocular camera and no dense range sensors. In this paper, we present a novel approach to monocular vision-based exploration that can safely cover large-scale unstructured indoor and outdoor 3D environments by explicitly accounting for the properties of a sparse monocular SLAM frontend in both mapping and planning. The mapping module solves the problems of sparse depth data, free-space gaps, and large depth uncertainty by oversampling free space in texture-sparse areas and keeping track of obstacle position uncertainty. The planning module handles the added free-space uncertainty through rapid replanning and perception-aware heading control. We further show that frontier-based exploration is possible with sparse monocular depth data when parallax requirements and the possibility of textureless surfaces are taken into account. We evaluate our approach extensively in diverse real-world and simulated environments, including ablation studies. To the best of the authors' knowledge, the proposed method is the first to achieve 3D monocular exploration in real-world unstructured outdoor environments. We open-source our implementation to support future research.

</details>


### [16] [FORWARD: Dataset of a forwarder operating in rough terrain](https://arxiv.org/abs/2511.17318)
*Mikael Lundbäck,Erik Wallin,Carola Häggström,Mattias Nyström,Andreas Grönlund,Mats Richardson,Petrus Jönsson,William Arnvik,Lucas Hedström,Arvid Fälldin,Martin Servin*

Main category: cs.RO

TL;DR: FORWARD数据集是一个高分辨率多模态数据集，记录了瑞典中部两个采伐现场的伐木集材机作业情况，包含RTK-GNSS、360度摄像头、振动传感器等多种传感器数据，用于开发林业机械的交通性、感知和自主控制模型。


<details>
  <summary>Details</summary>
Motivation: 为开发林业机械的交通性、感知和自主控制模型提供高质量的多模态数据集，支持人工智能、仿真和物理测试平台的研究。

Method: 使用配备多种传感器的大型Komatsu集材机在瑞典中部两个采伐现场收集数据，包括RTK-GNSS、360度摄像头、振动传感器、CAN总线信号和IMU等，数据采集频率为5Hz，覆盖约18小时的常规木材提取作业。

Result: 创建了一个包含厘米级精度的车辆位置、燃料消耗、起重机使用等详细数据的高质量数据集，还包括激光扫描的高分辨率地形数据（约1500点/平方米）和视频材料。

Conclusion: FORWARD数据集为林业机械的自动化研究提供了宝贵资源，可用于开发交通性模型、感知算法和自主控制系统，同时支持林业机械模拟器的自动生成和校准。

Abstract: We present FORWARD, a high-resolution multimodal dataset of a cut-to-length forwarder operating in rough terrain on two harvest sites in the middle part of Sweden. The forwarder is a large Komatsu model equipped with a variety of sensors, including RTK-GNSS, 360-camera, operator vibration sensors, internal CAN-bus signal recording, and multiple IMUs. The data includes event time logs recorded in 5 Hz with e.g., driving speed, fuel consumption, vehicle position with centimeter accuracy, and crane use while the vehicle operates in forest areas laser-scanned with very high-resolution, $\sim$1500 points per square meter. Production log files (StanForD standard) with time-stamped machine events, extensive video material, and terrain data in various formats are included as well. About 18 hours of regular wood extraction work during three days is annotated from 360-video material into individual work elements and included in the dataset. We also include scenario specifications of conducted experiments on forest roads and in terrain. Scenarios include repeatedly driving the same routes with and without steel tracks, different load weight, and different target driving speeds. The dataset is intended for developing models and algorithms for trafficability, perception, and autonomous control of forest machines using artificial intelligence, simulation, and experiments on physical testbeds. In part, we focus on forwarders traversing terrain, avoiding obstacles, and loading or unloading logs, with consideration for efficiency, fuel consumption, safety, and environmental impact. Other benefits of the open dataset include the ability to explore auto-generation and calibration of forestry machine simulators and automation scenario descriptions using the data recorded in the field.

</details>


### [17] [Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM](https://arxiv.org/abs/2511.17335)
*Chiori Hori,Yoshiki Masuyama,Siddarth Jain,Radu Corcodel,Devesh Jha,Diego Romeres,Jonathan Le Roux*

Main category: cs.RO

TL;DR: 提出长上下文Q-former和文本条件化方法，用于提升人机协作中的机器人动作确认和动作步骤生成性能


<details>
  <summary>Details</summary>
Motivation: 当前方法主要关注片段级处理，未能利用完整视频中的长上下文信息，限制了机器人对长时程任务的理解能力

Method: 提出长上下文Q-former整合视频左右上下文依赖，以及文本条件化方法直接向LLM解码器输入文本嵌入

Result: 在YouCook2语料库上的实验表明，确认生成的准确性是动作规划性能的主要因素，长上下文Q-former通过整合VideoLLaMA3提升了确认和动作规划能力

Conclusion: 长上下文信息和文本条件化方法能有效提升人机协作中机器人对任务的理解和动作规划能力

Abstract: Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment. This paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding. The state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former. Experiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning. Furthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3.

</details>


### [18] [METIS: Multi-Source Egocentric Training for Integrated Dexterous Vision-Language-Action Model](https://arxiv.org/abs/2511.17366)
*Yankai Fu,Ning Chen,Junkai Zhao,Shaozhe Shan,Guocai Yao,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Main category: cs.RO

TL;DR: METIS是一个用于灵巧操作的视觉-语言-动作模型，通过整合多源自我中心数据集和提取运动感知动态表示，实现了在六项真实世界任务中的最高平均成功率。


<details>
  <summary>Details</summary>
Motivation: 构建能够感知、推理和跨任务行动的通用机器人仍是一个开放挑战，特别是在灵巧操作领域。主要瓶颈在于缺乏大规模、动作标注的灵巧技能数据，而遥操作困难且成本高昂。

Method: 构建EgoAtlas数据集整合多源人类和机器人数据，提取运动感知动态作为紧凑离散化运动表示，并开发METIS视觉-语言-动作模型将推理和行动集成到统一框架中。

Result: 在六项真实世界任务中达到最高平均成功率，展现出优异的泛化能力和对分布外场景的鲁棒性。

Conclusion: METIS是朝着灵巧操作通用模型迈出的有希望的一步，强调了整合多源数据和统一表示的重要性。

Abstract: Building a generalist robot that can perceive, reason, and act across diverse tasks remains an open challenge, especially for dexterous manipulation. A major bottleneck lies in the scarcity of large-scale, action-annotated data for dexterous skills, as teleoperation is difficult and costly. Human data, with its vast scale and diverse manipulation behaviors, provides rich priors for learning robotic actions. While prior works have explored leveraging human demonstrations, they are often constrained by limited scenarios and a large visual gap between human and robots. To eliminate these limitations, we propose METIS, a vision-language-action (VLA) model for dexterous manipulation pretrained on multi-source egocentric datasets. We first construct EgoAtlas, which integrates large-scale human and robotic data from multiple sources, all unified under a consistent action space. We further extract motion-aware dynamics, a compact and discretized motion representation, which provides efficient and expressive supervision for VLA training. Built upon them, METIS integrates reasoning and acting into a unified framework, enabling effective deployment to downstream dexterous manipulation tasks. Our method demonstrates exceptional dexterous manipulation capabilities, achieving highest average success rate in six real-world tasks. Experimental results also highlight the superior generalization and robustness to out-of-distribution scenarios. These findings emphasize METIS as a promising step toward a generalist model for dexterous manipulation.

</details>


### [19] [Agility Meets Stability: Versatile Humanoid Control with Heterogeneous Data](https://arxiv.org/abs/2511.17373)
*Yixuan Pan,Ruoyi Qiao,Li Chen,Kashyap Chitta,Liang Pan,Haoguang Mai,Qingwen Bu,Hao Zhao,Cunyuan Zheng,Ping Luo,Hongyang Li*

Main category: cs.RO

TL;DR: AMS是一个统一动态运动跟踪和极端平衡保持的人形机器人控制框架，通过结合人类动作捕捉数据和合成平衡动作，实现了单个策略同时具备敏捷性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在敏捷动态技能和稳定性关键行为之间的权衡问题，实现既能执行敏捷动作又能保持极端平衡的统一控制框架。

Method: 利用异构数据源：人类动作捕捉数据提供丰富敏捷行为，物理约束的合成平衡动作捕获稳定配置；设计混合奖励方案，在所有数据上应用通用跟踪目标，仅在合成动作中注入平衡特定先验；采用自适应学习策略，包括性能驱动采样和运动特定奖励塑造。

Result: 在仿真和真实Unitree G1人形机器人上验证，单个策略能够执行舞蹈、跑步等敏捷技能，同时也能零样本执行叶问蹲等极端平衡动作。

Conclusion: AMS作为未来人形机器人应用的通用控制范式，成功统一了敏捷性和稳定性，展示了在单个策略中同时实现动态运动跟踪和极端平衡维护的能力。

Abstract: Humanoid robots are envisioned to perform a wide range of tasks in human-centered environments, requiring controllers that combine agility with robust balance. Recent advances in locomotion and whole-body tracking have enabled impressive progress in either agile dynamic skills or stability-critical behaviors, but existing methods remain specialized, focusing on one capability while compromising the other. In this work, we introduce AMS (Agility Meets Stability), the first framework that unifies both dynamic motion tracking and extreme balance maintenance in a single policy. Our key insight is to leverage heterogeneous data sources: human motion capture datasets that provide rich, agile behaviors, and physically constrained synthetic balance motions that capture stability configurations. To reconcile the divergent optimization goals of agility and stability, we design a hybrid reward scheme that applies general tracking objectives across all data while injecting balance-specific priors only into synthetic motions. Further, an adaptive learning strategy with performance-driven sampling and motion-specific reward shaping enables efficient training across diverse motion distributions. We validate AMS extensively in simulation and on a real Unitree G1 humanoid. Experiments demonstrate that a single policy can execute agile skills such as dancing and running, while also performing zero-shot extreme balance motions like Ip Man's Squat, highlighting AMS as a versatile control paradigm for future humanoid applications.

</details>


### [20] [Vector Cost Behavioral Planning for Autonomous Robotic Systems with Contemporary Validation Strategies](https://arxiv.org/abs/2511.17375)
*Benjamin R. Toaz,Quentin Goss,John Thompson,Seta Boğosyan,Shaunak D. Bopardikar,Mustafa İlhan Akbaş,Metin Gökaşan*

Main category: cs.RO

TL;DR: 本文提出了一种向量成本双矩阵游戏方法，用于多目标决策，使自主机器人系统能够同时优化多个目标，同时避免在忽略目标中出现最坏情况。该方法扩展到任意数量的目标，并与标量加权和方法在竞争性运动规划中进行比较。


<details>
  <summary>Details</summary>
Motivation: 现有的多目标决策方法在处理多个目标时存在局限性，特别是在避免最坏情况方面。本文旨在开发一种更全面、可解释的机器人行为规划框架，结合博弈论规划和智能系统验证。

Method: 使用向量成本双矩阵游戏方法进行多目标决策，应用可解释人工智能（XAI）软件分析高维决策数据，采用SEMBAS方法探索参数空间中的性能模式，并开发了一个新颖的综合仿真管道。

Result: 向量成本方法相比标量化方法表现出显著改进，提供了一个可解释且可推广的机器人行为规划框架。

Conclusion: 向量成本双矩阵游戏方法在多目标决策中优于传统的标量加权和方法，为机器人行为规划提供了一个全面且可解释的解决方案。

Abstract: The vector cost bimatrix game is a method for multi-objective decision making that enables autonomous robotic systems to optimize for multiple goals at once while avoiding worst-case scenarios in neglected objectives. We expand this approach to arbitrary numbers of objectives and compare its performance to scalar weighted sum methods during competitive motion planning. Explainable Artificial Intelligence (XAI) software is used to aid in the analysis of high dimensional decision-making data. State-space Exploration of Multidimensional Boundaries using Adherence Strategies (SEMBAS) is applied to explore performance modes in the parameter space as a sensitivity study for the baseline and proposed frameworks. While some works have explored aspects of game theoretic planning and intelligent systems validation separately, we combine each of these into a novel and comprehensive simulation pipeline. This integration demonstrates a dramatic improvement of the vector cost method over scalarization and offers an interpretable and generalizable framework for robotic behavioral planning. Code available at https://github.com/toazbenj/race_simulation. The video companion to this work is available at https://tinyurl.com/vectorcostvideo.

</details>


### [21] [IndustryNav: Exploring Spatial Reasoning of Embodied Agents in Dynamic Industrial Navigation](https://arxiv.org/abs/2511.17384)
*Yifan Li,Lichi Li,Anh Dao,Xinyu Zhou,Yicheng Qiao,Zheda Mai,Daeun Lee,Zichen Chen,Zhen Tan,Mohit Bansal,Yu Kong*

Main category: cs.RO

TL;DR: IndustryNav是首个动态工业导航基准，用于评估视觉大语言模型在动态仓库环境中的空间推理能力，发现现有模型在路径规划、碰撞避免和主动探索方面存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注静态家庭环境，无法评估模型在动态真实世界中的整体性能，需要开发能评估主动空间推理和安全的基准。

Method: 使用12个手动创建的高保真Unity仓库场景，结合动态物体和人员移动，采用PointGoal导航流程将自我中心视觉与全局里程计结合，评估局部-全局规划能力。

Result: 对9个最先进VLLM的评估显示，闭源模型保持优势，但所有智能体在稳健路径规划、碰撞避免和主动探索方面都存在明显不足。

Conclusion: 体现智能研究需要超越被动感知，转向需要稳定规划、主动探索和在动态真实环境中安全行为的任务。

Abstract: While Visual Large Language Models (VLLMs) show great promise as embodied agents, they continue to face substantial challenges in spatial reasoning. Existing embodied benchmarks largely focus on passive, static household environments and evaluate only isolated capabilities, failing to capture holistic performance in dynamic, real-world complexity. To fill this gap, we present IndustryNav, the first dynamic industrial navigation benchmark for active spatial reasoning. IndustryNav leverages 12 manually created, high-fidelity Unity warehouse scenarios featuring dynamic objects and human movement. Our evaluation employs a PointGoal navigation pipeline that effectively combines egocentric vision with global odometry to assess holistic local-global planning. Crucially, we introduce the "collision rate" and "warning rate" metrics to measure safety-oriented behaviors and distance estimation. A comprehensive study of nine state-of-the-art VLLMs (including models such as GPT-5-mini, Claude-4.5, and Gemini-2.5) reveals that closed-source models maintain a consistent advantage; however, all agents exhibit notable deficiencies in robust path planning, collision avoidance and active exploration. This highlights a critical need for embodied research to move beyond passive perception and toward tasks that demand stable planning, active exploration, and safe behavior in dynamic, real-world environment.

</details>


### [22] [Human Imitated Bipedal Locomotion with Frequency Based Gait Generator Network](https://arxiv.org/abs/2511.17387)
*Yusuf Baran Ates,Omer Morgul*

Main category: cs.RO

TL;DR: 结合从人类运动学习到的步态生成器网络和PPO扭矩控制器，实现轻量级双足行走框架，在仅平坦或缓坡训练后能泛化到陡坡和粗糙地面。


<details>
  <summary>Details</summary>
Motivation: 由于混合动力学和地形变化，学习类人、稳健的双足行走仍然困难。

Method: 使用从人类运动学习到的步态生成器网络与PPO控制器进行扭矩控制，结合频谱运动先验与深度强化学习。

Result: 尽管仅在平坦或缓坡地面训练，学习到的策略能够泛化到更陡的坡道和粗糙表面。

Conclusion: 将频谱运动先验与深度强化学习结合，为自然和稳健的双足行走提供了一条训练成本适中的实用路径。

Abstract: Learning human-like, robust bipedal walking remains difficult due to hybrid dynamics and terrain variability. We propose a lightweight framework that combines a gait generator network learned from human motion with Proximal Policy Optimization (PPO) controller for torque control. Despite being trained only on flat or mildly sloped ground, the learned policies generalize to steeper ramps and rough surfaces. Results suggest that pairing spectral motion priors with Deep Reinforcement Learning (DRL) offers a practical path toward natural and robust bipedal locomotion with modest training cost.

</details>


### [23] [Feasibility of Embodied Dynamics Based Bayesian Learning for Continuous Pursuit Motion Control of Assistive Mobile Robots in the Built Environment](https://arxiv.org/abs/2511.17401)
*Xiaoshan Zhou,Carol C. Menassa,Vineet R. Kamat*

Main category: cs.RO

TL;DR: 提出基于脑电图的贝叶斯推理框架，实现脑机接口的连续追踪运动控制，相比传统方法将速度预测误差降低72%


<details>
  <summary>Details</summary>
Motivation: 解决当前脑机接口运动控制系统大多限于离散命令，缺乏支持连续实时调整速度和方向的自然移动控制，这对轮椅用户在复杂环境中导航至关重要

Method: 采用脑启发的贝叶斯推理框架，解码基于加速度的运动表征中的体现动力学，使用自动相关性确定进行特征选择，并结合持续在线学习

Result: 在四名受试者16小时脑电图数据上，相比自回归和EEGNet方法，在会话累积迁移学习设置中将预测速度与真实速度的归一化均方误差降低了72%

Conclusion: 研究结果实证支持体现认知理论，揭示了大脑内在运动控制动力学的体现和预测性质，为更稳定直观的脑机接口控制提供了有前景的路径

Abstract: Non-invasive electroencephalography (EEG)-based brain-computer interfaces (BCIs) offer an intuitive means for individuals with severe motor impairments to independently operate assistive robotic wheelchairs and navigate built environments. Despite considerable progress in BCI research, most current motion control systems are limited to discrete commands, rather than supporting continuous pursuit, where users can freely adjust speed and direction in real time. Such natural mobility control is, however, essential for wheelchair users to navigate complex public spaces, such as transit stations, airports, hospitals, and indoor corridors, to interact socially with the dynamic populations with agility, and to move flexibly and comfortably as autonomous driving is refined to allow movement at will. In this study, we address the gap of continuous pursuit motion control in BCIs by proposing and validating a brain-inspired Bayesian inference framework, where embodied dynamics in acceleration-based motor representations are decoded. This approach contrasts with conventional kinematics-level decoding and deep learning-based methods. Using a public dataset with sixteen hours of EEG from four subjects performing motor imagery-based target-following, we demonstrate that our method, utilizing Automatic Relevance Determination for feature selection and continual online learning, reduces the normalized mean squared error between predicted and true velocities by 72% compared to autoregressive and EEGNet-based methods in a session-accumulative transfer learning setting. Theoretically, these findings empirically support embodied cognition theory and reveal the brain's intrinsic motor control dynamics in an embodied and predictive nature. Practically, grounding EEG decoding in the same dynamical principles that govern biological motion offers a promising path toward more stable and intuitive BCI control.

</details>


### [24] [SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding](https://arxiv.org/abs/2511.17411)
*Nikolay Nikolov,Giuliano Albanese,Sombit Dey,Aleksandar Yanev,Luc Van Gool,Jan-Nico Zaech,Danda Pani Paudel*

Main category: cs.RO

TL;DR: SPEAR-1是一个机器人基础模型，通过在非机器人图像数据中添加3D注释来增强预训练视觉语言模型的3D理解能力，从而提升机器人控制的泛化能力，同时大幅减少对机器人演示数据的需求。


<details>
  <summary>Details</summary>
Motivation: 当前机器人基础模型大多基于互联网预训练的视觉语言模型构建，但这些模型缺乏3D空间推理能力，而直接使用大规模机器人数据成本高昂且难以扩展。

Method: 提出SPEAR-VLM，一个能够从单张2D图像推断物体3D坐标的3D感知视觉语言模型，然后在此基础上构建SPEAR-1机器人基础模型，将基于3D的感知与语言指令的具身控制相结合。

Result: 在24个Open X-Embodiment数据集上训练，SPEAR-1性能达到或超过最先进模型如π₀-FAST和π₀.₅，同时使用的机器人演示数据减少了20倍。

Conclusion: 通过精心设计的训练策略，解锁了新的视觉语言模型能力，从而提升了具身控制的可靠性，超越了仅使用机器人数据所能达到的水平。

Abstract: Robotic Foundation Models (RFMs) hold great promise as generalist, end-to-end systems for robot control. Yet their ability to generalize across new environments, tasks, and embodiments remains limited. We argue that a major bottleneck lies in their foundations: most RFMs are built by fine-tuning internet-pretrained Vision-Language Models (VLMs). However, these VLMs are trained on 2D image-language tasks and lack the 3D spatial reasoning inherently required for embodied control in the 3D world. Bridging this gap directly with large-scale robotic data is costly and difficult to scale. Instead, we propose to enrich easy-to-collect non-robotic image data with 3D annotations and enhance a pretrained VLM with 3D understanding capabilities. Following this strategy, we train SPEAR-VLM, a 3D-aware VLM that infers object coordinates in 3D space from a single 2D image. Building on SPEAR-VLM, we introduce our main contribution, $~\textbf{SPEAR-1}$: a robotic foundation model that integrates grounded 3D perception with language-instructed embodied control. Trained on $\sim$45M frames from 24 Open X-Embodiment datasets, SPEAR-1 outperforms or matches state-of-the-art models such as $π_0$-FAST and $π_{0.5}$, while it uses 20$\times$ fewer robot demonstrations. This carefully-engineered training strategy unlocks new VLM capabilities and as a consequence boosts the reliability of embodied control beyond what is achievable with only robotic data. We make our model weights and 3D-annotated datasets publicly available.

</details>


### [25] [RoboCOIN: An Open-Sourced Bimanual Robotic Data COllection for INtegrated Manipulation](https://arxiv.org/abs/2511.17441)
*Shihan Wu,Xuecheng Liu,Shaoxuan Xie,Pengwei Wang,Xinghang Li,Bowen Yang,Zhe Li,Kai Zhu,Hongyu Wu,Yiheng Liu,Zhaoye Long,Yue Wang,Chong Liu,Dihan Wang,Ziqiang Ni,Xiang Yang,You Liu,Ruoxuan Feng,Runtian Xu,Lei Zhang,Denghang Huang,Chenghao Jin,Anlan Yin,Xinlong Wang,Zhenguo Sun,Junkai Zhao,Mengfei Du,Mingyu Cao,Xiansheng Chen,Hongyang Cheng,Xiaojie Zhang,Yankai Fu,Ning Chen,Cheng Chi,Sixiang Chen,Huaihai Lyu,Xiaoshuai Hao,Yankai Fu,Yequan Wang,Bo Lei,Dong Liu,Xi Yang,Yance Jiao,Tengfei Pan,Yunyan Zhang,Songjing Wang,Ziqian Zhang,Xu Liu,Ji Zhang,Caowei Meng,Zhizheng Zhang,Jiyang Gao,Song Wang,Xiaokun Leng,Zhiqiang Xie,Zhenzhen Zhou,Peng Huang,Wu Yang,Yandong Guo,Yichao Zhu,Suibing Zheng,Hao Cheng,Xinmin Ding,Yang Yue,Huanqian Wang,Chi Chen,Jingrui Pang,YuXi Qian,Haoran Geng,Lianli Gao,Haiyuan Li,Bin Fang,Gao Huang,Yaodong Yang,Hao Dong,He Wang,Hang Zhao,Yadong Mu,Di Hu,Hao Zhao,Tiejun Huang,Shanghang Zhang,Yonghua Lin,Zhongyuan Wang,Guocai Yao*

Main category: cs.RO

TL;DR: RoboCOIN是一个大规模多平台双手操作数据集，包含来自15个机器人平台的18万次演示，覆盖16个场景和421个任务，采用分层能力金字塔进行多级标注，并开发了CoRobot处理框架。


<details>
  <summary>Details</summary>
Motivation: 解决双手操作机器人数据集稀缺和硬件异构性的挑战，为人类级灵巧性机器人研究提供大规模多样化数据支持。

Method: 收集15个不同机器人平台的18万次演示，构建分层能力金字塔进行多级标注（轨迹级概念、段级子任务、帧级运动学），开发Robot Trajectory Markup Language (RTML)进行质量评估和统一管理。

Result: 实验证明RoboCOIN在多平台双手学习中的可靠性和有效性，在各种模型架构和机器人平台上均取得显著性能提升。

Conclusion: RoboCOIN数据集和CoRobot框架为双手操作研究提供了全面支持，已开源供进一步研究使用。

Abstract: Bimanual manipulation is essential for achieving human-like dexterity in robots, but the large-scale and diverse bimanual robot datasets remain scarce due to hardware heterogeneity across robotic platforms. To address the challenge, we present RoboCOIN, a comprehensive multi-embodiment bimanual manipulation dataset with over 180,000 demonstrations collected from 15 distinct robotic platforms. The dataset covers 16 scenarios, including residential, commercial, and working environments, with 421 tasks systematically organized by bimanual coordination patterns and object properties. Our key innovation is a hierarchical capability pyramid that provides multi-level annotations, spanning trajectory-level concepts, segment-level subtasks, and frame-level kinematics. We further develop CoRobot, a comprehensive processing framework featuring Robot Trajectory Markup Language (RTML) for quality assessment, automated annotation generation, and unified multi-embodiment management. Extensive experiments demonstrate the reliability and effectiveness of RoboCOIN in multi-embodiment bimanual learning, with significant performance improvements across various model architectures and robotic platforms. The complete dataset and framework are open-sourced and publicly available for further research purposes. Project website: https://FlagOpen.github.io/RoboCOIN/.

</details>


### [26] [MDG: Masked Denoising Generation for Multi-Agent Behavior Modeling in Traffic Environments](https://arxiv.org/abs/2511.17496)
*Zhiyu Huang,Zewei Zhou,Tianhui Cai,Yun Zhang,Jiaqi Ma*

Main category: cs.RO

TL;DR: 提出了Masked Denoising Generation (MDG)框架，通过独立加噪的时空张量重建来统一多智能体行为建模，避免了迭代采样和顺序解码的限制。


<details>
  <summary>Details</summary>
Motivation: 现有扩散和自回归方法受限于迭代采样、顺序解码或任务特定设计，影响了效率和可重用性。需要一种更高效统一的多智能体行为建模方法。

Method: MDG框架将多智能体行为建模重构为独立加噪时空张量的重建问题，使用连续、按智能体和时间步的噪声掩码，实现局部去噪和可控轨迹生成。

Result: 在Waymo Sim Agents和nuPlan Planning基准测试中取得有竞争力的闭环性能，同时提供高效、一致和可控的开环多智能体轨迹生成。

Conclusion: MDG作为一个简单而通用的范式，为多智能体行为建模提供了统一的解决方案。

Abstract: Modeling realistic and interactive multi-agent behavior is critical to autonomous driving and traffic simulation. However, existing diffusion and autoregressive approaches are limited by iterative sampling, sequential decoding, or task-specific designs, which hinder efficiency and reuse. We propose Masked Denoising Generation (MDG), a unified generative framework that reformulates multi-agent behavior modeling as the reconstruction of independently noised spatiotemporal tensors. Instead of relying on diffusion time steps or discrete tokenization, MDG applies continuous, per-agent and per-timestep noise masks that enable localized denoising and controllable trajectory generation in a single or few forward passes. This mask-driven formulation generalizes across open-loop prediction, closed-loop simulation, motion planning, and conditional generation within one model. Trained on large-scale real-world driving datasets, MDG achieves competitive closed-loop performance on the Waymo Sim Agents and nuPlan Planning benchmarks, while providing efficient, consistent, and controllable open-loop multi-agent trajectory generation. These results position MDG as a simple yet versatile paradigm for multi-agent behavior modeling.

</details>


### [27] [HALO: High-Altitude Language-Conditioned Monocular Aerial Exploration and Navigation](https://arxiv.org/abs/2511.17497)
*Yuezhan Tao,Dexter Ong,Fernando Cladera,Jason Hughes,Camillo J. Taylor,Pratik Chaudhari,Vijay Kumar*

Main category: cs.RO

TL;DR: HALO系统实现了基于单目相机、GPS和IMU的实时高空航拍度量语义建图与探索，能在大型户外环境中完成多任务自主执行


<details>
  <summary>Details</summary>
Motivation: 解决两个关键挑战：在大距离下使用视觉进行实时密集3D重建，以及在大型户外环境中进行具有精确场景几何和语义的建图与探索

Method: 使用单目相机、GPS和IMU的组合，开发了能够规划信息路径的系统，利用自然语言指定的多任务信息完成任务

Result: 在78000平方米的大规模环境仿真评估中，HALO以更少的探索时间完成任务，相比最先进的语义探索基线，行进距离竞争比提高达68%；在真实世界实验中，所有模块可在机器人上运行，在4060米高度覆盖24600平方米区域

Conclusion: HALO系统证明了在大型户外环境中实现实时高空航拍度量语义建图与探索的可行性，支持有效的多任务自主执行

Abstract: We demonstrate real-time high-altitude aerial metric-semantic mapping and exploration using a monocular camera paired with a global positioning system (GPS) and an inertial measurement unit (IMU). Our system, named HALO, addresses two key challenges: (i) real-time dense 3D reconstruction using vision at large distances, and (ii) mapping and exploration of large-scale outdoor environments with accurate scene geometry and semantics. We demonstrate that HALO can plan informative paths that exploit this information to complete missions with multiple tasks specified in natural language. In simulation-based evaluation across large-scale environments of size up to 78,000 sq. m., HALO consistently completes tasks with less exploration time and achieves up to 68% higher competitive ratio in terms of the distance traveled compared to the state-of-the-art semantic exploration baseline. We use real-world experiments on a custom quadrotor platform to demonstrate that (i) all modules can run onboard the robot, and that (ii) in diverse environments HALO can support effective autonomous execution of missions covering up to 24,600 sq. m. area at an altitude of 40 m. Experiment videos and more details can be found on our project page: https://tyuezhan.github.io/halo/.

</details>


### [28] [RynnVLA-002: A Unified Vision-Language-Action and World Model](https://arxiv.org/abs/2511.17502)
*Jun Cen,Siteng Huang,Yuqian Yuan,Hangjie Yuan,Chaohui Yu,Yuming Jiang,Jiayan Guo,Kehan Li,Hao Luo,Fan Wang,Xin Li,Deli Zhao,Hao Chen*

Main category: cs.RO

TL;DR: RynnVLA-002是一个统一的视觉-语言-动作模型和世界模型，通过联合学习环境动力学和动作规划，在仿真和真实机器人任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型和世界模型通常是分开训练的，缺乏相互增强的协同效应。本文旨在开发一个统一框架，让VLA模型和世界模型能够相互促进，提升整体性能。

Method: 提出RynnVLA-002统一框架，其中世界模型利用动作和视觉输入预测未来图像状态，学习环境物理规律；VLA模型从图像观察生成后续动作，增强视觉理解并支持世界模型的图像生成。

Result: 在LIBERO仿真基准测试中达到97.4%的成功率（无需预训练）；在真实世界LeRobot实验中，集成世界模型将整体成功率提升了50%。

Conclusion: RynnVLA-002证明了VLA模型和世界模型的统一框架能够实现相互增强，在仿真和真实机器人任务中都显著优于单独的模型。

Abstract: We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model. The world model leverages action and visual inputs to predict future image states, learning the underlying physics of the environment to refine action generation. Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world model's image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning. Our experiments show that RynnVLA-002 surpasses individual VLA and world models, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50%.

</details>
