{"id": "2507.14249", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14249", "abs": "https://arxiv.org/abs/2507.14249", "authors": ["Yuejiao Xie", "Maonan Wang", "Di Zhou", "Man-On Pun", "Zhu Han"], "title": "Real-Time Communication-Aware Ride-Sharing Route Planning for Urban Air Mobility: A Multi-Source Hybrid Attention Reinforcement Learning Approach", "comment": null, "summary": "Urban Air Mobility (UAM) systems are rapidly emerging as promising solutions\nto alleviate urban congestion, with path planning becoming a key focus area.\nUnlike ground transportation, UAM trajectory planning has to prioritize\ncommunication quality for accurate location tracking in constantly changing\nenvironments to ensure safety. Meanwhile, a UAM system, serving as an air taxi,\nrequires adaptive planning to respond to real-time passenger requests,\nespecially in ride-sharing scenarios where passenger demands are unpredictable\nand dynamic. However, conventional trajectory planning strategies based on\npredefined routes lack the flexibility to meet varied passenger ride demands.\nTo address these challenges, this work first proposes constructing a radio map\nto evaluate the communication quality of urban airspace. Building on this, we\nintroduce a novel Multi-Source Hybrid Attention Reinforcement Learning\n(MSHA-RL) framework for the challenge of effectively focusing on passengers and\nUAM locations, which arises from the significant dimensional disparity between\nthe representations. This model first generates the alignment among diverse\ndata sources with large gap dimensions before employing hybrid attention to\nbalance global and local insights, thereby facilitating responsive, real-time\npath planning. Extensive experimental results demonstrate that the approach\nenables communication-compliant trajectory planning, reducing travel time and\nenhancing operational efficiency while prioritizing passenger safety.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u7ebf\u7535\u5730\u56fe\u548c\u6df7\u5408\u6ce8\u610f\u529b\u5f3a\u5316\u5b66\u4e60\u7684UAM\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u4e58\u5ba2\u9700\u6c42\u548c\u901a\u4fe1\u8d28\u91cf\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\uff08UAM\uff09\u4e2d\u52a8\u6001\u4e58\u5ba2\u9700\u6c42\u548c\u901a\u4fe1\u8d28\u91cf\u5bf9\u8def\u5f84\u89c4\u5212\u7684\u6311\u6218\u3002", "method": "\u6784\u5efa\u65e0\u7ebf\u7535\u5730\u56fe\u8bc4\u4f30\u901a\u4fe1\u8d28\u91cf\uff0c\u63d0\u51fa\u591a\u6e90\u6df7\u5408\u6ce8\u610f\u529b\u5f3a\u5316\u5b66\u4e60\uff08MSHA-RL\uff09\u6846\u67b6\uff0c\u5e73\u8861\u5168\u5c40\u4e0e\u5c40\u90e8\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u5b9e\u73b0\u901a\u4fe1\u5408\u89c4\u7684\u8def\u5f84\u89c4\u5212\uff0c\u51cf\u5c11\u65c5\u884c\u65f6\u95f4\u5e76\u63d0\u5347\u6548\u7387\u3002", "conclusion": "MSHA-RL\u6846\u67b6\u4e3aUAM\u63d0\u4f9b\u4e86\u5b9e\u65f6\u3001\u5b89\u5168\u7684\u8def\u5f84\u89c4\u5212\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14274", "categories": ["cs.RO", "cs.NA", "math.DG", "math.DS", "math.GR", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.14274", "abs": "https://arxiv.org/abs/2507.14274", "authors": ["Andreas Mueller", "Shivesh Kumar", "Thomas Kordik"], "title": "A Recursive Lie-Group Formulation for the Second-Order Time Derivatives of the Inverse Dynamics of parallel Kinematic Manipulators", "comment": null, "summary": "Series elastic actuators (SEA) were introduced for serial robotic arms. Their\nmodel-based trajectory tracking control requires the second time derivatives of\nthe inverse dynamics solution, for which algorithms were proposed. Trajectory\ncontrol of parallel kinematics manipulators (PKM) equipped with SEAs has not\nyet been pursued. Key element for this is the computationally efficient\nevaluation of the second time derivative of the inverse dynamics solution. This\nhas not been presented in the literature, and is addressed in the present paper\nfor the first time. The special topology of PKM is exploited reusing the\nrecursive algorithms for evaluating the inverse dynamics of serial robots. A\nLie group formulation is used and all relations are derived within this\nframework. Numerical results are presented for a 6-DOF Gough-Stewart platform\n(as part of an exoskeleton), and for a planar PKM when a flatness-based control\nscheme is applied.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u5e76\u8054\u8fd0\u52a8\u5b66\u673a\u68b0\u81c2\uff08PKM\uff09\u914d\u5907\u4e32\u8054\u5f39\u6027\u6267\u884c\u5668\uff08SEA\uff09\u65f6\u9006\u52a8\u529b\u5b66\u89e3\u4e8c\u9636\u65f6\u95f4\u5bfc\u6570\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u6587\u732e\u7a7a\u767d\u3002", "motivation": "\u5e76\u8054\u8fd0\u52a8\u5b66\u673a\u68b0\u81c2\uff08PKM\uff09\u914d\u5907\u4e32\u8054\u5f39\u6027\u6267\u884c\u5668\uff08SEA\uff09\u7684\u8f68\u8ff9\u63a7\u5236\u5c1a\u672a\u5b9e\u73b0\uff0c\u5173\u952e\u5728\u4e8e\u9ad8\u6548\u8ba1\u7b97\u9006\u52a8\u529b\u5b66\u89e3\u7684\u4e8c\u9636\u65f6\u95f4\u5bfc\u6570\u3002", "method": "\u5229\u7528PKM\u7684\u7279\u6b8a\u62d3\u6251\u7ed3\u6784\uff0c\u590d\u7528\u4e32\u8054\u673a\u5668\u4eba\u9006\u52a8\u529b\u5b66\u7684\u9012\u5f52\u7b97\u6cd5\uff0c\u5e76\u91c7\u7528\u674e\u7fa4\u6846\u67b6\u63a8\u5bfc\u6240\u6709\u5173\u7cfb\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e6\u81ea\u7531\u5ea6Gough-Stewart\u5e73\u53f0\u548c\u5e73\u9762PKM\uff0c\u7ed3\u5408\u5e73\u5766\u6027\u63a7\u5236\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u89e3\u51b3\u4e86PKM\u914d\u5907SEA\u65f6\u7684\u9006\u52a8\u529b\u5b66\u89e3\u4e8c\u9636\u65f6\u95f4\u5bfc\u6570\u8ba1\u7b97\u95ee\u9898\uff0c\u4e3a\u8f68\u8ff9\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.14412", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14412", "abs": "https://arxiv.org/abs/2507.14412", "authors": ["Mengxue Fu", "Zhonghao Shi", "Minyu Huang", "Siqi Liu", "Mina Kian", "Yirui Song", "Maja J. Matari\u0107"], "title": "Personalized Socially Assistive Robots With End-to-End Speech-Language Models For Well-Being Support", "comment": null, "summary": "Socially assistive robots (SARs) have shown great potential for supplementing\nwell-being support. However, prior studies have found that existing dialogue\npipelines for SARs remain limited in real-time latency, back-channeling, and\npersonalized speech dialogue. Toward addressing these limitations, we propose\nusing integrated end-to-end speech-language models (SLMs) with SARs. This work\n1) evaluated the usability of an SLM-enabled SAR dialogue system through a\nsmall user study, and 2) identified remaining limitations through study user\nfeedback to inform future improvements. We conducted a small within-participant\nuser study with university students (N = 11) whose results showed that\nparticipants perceived an SLM-enabled SAR system as capable of providing\nempathetic feedback, natural turn-taking, back-channeling, and adaptive\nresponses. We also found that participants reported the robot's nonverbal\nbehaviors as lacking variability and synchronization with conversation, and the\nSLM's verbal feedback as generic and repetitive. These findings highlighted the\nneed for real-time robot movement synchronized with conversation, improved\nprompting or fine-tuning to generate outputs better aligned with mental health\npractices, and more expressive, adaptive vocal generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7aef\u5230\u7aef\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u7684\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\uff08SAR\uff09\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u5176\u53ef\u7528\u6027\uff0c\u53d1\u73b0\u5176\u5728\u5171\u60c5\u53cd\u9988\u548c\u81ea\u7136\u5bf9\u8bdd\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u975e\u8bed\u8a00\u884c\u4e3a\u548c\u4e2a\u6027\u5316\u53cd\u9988\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709SAR\u5bf9\u8bdd\u7cfb\u7edf\u5728\u5b9e\u65f6\u5ef6\u8fdf\u3001\u53cd\u9988\u673a\u5236\u548c\u4e2a\u6027\u5316\u5bf9\u8bdd\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u4e0eSAR\u7ed3\u5408\uff0c\u5e76\u901a\u8fc7\u5c0f\u89c4\u6a21\u7528\u6237\u7814\u7a76\uff08N=11\uff09\u8bc4\u4f30\u7cfb\u7edf\u53ef\u7528\u6027\u3002", "result": "\u7528\u6237\u8ba4\u4e3aSLM-enabled SAR\u7cfb\u7edf\u5728\u5171\u60c5\u53cd\u9988\u548c\u81ea\u7136\u5bf9\u8bdd\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u975e\u8bed\u8a00\u884c\u4e3a\u548c\u4e2a\u6027\u5316\u53cd\u9988\u4ecd\u9700\u6539\u8fdb\u3002", "conclusion": "\u672a\u6765\u9700\u4f18\u5316\u673a\u5668\u4eba\u52a8\u4f5c\u540c\u6b65\u3001\u63d0\u793a\u6216\u5fae\u8c03\u6a21\u578b\u4ee5\u66f4\u7b26\u5408\u5fc3\u7406\u5065\u5eb7\u5b9e\u8df5\uff0c\u5e76\u6539\u8fdb\u8bed\u97f3\u751f\u6210\u7684\u8868\u73b0\u529b\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2507.14455", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14455", "abs": "https://arxiv.org/abs/2507.14455", "authors": ["Chun-Ming Yang", "Pranav A. Bhounsule"], "title": "Koopman Operator Based Time-Delay Embeddings and State History Augmented LQR for Periodic Hybrid Systems: Bouncing Pendulum and Bipedal Walking", "comment": null, "summary": "Time-delay embedding is a technique that uses snapshots of state history over\ntime to build a linear state space model of a nonlinear smooth system. We\ndemonstrate that periodic non-smooth or hybrid system can also be modeled as a\nlinear state space system using this approach as long as its behavior is\nconsistent in modes and timings. We extended time-delay embeddings to generate\na linear model of two periodic hybrid systems: the bouncing pendulum and the\nsimplest walker with control inputs. This leads to a novel state history\naugmented linear quadratic regulator (LQR) which uses current and past state\nhistory for feedback control.", "AI": {"tldr": "\u65f6\u95f4\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\u7528\u4e8e\u6784\u5efa\u975e\u7ebf\u6027\u5149\u6ed1\u7cfb\u7edf\u7684\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u672c\u6587\u5c06\u5176\u6269\u5c55\u5230\u5468\u671f\u6027\u975e\u5149\u6ed1\u6216\u6df7\u5408\u7cfb\u7edf\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u72b6\u6001\u5386\u53f2\u7684\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668\uff08LQR\uff09\u3002", "motivation": "\u63a2\u7d22\u65f6\u95f4\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\u662f\u5426\u9002\u7528\u4e8e\u5468\u671f\u6027\u975e\u5149\u6ed1\u6216\u6df7\u5408\u7cfb\u7edf\uff0c\u4ee5\u6784\u5efa\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3002", "method": "\u6269\u5c55\u65f6\u95f4\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\uff0c\u5e94\u7528\u4e8e\u4e24\u4e2a\u5468\u671f\u6027\u6df7\u5408\u7cfb\u7edf\uff08\u5f39\u8df3\u6446\u548c\u7b80\u5355\u6b65\u884c\u5668\uff09\uff0c\u5e76\u8bbe\u8ba1\u72b6\u6001\u5386\u53f2\u589e\u5f3a\u7684LQR\u63a7\u5236\u5668\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u5468\u671f\u6027\u6df7\u5408\u7cfb\u7edf\u7684\u7ebf\u6027\u6a21\u578b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u72b6\u6001\u5386\u53f2\u589e\u5f3aLQR\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65f6\u95f4\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\u53ef\u63a8\u5e7f\u5230\u5468\u671f\u6027\u975e\u5149\u6ed1\u6216\u6df7\u5408\u7cfb\u7edf\uff0c\u72b6\u6001\u5386\u53f2\u589e\u5f3aLQR\u4e3a\u8fd9\u7c7b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u63a7\u5236\u65b9\u6cd5\u3002"}}
{"id": "2507.14538", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14538", "abs": "https://arxiv.org/abs/2507.14538", "authors": ["Jin Chai", "Xiang Yao", "Mengfan Hou", "Yanghong Li", "Erbao Dong"], "title": "A 21-DOF Humanoid Dexterous Hand with Hybrid SMA-Motor Actuation: CYJ Hand-0", "comment": null, "summary": "CYJ Hand-0 is a 21-DOF humanoid dexterous hand featuring a hybrid\ntendon-driven actuation system that combines shape memory alloys (SMAs) and DC\nmotors. The hand employs high-strength fishing line as artificial tendons and\nuses a fully 3D-printed AlSi10Mg metal frame designed to replicate the skeletal\nand tendon-muscle structure of the human hand. A linear motor-driven module\ncontrols finger flexion, while an SMA-based module enables finger extension and\nlateral abduction. These modules are integrated into a compact hybrid actuation\nunit mounted on a custom rear support structure. Mechanical and kinematic\nexperiments, conducted under an Arduino Mega 2560-based control system,\nvalidate the effectiveness of the design and demonstrate its biomimetic\ndexterity.", "AI": {"tldr": "CYJ Hand-0\u662f\u4e00\u79cd21\u81ea\u7531\u5ea6\u7684\u4eff\u4eba\u7075\u5de7\u624b\uff0c\u91c7\u7528\u6df7\u5408\u808c\u8171\u9a71\u52a8\u7cfb\u7edf\uff08\u7ed3\u5408\u5f62\u72b6\u8bb0\u5fc6\u5408\u91d1\u548c\u76f4\u6d41\u7535\u673a\uff09\uff0c\u901a\u8fc73D\u6253\u5370\u91d1\u5c5e\u6846\u67b6\u548c\u9ad8\u5f3a\u5ea6\u9493\u9c7c\u7ebf\u6a21\u62df\u4eba\u624b\u7ed3\u6784\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u4eff\u4eba\u7075\u5de7\u624b\uff0c\u7ed3\u5408\u591a\u79cd\u9a71\u52a8\u6280\u672f\u4ee5\u5b9e\u73b0\u9ad8\u4eff\u751f\u6027\u548c\u7075\u6d3b\u6027\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u9a71\u52a8\u7cfb\u7edf\uff08SMA\u548cDC\u7535\u673a\uff09\uff0c3D\u6253\u5370\u91d1\u5c5e\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u7535\u673a\u548cSMA\u6a21\u5757\u5206\u522b\u63a7\u5236\u624b\u6307\u7684\u5c48\u66f2\u548c\u4f38\u5c55/\u5916\u5c55\u3002", "result": "\u673a\u68b0\u548c\u8fd0\u52a8\u5b66\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u4eff\u751f\u7075\u6d3b\u6027\u3002", "conclusion": "CYJ Hand-0\u7684\u8bbe\u8ba1\u6210\u529f\u5b9e\u73b0\u4e86\u4eff\u4eba\u7075\u5de7\u624b\u7684\u751f\u7269\u529b\u5b66\u7279\u6027\uff0c\u5177\u6709\u6f5c\u5728\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.14582", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14582", "abs": "https://arxiv.org/abs/2507.14582", "authors": ["Zezhi Liu", "Shizhen Wu", "Hanqian Luo", "Deyun Qin", "Yongchun Fang"], "title": "BT-TL-DMPs: A Novel Robot TAMP Framework Combining Behavior Tree, Temporal Logic and Dynamical Movement Primitives", "comment": "11 pages, 8 figures", "summary": "In the field of Learning from Demonstration (LfD), enabling robots to\ngeneralize learned manipulation skills to novel scenarios for long-horizon\ntasks remains challenging. Specifically, it is still difficult for robots to\nadapt the learned skills to new environments with different task and motion\nrequirements, especially in long-horizon, multi-stage scenarios with intricate\nconstraints. This paper proposes a novel hierarchical framework, called\nBT-TL-DMPs, that integrates Behavior Tree (BT), Temporal Logic (TL), and\nDynamical Movement Primitives (DMPs) to address this problem. Within this\nframework, Signal Temporal Logic (STL) is employed to formally specify complex,\nlong-horizon task requirements and constraints. These STL specifications are\nsystematically transformed to generate reactive and modular BTs for high-level\ndecision-making task structure. An STL-constrained DMP optimization method is\nproposed to optimize the DMP forcing term, allowing the learned motion\nprimitives to adapt flexibly while satisfying intricate spatiotemporal\nrequirements and, crucially, preserving the essential dynamics learned from\ndemonstrations. The framework is validated through simulations demonstrating\ngeneralization capabilities under various STL constraints and real-world\nexperiments on several long-horizon robotic manipulation tasks. The results\ndemonstrate that the proposed framework effectively bridges the symbolic-motion\ngap, enabling more reliable and generalizable autonomous manipulation for\ncomplex robotic tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBT-TL-DMPs\u7684\u5206\u5c42\u6846\u67b6\uff0c\u7ed3\u5408\u884c\u4e3a\u6811\u3001\u65f6\u5e8f\u903b\u8f91\u548c\u52a8\u6001\u8fd0\u52a8\u57fa\u5143\uff0c\u4ee5\u89e3\u51b3\u673a\u5668\u4eba\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u6280\u80fd\u5e76\u6cdb\u5316\u5230\u65b0\u573a\u666f\u7684\u6311\u6218\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u4ece\u6f14\u793a\u5b66\u4e60\uff08LfD\uff09\u4e2d\uff0c\u5982\u4f55\u5c06\u5b66\u5230\u7684\u6280\u80fd\u6cdb\u5316\u5230\u65b0\u73af\u5883\u548c\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u6846\u67b6\u6574\u5408\u4e86\u884c\u4e3a\u6811\uff08BT\uff09\u3001\u65f6\u5e8f\u903b\u8f91\uff08TL\uff09\u548c\u52a8\u6001\u8fd0\u52a8\u57fa\u5143\uff08DMPs\uff09\uff0c\u5229\u7528\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff08STL\uff09\u89c4\u8303\u4efb\u52a1\u9700\u6c42\uff0c\u5e76\u901a\u8fc7\u4f18\u5316DMP\u5b9e\u73b0\u7075\u6d3b\u9002\u5e94\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5f25\u5408\u4e86\u7b26\u53f7\u4e0e\u8fd0\u52a8\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u81ea\u4e3b\u64cd\u4f5c\u7684\u6cdb\u5316\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.14605", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14605", "abs": "https://arxiv.org/abs/2507.14605", "authors": ["Chun-Ming Yang", "Pranav A. Bhounsule"], "title": "Koopman Operator Based Linear Model Predictive Control for 2D Quadruped Trotting, Bounding, and Gait Transition", "comment": null, "summary": "Online optimal control of quadrupedal robots would enable them to plan their\nmovement in novel scenarios. Linear Model Predictive Control (LMPC) has emerged\nas a practical approach for real-time control. In LMPC, an optimization problem\nwith a quadratic cost and linear constraints is formulated over a finite\nhorizon and solved on the fly. However, LMPC relies on linearizing the\nequations of motion (EOM), which may lead to poor solution quality. In this\npaper, we use Koopman operator theory and the Extended Dynamic Mode\nDecomposition (EDMD) to create a linear model of the system in high dimensional\nspace, thus retaining the nonlinearity of the EOM. We model the aerial phase\nand ground contact phases using different linear models. Then, using LMPC, we\ndemonstrate bounding, trotting, and bound-to-trot and trot-to-bound gait\ntransitions in level and rough terrains. The main novelty is the use of Koopman\noperator theory to create hybrid models of a quadrupedal system and demonstrate\nthe online generation of multiple gaits and gaits transitions.", "AI": {"tldr": "\u5229\u7528Koopman\u7b97\u5b50\u7406\u8bba\u548cEDMD\u65b9\u6cd5\uff0c\u6784\u5efa\u9ad8\u7ef4\u7ebf\u6027\u6a21\u578b\u4ee5\u4fdd\u7559\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7279\u6027\uff0c\u7ed3\u5408LMPC\u5b9e\u73b0\u56db\u8db3\u673a\u5668\u4eba\u591a\u79cd\u6b65\u6001\u53ca\u8f6c\u6362\u7684\u5728\u7ebf\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3LMPC\u56e0\u7ebf\u6027\u5316\u52a8\u529b\u5b66\u65b9\u7a0b\u5bfc\u81f4\u7684\u89e3\u8d28\u91cf\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u5347\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u80fd\u529b\u3002", "method": "\u91c7\u7528Koopman\u7b97\u5b50\u7406\u8bba\u548cEDMD\u6784\u5efa\u9ad8\u7ef4\u7ebf\u6027\u6a21\u578b\uff0c\u5206\u9636\u6bb5\u5efa\u6a21\u7a7a\u4e2d\u548c\u5730\u9762\u63a5\u89e6\u52a8\u529b\u5b66\uff0c\u7ed3\u5408LMPC\u5b9e\u73b0\u5b9e\u65f6\u63a7\u5236\u3002", "result": "\u6210\u529f\u6f14\u793a\u4e86\u5728\u5e73\u5766\u548c\u5d0e\u5c96\u5730\u5f62\u4e0a\u7684\u8df3\u8dc3\u3001\u5c0f\u8dd1\u53ca\u6b65\u6001\u8f6c\u6362\u3002", "conclusion": "Koopman\u7b97\u5b50\u7406\u8bba\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6df7\u5408\u6a21\u578b\uff0c\u652f\u6301\u5728\u7ebf\u751f\u6210\u591a\u79cd\u6b65\u6001\u53ca\u8f6c\u6362\uff0c\u6269\u5c55\u4e86LMPC\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.14694", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14694", "abs": "https://arxiv.org/abs/2507.14694", "authors": ["Yue Ma", "Kanglei Zhou", "Fuyang Yu", "Frederick W. B. Li", "Xiaohui Liang"], "title": "Uncertainty-aware Probabilistic 3D Human Motion Forecasting via Invertible Networks", "comment": null, "summary": "3D human motion forecasting aims to enable autonomous applications.\nEstimating uncertainty for each prediction (i.e., confidence based on\nprobability density or quantile) is essential for safety-critical contexts like\nhuman-robot collaboration to minimize risks. However, existing diverse motion\nforecasting approaches struggle with uncertainty quantification due to implicit\nprobabilistic representations hindering uncertainty modeling. We propose\nProbHMI, which introduces invertible networks to parameterize poses in a\ndisentangled latent space, enabling probabilistic dynamics modeling. A\nforecasting module then explicitly predicts future latent distributions,\nallowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI\nachieves strong performance for both deterministic and diverse prediction while\nvalidating uncertainty calibration, critical for risk-aware decision making.", "AI": {"tldr": "ProbHMI \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u9006\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e3D\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u573a\u666f\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\uff08\u5982\u4eba\u673a\u534f\u4f5c\uff09\u4e2d\uff0c\u91cf\u5316\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u9690\u5f0f\u6982\u7387\u8868\u793a\u800c\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u5f15\u5165\u53ef\u9006\u7f51\u7edc\u5c06\u59ff\u6001\u53c2\u6570\u5316\u5230\u89e3\u8026\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u663e\u5f0f\u9884\u6d4b\u672a\u6765\u6f5c\u5728\u5206\u5e03\u4ee5\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cProbHMI \u5728\u786e\u5b9a\u6027\u548c\u591a\u6837\u6027\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "ProbHMI \u4e3a\u98ce\u9669\u611f\u77e5\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u5b89\u5168\u6027\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.14700", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14700", "abs": "https://arxiv.org/abs/2507.14700", "authors": ["Nicholas Mohammad", "Nicola Bezzo"], "title": "Corridor-based Adaptive Control Barrier and Lyapunov Functions for Safe Mobile Robot Navigation", "comment": "To be presented in the 64th IEEE Conference on Decision and Control\n  (CDC 25)", "summary": "Safe navigation in unknown and cluttered environments remains a challenging\nproblem in robotics. Model Predictive Contour Control (MPCC) has shown promise\nfor performant obstacle avoidance by enabling precise and agile trajectory\ntracking, however, existing methods lack formal safety assurances. To address\nthis issue, we propose a general Control Lyapunov Function (CLF) and Control\nBarrier Function (CBF) enabled MPCC framework that enforces safety constraints\nderived from a free-space corridor around the planned trajectory. To enhance\nfeasibility, we dynamically adapt the CBF parameters at runtime using a Soft\nActor-Critic (SAC) policy. The approach is validated with extensive simulations\nand an experiment on mobile robot navigation in unknown cluttered environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CLF\u548cCBF\u7684MPCC\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574CBF\u53c2\u6570\u786e\u4fdd\u5b89\u5168\u5bfc\u822a\uff0c\u5e76\u5728\u672a\u77e5\u6742\u4e71\u73af\u5883\u4e2d\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709MPCC\u65b9\u6cd5\u7f3a\u4e4f\u6b63\u5f0f\u7684\u5b89\u5168\u4fdd\u8bc1\uff0c\u9700\u89e3\u51b3\u672a\u77e5\u6742\u4e71\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5bfc\u822a\u95ee\u9898\u3002", "method": "\u7ed3\u5408CLF\u548cCBF\u7684MPCC\u6846\u67b6\uff0c\u52a8\u6001\u8c03\u6574CBF\u53c2\u6570\uff08\u4f7f\u7528SAC\u7b56\u7565\uff09\uff0c\u786e\u4fdd\u8f68\u8ff9\u5b89\u5168\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u79fb\u52a8\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u672a\u77e5\u6742\u4e71\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5b89\u5168\u5bfc\u822a\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.14721", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14721", "abs": "https://arxiv.org/abs/2507.14721", "authors": ["Keita Kobashi", "Masayoshi Tomizuka"], "title": "Leveraging Extrinsic Dexterity for Occluded Grasping on Grasp Constraining Walls", "comment": "7 pages, 7 figures", "summary": "This study addresses the problem of occluded grasping, where primary grasp\nconfigurations of an object are not available due to occlusion with\nenvironment. Simple parallel grippers often struggle with such tasks due to\nlimited dexterity and actuation constraints. Prior works have explored object\npose reorientation such as pivoting by utilizing extrinsic contacts between an\nobject and an environment feature like a wall, to make the object graspable.\nHowever, such works often assume the presence of a short wall, and this\nassumption may not always hold in real-world scenarios. If the wall available\nfor interaction is too large or too tall, the robot may still fail to grasp the\nobject even after pivoting, and the robot must combine different types of\nactions to grasp. To address this, we propose a hierarchical reinforcement\nlearning (RL) framework. We use Q-learning to train a high-level policy that\nselects the type of action expected to yield the highest reward. The selected\nlow-level skill then samples a specific robot action in continuous space. To\nguide the robot to an appropriate location for executing the selected action,\nwe adopt a Conditional Variational Autoencoder (CVAE). We condition the CVAE on\nthe object point cloud and the skill ID, enabling it to infer a suitable\nlocation based on the object geometry and the selected skill. To promote\ngeneralization, we apply domain randomization during the training of low-level\nskills. The RL policy is trained entirely in simulation with a box-like object\nand deployed to six objects in real world. We conduct experiments to evaluate\nour method and demonstrate both its generalizability and robust sim-to-real\ntransfer performance with promising success rates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408Q\u5b66\u4e60\u548cCVAE\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u6293\u53d6\u88ab\u906e\u6321\u7269\u4f53\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u56e0\u73af\u5883\u906e\u6321\u5bfc\u81f4\u7269\u4f53\u4e3b\u8981\u6293\u53d6\u914d\u7f6e\u4e0d\u53ef\u7528\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5e73\u884c\u5939\u722a\u7075\u6d3b\u6027\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u9ad8\u5c42\u7b56\u7565\u901a\u8fc7Q\u5b66\u4e60\u9009\u62e9\u52a8\u4f5c\u7c7b\u578b\uff0c\u4f4e\u5c42\u6280\u80fd\u901a\u8fc7CVAE\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u91c7\u6837\u5177\u4f53\u52a8\u4f5c\uff0c\u5e76\u7ed3\u5408\u57df\u968f\u673a\u5316\u8bad\u7ec3\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u6d4b\u8bd5\u4e86\u516d\u79cd\u7269\u4f53\uff0c\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6cdb\u5316\u6027\u548c\u7a33\u5065\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u6027\u80fd\uff0c\u6210\u529f\u7387\u8f83\u9ad8\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7684\u6293\u53d6\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.14731", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14731", "abs": "https://arxiv.org/abs/2507.14731", "authors": ["Haitong Wang", "Aaron Hao Tan", "Angus Fung", "Goldie Nejat"], "title": "X-Nav: Learning End-to-End Cross-Embodiment Navigation for Mobile Robots", "comment": null, "summary": "Existing navigation methods are primarily designed for specific robot\nembodiments, limiting their generalizability across diverse robot platforms. In\nthis paper, we introduce X-Nav, a novel framework for end-to-end\ncross-embodiment navigation where a single unified policy can be deployed\nacross various embodiments for both wheeled and quadrupedal robots. X-Nav\nconsists of two learning stages: 1) multiple expert policies are trained using\ndeep reinforcement learning with privileged observations on a wide range of\nrandomly generated robot embodiments; and 2) a single general policy is\ndistilled from the expert policies via navigation action chunking with\ntransformer (Nav-ACT). The general policy directly maps visual and\nproprioceptive observations to low-level control commands, enabling\ngeneralization to novel robot embodiments. Simulated experiments demonstrated\nthat X-Nav achieved zero-shot transfer to both unseen embodiments and\nphotorealistic environments. A scalability study showed that the performance of\nX-Nav improves when trained with an increasing number of randomly generated\nembodiments. An ablation study confirmed the design choices of X-Nav.\nFurthermore, real-world experiments were conducted to validate the\ngeneralizability of X-Nav in real-world environments.", "AI": {"tldr": "X-Nav\u662f\u4e00\u4e2a\u8de8\u673a\u5668\u4eba\u5e73\u53f0\u5bfc\u822a\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5b66\u4e60\u5b9e\u73b0\u901a\u7528\u6027\uff0c\u5e76\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u5bfc\u822a\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u673a\u5668\u4eba\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u8de8\u5e73\u53f0\u7684\u901a\u7528\u6027\uff0cX-Nav\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "X-Nav\u5206\u4e24\u9636\u6bb5\uff1a1\uff09\u8bad\u7ec3\u591a\u4e2a\u4e13\u5bb6\u7b56\u7565\uff1b2\uff09\u901a\u8fc7Nav-ACT\u84b8\u998f\u4e3a\u5355\u4e00\u901a\u7528\u7b56\u7565\u3002", "result": "X-Nav\u5728\u4eff\u771f\u4e2d\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u901a\u7528\u6027\u3002", "conclusion": "X-Nav\u5c55\u793a\u4e86\u8de8\u673a\u5668\u4eba\u5e73\u53f0\u5bfc\u822a\u7684\u6f5c\u529b\uff0c\u6027\u80fd\u968f\u8bad\u7ec3\u6570\u636e\u589e\u52a0\u800c\u63d0\u5347\u3002"}}
{"id": "2507.14820", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14820", "abs": "https://arxiv.org/abs/2507.14820", "authors": ["Bingran Chen", "Baorun Li", "Jian Yang", "Yong Liu", "Guangyao Zhai"], "title": "KGN-Pro: Keypoint-Based Grasp Prediction through Probabilistic 2D-3D Correspondence Learning", "comment": null, "summary": "High-level robotic manipulation tasks demand flexible 6-DoF grasp estimation\nto serve as a basic function. Previous approaches either directly generate\ngrasps from point-cloud data, suffering from challenges with small objects and\nsensor noise, or infer 3D information from RGB images, which introduces\nexpensive annotation requirements and discretization issues. Recent methods\nmitigate some challenges by retaining a 2D representation to estimate grasp\nkeypoints and applying Perspective-n-Point (PnP) algorithms to compute 6-DoF\nposes. However, these methods are limited by their non-differentiable nature\nand reliance solely on 2D supervision, which hinders the full exploitation of\nrich 3D information. In this work, we present KGN-Pro, a novel grasping network\nthat preserves the efficiency and fine-grained object grasping of previous KGNs\nwhile integrating direct 3D optimization through probabilistic PnP layers.\nKGN-Pro encodes paired RGB-D images to generate Keypoint Map, and further\noutputs a 2D confidence map to weight keypoint contributions during\nre-projection error minimization. By modeling the weighted sum of squared\nre-projection errors probabilistically, the network effectively transmits 3D\nsupervision to its 2D keypoint predictions, enabling end-to-end learning.\nExperiments on both simulated and real-world platforms demonstrate that KGN-Pro\noutperforms existing methods in terms of grasp cover rate and success rate.", "AI": {"tldr": "KGN-Pro\u662f\u4e00\u79cd\u65b0\u578b\u6293\u53d6\u7f51\u7edc\uff0c\u901a\u8fc7\u6982\u7387PnP\u5c42\u76f4\u63a5\u4f18\u53163D\u4fe1\u606f\uff0c\u63d0\u53476-DoF\u6293\u53d6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6293\u53d6\u5c0f\u7269\u4f53\u548c\u5904\u7406\u4f20\u611f\u5668\u566a\u58f0\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u6216\u4f9d\u8d56\u6602\u8d35\u7684\u6807\u6ce8\u548c\u79bb\u6563\u5316\u95ee\u9898\u3002KGN-Pro\u65e8\u5728\u7ed3\u54082D\u6548\u7387\u548c3D\u4fe1\u606f\u3002", "method": "KGN-Pro\u901a\u8fc7RGB-D\u56fe\u50cf\u751f\u6210\u5173\u952e\u70b9\u56fe\u548c\u7f6e\u4fe1\u56fe\uff0c\u5229\u7528\u6982\u7387PnP\u5c42\u8fdb\u884c3D\u4f18\u5316\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKGN-Pro\u5728\u6293\u53d6\u8986\u76d6\u7387\u548c\u6210\u529f\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "KGN-Pro\u901a\u8fc7\u7ed3\u54082D\u548c3D\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6293\u53d6\u6027\u80fd\u3002"}}
{"id": "2507.14903", "categories": ["cs.RO", "I.2.9; I.2.10; I.2.11"], "pdf": "https://arxiv.org/pdf/2507.14903", "abs": "https://arxiv.org/abs/2507.14903", "authors": ["Pan Hu"], "title": "CoMoCAVs: Cohesive Decision-Guided Motion Planning for Connected and Autonomous Vehicles with Multi-Policy Reinforcement Learning", "comment": "8 pages, 5 figures", "summary": "Autonomous driving demands reliable and efficient solutions to closely\nrelated problems such as decision-making and motion planning. In this work,\ndecision-making refers specifically to highway lane selection, while motion\nplanning involves generating control commands (such as speed and steering) to\nreach the chosen lane. In the context of Connected Autonomous Vehicles (CAVs),\nachieving both flexible and safe lane selection alongside precise trajectory\nexecution remains a significant challenge. This paper proposes a framework\ncalled Cohesive Decision-Guided Motion Planning (CDGMP), which tightly\nintegrates decision-making and motion planning using a Mixture of Experts (MoE)\ninspired architecture combined with multi-policy reinforcement learning. By\ncoordinating multiple specialized sub-networks through a gating mechanism, the\nmethod decomposes the complex driving task into modular components. Each\nsub-network focuses on a specific aspect of driving, improving efficiency by\nactivating only the most relevant modules during inference. This design also\nenhances safety through modular specialization. CDGMP improves the adaptability\nand robustness of CAVs across diverse traffic scenarios, offering a scalable\nsolution to real-world autonomy challenges. The architectural principles behind\nCDGMP, especially the use of MoE, also provide a strong foundation for other\nhigh-dimensional decision and control tasks. Simulation results (available at\nhttps://youtu.be/_-4OXNHV0UY) demonstrate reliable performance in both lane\nselection and motion planning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCDGMP\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\u548c\u591a\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\uff0c\u7d27\u5bc6\u96c6\u6210\u51b3\u7b56\u5236\u5b9a\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u4ee5\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7684\u7075\u6d3b\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9700\u8981\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5904\u7406\u51b3\u7b56\u5236\u5b9a\uff08\u5982\u8f66\u9053\u9009\u62e9\uff09\u548c\u8fd0\u52a8\u89c4\u5212\uff08\u5982\u901f\u5ea6\u548c\u8f6c\u5411\u63a7\u5236\uff09\u7b49\u7d27\u5bc6\u76f8\u5173\u7684\u95ee\u9898\u3002\u5f53\u524d\u5728\u8054\u7f51\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08CAVs\uff09\u4e2d\uff0c\u5b9e\u73b0\u7075\u6d3b\u5b89\u5168\u7684\u8f66\u9053\u9009\u62e9\u548c\u7cbe\u786e\u7684\u8f68\u8ff9\u6267\u884c\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "CDGMP\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\u548c\u591a\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u534f\u8c03\u591a\u4e2a\u4e13\u7528\u5b50\u7f51\u7edc\uff0c\u5c06\u590d\u6742\u9a7e\u9a76\u4efb\u52a1\u5206\u89e3\u4e3a\u6a21\u5757\u5316\u7ec4\u4ef6\u3002\u6bcf\u4e2a\u5b50\u7f51\u7edc\u4e13\u6ce8\u4e8e\u9a7e\u9a76\u7684\u7279\u5b9a\u65b9\u9762\uff0c\u4ec5\u5728\u63a8\u7406\u65f6\u6fc0\u6d3b\u6700\u76f8\u5173\u7684\u6a21\u5757\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cCDGMP\u5728\u8f66\u9053\u9009\u62e9\u548c\u8fd0\u52a8\u89c4\u5212\u65b9\u9762\u8868\u73b0\u53ef\u9760\u3002\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86CAV\u5728\u4e0d\u540c\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "CDGMP\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u67b6\u6784\uff08\u5c24\u5176\u662fMoE\u7684\u5e94\u7528\uff09\u4e5f\u4e3a\u5176\u4ed6\u9ad8\u7ef4\u51b3\u7b56\u548c\u63a7\u5236\u4efb\u52a1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.14914", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14914", "abs": "https://arxiv.org/abs/2507.14914", "authors": ["Zhexuan Xu", "Jie Wang", "Siyuan Xu", "Zijie Geng", "Mingxuan Yuan", "Feng Wu"], "title": "One Step Beyond: Feedthrough & Placement-Aware Rectilinear Floorplanner", "comment": null, "summary": "Floorplanning determines the shapes and locations of modules on a chip canvas\nand plays a critical role in optimizing the chip's Power, Performance, and Area\n(PPA) metrics. However, existing floorplanning approaches often fail to\nintegrate with subsequent physical design stages, leading to suboptimal\nin-module component placement and excessive inter-module feedthrough. To tackle\nthis challenge, we propose Flora, a three-stage feedthrough and placement aware\nrectilinear floorplanner. In the first stage, Flora employs wiremask and\nposition mask techniques to achieve coarse-grained optimization of HPWL and\nfeedthrough. In the second stage, under the constraint of a fixed outline,\nFlora achieves a zero-whitespace layout by locally resizing module shapes,\nthereby performing fine-grained optimization of feedthrough and improving\ncomponent placement. In the third stage, Flora utilizes a fast tree\nsearch-based method to efficiently place components-including macros and\nstandard cells-within each module, subsequently adjusting module boundaries\nbased on the placement results to enable cross-stage optimization. Experimental\nresults show that Flora outperforms recent state-of-the-art floorplanning\napproaches, achieving an average reduction of 6% in HPWL, 5.16% in FTpin,\n29.15% in FTmod, and a 14% improvement in component placement performance.", "AI": {"tldr": "Flora\u662f\u4e00\u4e2a\u4e09\u9636\u6bb5\u3001\u8003\u8651\u9988\u901a\u548c\u5e03\u5c40\u7684\u77e9\u5f62\u5e73\u9762\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u4f18\u5316HPWL\u3001\u9988\u901a\u548c\u7ec4\u4ef6\u5e03\u5c40\uff0c\u663e\u8457\u63d0\u5347\u4e86\u82af\u7247\u8bbe\u8ba1\u7684PPA\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u5e73\u9762\u89c4\u5212\u65b9\u6cd5\u96be\u4ee5\u4e0e\u540e\u7eed\u7269\u7406\u8bbe\u8ba1\u9636\u6bb5\u96c6\u6210\uff0c\u5bfc\u81f4\u6a21\u5757\u5185\u7ec4\u4ef6\u5e03\u5c40\u4e0d\u4f18\u548c\u6a21\u5757\u95f4\u9988\u901a\u8fc7\u591a\u3002", "method": "Flora\u5206\u4e09\u9636\u6bb5\uff1a1) \u4f7f\u7528\u7ebf\u63a9\u7801\u548c\u4f4d\u7f6e\u63a9\u7801\u7c97\u4f18\u5316HPWL\u548c\u9988\u901a\uff1b2) \u5728\u56fa\u5b9a\u8f6e\u5ed3\u4e0b\u901a\u8fc7\u5c40\u90e8\u8c03\u6574\u6a21\u5757\u5f62\u72b6\u5b9e\u73b0\u96f6\u7a7a\u767d\u5e03\u5c40\uff1b3) \u57fa\u4e8e\u6811\u641c\u7d22\u5feb\u901f\u653e\u7f6e\u7ec4\u4ef6\u5e76\u8c03\u6574\u6a21\u5757\u8fb9\u754c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cFlora\u5e73\u5747\u51cf\u5c116% HPWL\u30015.16% FTpin\u300129.15% FTmod\uff0c\u7ec4\u4ef6\u5e03\u5c40\u6027\u80fd\u63d0\u534714%\u3002", "conclusion": "Flora\u901a\u8fc7\u8de8\u9636\u6bb5\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e73\u9762\u89c4\u5212\u7684\u6548\u679c\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.14929", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14929", "abs": "https://arxiv.org/abs/2507.14929", "authors": ["Tero Kaarlela", "Sami Salo", "Jose Outeiro"], "title": "Digital twin and extended reality for teleoperation of the electric vehicle battery disassembly", "comment": null, "summary": "Disassembling and sorting Electric Vehicle Batteries (EVBs) supports a\nsustainable transition to electric vehicles by enabling a closed-loop supply\nchain. Currently, the manual disassembly process exposes workers to hazards,\nincluding electrocution and toxic chemicals. We propose a teleoperated system\nfor the safe disassembly and sorting of EVBs. A human-in-the-loop can create\nand save disassembly sequences for unknown EVB types, enabling future\nautomation. An RGB camera aligns the physical and digital twins of the EVB, and\nthe digital twin of the robot is based on the Robot Operating System (ROS)\nmiddleware. This hybrid approach combines teleoperation and automation to\nimprove safety, adaptability, and efficiency in EVB disassembly and sorting.\nThe economic contribution is realized by reducing labor dependency and\nincreasing throughput in battery recycling. An online pilot study was set up to\nevaluate the usability of the presented approach, and the results demonstrate\nthe potential as a user-friendly solution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fdc\u7a0b\u64cd\u4f5c\u4e0e\u81ea\u52a8\u5316\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b89\u5168\u62c6\u5378\u548c\u5206\u7c7b\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\uff08EVB\uff09\uff0c\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027\u3001\u9002\u5e94\u6027\u548c\u6548\u7387\u3002", "motivation": "\u624b\u52a8\u62c6\u5378EVB\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff08\u5982\u89e6\u7535\u548c\u6709\u6bd2\u5316\u5b66\u7269\u8d28\uff09\uff0c\u4e14\u6548\u7387\u4f4e\u4e0b\u3002\u901a\u8fc7\u8fdc\u7a0b\u64cd\u4f5c\u548c\u81ea\u52a8\u5316\u6280\u672f\uff0c\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u63a8\u52a8\u53ef\u6301\u7eed\u7684\u7535\u52a8\u6c7d\u8f66\u4f9b\u5e94\u94fe\u3002", "method": "\u91c7\u7528\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u7ed3\u5408\u4eba\u7c7b\u64cd\u4f5c\u5458\u548c\u81ea\u52a8\u5316\u6280\u672f\u3002RGB\u6444\u50cf\u5934\u7528\u4e8e\u5bf9\u9f50\u7269\u7406\u548c\u6570\u5b57\u5b6a\u751f\uff0c\u673a\u5668\u4eba\u6570\u5b57\u5b6a\u751f\u57fa\u4e8eROS\u4e2d\u95f4\u4ef6\u3002", "result": "\u5728\u7ebf\u8bd5\u70b9\u7814\u7a76\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5177\u6709\u7528\u6237\u53cb\u597d\u6027\uff0c\u5e76\u80fd\u51cf\u5c11\u5bf9\u52b3\u52a8\u529b\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u7535\u6c60\u56de\u6536\u6548\u7387\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3aEVB\u62c6\u5378\u548c\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b89\u5168\u3001\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u53ef\u6301\u7eed\u7684\u7535\u52a8\u6c7d\u8f66\u4f9b\u5e94\u94fe\u3002"}}
{"id": "2507.14931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14931", "abs": "https://arxiv.org/abs/2507.14931", "authors": ["Qiaoqiao Ren", "Remko Proesmans", "Arend Pissens", "Lara Dehandschutter", "William Denecker", "Lotte Rouckhout", "Joke Carrette", "Peter Vanhopplinus", "Tony Belpaeme", "Francis wyffels"], "title": "Designing Robots with, not for: A Co-Design Framework for Empowering Interactions in Forensic Psychiatry", "comment": null, "summary": "Forensic mental health care involves the treatment of individuals with severe\nmental disorders who have committed violent offences. These settings are often\ncharacterized by high levels of bureaucracy, risk avoidance, and restricted\nautonomy. Patients frequently experience a profound loss of control over their\nlives, leading to heightened psychological stress-sometimes resulting in\nisolation as a safety measure. In this study, we explore how co-design can be\nused to collaboratively develop a companion robot that helps monitor and\nregulate stress while maintaining tracking of the patients' interaction\nbehaviours for long-term intervention. We conducted four co-design workshops in\na forensic psychiatric clinic with patients, caregivers, and therapists. Our\nprocess began with the presentation of an initial speculative prototype to\ntherapists, enabling reflection on shared concerns, ethical risks, and\ndesirable features. This was followed by a creative ideation session with\npatients, a third workshop focused on defining desired functions and emotional\nresponses, and we are planning a final prototype demo to gather direct patient\nfeedback. Our findings emphasize the importance of empowering patients in the\ndesign process and adapting proposals based on their current emotional state.\nThe goal was to empower the patient in the design process and ensure each\npatient's voice was heard.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u6cd5\u533b\u5fc3\u7406\u5065\u5eb7\u62a4\u7406\u4e2d\uff0c\u901a\u8fc7\u5171\u540c\u8bbe\u8ba1\u5f00\u53d1\u966a\u4f34\u673a\u5668\u4eba\uff0c\u5e2e\u52a9\u76d1\u6d4b\u548c\u8c03\u8282\u60a3\u8005\u538b\u529b\uff0c\u540c\u65f6\u8ddf\u8e2a\u4e92\u52a8\u884c\u4e3a\u4ee5\u8fdb\u884c\u957f\u671f\u5e72\u9884\u3002", "motivation": "\u6cd5\u533b\u5fc3\u7406\u5065\u5eb7\u62a4\u7406\u73af\u5883\u901a\u5e38\u5b98\u50da\u4e3b\u4e49\u4e25\u91cd\u3001\u98ce\u9669\u89c4\u907f\u6027\u5f3a\u4e14\u60a3\u8005\u81ea\u4e3b\u6743\u53d7\u9650\uff0c\u5bfc\u81f4\u60a3\u8005\u5fc3\u7406\u538b\u529b\u5927\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5171\u540c\u8bbe\u8ba1\u6539\u5584\u60a3\u8005\u4f53\u9a8c\u3002", "method": "\u5728\u6cd5\u533b\u7cbe\u795e\u75c5\u8bca\u6240\u8fdb\u884c\u4e86\u56db\u6b21\u5171\u540c\u8bbe\u8ba1\u5de5\u4f5c\u574a\uff0c\u53c2\u4e0e\u8005\u5305\u62ec\u60a3\u8005\u3001\u62a4\u7406\u4eba\u5458\u548c\u6cbb\u7597\u5e08\uff0c\u4ece\u539f\u578b\u5c55\u793a\u5230\u529f\u80fd\u5b9a\u4e49\u9010\u6b65\u63a8\u8fdb\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8ba9\u60a3\u8005\u53c2\u4e0e\u8bbe\u8ba1\u8fc7\u7a0b\u5e76\u57fa\u4e8e\u5176\u60c5\u7eea\u72b6\u6001\u8c03\u6574\u65b9\u6848\u81f3\u5173\u91cd\u8981\uff0c\u76ee\u6807\u662f\u786e\u4fdd\u6bcf\u4f4d\u60a3\u8005\u7684\u58f0\u97f3\u88ab\u542c\u5230\u3002", "conclusion": "\u5171\u540c\u8bbe\u8ba1\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u60a3\u8005\u81ea\u4e3b\u6743\uff0c\u4e3a\u6cd5\u533b\u5fc3\u7406\u5065\u5eb7\u62a4\u7406\u63d0\u4f9b\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14967", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14967", "abs": "https://arxiv.org/abs/2507.14967", "authors": ["Pratik Ingle", "Kasper St\u00f8y", "Andres Fai\u00f1a"], "title": "Heterogeneous object manipulation on nonlinear soft surface through linear controller", "comment": "8 pages, 3 figures", "summary": "Manipulation surfaces indirectly control and reposition objects by actively\nmodifying their shape or properties rather than directly gripping objects.\nThese surfaces, equipped with dense actuator arrays, generate dynamic\ndeformations. However, a high-density actuator array introduces considerable\ncomplexity due to increased degrees of freedom (DOF), complicating control\ntasks. High DOF restrict the implementation and utilization of manipulation\nsurfaces in real-world applications as the maintenance and control of such\nsystems exponentially increase with array/surface size. Learning-based control\napproaches may ease the control complexity, but they require extensive training\nsamples and struggle to generalize for heterogeneous objects. In this study, we\nintroduce a simple, precise and robust PID-based linear close-loop feedback\ncontrol strategy for heterogeneous object manipulation on MANTA-RAY\n(Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation\ndensity). Our approach employs a geometric transformation-driven PID\ncontroller, directly mapping tilt angle control outputs(1D/2D) to actuator\ncommands to eliminate the need for extensive black-box training. We validate\nthe proposed method through simulations and experiments on a physical system,\nsuccessfully manipulating objects with diverse geometries, weights and\ntextures, including fragile objects like eggs and apples. The outcomes\ndemonstrate that our approach is highly generalized and offers a practical and\nreliable solution for object manipulation on soft robotic manipulation,\nfacilitating real-world implementation without prohibitive training demands.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePID\u7684\u7ebf\u6027\u95ed\u73af\u53cd\u9988\u63a7\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u51cf\u5c11\u9ad8\u5bc6\u5ea6\u6267\u884c\u5668\u9635\u5217\u7684\u590d\u6742\u6027\uff0c\u5b9e\u73b0\u5f02\u6784\u7269\u4f53\u7684\u7cbe\u786e\u64cd\u63a7\u3002", "motivation": "\u9ad8\u5bc6\u5ea6\u6267\u884c\u5668\u9635\u5217\u7684\u590d\u6742\u6027\u548c\u63a7\u5236\u96be\u5ea6\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u4f20\u7edf\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6837\u672c\u4e14\u96be\u4ee5\u6cdb\u5316\u3002", "method": "\u91c7\u7528\u51e0\u4f55\u53d8\u6362\u9a71\u52a8\u7684PID\u63a7\u5236\u5668\uff0c\u76f4\u63a5\u5c06\u503e\u659c\u89d2\u5ea6\u63a7\u5236\u8f93\u51fa\u6620\u5c04\u5230\u6267\u884c\u5668\u547d\u4ee4\uff0c\u907f\u514d\u9ed1\u76d2\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u7269\u7406\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u6210\u529f\u64cd\u63a7\u4e86\u591a\u79cd\u51e0\u4f55\u3001\u91cd\u91cf\u548c\u7eb9\u7406\u7684\u7269\u4f53\uff0c\u5305\u62ec\u6613\u788e\u7269\u54c1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u9ad8\u5ea6\u6cdb\u5316\u6027\uff0c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u64cd\u63a7\u63d0\u4f9b\u4e86\u5b9e\u7528\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u3002"}}
{"id": "2507.14975", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14975", "abs": "https://arxiv.org/abs/2507.14975", "authors": ["Yufan Song", "Jiatao Zhang", "Zeng Gu", "Qingmiao Liang", "Tuocheng Hu", "Wei Song", "Shiqiang Zhu"], "title": "FCRF: Flexible Constructivism Reflection for Long-Horizon Robotic Task Planning with Large Language Models", "comment": "8 pages, 6 figures, IROS 2025", "summary": "Autonomous error correction is critical for domestic robots to achieve\nreliable execution of complex long-horizon tasks. Prior work has explored\nself-reflection in Large Language Models (LLMs) for task planning error\ncorrection; however, existing methods are constrained by inflexible\nself-reflection mechanisms that limit their effectiveness. Motivated by these\nlimitations and inspired by human cognitive adaptation, we propose the Flexible\nConstructivism Reflection Framework (FCRF), a novel Mentor-Actor architecture\nthat enables LLMs to perform flexible self-reflection based on task difficulty,\nwhile constructively integrating historical valuable experience with failure\nlessons. We evaluated FCRF on diverse domestic tasks through simulation in\nAlfWorld and physical deployment in the real-world environment. Experimental\nresults demonstrate that FCRF significantly improves overall performance and\nself-reflection flexibility in complex long-horizon robotic tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFCRF\u7684\u7075\u6d3b\u81ea\u6211\u53cd\u601d\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u5bb6\u7528\u673a\u5668\u4eba\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\u7ea0\u6b63\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u7684\u81ea\u6211\u53cd\u601d\u673a\u5236\u4e0d\u591f\u7075\u6d3b\uff0c\u9650\u5236\u4e86\u5176\u5728\u4efb\u52a1\u89c4\u5212\u9519\u8bef\u7ea0\u6b63\u4e2d\u7684\u6548\u679c\u3002", "method": "\u91c7\u7528Mentor-Actor\u67b6\u6784\uff0c\u7ed3\u5408\u4efb\u52a1\u96be\u5ea6\u548c\u5386\u53f2\u7ecf\u9a8c\u8fdb\u884c\u7075\u6d3b\u53cd\u601d\u3002", "result": "\u5728AlfWorld\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0cFCRF\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u53cd\u601d\u7075\u6d3b\u6027\u3002", "conclusion": "FCRF\u4e3a\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u81ea\u4e3b\u9519\u8bef\u7ea0\u6b63\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15022", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.15022", "abs": "https://arxiv.org/abs/2507.15022", "authors": ["Sumeadh MS", "Kevin Dsouza", "Ravi Prakash"], "title": "CPED-NCBFs: A Conformal Prediction for Expert Demonstration-based Neural Control Barrier Functions", "comment": "6pages, 4figures, Submitted to the prestigious Indian Control\n  Conference (ICC), 2025", "summary": "Among the promising approaches to enforce safety in control systems, learning\nControl Barrier Functions (CBFs) from expert demonstrations has emerged as an\neffective strategy. However, a critical challenge remains: verifying that the\nlearned CBFs truly enforce safety across the entire state space. This is\nespecially difficult when CBF is represented using neural networks (NCBFs).\nSeveral existing verification techniques attempt to address this problem\nincluding SMT-based solvers, mixed-integer programming (MIP), and interval or\nbound-propagation methods but these approaches often introduce loose,\nconservative bounds. To overcome these limitations, in this work we use\nCPED-NCBFs a split-conformal prediction based verification strategy to verify\nthe learned NCBF from the expert demonstrations. We further validate our method\non point mass systems and unicycle models to demonstrate the effectiveness of\nthe proposed theory.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5f62\u5171\u5f62\u9884\u6d4b\u7684\u9a8c\u8bc1\u7b56\u7565\uff08CPED-NCBFs\uff09\uff0c\u7528\u4e8e\u9a8c\u8bc1\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u5b66\u4e60\u7684\u795e\u7ecf\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08NCBFs\uff09\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5\uff08\u5982SMT\u6c42\u89e3\u5668\u3001\u6df7\u5408\u6574\u6570\u89c4\u5212\u7b49\uff09\u5728\u9a8c\u8bc1NCBFs\u65f6\u5b58\u5728\u4fdd\u5b88\u6027\u9ad8\u3001\u8fb9\u754c\u677e\u6563\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u9a8c\u8bc1\u7b56\u7565\u3002", "method": "\u91c7\u7528\u5206\u5f62\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\uff08CPED-NCBFs\uff09\u9a8c\u8bc1NCBFs\uff0c\u5e76\u5728\u70b9\u8d28\u91cf\u7cfb\u7edf\u548c\u975e\u5b8c\u6574\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCPED-NCBFs\u65b9\u6cd5\u80fd\u6709\u6548\u9a8c\u8bc1NCBFs\u7684\u5b89\u5168\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684CPED-NCBFs\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u9a8c\u8bc1\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u4e3aNCBFs\u7684\u5b89\u5168\u6027\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u66f4\u4f18\u65b9\u6848\u3002"}}
{"id": "2507.15062", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15062", "abs": "https://arxiv.org/abs/2507.15062", "authors": ["Xinyue Zhu", "Binghao Huang", "Yunzhu Li"], "title": "Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper", "comment": "More videos can be found on our\n  website:https://binghao-huang.github.io/touch_in_the_wild/", "summary": "Handheld grippers are increasingly used to collect human demonstrations due\nto their ease of deployment and versatility. However, most existing designs\nlack tactile sensing, despite the critical role of tactile feedback in precise\nmanipulation. We present a portable, lightweight gripper with integrated\ntactile sensors that enables synchronized collection of visual and tactile data\nin diverse, real-world, and in-the-wild settings. Building on this hardware, we\npropose a cross-modal representation learning framework that integrates visual\nand tactile signals while preserving their distinct characteristics. The\nlearning procedure allows the emergence of interpretable representations that\nconsistently focus on contacting regions relevant for physical interactions.\nWhen used for downstream manipulation tasks, these representations enable more\nefficient and effective policy learning, supporting precise robotic\nmanipulation based on multimodal feedback. We validate our approach on\nfine-grained tasks such as test tube insertion and pipette-based fluid\ntransfer, demonstrating improved accuracy and robustness under external\ndisturbances. Our project page is available at\nhttps://binghao-huang.github.io/touch_in_the_wild/ .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4fbf\u643a\u5f0f\u3001\u8f7b\u91cf\u7ea7\u5939\u6301\u5668\uff0c\u96c6\u6210\u4e86\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u7528\u4e8e\u540c\u6b65\u6536\u96c6\u89c6\u89c9\u548c\u89e6\u89c9\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6846\u67b6\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7cbe\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u624b\u6301\u5939\u6301\u5668\u7f3a\u4e4f\u89e6\u89c9\u53cd\u9988\uff0c\u800c\u89e6\u89c9\u53cd\u9988\u5728\u7cbe\u786e\u64cd\u4f5c\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86\u96c6\u6210\u89e6\u89c9\u4f20\u611f\u5668\u7684\u5939\u6301\u5668\uff0c\u5e76\u63d0\u51fa\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u4fe1\u53f7\u3002", "result": "\u5728\u8bd5\u7ba1\u63d2\u5165\u548c\u79fb\u6db2\u7ba1\u6d41\u4f53\u8f6c\u79fb\u7b49\u4efb\u52a1\u4e2d\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u652f\u6301\u57fa\u4e8e\u591a\u6a21\u6001\u53cd\u9988\u7684\u9ad8\u6548\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u63d0\u5347\u4e86\u7cbe\u786e\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2507.15088", "categories": ["cs.RO", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.15088", "abs": "https://arxiv.org/abs/2507.15088", "authors": ["Pouya Panahandeh", "Mohammad Pirani", "Baris Fidan", "Amir Khajepour"], "title": "Search-Based Autonomous Vehicle Motion Planning Using Game Theory", "comment": null, "summary": "In this paper, we propose a search-based interactive motion planning scheme\nfor autonomous vehicles (AVs), using a game-theoretic approach. In contrast to\ntraditional search-based approaches, the newly developed approach considers\nother road users (e.g. drivers and pedestrians) as intelligent agents rather\nthan static obstacles. This leads to the generation of a more realistic path\nfor the AV. Due to the low computational time, the proposed motion planning\nscheme is implementable in real-time applications. The performance of the\ndeveloped motion planning scheme is compared with existing motion planning\ntechniques and validated through experiments using WATonoBus, an electrical\nall-weather autonomous shuttle bus.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u641c\u7d22\u7684\u4ea4\u4e92\u5f0f\u8fd0\u52a8\u89c4\u5212\u65b9\u6848\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff0c\u91c7\u7528\u535a\u5f08\u8bba\u65b9\u6cd5\uff0c\u5c06\u5176\u4ed6\u9053\u8def\u4f7f\u7528\u8005\u89c6\u4e3a\u667a\u80fd\u4f53\u800c\u975e\u9759\u6001\u969c\u788d\u7269\uff0c\u751f\u6210\u66f4\u771f\u5b9e\u7684\u8def\u5f84\uff0c\u5e76\u5b9e\u73b0\u5b9e\u65f6\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u641c\u7d22\u65b9\u6cd5\u5c06\u5176\u4ed6\u9053\u8def\u7528\u6237\u89c6\u4e3a\u9759\u6001\u969c\u788d\u7269\uff0c\u7f3a\u4e4f\u5bf9\u667a\u80fd\u884c\u4e3a\u7684\u8003\u8651\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u771f\u5b9e\u7684\u89c4\u5212\u65b9\u6848\u3002", "method": "\u91c7\u7528\u535a\u5f08\u8bba\u65b9\u6cd5\uff0c\u5c06\u5176\u4ed6\u9053\u8def\u7528\u6237\u5efa\u6a21\u4e3a\u667a\u80fd\u4f53\uff0c\u7ed3\u5408\u641c\u7d22\u6280\u672f\u751f\u6210\u8fd0\u52a8\u8def\u5f84\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6848\u8ba1\u7b97\u65f6\u95f4\u77ed\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6848\u901a\u8fc7\u535a\u5f08\u8bba\u548c\u641c\u7d22\u6280\u672f\u7684\u7ed3\u5408\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u8fd0\u52a8\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2507.15155", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15155", "abs": "https://arxiv.org/abs/2507.15155", "authors": ["Majid Roshanfar", "Alex Zhang", "Changyan He", "Amir Hooshiar", "Dale J. Podolsky", "Thomas Looi", "Eric Diller"], "title": "Learning-Based Modeling of a Magnetically Steerable Soft Suction Device for Endoscopic Endonasal Interventions", "comment": null, "summary": "This letter introduces a novel learning-based modeling framework for a\nmagnetically steerable soft suction device designed for endoscopic endonasal\nbrain tumor resection. The device is miniaturized (4 mm outer diameter, 2 mm\ninner diameter, 40 mm length), 3D printed using biocompatible SIL 30 material,\nand integrates embedded Fiber Bragg Grating (FBG) sensors for real-time shape\nfeedback. Shape reconstruction is represented using four Bezier control points,\nenabling a compact and smooth model of the device's deformation. A data-driven\nmodel was trained on 5,097 experimental samples covering a range of magnetic\nfield magnitudes (0-14 mT), actuation frequencies (0.2-1.0 Hz), and vertical\ntip distances (90-100 mm), using both Neural Network (NN) and Random Forest\n(RF) architectures. The RF model outperformed the NN across all metrics,\nachieving a mean root mean square error of 0.087 mm in control point prediction\nand a mean shape reconstruction error of 0.064 mm. Feature importance analysis\nfurther revealed that magnetic field components predominantly influence distal\ncontrol points, while frequency and distance affect the base configuration.\nThis learning-based approach effectively models the complex nonlinear behavior\nof hyperelastic soft robots under magnetic actuation without relying on\nsimplified physical assumptions. By enabling sub-millimeter shape prediction\naccuracy and real-time inference, this work represents an advancement toward\nthe intelligent control of magnetically actuated soft robotic tools in\nminimally invasive neurosurgery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u78c1\u63a7\u8f6f\u5438\u5f15\u88c5\u7f6e\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u5185\u7aa5\u955c\u9f3b\u5185\u8111\u80bf\u7624\u5207\u9664\uff0c\u5b9e\u73b0\u4e86\u4e9a\u6beb\u7c73\u7ea7\u7684\u5f62\u72b6\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u5c0f\u578b\u5316\u3001\u751f\u7269\u517c\u5bb9\u7684\u78c1\u63a7\u8f6f\u5438\u5f15\u88c5\u7f6e\uff0c\u7528\u4e8e\u5fae\u521b\u795e\u7ecf\u5916\u79d1\u624b\u672f\uff0c\u89e3\u51b3\u4f20\u7edf\u7269\u7406\u5047\u8bbe\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u75283D\u6253\u5370\u6280\u672f\u5236\u9020\u88c5\u7f6e\uff0c\u96c6\u6210FBG\u4f20\u611f\u5668\u5b9e\u65f6\u53cd\u9988\u5f62\u72b6\uff0c\u91c7\u7528Bezier\u63a7\u5236\u70b9\u8868\u793a\u5f62\u72b6\u91cd\u5efa\uff0c\u5e76\u901a\u8fc7NN\u548cRF\u6a21\u578b\u8bad\u7ec3\u5b9e\u9a8c\u6570\u636e\u3002", "result": "RF\u6a21\u578b\u8868\u73b0\u4f18\u4e8eNN\uff0c\u63a7\u5236\u70b9\u9884\u6d4b\u5747\u65b9\u6839\u8bef\u5dee\u4e3a0.087 mm\uff0c\u5f62\u72b6\u91cd\u5efa\u8bef\u5dee\u4e3a0.064 mm\u3002", "conclusion": "\u8be5\u5b66\u4e60\u6846\u67b6\u6709\u6548\u5efa\u6a21\u4e86\u78c1\u63a7\u8f6f\u673a\u5668\u4eba\u7684\u975e\u7ebf\u6027\u884c\u4e3a\uff0c\u4e3a\u5fae\u521b\u795e\u7ecf\u5916\u79d1\u624b\u672f\u4e2d\u7684\u667a\u80fd\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.15189", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15189", "abs": "https://arxiv.org/abs/2507.15189", "authors": ["Kevin Christiansen Marsim", "Jinwoo Jeon", "Yeeun Kim", "Myeongwoo Jeong", "Hyun Myung"], "title": "CHADET: Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer", "comment": null, "summary": "Depth information which specifies the distance between objects and current\nposition of the robot is essential for many robot tasks such as navigation.\nRecently, researchers have proposed depth completion frameworks to provide\ndense depth maps that offer comprehensive information about the surrounding\nenvironment. However, existing methods show significant trade-offs between\ncomputational efficiency and accuracy during inference. The substantial memory\nand computational requirements make them unsuitable for real-time applications,\nhighlighting the need to improve the completeness and accuracy of depth\ninformation while improving processing speed to enhance robot performance in\nvarious tasks. To address these challenges, in this paper, we propose\nCHADET(cross-hierarchical-attention depth-completion transformer), a\nlightweight depth-completion network that can generate accurate dense depth\nmaps from RGB images and sparse depth points. For each pair, its feature is\nextracted from the depthwise blocks and passed to the equally lightweight\ntransformer-based decoder. In the decoder, we utilize the novel\ncross-hierarchical-attention module that refines the image features from the\ndepth information. Our approach improves the quality and reduces memory usage\nof the depth map prediction, as validated in both KITTI, NYUv2, and VOID\ndatasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u8865\u5168\u7f51\u7edcCHADET\uff0c\u901a\u8fc7RGB\u56fe\u50cf\u548c\u7a00\u758f\u6df1\u5ea6\u70b9\u751f\u6210\u7cbe\u786e\u7684\u5bc6\u96c6\u6df1\u5ea6\u56fe\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u4fe1\u606f\u5bf9\u673a\u5668\u4eba\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u8865\u5168\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6743\u8861\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u9700\u6c42\u3002", "method": "\u63d0\u51faCHADET\u7f51\u7edc\uff0c\u7ed3\u5408\u6df1\u5ea6\u5757\u7279\u5f81\u63d0\u53d6\u548c\u8f7b\u91cf\u7ea7\u57fa\u4e8eTransformer\u7684\u89e3\u7801\u5668\uff0c\u5229\u7528\u8de8\u5c42\u6b21\u6ce8\u610f\u529b\u6a21\u5757\u4f18\u5316\u56fe\u50cf\u7279\u5f81\u3002", "result": "\u5728KITTI\u3001NYUv2\u548cVOID\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63d0\u9ad8\u4e86\u6df1\u5ea6\u56fe\u9884\u6d4b\u8d28\u91cf\u5e76\u51cf\u5c11\u4e86\u5185\u5b58\u4f7f\u7528\u3002", "conclusion": "CHADET\u5728\u4fdd\u6301\u8f7b\u91cf\u7ea7\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6df1\u5ea6\u8865\u5168\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u673a\u5668\u4eba\u4efb\u52a1\u3002"}}
{"id": "2507.15266", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.15266", "abs": "https://arxiv.org/abs/2507.15266", "authors": ["Haichao Liu", "Haoren Guo", "Pei Liu", "Benshan Ma", "Yuxiang Zhang", "Jun Ma", "Tong Heng Lee"], "title": "VLM-UDMC: VLM-Enhanced Unified Decision-Making and Motion Control for Urban Autonomous Driving", "comment": "14 pages, 12 figures", "summary": "Scene understanding and risk-aware attentions are crucial for human drivers\nto make safe and effective driving decisions. To imitate this cognitive ability\nin urban autonomous driving while ensuring the transparency and\ninterpretability, we propose a vision-language model (VLM)-enhanced unified\ndecision-making and motion control framework, named VLM-UDMC. This framework\nincorporates scene reasoning and risk-aware insights into an upper-level slow\nsystem, which dynamically reconfigures the optimal motion planning for the\ndownstream fast system. The reconfiguration is based on real-time environmental\nchanges, which are encoded through context-aware potential functions. More\nspecifically, the upper-level slow system employs a two-step reasoning policy\nwith Retrieval-Augmented Generation (RAG), leveraging foundation models to\nprocess multimodal inputs and retrieve contextual knowledge, thereby generating\nrisk-aware insights. Meanwhile, a lightweight multi-kernel decomposed LSTM\nprovides real-time trajectory predictions for heterogeneous traffic\nparticipants by extracting smoother trend representations for short-horizon\ntrajectory prediction. The effectiveness of the proposed VLM-UDMC framework is\nverified via both simulations and real-world experiments with a full-size\nautonomous vehicle. It is demonstrated that the presented VLM-UDMC effectively\nleverages scene understanding and attention decomposition for rational driving\ndecisions, thus improving the overall urban driving performance. Our\nopen-source project is available at https://github.com/henryhcliu/vlmudmc.git.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u7edf\u4e00\u51b3\u7b56\u4e0e\u8fd0\u52a8\u63a7\u5236\u6846\u67b6VLM-UDMC\uff0c\u901a\u8fc7\u573a\u666f\u63a8\u7406\u548c\u98ce\u9669\u611f\u77e5\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6a21\u4eff\u4eba\u7c7b\u9a7e\u9a76\u5458\u7684\u573a\u666f\u7406\u89e3\u548c\u98ce\u9669\u611f\u77e5\u80fd\u529b\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528VLM\u589e\u5f3a\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u591a\u6838\u5206\u89e3LSTM\uff0c\u5b9e\u73b0\u52a8\u6001\u8fd0\u52a8\u89c4\u5212\u548c\u5b9e\u65f6\u8f68\u8ff9\u9884\u6d4b\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\uff0cVLM-UDMC\u663e\u8457\u63d0\u5347\u4e86\u57ce\u5e02\u9a7e\u9a76\u6027\u80fd\u3002", "conclusion": "VLM-UDMC\u6846\u67b6\u6709\u6548\u7ed3\u5408\u573a\u666f\u7406\u89e3\u548c\u6ce8\u610f\u529b\u5206\u89e3\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u66f4\u5408\u7406\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2507.15293", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15293", "abs": "https://arxiv.org/abs/2507.15293", "authors": ["Shanshan Zhang", "Tianshui Wen", "Siyue Wang", "Qi Zhang", "Ziheng Zhou", "Lingxiang Zheng", "Yu Yang"], "title": "RepILN: Reparameterized Inertial Localization Network", "comment": null, "summary": "Inertial localization is regarded as a promising positioning solution for\nconsumer-grade IoT devices due to its cost-effectiveness and independence from\nexternal infrastructure. However, data-driven inertial localization methods\noften rely on increasingly complex network architectures to improve accuracy,\nwhich challenges the limited computational resources of IoT devices. Moreover,\nthese methods frequently overlook the importance of modeling long-term\ndependencies in inertial measurements - a critical factor for accurate\ntrajectory reconstruction - thereby limiting localization performance. To\naddress these challenges, we propose a reparameterized inertial localization\nnetwork that uses a multi-branch structure during training to enhance feature\nextraction. At inference time, this structure is transformed into an equivalent\nsingle-path architecture to improve parameter efficiency. To further capture\nlong-term dependencies in motion trajectories, we introduce a temporal-scale\nsparse attention mechanism that selectively emphasizes key trajectory segments\nwhile suppressing noise. Additionally, a gated convolutional unit is\nincorporated to effectively integrate long-range dependencies with local\nfine-grained features. Extensive experiments on public benchmarks demonstrate\nthat our method achieves a favorable trade-off between accuracy and model\ncompactness. For example, on the RoNIN dataset, our approach reduces the\nAbsolute Trajectory Error (ATE) by 2.59% compared to RoNIN-ResNet while\nreducing the number of parameters by 3.86%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u91cd\u53c2\u6570\u5316\u7684\u60ef\u6027\u5b9a\u4f4d\u7f51\u7edc\uff0c\u901a\u8fc7\u591a\u5206\u652f\u7ed3\u6784\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u8f6c\u6362\u4e3a\u5355\u8def\u5f84\u67b6\u6784\u4ee5\u63d0\u9ad8\u53c2\u6570\u6548\u7387\u3002\u5f15\u5165\u65f6\u95f4\u5c3a\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u548c\u95e8\u63a7\u5377\u79ef\u5355\u5143\uff0c\u4ee5\u6355\u6349\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u5e76\u6574\u5408\u5c40\u90e8\u7279\u5f81\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u6a21\u578b\u7d27\u51d1\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "motivation": "\u60ef\u6027\u5b9a\u4f4d\u56e0\u5176\u6210\u672c\u6548\u76ca\u548c\u72ec\u7acb\u6027\u6210\u4e3a\u7269\u8054\u7f51\u8bbe\u5907\u7684\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u7f51\u7edc\u67b6\u6784\uff0c\u4e14\u5ffd\u89c6\u957f\u671f\u4f9d\u8d56\u5efa\u6a21\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u91c7\u7528\u591a\u5206\u652f\u8bad\u7ec3\u7ed3\u6784\uff0c\u63a8\u7406\u65f6\u8f6c\u6362\u4e3a\u5355\u8def\u5f84\u67b6\u6784\uff1b\u5f15\u5165\u65f6\u95f4\u5c3a\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u548c\u95e8\u63a7\u5377\u79ef\u5355\u5143\u3002", "result": "\u5728RoNIN\u6570\u636e\u96c6\u4e0a\uff0c\u7edd\u5bf9\u8f68\u8ff9\u8bef\u5dee\u964d\u4f4e2.59%\uff0c\u53c2\u6570\u6570\u91cf\u51cf\u5c113.86%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u6a21\u578b\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u8bbe\u5907\u3002"}}
{"id": "2507.15444", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15444", "abs": "https://arxiv.org/abs/2507.15444", "authors": ["Leonard Bauersfeld", "Davide Scaramuzza"], "title": "Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow Pipe", "comment": "17 pages", "summary": "Autonomous quadrotor flight in confined spaces such as pipes and tunnels\npresents significant challenges due to unsteady, self-induced aerodynamic\ndisturbances. Very recent advances have enabled flight in such conditions, but\nthey either rely on constant motion through the pipe to mitigate airflow\nrecirculation effects or suffer from limited stability during hovering. In this\nwork, we present the first closed-loop control system for quadrotors for\nhovering in narrow pipes that leverages real-time flow field measurements. We\ndevelop a low-latency, event-based smoke velocimetry method that estimates\nlocal airflow at high temporal resolution. This flow information is used by a\ndisturbance estimator based on a recurrent convolutional neural network, which\ninfers force and torque disturbances in real time. The estimated disturbances\nare integrated into a learning-based controller trained via reinforcement\nlearning. The flow-feedback control proves particularly effective during\nlateral translation maneuvers in the pipe cross-section. There, the real-time\ndisturbance information enables the controller to effectively counteract\ntransient aerodynamic effects, thereby preventing collisions with the pipe\nwall. To the best of our knowledge, this work represents the first\ndemonstration of an aerial robot with closed-loop control informed by real-time\nflow field measurements. This opens new directions for research on flight in\naerodynamically complex environments. In addition, our work also sheds light on\nthe characteristic flow structures that emerge during flight in narrow,\ncircular pipes, providing new insights at the intersection of robotics and\nfluid dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u65f6\u6d41\u573a\u6d4b\u91cf\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u95ed\u73af\u63a7\u5236\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u72ed\u7a84\u7ba1\u9053\u4e2d\u60ac\u505c\uff0c\u89e3\u51b3\u4e86\u6c14\u6d41\u6270\u52a8\u95ee\u9898\u3002", "motivation": "\u5728\u72ed\u7a84\u7ba1\u9053\u4e2d\u98de\u884c\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u9762\u4e34\u6c14\u6d41\u6270\u52a8\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6301\u7eed\u8fd0\u52a8\u6216\u7a33\u5b9a\u6027\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u4f4e\u5ef6\u8fdf\u4e8b\u4ef6\u5f0f\u70df\u96fe\u6d4b\u901f\u6cd5\uff0c\u7ed3\u5408\u5faa\u73af\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u6270\u52a8\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63a7\u5236\u5668\u3002", "result": "\u7cfb\u7edf\u5728\u7ba1\u9053\u6a2a\u5411\u79fb\u52a8\u4e2d\u6709\u6548\u62b5\u6d88\u6c14\u6d41\u6270\u52a8\uff0c\u907f\u514d\u78b0\u649e\uff0c\u9996\u6b21\u5b9e\u73b0\u57fa\u4e8e\u5b9e\u65f6\u6d41\u573a\u6d4b\u91cf\u7684\u95ed\u73af\u63a7\u5236\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u590d\u6742\u6c14\u6d41\u73af\u5883\u4e2d\u7684\u98de\u884c\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u7ba1\u9053\u98de\u884c\u4e2d\u7684\u6d41\u4f53\u52a8\u529b\u5b66\u7279\u6027\u3002"}}
{"id": "2507.15469", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15469", "abs": "https://arxiv.org/abs/2507.15469", "authors": ["Thanh Thi Nguyen", "Saeid Nahavandi", "Imran Razzak", "Dung Nguyen", "Nhat Truong Pham", "Quoc Viet Hung Nguyen"], "title": "The Emergence of Deep Reinforcement Learning for Path Planning", "comment": "Accepted for publication in the Proceedings of the 2025 IEEE\n  International Conference on Systems, Man, and Cybernetics (SMC)", "summary": "The increasing demand for autonomous systems in complex and dynamic\nenvironments has driven significant research into intelligent path planning\nmethodologies. For decades, graph-based search algorithms, linear programming\ntechniques, and evolutionary computation methods have served as foundational\napproaches in this domain. Recently, deep reinforcement learning (DRL) has\nemerged as a powerful method for enabling autonomous agents to learn optimal\nnavigation strategies through interaction with their environments. This survey\nprovides a comprehensive overview of traditional approaches as well as the\nrecent advancements in DRL applied to path planning tasks, focusing on\nautonomous vehicles, drones, and robotic platforms. Key algorithms across both\nconventional and learning-based paradigms are categorized, with their\ninnovations and practical implementations highlighted. This is followed by a\nthorough discussion of their respective strengths and limitations in terms of\ncomputational efficiency, scalability, adaptability, and robustness. The survey\nconcludes by identifying key open challenges and outlining promising avenues\nfor future research. Special attention is given to hybrid approaches that\nintegrate DRL with classical planning techniques to leverage the benefits of\nboth learning-based adaptability and deterministic reliability, offering\npromising directions for robust and resilient autonomous navigation.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u667a\u80fd\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u5305\u62ec\u4f20\u7edf\u56fe\u641c\u7d22\u3001\u7ebf\u6027\u89c4\u5212\u548c\u8fdb\u5316\u8ba1\u7b97\uff0c\u4ee5\u53ca\u65b0\u5174\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\uff0c\u5e76\u63a2\u8ba8\u4e86\u5b83\u4eec\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u65e0\u4eba\u673a\u548c\u673a\u5668\u4eba\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u5bf9\u81ea\u4e3b\u7cfb\u7edf\u7684\u9700\u6c42\u589e\u52a0\uff0c\u63a8\u52a8\u4e86\u667a\u80fd\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u7684\u7814\u7a76\u3002", "method": "\u5206\u7c7b\u6bd4\u8f83\u4e86\u4f20\u7edf\u65b9\u6cd5\u548cDRL\u7b97\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b83\u4eec\u7684\u8ba1\u7b97\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "result": "\u603b\u7ed3\u4e86\u5404\u7c7b\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u7ed3\u5408DRL\u4e0e\u4f20\u7edf\u6280\u672f\u7684\u6df7\u5408\u65b9\u6cd5\u3002", "conclusion": "\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5f00\u653e\u6311\u6218\uff0c\u5c24\u5176\u662f\u6df7\u5408\u65b9\u6cd5\u5728\u63d0\u5347\u81ea\u4e3b\u5bfc\u822a\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.15474", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15474", "abs": "https://arxiv.org/abs/2507.15474", "authors": ["Charith Premachandra", "Achala Athukorala", "U-Xuan Tan"], "title": "All-UWB SLAM Using UWB Radar and UWB AOA", "comment": null, "summary": "There has been a growing interest in autonomous systems designed to operate\nin adverse conditions (e.g. smoke, dust), where the visible light spectrum\nfails. In this context, Ultra-wideband (UWB) radar is capable of penetrating\nthrough such challenging environmental conditions due to the lower frequency\ncomponents within its broad bandwidth. Therefore, UWB radar has emerged as a\npotential sensing technology for Simultaneous Localization and Mapping (SLAM)\nin vision-denied environments where optical sensors (e.g. LiDAR, Camera) are\nprone to failure. Existing approaches involving UWB radar as the primary\nexteroceptive sensor generally extract features in the environment, which are\nlater initialized as landmarks in a map. However, these methods are constrained\nby the number of distinguishable features in the environment. Hence, this paper\nproposes a novel method incorporating UWB Angle of Arrival (AOA) measurements\ninto UWB radar-based SLAM systems to improve the accuracy and scalability of\nSLAM in feature-deficient environments. The AOA measurements are obtained using\nUWB anchor-tag units which are dynamically deployed by the robot in featureless\nareas during mapping of the environment. This paper thoroughly discusses\nprevailing constraints associated with UWB AOA measurement units and presents\nsolutions to overcome them. Our experimental results show that integrating UWB\nAOA units with UWB radar enables SLAM in vision-denied feature-deficient\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408UWB\u96f7\u8fbe\u548cAOA\u6d4b\u91cf\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u89c6\u89c9\u53d7\u9650\u4e14\u7279\u5f81\u7f3a\u4e4f\u7684\u73af\u5883\u4e2d\u63d0\u5347SLAM\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5728\u6076\u52a3\u73af\u5883\uff08\u5982\u70df\u96fe\u3001\u7070\u5c18\uff09\u4e2d\uff0c\u5149\u5b66\u4f20\u611f\u5668\u6613\u5931\u6548\uff0cUWB\u96f7\u8fbe\u56e0\u5176\u7a7f\u900f\u80fd\u529b\u6210\u4e3aSLAM\u7684\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u73af\u5883\u7279\u5f81\u6570\u91cf\u3002", "method": "\u901a\u8fc7\u52a8\u6001\u90e8\u7f72UWB\u951a\u70b9-\u6807\u7b7e\u5355\u5143\u83b7\u53d6AOA\u6d4b\u91cf\uff0c\u5c06\u5176\u6574\u5408\u5230UWB\u96f7\u8fbeSLAM\u7cfb\u7edf\u4e2d\uff0c\u4ee5\u89e3\u51b3\u7279\u5f81\u7f3a\u4e4f\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408UWB AOA\u5355\u5143\u7684SLAM\u7cfb\u7edf\u80fd\u5728\u89c6\u89c9\u53d7\u9650\u4e14\u7279\u5f81\u7f3a\u4e4f\u7684\u73af\u5883\u4e2d\u6709\u6548\u5de5\u4f5c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86SLAM\u5728\u6076\u52a3\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.15478", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15478", "abs": "https://arxiv.org/abs/2507.15478", "authors": ["Simon Kohaut", "Felix Divo", "Navid Hamid", "Benedict Flade", "Julian Eggert", "Devendra Singh Dhami", "Kristian Kersting"], "title": "The Constitutional Controller: Doubt-Calibrated Steering of Compliant Agents", "comment": null, "summary": "Ensuring reliable and rule-compliant behavior of autonomous agents in\nuncertain environments remains a fundamental challenge in modern robotics. Our\nwork shows how neuro-symbolic systems, which integrate probabilistic, symbolic\nwhite-box reasoning models with deep learning methods, offer a powerful\nsolution to this challenge. This enables the simultaneous consideration of\nexplicit rules and neural models trained on noisy data, combining the strength\nof structured reasoning with flexible representations. To this end, we\nintroduce the Constitutional Controller (CoCo), a novel framework designed to\nenhance the safety and reliability of agents by reasoning over deep\nprobabilistic logic programs representing constraints such as those found in\nshared traffic spaces. Furthermore, we propose the concept of self-doubt,\nimplemented as a probability density conditioned on doubt features such as\ntravel velocity, employed sensors, or health factors. In a real-world aerial\nmobility study, we demonstrate CoCo's advantages for intelligent autonomous\nsystems to learn appropriate doubts and navigate complex and uncertain\nenvironments safely and compliantly.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u6846\u67b6CoCo\uff0c\u7ed3\u5408\u6982\u7387\u7b26\u53f7\u63a8\u7406\u4e0e\u6df1\u5ea6\u5b66\u4e60\uff0c\u63d0\u5347\u81ea\u4e3b\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u53ef\u9760\u4e14\u5408\u89c4\u884c\u4e3a\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165Constitutional Controller (CoCo)\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u6982\u7387\u903b\u8f91\u7a0b\u5e8f\u548c\u81ea\u6000\u7591\u673a\u5236\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7a7a\u4e2d\u4ea4\u901a\u7814\u7a76\u4e2d\uff0cCoCo\u6210\u529f\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u5408\u89c4\u6027\u3002", "conclusion": "CoCo\u6846\u67b6\u4e3a\u590d\u6742\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15484", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15484", "abs": "https://arxiv.org/abs/2507.15484", "authors": ["Jamie Bell"], "title": "Robots for Kiwifruit Harvesting and Pollination", "comment": null, "summary": "This research was a part of a project that developed mobile robots that\nperformed targeted pollen spraying and automated harvesting in pergola\nstructured kiwifruit orchards. Multiple kiwifruit detachment mechanisms were\ndesigned and field testing of one of the concepts showed that the mechanism\ncould reliably pick kiwifruit. Furthermore, this kiwifruit detachment mechanism\nwas able to reach over 80 percent of fruit in the cluttered kiwifruit canopy,\nwhereas the previous state of the art mechanism was only able to reach less\nthan 70 percent of the fruit. Artificial pollination was performed by detecting\nflowers and then spraying pollen in solution onto the detected flowers from a\nline of sprayers on a boom, while driving at up to 1.4 ms-1. In addition, the\nheight of the canopy was measured and the spray boom was moved up and down to\nkeep the boom close enough to the flowers for the spray to reach the flowers,\nwhile minimising collisions with the canopy. Mobile robot navigation was\nperformed using a 2D lidar in apple orchards and vineyards. Lidar navigation in\nkiwifruit orchards was more challenging because the pergola structure only\nprovides a small amount of data for the direction of rows, compared to the\namount of data from the overhead canopy, the undulating ground and other\nobjects in the orchards. Multiple methods are presented here for extracting\nstructure defining features from 3D lidar data in kiwifruit orchards. In\naddition, a 3D lidar navigation system -- which performed row following, row\nend detection and row end turns -- was tested for over 30 km of autonomous\ndriving in kiwifruit orchards. Computer vision algorithms for row detection and\nrow following were also tested. The computer vision algorithm worked as well as\nthe 3D lidar row following method in testing.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u7528\u4e8e\u7315\u7334\u6843\u679c\u56ed\u7684\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u4e86\u5b9a\u5411\u82b1\u7c89\u55b7\u6d12\u548c\u81ea\u52a8\u5316\u91c7\u6458\uff0c\u6539\u8fdb\u4e86\u91c7\u6458\u673a\u5236\u548c\u5bfc\u822a\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u7315\u7334\u6843\u679c\u56ed\u4e2d\u81ea\u52a8\u5316\u91c7\u6458\u548c\u82b1\u7c89\u55b7\u6d12\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u8986\u76d6\u8303\u56f4\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u79cd\u7315\u7334\u6843\u91c7\u6458\u673a\u5236\uff0c\u6d4b\u8bd5\u4e86\u5176\u4e2d\u4e00\u79cd\uff1b\u4f7f\u75282D\u548c3D\u6fc0\u5149\u96f7\u8fbe\u8fdb\u884c\u5bfc\u822a\uff1b\u5f00\u53d1\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u7b97\u6cd5\u3002", "result": "\u91c7\u6458\u673a\u5236\u8986\u76d6\u4e8680%\u4ee5\u4e0a\u7684\u679c\u5b9e\uff0c\u4f18\u4e8e\u4e4b\u524d\u768470%\uff1b\u82b1\u7c89\u55b7\u6d12\u548c\u5bfc\u822a\u7cfb\u7edf\u5728\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u79fb\u52a8\u673a\u5668\u4eba\u548c\u65b0\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u7315\u7334\u6843\u679c\u56ed\u7684\u81ea\u52a8\u5316\u4f5c\u4e1a\u6548\u7387\u3002"}}
{"id": "2507.15493", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15493", "abs": "https://arxiv.org/abs/2507.15493", "authors": ["Chilam Cheang", "Sijin Chen", "Zhongren Cui", "Yingdong Hu", "Liqun Huang", "Tao Kong", "Hang Li", "Yifeng Li", "Yuxiao Liu", "Xiao Ma", "Hao Niu", "Wenxuan Ou", "Wanli Peng", "Zeyu Ren", "Haixin Shi", "Jiawen Tian", "Hongtao Wu", "Xin Xiao", "Yuyang Xiao", "Jiafeng Xu", "Yichu Yang"], "title": "GR-3 Technical Report", "comment": "Tech report. Authors are listed in alphabetical order. Project page:\n  https://seed.bytedance.com/GR3/", "summary": "We report our recent progress towards building generalist robot policies, the\ndevelopment of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.\nIt showcases exceptional capabilities in generalizing to novel objects,\nenvironments, and instructions involving abstract concepts. Furthermore, it can\nbe efficiently fine-tuned with minimal human trajectory data, enabling rapid\nand cost-effective adaptation to new settings. GR-3 also excels in handling\nlong-horizon and dexterous tasks, including those requiring bi-manual\nmanipulation and mobile movement, showcasing robust and reliable performance.\nThese capabilities are achieved through a multi-faceted training recipe that\nincludes co-training with web-scale vision-language data, efficient fine-tuning\nfrom human trajectory data collected via VR devices, and effective imitation\nlearning with robot trajectory data. In addition, we introduce ByteMini, a\nversatile bi-manual mobile robot designed with exceptional flexibility and\nreliability, capable of accomplishing a wide range of tasks when integrated\nwith GR-3. Through extensive real-world experiments, we show GR-3 surpasses the\nstate-of-the-art baseline method, $\\pi_0$, on a wide variety of challenging\ntasks. We hope GR-3 can serve as a step towards building generalist robots\ncapable of assisting humans in daily life.", "AI": {"tldr": "GR-3\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5728\u65b0\u5bf9\u8c61\u3001\u73af\u5883\u548c\u62bd\u8c61\u6307\u4ee4\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u80fd\u9ad8\u6548\u5fae\u8c03\u3002", "motivation": "\u76ee\u6807\u662f\u6784\u5efa\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\uff0c\u4ee5\u8f85\u52a9\u4eba\u7c7b\u65e5\u5e38\u751f\u6d3b\u3002", "method": "\u901a\u8fc7\u591a\u65b9\u9762\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5305\u62ec\u7f51\u7edc\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00\u6570\u636e\u534f\u540c\u8bad\u7ec3\u3001VR\u8bbe\u5907\u6536\u96c6\u7684\u4eba\u7c7b\u8f68\u8ff9\u6570\u636e\u5fae\u8c03\uff0c\u4ee5\u53ca\u673a\u5668\u4eba\u8f68\u8ff9\u6570\u636e\u7684\u6a21\u4eff\u5b66\u4e60\u3002", "result": "GR-3\u5728\u591a\u79cd\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u03c0\u2080\uff0c\u5c24\u5176\u5728\u957f\u65f6\u7a0b\u548c\u7075\u5de7\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "GR-3\u662f\u8fc8\u5411\u901a\u7528\u673a\u5668\u4eba\u6280\u672f\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u5c55\u793a\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.15499", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15499", "abs": "https://arxiv.org/abs/2507.15499", "authors": ["Jongseok Lee", "Timo Birr", "Rudolph Triebel", "Tamim Asfour"], "title": "CLEVER: Stream-based Active Learning for Robust Semantic Perception from Human Instructions", "comment": "8 pages. Accepted to IEEE RAL", "summary": "We propose CLEVER, an active learning system for robust semantic perception\nwith Deep Neural Networks (DNNs). For data arriving in streams, our system\nseeks human support when encountering failures and adapts DNNs online based on\nhuman instructions. In this way, CLEVER can eventually accomplish the given\nsemantic perception tasks. Our main contribution is the design of a system that\nmeets several desiderata of realizing the aforementioned capabilities. The key\nenabler herein is our Bayesian formulation that encodes domain knowledge\nthrough priors. Empirically, we not only motivate CLEVER's design but further\ndemonstrate its capabilities with a user validation study as well as\nexperiments on humanoid and deformable objects. To our knowledge, we are the\nfirst to realize stream-based active learning on a real robot, providing\nevidence that the robustness of the DNN-based semantic perception can be\nimproved in practice. The project website can be accessed at\nhttps://sites.google.com/view/thecleversystem.", "AI": {"tldr": "CLEVER\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u4e3b\u52a8\u5b66\u4e60\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728\u7ebf\u83b7\u53d6\u4eba\u7c7b\u652f\u6301\u5e76\u9002\u5e94\u6307\u4ee4\uff0c\u63d0\u5347\u8bed\u4e49\u611f\u77e5\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u6d41\u6570\u636e\u8bed\u4e49\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u5931\u8d25\u95ee\u9898\uff0c\u901a\u8fc7\u4eba\u7c7b\u5e72\u9884\u548c\u5728\u7ebf\u9002\u5e94\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u6846\u67b6\u7f16\u7801\u9886\u57df\u77e5\u8bc6\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6ee1\u8db3\u591a\u9700\u6c42\u7684\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u9a8c\u8bc1\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u80fd\u529b\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u6d41\u5f0f\u4e3b\u52a8\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u8bed\u4e49\u611f\u77e5\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "CLEVER\u7cfb\u7edf\u9996\u6b21\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u6d41\u5f0f\u4e3b\u52a8\u5b66\u4e60\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u4e2d\u63d0\u5347\u8bed\u4e49\u611f\u77e5\u9c81\u68d2\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.15604", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15604", "abs": "https://arxiv.org/abs/2507.15604", "authors": ["Johannes Hartwig", "Philipp Lienhardt", "Dominik Henrich"], "title": "Estimation of Payload Inertial Parameters from Human Demonstrations by Hand Guiding", "comment": "Accepted for publication in Annals of Scientific Society for\n  Assembly, Handling and Industrial Robotics 2025 (to appear)", "summary": "As the availability of cobots increases, it is essential to address the needs\nof users with little to no programming knowledge to operate such systems\nefficiently. Programming concepts often use intuitive interaction modalities,\nsuch as hand guiding, to address this. When programming in-contact motions,\nsuch frameworks require knowledge of the robot tool's payload inertial\nparameters (PIP) in addition to the demonstrated velocities and forces to\nensure effective hybrid motion-force control. This paper aims to enable\nnon-expert users to program in-contact motions more efficiently by eliminating\nthe need for a dedicated PIP calibration, thereby enabling flexible robot tool\nchanges. Since demonstrated tasks generally also contain motions with\nnon-contact, our approach uses these parts to estimate the robot's PIP using\nestablished estimation techniques. The results show that the estimation of the\npayload's mass is accurate, whereas the center of mass and the inertia tensor\nare affected by noise and a lack of excitation. Overall, these findings show\nthe feasibility of PIP estimation during hand guiding but also highlight the\nneed for sufficient payload accelerations for an accurate estimation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u975e\u63a5\u89e6\u8fd0\u52a8\u90e8\u5206\u4f30\u8ba1\u673a\u5668\u4eba\u5de5\u5177\u7684\u8d1f\u8f7d\u60ef\u6027\u53c2\u6570\uff08PIP\uff09\uff0c\u4ece\u800c\u65e0\u9700\u4e13\u95e8\u7684PIP\u6821\u51c6\uff0c\u4f7f\u975e\u4e13\u5bb6\u7528\u6237\u80fd\u66f4\u9ad8\u6548\u5730\u7f16\u7a0b\u63a5\u89e6\u8fd0\u52a8\u3002", "motivation": "\u968f\u7740\u534f\u4f5c\u673a\u5668\u4eba\uff08cobot\uff09\u7684\u666e\u53ca\uff0c\u9700\u8981\u8ba9\u7f3a\u4e4f\u7f16\u7a0b\u77e5\u8bc6\u7684\u7528\u6237\u4e5f\u80fd\u9ad8\u6548\u64cd\u4f5c\u7cfb\u7edf\u3002\u73b0\u6709\u7684\u7f16\u7a0b\u65b9\u6cd5\u4f9d\u8d56\u624b\u5f15\u5bfc\u7b49\u76f4\u89c2\u4ea4\u4e92\uff0c\u4f46\u63a5\u89e6\u8fd0\u52a8\u7f16\u7a0b\u9700\u8981\u77e5\u9053\u5de5\u5177\u8d1f\u8f7d\u60ef\u6027\u53c2\u6570\uff08PIP\uff09\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u3002", "method": "\u5229\u7528\u4efb\u52a1\u4e2d\u7684\u975e\u63a5\u89e6\u8fd0\u52a8\u90e8\u5206\uff0c\u901a\u8fc7\u5df2\u6709\u4f30\u8ba1\u6280\u672f\u4f30\u7b97PIP\uff0c\u907f\u514d\u4e13\u95e8\u7684\u6821\u51c6\u6b65\u9aa4\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8d1f\u8f7d\u8d28\u91cf\u7684\u4f30\u8ba1\u51c6\u786e\uff0c\u4f46\u8d28\u5fc3\u548c\u60ef\u6027\u5f20\u91cf\u53d7\u566a\u58f0\u548c\u6fc0\u52b1\u4e0d\u8db3\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u5728\u624b\u5f15\u5bfc\u4e2d\u4f30\u8ba1PIP\u7684\u53ef\u884c\u6027\uff0c\u4f46\u9700\u8981\u8db3\u591f\u7684\u8d1f\u8f7d\u52a0\u901f\u5ea6\u4ee5\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u3002"}}
{"id": "2507.15607", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15607", "abs": "https://arxiv.org/abs/2507.15607", "authors": ["Yanbo Chen", "Yunzhe Tan", "Yaojia Wang", "Zhengzhe Xu", "Junbo Tan", "Xueqian Wang"], "title": "A Universal Vehicle-Trailer Navigation System with Neural Kinematics and Online Residual Learning", "comment": "8 pages, 10 figures", "summary": "Autonomous navigation of vehicle-trailer systems is crucial in environments\nlike airports, supermarkets, and concert venues, where various types of\ntrailers are needed to navigate with different payloads and conditions.\nHowever, accurately modeling such systems remains challenging, especially for\ntrailers with castor wheels. In this work, we propose a novel universal\nvehicle-trailer navigation system that integrates a hybrid nominal kinematic\nmodel--combining classical nonholonomic constraints for vehicles and neural\nnetwork-based trailer kinematics--with a lightweight online residual learning\nmodule to correct real-time modeling discrepancies and disturbances.\nAdditionally, we develop a model predictive control framework with a weighted\nmodel combination strategy that improves long-horizon prediction accuracy and\nensures safer motion planning. Our approach is validated through extensive\nreal-world experiments involving multiple trailer types and varying payload\nconditions, demonstrating robust performance without manual tuning or\ntrailer-specific calibration.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u901a\u7528\u8f66\u8f86-\u62d6\u8f66\u5bfc\u822a\u7cfb\u7edf\uff0c\u7ed3\u5408\u6df7\u5408\u8fd0\u52a8\u5b66\u6a21\u578b\u548c\u5728\u7ebf\u6b8b\u5dee\u5b66\u4e60\u6a21\u5757\uff0c\u901a\u8fc7\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u5b9e\u73b0\u9c81\u68d2\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8f66\u8f86-\u62d6\u8f66\u7cfb\u7edf\uff08\u5c24\u5176\u662f\u5e26\u811a\u8f6e\u7684\u62d6\u8f66\uff09\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u7cbe\u786e\u5efa\u6a21\u548c\u5bfc\u822a\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u7ecf\u5178\u975e\u5b8c\u6574\u7ea6\u675f\u548c\u795e\u7ecf\u7f51\u7edc\u62d6\u8f66\u8fd0\u52a8\u5b66\u7684\u6df7\u5408\u6a21\u578b\uff0c\u8f85\u4ee5\u5728\u7ebf\u6b8b\u5dee\u5b66\u4e60\u6a21\u5757\uff1b\u91c7\u7528\u52a0\u6743\u6a21\u578b\u7ec4\u5408\u7b56\u7565\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u3002", "result": "\u5728\u591a\u79cd\u62d6\u8f66\u7c7b\u578b\u548c\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u9a8c\u8bc1\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u6574\u6216\u62d6\u8f66\u7279\u5b9a\u6821\u51c6\uff0c\u8868\u73b0\u9c81\u68d2\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5b89\u5168\u7684\u8f66\u8f86-\u62d6\u8f66\u5bfc\u822a\u3002"}}
{"id": "2507.15608", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15608", "abs": "https://arxiv.org/abs/2507.15608", "authors": ["Johannes Hartwig", "Fabian Viessmann", "Dominik Henrich"], "title": "Optimizing Force Signals from Human Demonstrations of In-Contact Motions", "comment": "Accepted for publication in Annals of Scientific Society for\n  Assembly, Handling and Industrial Robotics 2024 (to appear)", "summary": "For non-robot-programming experts, kinesthetic guiding can be an intuitive\ninput method, as robot programming of in-contact tasks is becoming more\nprominent. However, imprecise and noisy input signals from human demonstrations\npose problems when reproducing motions directly or using the signal as input\nfor machine learning methods. This paper explores optimizing force signals to\ncorrespond better to the human intention of the demonstrated signal. We compare\ndifferent signal filtering methods and propose a peak detection method for\ndealing with first-contact deviations in the signal. The evaluation of these\nmethods considers a specialized error criterion between the input and the\nhuman-intended signal. In addition, we analyze the critical parameters'\ninfluence on the filtering methods. The quality for an individual motion could\nbe increased by up to \\SI{20}{\\percent} concerning the error criterion. The\nproposed contribution can improve the usability of robot programming and the\ninteraction between humans and robots.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u4f18\u5316\u529b\u4fe1\u53f7\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u4eba\u7c7b\u610f\u56fe\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u4fe1\u53f7\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5cf0\u503c\u68c0\u6d4b\u65b9\u6cd5\u4ee5\u51cf\u5c11\u9996\u6b21\u63a5\u89e6\u504f\u5dee\u3002", "motivation": "\u4e3a\u975e\u673a\u5668\u4eba\u7f16\u7a0b\u4e13\u5bb6\u63d0\u4f9b\u66f4\u76f4\u89c2\u7684\u8f93\u5165\u65b9\u6cd5\uff0c\u89e3\u51b3\u4eba\u7c7b\u6f14\u793a\u4e2d\u4e0d\u7cbe\u786e\u548c\u566a\u58f0\u4fe1\u53f7\u7684\u95ee\u9898\u3002", "method": "\u6bd4\u8f83\u4e0d\u540c\u4fe1\u53f7\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u63d0\u51fa\u5cf0\u503c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u5173\u952e\u53c2\u6570\u5bf9\u6ee4\u6ce2\u65b9\u6cd5\u7684\u5f71\u54cd\u3002", "result": "\u5355\u4e2a\u8fd0\u52a8\u7684\u8bef\u5dee\u6807\u51c6\u53ef\u63d0\u9ad8\u8fbe20%\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u63d0\u5347\u673a\u5668\u4eba\u7f16\u7a0b\u7684\u53ef\u7528\u6027\u548c\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\u3002"}}
{"id": "2507.15649", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15649", "abs": "https://arxiv.org/abs/2507.15649", "authors": ["Haocheng Xu", "Haodong Zhang", "Zhenghan Chen", "Rong Xiong"], "title": "EMP: Executable Motion Prior for Humanoid Robot Standing Upper-body Motion Imitation", "comment": null, "summary": "To support humanoid robots in performing manipulation tasks, it is essential\nto study stable standing while accommodating upper-body motions. However, the\nlimited controllable range of humanoid robots in a standing position affects\nthe stability of the entire body. Thus we introduce a reinforcement learning\nbased framework for humanoid robots to imitate human upper-body motions while\nmaintaining overall stability. Our approach begins with designing a retargeting\nnetwork that generates a large-scale upper-body motion dataset for training the\nreinforcement learning (RL) policy, which enables the humanoid robot to track\nupper-body motion targets, employing domain randomization for enhanced\nrobustness. To avoid exceeding the robot's execution capability and ensure\nsafety and stability, we propose an Executable Motion Prior (EMP) module, which\nadjusts the input target movements based on the robot's current state. This\nadjustment improves standing stability while minimizing changes to motion\namplitude. We evaluate our framework through simulation and real-world tests,\ndemonstrating its practical applicability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u6a21\u4eff\u4eba\u7c7b\u4e0a\u534a\u8eab\u52a8\u4f5c\u5e76\u4fdd\u6301\u6574\u4f53\u7a33\u5b9a\u6027\u3002", "motivation": "\u7814\u7a76\u4eba\u5f62\u673a\u5668\u4eba\u5728\u7ad9\u7acb\u65f6\u5982\u4f55\u7a33\u5b9a\u6267\u884c\u4e0a\u534a\u8eab\u52a8\u4f5c\uff0c\u89e3\u51b3\u5176\u53ef\u63a7\u8303\u56f4\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u91cd\u5b9a\u5411\u7f51\u7edc\u751f\u6210\u5927\u89c4\u6a21\u52a8\u4f5c\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u548c\u9886\u57df\u968f\u673a\u5316\uff0c\u5e76\u5f15\u5165\u53ef\u6267\u884c\u8fd0\u52a8\u5148\u9a8c\uff08EMP\uff09\u6a21\u5757\u8c03\u6574\u76ee\u6807\u52a8\u4f5c\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u5b9e\u7528\u6027\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u7684\u7a33\u5b9a\u6027\u548c\u52a8\u4f5c\u6267\u884c\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u6a21\u4eff\u4eba\u7c7b\u52a8\u4f5c\u65f6\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.15677", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15677", "abs": "https://arxiv.org/abs/2507.15677", "authors": ["Huayue Liang", "Yanbo Chen", "Hongyang Cheng", "Yanzhao Yu", "Shoujie Li", "Junbo Tan", "Xueqian Wang", "Long Zeng"], "title": "Data-Driven MPC with Data Selection for Flexible Cable-Driven Robotic Arms", "comment": null, "summary": "Flexible cable-driven robotic arms (FCRAs) offer dexterous and compliant\nmotion. Still, the inherent properties of cables, such as resilience,\nhysteresis, and friction, often lead to particular difficulties in modeling and\ncontrol. This paper proposes a model predictive control (MPC) method that\nrelies exclusively on input-output data, without a physical model, to improve\nthe control accuracy of FCRAs. First, we develop an implicit model based on\ninput-output data and integrate it into an MPC optimization framework. Second,\na data selection algorithm (DSA) is introduced to filter the data that best\ncharacterize the system, thereby reducing the solution time per step to\napproximately 4 ms, which is an improvement of nearly 80%. Lastly, the\ninfluence of hyperparameters on tracking error is investigated through\nsimulation. The proposed method has been validated on a real FCRA platform,\nincluding five-point positioning accuracy tests, a five-point response tracking\ntest, and trajectory tracking for letter drawing. The results demonstrate that\nthe average positioning accuracy is approximately 2.070 mm. Moreover, compared\nto the PID method with an average tracking error of 1.418{\\deg}, the proposed\nmethod achieves an average tracking error of 0.541{\\deg}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f93\u5165\u8f93\u51fa\u6570\u636e\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u67d4\u6027\u7535\u7f06\u9a71\u52a8\u673a\u68b0\u81c2\uff08FCRA\uff09\u7684\u63a7\u5236\u7cbe\u5ea6\uff0c\u65e0\u9700\u7269\u7406\u6a21\u578b\u3002\u901a\u8fc7\u6570\u636e\u9009\u62e9\u7b97\u6cd5\uff08DSA\uff09\u4f18\u5316\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u4f18\u4e8e\u4f20\u7edfPID\u65b9\u6cd5\u3002", "motivation": "\u67d4\u6027\u7535\u7f06\u9a71\u52a8\u673a\u68b0\u81c2\uff08FCRAs\uff09\u7684\u7535\u7f06\u7279\u6027\uff08\u5982\u5f39\u6027\u3001\u8fdf\u6ede\u548c\u6469\u64e6\uff09\u5bfc\u81f4\u5efa\u6a21\u548c\u63a7\u5236\u56f0\u96be\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u7cbe\u786e\u63a7\u5236\u3002", "method": "1. \u57fa\u4e8e\u8f93\u5165\u8f93\u51fa\u6570\u636e\u6784\u5efa\u9690\u5f0f\u6a21\u578b\uff0c\u5e76\u96c6\u6210\u5230MPC\u6846\u67b6\u4e2d\uff1b2. \u5f15\u5165\u6570\u636e\u9009\u62e9\u7b97\u6cd5\uff08DSA\uff09\u4f18\u5316\u8ba1\u7b97\u6548\u7387\uff1b3. \u901a\u8fc7\u4eff\u771f\u7814\u7a76\u8d85\u53c2\u6570\u5bf9\u8ddf\u8e2a\u8bef\u5dee\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5e73\u5747\u5b9a\u4f4d\u7cbe\u5ea6\u7ea6\u4e3a2.070\u6beb\u7c73\uff0c\u8ddf\u8e2a\u8bef\u5dee\u4e3a0.541\u5ea6\uff0c\u4f18\u4e8ePID\u65b9\u6cd5\u76841.418\u5ea6\u3002\u8ba1\u7b97\u6548\u7387\u63d0\u534780%\uff0c\u6bcf\u6b65\u6c42\u89e3\u65f6\u95f4\u964d\u81f3\u7ea64\u6beb\u79d2\u3002", "conclusion": "\u63d0\u51fa\u7684MPC\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86FCRA\u7684\u63a7\u5236\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.15693", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15693", "abs": "https://arxiv.org/abs/2507.15693", "authors": ["Georges Chebly", "Spencer Little", "Nisal Perera", "Aliya Abedeen", "Ken Suzuki", "Donghyun Kim"], "title": "Strong, Accurate, and Low-Cost Robot Manipulator", "comment": null, "summary": "This paper presents Forte, a fully 3D-printable, 6-DoF robotic arm designed\nto achieve near industrial-grade performance - 0.63 kg payload, 0.467 m reach,\nand sub-millimeter repeatability - at a material cost under $215. As an\naccessible robot for broad applications across classroom education to AI\nexperiments, Forte pushes forward the performance limitations of existing\nlow-cost educational arms. We introduce a cost-effective mechanical design that\ncombines capstan-based cable drives, timing belts, simple tensioning\nmechanisms, and lightweight 3D-printed structures, along with topology\noptimization for structural stiffness. Through careful drivetrain engineering,\nwe minimize backlash and maintain control fidelity without relying on\nhigh-power electronics or expensive manufacturing processes. Experimental\nvalidation demonstrates that Forte achieves high repeatability and load\ncapacity, offering a compelling robotic platform for both classroom instruction\nand advanced robotics research.", "AI": {"tldr": "Forte\u662f\u4e00\u6b3e\u51683D\u6253\u5370\u76846\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\uff0c\u5177\u6709\u63a5\u8fd1\u5de5\u4e1a\u7ea7\u7684\u6027\u80fd\uff080.63 kg\u8d1f\u8f7d\u30010.467 m\u5de5\u4f5c\u8303\u56f4\u548c\u4e9a\u6beb\u7c73\u7ea7\u91cd\u590d\u7cbe\u5ea6\uff09\uff0c\u6210\u672c\u4f4e\u4e8e215\u7f8e\u5143\u3002", "motivation": "\u63a8\u52a8\u4f4e\u6210\u672c\u6559\u80b2\u673a\u68b0\u81c2\u7684\u6027\u80fd\u6781\u9650\uff0c\u9002\u7528\u4e8e\u8bfe\u5802\u6559\u80b2\u548cAI\u5b9e\u9a8c\u3002", "method": "\u91c7\u7528\u6210\u672c\u6548\u76ca\u9ad8\u7684\u673a\u68b0\u8bbe\u8ba1\uff0c\u5305\u62ec\u57fa\u4e8e\u7ede\u76d8\u7684\u7535\u7f06\u9a71\u52a8\u3001\u540c\u6b65\u5e26\u3001\u7b80\u5355\u5f20\u7d27\u673a\u6784\u548c\u8f7b\u91cf\u53163D\u6253\u5370\u7ed3\u6784\uff0c\u7ed3\u5408\u62d3\u6251\u4f18\u5316\u63d0\u5347\u7ed3\u6784\u521a\u5ea6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793aForte\u5177\u6709\u9ad8\u91cd\u590d\u6027\u548c\u8d1f\u8f7d\u80fd\u529b\u3002", "conclusion": "Forte\u4e3a\u8bfe\u5802\u6559\u5b66\u548c\u9ad8\u7ea7\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u5438\u5f15\u529b\u7684\u5e73\u53f0\u3002"}}
{"id": "2507.15710", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15710", "abs": "https://arxiv.org/abs/2507.15710", "authors": ["Lu Huang", "Lingxiao Meng", "Jiankun Wang", "Xingjian Jing"], "title": "Selective Densification for Rapid Motion Planning in High Dimensions with Narrow Passages", "comment": null, "summary": "Sampling-based algorithms are widely used for motion planning in\nhigh-dimensional configuration spaces. However, due to low sampling efficiency,\ntheir performance often diminishes in complex configuration spaces with narrow\ncorridors. Existing approaches address this issue using handcrafted or learned\nheuristics to guide sampling toward useful regions. Unfortunately, these\nstrategies often lack generalizability to various problems or require extensive\nprior training. In this paper, we propose a simple yet efficient sampling-based\nplanning framework along with its bidirectional version that overcomes these\nissues by integrating different levels of planning granularity. Our approach\nprobes configuration spaces with uniform random samples at varying resolutions\nand explores these multi-resolution samples online with a bias towards sparse\nsamples when traveling large free configuration spaces. By seamlessly\ntransitioning between sparse and dense samples, our approach can navigate\ncomplex configuration spaces while maintaining planning speed and completeness.\nThe simulation results demonstrate that our approach outperforms several\nstate-of-the-art sampling-based planners in $\\mathbb{SE}(2)$, $\\mathbb{SE}(3)$,\nand $\\mathbb{R}^{14}$ with challenging terrains. Furthermore, experiments\nconducted with the Franka Emika Panda robot operating in a constrained\nworkspace provide additional evidence of the superiority of the proposed\nmethod.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u5206\u8fa8\u7387\u91c7\u6837\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u91c7\u6837\u5bc6\u5ea6\uff0c\u5728\u590d\u6742\u914d\u7f6e\u7a7a\u95f4\u4e2d\u4fdd\u6301\u89c4\u5212\u901f\u5ea6\u548c\u5b8c\u6574\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u91c7\u6837\u89c4\u5212\u7b97\u6cd5\u5728\u590d\u6742\u914d\u7f6e\u7a7a\u95f4\u4e2d\u6548\u7387\u4f4e\u3001\u901a\u7528\u6027\u5dee\u6216\u9700\u5927\u91cf\u8bad\u7ec3\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u4e0d\u540c\u7c92\u5ea6\u7684\u89c4\u5212\uff0c\u52a8\u6001\u8c03\u6574\u7a00\u758f\u4e0e\u5bc6\u96c6\u91c7\u6837\uff0c\u5728\u7ebf\u63a2\u7d22\u591a\u5206\u8fa8\u7387\u6837\u672c\u3002", "result": "\u5728\u591a\u79cd\u914d\u7f6e\u7a7a\u95f4\u548c\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u9ad8\u6548\u4e14\u901a\u7528\uff0c\u65e0\u9700\u5927\u91cf\u5148\u9a8c\u8bad\u7ec3\u3002"}}
{"id": "2507.15716", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15716", "abs": "https://arxiv.org/abs/2507.15716", "authors": ["Ziyu Wan", "Lin Zhao"], "title": "DiffPF: Differentiable Particle Filtering with Generative Sampling via Conditional Diffusion Models", "comment": null, "summary": "This paper proposes DiffPF, a differentiable particle filter that leverages\ndiffusion models for state estimation in dynamic systems. Unlike conventional\ndifferentiable particle filters, which require importance weighting and\ntypically rely on predefined or low-capacity proposal distributions. DiffPF\nlearns a flexible posterior sampler by conditioning a diffusion model on\npredicted particles and the current observation. This enables accurate,\nequally-weighted sampling from complex, high-dimensional, and multimodal\nfiltering distributions. We evaluate DiffPF across a range of scenarios,\nincluding both unimodal and highly multimodal distributions, and test it on\nsimulated as well as real-world tasks, where it consistently outperforms\nexisting filtering baselines. In particular, DiffPF achieves an 82.8%\nimprovement in estimation accuracy on a highly multimodal global localization\nbenchmark, and a 26% improvement on the real-world KITTI visual odometry\nbenchmark, compared to state-of-the-art differentiable filters. To the best of\nour knowledge, DiffPF is the first method to integrate conditional diffusion\nmodels into particle filtering, enabling high-quality posterior sampling that\nproduces more informative particles and significantly improves state\nestimation.", "AI": {"tldr": "DiffPF\u662f\u4e00\u79cd\u53ef\u5fae\u5206\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u52a8\u6001\u7cfb\u7edf\u7684\u72b6\u6001\u4f30\u8ba1\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u53ef\u5fae\u5206\u7c92\u5b50\u6ee4\u6ce2\u5668\u4f9d\u8d56\u9884\u5b9a\u4e49\u6216\u4f4e\u5bb9\u91cf\u63d0\u8bae\u5206\u5e03\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002DiffPF\u65e8\u5728\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5b66\u4e60\u7075\u6d3b\u7684\u540e\u9a8c\u91c7\u6837\u5668\uff0c\u63d0\u5347\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002", "method": "DiffPF\u901a\u8fc7\u5c06\u6269\u6563\u6a21\u578b\u6761\u4ef6\u5316\u4e8e\u9884\u6d4b\u7c92\u5b50\u548c\u5f53\u524d\u89c2\u6d4b\uff0c\u5b9e\u73b0\u590d\u6742\u3001\u9ad8\u7ef4\u3001\u591a\u6a21\u6001\u5206\u5e03\u7684\u9ad8\u8d28\u91cf\u91c7\u6837\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4efb\u52a1\u4e2d\uff0cDiffPF\u8868\u73b0\u4f18\u5f02\uff0c\u5982\u5728\u591a\u6a21\u6001\u5168\u5c40\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7cbe\u5ea6\u63d0\u534782.8%\uff0c\u5728KITTI\u89c6\u89c9\u6d4b\u8ddd\u4efb\u52a1\u4e2d\u63d0\u534726%\u3002", "conclusion": "DiffPF\u9996\u6b21\u5c06\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5f15\u5165\u7c92\u5b50\u6ee4\u6ce2\uff0c\u663e\u8457\u63d0\u5347\u4e86\u540e\u9a8c\u91c7\u6837\u8d28\u91cf\u548c\u72b6\u6001\u4f30\u8ba1\u6027\u80fd\u3002"}}
{"id": "2507.15729", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.15729", "abs": "https://arxiv.org/abs/2507.15729", "authors": ["Jens V. R\u00fcppel", "Andrey Rudenko", "Tim Schreiter", "Martin Magnusson", "Achim J. Lilienthal"], "title": "Gaze-supported Large Language Model Framework for Bi-directional Human-Robot Interaction", "comment": "This paper has been accepted to the 34th IEEE International\n  Conference on Robot and Human Interactive Communication (RO-MAN), which will\n  be held in Eindhoven, Netherlands on August 25-29, 2025. Copyright 2025 IEEE.\n  Personal use of this material is permitted. Permission from IEEE must be\n  obtained for all other uses", "summary": "The rapid development of Large Language Models (LLMs) creates an exciting\npotential for flexible, general knowledge-driven Human-Robot Interaction (HRI)\nsystems for assistive robots. Existing HRI systems demonstrate great progress\nin interpreting and following user instructions, action generation, and robot\ntask solving. On the other hand, bi-directional, multi-modal, and context-aware\nsupport of the user in collaborative tasks still remains an open challenge. In\nthis paper, we present a gaze- and speech-informed interface to the assistive\nrobot, which is able to perceive the working environment from multiple vision\ninputs and support the dynamic user in their tasks. Our system is designed to\nbe modular and transferable to adapt to diverse tasks and robots, and it is\ncapable of real-time use of language-based interaction state representation and\nfast on board perception modules. Its development was supported by multiple\npublic dissemination events, contributing important considerations for improved\nrobustness and user experience. Furthermore, in two lab studies, we compare the\nperformance and user ratings of our system with those of a traditional scripted\nHRI pipeline. Our findings indicate that an LLM-based approach enhances\nadaptability and marginally improves user engagement and task execution metrics\nbut may produce redundant output, while a scripted pipeline is well suited for\nmore straightforward tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8f85\u52a9\u673a\u5668\u4eba\u4ea4\u4e92\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8f93\u5165\uff08\u5982\u89c6\u7ebf\u548c\u8bed\u97f3\uff09\u5b9e\u73b0\u52a8\u6001\u4efb\u52a1\u652f\u6301\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4e0e\u4f20\u7edf\u811a\u672c\u5316\u7cfb\u7edf\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u5728\u5355\u5411\u6307\u4ee4\u6267\u884c\u548c\u4efb\u52a1\u89e3\u51b3\u65b9\u9762\u5df2\u6709\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u53cc\u5411\u3001\u591a\u6a21\u6001\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u534f\u4f5c\u4efb\u52a1\u652f\u6301\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u8fc1\u79fb\u7684\u7cfb\u7edf\uff0c\u7ed3\u5408\u591a\u89c6\u89c9\u8f93\u5165\u548c\u5b9e\u65f6\u8bed\u8a00\u4ea4\u4e92\u72b6\u6001\u8868\u793a\uff0c\u652f\u6301\u52a8\u6001\u7528\u6237\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u5728\u9002\u5e94\u6027\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u4e0a\u7565\u6709\u63d0\u5347\uff0c\u4f46\u53ef\u80fd\u4ea7\u751f\u5197\u4f59\u8f93\u51fa\uff1b\u811a\u672c\u5316\u7cfb\u7edf\u66f4\u9002\u5408\u7b80\u5355\u4efb\u52a1\u3002", "conclusion": "LLM\u65b9\u6cd5\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u9700\u4f18\u5316\u4ee5\u51cf\u5c11\u5197\u4f59\uff1b\u811a\u672c\u5316\u7cfb\u7edf\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u4ecd\u5177\u4f18\u52bf\u3002"}}
{"id": "2507.15782", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15782", "abs": "https://arxiv.org/abs/2507.15782", "authors": ["Ruochu Yang", "Yu Zhou", "Fumin Zhang", "Mengxue Hou"], "title": "Interleaved LLM and Motion Planning for Generalized Multi-Object Collection in Large Scene Graphs", "comment": null, "summary": "Household robots have been a longstanding research topic, but they still lack\nhuman-like intelligence, particularly in manipulating open-set objects and\nnavigating large environments efficiently and accurately. To push this\nboundary, we consider a generalized multi-object collection problem in large\nscene graphs, where the robot needs to pick up and place multiple objects\nacross multiple locations in a long mission of multiple human commands. This\nproblem is extremely challenging since it requires long-horizon planning in a\nvast action-state space under high uncertainties. To this end, we propose a\nnovel interleaved LLM and motion planning algorithm Inter-LLM. By designing a\nmultimodal action cost similarity function, our algorithm can both reflect the\nhistory and look into the future to optimize plans, striking a good balance of\nquality and efficiency. Simulation experiments demonstrate that compared with\nlatest works, our algorithm improves the overall mission performance by 30% in\nterms of fulfilling human commands, maximizing mission success rates, and\nminimizing mission costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684Inter-LLM\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5bb6\u5ead\u673a\u5668\u4eba\u5728\u591a\u76ee\u6807\u6536\u96c6\u4efb\u52a1\u4e2d\u7684\u957f\u671f\u89c4\u5212\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u6548\u7387\u3002", "motivation": "\u5bb6\u5ead\u673a\u5668\u4eba\u5728\u5904\u7406\u5f00\u653e\u96c6\u5bf9\u8c61\u548c\u5927\u578b\u73af\u5883\u5bfc\u822a\u65f6\u7f3a\u4e4f\u4eba\u7c7b\u667a\u80fd\uff0c\u9700\u8981\u89e3\u51b3\u591a\u76ee\u6807\u6536\u96c6\u4efb\u52a1\u4e2d\u7684\u957f\u671f\u89c4\u5212\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u52a8\u4f5c\u6210\u672c\u76f8\u4f3c\u6027\u51fd\u6570\uff0c\u7ed3\u5408LLM\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u4f18\u5316\u957f\u671f\u4efb\u52a1\u89c4\u5212\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\uff0cInter-LLM\u7b97\u6cd5\u5728\u4efb\u52a1\u5b8c\u6210\u7387\u3001\u6210\u529f\u7387\u548c\u6210\u672c\u65b9\u9762\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e8630%\u3002", "conclusion": "Inter-LLM\u7b97\u6cd5\u5728\u591a\u76ee\u6807\u6536\u96c6\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e73\u8861\u4e86\u89c4\u5212\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2507.15833", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15833", "abs": "https://arxiv.org/abs/2507.15833", "authors": ["Ian Chuang", "Andrew Lee", "Dechen Gao", "Jinyu Zou", "Iman Soltani"], "title": "Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers", "comment": "13 pages, 10 figures", "summary": "Human vision is a highly active process driven by gaze, which directs\nattention and fixation to task-relevant regions and dramatically reduces visual\nprocessing. In contrast, robot learning systems typically rely on passive,\nuniform processing of raw camera images. In this work, we explore how\nincorporating human-like active gaze into robotic policies can enhance both\nefficiency and performance. We build on recent advances in foveated image\nprocessing and apply them to an Active Vision robot system that emulates both\nhuman head movement and eye tracking. Extending prior work on the AV-ALOHA\nrobot simulation platform, we introduce a framework for simultaneously\ncollecting eye-tracking data and robot demonstrations from a human operator as\nwell as a simulation benchmark and dataset for training robot policies that\nincorporate human gaze. Given the widespread use of Vision Transformers (ViTs)\nin robot learning, we integrate gaze information into ViTs using a foveated\npatch tokenization scheme inspired by recent work in image segmentation.\nCompared to uniform patch tokenization, this significantly reduces the number\nof tokens-and thus computation-without sacrificing visual fidelity near regions\nof interest. We also explore two approaches to gaze imitation and prediction\nfrom human data. The first is a two-stage model that predicts gaze to guide\nfoveation and action; the second integrates gaze into the action space,\nallowing the policy to jointly predict gaze and actions end-to-end. Our results\nshow that our method for foveated robot vision not only drastically reduces\ncomputational overhead, but also improves performance for high precision tasks\nand robustness to unseen distractors. Together, these findings suggest that\nhuman-inspired visual processing offers a useful inductive bias for robotic\nvision systems. https://ian-chuang.github.io/gaze-av-aloha/", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5c06\u4eba\u7c7b\u4e3b\u52a8\u6ce8\u89c6\u673a\u5236\u5f15\u5165\u673a\u5668\u4eba\u89c6\u89c9\u7cfb\u7edf\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u6027\u80fd\u3002\u901a\u8fc7\u7ed3\u5408\u773c\u52a8\u6570\u636e\u548c\u673a\u5668\u4eba\u6f14\u793a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eViTs\u7684\u6ce8\u89c6\u5f15\u5bfc\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u91cf\u5e76\u63d0\u5347\u4e86\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u4eba\u7c7b\u89c6\u89c9\u901a\u8fc7\u6ce8\u89c6\u4e3b\u52a8\u5904\u7406\u4efb\u52a1\u76f8\u5173\u533a\u57df\uff0c\u800c\u673a\u5668\u4eba\u901a\u5e38\u88ab\u52a8\u5904\u7406\u56fe\u50cf\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u6ce8\u89c6\u673a\u5236\u63d0\u5347\u673a\u5668\u4eba\u89c6\u89c9\u7cfb\u7edf\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u773c\u52a8\u6570\u636e\u548c\u673a\u5668\u4eba\u6f14\u793a\u7684\u6846\u67b6\uff0c\u4f7f\u7528ViTs\u7684\u6ce8\u89c6\u5f15\u5bfcpatch tokenization\u65b9\u6848\uff0c\u5e76\u63a2\u7d22\u4e86\u4e24\u79cd\u6ce8\u89c6\u9884\u6d4b\u65b9\u6cd5\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u91cf\uff0c\u63d0\u5347\u4e86\u9ad8\u7cbe\u5ea6\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u4e86\u5bf9\u672a\u77e5\u5e72\u6270\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u4eba\u7c7b\u89c6\u89c9\u5904\u7406\u673a\u5236\u4e3a\u673a\u5668\u4eba\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002"}}
