{"id": "2509.02727", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02727", "abs": "https://arxiv.org/abs/2509.02727", "authors": ["Guillaume Gagn\u00e9-Labelle", "Vassil Atanassov", "Ioannis Havoutis"], "title": "Acrobotics: A Generalist Approahc To Quadrupedal Robots' Parkour", "comment": "Supplementary material can be found here:\n  https://drive.google.com/drive/folders/18h25azbCFfPF4fhSsRfxKrnZo3dPKs_j?usp=sharing", "summary": "Climbing, crouching, bridging gaps, and walking up stairs are just a few of\nthe advantages that quadruped robots have over wheeled robots, making them more\nsuitable for navigating rough and unstructured terrain. However, executing such\nmanoeuvres requires precise temporal coordination and complex agent-environment\ninteractions. Moreover, legged locomotion is inherently more prone to slippage\nand tripping, and the classical approach of modeling such cases to design a\nrobust controller thus quickly becomes impractical. In contrast, reinforcement\nlearning offers a compelling solution by enabling optimal control through trial\nand error. We present a generalist reinforcement learning algorithm for\nquadrupedal agents in dynamic motion scenarios. The learned policy rivals\nstate-of-the-art specialist policies trained using a mixture of experts\napproach, while using only 25% as many agents during training. Our experiments\nalso highlight the key components of the generalist locomotion policy and the\nprimary factors contributing to its success.", "AI": {"tldr": "\u901a\u7528\u578b\u56f4\u6559\u5b66\u4e60\u7b97\u6cd5\u5728\u56db\u8db3\u673a\u5668\u4eba\u52a8\u6001\u8fd0\u52a8\u573a\u666f\u4e2d\u53d6\u5f97\u4e86\u4e0e\u4e13\u5bb6\u7b56\u7565\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u800c\u8bad\u7ec3\u9700\u8981\u7684\u673a\u5668\u4eba\u6570\u91cf\u4ec5\u4e3a\u5bf9\u65b9\u768425%", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u5bf9\u6ed1\u5237\u548c\u7ede\u5012\u7b49\u95ee\u9898\u7684\u5efa\u6a21\u590d\u6742\u5ea6\u9ad8\uff0c\u9700\u8981\u66f4\u7b80\u6d01\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u91c7\u7528\u56f4\u6559\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c1d\u8bd5\u9519\u8bef\u5b66\u4e60\u6700\u4f18\u63a7\u5236\u7b56\u7565\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u5efa\u6a21\u65b9\u6cd5\u7684\u590d\u6742\u6027", "result": "\u5b66\u4e60\u5230\u7684\u7b56\u7565\u5728\u6027\u80fd\u4e0a\u53ef\u4e0e\u6700\u5148\u8fdb\u7684\u4e13\u5bb6\u7b56\u7565\u76f8\u6bd4\u62fc\uff0c\u800c\u8bad\u7ec3\u6548\u7387\u63d0\u5347\u4e864\u500d\uff08\u4ec5\u970025%\u7684\u673a\u5668\u4eba\u6570\u91cf\uff09", "conclusion": "\u56f4\u6559\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u901a\u7528\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u51cf\u5c11\u8bad\u7ec3\u8d44\u6e90\u7684\u540c\u65f6\u5b9e\u73b0\u4f18\u5f02\u7684\u56db\u8db3\u673a\u5668\u4eba\u52a8\u6001\u8fd0\u52a8\u63a7\u5236"}}
{"id": "2509.02749", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02749", "abs": "https://arxiv.org/abs/2509.02749", "authors": ["Giorgia Buracchio", "Ariele Callegari", "Massimo Donini", "Cristina Gena", "Antonio Lieto", "Alberto Lillo", "Claudio Mattutino", "Alessandro Mazzei", "Linda Pigureddu", "Manuel Striani", "Fabiana Vernero"], "title": "The Impact of Adaptive Emotional Alignment on Mental State Attribution and User Empathy in HRI", "comment": "autohor copy of the paper accepted at ROMAN2025", "summary": "The paper presents an experiment on the effects of adaptive emotional\nalignment between agents, considered a prerequisite for empathic communication,\nin Human-Robot Interaction (HRI). Using the NAO robot, we investigate the\nimpact of an emotionally aligned, empathic, dialogue on these aspects: (i) the\nrobot's persuasive effectiveness, (ii) the user's communication style, and\n(iii) the attribution of mental states and empathy to the robot. In an\nexperiment with 42 participants, two conditions were compared: one with neutral\ncommunication and another where the robot provided responses adapted to the\nemotions expressed by the users. The results show that emotional alignment does\nnot influence users' communication styles or have a persuasive effect. However,\nit significantly influences attribution of mental states to the robot and its\nperceived empathy", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7NAO\u673a\u5668\u4eba\u5b9e\u9a8c\u7814\u7a76\u4e86\u60c5\u611f\u5bf9\u9f50\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u60c5\u611f\u5bf9\u9f50\u867d\u7136\u4e0d\u5f71\u54cd\u7528\u6237\u6c9f\u901a\u98ce\u683c\u548c\u8bf4\u670d\u6548\u679c\uff0c\u4f46\u663e\u8457\u63d0\u9ad8\u4e86\u7528\u6237\u5bf9\u673a\u5668\u4eba\u5fc3\u667a\u72b6\u6001\u548c\u5171\u60c5\u80fd\u529b\u7684\u5f52\u56e0", "motivation": "\u7814\u7a76\u60c5\u611f\u5bf9\u9f50\u4f5c\u4e3a\u5171\u60c5\u6c9f\u901a\u7684\u524d\u63d0\u6761\u4ef6\uff0c\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u5f71\u54cd\u6548\u679c", "method": "\u4f7f\u7528NAO\u673a\u5668\u4eba\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5bf942\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u4e24\u79cd\u6761\u4ef6\u5bf9\u6bd4\uff1a\u4e2d\u6027\u6c9f\u901a\u548c\u60c5\u611f\u9002\u914d\u6c9f\u901a\uff0c\u6d4b\u91cf\u673a\u5668\u4eba\u7684\u8bf4\u670d\u6548\u679c\u3001\u7528\u6237\u6c9f\u901a\u98ce\u683c\u548c\u5fc3\u667a\u72b6\u6001\u5f52\u56e0", "result": "\u60c5\u611f\u5bf9\u9f50\u5bf9\u7528\u6237\u6c9f\u901a\u98ce\u683c\u548c\u8bf4\u670d\u6548\u679c\u6ca1\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u663e\u8457\u63d0\u9ad8\u4e86\u7528\u6237\u5bf9\u673a\u5668\u4eba\u5fc3\u667a\u72b6\u6001\u548c\u5171\u60c5\u80fd\u529b\u7684\u5f52\u56e0", "conclusion": "\u60c5\u611f\u5bf9\u9f50\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u867d\u7136\u4e0d\u80fd\u63d0\u9ad8\u8bf4\u670d\u6548\u679c\uff0c\u4f46\u80fd\u591f\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u7684\u4eba\u6027\u5316\u611f\u77e5\uff0c\u5bf9\u4e8e\u5efa\u7acb\u66f4\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\u5177\u6709\u91cd\u8981\u610f\u4e49"}}
{"id": "2509.02760", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02760", "abs": "https://arxiv.org/abs/2509.02760", "authors": ["Maximilian Neidhardt", "Ludwig Bosse", "Vidas Raudonis", "Kristina Allgoewer", "Axel Heinemann", "Benjamin Ondruschka", "Alexander Schlaefer"], "title": "A Digital Twin for Robotic Post Mortem Tissue Sampling using Virtual Reality", "comment": null, "summary": "Studying tissue samples obtained during autopsies is the gold standard when\ndiagnosing the cause of death and for understanding disease pathophysiology.\nRecently, the interest in post mortem minimally invasive biopsies has grown\nwhich is a less destructive approach in comparison to an open autopsy and\nreduces the risk of infection. While manual biopsies under ultrasound guidance\nare more widely performed, robotic post mortem biopsies have been recently\nproposed. This approach can further reduce the risk of infection for\nphysicians. However, planning of the procedure and control of the robot need to\nbe efficient and usable. We explore a virtual reality setup with a digital twin\nto realize fully remote planning and control of robotic post mortem biopsies.\nThe setup is evaluated with forensic pathologists in a usability study for\nthree interaction methods. Furthermore, we evaluate clinical feasibility and\nevaluate the system with three human cadavers. Overall, 132 needle insertions\nwere performed with an off-axis needle placement error of 5.30+-3.25 mm. Tissue\nsamples were successfully biopsied and histopathologically verified. Users\nreported a very intuitive needle placement approach, indicating that the system\nis a promising, precise, and low-risk alternative to conventional approaches.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u865a\u62df\u73b0\u5b9e\u548c\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5b9e\u73b0\u673a\u5668\u4eba\u5c38\u68c0\u6d3b\u68c0\u7684\u8fdc\u7a0b\u89c4\u5212\u548c\u63a7\u5236\u7cfb\u7edf\uff0c\u901a\u8fc7\u4eba\u4f53\u5c38\u4f53\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u7cbe\u786e\u6027\u548c\u53ef\u884c\u6027\u3002", "motivation": "\u4f20\u7edf\u5c38\u68c0\u7ec4\u7ec7\u91c7\u6837\u5177\u6709\u7834\u574f\u6027\u548c\u611f\u67d3\u98ce\u9669\uff0c\u800c\u673a\u5668\u4eba\u5c38\u68c0\u6d3b\u68c0\u53ef\u4ee5\u964d\u4f4e\u533b\u751f\u611f\u67d3\u98ce\u9669\uff0c\u4f46\u9700\u8981\u9ad8\u6548\u7684\u7a0b\u5e8f\u89c4\u5212\u548c\u673a\u5668\u4eba\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u865a\u62df\u73b0\u5b9e\u8bbe\u7f6e\u548c\u6570\u5b57\u5b6a\u751f\u6280\u672f\uff0c\u5b9e\u73b0\u5b8c\u5168\u8fdc\u7a0b\u7684\u673a\u5668\u4eba\u5c38\u68c0\u6d3b\u68c0\u89c4\u5212\u548c\u63a7\u5236\uff0c\u5e76\u901a\u8fc7\u53ef\u7528\u6027\u7814\u7a76\u8bc4\u4f30\u4e09\u79cd\u4ea4\u4e92\u65b9\u6cd5\u3002", "result": "\u5728\u4e09\u4e2a\u5c38\u4f53\u4e0a\u8fdb\u884c\u4e86132\u6b21\u9488\u5934\u63d2\u5165\uff0c\u79bb\u8f74\u9488\u5934\u653e\u7f6e\u8bef\u5dee\u4e3a5.30\u00b13.25\u6beb\u7c73\uff0c\u6210\u529f\u6d3b\u68c0\u7ec4\u7ec7\u6837\u672c\u5e76\u901a\u8fc7\u7ec4\u7ec7\u75c5\u7406\u5b66\u9a8c\u8bc1\u3002", "conclusion": "\u7528\u6237\u62a5\u544a\u9488\u5934\u653e\u7f6e\u65b9\u6cd5\u975e\u5e38\u76f4\u89c2\uff0c\u8868\u660e\u8be5\u7cfb\u7edf\u662f\u4f20\u7edf\u65b9\u6cd5\u7684\u6709\u524d\u666f\u3001\u7cbe\u786e\u4e14\u4f4e\u98ce\u9669\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.02808", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.02808", "abs": "https://arxiv.org/abs/2509.02808", "authors": ["Isaac Ronald Ward", "Mark Paral", "Kristopher Riordan", "Mykel J. Kochenderfer"], "title": "Improving the Resilience of Quadrotors in Underground Environments by Combining Learning-based and Safety Controllers", "comment": "Accepted and awarded best paper at the 11th International Conference\n  on Control, Decision and Information Technologies (CoDIT 2025 -\n  https://codit2025.org/)", "summary": "Autonomously controlling quadrotors in large-scale subterranean environments\nis applicable to many areas such as environmental surveying, mining operations,\nand search and rescue. Learning-based controllers represent an appealing\napproach to autonomy, but are known to not generalize well to\n`out-of-distribution' environments not encountered during training. In this\nwork, we train a normalizing flow-based prior over the environment, which\nprovides a measure of how far out-of-distribution the quadrotor is at any given\ntime. We use this measure as a runtime monitor, allowing us to switch between a\nlearning-based controller and a safe controller when we are sufficiently\nout-of-distribution. Our methods are benchmarked on a point-to-point navigation\ntask in a simulated 3D cave environment based on real-world point cloud data\nfrom the DARPA Subterranean Challenge Final Event Dataset. Our experimental\nresults show that our combined controller simultaneously possesses the liveness\nof the learning-based controller (completing the task quickly) and the safety\nof the safety controller (avoiding collision).", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b66\u4e60\u63a7\u5236\u5668\u548c\u5b89\u5168\u63a7\u5236\u5668\u7684\u65b9\u6848\uff0c\u901a\u8fc7\u6b63\u89c4\u5316\u6d41\u524d\u7f6e\u8fdb\u884c\u73af\u5883\u5206\u5e03\u76d1\u6d4b\uff0c\u5728\u5730\u4e0b\u73af\u5883\u4e2d\u5b9e\u73b0\u56db\u65cb\u7ffc\u673a\u7684\u5b89\u5168\u81ea\u4e3b\u5bfc\u822a\u3002", "motivation": "\u5b66\u4e60\u57fa\u4e8e\u63a7\u5236\u5668\u5728\u5927\u89c4\u6a21\u5730\u4e0b\u73af\u5883\u4e2d\u5e94\u7528\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5b58\u5728\u5206\u5e03\u5916\u73af\u5883\u4e0d\u9002\u5e94\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u5408\u5b66\u4e60\u63a7\u5236\u5668\u6548\u7387\u548c\u5b89\u5168\u63a7\u5236\u5668\u53ef\u9760\u6027\u7684\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u6b63\u89c4\u5316\u6d41\u8bad\u7ec3\u73af\u5883\u524d\u7f6e\u5206\u5e03\uff0c\u901a\u8fc7\u8fd0\u884c\u65f6\u76d1\u6d4b\u56db\u65cb\u7ffc\u673a\u7684\u5206\u5e03\u504f\u79bb\u7a0b\u5ea6\uff0c\u5728\u8fc7\u5ea6\u5206\u5e03\u65f6\u81ea\u52a8\u5207\u6362\u5b66\u4e60\u63a7\u5236\u5668\u548c\u5b89\u5168\u63a7\u5236\u5668\u3002", "result": "\u5728\u57fa\u4e8eDARPA\u5730\u4e0b\u6311\u6218\u8d5b\u771f\u5b9e\u6570\u636e\u7684\u6a21\u62df\u6d1e\u7a74\u73af\u5883\u4e2d\u8fdb\u884c\u70b9\u5230\u70b9\u5bfc\u822a\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u7ed3\u5408\u63a7\u5236\u5668\u65e2\u4fdd\u6301\u4e86\u5b66\u4e60\u63a7\u5236\u5668\u7684\u9ad8\u6548\u6027\uff0c\u53c8\u786e\u4fdd\u4e86\u5b89\u5168\u63a7\u5236\u5668\u7684\u907f\u6495\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5b66\u4e60\u63a7\u5236\u5668\u5728\u5206\u5e03\u5916\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u95ee\u9898\uff0c\u4e3a\u5730\u4e0b\u73af\u5883\u4e2d\u7684\u56db\u65cb\u7ffc\u673a\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e2\u5b89\u5168\u53c8\u9ad8\u6548\u7684\u63a7\u5236\u65b9\u6848\u3002"}}
{"id": "2509.02815", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02815", "abs": "https://arxiv.org/abs/2509.02815", "authors": ["Nico Bohlinger", "Jan Peters"], "title": "Multi-Embodiment Locomotion at Scale with extreme Embodiment Randomization", "comment": null, "summary": "We present a single, general locomotion policy trained on a diverse\ncollection of 50 legged robots. By combining an improved embodiment-aware\narchitecture (URMAv2) with a performance-based curriculum for extreme\nEmbodiment Randomization, our policy learns to control millions of\nmorphological variations. Our policy achieves zero-shot transfer to unseen\nreal-world humanoid and quadruped robots.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u8fd0\u52a8\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u6539\u8fdb\u7684URMAv2\u67b6\u6784\u548c\u6027\u80fd\u5bfc\u5411\u8bfe\u7a0b\u5b66\u4e60\uff0c\u80fd\u591f\u63a7\u5236\u6570\u767e\u4e07\u79cd\u5f62\u6001\u53d8\u5316\u7684\u817f\u5f0f\u673a\u5668\u4eba\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u4eba\u5f62\u548c\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4e0d\u540c\u5f62\u6001\u817f\u5f0f\u673a\u5668\u4eba\u7684\u901a\u7528\u8fd0\u52a8\u63a7\u5236\u95ee\u9898\uff0c\u907f\u514d\u4e3a\u6bcf\u4e2a\u7279\u5b9a\u673a\u5668\u4eba\u8bbe\u8ba1\u5355\u72ec\u7684\u63a7\u5236\u7b56\u7565\uff0c\u5b9e\u73b0\u8de8\u5e73\u53f0\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684URMAv2\uff08\u5177\u8eab\u611f\u77e5\u67b6\u6784\uff09\u7ed3\u5408\u6027\u80fd\u5bfc\u5411\u7684\u6781\u7aef\u5177\u8eab\u968f\u673a\u5316\u8bfe\u7a0b\u5b66\u4e60\uff0c\u572850\u79cd\u817f\u5f0f\u673a\u5668\u4eba\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u7b56\u7565\u6210\u529f\u5b66\u4f1a\u4e86\u63a7\u5236\u6570\u767e\u4e07\u79cd\u5f62\u6001\u53d8\u5316\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u4eba\u5f62\u548c\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u901a\u8fc7\u9002\u5f53\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u53ef\u4ee5\u5f00\u53d1\u51fa\u5177\u6709\u9ad8\u5ea6\u6cdb\u5316\u80fd\u529b\u7684\u901a\u7528\u8fd0\u52a8\u63a7\u5236\u7b56\u7565\uff0c\u4e3a\u591a\u5f62\u6001\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.02870", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02870", "abs": "https://arxiv.org/abs/2509.02870", "authors": ["Harsh Muriki", "Hong Ray Teo", "Ved Sengupta", "Ai-Ping Hu"], "title": "Robotic 3D Flower Pose Estimation for Small-Scale Urban Farms", "comment": "7 pages, 7 figures", "summary": "The small scale of urban farms and the commercial availability of low-cost\nrobots (such as the FarmBot) that automate simple tending tasks enable an\naccessible platform for plant phenotyping. We have used a FarmBot with a custom\ncamera end-effector to estimate strawberry plant flower pose (for robotic\npollination) from acquired 3D point cloud models. We describe a novel algorithm\nthat translates individual occupancy grids along orthogonal axes of a point\ncloud to obtain 2D images corresponding to the six viewpoints. For each image,\n2D object detection models for flowers are used to identify 2D bounding boxes\nwhich can be converted into the 3D space to extract flower point clouds. Pose\nestimation is performed by fitting three shapes (superellipsoids, paraboloids\nand planes) to the flower point clouds and compared with manually labeled\nground truth. Our method successfully finds approximately 80% of flowers\nscanned using our customized FarmBot platform and has a mean flower pose error\nof 7.7 degrees, which is sufficient for robotic pollination and rivals previous\nresults. All code will be made available at\nhttps://github.com/harshmuriki/flowerPose.git.", "AI": {"tldr": "\u4f7f\u7528FarmBot\u673a\u5668\u4eba\u548c\u5b9a\u5236\u76f8\u673a\u7cfb\u7edf\u8fdb\u884c\u8349\u8393\u82b1\u59ff\u6001\u4f30\u8ba1\uff0c\u901a\u8fc73D\u70b9\u4e91\u548c2D\u56fe\u50cf\u68c0\u6d4b\u7b97\u6cd5\u5b9e\u73b080%\u7684\u82b1\u5349\u68c0\u6d4b\u7387\u548c7.7\u5ea6\u7684\u5e73\u5747\u59ff\u6001\u8bef\u5dee\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u6388\u7c89\u3002", "motivation": "\u5229\u7528\u4f4e\u6210\u672c\u673a\u5668\u4eba\u5e73\u53f0\u5b9e\u73b0\u690d\u7269\u8868\u578b\u5206\u6790\uff0c\u7279\u522b\u662f\u9488\u5bf9\u8349\u8393\u82b1\u7684\u673a\u5668\u4eba\u6388\u7c89\u9700\u6c42\uff0c\u89e3\u51b3\u5c0f\u89c4\u6a21\u57ce\u5e02\u519c\u573a\u4e2d\u7684\u81ea\u52a8\u5316\u690d\u7269\u76d1\u6d4b\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u65b0\u9896\u7b97\u6cd5\uff0c\u5c063D\u70b9\u4e91\u8f6c\u6362\u4e3a\u516d\u4e2a\u6b63\u4ea4\u89c6\u89d2\u76842D\u56fe\u50cf\uff0c\u4f7f\u75282D\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u8bc6\u522b\u82b1\u5349\u8fb9\u754c\u6846\uff0c\u7136\u540e\u901a\u8fc7\u62df\u5408\u8d85\u692d\u7403\u4f53\u3001\u629b\u7269\u9762\u548c\u5e73\u9762\u4e09\u79cd\u5f62\u72b6\u8fdb\u884c3D\u59ff\u6001\u4f30\u8ba1\u3002", "result": "\u6210\u529f\u68c0\u6d4b\u7ea680%\u7684\u82b1\u5349\uff0c\u5e73\u5747\u59ff\u6001\u8bef\u5dee\u4e3a7.7\u5ea6\uff0c\u6027\u80fd\u8db3\u4ee5\u652f\u6301\u673a\u5668\u4eba\u6388\u7c89\u5e94\u7528\uff0c\u4e0e\u5148\u524d\u7814\u7a76\u7ed3\u679c\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f4e\u6210\u672c\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u690d\u7269\u8868\u578b\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u82b1\u5349\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u4f9b\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2509.02876", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02876", "abs": "https://arxiv.org/abs/2509.02876", "authors": ["Hongrui Yu", "Vineet R. Kamat", "Carol C. Menassa"], "title": "Generalizable Skill Learning for Construction Robots with Crowdsourced Natural Language Instructions, Composable Skills Standardization, and Large Language Model", "comment": "Under review for ASCE OPEN: Multidisciplinary Journal of Civil\n  Engineering", "summary": "The quasi-repetitive nature of construction work and the resulting lack of\ngeneralizability in programming construction robots presents persistent\nchallenges to the broad adoption of robots in the construction industry. Robots\ncannot achieve generalist capabilities as skills learnt from one domain cannot\nreadily transfer to another work domain or be directly used to perform a\ndifferent set of tasks. Human workers have to arduously reprogram their\nscene-understanding, path-planning, and manipulation components to enable the\nrobots to perform alternate work tasks. The methods presented in this paper\nresolve a significant proportion of such reprogramming workload by proposing a\ngeneralizable learning architecture that directly teaches robots versatile\ntask-performance skills through crowdsourced online natural language\ninstructions. A Large Language Model (LLM), a standardized and modularized\nhierarchical modeling approach, and Building Information Modeling-Robot sematic\ndata pipeline are developed to address the multi-task skill transfer problem.\nThe proposed skill standardization scheme and LLM-based hierarchical skill\nlearning framework were tested with a long-horizon drywall installation\nexperiment using a full-scale industrial robotic manipulator. The resulting\nrobot task learning scheme achieves multi-task reprogramming with minimal\neffort and high quality.", "AI": {"tldr": "\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6807\u51c6\u5316\u6280\u80fd\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u5efa\u7b51\u884c\u4e1a\u673a\u5668\u4eba\u591a\u4efb\u52a1\u6280\u80fd\u8f6c\u79fb\u95ee\u9898\uff0c\u51cf\u5c11\u91cd\u590d\u7f16\u7a0b\u5de5\u4f5c\u91cf", "motivation": "\u5efa\u7b51\u884c\u4e1a\u673a\u5668\u4eba\u9762\u4e34\u51e0\u4f55\u91cd\u590d\u6027\u5de5\u4f5c\u5bfc\u81f4\u7684\u666e\u904d\u6027\u95ee\u9898\uff0c\u6280\u80fd\u65e0\u6cd5\u5728\u4e0d\u540c\u9886\u57df\u95f4\u8f6c\u79fb\uff0c\u9700\u8981\u91cd\u590d\u7f16\u7a0b\u573a\u666f\u7406\u89e3\u3001\u8def\u5f84\u89c4\u5212\u548c\u64cd\u63a7\u7ec4\u4ef6", "method": "\u63d0\u51fa\u901a\u7528\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7\u7fa4\u4f17\u6e90\u5728\u7ebf\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u76f4\u63a5\u6559\u638c\u591a\u6837\u5316\u4efb\u52a1\u6280\u80fd\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u3001\u6807\u51c6\u5316\u6a21\u5757\u5316\u5c42\u6b21\u5efa\u6a21\u65b9\u6cd5\u548cBIM-\u673a\u5668\u4eba\u8bed\u4e49\u6570\u636e\u7ba1\u9053", "result": "\u5728\u5b8c\u6574\u89c4\u6a21\u5de5\u4e1a\u673a\u5668\u64cd\u7eb5\u5668\u4e0a\u8fdb\u884c\u957f\u65f6\u95f4\u5e72\u5899\u5b89\u88c5\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6280\u80fd\u6807\u51c6\u5316\u65b9\u6848\u548cLLM\u57fa\u7840\u7684\u5c42\u6b21\u6280\u80fd\u5b66\u4e60\u6846\u67b6", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ee5\u6700\u5c0f\u5316\u7684\u52aa\u529b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u591a\u4efb\u52a1\u91cd\u65b0\u7f16\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u5efa\u7b51\u884c\u4e1a\u673a\u5668\u4eba\u666e\u904d\u6027\u4e0d\u8db3\u7684\u6311\u6218"}}
{"id": "2509.02972", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02972", "abs": "https://arxiv.org/abs/2509.02972", "authors": ["Haolan Zhang", "Thanh Nguyen Canh", "Chenghao Li", "Ruidong Yang", "Yonghoon Ji", "Nak Young Chong"], "title": "IL-SLAM: Intelligent Line-assisted SLAM Based on Feature Awareness for Dynamic Environments", "comment": "submitted to International Conference on Robotic Computing and\n  Communication(IEEE IRC)", "summary": "Visual Simultaneous Localization and Mapping (SLAM) plays a crucial role in\nautonomous systems. Traditional SLAM methods, based on static environment\nassumptions, struggle to handle complex dynamic environments. Recent dynamic\nSLAM systems employ geometric constraints and deep learning to remove dynamic\nfeatures, yet this creates a new challenge: insufficient remaining point\nfeatures for subsequent SLAM processes. Existing solutions address this by\ncontinuously introducing additional line and plane features to supplement point\nfeatures, achieving robust tracking and pose estimation. However, current\nmethods continuously introduce additional features regardless of necessity,\ncausing two problems: unnecessary computational overhead and potential\nperformance degradation from accumulated low-quality additional features and\nnoise. To address these issues, this paper proposes a feature-aware mechanism\nthat evaluates whether current features are adequate to determine if line\nfeature support should be activated. This decision mechanism enables the system\nto introduce line features only when necessary, significantly reducing\ncomputational complexity of additional features while minimizing the\nintroduction of low-quality features and noise. In subsequent processing, the\nintroduced line features assist in obtaining better initial camera poses\nthrough tracking, local mapping, and loop closure, but are excluded from global\noptimization to avoid potential negative impacts from low-quality additional\nfeatures in long-term process. Extensive experiments on TUM datasets\ndemonstrate substantial improvements in both ATE and RPE metrics compared to\nORB-SLAM3 baseline and superior performance over other dynamic SLAM and\nmulti-feature methods.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7279\u5f81\u611f\u77e5\u673a\u5236\u6765\u89e3\u51b3\u52a8\u6001SLAM\u4e2d\u7279\u5f81\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4ec5\u5728\u5fc5\u8981\u65f6\u6fc0\u6d3b\u7ebf\u6027\u7279\u5f81\u6765\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5e94\u5bf9\u4f4e\u8d28\u91cf\u7279\u5f81\u7684\u6316\u6cd5\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u52a8\u6001SLAM\u7cfb\u7edf\u5728\u79fb\u9664\u52a8\u6001\u7279\u5f81\u540e\u5bfc\u81f4\u7279\u5f81\u4e0d\u8db3\uff0c\u800c\u73b0\u6709\u65b9\u6848\u4e0d\u5206\u60c5\u51b5\u5730\u4e0d\u65ad\u6dfb\u52a0\u7279\u5f81\uff0c\u9020\u6210\u4e86\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7279\u5f81\u611f\u77e5\u673a\u5236\uff0c\u8bc4\u4f30\u5f53\u524d\u7279\u5f81\u662f\u5426\u8db3\u591f\uff0c\u4ee5\u51b3\u5b9a\u662f\u5426\u6fc0\u6d3b\u7ebf\u6027\u7279\u5f81\u652f\u6301\u3002\u5728\u540e\u7eed\u5904\u7406\u4e2d\uff0c\u5f15\u5165\u7684\u7ebf\u6027\u7279\u5f81\u53ea\u7528\u4e8e\u8f85\u52a9\u83b7\u53d6\u66f4\u597d\u7684\u521d\u59cb\u76f8\u673a\u4f4d\u59cb\uff0c\u4f46\u6392\u9664\u5728\u5168\u5c40\u4f18\u5316\u4e4b\u5916\u3002", "result": "\u5728TUM\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u4e0eORB-SLAM3\u57fa\u51c6\u7ebf\u548c\u5176\u4ed6\u52a8\u6001SLAM\u548c\u591a\u7279\u5f81\u65b9\u6cd5\u76f8\u6bd4\uff0cATE\u548cRPE\u6307\u6807\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u52a8\u6001SLAM\u4e2d\u7279\u5f81\u4e0d\u8db3\u7684\u6316\u6cd5\u95ee\u9898\uff0c\u901a\u8fc7\u667a\u80fd\u5730\u63a7\u5236\u7279\u5f81\u6dfb\u52a0\uff0c\u5728\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u7684\u540c\u65f6\u5927\u5927\u51cf\u5c11\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2509.02983", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.02983", "abs": "https://arxiv.org/abs/2509.02983", "authors": ["Jinghe Yang", "Minh-Quan Le", "Mingming Gong", "Ye Pu"], "title": "DUViN: Diffusion-Based Underwater Visual Navigation via Knowledge-Transferred Depth Features", "comment": null, "summary": "Autonomous underwater navigation remains a challenging problem due to limited\nsensing capabilities and the difficulty of constructing accurate maps in\nunderwater environments. In this paper, we propose a Diffusion-based Underwater\nVisual Navigation policy via knowledge-transferred depth features, named DUViN,\nwhich enables vision-based end-to-end 4-DoF motion control for underwater\nvehicles in unknown environments. DUViN guides the vehicle to avoid obstacles\nand maintain a safe and perception awareness altitude relative to the terrain\nwithout relying on pre-built maps. To address the difficulty of collecting\nlarge-scale underwater navigation datasets, we propose a method that ensures\nrobust generalization under domain shifts from in-air to underwater\nenvironments by leveraging depth features and introducing a novel model\ntransfer strategy. Specifically, our training framework consists of two phases:\nwe first train the diffusion-based visual navigation policy on in-air datasets\nusing a pre-trained depth feature extractor. Secondly, we retrain the extractor\non an underwater depth estimation task and integrate the adapted extractor into\nthe trained navigation policy from the first step. Experiments in both\nsimulated and real-world underwater environments demonstrate the effectiveness\nand generalization of our approach. The experimental videos are available at\nhttps://www.youtube.com/playlist?list=PLqt2s-RyCf1gfXJgFzKjmwIqYhrP4I-7Y.", "AI": {"tldr": "\u63d0\u51faDUViN\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e5\u8bc6\u8fc1\u79fb\u7684\u6df1\u5ea6\u7279\u5f81\u5b9e\u73b0\u6c34\u4e0b\u89c6\u89c9\u5bfc\u822a\uff0c\u65e0\u9700\u9884\u5efa\u5730\u56fe\u5373\u53ef\u8fdb\u884c4\u81ea\u7531\u5ea6\u8fd0\u52a8\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u6c34\u4e0b\u6570\u636e\u7a00\u7f3a\u548c\u9886\u57df\u8fc1\u79fb\u95ee\u9898", "motivation": "\u6c34\u4e0b\u81ea\u4e3b\u5bfc\u822a\u9762\u4e34\u4f20\u611f\u80fd\u529b\u6709\u9650\u548c\u6c34\u4e0b\u73af\u5883\u5efa\u56fe\u56f0\u96be\u7b49\u6311\u6218\uff0c\u540c\u65f6\u5927\u89c4\u6a21\u6c34\u4e0b\u5bfc\u822a\u6570\u636e\u96c6\u96be\u4ee5\u6536\u96c6\uff0c\u9700\u8981\u89e3\u51b3\u4ece\u7a7a\u4e2d\u5230\u6c34\u4e0b\u73af\u5883\u7684\u9886\u57df\u8fc1\u79fb\u95ee\u9898", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u9996\u5148\u5728\u7a7a\u57df\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u9884\u8bad\u7ec3\u6df1\u5ea6\u7279\u5f81\u63d0\u53d6\u5668\u8bad\u7ec3\u6269\u6563\u89c6\u89c9\u5bfc\u822a\u7b56\u7565\uff1b\u7136\u540e\u5728\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\u4e0a\u91cd\u65b0\u8bad\u7ec3\u63d0\u53d6\u5668\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u5df2\u8bad\u7ec3\u7684\u5bfc\u822a\u7b56\u7565\u4e2d", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b", "conclusion": "DUViN\u65b9\u6cd5\u901a\u8fc7\u77e5\u8bc6\u8fc1\u79fb\u7b56\u7565\u6210\u529f\u5b9e\u73b0\u4e86\u6c34\u4e0b\u89c6\u89c9\u5bfc\u822a\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u9886\u57df\u8fc1\u79fb\u95ee\u9898\uff0c\u4e3a\u6c34\u4e0b\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.02986", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02986", "abs": "https://arxiv.org/abs/2509.02986", "authors": ["Rankun Li", "Hao Wang", "Qi Li", "Zhuo Han", "Yifei Chu", "Linqi Ye", "Wende Xie", "Wenlong Liao"], "title": "CTBC: Contact-Triggered Blind Climbing for Wheeled Bipedal Robots with Instruction Learning and Reinforcement Learning", "comment": null, "summary": "In recent years, wheeled bipedal robots have gained increasing attention due\nto their advantages in mobility, such as high-speed locomotion on flat terrain.\nHowever, their performance on complex environments (e.g., staircases) remains\ninferior to that of traditional legged robots. To overcome this limitation, we\npropose a general contact-triggered blind climbing (CTBC) framework for wheeled\nbipedal robots. Upon detecting wheel-obstacle contact, the robot triggers a\nleg-lifting motion to overcome the obstacle. By leveraging a strongly-guided\nfeedforward trajectory, our method enables the robot to rapidly acquire agile\nleg-lifting skills, significantly enhancing its capability to traverse\nunstructured terrains. The approach has been experimentally validated and\nsuccessfully deployed on LimX Dynamics' wheeled bipedal robot, Tron1.\nReal-world tests demonstrate that Tron1 can reliably climb obstacles well\nbeyond its wheel radius using only proprioceptive feedback.", "AI": {"tldr": "\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4eba\u7684\u63a2\u89e6\u89e6\u53d1\u76f2\u5347\u684c\u6846\u67b6(CTBC)\uff0c\u901a\u8fc7\u8f6e\u5b50-\u969c\u788d\u7269\u63a2\u6d4b\u89e6\u53d1\u817f\u90e8\u62ac\u8d77\u52a8\u4f5c\uff0c\u4f7f\u5f97\u673a\u5668\u4eba\u80fd\u591f\u4ec5\u9760\u672c\u4f53\u611f\u77e5\u53ef\u9760\u8fc7\u8d85\u8fc7\u8f6e\u5b50\u534a\u5f84\u7684\u969c\u788d\u7269", "motivation": "\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4eba\u5728\u5e73\u5766\u5730\u5f62\u4e0a\u5177\u6709\u9ad8\u901f\u79fb\u52a8\u4f18\u52bf\uff0c\u4f46\u5728\u590d\u6742\u73af\u5883(\u5982\u697c\u68af)\u4e0a\u7684\u6027\u80fd\u4e0d\u5982\u4f20\u7edf\u8db3\u5f0f\u673a\u5668\u4eba\uff0c\u9700\u8981\u63d0\u5347\u5176\u8de8\u8d8a\u975e\u7ed3\u6784\u5316\u5730\u5f62\u7684\u80fd\u529b", "method": "\u63d0\u51faCTBC\u6846\u67b6\uff0c\u5728\u68c0\u6d4b\u5230\u8f6e\u5b50-\u969c\u788d\u7269\u63a5\u89e6\u65f6\u89e6\u53d1\u817f\u90e8\u62ac\u8d77\u52a8\u4f5c\uff0c\u91c7\u7528\u5f3a\u5bfc\u5411\u524d\u4f20\u8f68\u8ff9\u8ba9\u673a\u5668\u4eba\u5feb\u901f\u638c\u63e1\u7075\u6d3b\u7684\u817f\u90e8\u62ac\u8d77\u6280\u80fd", "result": "\u5728LimX Dynamics\u7684\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4ebaTron1\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5b9e\u9645\u6d4b\u8bd5\u8bc1\u660e\u673a\u5668\u4eba\u53ef\u9760\u5730\u5347\u8d85\u8fc7\u5176\u8f6e\u5b50\u534a\u5f84\u7684\u969c\u788d\u7269\uff0c\u4ec5\u4f7f\u7528\u672c\u4f53\u611f\u77e5\u53cd\u9988", "conclusion": "CTBC\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u5730\u5f62\u4e0a\u7684\u8fc7\u969c\u80fd\u529b\uff0c\u4e3a\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4eba\u7684\u5e94\u7528\u6269\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.03012", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03012", "abs": "https://arxiv.org/abs/2509.03012", "authors": ["Uddeshya Upadhyay"], "title": "Uncertainty-aware Test-Time Training (UT$^3$) for Efficient On-the-fly Domain Adaptive Dense Regression", "comment": null, "summary": "Deep neural networks (DNNs) are increasingly being used in autonomous\nsystems. However, DNNs do not generalize well to domain shift. Adapting to a\ncontinuously evolving environment is a safety-critical challenge inevitably\nfaced by all autonomous systems deployed to the real world. Recent work on\ntest-time training proposes methods that adapt to a new test distribution on\nthe fly by optimizing the DNN model for each test input using self-supervision.\nHowever, these techniques result in a sharp increase in inference time as\nmultiple forward and backward passes are required for a single test sample (for\ntest-time training) before finally making the prediction based on the\nfine-tuned features. This is undesirable for real-world robotics applications\nwhere these models may be deployed to resource constraint hardware with strong\nlatency requirements. In this work, we propose a new framework (called UT$^3$)\nthat leverages test-time training for improved performance in the presence of\ncontinuous domain shift while also decreasing the inference time, making it\nsuitable for real-world applications. Our method proposes an uncertainty-aware\nself-supervision task for efficient test-time training that leverages the\nquantified uncertainty to selectively apply the training leading to sharp\nimprovements in the inference time while performing comparably to standard\ntest-time training protocol. Our proposed protocol offers a continuous setting\nto identify the selected keyframes, allowing the end-user to control how often\nto apply test-time training. We demonstrate the efficacy of our method on a\ndense regression task - monocular depth estimation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faUT^3\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u81ea\u76d1\u7763\u4efb\u52a1\u4f18\u5316\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u7f29\u77ed\u63a8\u7406\u65f6\u95f4\uff0c\u9002\u7528\u4e8e\u81ea\u4e3b\u7cfb\u7edf\u7684\u8fde\u7eed\u57df\u8fc1\u79fb\u73af\u5883\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u57df\u8fc1\u79fb\u73af\u5883\u4e0b\u666e\u904d\u6027\u80fd\u4e0d\u4f73\uff0c\u800c\u73b0\u6709\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u65b9\u6cd5\u9700\u8981\u591a\u6b21\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\uff0c\u5bfc\u81f4\u63a8\u7406\u65f6\u95f4\u5927\u5e45\u589e\u52a0\uff0c\u4e0d\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u771f\u5b9e\u673a\u5668\u4eba\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u81ea\u76d1\u7763\u4efb\u52a1\uff0c\u5229\u7528\u91cf\u5316\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u9009\u62e9\u6027\u5730\u5e94\u7528\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff0c\u5e76\u63d0\u4f9b\u8fde\u7eed\u8bbe\u7f6e\u6765\u63a7\u5236\u8bad\u7ec3\u9891\u7387\u3002", "result": "\u5728\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\u4e0a\uff0c\u65b9\u6cd5\u5728\u4fdd\u6301\u4e0e\u6807\u51c6\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u76f8\u4f3c\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u7f29\u77ed\u4e86\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "UT^3\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u8fde\u7eed\u57df\u8fc1\u79fb\u6311\u6218\uff0c\u540c\u65f6\u6ee1\u8db3\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u5bf9\u5ef6\u8fdf\u7684\u4e25\u683c\u8981\u6c42\uff0c\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03119", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.03119", "abs": "https://arxiv.org/abs/2509.03119", "authors": ["Yash Vyas", "Matteo Bottin"], "title": "Forbal: Force Balanced 2-5 Degree of Freedom Robot Manipulator Built from a Five Bar Linkage", "comment": null, "summary": "A force balanced manipulator design based on the closed chain planar five bar\nlinkage is developed and experimentally validated. We present 2 variants as a\nmodular design: Forbal-2, a planar 2-DOF manipulator, and its extension to\n5-DOF spatial motion called Forbal-5. The design considerations in terms of\ngeometric, kinematic, and dynamic design that fulfill the force balance\nconditions while maximizing workspace are discussed. Then, the inverse\nkinematics of both variants are derived from geometric principles.\n  We validate the improvements from force balancing the manipulator through\ncomparative experiments with counter mass balanced and unbalanced\nconfigurations. The results show how the balanced configuration yields a\nreduction in the average reaction moments of up to 66\\%, a reduction of average\njoint torques of up to 79\\%, as well as a noticeable reduction in position\nerror for Forbal-2. For Forbal-5, which has a higher end effector payload mass,\nthe joint torques are reduced up to 84\\% for the balanced configuration.\nExperimental results validate that the balanced manipulator design is suitable\nfor applications where the reduction of joint torques and reaction\nforces/moments helps achieve millimeter level precision.", "AI": {"tldr": "\u57fa\u4e8e\u95ed\u94fe\u5e73\u9762\u4e94\u6761\u6746\u67b6\u6784\u7684\u529b\u5e73\u8861\u64cd\u4f5c\u5668\u8bbe\u8ba1\uff0c\u5305\u62ec2-DOF\u5e73\u9762\u7248\u672cForbal-2\u548c5-DOF\u7a7a\u95f4\u7248\u672cForbal-5\uff0c\u901a\u8fc7\u529b\u5e73\u8861\u8bbe\u8ba1\u663e\u8457\u51cf\u5c11\u5173\u8282\u626d\u77e9\u548c\u53cd\u529b\u77e9", "motivation": "\u8bbe\u8ba1\u529b\u5e73\u8861\u64cd\u4f5c\u5668\u4ee5\u51cf\u5c11\u5173\u8282\u626d\u77e9\u548c\u53cd\u529b\u77e9\uff0c\u63d0\u9ad8\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u9700\u8981\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u7684\u5e94\u7528\u573a\u666f", "method": "\u57fa\u4e8e\u95ed\u94fe\u5e73\u9762\u4e94\u6761\u6746\u67b6\u6784\u8fdb\u884c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u8003\u8651\u51e0\u4f55\u3001\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u8bbe\u8ba1\uff0c\u6ee1\u8db3\u529b\u5e73\u8861\u6761\u4ef6\u5e76\u6700\u5927\u5316\u5de5\u4f5c\u7a7a\u95f4\uff0c\u6d4e\u51e0\u4f55\u539f\u7406\u6c42\u89e3\u9006\u8fd0\u52a8\u5b66", "result": "\u5e73\u8861\u914d\u7f6e\u5bfc\u81f4\u5e73\u5747\u53cd\u529b\u77e9\u51cf\u5c1166%\uff0c\u5e73\u5747\u5173\u8282\u626d\u77e9\u51cf\u5c1179%\uff08Forbal-2\uff09\u548c84%\uff08Forbal-5\uff09\uff0cForbal-2\u4f4d\u7f6e\u8bef\u5dee\u663e\u8457\u51cf\u5c11", "conclusion": "\u529b\u5e73\u8861\u64cd\u4f5c\u5668\u8bbe\u8ba1\u80fd\u591f\u6709\u6548\u51cf\u5c11\u5173\u8282\u626d\u77e9\u548c\u53cd\u529b\u77e9\uff0c\u9002\u5408\u9700\u8981\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u7684\u5e94\u7528"}}
{"id": "2509.03211", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03211", "abs": "https://arxiv.org/abs/2509.03211", "authors": ["Beibei Zhou", "Zhiyuan Zhang", "Zhenbo Song", "Jianhui Guo", "Hui Kong"], "title": "Efficient Active Training for Deep LiDAR Odometry", "comment": null, "summary": "Robust and efficient deep LiDAR odometry models are crucial for accurate\nlocalization and 3D reconstruction, but typically require extensive and diverse\ntraining data to adapt to diverse environments, leading to inefficiencies. To\ntackle this, we introduce an active training framework designed to selectively\nextract training data from diverse environments, thereby reducing the training\nload and enhancing model generalization. Our framework is based on two key\nstrategies: Initial Training Set Selection (ITSS) and Active Incremental\nSelection (AIS). ITSS begins by breaking down motion sequences from general\nweather into nodes and edges for detailed trajectory analysis, prioritizing\ndiverse sequences to form a rich initial training dataset for training the base\nmodel. For complex sequences that are difficult to analyze, especially under\nchallenging snowy weather conditions, AIS uses scene reconstruction and\nprediction inconsistency to iteratively select training samples, refining the\nmodel to handle a wide range of real-world scenarios. Experiments across\ndatasets and weather conditions validate our approach's effectiveness. Notably,\nour method matches the performance of full-dataset training with just 52\\% of\nthe sequence volume, demonstrating the training efficiency and robustness of\nour active training paradigm. By optimizing the training process, our approach\nsets the stage for more agile and reliable LiDAR odometry systems, capable of\nnavigating diverse environmental conditions with greater precision.", "AI": {"tldr": "\u4e3b\u52a8\u8bad\u7ec3\u6846\u67b6\u901a\u8fc7\u7b80\u6d01\u7684\u6570\u636e\u9009\u62e9\u63d0\u5347\u6df1\u5ea6LiDAR\u6d4b\u91cf\u8ba1\u7b97\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6f14\u8fdb\u6027\u80fd\uff0c\u4ec5\u9700\u539f\u6570\u636e\u91cf\u768452%\u5373\u53ef\u8fbe\u5230\u5168\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6548\u679c", "motivation": "\u89e3\u51b3\u6df1\u5ea6LiDAR\u6d4b\u91cf\u8ba1\u7b97\u6a21\u578b\u9700\u8981\u5927\u91cf\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u624d\u80fd\u9002\u5e94\u591a\u6837\u73af\u5883\u7684\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6f14\u8fdb\u6027\u80fd", "method": "\u63d0\u51fa\u4e3b\u52a8\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u62ec\u521d\u59cb\u8bad\u7ec3\u96c6\u9009\u62e9(ITSS)\u548c\u4e3b\u52a8\u589e\u91cf\u9009\u62e9(AIS)\u4e24\u79cd\u7b56\u7565\uff1aITSS\u901a\u8fc7\u8f68\u8ff9\u5206\u6790\u9009\u62e9\u591a\u6837\u5316\u5e8f\u5217\uff0cAIS\u5229\u7528\u573a\u666f\u91cd\u5efa\u548c\u9884\u6d4b\u4e0d\u4e00\u81f4\u6027\u8fed\u4ee3\u9009\u62e9\u590d\u6742\u5e8f\u5217\u7684\u8bad\u7ec3\u6837\u672c", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u5929\u6c14\u6761\u4ef6\u4e0b\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u4ec5\u4f7f\u752852%\u7684\u5e8f\u5217\u91cf\u5373\u53ef\u8fbe\u5230\u5168\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6027\u80fd\u6c34\u5e73", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4e3a\u66f4\u7075\u6d3b\u53ef\u9760\u7684LiDAR\u6d4b\u91cf\u8ba1\u7b97\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u80fd\u591f\u5728\u591a\u6837\u73af\u5883\u6761\u4ef6\u4e0b\u5b9e\u73b0\u66f4\u9ad8\u7cbe\u5ea6\u7684\u5b9a\u4f4d"}}
{"id": "2509.03222", "categories": ["cs.RO", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03222", "abs": "https://arxiv.org/abs/2509.03222", "authors": ["Sophia Bianchi Moyen", "Rickmer Krohn", "Sophie Lueth", "Kay Pompetzki", "Jan Peters", "Vignesh Prasad", "Georgia Chalvatzaki"], "title": "The Role of Embodiment in Intuitive Whole-Body Teleoperation for Mobile Manipulation", "comment": "8 pages, 8 figures, Accepted at the IEEE-RAS International Conference\n  on Humanoid Robots (Humanoids) 2025", "summary": "Intuitive Teleoperation interfaces are essential for mobile manipulation\nrobots to ensure high quality data collection while reducing operator workload.\nA strong sense of embodiment combined with minimal physical and cognitive\ndemands not only enhances the user experience during large-scale data\ncollection, but also helps maintain data quality over extended periods. This\nbecomes especially crucial for challenging long-horizon mobile manipulation\ntasks that require whole-body coordination. We compare two distinct robot\ncontrol paradigms: a coupled embodiment integrating arm manipulation and base\nnavigation functions, and a decoupled embodiment treating these systems as\nseparate control entities. Additionally, we evaluate two visual feedback\nmechanisms: immersive virtual reality and conventional screen-based\nvisualization of the robot's field of view. These configurations were\nsystematically assessed across a complex, multi-stage task sequence requiring\nintegrated planning and execution. Our results show that the use of VR as a\nfeedback modality increases task completion time, cognitive workload, and\nperceived effort of the teleoperator. Coupling manipulation and navigation\nleads to a comparable workload on the user as decoupling the embodiments, while\npreliminary experiments suggest that data acquired by coupled teleoperation\nleads to better imitation learning performance. Our holistic view on intuitive\nteleoperation interfaces provides valuable insight into collecting\nhigh-quality, high-dimensional mobile manipulation data at scale with the human\noperator in mind. Project\nwebsite:https://sophiamoyen.github.io/role-embodiment-wbc-moma-teleop/", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u79fb\u52a8\u64cd\u4f5c\u673a\u5668\u4eba\u7684\u4e24\u79cd\u63a7\u5236\u8303\u5f0f\uff08\u8026\u5408\u4e0e\u89e3\u8026\uff09\u548c\u4e24\u79cd\u89c6\u89c9\u53cd\u9988\u673a\u5236\uff08VR\u4e0e\u5c4f\u5e55\uff09\uff0c\u53d1\u73b0VR\u4f1a\u589e\u52a0\u64cd\u4f5c\u8d1f\u62c5\uff0c\u8026\u5408\u63a7\u5236\u5728\u5de5\u4f5c\u8d1f\u62c5\u76f8\u5f53\u7684\u60c5\u51b5\u4e0b\u53ef\u80fd\u5e26\u6765\u66f4\u597d\u7684\u6a21\u4eff\u5b66\u4e60\u6027\u80fd", "motivation": "\u5f00\u53d1\u76f4\u89c2\u7684\u9065\u64cd\u4f5c\u754c\u9762\u5bf9\u4e8e\u79fb\u52a8\u64cd\u4f5c\u673a\u5668\u4eba\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5728\u9ad8\u7ef4\u6570\u636e\u6536\u96c6\u548c\u964d\u4f4e\u64cd\u4f5c\u5458\u5de5\u4f5c\u8d1f\u62c5\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5168\u8eab\u534f\u8c03\u7684\u590d\u6742\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e24\u79cd\u63a7\u5236\u8303\u5f0f\uff08\u8026\u5408\u5f0f\u96c6\u6210\u81c2\u64cd\u4f5c\u4e0e\u5e95\u5ea7\u5bfc\u822a\u529f\u80fd\uff0c\u89e3\u8026\u5f0f\u5206\u522b\u63a7\u5236\uff09\u548c\u4e24\u79cd\u89c6\u89c9\u53cd\u9988\u673a\u5236\uff08\u6c89\u6d78\u5f0fVR\u548c\u4f20\u7edf\u5c4f\u5e55\u53ef\u89c6\u5316\uff09\uff0c\u5728\u590d\u6742\u591a\u9636\u6bb5\u4efb\u52a1\u5e8f\u5217\u4e2d\u8fdb\u884c\u6d4b\u8bd5", "result": "\u4f7f\u7528VR\u4f5c\u4e3a\u53cd\u9988\u65b9\u5f0f\u4f1a\u589e\u52a0\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3001\u8ba4\u77e5\u5de5\u4f5c\u8d1f\u62c5\u548c\u611f\u77e5\u52aa\u529b\uff1b\u8026\u5408\u64cd\u4f5c\u4e0e\u89e3\u8026\u64cd\u4f5c\u5bf9\u7528\u6237\u5de5\u4f5c\u8d1f\u62c5\u76f8\u5f53\uff0c\u4f46\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\u8026\u5408\u9065\u64cd\u4f5c\u6536\u96c6\u7684\u6570\u636e\u80fd\u5e26\u6765\u66f4\u597d\u7684\u6a21\u4eff\u5b66\u4e60\u6027\u80fd", "conclusion": "\u7814\u7a76\u4e3a\u5982\u4f55\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u6536\u96c6\u9ad8\u8d28\u91cf\u3001\u9ad8\u7ef4\u5ea6\u7684\u79fb\u52a8\u64cd\u4f5c\u6570\u636e\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u8026\u5408\u63a7\u5236\u8303\u5f0f\u5728\u4fdd\u6301\u7528\u6237\u5de5\u4f5c\u8d1f\u62c5\u7684\u540c\u65f6\u53ef\u80fd\u63d0\u5347\u540e\u7eed\u5b66\u4e60\u6027\u80fd"}}
{"id": "2509.03231", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.03231", "abs": "https://arxiv.org/abs/2509.03231", "authors": ["Stephan Vonschallen", "Larissa Julia Corina Finsler", "Theresa Schmiedel", "Friederike Eyssel"], "title": "Exploring persuasive Interactions with generative social robots: An experimental framework", "comment": "A shortened version of this paper was accepted as poster for the\n  Thirteenth International Conference on Human-Agent Interaction (HAI2025)", "summary": "Integrating generative AI such as large language models into social robots\nhas improved their ability to engage in natural, human-like communication. This\nstudy presents a method to examine their persuasive capabilities. We designed\nan experimental framework focused on decision making and tested it in a pilot\nthat varied robot appearance and self-knowledge. Using qualitative analysis, we\nevaluated interaction quality, persuasion effectiveness, and the robot's\ncommunicative strategies. Participants generally experienced the interaction\npositively, describing the robot as competent, friendly, and supportive, while\nnoting practical limits such as delayed responses and occasional\nspeech-recognition errors. Persuasiveness was highly context dependent and\nshaped by robot behavior: participants responded well to polite, reasoned\nsuggestions and expressive gestures, but emphasized the need for more\npersonalized, context-aware arguments and clearer social roles. These findings\nsuggest that generative social robots can influence user decisions, but their\neffectiveness depends on communicative nuance and contextual relevance. We\npropose refinements to the framework to further study persuasive dynamics\nbetween robots and human users.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\u6846\u67b6\u5206\u6790\u4e86\u96c6\u6210\u751f\u6210\u5f0fAI\u7684\u793e\u4ea4\u673a\u5668\u4eba\u7684\u8bf4\u670d\u80fd\u529b\uff0c\u53d1\u73b0\u673a\u5668\u4eba\u7684\u8bf4\u670d\u6548\u679c\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587\u548c\u6c9f\u901a\u7ec6\u8282\uff0c\u5efa\u8bae\u8fdb\u4e00\u6b65\u4f18\u5316\u6846\u67b6\u4ee5\u6df1\u5165\u7814\u7a76\u4eba\u673a\u8bf4\u670d\u52a8\u6001", "motivation": "\u7814\u7a76\u96c6\u6210\u751f\u6210\u5f0fAI\u7684\u793e\u4ea4\u673a\u5668\u4eba\u7684\u8bf4\u670d\u80fd\u529b\uff0c\u4ee5\u4e86\u89e3\u5b83\u4eec\u5982\u4f55\u5f71\u54cd\u7528\u6237\u51b3\u7b56", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5173\u6ce8\u51b3\u7b56\u5236\u5b9a\u7684\u5b9e\u9a8c\u6846\u67b6\uff0c\u901a\u8fc7\u53d8\u5316\u673a\u5668\u4eba\u5916\u89c2\u548c\u81ea\u6211\u77e5\u8bc6\u8fdb\u884c\u8bd5\u9a8c\uff0c\u91c7\u7528\u5b9a\u6027\u5206\u6790\u8bc4\u4f30\u4ea4\u4e92\u8d28\u91cf\u3001\u8bf4\u670d\u6548\u679c\u548c\u6c9f\u901a\u7b56\u7565", "result": "\u53c2\u4e0e\u8005\u5bf9\u4ea4\u4e92\u611f\u53d7\u6b63\u9762\uff0c\u8ba4\u4e3a\u673a\u5668\u4eba\u80fd\u5e72\u3001\u53cb\u597d\u4e14\u652f\u6301\u6027\u5f3a\uff0c\u4f46\u4e5f\u6307\u51fa\u5ef6\u8fdf\u54cd\u5e94\u548c\u8bed\u97f3\u8bc6\u522b\u9519\u8bef\u7b49\u9650\u5236\u3002\u8bf4\u670d\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u4e0a\u4e0b\u6587\uff0c\u53c2\u4e0e\u8005\u5bf9\u793c\u8c8c\u3001\u5408\u7406\u7684\u5efa\u8bae\u548c\u8868\u60c5\u4f53\u6001\u53cd\u5e94\u6b63\u9762\uff0c\u4f46\u9700\u8981\u66f4\u4e2a\u6027\u5316\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u8bba\u636e\u548c\u66f4\u6e05\u6670\u7684\u793e\u4ea4\u89d2\u8272", "conclusion": "\u751f\u6210\u5f0f\u793e\u4ea4\u673a\u5668\u4eba\u53ef\u4ee5\u5f71\u54cd\u7528\u6237\u51b3\u7b56\uff0c\u4f46\u5176\u6548\u679c\u53d6\u51b3\u4e8e\u6c9f\u901a\u7ec6\u8282\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\uff0c\u5efa\u8bae\u4f18\u5316\u6846\u67b6\u4ee5\u6df1\u5165\u7814\u7a76\u4eba\u673a\u8bf4\u670d\u52a8\u6001"}}
{"id": "2509.03238", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.03238", "abs": "https://arxiv.org/abs/2509.03238", "authors": ["Martin Goubej", "Lauria Clarke", "Martin Hraba\u010dka", "David Tolar"], "title": "Vibration Damping in Underactuated Cable-suspended Artwork -- Flying Belt Motion Control", "comment": "10 pages, 10 figures", "summary": "This paper presents a comprehensive refurbishment of the interactive robotic\nart installation Standards and Double Standards by Rafael Lozano-Hemmer. The\ninstallation features an array of belts suspended from the ceiling, each\nactuated by stepper motors and dynamically oriented by a vision-based tracking\nsystem that follows the movements of exhibition visitors. The original system\nwas limited by oscillatory dynamics, resulting in torsional and pendulum-like\nvibrations that constrained rotational speed and reduced interactive\nresponsiveness. To address these challenges, the refurbishment involved\nsignificant upgrades to both hardware and motion control algorithms. A detailed\nmathematical model of the flying belt system was developed to accurately\ncapture its dynamic behavior, providing a foundation for advanced control\ndesign. An input shaping method, formulated as a convex optimization problem,\nwas implemented to effectively suppress vibrations, enabling smoother and\nfaster belt movements. Experimental results demonstrate substantial\nimprovements in system performance and audience interaction. This work\nexemplifies the integration of robotics, control engineering, and interactive\nart, offering new solutions to technical challenges in real-time motion control\nand vibration damping for large-scale kinetic installations.", "AI": {"tldr": "\u5bf9Rafael Lozano-Hemmer\u7684\u4e92\u52a8\u673a\u5668\u4eba\u827a\u672f\u88c5\u7f6eStandards and Double Standards\u8fdb\u884c\u5168\u9762\u7ffb\u65b0\uff0c\u901a\u8fc7\u6570\u5b66\u5efa\u6a21\u548c\u8f93\u5165\u6574\u5f62\u63a7\u5236\u7b97\u6cd5\u89e3\u51b3\u4e86\u539f\u7cfb\u7edf\u7684\u632f\u52a8\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u548c\u4e92\u52a8\u54cd\u5e94\u6027\u3002", "motivation": "\u539f\u827a\u672f\u88c5\u7f6e\u5b58\u5728\u632f\u8361\u52a8\u529b\u5b66\u95ee\u9898\uff0c\u5bfc\u81f4\u626d\u8f6c\u548c\u6446\u9524\u5f0f\u632f\u52a8\uff0c\u9650\u5236\u4e86\u65cb\u8f6c\u901f\u5ea6\u5e76\u964d\u4f4e\u4e86\u4e92\u52a8\u54cd\u5e94\u6027\uff0c\u9700\u8981\u8fdb\u884c\u786c\u4ef6\u548c\u8fd0\u52a8\u63a7\u5236\u7b97\u6cd5\u7684\u5347\u7ea7\u3002", "method": "\u5f00\u53d1\u4e86\u98de\u884c\u76ae\u5e26\u7cfb\u7edf\u7684\u8be6\u7ec6\u6570\u5b66\u6a21\u578b\uff0c\u91c7\u7528\u51f8\u4f18\u5316\u95ee\u9898\u5f62\u5f0f\u7684\u8f93\u5165\u6574\u5f62\u65b9\u6cd5\u6765\u6291\u5236\u632f\u52a8\uff0c\u5b9e\u73b0\u66f4\u5e73\u6ed1\u5feb\u901f\u7684\u76ae\u5e26\u8fd0\u52a8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u7cfb\u7edf\u6027\u80fd\u548c\u89c2\u4f17\u4e92\u52a8\u4f53\u9a8c\u5f97\u5230\u663e\u8457\u6539\u5584\uff0c\u632f\u52a8\u5f97\u5230\u6709\u6548\u6291\u5236\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u673a\u5668\u4eba\u6280\u672f\u3001\u63a7\u5236\u5de5\u7a0b\u548c\u4e92\u52a8\u827a\u672f\u7684\u878d\u5408\uff0c\u4e3a\u5927\u578b\u52a8\u6001\u88c5\u7f6e\u5b9e\u65f6\u8fd0\u52a8\u63a7\u5236\u548c\u632f\u52a8\u963b\u5c3c\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03261", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.03261", "abs": "https://arxiv.org/abs/2509.03261", "authors": ["Elias Fontanari", "Gianni Lunardi", "Matteo Saveriano", "Andrea Del Prete"], "title": "Parallel-Constraint Model Predictive Control: Exploiting Parallel Computation for Improving Safety", "comment": null, "summary": "Ensuring constraint satisfaction is a key requirement for safety-critical\nsystems, which include most robotic platforms. For example, constraints can be\nused for modeling joint position/velocity/torque limits and collision\navoidance. Constrained systems are often controlled using Model Predictive\nControl, because of its ability to naturally handle constraints, relying on\nnumerical optimization. However, ensuring constraint satisfaction is\nchallenging for nonlinear systems/constraints. A well-known tool to make\ncontrollers safe is the so-called control-invariant set (a.k.a. safe set). In\nour previous work, we have shown that safety can be improved by letting the\nsafe-set constraint recede along the MPC horizon. In this paper, we push that\nidea further by exploiting parallel computation to improve safety. We solve\nseveral MPC problems at the same time, where each problem instantiates the\nsafe-set constraint at a different time step along the horizon. Finally, the\ncontroller can select the best solution according to some user-defined\ncriteria. We validated this idea through extensive simulations with a 3-joint\nrobotic arm, showing that significant improvements can be achieved in terms of\nsafety and performance, even using as little as 4 computational cores.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5e76\u884c\u8ba1\u7b97\u7684\u5b89\u5168\u96c6\u7ea6\u675fMPC\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u65f6\u6c42\u89e3\u591a\u4e2a\u4e0d\u540c\u65f6\u95f4\u6b65\u5b89\u5168\u96c6\u7ea6\u675f\u7684MPC\u95ee\u9898\u6765\u63d0\u5347\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd", "motivation": "\u786e\u4fdd\u7ea6\u675f\u6ee1\u8db3\u662f\u5b89\u5168\u5173\u952e\u7cfb\u7edf\uff08\u5982\u673a\u5668\u4eba\uff09\u7684\u6838\u5fc3\u8981\u6c42\uff0c\u4f46\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u7ea6\u675f\u6ee1\u8db3\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u6539\u8fdb\u4f20\u7edfMPC\u7684\u5b89\u5168\u4fdd\u969c\u80fd\u529b", "method": "\u5e76\u884c\u6c42\u89e3\u591a\u4e2aMPC\u95ee\u9898\uff0c\u6bcf\u4e2a\u95ee\u9898\u5728\u9884\u6d4b\u65f6\u57df\u7684\u4e0d\u540c\u65f6\u95f4\u6b65\u5b9e\u4f8b\u5316\u5b89\u5168\u96c6\u7ea6\u675f\uff0c\u7136\u540e\u6839\u636e\u7528\u6237\u5b9a\u4e49\u6807\u51c6\u9009\u62e9\u6700\u4f18\u89e3", "result": "\u901a\u8fc73\u5173\u8282\u673a\u68b0\u81c2\u7684\u5e7f\u6cdb\u4eff\u771f\u9a8c\u8bc1\uff0c\u5373\u4f7f\u4f7f\u75284\u4e2a\u8ba1\u7b97\u6838\u5fc3\u4e5f\u80fd\u5728\u5b89\u5168\u6027\u548c\u6027\u80fd\u65b9\u9762\u5b9e\u73b0\u663e\u8457\u6539\u8fdb", "conclusion": "\u5e76\u884c\u8ba1\u7b97\u7684\u5b89\u5168\u96c6\u7ea6\u675fMPC\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u975e\u7ebf\u6027\u7ea6\u675f\u7cfb\u7edf\u7684\u5b89\u5168\u4fdd\u8bc1\u548c\u6027\u80fd\u8868\u73b0"}}
{"id": "2509.03436", "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY", "I.2.9; C.3; J.3"], "pdf": "https://arxiv.org/pdf/2509.03436", "abs": "https://arxiv.org/abs/2509.03436", "authors": ["Md Mhamud Hussen Sifat", "Md Maruf", "Md Rokunuzzaman"], "title": "Cost-Optimized Systems Engineering for IoT-Enabled Robot Nurse in Infectious Pandemic Management", "comment": "11 pages, 10 figures, 4 tables, 1 algorithm. Corresponding author: Md\n  Maruf (maruf.mte.17@gmail.com)", "summary": "The utilization of robotic technology has gained traction in healthcare\nfacilities due to progress in the field that enables time and cost savings,\nminimizes waste, and improves patient care. Digital healthcare technologies\nthat leverage automation, such as robotics and artificial intelligence, have\nthe potential to enhance the sustainability and profitability of healthcare\nsystems in the long run. However, the recent COVID-19 pandemic has amplified\nthe need for cyber-physical robots to automate check-ups and medication\nadministration. A robot nurse is controlled by the Internet of Things (IoT) and\ncan serve as an automated medical assistant while also allowing supervisory\ncontrol based on custom commands. This system helps reduce infection risk and\nimproves outcomes in pandemic settings. This research presents a test case with\na nurse robot that can assess a patient's health status and take action\naccordingly. We also evaluate the system's performance in medication\nadministration, health-status monitoring, and life-cycle considerations.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u7269\u8054\u7f51\u7684\u62a4\u58eb\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5728\u75ab\u60c5\u60c5\u51b5\u4e0b\u5b9e\u73b0\u81ea\u52a8\u5316\u5065\u5eb7\u68c0\u67e5\u548c\u836f\u7269\u7ba1\u7406\uff0c\u4ee5\u964d\u4f4e\u611f\u67d3\u98ce\u9669\u548c\u6539\u5584\u75c5\u4eba\u7ed3\u679c\u3002", "motivation": "\u65b0\u51a0\u75ab\u60c5\u5f3a\u5316\u4e86\u5bf9\u8f7b\u578b\u8bbe\u5907\u7684\u9700\u6c42\uff0c\u9700\u8981\u673a\u5668\u4eba\u6280\u672f\u6765\u81ea\u52a8\u5316\u68c0\u67e5\u548c\u836f\u7269\u7ba1\u7406\uff0c\u4ee5\u51cf\u5c11\u611f\u67d3\u98ce\u9669\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7531\u7269\u8054\u7f51\u63a7\u5236\u7684\u62a4\u58eb\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u53ef\u4ee5\u8bc4\u4f30\u75c5\u4eba\u5065\u5eb7\u72b6\u51b5\u5e76\u91c7\u53d6\u76f8\u5e94\u884c\u52a8\u3002\u8fdb\u884c\u4e86\u836f\u7269\u7ba1\u7406\u3001\u5065\u5eb7\u76d1\u6d4b\u548c\u751f\u547d\u5468\u671f\u8003\u8651\u7b49\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u964d\u4f4e\u611f\u67d3\u98ce\u9669\uff0c\u6539\u5584\u75ab\u60c5\u73af\u5883\u4e0b\u7684\u75c5\u4eba\u7ed3\u679c\uff0c\u5e76\u4f5c\u4e3a\u81ea\u52a8\u5316\u533b\u7597\u52a9\u624b\u63d0\u4f9b\u76d1\u7763\u63a7\u5236\u529f\u80fd\u3002", "conclusion": "\u673a\u5668\u4eba\u62a4\u7406\u6280\u672f\u901a\u8fc7\u81ea\u52a8\u5316\u68c0\u67e5\u548c\u836f\u7269\u7ba1\u7406\uff0c\u5728\u75ab\u60c5\u671f\u95f4\u53ef\u4ee5\u63d0\u9ad8\u533b\u7597\u7cfb\u7edf\u7684\u53ef\u6301\u7eed\u6027\u548c\u76c8\u5229\u80fd\u529b\uff0c\u4e3a\u533b\u7597\u670d\u52a1\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2509.03500", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03500", "abs": "https://arxiv.org/abs/2509.03500", "authors": ["Itai Zilberstein", "Alberto Candela", "Steve Chien"], "title": "Real-Time Instrument Planning and Perception for Novel Measurements of Dynamic Phenomena", "comment": "Appears in Proceedings of 18th Symposium on Advanced Space\n  Technologies in Robotics and Automation", "summary": "Advancements in onboard computing mean remote sensing agents can employ\nstate-of-the-art computer vision and machine learning at the edge. These\ncapabilities can be leveraged to unlock new rare, transient, and pinpoint\nmeasurements of dynamic science phenomena. In this paper, we present an\nautomated workflow that synthesizes the detection of these dynamic events in\nlook-ahead satellite imagery with autonomous trajectory planning for a\nfollow-up high-resolution sensor to obtain pinpoint measurements. We apply this\nworkflow to the use case of observing volcanic plumes. We analyze\nclassification approaches including traditional machine learning algorithms and\nconvolutional neural networks. We present several trajectory planning\nalgorithms that track the morphological features of a plume and integrate these\nalgorithms with the classifiers. We show through simulation an order of\nmagnitude increase in the utility return of the high-resolution instrument\ncompared to baselines while maintaining efficient runtimes.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u901a\u8fc7\u7ed3\u5408\u536b\u661f\u56fe\u50cf\u4e2d\u52a8\u6001\u4e8b\u4ef6\u68c0\u6d4b\u4e0e\u81ea\u4e3b\u8f68\u8ff9\u89c4\u5212\uff0c\u5b9e\u73b0\u5bf9\u706b\u5c71\u70df\u67f1\u7b49\u79d1\u5b66\u73b0\u8c61\u7684\u9ad8\u5206\u8fa8\u7387\u8ddf\u8e2a\u89c2\u6d4b\uff0c\u5e76\u5728\u6a21\u62df\u4e2d\u5c55\u793a\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u89c2\u6d4b\u6548\u679c\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5229\u7528\u8fb9\u7f18\u8ba1\u7b97\u6280\u672f\u5728\u9065\u611f\u4ee3\u7406\u4e2d\u5b9e\u73b0\u6700\u65b0\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u80fd\u529b\uff0c\u4ee5\u83b7\u53d6\u7a00\u6709\u3001\u77ac\u6001\u548c\u51c6\u786e\u7684\u52a8\u6001\u79d1\u5b66\u73b0\u8c61\u6d4b\u91cf\u3002", "method": "\u63d0\u51fa\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u7ed3\u5408\u9884\u5148\u536b\u661f\u56fe\u50cf\u4e2d\u52a8\u6001\u4e8b\u4ef6\u68c0\u6d4b\u4e0e\u81ea\u4e3b\u8f68\u8ff9\u89c4\u5212\u3002\u91c7\u7528\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5206\u7c7b\uff0c\u5f00\u53d1\u591a\u79cd\u8f68\u8ff9\u89c4\u5212\u7b97\u6cd5\u6765\u8ddf\u8e2a\u70df\u67f1\u5f62\u6001\u7279\u5f81\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u5c55\u793a\uff0c\u9ad8\u5206\u8fa8\u7387\u4eea\u5668\u7684\u6548\u7528\u56de\u62a5\u6bd4\u57fa\u51c6\u65b9\u6cd5\u63d0\u9ad8\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u7684\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u6d41\u7a0b\u80fd\u591f\u6709\u6548\u5730\u7ed3\u5408\u52a8\u6001\u4e8b\u4ef6\u68c0\u6d4b\u4e0e\u81ea\u4e3b\u8f68\u8ff9\u89c4\u5212\uff0c\u4e3a\u706b\u5c71\u70df\u67f1\u7b49\u79d1\u5b66\u73b0\u8c61\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u9ad8\u5206\u8fa8\u7387\u89c2\u6d4b\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fdc\u7a0b\u611f\u77e5\u4ee3\u7406\u7684\u89c2\u6d4b\u80fd\u529b\u3002"}}
{"id": "2509.03515", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.03515", "abs": "https://arxiv.org/abs/2509.03515", "authors": ["Yanlin Zhang", "Sungyong Chung", "Nachuan Li", "Dana Monzer", "Hani S. Mahmassani", "Samer H. Hamdar", "Alireza Talebpour"], "title": "Can the Waymo Open Motion Dataset Support Realistic Behavioral Modeling? A Validation Study with Naturalistic Trajectories", "comment": null, "summary": "The Waymo Open Motion Dataset (WOMD) has become a popular resource for\ndata-driven modeling of autonomous vehicles (AVs) behavior. However, its\nvalidity for behavioral analysis remains uncertain due to proprietary\npost-processing, the absence of error quantification, and the segmentation of\ntrajectories into 20-second clips. This study examines whether WOMD accurately\ncaptures the dynamics and interactions observed in real-world AV operations.\nLeveraging an independently collected naturalistic dataset from Level 4 AV\noperations in Phoenix, Arizona (PHX), we perform comparative analyses across\nthree representative urban driving scenarios: discharging at signalized\nintersections, car-following, and lane-changing behaviors. For the discharging\nanalysis, headways are manually extracted from aerial video to ensure\nnegligible measurement error. For the car-following and lane-changing cases, we\napply the Simulation-Extrapolation (SIMEX) method to account for empirically\nestimated error in the PHX data and use Dynamic Time Warping (DTW) distances to\nquantify behavioral differences. Results across all scenarios consistently show\nthat behavior in PHX falls outside the behavioral envelope of WOMD. Notably,\nWOMD underrepresents short headways and abrupt decelerations. These findings\nsuggest that behavioral models calibrated solely on WOMD may systematically\nunderestimate the variability, risk, and complexity of naturalistic driving.\nCaution is therefore warranted when using WOMD for behavior modeling without\nproper validation against independently collected data.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0Waymo\u5f00\u653e\u8fd0\u52a8\u6570\u636e\u96c6(WOMD)\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u771f\u5b9e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u884c\u4e3a\u52a8\u6001\u548c\u4ea4\u4e92\uff0c\u5b58\u5728\u7cfb\u7edf\u6027\u4f4e\u4f30\u9a7e\u9a76\u884c\u4e3a\u53d8\u5f02\u6027\u3001\u98ce\u9669\u548c\u590d\u6742\u6027\u7684\u95ee\u9898", "motivation": "\u9a8c\u8bc1WOMD\u6570\u636e\u96c6\u5728\u81ea\u52a8\u9a7e\u9a76\u884c\u4e3a\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\uff0c\u56e0\u4e3a\u8be5\u6570\u636e\u96c6\u5b58\u5728\u4e13\u6709\u540e\u5904\u7406\u3001\u7f3a\u4e4f\u8bef\u5dee\u91cf\u5316\u4ee5\u53ca\u8f68\u8ff9\u5206\u6bb5\u7b49\u95ee\u9898", "method": "\u4f7f\u7528\u51e4\u51f0\u57ce\u72ec\u7acb\u6536\u96c6\u7684L4\u7ea7\u81ea\u52a8\u9a7e\u9a76\u81ea\u7136\u6570\u636e\u96c6(PHX)\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\uff0c\u5305\u62ec\u4fe1\u53f7\u4ea4\u53c9\u53e3\u6392\u961f\u3001\u8ddf\u8f66\u548c\u6362\u9053\u4e09\u79cd\u573a\u666f\uff0c\u91c7\u7528SIMEX\u65b9\u6cd5\u5904\u7406\u6d4b\u91cf\u8bef\u5dee\u548cDTW\u8ddd\u79bb\u91cf\u5316\u884c\u4e3a\u5dee\u5f02", "result": "\u6240\u6709\u573a\u666f\u4e0bPHX\u6570\u636e\u7684\u884c\u4e3a\u90fd\u8d85\u51fa\u4e86WOMD\u7684\u884c\u4e3a\u8303\u56f4\uff0cWOMD\u4f4e\u4f30\u4e86\u77ed\u8f66\u5934\u65f6\u8ddd\u548c\u6025\u51cf\u901f\u884c\u4e3a", "conclusion": "\u4ec5\u57fa\u4e8eWOMD\u6821\u51c6\u7684\u884c\u4e3a\u6a21\u578b\u53ef\u80fd\u7cfb\u7edf\u6027\u5730\u4f4e\u4f30\u81ea\u7136\u9a7e\u9a76\u7684\u53d8\u5f02\u6027\u3001\u98ce\u9669\u548c\u590d\u6742\u6027\uff0c\u4f7f\u7528\u65f6\u9700\u8981\u8c28\u614e\u5e76\u8fdb\u884c\u72ec\u7acb\u6570\u636e\u9a8c\u8bc1"}}
