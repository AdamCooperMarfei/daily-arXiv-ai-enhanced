<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 48]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [VAR-SLAM: Visual Adaptive and Robust SLAM for Dynamic Environments](https://arxiv.org/abs/2510.16205)
*João Carlos Virgolino Soares,Gabriel Fischer Abati,Claudio Semini*

Main category: cs.RO

TL;DR: VAR-SLAM是一个基于ORB-SLAM3的动态环境视觉SLAM系统，结合轻量级语义关键点过滤器和自适应鲁棒损失函数，有效处理已知和未知移动物体。


<details>
  <summary>Details</summary>
Motivation: 现有动态环境SLAM方法依赖语义过滤只能处理已知物体类别，或使用固定鲁棒核无法适应未知移动物体，导致场景中出现未知物体时精度下降。

Method: 结合轻量级语义关键点过滤器处理已知移动物体，使用Barron自适应鲁棒损失处理未知物体，通过残差在线估计鲁棒核的形状参数，自动在Gaussian和重尾行为之间调整。

Result: 在TUM RGB-D、Bonn RGB-D Dynamic和OpenLORIS数据集上评估，相比最先进基线方法轨迹精度和鲁棒性均有提升，在挑战性序列上比NGD-SLAM降低高达25%的ATE RMSE，同时保持平均27 FPS的性能。

Conclusion: VAR-SLAM通过结合语义过滤和自适应鲁棒损失，在动态环境中实现了更高的轨迹精度和鲁棒性，能够有效处理已知和未知移动物体。

Abstract: Visual SLAM in dynamic environments remains challenging, as several existing
methods rely on semantic filtering that only handles known object classes, or
use fixed robust kernels that cannot adapt to unknown moving objects, leading
to degraded accuracy when they appear in the scene. We present VAR-SLAM (Visual
Adaptive and Robust SLAM), an ORB-SLAM3-based system that combines a
lightweight semantic keypoint filter to deal with known moving objects, with
Barron's adaptive robust loss to handle unknown ones. The shape parameter of
the robust kernel is estimated online from residuals, allowing the system to
automatically adjust between Gaussian and heavy-tailed behavior. We evaluate
VAR-SLAM on the TUM RGB-D, Bonn RGB-D Dynamic, and OpenLORIS datasets, which
include both known and unknown moving objects. Results show improved trajectory
accuracy and robustness over state-of-the-art baselines, achieving up to 25%
lower ATE RMSE than NGD-SLAM on challenging sequences, while maintaining
performance at 27 FPS on average.

</details>


### [2] [DeGrip: A Compact Cable-driven Robotic Gripper for Desktop Disassembly](https://arxiv.org/abs/2510.16231)
*Bihao Zhang,Davood Soleymanzadeh,Xiao Liang,Minghui Zheng*

Main category: cs.RO

TL;DR: DeGrip是一款专为废旧电脑台式机拆卸设计的定制化夹爪，具有3个自由度，采用线缆驱动机制，能在狭小空间操作，并在Isaac Sim仿真环境中验证了其拆卸能力。


<details>
  <summary>Details</summary>
Motivation: 智能机器人拆卸报废产品一直是机器人领域的长期挑战，现有机器学习技术因缺乏专用硬件而难以在现实场景中应用。

Method: 开发DeGrip定制夹爪，提供3个自由度，采用线缆驱动传输机制减小尺寸，设计腕部解耦腕关节和夹爪关节的驱动，并在Isaac Sim中建立拆卸环境进行评估。

Result: 评估结果证实DeGrip能够在狭小空间操作，并能拆卸任意配置的组件，具备废旧台式机拆卸的能力。

Conclusion: DeGrip夹爪通过专用硬件设计解决了机器人拆卸报废产品的实际应用难题，为现实场景中的智能拆卸提供了可行方案。

Abstract: Intelligent robotic disassembly of end-of-life (EOL) products has been a
long-standing challenge in robotics. While machine learning techniques have
shown promise, the lack of specialized hardware limits their application in
real-world scenarios. We introduce DeGrip, a customized gripper designed for
the disassembly of EOL computer desktops. DeGrip provides three degrees of
freedom (DOF), enabling arbitrary configurations within the disassembly
environment when mounted on a robotic manipulator. It employs a cable-driven
transmission mechanism that reduces its overall size and enables operation in
confined spaces. The wrist is designed to decouple the actuation of wrist and
jaw joints. We also developed an EOL desktop disassembly environment in Isaac
Sim to evaluate the effectiveness of DeGrip. The tasks were designed to
demonstrate its ability to operate in confined spaces and disassemble
components in arbitrary configurations. The evaluation results confirm the
capability of DeGrip for EOL desktop disassembly.

</details>


### [3] [Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning](https://arxiv.org/abs/2510.16240)
*Lukas Zbinden,Nigel Nelson,Juo-Tung Chen,Xinhao Chen,Ji Woong,Kim,Mahdi Azizian,Axel Krieger,Sean Huver*

Main category: cs.RO

TL;DR: 本文介绍了Cosmos-Surg-dVRK，一种基于Cosmos世界基础模型的外科手术微调模型，结合视频分类器实现手术策略的自动化在线评估和基准测试，解决了在物理机器人平台上直接评估手术策略的高成本、耗时和可重复性挑战。


<details>
  <summary>Details</summary>
Motivation: 由于在物理机器人平台（如da Vinci Research Kit）上直接评估手术策略存在高成本、时间需求大、可重复性挑战和执行变异性等问题，需要一种高保真度的模拟方法来评估复杂的外科手术任务。

Method: 开发了Cosmos-Surg-dVRK，这是Cosmos世界基础模型的外科手术微调版本，结合训练的视频分类器，构建了完全自动化的在线评估管道。在两个不同的外科数据集上进行评估：桌面缝合垫任务和离体猪胆囊切除术任务。

Result: 在桌面缝合垫任务中，自动化管道在Cosmos-Surg-dVRK中的在线推演与真实dVRK Si平台上的策略结果之间实现了强相关性，人类标注者与V-JEPA 2衍生的视频分类器之间也达成了良好一致性。离体猪胆囊切除术任务的初步实验显示与真实世界评估的潜在一致性。

Conclusion: Cosmos-Surg-dVRK平台展示了在复杂外科手术程序中自动评估手术策略的潜力，为外科机器人策略的高效评估和基准测试提供了可行的解决方案。

Abstract: The rise of surgical robots and vision-language-action models has accelerated
the development of autonomous surgical policies and efficient assessment
strategies. However, evaluating these policies directly on physical robotic
platforms such as the da Vinci Research Kit (dVRK) remains hindered by high
costs, time demands, reproducibility challenges, and variability in execution.
World foundation models (WFM) for physical AI offer a transformative approach
to simulate complex real-world surgical tasks, such as soft tissue deformation,
with high fidelity. This work introduces Cosmos-Surg-dVRK, a surgical finetune
of the Cosmos WFM, which, together with a trained video classifier, enables
fully automated online evaluation and benchmarking of surgical policies. We
evaluate Cosmos-Surg-dVRK using two distinct surgical datasets. On tabletop
suture pad tasks, the automated pipeline achieves strong correlation between
online rollouts in Cosmos-Surg-dVRK and policy outcomes on the real dVRK Si
platform, as well as good agreement between human labelers and the V-JEPA
2-derived video classifier. Additionally, preliminary experiments with ex-vivo
porcine cholecystectomy tasks in Cosmos-Surg-dVRK demonstrate promising
alignment with real-world evaluations, highlighting the platform's potential
for more complex surgical procedures.

</details>


### [4] [NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?](https://arxiv.org/abs/2510.16263)
*Jierui Peng,Yanyan Zhang,Yicheng Duan,Tuo Liang,Vipin Chaudhary,Yu Yin*

Main category: cs.RO

TL;DR: NEBULA是一个用于单臂操作任务的统一评估生态系统，通过细粒度能力测试和系统性压力测试来解决VLA智能体评估中粗粒度指标和碎片化数据的问题。


<details>
  <summary>Details</summary>
Motivation: 当前VLA智能体的评估存在两个主要问题：1）粗粒度的最终任务成功率无法提供精确的技能诊断或测量对现实世界扰动的鲁棒性；2）碎片化的数据环境阻碍了可重复研究和通用模型的发展。

Method: 提出NEBULA生态系统，包含：1）双轴评估协议，结合细粒度能力测试进行精确技能诊断和系统性压力测试测量鲁棒性；2）标准化API和大规模聚合数据集，支持跨数据集训练和公平比较。

Result: 使用NEBULA评估发现，表现最好的VLA智能体在空间推理和动态适应等关键能力方面存在困难，这些缺陷被传统的最终任务成功率指标所掩盖。

Conclusion: NEBULA通过同时测量智能体能够做什么以及在什么情况下可靠地执行，为构建鲁棒、通用的具身智能体提供了实用基础。

Abstract: The evaluation of Vision-Language-Action (VLA) agents is hindered by the
coarse, end-task success metric that fails to provide precise skill diagnosis
or measure robustness to real-world perturbations. This challenge is
exacerbated by a fragmented data landscape that impedes reproducible research
and the development of generalist models. To address these limitations, we
introduce \textbf{NEBULA}, a unified ecosystem for single-arm manipulation that
enables diagnostic and reproducible evaluation. NEBULA features a novel
dual-axis evaluation protocol that combines fine-grained \textit{capability
tests} for precise skill diagnosis with systematic \textit{stress tests} that
measure robustness. A standardized API and a large-scale, aggregated dataset
are provided to reduce fragmentation and support cross-dataset training and
fair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle
with key capabilities such as spatial reasoning and dynamic adaptation, which
are consistently obscured by conventional end-task success metrics. By
measuring both what an agent can do and when it does so reliably, NEBULA
provides a practical foundation for robust, general-purpose embodied agents.

</details>


### [5] [Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification](https://arxiv.org/abs/2510.16281)
*Yilin Wu,Anqi Li,Tucker Hermans,Fabio Ramos,Andrea Bajcsy,Claudia P'erez-D'Arpino*

Main category: cs.RO

TL;DR: 提出了一种无需训练的运行时策略引导方法，通过模拟候选动作序列并选择与文本计划最一致的结果，提升推理VLA模型在分布外场景下的动作执行忠实度。


<details>
  <summary>Details</summary>
Motivation: 现有推理VLA模型虽然能生成正确的文本计划，但在执行动作时仍可能偏离预期结果，特别是在分布外场景下。这体现了推理与动作执行之间的忠实度不足问题。

Method: 从同一模型中采样多个候选动作序列，通过模拟预测其执行结果，使用预训练的视觉语言模型选择与文本计划最一致的动作序列执行。

Result: 在行为组合任务上比先前工作提升15%性能，能够有效应对语义和视觉分布外扰动，且性能随计算资源和数据多样性而扩展。

Conclusion: 将基础VLA的自然动作多样性从错误来源转变为优势，无需成本高昂的重新训练即可提升鲁棒性并实现新颖行为组合。

Abstract: Reasoning Vision Language Action (VLA) models improve robotic
instruction-following by generating step-by-step textual plans before low-level
actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language
models. Yet even with a correct textual plan, the generated actions can still
miss the intended outcomes in the plan, especially in out-of-distribution (OOD)
scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness,
and introduce a training-free, runtime policy steering method for
reasoning-action alignment. Given a reasoning VLA's intermediate textual plan,
our framework samples multiple candidate action sequences from the same model,
predicts their outcomes via simulation, and uses a pre-trained Vision-Language
Model (VLM) to select the sequence whose outcome best aligns with the VLA's own
textual plan. Only executing action sequences that align with the textual
reasoning turns our base VLA's natural action diversity from a source of error
into a strength, boosting robustness to semantic and visual OOD perturbations
and enabling novel behavior composition without costly re-training. We also
contribute a reasoning-annotated extension of LIBERO-100, environment
variations tailored for OOD evaluation, and demonstrate up to 15% performance
gain over prior work on behavior composition tasks and scales with compute and
data diversity. Project Website at:
https://yilin-wu98.github.io/steering-reasoning-vla/

</details>


### [6] [SPOT: Sensing-augmented Trajectory Planning via Obstacle Threat Modeling](https://arxiv.org/abs/2510.16308)
*Chi Zhang,Xian Huang,Wei Dong*

Main category: cs.RO

TL;DR: SPOT是一个统一的感知增强规划框架，通过障碍物威胁建模将感知目标明确整合到运动优化中，解决了单深度相机无人机在动态障碍物避障中的视野限制和盲区问题。


<details>
  <summary>Details</summary>
Motivation: 配备单深度相机的无人机由于视野有限和不可避免的盲区，在动态障碍物避障方面面临重大挑战。现有方法将运动规划与感知考虑分离，导致障碍物响应效果不佳且延迟。

Method: 基于高斯过程的障碍物信念地图建立统一概率表示，通过碰撞感知推理机制将空间不确定性和轨迹接近度转化为时变观测紧急度地图，并在当前视野内整合紧急度值定义可微分目标。

Result: 在动态、杂乱和遮挡环境中的仿真和真实世界实验表明，该方法比基线方法提前2.8秒检测到潜在动态障碍物，动态障碍物可见性提高超过500%，能够在杂乱、遮挡环境中安全导航。

Conclusion: SPOT框架通过将感知目标整合到运动优化中，实现了实时、感知感知的轨迹规划，计算时间低于10毫秒，显著提高了动态障碍物检测和避障性能。

Abstract: UAVs equipped with a single depth camera encounter significant challenges in
dynamic obstacle avoidance due to limited field of view and inevitable blind
spots. While active vision strategies that steer onboard cameras have been
proposed to expand sensing coverage, most existing methods separate motion
planning from sensing considerations, resulting in less effective and delayed
obstacle response. To address this limitation, we introduce SPOT
(Sensing-augmented Planning via Obstacle Threat modeling), a unified planning
framework for observation-aware trajectory planning that explicitly
incorporates sensing objectives into motion optimization. At the core of our
method is a Gaussian Process-based obstacle belief map, which establishes a
unified probabilistic representation of both recognized (previously observed)
and potential obstacles. This belief is further processed through a
collision-aware inference mechanism that transforms spatial uncertainty and
trajectory proximity into a time-varying observation urgency map. By
integrating urgency values within the current field of view, we define
differentiable objectives that enable real-time, observation-aware trajectory
planning with computation times under 10 ms. Simulation and real-world
experiments in dynamic, cluttered, and occluded environments show that our
method detects potential dynamic obstacles 2.8 seconds earlier than baseline
approaches, increasing dynamic obstacle visibility by over 500\%, and enabling
safe navigation through cluttered, occluded environments.

</details>


### [7] [Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models](https://arxiv.org/abs/2510.16344)
*Chenrui Tie,Shengxiang Sun,Yudi Lin,Yanbo Wang,Zhongrui Li,Zhouhan Zhong,Jinxuan Zhu,Yiman Pang,Haonan Chen,Junting Chen,Ruihai Wu,Lin Shao*

Main category: cs.RO

TL;DR: 提出Manual2Skill++框架，将连接关系作为装配任务的一等公民，从装配手册中自动提取结构化连接信息，构建层次化图表示，并在仿真环境中验证了从任务理解到执行的完整流程。


<details>
  <summary>Details</summary>
Motivation: 传统机器人装配方法将连接器视为次要考虑，而连接实际上是决定装配成败的关键。本文主张将连接关系作为装配表示的核心要素。

Method: 使用大规模视觉语言模型解析装配手册中的符号图和注释，构建层次化图表示（节点为零件和子装配体，边为组件间的连接关系），并开发了Manual2Skill++框架自动提取结构化连接信息。

Result: 在包含20多个装配任务的数据集上验证了表示提取方法，并在仿真环境中评估了四个复杂装配场景（家具、玩具、制造组件）的完整任务理解到执行流程。

Conclusion: 将连接关系作为一等公民的装配表示方法能够有效提升机器人装配的可靠性和成功率，Manual2Skill++框架为从人类设计的装配手册中提取连接知识提供了可行方案。

Abstract: Assembly hinges on reliably forming connections between parts; yet most
robotic approaches plan assembly sequences and part poses while treating
connectors as an afterthought. Connections represent the critical "last mile"
of assembly execution, while task planning may sequence operations and motion
plan may position parts, the precise establishment of physical connections
ultimately determines assembly success or failure. In this paper, we consider
connections as first-class primitives in assembly representation, including
connector types, specifications, quantities, and placement locations. Drawing
inspiration from how humans learn assembly tasks through step-by-step
instruction manuals, we present Manual2Skill++, a vision-language framework
that automatically extracts structured connection information from assembly
manuals. We encode assembly tasks as hierarchical graphs where nodes represent
parts and sub-assemblies, and edges explicitly model connection relationships
between components. A large-scale vision-language model parses symbolic
diagrams and annotations in manuals to instantiate these graphs, leveraging the
rich connection knowledge embedded in human-designed instructions. We curate a
dataset containing over 20 assembly tasks with diverse connector types to
validate our representation extraction approach, and evaluate the complete task
understanding-to-execution pipeline across four complex assembly scenarios in
simulation, spanning furniture, toys, and manufacturing components with
real-world correspondence.

</details>


### [8] [Learning to Optimize Edge Robotics: A Fast Integrated Perception-Motion-Communication Approach](https://arxiv.org/abs/2510.16424)
*Dan Guo,Xibin Jin,Shuai Wang,Zhigang Wen,Miaowen Wen,Chengzhong Xu*

Main category: cs.RO

TL;DR: 该论文提出了集成感知、运动和通信(IPMC)的边缘机器人系统，通过模仿学习神经网络动态调整通信策略，显著降低通信开销和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了机器人功能与通信条件之间的相互依赖关系，导致通信开销过大。需要一种能够动态适应通信策略的系统来减少传感器数据上传需求。

Method: 采用学习优化(LTO)范式，设计并实现模仿学习神经网络，使机器人能够根据感知和运动动态知识动态调整通信策略（压缩比、传输频率、发射功率）。

Result: 实验证明IPMC系统优于现有方法，LTO方法比最优化求解器的计算复杂度降低了10倍以上，具备实时执行能力。

Conclusion: IPMC系统通过集成感知、运动和通信，结合LTO方法，有效解决了边缘机器人通信开销问题，实现了高效的实时通信策略优化。

Abstract: Edge robotics involves frequent exchanges of large-volume multi-modal data.
Existing methods ignore the interdependency between robotic functionalities and
communication conditions, leading to excessive communication overhead. This
paper revolutionizes edge robotics systems through integrated perception,
motion, and communication (IPMC). As such, robots can dynamically adapt their
communication strategies (i.e., compression ratio, transmission frequency,
transmit power) by leveraging the knowledge of robotic perception and motion
dynamics, thus reducing the need for excessive sensor data uploads.
Furthermore, by leveraging the learning to optimize (LTO) paradigm, an
imitation learning neural network is designed and implemented, which reduces
the computational complexity by over 10x compared to state-of-the art
optimization solvers. Experiments demonstrate the superiority of the proposed
IPMC and the real-time execution capability of LTO.

</details>


### [9] [What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics](https://arxiv.org/abs/2510.16435)
*Lennart Wachowiak,Andrew Coles,Gerard Canal,Oya Celiktutan*

Main category: cs.RO

TL;DR: 该研究收集了1,893个家庭机器人用户问题数据集，涵盖12个类别和70个子类别，揭示了用户最关心的问题类型及其重要性排序，为机器人对话界面设计提供重要参考。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和对话界面在人类-机器人交互中的广泛应用，机器人回答用户问题的能力变得至关重要。现有可解释机器人研究主要关注"为什么"问题，缺乏对用户实际提问多样性的理解。

Method: 通过创建15个视频刺激和7个文本刺激，展示机器人执行各种家庭任务的情景，在Prolific平台上收集100名参与者的问题，最终形成包含1,893个问题的数据集。

Result: 最常见的问题类别是任务执行细节(22.5%)、机器人能力(12.7%)和性能评估(11.3%)。虽然关于机器人处理困难场景和确保正确行为的问题较少，但用户认为这些是最重要的。新手用户倾向于询问简单事实性问题。

Conclusion: 该数据集为识别机器人需要记录和暴露的信息、基准测试问答模块以及设计符合用户期望的解释策略提供了宝贵基础，有助于推动家庭机器人对话能力的发展。

Abstract: With the growing use of large language models and conversational interfaces
in human-robot interaction, robots' ability to answer user questions is more
important than ever. We therefore introduce a dataset of 1,893 user questions
for household robots, collected from 100 participants and organized into 12
categories and 70 subcategories. Most work in explainable robotics focuses on
why-questions. In contrast, our dataset provides a wide variety of questions,
from questions about simple execution details to questions about how the robot
would act in hypothetical scenarios -- thus giving roboticists valuable
insights into what questions their robot needs to be able to answer. To collect
the dataset, we created 15 video stimuli and 7 text stimuli, depicting robots
performing varied household tasks. We then asked participants on Prolific what
questions they would want to ask the robot in each portrayed situation. In the
final dataset, the most frequent categories are questions about task execution
details (22.5%), the robot's capabilities (12.7%), and performance assessments
(11.3%). Although questions about how robots would handle potentially difficult
scenarios and ensure correct behavior are less frequent, users rank them as the
most important for robots to be able to answer. Moreover, we find that users
who identify as novices in robotics ask different questions than more
experienced users. Novices are more likely to inquire about simple facts, such
as what the robot did or the current state of the environment. As robots enter
environments shared with humans and language becomes central to giving
instructions and interaction, this dataset provides a valuable foundation for
(i) identifying the information robots need to log and expose to conversational
interfaces, (ii) benchmarking question-answering modules, and (iii) designing
explanation strategies that align with user expectations.

</details>


### [10] [Advancing Off-Road Autonomous Driving: The Large-Scale ORAD-3D Dataset and Comprehensive Benchmarks](https://arxiv.org/abs/2510.16500)
*Chen Min,Jilin Mei,Heng Zhai,Shuai Wang,Tong Sun,Fanjie Kong,Haoyang Li,Fangyuan Mao,Fuyang Liu,Shuo Wang,Yiming Nie,Qi Zhu,Liang Xiao,Dawei Zhao,Yu Hu*

Main category: cs.RO

TL;DR: ORAD-3D是目前最大的越野自动驾驶数据集，涵盖多种地形和环境条件，并建立了包含5个核心任务的基准评估体系


<details>
  <summary>Details</summary>
Motivation: 越野自动驾驶研究面临大规模高质量数据集稀缺的瓶颈，需要填补这一空白

Method: 构建ORAD-3D数据集，覆盖林地、农田、草地、河岸、碎石路、水泥路和农村地区等多种地形，并捕捉不同天气和光照条件的变化

Result: 建立了包含2D自由空间检测、3D占据预测、粗略GPS引导路径规划、视觉语言模型驱动自动驾驶和越野环境世界模型等5个任务的综合基准评估

Conclusion: 该数据集和基准为推进挑战性越野场景下的感知和规划提供了统一且强大的资源

Abstract: A major bottleneck in off-road autonomous driving research lies in the
scarcity of large-scale, high-quality datasets and benchmarks. To bridge this
gap, we present ORAD-3D, which, to the best of our knowledge, is the largest
dataset specifically curated for off-road autonomous driving. ORAD-3D covers a
wide spectrum of terrains, including woodlands, farmlands, grasslands,
riversides, gravel roads, cement roads, and rural areas, while capturing
diverse environmental variations across weather conditions (sunny, rainy,
foggy, and snowy) and illumination levels (bright daylight, daytime, twilight,
and nighttime). Building upon this dataset, we establish a comprehensive suite
of benchmark evaluations spanning five fundamental tasks: 2D free-space
detection, 3D occupancy prediction, rough GPS-guided path planning,
vision-language model-driven autonomous driving, and world model for off-road
environments. Together, the dataset and benchmarks provide a unified and robust
resource for advancing perception and planning in challenging off-road
scenarios. The dataset and code will be made publicly available at
https://github.com/chaytonmin/ORAD-3D.

</details>


### [11] [A Novel Gripper with Semi-Peaucellier Linkage and Idle-Stroke Mechanism for Linear Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.16517)
*Haokai Ding,Wenzeng Zhang*

Main category: cs.RO

TL;DR: SPD夹爪采用线性平行夹持机制，解决了传统夹爪弧形运动需要调整机械臂高度的问题，具有自适应能力，能够抓取不同形状和大小的物体。


<details>
  <summary>Details</summary>
Motivation: 传统工业夹爪的指尖呈弧形运动，需要整个机械臂调整高度以避免与桌面碰撞，这限制了抓取效率。SPD夹爪旨在通过线性运动轨迹解决这一问题。

Method: 设计具有手掌和两个机械相同、对称排列手指的夹爪，指尖遵循线性运动轨迹，可独立驱动或由单个电机驱动，并进行了优化分析理论研究和原型开发测试。

Result: 实验结果表明，SPD夹爪成功实现了线性平行夹持功能，并表现出良好的适应性，能够有效抓取桌面上的各种物体。

Conclusion: SPD夹爪为各种机器人提供了有效的抓取解决方案，为收集数据以增强深度学习训练奠定了坚实基础，在具身智能技术发展中具有重要应用价值。

Abstract: This paper introduces a novel robotic gripper, named as the SPD gripper. It
features a palm and two mechanically identical and symmetrically arranged
fingers, which can be driven independently or by a single motor. The fingertips
of the fingers follow a linear motion trajectory, facilitating the grasping of
objects of various sizes on a tabletop without the need to adjust the overall
height of the gripper. Traditional industrial grippers with parallel gripping
capabilities often exhibit an arcuate motion at the fingertips, requiring the
entire robotic arm to adjust its height to avoid collisions with the tabletop.
The SPD gripper, with its linear parallel gripping mechanism, effectively
addresses this issue. Furthermore, the SPD gripper possesses adaptive
capabilities, accommodating objects of different shapes and sizes. This paper
presents the design philosophy, fundamental composition principles, and
optimization analysis theory of the SPD gripper. Based on the design theory, a
robotic gripper prototype was developed and tested. The experimental results
demonstrate that the robotic gripper successfully achieves linear parallel
gripping functionality and exhibits good adaptability. In the context of the
ongoing development of embodied intelligence technologies, this robotic gripper
can assist various robots in achieving effective grasping, laying a solid
foundation for collecting data to enhance deep learning training.

</details>


### [12] [DIV-Nav: Open-Vocabulary Spatial Relationships for Multi-Object Navigation](https://arxiv.org/abs/2510.16518)
*Jesús Ortega-Peimbert,Finn Lukas Busch,Timon Homberger,Quantao Yang,Olov Andersson*

Main category: cs.RO

TL;DR: DIV-Nav是一个实时导航系统，能够处理包含空间关系的复杂自由文本查询，通过分解指令、计算语义信念图交集和验证空间约束来实现高效导航。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本物体导航通常只支持简单查询（如"电视"或"蓝色地毯"），但无法处理包含空间关系的复杂自由文本查询（如"在桌子上找遥控器"）。

Method: 通过三个步骤：i) 将复杂空间约束的自然语言指令分解为语义图上的简单对象查询；ii) 计算个体语义信念图的交集以识别所有对象共存的区域；iii) 使用LVLM验证发现的对象是否符合原始复杂空间约束。

Result: 在MultiON基准测试和波士顿动力Spot机器人上的真实世界部署中进行了广泛实验验证。

Conclusion: DIV-Nav系统能够有效处理复杂空间关系的导航查询，并通过适应在线语义映射的前沿探索目标来更有效地指导搜索过程。

Abstract: Advances in open-vocabulary semantic mapping and object navigation have
enabled robots to perform an informed search of their environment for an
arbitrary object. However, such zero-shot object navigation is typically
designed for simple queries with an object name like "television" or "blue
rug". Here, we consider more complex free-text queries with spatial
relationships, such as "find the remote on the table" while still leveraging
robustness of a semantic map. We present DIV-Nav, a real-time navigation system
that efficiently addresses this problem through a series of relaxations: i)
Decomposing natural language instructions with complex spatial constraints into
simpler object-level queries on a semantic map, ii) computing the Intersection
of individual semantic belief maps to identify regions where all objects
co-exist, and iii) Validating the discovered objects against the original,
complex spatial constrains via a LVLM. We further investigate how to adapt the
frontier exploration objectives of online semantic mapping to such spatial
search queries to more effectively guide the search process. We validate our
system through extensive experiments on the MultiON benchmark and real-world
deployment on a Boston Dynamics Spot robot using a Jetson Orin AGX. More
details and videos are available at https://anonsub42.github.io/reponame/

</details>


### [13] [Semi-Peaucellier Linkage and Differential Mechanism for Linear Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.16524)
*Haokai Ding,Zhaohan Chen,Tao Yang,Wenzeng Zhang*

Main category: cs.RO

TL;DR: SP-Diff平行夹爪系统采用创新的差动连杆机构和模块化对称双指配置，实现线性平行抓取，通过行星齿轮传动实现同步线性运动和独立手指姿态调整，减少Z轴重新校准需求30%。


<details>
  <summary>Details</summary>
Motivation: 解决传统末端执行器在智能工业自动化中适应性有限的问题，开发能够适应多样化工业工件和可变形物体的自适应抓取系统。

Method: 采用差动连杆机构、模块化对称双指配置、行星齿轮传动、运动学优化的平行四边形连杆和差动机构，集成未来就绪接口用于力/视觉传感器集成。

Result: 系统展示了自适应抓取能力，适用于各种工业工件和可变形物体（如柑橘类水果），Z轴重新校准需求比弧形轨迹夹爪减少30%。

Conclusion: SP-Diff通过其自适应架构推进了机器人末端执行器的智能化，在协作机器人、物流自动化和专业操作场景中具有广阔应用前景。

Abstract: This paper presents the SP-Diff parallel gripper system, addressing the
limited adaptability of conventional end-effectors in intelligent industrial
automation. The proposed design employs an innovative differential linkage
mechanism with a modular symmetric dual-finger configuration to achieve
linear-parallel grasping. By integrating a planetary gear transmission, the
system enables synchronized linear motion and independent finger pose
adjustment while maintaining structural rigidity, reducing Z-axis recalibration
requirements by 30% compared to arc-trajectory grippers. The compact palm
architecture incorporates a kinematically optimized parallelogram linkage and
Differential mechanism, demonstrating adaptive grasping capabilities for
diverse industrial workpieces and deformable objects such as citrus fruits.
Future-ready interfaces are embedded for potential force/vision sensor
integration to facilitate multimodal data acquisition (e.g., trajectory
planning and object deformation) in digital twin frameworks. Designed as a
flexible manufacturing solution, SP-Diff advances robotic end-effector
intelligence through its adaptive architecture, showing promising applications
in collaborative robotics, logistics automation, and specialized operational
scenarios.

</details>


### [14] [MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation](https://arxiv.org/abs/2510.16617)
*Ruihan Zhao,Tyler Ingebrand,Sandeep Chinchali,Ufuk Topcu*

Main category: cs.RO

TL;DR: MoS-VLA是一个将机器人操作策略表示为有限学习基函数线性组合的框架，通过轻量级凸优化实现单次演示的快速技能适应，在未见数据集上表现优于预训练VLA模型。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在新环境、新任务中往往直接失败，需要能够快速适应新场景的通用机器人控制方法。

Method: 在预训练阶段联合学习基函数构建结构化技能空间，测试时仅需单次专家演示，通过L1动作误差最小化的凸优化推断技能表示，无需梯度更新。

Result: 在五个未见数据集上获得更低的动作预测误差，在仿真和真实机器人任务中成功完成预训练VLA模型失败的任务。

Conclusion: MoS-VLA通过技能混合和轻量级适应实现了跨域机器人控制的快速泛化，为通用机器人学习提供了有效框架。

Abstract: Vision-Language-Action (VLA) models trained on large robot datasets promise
general-purpose, robust control across diverse domains and embodiments.
However, existing approaches often fail out-of-the-box when deployed in novel
environments, embodiments, or tasks. We introduce Mixture of Skills VLA
(MoS-VLA), a framework that represents robot manipulation policies as linear
combinations of a finite set of learned basis functions. During pretraining,
MoS-VLA jointly learns these basis functions across datasets from the Open
X-Embodiment project, producing a structured skill space. At test time,
adapting to a new task requires only a single expert demonstration. The
corresponding skill representation is then inferred via a lightweight convex
optimization problem that minimizes the L1 action error, without requiring
gradient updates. This gradient-free adaptation incurs minimal overhead while
enabling rapid instantiation of new skills. Empirically, MoS-VLA achieves lower
action-prediction error on five out of five unseen datasets and succeeds in
both simulation and real-robot tasks where a pretrained VLA model fails
outright. Project page: mos-vla.github.io/

</details>


### [15] [First Responders' Perceptions of Semantic Information for Situational Awareness in Robot-Assisted Emergency Response](https://arxiv.org/abs/2510.16692)
*Tianshu Ruan,Zoe Betta,Georgios Tzoumas,Rustam Stolkin,Manolis Chiou*

Main category: cs.RO

TL;DR: 本研究调查了急救人员对在紧急行动中使用语义信息和态势感知的机器人系统的态度。结果显示急救人员对机器人持积极态度，重视语义信息在构建态势感知和预测突发事件中的作用，并愿意使用准确率约70-75%的不完美AI支持工具。


<details>
  <summary>Details</summary>
Motivation: 了解急救人员对语义增强态势感知机器人系统的态度和需求，填补该领域跨国调查的空白，促进开发更符合用户需求的应急响应机器人系统。

Method: 对来自8个国家的22名急救人员进行了结构化问卷调查，收集了人口统计信息、对机器人的总体态度以及语义增强态势感知的体验数据。

Result: 大多数急救人员对机器人持积极态度，语义信息对构建态势感知的有用性评分为3.6/5，对预测突发事件的价值评分为3.9/5。参与者要求语义输出准确率达到74.6%才可信，67.8%才被认为有用。

Conclusion: 研究揭示了急救人员最重视的语义信息类型（物体识别、空间关系、风险背景），并暴露了实验室机器人能力与现场部署现实之间的关键差距，强调了急救人员与机器人研究人员之间更有意义的合作的必要性。

Abstract: This study investigates First Responders' (FRs) attitudes toward the use of
semantic information and Situational Awareness (SA) in robotic systems during
emergency operations. A structured questionnaire was administered to 22 FRs
across eight countries, capturing their demographic profiles, general attitudes
toward robots, and experiences with semantics-enhanced SA. Results show that
most FRs expressed positive attitudes toward robots, and rated the usefulness
of semantic information for building SA at an average of 3.6 out of 5. Semantic
information was also valued for its role in predicting unforeseen emergencies
(mean 3.9). Participants reported requiring an average of 74.6\% accuracy to
trust semantic outputs and 67.8\% for them to be considered useful, revealing a
willingness to use imperfect but informative AI support tools.
  To the best of our knowledge, this study offers novel insights by being one
of the first to directly survey FRs on semantic-based SA in a cross-national
context. It reveals the types of semantic information most valued in the field,
such as object identity, spatial relationships, and risk context-and connects
these preferences to the respondents' roles, experience, and education levels.
The findings also expose a critical gap between lab-based robotics capabilities
and the realities of field deployment, highlighting the need for more
meaningful collaboration between FRs and robotics researchers. These insights
contribute to the development of more user-aligned and situationally aware
robotic systems for emergency response.

</details>


### [16] [Towards Active Excitation-Based Dynamic Inertia Identification in Satellites](https://arxiv.org/abs/2510.16738)
*Matteo El-Hariry,Vittorio Franzese,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: 该论文分析了激励设计对刚体纳卫星和微卫星惯性参数识别的影响，比较了最小二乘法和扩展卡尔曼滤波器在不同激励条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 研究激励设计如何影响卫星惯性参数的识别精度，为在轨自适应惯性识别提供实用指导。

Method: 模拟非线性姿态动力学，考虑反作用轮耦合、执行器限制和外部扰动，使用八种不同频谱丰富度的扭矩剖面激励系统，比较批处理最小二乘法和扩展卡尔曼滤波器。

Result: 结果表明，激励频率内容和估计器假设共同决定了估计精度和鲁棒性，明确了每种方法表现最佳的条件。

Conclusion: 为在轨自适应惯性识别提供了实用指导，代码已开源提供。

Abstract: This paper presents a comprehensive analysis of how excitation design
influences the identification of the inertia properties of rigid nano- and
micro-satellites. We simulate nonlinear attitude dynamics with reaction-wheel
coupling, actuator limits, and external disturbances, and excite the system
using eight torque profiles of varying spectral richness. Two estimators are
compared, a batch Least Squares method and an Extended Kalman Filter, across
three satellite configurations and time-varying inertia scenarios. Results show
that excitation frequency content and estimator assumptions jointly determine
estimation accuracy and robustness, offering practical guidance for in-orbit
adaptive inertia identification by outlining the conditions under which each
method performs best. The code is provided as open-source .

</details>


### [17] [Adaptive Invariant Extended Kalman Filter for Legged Robot State Estimation](https://arxiv.org/abs/2510.16755)
*Kyung-Hwan Kim,DongHyun Ahn,Dong-hyun Lee,JuYoung Yoon,Dong Jin Hyun*

Main category: cs.RO

TL;DR: 提出自适应不变扩展卡尔曼滤波器，通过在线协方差估计自适应调整接触足模型噪声水平，提高腿式机器人在变化接触条件下的状态估计性能


<details>
  <summary>Details</summary>
Motivation: 状态估计对腿式机器人至关重要，直接影响控制性能和运动稳定性。传统方法难以处理小滑动，且过于敏感的滑动拒绝设置可能导致滤波器发散

Method: 使用自适应不变扩展卡尔曼滤波器，基于在线协方差估计调整接触足模型噪声水平；采用接触检测算法而非接触传感器，减少对额外硬件的依赖

Result: 在四足机器人LeoQuad上进行的真实世界实验验证了该方法在动态运动场景中具有增强的状态估计性能

Conclusion: 该方法能有效处理传统滑动拒绝方法无法解决的小滑动问题，在变化接触条件下显著改善腿式机器人的状态估计

Abstract: State estimation is crucial for legged robots as it directly affects control
performance and locomotion stability. In this paper, we propose an Adaptive
Invariant Extended Kalman Filter to improve proprioceptive state estimation for
legged robots. The proposed method adaptively adjusts the noise level of the
contact foot model based on online covariance estimation, leading to improved
state estimation under varying contact conditions. It effectively handles small
slips that traditional slip rejection fails to address, as overly sensitive
slip rejection settings risk causing filter divergence. Our approach employs a
contact detection algorithm instead of contact sensors, reducing the reliance
on additional hardware. The proposed method is validated through real-world
experiments on the quadruped robot LeoQuad, demonstrating enhanced state
estimation performance in dynamic locomotion scenarios.

</details>


### [18] [T3 Planner: A Self-Correcting LLM Framework for Robotic Motion Planning with Temporal Logic](https://arxiv.org/abs/2510.16767)
*Jia Li,Guoxiang Zhao*

Main category: cs.RO

TL;DR: T3 Planner是一个基于大语言模型的机器人运动规划框架，通过三个级联模块分解时空任务约束，使用STL验证器自我修正输出，生成满足复杂约束的可行运动轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖领域专业知识定制规划器，难以处理时空耦合问题，常导致不可行运动或任务规划与运动执行不一致。大语言模型虽然擅长高层语义推理，但会产生幻觉导致不可行运动规划。

Method: 通过三个级联模块分解时空任务约束，每个模块刺激LLM生成候选轨迹序列，使用信号时序逻辑（STL）验证器检查可行性，直到找到满足复杂空间、时间和逻辑约束的轨迹。

Result: 在不同场景下的实验表明，T3 Planner显著优于基线方法。所需的推理能力可以蒸馏到轻量级的Qwen3-4B模型中，实现高效部署。

Conclusion: T3 Planner提供了一个有效的LLM赋能机器人运动规划框架，能够自我修正输出，解决传统方法的局限性，并通过知识蒸馏实现高效部署。

Abstract: Translating natural language instructions into executable motion plans is a
fundamental challenge in robotics. Traditional approaches are typically
constrained by their reliance on domain-specific expertise to customize
planners, and often struggle with spatio-temporal couplings that usually lead
to infeasible motions or discrepancies between task planning and motion
execution. Despite the proficiency of Large Language Models (LLMs) in
high-level semantic reasoning, hallucination could result in infeasible motion
plans. In this paper, we introduce the T3 Planner, an LLM-enabled robotic
motion planning framework that self-corrects it output with formal methods. The
framework decomposes spatio-temporal task constraints via three cascaded
modules, each of which stimulates an LLM to generate candidate trajectory
sequences and examines their feasibility via a Signal Temporal Logic (STL)
verifier until one that satisfies complex spatial, temporal, and logical
constraints is found.Experiments across different scenarios show that T3
Planner significantly outperforms the baselines. The required reasoning can be
distilled into a lightweight Qwen3-4B model that enables efficient deployment.
All supplementary materials are accessible at
https://github.com/leeejia/T3_Planner.

</details>


### [19] [A Preliminary Exploration of the Differences and Conjunction of Traditional PNT and Brain-inspired PNT](https://arxiv.org/abs/2510.16771)
*Xu He,Xiaolin Meng,Wenxuan Yin,Youdong Zhang,Lingfei Mo,Xiangdong An,Fangwen Yu,Shuguo Pan,Yufeng Liu,Jingnan Liu,Yujia Zhang,Wang Gao*

Main category: cs.RO

TL;DR: 提出从"工具导向"转向"认知驱动"的PNT发展新视角和路线图，通过融合机器PNT的高精度与脑启发空间认知导航，开发更鲁棒、节能和智能的通用定位导航授时系统。


<details>
  <summary>Details</summary>
Motivation: 当前复杂环境需要更鲁棒、节能和认知能力更强的PNT系统，目标是赋予无人系统脑启发的空间认知导航能力，同时利用机器PNT的高精度来推进通用PNT发展。

Method: 提出四层（观测-能力-决策-硬件）融合框架，将数值精度与脑启发智能相结合；对传统PNT、生物脑PNT和脑启发PNT进行多层次差异分析。

Result: 建立了认知驱动的PNT新范式，提供了融合机器精度与生物智能的系统框架，为开发更先进的PNT系统奠定了基础。

Conclusion: 脑启发PNT代表了PNT发展的未来方向，通过认知驱动的方法可以实现更鲁棒、节能和智能的定位导航授时系统，为无人系统提供类似生物的空间认知能力。

Abstract: Developing universal Positioning, Navigation, and Timing (PNT) is our
enduring goal. Today's complex environments demand PNT that is more resilient,
energy-efficient and cognitively capable. This paper asks how we can endow
unmanned systems with brain-inspired spatial cognition navigation while
exploiting the high precision of machine PNT to advance universal PNT. We
provide a new perspective and roadmap for shifting PNT from "tool-oriented" to
"cognition-driven". Contributions: (1) multi-level dissection of differences
among traditional PNT, biological brain PNT and brain-inspired PNT; (2) a
four-layer (observation-capability-decision-hardware) fusion framework that
unites numerical precision and brain-inspired intelligence; (3) forward-looking
recommendations for future development of brain-inspired PNT.

</details>


### [20] [C-Free-Uniform: A Map-Conditioned Trajectory Sampler for Model Predictive Path Integral Control](https://arxiv.org/abs/2510.16905)
*Yukang Cao,Rahul Moorthy,O. Goktug Poyrazoglu,Volkan Isler*

Main category: cs.RO

TL;DR: 提出C-Free-Uniform轨迹采样方法，结合局部环境信息生成控制输入分布，实现自由配置空间的均匀采样，并集成到MPPI控制器中，在复杂导航任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统轨迹采样方法生成的控制输入分布独立于环境，无法充分利用局部环境信息，限制了在复杂环境中的导航性能。

Method: 引入C-Free-Uniform概念，生成基于当前局部地图的控制输入分布，实现自由配置空间的均匀采样，并将其集成到MPPI控制器中形成CFU-MPPI。

Result: 在复杂多边形环境中的导航任务中，CFU-MPPI相比现有方法具有更高的成功率，且所需采样预算显著减少。

Conclusion: C-Free-Uniform方法通过环境感知的轨迹采样机制，有效提升了基于采样的控制方法在复杂导航任务中的性能。

Abstract: Trajectory sampling is a key component of sampling-based control mechanisms.
Trajectory samplers rely on control input samplers, which generate control
inputs u from a distribution p(u | x) where x is the current state. We
introduce the notion of Free Configuration Space Uniformity (C-Free-Uniform for
short) which has two key features: (i) it generates a control input
distribution so as to uniformly sample the free configuration space, and (ii)
in contrast to previously introduced trajectory sampling mechanisms where the
distribution p(u | x) is independent of the environment, C-Free-Uniform is
explicitly conditioned on the current local map. Next, we integrate this
sampler into a new Model Predictive Path Integral (MPPI) Controller, CFU-MPPI.
Experiments show that CFU-MPPI outperforms existing methods in terms of success
rate in challenging navigation tasks in cluttered polygonal environments while
requiring a much smaller sampling budget.

</details>


### [21] [Design of an Affordable, Fully-Actuated Biomimetic Hand for Dexterous Teleoperation Systems](https://arxiv.org/abs/2510.16931)
*Zhaoliang Wan,Zida Zhou,Zetong Bi,Zehui Yang,Hao Ding,Hui Cheng*

Main category: cs.RO

TL;DR: RAPID Hand是首个低成本、20自由度的灵巧手，采用创新的拟人化驱动和传动方案，通过3D打印部件和定制齿轮实现经济性，在灵巧遥操作任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决灵巧遥操作中缺乏经济实惠的五指灵巧手的问题，这对于在"从演示中学习"范式中收集大规模真实机器人数据至关重要。

Method: 采用新型拟人化驱动和传动方案，包括非拇指手指的通用指骨传动方案和全向拇指驱动机制，使用3D打印部件结合定制齿轮以降低成本并便于更换维修。

Result: 在灵巧遥操作系统中通过定量指标和定性测试评估，在多项挑战性任务中表现良好，包括多指抓取、勺子操作和类人钢琴演奏。

Conclusion: RAPID Hand的完全驱动20自由度设计在灵巧遥操作方面具有显著潜力。

Abstract: This paper addresses the scarcity of affordable, fully-actuated five-fingered
hands for dexterous teleoperation, which is crucial for collecting large-scale
real-robot data within the "Learning from Demonstrations" paradigm. We
introduce the prototype version of the RAPID Hand, the first low-cost,
20-degree-of-actuation (DoA) dexterous hand that integrates a novel
anthropomorphic actuation and transmission scheme with an optimized motor
layout and structural design to enhance dexterity. Specifically, the RAPID Hand
features a universal phalangeal transmission scheme for the non-thumb fingers
and an omnidirectional thumb actuation mechanism. Prioritizing affordability,
the hand employs 3D-printed parts combined with custom gears for easier
replacement and repair. We assess the RAPID Hand's performance through
quantitative metrics and qualitative testing in a dexterous teleoperation
system, which is evaluated on three challenging tasks: multi-finger retrieval,
ladle handling, and human-like piano playing. The results indicate that the
RAPID Hand's fully actuated 20-DoF design holds significant promise for
dexterous teleoperation.

</details>


### [22] [DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation](https://arxiv.org/abs/2510.17038)
*Pedram Fekri,Majid Roshanfar,Samuel Barbeau,Seyedfarzad Famouri,Thomas Looi,Dale Podolsky,Mehrdad Zadeh,Javad Dargahi*

Main category: cs.RO

TL;DR: DINO-CVA是一个多模态目标条件行为克隆框架，用于实现自主导管导航，融合视觉观察和操纵杆运动学，减少对操作员的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有导管导航系统主要依赖手动操作，导致操作员疲劳、辐射暴露增加和结果变异性。需要开发智能自主系统来改善这些问题。

Method: 提出DINO-CVA框架，将视觉观察和操纵杆运动学融合到联合嵌入空间，通过专家演示自回归预测动作，使用目标条件指导导航到指定目的地。

Result: DINO-CVA在预测动作方面达到高精度，与仅使用运动学的基线性能相当，同时将预测基于解剖环境。

Conclusion: 多模态目标条件架构在导管导航中具有可行性，是减少操作员依赖性和提高导管治疗可靠性的重要一步。

Abstract: Cardiac catheterization remains a cornerstone of minimally invasive
interventions, yet it continues to rely heavily on manual operation. Despite
advances in robotic platforms, existing systems are predominantly follow-leader
in nature, requiring continuous physician input and lacking intelligent
autonomy. This dependency contributes to operator fatigue, more radiation
exposure, and variability in procedural outcomes. This work moves towards
autonomous catheter navigation by introducing DINO-CVA, a multimodal
goal-conditioned behavior cloning framework. The proposed model fuses visual
observations and joystick kinematics into a joint embedding space, enabling
policies that are both vision-aware and kinematic-aware. Actions are predicted
autoregressively from expert demonstrations, with goal conditioning guiding
navigation toward specified destinations. A robotic experimental setup with a
synthetic vascular phantom was designed to collect multimodal datasets and
evaluate performance. Results show that DINO-CVA achieves high accuracy in
predicting actions, matching the performance of a kinematics-only baseline
while additionally grounding predictions in the anatomical environment. These
findings establish the feasibility of multimodal, goal-conditioned
architectures for catheter navigation, representing an important step toward
reducing operator dependency and improving the reliability of catheterbased
therapies.

</details>


### [23] [Learning to Design Soft Hands using Reward Models](https://arxiv.org/abs/2510.17086)
*Xueqian Bai,Nicklas Hansen,Adabhav Singh,Michael T. Tolley,Yan Duan,Pieter Abbeel,Xiaolong Wang,Sha Yi*

Main category: cs.RO

TL;DR: 提出CEM-RM框架，通过基于遥操作控制策略优化肌腱驱动软体机械手设计，相比纯优化方法减少一半以上设计评估次数，同时从预收集的遥操作数据中学习优化手设计分布。


<details>
  <summary>Details</summary>
Motivation: 软体机械手虽然能提供柔顺安全的交互，但设计既柔顺又功能多样的软手仍具挑战性。硬件与控制协同设计虽然能更好耦合形态与行为，但搜索空间高维，即使基于仿真的评估也计算昂贵。

Method: 采用交叉熵方法与奖励模型(CEM-RM)框架，为肌腱驱动软体机械手定义设计空间（由柔性软手指组成），在仿真中实现并行化训练，然后3D打印优化设计并在真实世界中使用遥操作数据进行部署。

Result: 在仿真和硬件实验中，优化设计在多种挑战性物体上的抓取成功率显著优于基线手。

Conclusion: CEM-RM框架能有效优化软体机械手设计，大幅减少设计评估次数，同时提升抓取性能。

Abstract: Soft robotic hands promise to provide compliant and safe interaction with
objects and environments. However, designing soft hands to be both compliant
and functional across diverse use cases remains challenging. Although co-design
of hardware and control better couples morphology to behavior, the resulting
search space is high-dimensional, and even simulation-based evaluation is
computationally expensive. In this paper, we propose a Cross-Entropy Method
with Reward Model (CEM-RM) framework that efficiently optimizes tendon-driven
soft robotic hands based on teleoperation control policy, reducing design
evaluations by more than half compared to pure optimization while learning a
distribution of optimized hand designs from pre-collected teleoperation data.
We derive a design space for a soft robotic hand composed of flexural soft
fingers and implement parallelized training in simulation. The optimized hands
are then 3D-printed and deployed in the real world using both teleoperation
data and real-time teleoperation. Experiments in both simulation and hardware
demonstrate that our optimized design significantly outperforms baseline hands
in grasping success rates across a diverse set of challenging objects.

</details>


### [24] [Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey](https://arxiv.org/abs/2510.17111)
*Weifan Guan,Qinghao Hu,Aosheng Li,Jian Cheng*

Main category: cs.RO

TL;DR: 这篇论文系统综述了提升视觉-语言-动作模型效率的方法，重点关注减少延迟、内存占用和训练/推理成本，将现有解决方案分为四个维度进行总结。


<details>
  <summary>Details</summary>
Motivation: VLA模型在机器人控制中面临巨大的计算和内存需求，与边缘平台实时性能要求存在冲突，需要提高效率以适应资源受限的部署环境。

Method: 将效率提升方法分类为四个维度：模型架构、感知特征、动作生成以及训练/推理策略，并在每个类别中总结代表性技术。

Result: 提供了VLA模型效率优化的系统性框架和分类方法，总结了各维度的代表性技术方案。

Conclusion: 讨论了未来趋势和开放挑战，强调了推进高效具身智能的发展方向。

Abstract: Vision-Language-Action (VLA) models extend vision-language models to embodied
control by mapping natural-language instructions and visual observations to
robot actions. Despite their capabilities, VLA systems face significant
challenges due to their massive computational and memory demands, which
conflict with the constraints of edge platforms such as on-board mobile
manipulators that require real-time performance. Addressing this tension has
become a central focus of recent research. In light of the growing efforts
toward more efficient and scalable VLA systems, this survey provides a
systematic review of approaches for improving VLA efficiency, with an emphasis
on reducing latency, memory footprint, and training and inference costs. We
categorize existing solutions into four dimensions: model architecture,
perception feature, action generation, and training/inference strategies,
summarizing representative techniques within each category. Finally, we discuss
future trends and open challenges, highlighting directions for advancing
efficient embodied intelligence.

</details>


### [25] [Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning](https://arxiv.org/abs/2510.17143)
*Shantnav Agarwal,Javier Alonso-Mora,Sihao Sun*

Main category: cs.RO

TL;DR: 提出了一种基于机器学习的分散式运动规划方法，用于多无人机协同运输电缆悬挂负载，无需集中控制或通信。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖集中控制架构或可靠的通信，限制了在部分可观测和无通信条件下的应用。

Method: 使用模仿学习训练分散式学生策略，模仿具有全局观测的集中式运动规划器，采用物理信息神经网络生成平滑轨迹。

Result: 在仿真和真实环境中验证，能够跟踪敏捷参考轨迹，性能接近集中式方法，且训练时间短。

Conclusion: 该方法在分散、无通信条件下实现了高效的负载运输，具有实际应用潜力。

Abstract: Existing approaches for transporting and manipulating cable-suspended loads
using multiple UAVs along reference trajectories typically rely on either
centralized control architectures or reliable inter-agent communication. In
this work, we propose a novel machine learning based method for decentralized
kinodynamic planning that operates effectively under partial observability and
without inter-agent communication. Our method leverages imitation learning to
train a decentralized student policy for each UAV by imitating a centralized
kinodynamic motion planner with access to privileged global observations. The
student policy generates smooth trajectories using physics-informed neural
networks that respect the derivative relationships in motion. During training,
the student policies utilize the full trajectory generated by the teacher
policy, leading to improved sample efficiency. Moreover, each student policy
can be trained in under two hours on a standard laptop. We validate our method
in both simulation and real-world environments to follow an agile reference
trajectory, demonstrating performance comparable to that of centralized
approaches.

</details>


### [26] [DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment](https://arxiv.org/abs/2510.17148)
*Yu Gao,Yiru Wang,Anqing Jiang,Heng Yuwen,Wang Shuo,Sun Hao,Wang Jijun*

Main category: cs.RO

TL;DR: DiffVLA++是一个增强型自动驾驶框架，通过度量引导对齐显式桥接认知推理和端到端规划，结合VLA模型的世界知识和E2E模型的物理可行性。


<details>
  <summary>Details</summary>
Motivation: 传统E2E驾驶模型能生成物理可行轨迹但缺乏世界知识处理长尾场景，而VLA模型有世界知识但3D推理能力有限导致物理不可行动作。需要结合两者优势。

Method: 1) VLA模块直接生成语义基础的驾驶轨迹；2) E2E模块使用密集轨迹词汇确保物理可行性；3) 度量引导轨迹评分器指导对齐VLA和E2E模块输出。

Result: 在ICCV 2025自动驾驶大挑战排行榜上，DiffVLA++实现了49.12的EPDMS分数。

Conclusion: DiffVLA++通过显式桥接认知推理和端到端规划，成功整合了VLA的世界知识和E2E的物理可行性，在自动驾驶任务中表现出色。

Abstract: Conventional end-to-end (E2E) driving models are effective at generating
physically plausible trajectories, but often fail to generalize to long-tail
scenarios due to the lack of essential world knowledge to understand and reason
about surrounding environments. In contrast, Vision-Language-Action (VLA)
models leverage world knowledge to handle challenging cases, but their limited
3D reasoning capability can lead to physically infeasible actions. In this work
we introduce DiffVLA++, an enhanced autonomous driving framework that
explicitly bridges cognitive reasoning and E2E planning through metric-guided
alignment. First, we build a VLA module directly generating semantically
grounded driving trajectories. Second, we design an E2E module with a dense
trajectory vocabulary that ensures physical feasibility. Third, and most
critically, we introduce a metric-guided trajectory scorer that guides and
aligns the outputs of the VLA and E2E modules, thereby integrating their
complementary strengths. The experiment on the ICCV 2025 Autonomous Grand
Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.

</details>


### [27] [OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation](https://arxiv.org/abs/2510.17150)
*Heng Zhang,Wei-Hsing Huang,Gokhan Solak,Arash Ajoudani*

Main category: cs.RO

TL;DR: OmniVIC是一种基于视觉语言模型的通用可变阻抗控制器，通过检索增强生成和上下文学习技术，在接触丰富的机器人操作任务中提高安全性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统可变阻抗控制器在物理交互中具有优势，但在未见过的复杂非结构化安全交互场景中缺乏泛化能力。需要一种能够理解任务上下文并生成自适应阻抗参数的通用控制器。

Method: 使用自改进的检索增强生成(RAG)和上下文学习(ICL)，RAG从结构化记忆库中检索相关先验经验，ICL利用检索到的示例和当前任务提示查询VLM，生成上下文感知的自适应阻抗参数，并结合实时力/力矩反馈确保交互力在安全阈值内。

Result: 在仿真和真实机器人任务中，OmniVIC在复杂接触丰富任务上优于基线方法，平均成功率从27%（基线）提升到61.4%（OmniVIC），同时减少了力违规。

Conclusion: OmniVIC在高层语义推理和低层顺应控制之间架起了桥梁，实现了更安全和更通用的机器人操作。

Abstract: We present OmniVIC, a universal variable impedance controller (VIC) enhanced
by a vision language model (VLM), which improves safety and adaptation in any
contact-rich robotic manipulation task to enhance safe physical interaction.
Traditional VIC have shown advantages when the robot physically interacts with
the environment, but lack generalization in unseen, complex, and unstructured
safe interactions in universal task scenarios involving contact or uncertainty.
To this end, the proposed OmniVIC interprets task context derived reasoning
from images and natural language and generates adaptive impedance parameters
for a VIC controller. Specifically, the core of OmniVIC is a self-improving
Retrieval-Augmented Generation(RAG) and in-context learning (ICL), where RAG
retrieves relevant prior experiences from a structured memory bank to inform
the controller about similar past tasks, and ICL leverages these retrieved
examples and the prompt of current task to query the VLM for generating
context-aware and adaptive impedance parameters for the current manipulation
scenario. Therefore, a self-improved RAG and ICL guarantee OmniVIC works in
universal task scenarios. The impedance parameter regulation is further
informed by real-time force/torque feedback to ensure interaction forces remain
within safe thresholds. We demonstrate that our method outperforms baselines on
a suite of complex contact-rich tasks, both in simulation and on real-world
robotic tasks, with improved success rates and reduced force violations.
OmniVIC takes a step towards bridging high-level semantic reasoning and
low-level compliant control, enabling safer and more generalizable
manipulation. Overall, the average success rate increases from 27% (baseline)
to 61.4% (OmniVIC).

</details>


### [28] [SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving](https://arxiv.org/abs/2510.17191)
*Peiru Zheng,Yun Zhao,Zhan Gong,Hong Zhu,Shaohua Wu*

Main category: cs.RO

TL;DR: 提出了SimpleVSF框架，通过融合视觉语言模型和轨迹融合技术来增强端到端自动驾驶规划，在ICCV 2025挑战赛中取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法在复杂场景下决策能力不足，需要更智能的规划策略。

Method: 结合传统评分器和VLM增强评分器，使用权重融合器进行定量聚合和VLM融合器进行定性、上下文感知决策。

Result: 在ICCV 2025 NAVSIM v2端到端驾驶挑战赛中取得最先进性能，在安全性、舒适性和效率之间达到优越平衡。

Conclusion: SimpleVSF框架通过融合VLM认知能力和先进轨迹融合技术，显著提升了端到端自动驾驶的规划性能。

Abstract: End-to-end autonomous driving has emerged as a promising paradigm for
achieving robust and intelligent driving policies. However, existing end-to-end
methods still face significant challenges, such as suboptimal decision-making
in complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring
Fusion), a novel framework that enhances end-to-end planning by leveraging the
cognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory
fusion techniques. We utilize the conventional scorers and the novel
VLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative
aggregation and a powerful VLM-based fusioner for qualitative, context-aware
decision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End
Driving Challenge, our SimpleVSF framework demonstrates state-of-the-art
performance, achieving a superior balance between safety, comfort, and
efficiency.

</details>


### [29] [Performance Evaluation of an Integrated System for Visible Light Communication and Positioning Using an Event Camera](https://arxiv.org/abs/2510.17203)
*Ryota Soga,Masataka Kobayashi,Tsukasa Shimizu,Shintaro Shiba,Quan Kong,Shan Lu,Takaya Yamazato*

Main category: cs.RO

TL;DR: 提出了一种基于事件相机的新型自定位系统，通过单台事件相机同时实现可见光通信(VLC)和可见光定位(VLP)，使车辆能够在GPS失效环境中估计位置。


<details>
  <summary>Details</summary>
Motivation: 利用事件相机的高时间分辨率和高动态范围特性，在隧道等GPS失效环境中实现车辆定位，解决传统传感器在快速移动物体和极端光照对比场景中的局限性。

Method: 使用Walsh-Hadamard码为多个LED分配唯一导频序列，通过事件相机识别视野内各LED，利用相位相关(POC)进行距离估计，实现同步的高容量MISO通信和精确定位。

Result: 在30km/h车速下进行实地测试，距离估计的均方根误差在100米范围内小于0.75米，比特误码率在相同范围内低于0.01。

Conclusion: 这是首个使用单台事件相机同时实现VLC和VLP功能的车辆载系统，展示了在真实环境中的鲁棒性能，为GPS失效环境下的车辆定位提供了有效解决方案。

Abstract: Event cameras, featuring high temporal resolution and high dynamic range,
offer visual sensing capabilities comparable to conventional image sensors
while capturing fast-moving objects and handling scenes with extreme lighting
contrasts such as tunnel exits. Leveraging these properties, this study
proposes a novel self-localization system that integrates visible light
communication (VLC) and visible light positioning (VLP) within a single event
camera. The system enables a vehicle to estimate its position even in
GPS-denied environments, such as tunnels, by using VLC to obtain coordinate
information from LED transmitters and VLP to estimate the distance to each
transmitter.
  Multiple LEDs are installed on the transmitter side, each assigned a unique
pilot sequence based on Walsh-Hadamard codes. The event camera identifies
individual LEDs within its field of view by correlating the received signal
with these codes, allowing clear separation and recognition of each light
source. This mechanism enables simultaneous high-capacity MISO (multi-input
single-output) communication through VLC and precise distance estimation via
phase-only correlation (POC) between multiple LED pairs.
  To the best of our knowledge, this is the first vehicle-mounted system to
achieve simultaneous VLC and VLP functionalities using a single event camera.
Field experiments were conducted by mounting the system on a vehicle traveling
at 30 km/h (8.3 m/s). The results demonstrated robust real-world performance,
with a root mean square error (RMSE) of distance estimation within 0.75 m for
ranges up to 100 m and a bit error rate (BER) below 0.01 across the same range.

</details>


### [30] [Pole-Image: A Self-Supervised Pole-Anchored Descriptor for Long-Term LiDAR Localization and Map Maintenance](https://arxiv.org/abs/2510.17237)
*Wuhao Xie,Kanji Tanaka*

Main category: cs.RO

TL;DR: 提出Pole-Image表示法，利用杆状地标作为锚点生成周围3D结构的签名，通过对比学习获得视角不变且高区分度的描述符，实现鲁棒自定位和高灵敏度变化检测。


<details>
  <summary>Details</summary>
Motivation: 解决传统地标方法在可检测性和区分度之间的权衡问题，需要同时实现鲁棒的自定位和可靠的地图维护。

Method: 提出Pole-Image表示法，将杆状地标及其周围环境表示为以杆为中心的2D极坐标图像，利用对比学习训练视角不变描述符。

Result: 描述符克服了感知混淆，实现了鲁棒自定位；高精度编码实现了高灵敏度变化检测，有助于地图维护。

Conclusion: Pole-Image方法成功解决了地标检测与区分度的权衡问题，为移动机器人长期自主性提供了有效的解决方案。

Abstract: Long-term autonomy for mobile robots requires both robust self-localization
and reliable map maintenance. Conventional landmark-based methods face a
fundamental trade-off between landmarks with high detectability but low
distinctiveness (e.g., poles) and those with high distinctiveness but difficult
stable detection (e.g., local point cloud structures). This work addresses the
challenge of descriptively identifying a unique "signature" (local point cloud)
by leveraging a detectable, high-precision "anchor" (like a pole). To solve
this, we propose a novel canonical representation, "Pole-Image," as a hybrid
method that uses poles as anchors to generate signatures from the surrounding
3D structure. Pole-Image represents a pole-like landmark and its surrounding
environment, detected from a LiDAR point cloud, as a 2D polar coordinate image
with the pole itself as the origin. This representation leverages the pole's
nature as a high-precision reference point, explicitly encoding the "relative
geometry" between the stable pole and the variable surrounding point cloud. The
key advantage of pole landmarks is that "detection" is extremely easy. This
ease of detection allows the robot to easily track the same pole, enabling the
automatic and large-scale collection of diverse observational data (positive
pairs). This data acquisition feasibility makes "Contrastive Learning (CL)"
applicable. By applying CL, the model learns a viewpoint-invariant and highly
discriminative descriptor. The contributions are twofold: 1) The descriptor
overcomes perceptual aliasing, enabling robust self-localization. 2) The
high-precision encoding enables high-sensitivity change detection, contributing
to map maintenance.

</details>


### [31] [An adaptive hierarchical control framework for quadrupedal robots in planetary exploration](https://arxiv.org/abs/2510.17249)
*Franek Stark,Rohit Kumar,Shubham Vyas,Hannah Isermann,Jonas Haack,Mihaela Popescu,Jakob Middelberg,Dennis Mronga,Frank Kirchner*

Main category: cs.RO

TL;DR: 提出了一种模块化控制框架，结合基于模型的动态控制、在线模型适应和自适应步态规划，用于解决四足机器人在未知环境中的不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 行星探索任务需要能在极端未知环境中导航的机器人。轮式机器人在可通行表面受限，而四足机器人能处理不平坦、障碍物多和可变形地形，但在未知条件下部署面临环境特定控制的挑战。

Method: 开发了模块化控制框架，结合模型动态控制、在线模型适应和自适应步态规划。包含有/无接触传感的状态估计，支持运行时重新配置，并集成到ROS 2中开源可用。

Result: 在两个四足机器人平台、多种硬件架构上验证性能，并在火山实地测试中机器人行走超过700米。

Conclusion: 该框架成功解决了机器人和地形属性不确定性问题，为四足机器人在未知环境中的可靠部署提供了有效解决方案。

Abstract: Planetary exploration missions require robots capable of navigating extreme
and unknown environments. While wheeled rovers have dominated past missions,
their mobility is limited to traversable surfaces. Legged robots, especially
quadrupeds, can overcome these limitations by handling uneven, obstacle-rich,
and deformable terrains. However, deploying such robots in unknown conditions
is challenging due to the need for environment-specific control, which is
infeasible when terrain and robot parameters are uncertain. This work presents
a modular control framework that combines model-based dynamic control with
online model adaptation and adaptive footstep planning to address uncertainties
in both robot and terrain properties. The framework includes state estimation
for quadrupeds with and without contact sensing, supports runtime
reconfiguration, and is integrated into ROS 2 with open-source availability.
Its performance was validated on two quadruped platforms, multiple hardware
architectures, and in a volcano field test, where the robot walked over 700 m.

</details>


### [32] [High-Level Multi-Robot Trajectory Planning And Spurious Behavior Detection](https://arxiv.org/abs/2510.17261)
*Fernando Salanova,Jesús Roche,Cristian Mahuela,Eduardo Montijano*

Main category: cs.RO

TL;DR: 提出了一个基于Nets-within-Nets范式的结构化数据生成框架和Transformer异常检测管道，用于检测多机器人系统中LTL规范下的异常行为执行。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统中异构代理的高层任务可靠执行需要检测虚假行为的鲁棒方法，包括错误任务序列、空间约束违反、时间不一致和任务语义偏离。

Method: 使用Nets-within-Nets范式协调机器人动作与LTL全局任务规范，提出基于Transformer的异常检测管道对机器人轨迹进行分类。

Result: 实验评估显示该方法在执行效率异常检测上达到91.3%准确率，核心任务违反检测88.3%，约束自适应异常检测66.8%。消融实验表明新方法优于简单表示。

Conclusion: 该方法能有效识别多机器人系统中LTL规范下的异常行为，为可靠任务执行提供保障。

Abstract: The reliable execution of high-level missions in multi-robot systems with
heterogeneous agents, requires robust methods for detecting spurious behaviors.
In this paper, we address the challenge of identifying spurious executions of
plans specified as a Linear Temporal Logic (LTL) formula, as incorrect task
sequences, violations of spatial constraints, timing inconsis- tencies, or
deviations from intended mission semantics. To tackle this, we introduce a
structured data generation framework based on the Nets-within-Nets (NWN)
paradigm, which coordinates robot actions with LTL-derived global mission
specifications. We further propose a Transformer-based anomaly detection
pipeline that classifies robot trajectories as normal or anomalous. Experi-
mental evaluations show that our method achieves high accuracy (91.3%) in
identifying execution inefficiencies, and demonstrates robust detection
capabilities for core mission violations (88.3%) and constraint-based adaptive
anomalies (66.8%). An ablation experiment of the embedding and architecture was
carried out, obtaining successful results where our novel proposition performs
better than simpler representations.

</details>


### [33] [Floating-Base Deep Lagrangian Networks](https://arxiv.org/abs/2510.17270)
*Lucas Schulze,Juliano Decico Negri,Victor Barasuol,Vivian Suzano Medeiros,Marcelo Becker,Jan Peters,Oleg Arenz*

Main category: cs.RO

TL;DR: 提出了Floating-Base Deep Lagrangian Networks (FeLaN)，一种针对浮动基系统（如人形机器人和四足机器人）的灰盒系统辨识方法，通过参数化满足物理约束的惯性矩阵来提高物理一致性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的灰盒模型忽略了浮动基系统的特定物理约束，如惯性矩阵的正定性、分支诱导稀疏性、输入独立性以及复合空间惯性的特征值三角不等式等性质，导致物理不一致性。

Method: 受Deep Lagrangian Networks (DeLaN)启发，训练神经网络预测满足所有物理约束的惯性矩阵，在拉格朗日力学框架下最小化逆动力学误差。

Result: 在多个四足机器人和人形机器人数据集上的实验表明，FeLaN在仿真和真实机器人上都取得了极具竞争力的性能，同时提供了更好的物理可解释性。

Conclusion: FeLaN方法成功地将物理约束融入深度学习模型，为浮动基系统提供了物理一致且高性能的系统辨识解决方案。

Abstract: Grey-box methods for system identification combine deep learning with
physics-informed constraints, capturing complex dependencies while improving
out-of-distribution generalization. Yet, despite the growing importance of
floating-base systems such as humanoids and quadrupeds, current grey-box models
ignore their specific physical constraints. For instance, the inertia matrix is
not only positive definite but also exhibits branch-induced sparsity and input
independence. Moreover, the 6x6 composite spatial inertia of the floating base
inherits properties of single-rigid-body inertia matrices. As we show, this
includes the triangle inequality on the eigenvalues of the composite rotational
inertia. To address the lack of physical consistency in deep learning models of
floating-base systems, we introduce a parameterization of inertia matrices that
satisfies all these constraints. Inspired by Deep Lagrangian Networks (DeLaN),
we train neural networks to predict physically plausible inertia matrices that
minimize inverse dynamics error under Lagrangian mechanics. For evaluation, we
collected and released a dataset on multiple quadrupeds and humanoids. In these
experiments, our Floating-Base Deep Lagrangian Networks (FeLaN) achieve highly
competitive performance on both simulated and real robots, while providing
greater physical interpretability.

</details>


### [34] [Implicit State Estimation via Video Replanning](https://arxiv.org/abs/2510.17315)
*Po-Chen Ko,Jiayuan Mao,Yu-Hsiang Fu,Hsien-Jeng Yeh,Chu-Rong Chen,Wei-Chiu Ma,Yilun Du,Shao-Hua Sun*

Main category: cs.RO

TL;DR: 提出了一种集成交互时数据的新型视频规划框架，通过在线更新模型参数和过滤失败计划来适应部分观测环境中的不确定性，实现隐式状态估计。


<details>
  <summary>Details</summary>
Motivation: 现有视频规划框架难以适应交互时的失败情况，因为它们无法在部分观测环境中对不确定性进行推理。

Method: 集成交互时数据到规划过程，在线更新模型参数，在生成过程中过滤掉先前失败的计划，实现隐式状态估计。

Result: 在新模拟操作基准上的广泛实验表明，该方法能够提高重新规划性能。

Conclusion: 该框架推进了基于视频的决策领域发展，能够在没有显式建模未知状态变量的情况下动态适应环境。

Abstract: Video-based representations have gained prominence in planning and
decision-making due to their ability to encode rich spatiotemporal dynamics and
geometric relationships. These representations enable flexible and
generalizable solutions for complex tasks such as object manipulation and
navigation. However, existing video planning frameworks often struggle to adapt
to failures at interaction time due to their inability to reason about
uncertainties in partially observed environments. To overcome these
limitations, we introduce a novel framework that integrates interaction-time
data into the planning process. Our approach updates model parameters online
and filters out previously failed plans during generation. This enables
implicit state estimation, allowing the system to adapt dynamically without
explicitly modeling unknown state variables. We evaluate our framework through
extensive experiments on a new simulated manipulation benchmark, demonstrating
its ability to improve replanning performance and advance the field of
video-based decision-making.

</details>


### [35] [DDBot: Differentiable Physics-based Digging Robot for Unknown Granular Materials](https://arxiv.org/abs/2510.17335)
*Xintong Yang,Minglun Wei,Ze Ji,Yu-Kun Lai*

Main category: cs.RO

TL;DR: 提出DDBot框架，用于自动化操控颗粒材料，通过可微分物理模拟器实现高效系统识别和挖掘技能优化，在真实环境中实现零样本部署。


<details>
  <summary>Details</summary>
Motivation: 颗粒材料操控面临复杂接触动力学、不可预测材料特性和复杂系统状态的挑战，现有方法难以实现高效精确的操作。

Method: 采用可微分物理模拟器，结合GPU加速并行计算和自动微分，实现可微分系统识别和挖掘技能优化，包括可微分技能到动作映射、任务导向演示方法、梯度裁剪和线搜索梯度下降。

Result: DDBot能在5-20分钟内收敛，高效识别未知颗粒材料动力学并优化挖掘技能，在零样本真实世界部署中实现高精度结果。

Conclusion: DDBot在挖掘任务中展现出鲁棒性和高效性，基准测试结果优于现有最优方法。

Abstract: Automating the manipulation of granular materials poses significant
challenges due to complex contact dynamics, unpredictable material properties,
and intricate system states. Existing approaches often fail to achieve
efficiency and accuracy in such tasks. To fill the research gap, this paper
studies the small-scale and high-precision granular material digging task with
unknown physical properties. A new framework, named differentiable digging
robot (DDBot), is proposed to manipulate granular materials, including sand and
soil.
  Specifically, we equip DDBot with a differentiable physics-based simulator,
tailored for granular material manipulation, powered by GPU-accelerated
parallel computing and automatic differentiation. DDBot can perform efficient
differentiable system identification and high-precision digging skill
optimisation for unknown granular materials, which is enabled by a
differentiable skill-to-action mapping, a task-oriented demonstration method,
gradient clipping and line search-based gradient descent.
  Experimental results show that DDBot can efficiently (converge within 5 to 20
minutes) identify unknown granular material dynamics and optimise digging
skills, with high-precision results in zero-shot real-world deployments,
highlighting its practicality. Benchmark results against state-of-the-art
baselines also confirm the robustness and efficiency of DDBot in such digging
tasks.

</details>


### [36] [Interactive Force-Impedance Control](https://arxiv.org/abs/2510.17341)
*Fan Shao,Satoshi Endo,Sandra Hirche,Fanny Ficuciello*

Main category: cs.RO

TL;DR: 提出了统一的交互力-阻抗控制(IFIC)框架，通过适应交互功率流确保在接触密集环境中的轻松安全交互。


<details>
  <summary>Details</summary>
Motivation: 解决在混合或统一力-阻抗控制下，机器人系统可能失去被动性从而危及安全的问题，特别是在与主动人类或非被动环境物理交互时。

Method: 在端口哈密顿框架内制定控制架构，包含交互和任务控制端口，通过该框架保证系统被动性。

Result: IFIC框架能够适应交互功率流，确保在接触密集环境中的轻松安全交互。

Conclusion: 提出的统一交互力-阻抗控制框架通过端口哈密顿方法有效解决了机器人系统在复杂交互环境中的被动性和安全性问题。

Abstract: Human collaboration with robots requires flexible role adaptation, enabling
robot to switch between active leader and passive follower. Effective role
switching depends on accurately estimating human intention, which is typically
achieved through external force analysis, nominal robot dynamics, or
data-driven approaches. However, these methods are primarily effective in
contact-sparse environments. When robots under hybrid or unified
force-impedance control physically interact with active humans or non-passive
environments, the robotic system may lose passivity and thus compromise safety.
To address this challenge, this paper proposes the unified Interactive
Force-Impedance Control (IFIC) framework that adapts to the interaction power
flow, ensuring effortless and safe interaction in contact-rich environments.
The proposed control architecture is formulated within a port-Hamiltonian
framework, incorporating both interaction and task control ports, through which
system passivity is guaranteed.

</details>


### [37] [Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots](https://arxiv.org/abs/2510.17369)
*Haochen Su,Cristian Meo,Francesco Stella,Andrea Peirone,Kai Junge,Josie Hughes*

Main category: cs.RO

TL;DR: 将视觉-语言-动作模型部署到软体连续机械臂上，通过微调实现安全的人机交互，在人类共享环境中实现灵活安全的具身AI。


<details>
  <summary>Details</summary>
Motivation: 机器人系统需要在人类中心的无结构环境中安全运行，但现有VLA模型主要限于刚性机械臂，缺乏安全交互能力。软体机械臂的柔性和安全性使其更适合人类共享环境。

Method: 提出结构化的微调和部署流程，评估两种最先进的VLA模型在代表性操作任务中的表现，通过针对性微调弥补本体不匹配问题。

Result: 现成策略因本体不匹配而失败，但经过微调后软体机器人性能与刚性机器人相当，证明了微调在弥合本体差距中的必要性。

Conclusion: 将VLA模型与软体机器人结合能够在人类共享环境中实现安全和灵活的具身AI，微调是解决本体不匹配问题的关键。

Abstract: Robotic systems are increasingly expected to operate in human-centered,
unstructured environments where safety, adaptability, and generalization are
essential. Vision-Language-Action (VLA) models have been proposed as a language
guided generalized control framework for real robots. However, their deployment
has been limited to conventional serial link manipulators. Coupled by their
rigidity and unpredictability of learning based control, the ability to safely
interact with the environment is missing yet critical. In this work, we present
the deployment of a VLA model on a soft continuum manipulator to demonstrate
autonomous safe human-robot interaction. We present a structured finetuning and
deployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and
$\pi_0$) across representative manipulation tasks, and show while
out-of-the-box policies fail due to embodiment mismatch, through targeted
finetuning the soft robot performs equally to the rigid counterpart. Our
findings highlight the necessity of finetuning for bridging embodiment gaps,
and demonstrate that coupling VLA models with soft robots enables safe and
flexible embodied AI in human-shared environments.

</details>


### [38] [Integrating Trustworthy Artificial Intelligence with Energy-Efficient Robotic Arms for Waste Sorting](https://arxiv.org/abs/2510.17408)
*Halima I. Kure,Jishna Retnakumari,Augustine O. Nwajana,Umar M. Ismail,Bilyaminu A. Romo,Ehigiator Egho-Promise*

Main category: cs.RO

TL;DR: 提出了一种结合可信AI与节能机械臂的智能垃圾分类系统，使用基于MobileNetV2的CNN模型实现六类垃圾的准确分类，并通过机械臂模拟器进行虚拟分拣和能耗优化。


<details>
  <summary>Details</summary>
Motivation: 解决城市环境中智能废物管理的需求，通过集成可信AI技术提高垃圾分类的准确性和效率，同时确保系统的透明度、鲁棒性和安全性。

Method: 使用基于MobileNetV2的卷积神经网络进行迁移学习，准确分类塑料、玻璃、金属、纸张、纸板和垃圾六类；实现机械臂模拟器，通过欧几里得距离计算能耗以优化运动路径。

Result: 模型训练准确率达到99.8%，验证准确率为80.5%；系统成功实现虚拟垃圾分类，并优化了机械臂的能耗效率。

Conclusion: 该框架为城市智能废物管理系统提供了一个可靠、可扩展的解决方案，结合了可信AI的关键要素，具有良好的实际应用前景。

Abstract: This paper presents a novel methodology that integrates trustworthy
artificial intelligence (AI) with an energy-efficient robotic arm for
intelligent waste classification and sorting. By utilizing a convolutional
neural network (CNN) enhanced through transfer learning with MobileNetV2, the
system accurately classifies waste into six categories: plastic, glass, metal,
paper, cardboard, and trash. The model achieved a high training accuracy of
99.8% and a validation accuracy of 80.5%, demonstrating strong learning and
generalization. A robotic arm simulator is implemented to perform virtual
sorting, calculating the energy cost for each action using Euclidean distance
to ensure optimal and efficient movement. The framework incorporates key
elements of trustworthy AI, such as transparency, robustness, fairness, and
safety, making it a reliable and scalable solution for smart waste management
systems in urban settings.

</details>


### [39] [From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors](https://arxiv.org/abs/2510.17439)
*Zhengshen Zhang,Hao Li,Yalun Dai,Zhengbang Zhu,Lei Zhou,Chenchen Liu,Dong Wang,Francis E. H. Tay,Sijin Chen,Ziwei Liu,Yuxiao Liu,Xinghang Li,Pan Zhou*

Main category: cs.RO

TL;DR: FALCON通过将丰富的3D空间token注入动作头来解决现有VLA模型的空间推理差距问题，无需改变架构即可融合多种模态，在仿真和真实世界任务中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型基于2D编码器，存在空间推理差距，限制了泛化能力和适应性。现有的3D集成技术要么需要专用传感器且跨模态迁移差，要么注入缺乏几何信息的弱线索并损害视觉-语言对齐。

Method: 提出FALCON范式，利用空间基础模型从RGB图像中提取强几何先验，通过空间增强动作头消费空间token而非将其连接到视觉-语言骨干网络。可选融合深度或姿态信息以提高保真度，无需重新训练或架构更改。

Result: 在三个仿真基准和十一个真实世界任务的综合评估中，FALCON实现了最先进的性能，持续超越竞争基线，并在杂乱环境、空间提示条件以及物体尺度和高度变化下保持鲁棒性。

Conclusion: FALCON通过创新的空间token注入方法有效解决了空间表示、模态可迁移性和对齐方面的限制，为VLA模型提供了更强的空间推理能力。

Abstract: Existing vision-language-action (VLA) models act in 3D real-world but are
typically built on 2D encoders, leaving a spatial reasoning gap that limits
generalization and adaptability. Recent 3D integration techniques for VLAs
either require specialized sensors and transfer poorly across modalities, or
inject weak cues that lack geometry and degrade vision-language alignment. In
this work, we introduce FALCON (From Spatial to Action), a novel paradigm that
injects rich 3D spatial tokens into the action head. FALCON leverages spatial
foundation models to deliver strong geometric priors from RGB alone, and
includes an Embodied Spatial Model that can optionally fuse depth, or pose for
higher fidelity when available, without retraining or architectural changes. To
preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced
Action Head rather than being concatenated into the vision-language backbone.
These designs enable FALCON to address limitations in spatial representation,
modality transferability, and alignment. In comprehensive evaluations across
three simulation benchmarks and eleven real-world tasks, our proposed FALCON
achieves state-of-the-art performance, consistently surpasses competitive
baselines, and remains robust under clutter, spatial-prompt conditioning, and
variations in object scale and height.

</details>


### [40] [A Generalization of Input-Output Linearization via Dynamic Switching Between Melds of Output Functions](https://arxiv.org/abs/2510.17448)
*Mirko Mizzoni,Pieter van Goor,Barbara Bazzana,Antonio Franchi*

Main category: cs.RO

TL;DR: 提出了一个系统框架，用于通过反馈线性化在非线性系统的不同输出集之间切换，确保系统状态的统一有界性。


<details>
  <summary>Details</summary>
Motivation: 为了在非线性系统控制中实现不同输出集之间的灵活切换，同时保证系统的稳定性和性能。

Method: 引入了meld概念来定义有效的、可反馈线性化的输出子集，并建立了在适当驻留时间和兼容性条件下切换不同meld的正式证明。

Result: 证明了在切换区间内活动输出的误差动态保持指数稳定，且连续meld共有的输出在转换过程中能够无缝跟踪。

Conclusion: 该理论适用于任何可反馈线性化的非线性系统，如机器人、空中和地面车辆等，并通过机器人操纵器的数值模拟进行了验证。

Abstract: This letter presents a systematic framework for switching between different
sets of outputs for the control of nonlinear systems via feedback
linearization. We introduce the concept of a meld to formally define a valid,
feedback-linearizable subset of outputs that can be selected from a larger deck
of possible outputs. The main contribution is a formal proof establishing that
under suitable dwell-time and compatibility conditions, it is possible to
switch between different melds while guaranteeing the uniform boundedness of
the system state. We further show that the error dynamics of the active outputs
remain exponentially stable within each switching interval and that outputs
common to consecutive melds are tracked seamlessly through transitions. The
proposed theory is valid for any feedback linearizable nonlinear system, such
as, e.g., robots, aerial and terrestrial vehicles, etc.. We demonstrate it on a
simple numerical simulation of a robotic manipulator.

</details>


### [41] [HumanMPC - Safe and Efficient MAV Navigation among Humans](https://arxiv.org/abs/2510.17525)
*Simon Schaefer,Helen Oleynikova,Sandra Hirche,Stefan Leutenegger*

Main category: cs.RO

TL;DR: HumanMPC是一个用于3D微型飞行器在人群中导航的模型预测控制框架，结合了理论安全保证和数据驱动的人类运动预测模型，实现了安全高效的导航。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注简化的2D人群导航，未能充分考虑人体动态的复杂性。需要开发能够结合安全保证和现实人类运动预测的3D导航方法。

Method: 提出了一种新颖的基于可达性的安全公式，仅约束初始控制输入以确保安全，同时在整个规划范围内建模其效果。结合数据驱动的人类运动预测模型。

Result: 在模拟实验和真实世界验证中，HumanMPC在目标导向导航和视觉伺服跟踪等任务中表现出色，确保安全而不过度保守，在效率和可靠性方面优于基线方法。

Conclusion: 该方法虽然应用于微型飞行器，但具有通用性，可适应其他平台，为机器人在人类环境中安全高效导航提供了有效解决方案。

Abstract: Safe and efficient robotic navigation among humans is essential for
integrating robots into everyday environments. Most existing approaches focus
on simplified 2D crowd navigation and fail to account for the full complexity
of human body dynamics beyond root motion. We present HumanMPC, a Model
Predictive Control (MPC) framework for 3D Micro Air Vehicle (MAV) navigation
among humans that combines theoretical safety guarantees with data-driven
models for realistic human motion forecasting. Our approach introduces a novel
twist to reachability-based safety formulation that constrains only the initial
control input for safety while modeling its effects over the entire planning
horizon, enabling safe yet efficient navigation. We validate HumanMPC in both
simulated experiments using real human trajectories and in the real-world,
demonstrating its effectiveness across tasks ranging from goal-directed
navigation to visual servoing for human tracking. While we apply our method to
MAVs in this work, it is generic and can be adapted by other platforms. Our
results show that the method ensures safety without excessive conservatism and
outperforms baseline approaches in both efficiency and reliability.

</details>


### [42] [Distributed Spatial-Temporal Trajectory Optimization for Unmanned-Aerial-Vehicle Swarm](https://arxiv.org/abs/2510.17541)
*Xiaobo Zheng,Pan Tang,Defu Lin,Shaoming He*

Main category: cs.RO

TL;DR: 提出了一种基于ADMM和DDP的分布式空间-时间轨迹优化框架D-PDDP，用于解决大规模无人机群轨迹优化问题，通过参数化DDP进行局部规划，ADMM实现时空参数共识，并引入自适应惩罚参数调整来减少迭代次数。


<details>
  <summary>Details</summary>
Motivation: 现有群轨迹优化方法需要预先设定最终时间且迭代次数多，难以应用于大规模无人机群的实际场景，需要开发更高效的分布式算法。

Method: 采用双层架构：使用参数化DDP作为单个无人机的轨迹优化器，ADMM满足局部约束并实现所有无人机间的时空参数共识，形成分布式参数化DDP算法，并基于谱梯度法提出自适应惩罚参数调整准则。

Result: 通过多个仿真实验验证了所提算法的有效性，能够高效解决大规模无人机群的轨迹优化问题。

Conclusion: 提出的D-PDDP框架能够有效解决大规模无人机群的轨迹优化问题，通过分布式架构和自适应参数调整提高了算法效率。

Abstract: Swarm trajectory optimization problems are a well-recognized class of
multi-agent optimal control problems with strong nonlinearity. However, the
heuristic nature of needing to set the final time for agents beforehand and the
time-consuming limitation of the significant number of iterations prohibit the
application of existing methods to large-scale swarm of Unmanned Aerial
Vehicles (UAVs) in practice. In this paper, we propose a spatial-temporal
trajectory optimization framework that accomplishes multi-UAV consensus based
on the Alternating Direction Multiplier Method (ADMM) and uses Differential
Dynamic Programming (DDP) for fast local planning of individual UAVs. The
introduced framework is a two-level architecture that employs Parameterized DDP
(PDDP) as the trajectory optimizer for each UAV, and ADMM to satisfy the local
constraints and accomplish the spatial-temporal parameter consensus among all
UAVs. This results in a fully distributed algorithm called Distributed
Parameterized DDP (D-PDDP). In addition, an adaptive tuning criterion based on
the spectral gradient method for the penalty parameter is proposed to reduce
the number of algorithmic iterations. Several simulation examples are presented
to verify the effectiveness of the proposed algorithm.

</details>


### [43] [Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries](https://arxiv.org/abs/2510.17576)
*Cansu Erdogan,Cesar Alan Contreras,Alireza Rastegarpanah,Manolis Chiou,Rustam Stolkin*

Main category: cs.RO

TL;DR: 本文提出了一种意图驱动的规划管道，用于多机器人协作完成复杂操作任务，如电动汽车电池拆解。该管道整合了感知到文本的场景编码、LLM集合生成候选动作序列、基于LLM的验证器以及确定性一致性过滤器，能够根据人类简单语言指令生成可执行的多机器人计划。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人协作完成复杂操作任务时的规划问题，这些任务需要在非结构化场景中处理任意位置和配置的物体，需要根据人类意图生成可执行的连续动作序列。

Method: 提出意图驱动的规划管道，包括：(i) 感知到文本的场景编码，(ii) 基于LLM集合生成候选移除序列，(iii) LLM验证器强制执行格式和优先级约束，(iv) 确定性一致性过滤器拒绝幻觉物体。

Result: 在200个真实场景和600个操作员提示的评估中，使用完整序列正确性和下一任务正确性指标评估了五种基于LLM的规划器。结果表明，集成验证方法能够可靠地将操作员意图映射到安全、可执行的多机器人计划，同时保持较低的用户工作量。

Conclusion: 该意图驱动的规划管道能够可靠地将人类意图映射为安全、可执行的多机器人协作计划，在保持低用户工作量的同时，有效解决了复杂操作任务的规划问题。

Abstract: This paper addresses the problem of planning complex manipulation tasks, in
which multiple robots with different end-effectors and capabilities, informed
by computer vision, must plan and execute concatenated sequences of actions on
a variety of objects that can appear in arbitrary positions and configurations
in unstructured scenes. We propose an intent-driven planning pipeline which can
robustly construct such action sequences with varying degrees of supervisory
input from a human using simple language instructions. The pipeline integrates:
(i) perception-to-text scene encoding, (ii) an ensemble of large language
models (LLMs) that generate candidate removal sequences based on the operator's
intent, (iii) an LLM-based verifier that enforces formatting and precedence
constraints, and (iv) a deterministic consistency filter that rejects
hallucinated objects. The pipeline is evaluated on an example task in which two
robot arms work collaboratively to dismantle an Electric Vehicle battery for
recycling applications. A variety of components must be grasped and removed in
specific sequences, determined by human instructions and/or by task-order
feasibility decisions made by the autonomous system. On 200 real scenes with
600 operator prompts across five component classes, we used metrics of
full-sequence correctness and next-task correctness to evaluate and compare
five LLM-based planners (including ablation analyses of pipeline components).
We also evaluated the LLM-based human interface in terms of time to execution
and NASA TLX with human participant experiments. Results indicate that our
ensemble-with-verification approach reliably maps operator intent to safe,
executable multi-robot plans while maintaining low user effort.

</details>


### [44] [Learned Inertial Odometry for Cycling Based on Mixture of Experts Algorithm](https://arxiv.org/abs/2510.17604)
*Hao Qiao,Yan Wang,Shuo Yang,Xiaoyao Yu,Jian kuang,Xiaoji Niu*

Main category: cs.RO

TL;DR: 本文提出了一种改进的混合专家模型，将TLIO方法扩展到自行车定位，在保持精度的同时显著降低了计算成本和参数数量。


<details>
  <summary>Details</summary>
Motivation: 随着共享单车和多样化骑行应用的增长，精确的自行车定位变得至关重要。传统GNSS方法存在多径效应问题，而现有惯性导航方法依赖精确建模且鲁棒性有限。TLIO方法虽然能实现低位置漂移，但计算成本高，难以在移动设备上部署。

Method: 将TLIO方法扩展到自行车定位，并引入改进的混合专家模型来降低训练和推理成本。

Result: 与最先进的LLIO框架相比，该方法在保持相当精度的同时，参数减少了64.7%，计算成本降低了81.8%。

Conclusion: 改进的MoE模型在自行车定位中实现了计算效率和定位精度的良好平衡，为移动设备部署提供了可行方案。

Abstract: With the rapid growth of bike sharing and the increasing diversity of cycling
applications, accurate bicycle localization has become essential. traditional
GNSS-based methods suffer from multipath effects, while existing inertial
navigation approaches rely on precise modeling and show limited robustness.
Tight Learned Inertial Odometry (TLIO) achieves low position drift by combining
raw IMU data with predicted displacements by neural networks, but its high
computational cost restricts deployment on mobile devices. To overcome this, we
extend TLIO to bicycle localization and introduce an improved Mixture-of
Experts (MoE) model that reduces both training and inference costs. Experiments
show that, compared to the state-of-the-art LLIO framework, our method achieves
comparable accuracy while reducing parameters by 64.7% and computational cost
by 81.8%.

</details>


### [45] [RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation](https://arxiv.org/abs/2510.17640)
*Yuquan Xue,Guanxing Lu,Zhenyu Wu,Chuanrui Zhang,Bofang Jia,Zhengyi Gu,Yansong Tang,Ziwei Wang*

Main category: cs.RO

TL;DR: 提出RESample框架，通过探索性采样自动增强OOD数据，提升VLA模型在分布偏移状态下的鲁棒性和恢复能力。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习数据集只包含成功轨迹，缺乏失败和恢复数据，导致VLA模型在处理偏离训练分布的OOD状态时表现不佳。

Method: 利用离线强化学习获取动作价值网络识别次优动作，通过rollout采样潜在OOD状态，设计探索性采样机制将动作代理自适应纳入训练数据集。

Result: 在LIBERO基准测试和真实机器人操作任务上的实验表明，RESample持续提升了VLA模型的稳定性和泛化能力。

Conclusion: RESample框架有效增强了VLA模型从OOD状态恢复的能力，提高了对分布偏移的鲁棒性。

Abstract: Vision-Language-Action models (VLAs) have demonstrated remarkable performance
on complex robotic manipulation tasks through imitation learning. However,
existing imitation learning datasets contain only successful trajectories and
lack failure or recovery data, especially for out-of-distribution (OOD) states
where the robot deviates from the main policy due to minor perturbations or
errors, leading VLA models to struggle with states deviating from the training
distribution. To this end, we propose an automated OOD data augmentation
framework named RESample through exploratory sampling. Specifically, we first
leverage offline reinforcement learning to obtain an action-value network that
accurately identifies sub-optimal actions under the current manipulation
policy. We further sample potential OOD states from trajectories via rollout,
and design an exploratory sampling mechanism that adaptively incorporates these
action proxies into the training dataset to ensure efficiency. Subsequently,
our framework explicitly encourages the VLAs to recover from OOD states and
enhances their robustness against distributional shifts. We conduct extensive
experiments on the LIBERO benchmark as well as real-world robotic manipulation
tasks, demonstrating that RESample consistently improves the stability and
generalization ability of VLA models.

</details>


### [46] [Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats](https://arxiv.org/abs/2510.17783)
*Simeon Adebola,Chung Min Kim,Justin Kerr,Shuangyu Xie,Prithvi Akella,Jose Luis Susa Rincon,Eugen Solowjow,Ken Goldberg*

Main category: cs.RO

TL;DR: Botany-Bot是一个使用立体相机、数字转台、工业机器人臂和3D分割高斯溅射模型构建植物详细数字孪生的系统，能够通过操纵叶片拍摄被遮挡区域的细节图像。


<details>
  <summary>Details</summary>
Motivation: 商用植物表型系统因叶片遮挡无法感知植物细节，需要开发能够获取被遮挡区域详细图像的系统。

Method: 使用两个立体相机、数字转台、工业机器人臂和3D分割高斯溅射模型，开发机器人算法来操纵叶片拍摄被遮挡细节的高分辨率图像。

Result: 叶片分割准确率90.8%，叶片检测准确率86.2%，叶片提升/推动准确率77.9%，拍摄上下叶面细节图像准确率77.3%。

Conclusion: Botany-Bot能够有效构建植物的详细数字孪生，成功获取被遮挡区域的细节图像，代码和数据集已开源。

Abstract: Commercial plant phenotyping systems using fixed cameras cannot perceive many
plant details due to leaf occlusion. In this paper, we present Botany-Bot, a
system for building detailed "annotated digital twins" of living plants using
two stereo cameras, a digital turntable inside a lightbox, an industrial robot
arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms
for manipulating leaves to take high-resolution indexable images of occluded
details such as stem buds and the underside/topside of leaves. Results from
experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,
detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and
take detailed overside/underside images with 77.3% accuracy. Code, videos, and
datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.

</details>


### [47] [SoftMimic: Learning Compliant Whole-body Control from Examples](https://arxiv.org/abs/2510.17792)
*Gabriel B. Margolis,Michelle Wang,Nolan Fey,Pulkit Agrawal*

Main category: cs.RO

TL;DR: SoftMimic是一个从示例动作学习人形机器人柔顺全身控制策略的框架，通过奖励策略匹配柔顺响应而非刚性跟踪参考动作，使机器人能够顺应外部力同时保持平衡和姿态。


<details>
  <summary>Details</summary>
Motivation: 现有方法激励僵硬控制，当机器人遇到意外接触时会导致脆弱和不安全行为，需要开发能够顺应外部力的控制策略。

Method: 利用逆运动学求解器生成可行的柔顺动作增强数据集，训练强化学习策略，奖励策略匹配柔顺响应而非刚性跟踪参考动作。

Result: 在仿真和真实世界实验中验证了该方法，展示了与环境的安​​全有效交互，能够吸收干扰并从单个动作片段泛化到各种任务。

Conclusion: SoftMimic能够学习柔顺的全身控制策略，使机器人能够顺应外部力，同时保持平衡和姿态，提高了与环境的交互安全性。

Abstract: We introduce SoftMimic, a framework for learning compliant whole-body control
policies for humanoid robots from example motions. Imitating human motions with
reinforcement learning allows humanoids to quickly learn new skills, but
existing methods incentivize stiff control that aggressively corrects
deviations from a reference motion, leading to brittle and unsafe behavior when
the robot encounters unexpected contacts. In contrast, SoftMimic enables robots
to respond compliantly to external forces while maintaining balance and
posture. Our approach leverages an inverse kinematics solver to generate an
augmented dataset of feasible compliant motions, which we use to train a
reinforcement learning policy. By rewarding the policy for matching compliant
responses rather than rigidly tracking the reference motion, SoftMimic learns
to absorb disturbances and generalize to varied tasks from a single motion
clip. We validate our method through simulations and real-world experiments,
demonstrating safe and effective interaction with the environment.

</details>


### [48] [Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain](https://arxiv.org/abs/2510.17801)
*Yulin Luo,Chun-Kai Fan,Menghang Dong,Jiayu Shi,Mengdi Zhao,Bo-Wen Zhang,Cheng Chi,Jiaming Liu,Gaole Dai,Rongyu Zhang,Ruichuan An,Kun Wu,Zhengping Che,Shaoxuan Xie,Guocai Yao,Zhongxia Zhao,Pengwei Wang,Guang Liu,Zhongyuan Wang,Tiejun Huang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 提出了RoboBench基准，用于系统评估多模态大语言模型作为具身大脑在机器人操作任务中的认知能力，涵盖5个维度、14种能力、25个任务和6092个问答对。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注执行成功率，在高级推理方面存在维度不完整和任务真实性有限的问题，无法全面评估认知能力。需要系统评估具身大脑在完整操作流程中的关键作用。

Method: 构建RoboBench基准，定义五个评估维度：指令理解、感知推理、泛化规划、功能预测和失败分析。采用MLLM-as-world-simulator框架评估规划可行性，通过模拟预测计划是否能实现关键物体状态变化。

Result: 对14个MLLM的实验揭示了基本局限性：在隐式指令理解、时空推理、跨场景规划、细粒度功能理解和执行失败诊断方面存在困难。

Conclusion: RoboBench为量化高级认知能力提供了全面框架，可指导下一代具身MLLM的开发。

Abstract: Building robots that can perceive, reason, and act in dynamic, unstructured
environments remains a core challenge. Recent embodied systems often adopt a
dual-system paradigm, where System 2 handles high-level reasoning while System
1 executes low-level control. In this work, we refer to System 2 as the
embodied brain, emphasizing its role as the cognitive core for reasoning and
decision-making in manipulation tasks. Given this role, systematic evaluation
of the embodied brain is essential. Yet existing benchmarks emphasize execution
success, or when targeting high-level reasoning, suffer from incomplete
dimensions and limited task realism, offering only a partial picture of
cognitive capability. To bridge this gap, we introduce RoboBench, a benchmark
that systematically evaluates multimodal large language models (MLLMs) as
embodied brains. Motivated by the critical roles across the full manipulation
pipeline, RoboBench defines five dimensions-instruction comprehension,
perception reasoning, generalized planning, affordance prediction, and failure
analysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure
realism, we curate datasets across diverse embodiments, attribute-rich objects,
and multi-view scenes, drawing from large-scale real robotic data. For
planning, RoboBench introduces an evaluation framework,
MLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether
predicted plans can achieve critical object-state changes. Experiments on 14
MLLMs reveal fundamental limitations: difficulties with implicit instruction
comprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained
affordance understanding, and execution failure diagnosis. RoboBench provides a
comprehensive scaffold to quantify high-level cognition, and guide the
development of next-generation embodied MLLMs. The project page is in
https://robo-bench.github.io.

</details>
