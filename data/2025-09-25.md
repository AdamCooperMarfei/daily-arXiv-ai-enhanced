<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 54]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [HUNT: High-Speed UAV Navigation and Tracking in Unstructured Environments via Instantaneous Relative Frames](https://arxiv.org/abs/2509.19452)
*Alessandro Saviolo,Jeffrey Mao,Giuseppe Loianno*

Main category: cs.RO

TL;DR: HUNT框架将无人机的高速穿越、目标捕获和跟踪统一在相对导航框架中，仅使用机载瞬时观测数据实现自主导航，无需全局定位。


<details>
  <summary>Details</summary>
Motivation: 解决无人机在未知非结构化环境中高速穿越和目标跟踪的挑战，特别是在传感器性能下降和缺乏全局定位的情况下。

Method: 基于相对导航方法，将导航目标直接定义为机载瞬时观测数据（姿态、高度、速度），实现统一的感知-控制流水线。

Result: 在密集森林、集装箱场地和搜救场景中的户外实验表明，该框架在全局方法失效的情况下仍能实现稳健的自主性。

Conclusion: HUNT框架成功地将搜索和跟踪任务统一起来，为无人机在复杂环境中的自主操作提供了有效解决方案。

Abstract: Search and rescue operations require unmanned aerial vehicles to both
traverse unknown unstructured environments at high speed and track targets once
detected. Achieving both capabilities under degraded sensing and without global
localization remains an open challenge. Recent works on relative navigation
have shown robust tracking by anchoring planning and control to a visible
detected object, but cannot address navigation when no target is in the field
of view. We present HUNT (High-speed UAV Navigation and Tracking), a real-time
framework that unifies traversal, acquisition, and tracking within a single
relative formulation. HUNT defines navigation objectives directly from onboard
instantaneous observables such as attitude, altitude, and velocity, enabling
reactive high-speed flight during search. Once a target is detected, the same
perception-control pipeline transitions seamlessly to tracking. Outdoor
experiments in dense forests, container compounds, and search-and-rescue
operations with vehicles and mannequins demonstrate robust autonomy where
global methods fail.

</details>


### [2] [ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation](https://arxiv.org/abs/2509.19454)
*Jason Chen,I-Chun Arthur Liu,Gaurav Sukhatme,Daniel Seita*

Main category: cs.RO

TL;DR: ROPA是一种离线模仿学习数据增强方法，通过微调Stable Diffusion合成第三人称RGB和RGB-D观察数据，同时生成对应的关节空间动作标签，用于增强双手操作策略训练。


<details>
  <summary>Details</summary>
Motivation: 收集多样且精确的真实世界双手操作演示数据成本高且耗时，限制了可扩展性。现有数据增强方法主要针对手腕摄像头设置或仅生成新图像而无配对动作，缺乏针对第三人称RGB-D训练的动作标签增强。

Method: 提出ROPA方法，通过约束优化强制执行物理一致性，在双手场景中施加适当的夹爪-物体接触约束，同时合成新的机器人姿态观察数据和对应的动作标签。

Result: 在5个模拟任务和3个真实世界任务上进行评估，2625次模拟试验和300次真实世界试验结果表明，ROPA优于基线方法和消融实验。

Conclusion: ROPA展示了在第三人称双手操作中进行RGB和RGB-D数据增强的可扩展潜力，能够有效提升模仿学习性能。

Abstract: Training robust bimanual manipulation policies via imitation learning
requires demonstration data with broad coverage over robot poses, contacts, and
scene contexts. However, collecting diverse and precise real-world
demonstrations is costly and time-consuming, which hinders scalability. Prior
works have addressed this with data augmentation, typically for either
eye-in-hand (wrist camera) setups with RGB inputs or for generating novel
images without paired actions, leaving augmentation for eye-to-hand
(third-person) RGB-D training with new action labels less explored. In this
paper, we propose Synthetic Robot Pose Generation for RGB-D Bimanual Data
Augmentation (ROPA), an offline imitation learning data augmentation method
that fine-tunes Stable Diffusion to synthesize third-person RGB and RGB-D
observations of novel robot poses. Our approach simultaneously generates
corresponding joint-space action labels while employing constrained
optimization to enforce physical consistency through appropriate
gripper-to-object contact constraints in bimanual scenarios. We evaluate our
method on 5 simulated and 3 real-world tasks. Our results across 2625
simulation trials and 300 real-world trials demonstrate that ROPA outperforms
baselines and ablations, showing its potential for scalable RGB and RGB-D data
augmentation in eye-to-hand bimanual manipulation. Our project website is
available at: https://ropaaug.github.io/.

</details>


### [3] [Self-evolved Imitation Learning in Simulated World](https://arxiv.org/abs/2509.19460)
*Yifan Ye,Jun Cen,Jing Chen,Zhihe Lu*

Main category: cs.RO

TL;DR: SEIL是一个自进化模仿学习框架，通过模拟器交互逐步改进少样本模型，利用双级增强和轻量级选择器来减少对大规模专家演示的依赖。


<details>
  <summary>Details</summary>
Motivation: 模仿学习需要大量专家演示，但收集成本高昂。为了解决监督数据有限的问题，需要开发能够用少量样本实现高性能的方法。

Method: SEIL框架通过模拟器交互收集成功轨迹作为新演示进行迭代优化。采用双级增强：模型级使用EMA模型协作，环境级引入初始位置变化。使用轻量级选择器筛选高质量轨迹。

Result: 在LIBERO基准测试中，SEIL在少样本模仿学习场景下达到了新的最先进性能。

Conclusion: SEIL通过自进化学习机制，能够用更少的训练样本实现有竞争力的性能，为少样本模仿学习提供了有效解决方案。

Abstract: Imitation learning has been a trend recently, yet training a generalist agent
across multiple tasks still requires large-scale expert demonstrations, which
are costly and labor-intensive to collect. To address the challenge of limited
supervision, we propose Self-Evolved Imitation Learning (SEIL), a framework
that progressively improves a few-shot model through simulator interactions.
The model first attempts tasksin the simulator, from which successful
trajectories are collected as new demonstrations for iterative refinement. To
enhance the diversity of these demonstrations, SEIL employs dual-level
augmentation: (i) Model-level, using an Exponential Moving Average (EMA) model
to collaborate with the primary model, and (ii) Environment-level, introducing
slight variations in initial object positions. We further introduce a
lightweight selector that filters complementary and informative trajectories
from the generated pool to ensure demonstration quality. These curated samples
enable the model to achieve competitive performance with far fewer training
examples. Extensive experiments on the LIBERO benchmark show that SEIL achieves
a new state-of-the-art performance in few-shot imitation learning scenarios.
Code is available at https://github.com/Jasper-aaa/SEIL.git.

</details>


### [4] [CU-Multi: A Dataset for Multi-Robot Collaborative Perception](https://arxiv.org/abs/2509.19463)
*Doncey Albin,Daniel McGann,Miles Mena,Annika Thomas,Harel Biggie,Xuefei Sun,Steve McGuire,Jonathan P. How,Christoffer Heckman*

Main category: cs.RO

TL;DR: CU-Multi是一个新的多机器人数据集，解决了现有数据集轨迹短、机器人间重叠有限的问题，为多机器人协作感知任务提供标准化评估基础。


<details>
  <summary>Details</summary>
Motivation: 当前多机器人系统面临感知数据融合的挑战，但缺乏专门的多机器人数据集。现有数据集要么是通过分割单机器人轨迹模拟，要么轨迹短且机器人间重叠有限，导致评估结果难以比较和解释。

Method: 在科罗拉多大学博尔德分校的两个大型户外场地收集数据，包含四个同步运行的机器人轨迹，具有对齐的起始时间和可控的轨迹重叠，模拟真实机器人团队的不同视角。数据集包含RGB-D感知、RTK GPS、语义LiDAR和精炼的地面真值里程计。

Result: 创建了CU-Multi数据集，通过结合重叠变化和密集语义标注，为多机器人协作感知任务提供了可重复评估的坚实基础。

Conclusion: CU-Multi数据集克服了现有数据集的局限性，为多机器人协作SLAM和感知研究提供了标准化、真实的评估平台。

Abstract: A central challenge for multi-robot systems is fusing independently gathered
perception data into a unified representation. Despite progress in
Collaborative SLAM (C-SLAM), benchmarking remains hindered by the scarcity of
dedicated multi-robot datasets. Many evaluations instead partition single-robot
trajectories, a practice that may only partially reflect true multi-robot
operations and, more critically, lacks standardization, leading to results that
are difficult to interpret or compare across studies. While several multi-robot
datasets have recently been introduced, they mostly contain short trajectories
with limited inter-robot overlap and sparse intra-robot loop closures. To
overcome these limitations, we introduce CU-Multi, a dataset collected over
multiple days at two large outdoor sites on the University of Colorado Boulder
campus. CU-Multi comprises four synchronized runs with aligned start times and
controlled trajectory overlap, replicating the distinct perspectives of a robot
team. It includes RGB-D sensing, RTK GPS, semantic LiDAR, and refined
ground-truth odometry. By combining overlap variation with dense semantic
annotations, CU-Multi provides a strong foundation for reproducible evaluation
in multi-robot collaborative perception tasks.

</details>


### [5] [Crater Observing Bio-inspired Rolling Articulator (COBRA)](https://arxiv.org/abs/2509.19473)
*Adarsh Salagame,Henry Noyes,Alireza Ramezani,Eric Sihite,Arash Kalantari*

Main category: cs.RO

TL;DR: COBRA是一种多模态蛇形机器人，专为在月球沙克尔顿陨石坑的崎岖地形中导航而设计，结合滑行和翻滚两种运动模式来适应不同地形条件。


<details>
  <summary>Details</summary>
Motivation: NASA计划在月球建立可持续的人类基地，但现有月球车难以在永久阴影区的陨石坑等恶劣地形中导航。发现月球水冰资源对提供饮用水、氧气和火箭燃料至关重要，因此需要开发能够安全探索这些区域的机器人。

Method: COBRA采用蛇形机器人设计，具有两种运动模式：蛇模式下使用侧向滑行在平坦或低倾斜表面移动；翻滚模式下通过连接头尾形成圆形滚筒，在陡坡上实现快速低能耗移动。机器人配备机载计算机、立体相机、惯性测量单元和关节编码器，支持实时数据收集和自主操作。

Result: 通过仿真和实验验证，COBRA在极端地形导航中表现出鲁棒性和高效性，能够克服传统月球车在永久阴影陨石坑中的移动限制。

Conclusion: COBRA的多模态运动设计为解决月球恶劣地形探索提供了创新解决方案，为未来月球水冰资源探测和可持续月球基地建设提供了重要技术支持。

Abstract: NASA aims to establish a sustainable human basecamp on the Moon as a stepping
stone for future missions to Mars and beyond. The discovery of water ice on the
Moon's craters located in permanently shadowed regions, which can provide
drinking water, oxygen, and rocket fuel, is therefore of critical importance.
However, current methods to access lunar ice deposits are limited. While rovers
have been used to explore the lunar surface for decades, they face significant
challenges in navigating harsh terrains, such as permanently shadowed craters,
due to the high risk of immobilization. This report introduces COBRA (Crater
Observing Bio-inspired Rolling Articulator), a multi-modal snake-style robot
designed to overcome mobility challenges in Shackleton Crater's rugged
environment. COBRA combines slithering and tumbling locomotion to adapt to
various crater terrains. In snake mode, it uses sidewinding to traverse flat or
low inclined surfaces, while in tumbling mode, it forms a circular barrel by
linking its head and tail, enabling rapid movement with minimal energy on steep
slopes. Equipped with an onboard computer, stereo camera, inertial measurement
unit, and joint encoders, COBRA facilitates real-time data collection and
autonomous operation. This paper highlights COBRAs robustness and efficiency in
navigating extreme terrains through both simulations and experimental
validation.

</details>


### [6] [OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation](https://arxiv.org/abs/2509.19480)
*Noriaki Hirose,Catherine Glossop,Dhruv Shah,Sergey Levine*

Main category: cs.RO

TL;DR: 提出了一种用于机器人基础模型的训练框架OmniVLA，支持多种目标模态（2D位姿、第一人称图像、自然语言）的视觉导航，通过随机模态融合策略实现强泛化能力和对新模态的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人导航策略通常只针对单一模态进行训练，限制了在现实场景中的适应性。人类能够灵活解释和组合不同形式的目标规范，因此需要开发能够处理多种目标模态的导航策略。

Method: 使用高容量的视觉-语言-动作（VLA）骨干网络，通过随机模态融合策略训练三种主要目标模态（2D位姿、第一人称图像、自然语言）及其组合，扩大可用数据集并鼓励策略学习更丰富的几何、语义和视觉表示。

Result: OmniVLA模型在未见环境中表现出强泛化能力，对稀缺模态具有鲁棒性，能够遵循新的自然语言指令。在多模态基准测试中优于专业基线模型，为新模态和任务的微调提供了灵活基础。

Conclusion: OmniVLA为实现广泛泛化和灵活导航策略迈出了重要一步，为构建全模态机器人基础模型提供了可扩展的路径，展示了向更通用机器人导航系统发展的潜力。

Abstract: Humans can flexibly interpret and compose different goal specifications, such
as language instructions, spatial coordinates, or visual references, when
navigating to a destination. In contrast, most existing robotic navigation
policies are trained on a single modality, limiting their adaptability to
real-world scenarios where different forms of goal specification are natural
and complementary. In this work, we present a training framework for robotic
foundation models that enables omni-modal goal conditioning for vision-based
navigation. Our approach leverages a high-capacity vision-language-action (VLA)
backbone and trains with three primary goal modalities: 2D poses, egocentric
images, and natural language, as well as their combinations, through a
randomized modality fusion strategy. This design not only expands the pool of
usable datasets but also encourages the policy to develop richer geometric,
semantic, and visual representations. The resulting model, OmniVLA, achieves
strong generalization to unseen environments, robustness to scarce modalities,
and the ability to follow novel natural language instructions. We demonstrate
that OmniVLA outperforms specialist baselines across modalities and offers a
flexible foundation for fine-tuning to new modalities and tasks. We believe
OmniVLA provides a step toward broadly generalizable and flexible navigation
policies, and a scalable path for building omni-modal robotic foundation
models. We present videos showcasing OmniVLA performance and will release its
checkpoints and training code on our project page.

</details>


### [7] [Supercomputing for High-speed Avoidance and Reactive Planning in Robots](https://arxiv.org/abs/2509.19486)
*Kieran S. Lachmansingh,José R. González-Estrada,Ryan E. Grant,Matthew K. X. J. Pan*

Main category: cs.RO

TL;DR: SHARP是一个概念验证研究，展示了高性能计算如何实现毫秒级响应的机器人控制。通过将轨迹规划任务卸载到HPC集群，在7自由度机械臂躲避高速泡沫弹的测试中，实现了22.9ms（本地）和30.0ms（远程）的平均规划延迟，成功率分别为84%和88%。


<details>
  <summary>Details</summary>
Motivation: 现代机器人在人机共享工作空间中需要更高的反应能力，但机载处理器受到尺寸、功耗和成本的限制。利用HPC的并行计算能力来解决实时机器人控制中的规划瓶颈问题。

Method: 使用MPI在本地和远程HPC集群上实现并行化多目标A*搜索算法，进行轨迹规划。在7自由度机械臂躲避高速泡沫弹的应力测试场景中进行评估。

Result: 系统实现了平均规划延迟22.9ms（本地）和30.0ms（远程，约300公里外），躲避成功率分别为84%和88%。当往返延迟保持在几十毫秒范围内时，HPC端计算不再是瓶颈。

Conclusion: SHARP证明了HPC卸载作为实现动态环境中可靠、反应灵敏机器人的可行途径。提出了混合控制架构：低级反射保持机载以确保安全，而突发性、高吞吐量的规划任务则卸载到HPC以实现可扩展性。

Abstract: This paper presents SHARP (Supercomputing for High-speed Avoidance and
Reactive Planning), a proof-of-concept study demonstrating how high-performance
computing (HPC) can enable millisecond-scale responsiveness in robotic control.
While modern robots face increasing demands for reactivity in human--robot
shared workspaces, onboard processors are constrained by size, power, and cost.
Offloading to HPC offers massive parallelism for trajectory planning, but its
feasibility for real-time robotics remains uncertain due to network latency and
jitter. We evaluate SHARP in a stress-test scenario where a 7-DOF manipulator
must dodge high-speed foam projectiles. Using a parallelized multi-goal A*
search implemented with MPI on both local and remote HPC clusters, the system
achieves mean planning latencies of 22.9 ms (local) and 30.0 ms (remote, ~300
km away), with avoidance success rates of 84% and 88%, respectively. These
results show that when round-trip latency remains within the
tens-of-milliseconds regime, HPC-side computation is no longer the bottleneck,
enabling avoidance well below human reaction times. The SHARP results motivate
hybrid control architectures: low-level reflexes remain onboard for safety,
while bursty, high-throughput planning tasks are offloaded to HPC for
scalability. By reporting per-stage timing and success rates, this study
provides a reproducible template for assessing real-time feasibility of
HPC-driven robotics. Collectively, SHARP reframes HPC offloading as a viable
pathway toward dependable, reactive robots in dynamic environments.

</details>


### [8] [A Bimanual Gesture Interface for ROS-Based Mobile Manipulators Using TinyML and Sensor Fusion](https://arxiv.org/abs/2509.19521)
*Najeeb Ahmed Bhuiyan,M. Nasimul Huq,Sakib H. Chowdhury,Rahul Mangharam*

Main category: cs.RO

TL;DR: 本文提出了一种基于双手机势控制的移动机械臂系统，通过集成TinyML、频谱分析和传感器融合技术，解决了手势控制在可靠性、效率和直观性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统手势控制在移动机械臂应用中存在可靠性差、效率低和不够直观的问题，需要一种能够同时支持导航和操作的实时低功耗解决方案。

Method: 采用双手机势界面：左手通过加速度计和弯曲传感器捕捉倾斜和手指弯曲来控制移动底座导航；右手IMU信号通过频谱分析和轻量级神经网络分类来控制7自由度Kinova Gen3机械臂。系统基于ROS框架实现多模态传感器融合。

Result: 该系统实现了同时导航和操作的协调控制，相比顺序方法提高了效率和协调性。具备实时低功耗手势识别能力，支持稳健的多模态传感器融合。

Conclusion: 该方法为人机交互在工业自动化、辅助机器人和危险环境中的应用提供了成本效益高、开源且具有实际部署潜力的解决方案，为进一步优化奠定了基础。

Abstract: Gesture-based control for mobile manipulators faces persistent challenges in
reliability, efficiency, and intuitiveness. This paper presents a dual-hand
gesture interface that integrates TinyML, spectral analysis, and sensor fusion
within a ROS framework to address these limitations. The system uses left-hand
tilt and finger flexion, captured using accelerometer and flex sensors, for
mobile base navigation, while right-hand IMU signals are processed through
spectral analysis and classified by a lightweight neural network. This pipeline
enables TinyML-based gesture recognition to control a 7-DOF Kinova Gen3
manipulator. By supporting simultaneous navigation and manipulation, the
framework improves efficiency and coordination compared to sequential methods.
Key contributions include a bimanual control architecture, real-time low-power
gesture recognition, robust multimodal sensor fusion, and a scalable ROS-based
implementation. The proposed approach advances Human-Robot Interaction (HRI)
for industrial automation, assistive robotics, and hazardous environments,
offering a cost-effective, open-source solution with strong potential for
real-world deployment and further optimization.

</details>


### [9] [Bioinspired SLAM Approach for Unmanned Surface Vehicle](https://arxiv.org/abs/2509.19522)
*Fabio Coelho,Joao Victor T. Borges,Paulo Padrao,Jose Fuentes,Ramon R. Costa,Liu Hsu,Leonardo Bobadilla*

Main category: cs.RO

TL;DR: OpenRatSLAM2是一个基于啮齿动物海马体计算模型的生物启发式SLAM框架新版本，提供低计算成本的视觉-惯性SLAM，适用于GPS缺失环境。


<details>
  <summary>Details</summary>
Motivation: 开发适用于GPS缺失环境（特别是无人水面艇USV）的低成本SLAM系统，填补RatSLAM在USV应用上的空白。

Method: 采用ROS2架构，结合视觉和惯性传感器数据，基于生物启发式算法进行SLAM，使用Hausdorff距离与地面真实数据对比评估轨迹精度。

Result: 算法能够生成半度量地图，误差范围在大多数机器人应用可接受范围内，首次在USV上成功应用RatSLAM。

Conclusion: OpenRatSLAM2为GPS缺失环境提供了一种有效的低成本SLAM解决方案，特别适用于水道环境下的无人水面艇导航。

Abstract: This paper presents OpenRatSLAM2, a new version of OpenRatSLAM - a
bioinspired SLAM framework based on computational models of the rodent
hippocampus. OpenRatSLAM2 delivers low-computation-cost visual-inertial based
SLAM, suitable for GPS-denied environments. Our contributions include a
ROS2-based architecture, experimental results on new waterway datasets, and
insights into system parameter tuning. This work represents the first known
application of RatSLAM on USVs. The estimated trajectory was compared with
ground truth data using the Hausdorff distance. The results show that the
algorithm can generate a semimetric map with an error margin acceptable for
most robotic applications.

</details>


### [10] [Real-Time Reinforcement Learning for Dynamic Tasks with a Parallel Soft Robot](https://arxiv.org/abs/2509.19525)
*James Avtges,Jake Ketchum,Millicent Schlafly,Helena Young,Taekyoung Kim,Allison Pinosky,Ryan L. Truby,Todd D. Murphey*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的单次部署学习方法，用于软机器人的实时动态平衡控制，能够在硬件上快速学习控制策略，甚至在部分执行器损坏的情况下仍能保持性能。


<details>
  <summary>Details</summary>
Motivation: 软机器人的闭环控制面临挑战，非线性响应、迟滞效应和大变形使得传统分析方法难以应用。传统控制方法为避免非线性问题而未能充分利用配置空间，而数据驱动方法如强化学习又受限于样本效率和初始化不一致性。

Method: 使用基于电机驱动剪切拉胀结构的3D打印软执行器构建变形Stewart平台，引入基于已知平衡点扩展邻域的课程学习方法，采用最大扩散强化学习在单次硬件部署中学习动态平衡控制策略。

Result: 在单次部署中，仅需15分钟无先验数据训练，即可实现任意坐标的动态平衡控制。即使一半执行器被破坏（通过诱导屈曲和切割），系统仍能学习并保持与完整平台几乎相同的性能。

Conclusion: 硬件上的单次学习使软机器人系统能够在现实世界中可靠学习，这将推动更多样化、能力更强的软机器人发展。

Abstract: Closed-loop control remains an open challenge in soft robotics. The nonlinear
responses of soft actuators under dynamic loading conditions limit the use of
analytic models for soft robot control. Traditional methods of controlling soft
robots underutilize their configuration spaces to avoid nonlinearity,
hysteresis, large deformations, and the risk of actuator damage. Furthermore,
episodic data-driven control approaches such as reinforcement learning (RL) are
traditionally limited by sample efficiency and inconsistency across
initializations. In this work, we demonstrate RL for reliably learning control
policies for dynamic balancing tasks in real-time single-shot hardware
deployments. We use a deformable Stewart platform constructed using parallel,
3D-printed soft actuators based on motorized handed shearing auxetic (HSA)
structures. By introducing a curriculum learning approach based on expanding
neighborhoods of a known equilibrium, we achieve reliable single-deployment
balancing at arbitrary coordinates. In addition to benchmarking the performance
of model-based and model-free methods, we demonstrate that in a single
deployment, Maximum Diffusion RL is capable of learning dynamic balancing after
half of the actuators are effectively disabled, by inducing buckling and by
breaking actuators with bolt cutters. Training occurs with no prior data, in as
fast as 15 minutes, with performance nearly identical to the fully-intact
platform. Single-shot learning on hardware facilitates soft robotic systems
reliably learning in the real world and will enable more diverse and capable
soft robots.

</details>


### [11] [Autonomous Elemental Characterization Enabled by a Low Cost Robotic Platform Built Upon a Generalized Software Architecture](https://arxiv.org/abs/2509.19541)
*Xuan Cao,Yuxin Wu,Michael L. Whittaker*

Main category: cs.RO

TL;DR: 本文提出了一种用于科学实验室机器人系统的软件架构，通过双层的动作服务器设计实现用户友好的Web界面和ROS行为树任务规划，并构建了一个低成本的开源机器人平台用于矿物和材料样品的自动化表征。


<details>
  <summary>Details</summary>
Motivation: 当前科学实验室中机器人应用较少，主要由于缺乏通用方法和硬件成本高昂。本文旨在开发一种既能降低成本又能保持通用性的自动化表征方法。

Method: 采用双层的动作服务器设计，结合Socket.IO和ROS，实现Web前端用户界面和ROS行为树任务规划。构建了基于开源低成本三轴数控龙门系统的机器人平台，集成手持LIBS分析仪进行自动化2D化学映射。

Result: 成功实现了对含锂辉石伟晶岩岩心样品表面的1071点密集高光谱映射，采集速率为1520比特/秒。自动化LIBS扫描能够在实验室中进行受控的化学定量分析。

Conclusion: 该自动化系统能够将现场测量与实验室分析联系起来，为锂基电池材料供应链中的资源勘探和加工步骤提供支持。

Abstract: Despite the rapidly growing applications of robots in industry, the use of
robots to automate tasks in scientific laboratories is less prolific due to
lack of generalized methodologies and high cost of hardware. This paper focuses
on the automation of characterization tasks necessary for reducing cost while
maintaining generalization, and proposes a software architecture for building
robotic systems in scientific laboratory environment. A dual-layer (Socket.IO
and ROS) action server design is the basic building block, which facilitates
the implementation of a web-based front end for user-friendly operations and
the use of ROS Behavior Tree for convenient task planning and execution. A
robotic platform for automating mineral and material sample characterization is
built upon the architecture, with an open source, low-cost three-axis computer
numerical control gantry system serving as the main robot. A handheld laser
induced breakdown spectroscopy (LIBS) analyzer is integrated with a 3D printed
adapter, enabling automated 2D chemical mapping. We demonstrate the utility of
automated chemical mapping by scanning of the surface of a spodumene-bearing
pegmatite core sample with a 1071-point dense hyperspectral map acquired at a
rate of 1520 bits per second. Automated LIBS scanning enables controlled
chemical quantification in the laboratory that complements field-based
measurements acquired with the same handheld device, linking resource
exploration and processing steps in the supply chain for lithium-based battery
materials.

</details>


### [12] [RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots](https://arxiv.org/abs/2509.19545)
*Min Dai,Aaron D. Ames*

Main category: cs.RO

TL;DR: RoMoCo是一个开源的C++工具箱，用于双足和人形机器人的降阶模型规划器和全身控制器的合成与评估。


<details>
  <summary>Details</summary>
Motivation: 开发一个统一的模块化架构，将最先进的规划器和全身运动控制器整合到一致的API下，实现快速原型设计和可重复的基准测试。

Method: 利用降阶模型进行平台无关的步态生成，采用模块化架构设计，支持多种机器人的灵活控制器开发。

Result: 在Cassie、Unitree H1和G1机器人上进行了广泛的仿真测试，并在Cassie和G1人形机器人上进行了硬件实验验证。

Conclusion: RoMoCo展示了其多功能性和高性能，能够有效支持双足和人形机器人的规划与控制开发。

Abstract: We present RoMoCo, an open-source C++ toolbox for the synthesis and
evaluation of reduced-order model-based planners and whole-body controllers for
bipedal and humanoid robots. RoMoCo's modular architecture unifies
state-of-the-art planners and whole-body locomotion controllers under a
consistent API, enabling rapid prototyping and reproducible benchmarking. By
leveraging reduced-order models for platform-agnostic gait generation, RoMoCo
enables flexible controller design across diverse robots. We demonstrate its
versatility and performance through extensive simulations on the Cassie,
Unitree H1, and G1 robots, and validate its real-world efficacy with hardware
experiments on the Cassie and G1 humanoids.

</details>


### [13] [AnySafe: Adapting Latent Safety Filters at Runtime via Safety Constraint Parameterization in the Latent Space](https://arxiv.org/abs/2509.19555)
*Sankalp Agrawal,Junwon Seo,Kensuke Nakamura,Ran Tian,Andrea Bajcsy*

Main category: cs.RO

TL;DR: 本文提出了一种约束参数化的潜在安全过滤器，能够在运行时适应任意安全约束，解决了传统方法需要预先固定安全约束的限制。


<details>
  <summary>Details</summary>
Motivation: 现有的潜在安全过滤器方法假设安全约束是预先已知且在部署期间保持固定的，这限制了安全过滤器在不同场景下的适应性。

Method: 通过基于约束图像编码的条件化定义安全约束，使用潜在空间相似性度量，并通过保形校准来对齐失败相似性概念，使安全过滤器能够在世界模型的想象中完全训练。

Result: 在基于视觉的控制任务中，该方法能够在运行时通过条件化用户指定的约束图像编码来适应，而不会牺牲性能。

Conclusion: 该方法实现了对任意安全约束的运行时适应，提高了安全过滤器的灵活性和实用性。

Abstract: Recent works have shown that foundational safe control methods, such as
Hamilton-Jacobi (HJ) reachability analysis, can be applied in the latent space
of world models. While this enables the synthesis of latent safety filters for
hard-to-model vision-based tasks, they assume that the safety constraint is
known a priori and remains fixed during deployment, limiting the safety
filter's adaptability across scenarios. To address this, we propose
constraint-parameterized latent safety filters that can adapt to user-specified
safety constraints at runtime. Our key idea is to define safety constraints by
conditioning on an encoding of an image that represents a constraint, using a
latent-space similarity measure. The notion of similarity to failure is aligned
in a principled way through conformal calibration, which controls how closely
the system may approach the constraint representation. The parameterized safety
filter is trained entirely within the world model's imagination, treating any
image seen by the model as a potential test-time constraint, thereby enabling
runtime adaptation to arbitrary safety constraints. In simulation and hardware
experiments on vision-based control tasks with a Franka manipulator, we show
that our method adapts at runtime by conditioning on the encoding of
user-specified constraint images, without sacrificing performance. Video
results can be found on https://any-safe.github.io

</details>


### [14] [Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action](https://arxiv.org/abs/2509.19571)
*Sacha Morin,Kumaraditya Gupta,Mahtab Sandhu,Charlie Gauthier,Francesco Argenziano,Kirsty Ellis,Liam Paull*

Main category: cs.RO

TL;DR: ASP是一个基于场景表示的智能体框架，通过语义、空间和功能查询能力实现语言条件化的机器人策略，能够零样本执行开放词汇查询并处理复杂技能。


<details>
  <summary>Details</summary>
Motivation: 解决开放自然语言查询执行问题，克服现有视觉-语言-动作模型在复杂指令和新场景下的局限性。

Method: 设计显式场景表示作为机器人与世界之间的可查询接口，利用查询结果指导下游运动规划，通过对象功能推理处理复杂技能。

Result: 在桌面操作问题上与VLA模型对比实验，展示ASP能够处理房间级查询，通过功能引导导航和扩展场景表示实现规模化应用。

Conclusion: ASP框架通过利用现代场景表示的高级查询能力，实现了更强大的语言条件化机器人策略，为复杂开放词汇查询执行提供了有效解决方案。

Abstract: Executing open-ended natural language queries is a core problem in robotics.
While recent advances in imitation learning and vision-language-actions models
(VLAs) have enabled promising end-to-end policies, these models struggle when
faced with complex instructions and new scenes. An alternative is to design an
explicit scene representation as a queryable interface between the robot and
the world, using query results to guide downstream motion planning. In this
work, we present Agentic Scene Policies (ASP), an agentic framework that
leverages the advanced semantic, spatial, and affordance-based querying
capabilities of modern scene representations to implement a capable
language-conditioned robot policy. ASP can execute open-vocabulary queries in a
zero-shot manner by explicitly reasoning about object affordances in the case
of more complex skills. Through extensive experiments, we compare ASP with VLAs
on tabletop manipulation problems and showcase how ASP can tackle room-level
queries through affordance-guided navigation, and a scaled-up scene
representation. (Project page:
https://montrealrobotics.ca/agentic-scene-policies.github.io/)

</details>


### [15] [Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning](https://arxiv.org/abs/2509.19573)
*Zachary Olkin,Kejun Li,William D. Compton,Aaron D. Ames*

Main category: cs.RO

TL;DR: 本文提出了一种结合非线性控制理论和强化学习的方法CLF-RL，用于人形机器人的动态运动控制，特别是跑步行为。该方法通过控制Lyapunov函数和优化动态参考轨迹来塑造奖励函数，无需手动调整启发式奖励项。


<details>
  <summary>Details</summary>
Motivation: 人形机器人的动态行为（如跑步）需要既鲁棒又精确的控制器，但设计具有挑战性。传统控制方法难以实时处理非线性和混合动力学，而强化学习虽然能处理复杂动力学，但需要精心设计的奖励函数。

Method: CLF-RL方法将控制Lyapunov函数（CLFs）和优化的动态参考轨迹嵌入到强化学习训练过程中，通过这种方式塑造奖励函数，鼓励可证明的稳定性并提供有意义的中间奖励。

Result: 该方法在跑步机上和室外环境中都能可靠运行，对躯干和脚部的干扰表现出鲁棒性。仅使用机载传感器就能实现准确的全局参考跟踪。

Conclusion: CLF-RL方法为人形机器人的动态运动控制提供了一种有效解决方案，是实现全自主堆栈集成的重要一步。

Abstract: Achieving highly dynamic behaviors on humanoid robots, such as running,
requires controllers that are both robust and precise, and hence difficult to
design. Classical control methods offer valuable insight into how such systems
can stabilize themselves, but synthesizing real-time controllers for nonlinear
and hybrid dynamics remains challenging. Recently, reinforcement learning (RL)
has gained popularity for locomotion control due to its ability to handle these
complex dynamics. In this work, we embed ideas from nonlinear control theory,
specifically control Lyapunov functions (CLFs), along with optimized dynamic
reference trajectories into the reinforcement learning training process to
shape the reward. This approach, CLF-RL, eliminates the need to handcraft and
tune heuristic reward terms, while simultaneously encouraging certifiable
stability and providing meaningful intermediate rewards to guide learning. By
grounding policy learning in dynamically feasible trajectories, we expand the
robot's dynamic capabilities and enable running that includes both flight and
single support phases. The resulting policy operates reliably on a treadmill
and in outdoor environments, demonstrating robustness to disturbances applied
to the torso and feet. Moreover, it achieves accurate global reference tracking
utilizing only on-board sensors, making a critical step toward integrating
these dynamic motions into a full autonomy stack.

</details>


### [16] [Terra: Hierarchical Terrain-Aware 3D Scene Graph for Task-Agnostic Outdoor Mapping](https://arxiv.org/abs/2509.19579)
*Chad R. Samuelson,Abigail Austin,Seth Knoop,Blake Romrell,Gabriel R. Slade,Timothy W. McLain,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: 提出一种结合室内3D场景图技术与户外几何映射和地形感知推理的新方法，生成地形感知的位置节点和层次化组织的户外环境区域，构建轻量级的任务无关度量-语义稀疏地图和3DSG。


<details>
  <summary>Details</summary>
Motivation: 户外智能自主机器人操作需要具有充分表达能力的环境地图。传统几何地图方法缺乏语义理解和组织，无法支持高级机器人推理。3D场景图(3DSG)通过整合几何、拓扑和语义关系来解决这一限制。

Method: 将室内3DSG技术与标准户外几何映射和地形感知推理相结合，生成地形感知的位置节点和层次化组织的区域，构建任务无关的度量-语义稀疏地图并从中构建3DSG。

Result: 评估表明该方法在物体检索方面与最先进的基于相机的3DSG方法相当，在区域分类方面超越它们，同时保持内存高效。在仿真和真实环境中验证了其在物体检索和区域监控等机器人任务中的有效性。

Conclusion: 该方法成功地将3DSG技术扩展到户外环境，提供了轻量级且高效的解决方案，支持多样化的自主机器人操作任务。

Abstract: Outdoor intelligent autonomous robotic operation relies on a sufficiently
expressive map of the environment. Classical geometric mapping methods retain
essential structural environment information, but lack a semantic understanding
and organization to allow high-level robotic reasoning. 3D scene graphs (3DSGs)
address this limitation by integrating geometric, topological, and semantic
relationships into a multi-level graph-based map. Outdoor autonomous operations
commonly rely on terrain information either due to task-dependence or the
traversability of the robotic platform. We propose a novel approach that
combines indoor 3DSG techniques with standard outdoor geometric mapping and
terrain-aware reasoning, producing terrain-aware place nodes and hierarchically
organized regions for outdoor environments. Our method generates a
task-agnostic metric-semantic sparse map and constructs a 3DSG from this map
for downstream planning tasks, all while remaining lightweight for autonomous
robotic operation. Our thorough evaluation demonstrates our 3DSG method
performs on par with state-of-the-art camera-based 3DSG methods in object
retrieval and surpasses them in region classification while remaining memory
efficient. We demonstrate its effectiveness in diverse robotic tasks of object
retrieval and region monitoring in both simulation and real-world environments.

</details>


### [17] [From Space to Time: Enabling Adaptive Safety with Learned Value Functions via Disturbance Recasting](https://arxiv.org/abs/2509.19597)
*Sander Tonkens,Nikhil Uday Shinde,Azra Begzadić,Michael C. Yip,Jorge Cortés,Sylvia L. Herbert*

Main category: cs.RO

TL;DR: SPACE2TIME是一种安全过滤器方法，能够将空间变化的扰动重新参数化为时间变化，从而在未知、空间变化的扰动环境下安全部署离线学习的安全过滤器。


<details>
  <summary>Details</summary>
Motivation: 当前基于值函数的安全过滤器方法假设对所有可能的模型失配源有详细先验知识，这在现实世界中很少可用。无人机在复杂环境中会遇到空间变化的扰动，如载荷-无人机交互、湍流等。

Method: SPACE2TIME的关键思想是将空间变化的扰动重新参数化为时间变化，使得在在线操作期间能够使用预计算的值函数。

Result: 通过在四旋翼无人机上进行广泛的仿真和硬件实验验证，SPACE2TIME相比基线方法表现出显著改进。

Conclusion: 该方法能够在未知、空间变化的扰动环境下实现安全且自适应的离线学习安全过滤器部署。

Abstract: The widespread deployment of autonomous systems in safety-critical
environments such as urban air mobility hinges on ensuring reliable,
performant, and safe operation under varying environmental conditions. One such
approach, value function-based safety filters, minimally modifies a nominal
controller to ensure safety. Recent advances leverage offline learned value
functions to scale these safety filters to high-dimensional systems. However,
these methods assume detailed priors on all possible sources of model mismatch,
in the form of disturbances in the environment -- information that is rarely
available in real world settings. Even in well-mapped environments like urban
canyons or industrial sites, drones encounter complex, spatially-varying
disturbances arising from payload-drone interaction, turbulent airflow, and
other environmental factors. We introduce SPACE2TIME, which enables safe and
adaptive deployment of offline-learned safety filters under unknown,
spatially-varying disturbances. The key idea is to reparameterize spatial
variations in disturbance as temporal variations, enabling the use of
precomputed value functions during online operation. We validate SPACE2TIME on
a quadcopter through extensive simulations and hardware experiments,
demonstrating significant improvement over baselines.

</details>


### [18] [Look as You Leap: Planning Simultaneous Motion and Perception for High-DOF Robots](https://arxiv.org/abs/2509.19610)
*Qingxi Meng,Emiliano Flores,Carlos Quintero-Peña,Peizhu Qian,Zachary Kingston,Shannan K. Hamlin,Vaibhav Unhelkar,Lydia E. Kavraki*

Main category: cs.RO

TL;DR: 本文提出了一种GPU并行化的感知评分引导概率路线图规划器（PS-PRM），用于高自由度机器人在动态环境中的运动规划，同时考虑感知任务质量和避障需求。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，机器人需要同时完成导航和感知任务，但这两个目标往往存在冲突。现有方法要么不考虑障碍物，要么只适用于低自由度机器人，或者使用简化的感知模型。在动态环境中，直接评估感知质量（如物体检测置信度）通常计算成本高昂或不可行。

Method: 提出PS-PRM方法，使用学习的神经网络代理模型来近似感知评分，并利用GPU并行化实现在动态环境中的高效在线重规划。该方法将感知任务质量估计显式地纳入高自由度机器人的运动规划中。

Result: 在仿真和真实机器人实验中，该方法在高自由度机器人上优于基线方法，在静态和动态环境中都表现出色。

Conclusion: PS-PRM方法能够有效解决高自由度机器人在动态环境中同时完成导航和感知任务的挑战，通过GPU并行化和学习模型实现了高效的在线重规划能力。

Abstract: In this work, we address the problem of planning robot motions for a
high-degree-of-freedom (DoF) robot that effectively achieves a given perception
task while the robot and the perception target move in a dynamic environment.
Achieving navigation and perception tasks simultaneously is challenging, as
these objectives often impose conflicting requirements. Existing methods that
compute motion under perception constraints fail to account for obstacles, are
designed for low-DoF robots, or rely on simplified models of perception.
Furthermore, in dynamic real-world environments, robots must replan and react
quickly to changes and directly evaluating the quality of perception (e.g.,
object detection confidence) is often expensive or infeasible at runtime. This
problem is especially important in human-centered environments such as homes
and hospitals, where effective perception is essential for safe and reliable
operation. To address these challenges, we propose a GPU-parallelized
perception-score-guided probabilistic roadmap planner with a neural surrogate
model (PS-PRM). The planner explicitly incorporates the estimated quality of a
perception task into motion planning for high-DoF robots. Our method uses a
learned model to approximate perception scores and leverages GPU parallelism to
enable efficient online replanning in dynamic settings. We demonstrate that our
planner, evaluated on high-DoF robots, outperforms baseline methods in both
static and dynamic environments in both simulation and real-robot experiments.

</details>


### [19] [EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data](https://arxiv.org/abs/2509.19626)
*Ryan Punamiya,Dhruv Patel,Patcharapong Aphiwetsa,Pranav Kuppili,Lawrence Y. Zhu,Simar Kareer,Judy Hoffman,Danfei Xu*

Main category: cs.RO

TL;DR: EgoBridge是一个统一的协同训练框架，通过域适应方法对齐人类和机器人数据之间的策略潜在空间，显著提高了机器人操作的模仿学习性能。


<details>
  <summary>Details</summary>
Motivation: 利用人类自我中心体验数据扩展机器人端到端模仿学习，但人类和机器人之间存在视觉外观、传感器模态和运动学的显著域差距，阻碍了知识迁移。

Method: 使用基于最优传输（OT）的域适应方法，在联合策略潜在特征和动作上测量差异，学习既能在人类和机器人域之间对齐又能保留策略学习关键动作相关信息的观察表示。

Result: 在三个真实世界单臂和双手操作任务中，相比人类增强的跨具身基线方法，EgoBridge实现了44%的绝对策略成功率提升，并能泛化到仅在人类数据中见过的新物体、场景和任务。

Conclusion: EgoBridge通过有效的域对齐方法成功解决了人类-机器人数据迁移的域差距问题，为利用人类数据扩展机器人学习提供了有效解决方案。

Abstract: Egocentric human experience data presents a vast resource for scaling up
end-to-end imitation learning for robotic manipulation. However, significant
domain gaps in visual appearance, sensor modalities, and kinematics between
human and robot impede knowledge transfer. This paper presents EgoBridge, a
unified co-training framework that explicitly aligns the policy latent spaces
between human and robot data using domain adaptation. Through a measure of
discrepancy on the joint policy latent features and actions based on Optimal
Transport (OT), we learn observation representations that not only align
between the human and robot domain but also preserve the action-relevant
information critical for policy learning. EgoBridge achieves a significant
absolute policy success rate improvement by 44% over human-augmented
cross-embodiment baselines in three real-world single-arm and bimanual
manipulation tasks. EgoBridge also generalizes to new objects, scenes, and
tasks seen only in human data, where baselines fail entirely. Videos and
additional information can be found at https://ego-bridge.github.io

</details>


### [20] [Minimalistic Autonomous Stack for High-Speed Time-Trial Racing](https://arxiv.org/abs/2509.19636)
*Mahmoud Ali,Hassan Jardali,Youwei Yu,Durgakant Pushp,Lantao Liu*

Main category: cs.RO

TL;DR: 提出了一种简约的自动驾驶赛车堆栈，可在有限赛道测试条件下实现高速计时赛，仅用11小时赛道练习就达到了206公里/小时的最高速度。


<details>
  <summary>Details</summary>
Motivation: 传统自动驾驶赛车开发需要大量赛道测试时间，但专业赛道资源有限，限制了实际验证机会。本文旨在开发一个能快速部署、集成效率高且最小化赛道测试需求的系统。

Method: 采用简约化的自动驾驶赛车堆栈设计，强调系统快速部署和高效集成，通过最小化的实地赛道测试进行验证。

Result: 在真实赛道上验证成功，仅用11小时赛道练习（总里程325公里）就实现了206公里/小时的最高速度，并提供了跟踪精度、车辆动力学和安全性的系统性能分析。

Conclusion: 该研究为受赛道访问限制的团队提供了快速开发和部署自动驾驶赛车堆栈的可行方案，证明了在有限测试条件下实现高性能自动驾驶赛车的可能性。

Abstract: Autonomous racing has seen significant advancements, driven by competitions
such as the Indy Autonomous Challenge (IAC) and the Abu Dhabi Autonomous Racing
League (A2RL). However, developing an autonomous racing stack for a full-scale
car is often constrained by limited access to dedicated test tracks,
restricting opportunities for real-world validation. While previous work
typically requires extended development cycles and significant track time, this
paper introduces a minimalistic autonomous racing stack for high-speed
time-trial racing that emphasizes rapid deployment and efficient system
integration with minimal on-track testing. The proposed stack was validated on
real speedways, achieving a top speed of 206 km/h within just 11 hours'
practice run on the track with 325 km in total. Additionally, we present the
system performance analysis, including tracking accuracy, vehicle dynamics, and
safety considerations, offering insights for teams seeking to rapidly develop
and deploy an autonomous racing stack with limited track access.

</details>


### [21] [RoboSSM: Scalable In-context Imitation Learning via State-Space Models](https://arxiv.org/abs/2509.19658)
*Youngju Yoo,Jiaheng Hu,Yifeng Zhu,Bo Liu,Qiang Liu,Roberto Martín-Martín,Peter Stone*

Main category: cs.RO

TL;DR: RoboSSM提出了一种基于状态空间模型（SSM）的可扩展上下文模仿学习方法，用Longhorn SSM替代Transformer，解决了Transformer在长提示序列下的计算限制和外推性能不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文模仿学习（ICIL）方法依赖Transformer，但Transformer存在计算限制，且在训练时未见过的长提示序列下表现不佳。需要一种更高效且能处理长上下文的方法。

Method: RoboSSM使用Longhorn状态空间模型替代Transformer，该模型提供线性时间推理和强大的外推能力，特别适合处理长上下文提示。在LIBERO基准上进行评估。

Result: 实验表明RoboSSM能有效外推到不同数量的上下文演示，在未见任务上表现优异，在长时域场景中保持鲁棒性，性能优于基于Transformer的ICIL基线方法。

Conclusion: SSM有潜力成为ICIL的高效可扩展骨干网络，RoboSSM展示了SSM在机器人模仿学习中的优势。

Abstract: In-context imitation learning (ICIL) enables robots to learn tasks from
prompts consisting of just a handful of demonstrations. By eliminating the need
for parameter updates at deployment time, this paradigm supports few-shot
adaptation to novel tasks. However, recent ICIL methods rely on Transformers,
which have computational limitations and tend to underperform when handling
longer prompts than those seen during training. In this work, we introduce
RoboSSM, a scalable recipe for in-context imitation learning based on
state-space models (SSM). Specifically, RoboSSM replaces Transformers with
Longhorn -- a state-of-the-art SSM that provides linear-time inference and
strong extrapolation capabilities, making it well-suited for long-context
prompts. We evaluate our approach on the LIBERO benchmark and compare it
against strong Transformer-based ICIL baselines. Experiments show that RoboSSM
extrapolates effectively to varying numbers of in-context demonstrations,
yields high performance on unseen tasks, and remains robust in long-horizon
scenarios. These results highlight the potential of SSMs as an efficient and
scalable backbone for ICIL. Our code is available at
https://github.com/youngjuY/RoboSSM.

</details>


### [22] [Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains](https://arxiv.org/abs/2509.19672)
*Dongzhe Zheng,Wenjie Mei*

Main category: cs.RO

TL;DR: 本文提出了一种记忆增强势场理论，通过整合历史经验来改进随机最优控制方法在非凸环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统随机最优控制方法在复杂非凸环境中容易陷入局部最优，无法有效利用历史轨迹数据来改进控制策略。

Method: 开发了记忆增强势场理论框架，动态构建基于记忆的势场来识别状态空间的关键拓扑特征，并实现了记忆增强模型预测路径积分控制器。

Result: 理论分析表明该方法具有非凸逃逸特性、渐近收敛性和计算效率，在挑战性非凸环境中表现出显著改进的性能。

Conclusion: 该框架为控制系统（特别是机器人动力学）提供了一种通用的基于经验的学习方法，增强了在复杂状态空间中导航的能力，无需专门的领域知识或大量离线训练。

Abstract: Stochastic optimal control methods often struggle in complex non-convex
landscapes, frequently becoming trapped in local optima due to their inability
to learn from historical trajectory data. This paper introduces
Memory-Augmented Potential Field Theory, a unified mathematical framework that
integrates historical experience into stochastic optimal control. Our approach
dynamically constructs memory-based potential fields that identify and encode
key topological features of the state space, enabling controllers to
automatically learn from past experiences and adapt their optimization
strategy. We provide a theoretical analysis showing that memory-augmented
potential fields possess non-convex escape properties, asymptotic convergence
characteristics, and computational efficiency. We implement this theoretical
framework in a Memory-Augmented Model Predictive Path Integral (MPPI)
controller that demonstrates significantly improved performance in challenging
non-convex environments. The framework represents a generalizable approach to
experience-based learning within control systems (especially robotic dynamics),
enhancing their ability to navigate complex state spaces without requiring
specialized domain knowledge or extensive offline training.

</details>


### [23] [Formal Safety Verification and Refinement for Generative Motion Planners via Certified Local Stabilization](https://arxiv.org/abs/2509.19688)
*Devesh Nath,Haoran Yin,Glen Chou*

Main category: cs.RO

TL;DR: 提出了一种基于学习生成运动规划器的形式化安全验证方法，通过小规模神经跟踪控制器稳定GMP生成的参考轨迹，然后对闭环动力学应用神经网络验证，从而在保持GMP表达能力的同时实现可验证的安全性保证。


<details>
  <summary>Details</summary>
Motivation: 生成运动规划器(GMPs)相比传统规划器具有优势，但由于神经网络验证工具只能处理几百个神经元，而GMPs通常包含数百万参数，因此验证其输出的安全性和动态可行性非常困难。

Method: 关键思路是通过小规模神经跟踪控制器来稳定从GMP采样的参考轨迹，然后对闭环动力学应用神经网络验证，生成可达集来严格认证闭环安全性，同时控制器保证动态可行性。基于此构建已验证GMP参考库，并在安全时在线部署以模仿原始GMP分布。

Result: 在多种规划器（包括扩散模型、流匹配和视觉语言模型）上进行了评估，在仿真（地面机器人和四旋翼）和硬件（差速驱动机器人）上均提高了安全性。

Conclusion: 该方法能够在保持GMP表达能力的同时实现形式化安全验证，无需重新训练即可提高安全性，适用于多种类型的生成运动规划器。

Abstract: We present a method for formal safety verification of learning-based
generative motion planners. Generative motion planners (GMPs) offer advantages
over traditional planners, but verifying the safety and dynamic feasibility of
their outputs is difficult since neural network verification (NNV) tools scale
only to a few hundred neurons, while GMPs often contain millions. To preserve
GMP expressiveness while enabling verification, our key insight is to imitate
the GMP by stabilizing references sampled from the GMP with a small neural
tracking controller and then applying NNV to the closed-loop dynamics. This
yields reachable sets that rigorously certify closed-loop safety, while the
controller enforces dynamic feasibility. Building on this, we construct a
library of verified GMP references and deploy them online in a way that
imitates the original GMP distribution whenever it is safe to do so, improving
safety without retraining. We evaluate across diverse planners, including
diffusion, flow matching, and vision-language models, improving safety in
simulation (on ground robots and quadcopters) and on hardware
(differential-drive robot).

</details>


### [24] [Diffusion-Based Impedance Learning for Contact-Rich Manipulation Tasks](https://arxiv.org/abs/2509.19696)
*Noah Geiger,Tamim Asfour,Neville Hogan,Johannes Lachner*

Main category: cs.RO

TL;DR: 本文提出了一种基于扩散模型的阻抗学习框架，结合Transformer和扩散模型重建模拟零力轨迹，通过能量估计器更新刚度和阻尼参数，实现实时扭矩控制和自主刚度适应。


<details>
  <summary>Details</summary>
Motivation: 传统学习方法擅长信息域的运动生成但不适合物理交互，而阻抗控制需要任务感知调参。本文旨在结合两个领域，实现物理AI的融合。

Method: 使用基于Transformer的扩散模型，通过交叉注意力机制处理外部力矩，重建模拟零力轨迹。引入SLERP-based四元数噪声调度器保证旋转的几何一致性。通过能量估计器更新阻抗参数，应用方向性规则减少非任务轴阻抗。

Result: 在仅有数万个样本的情况下，模型达到亚毫米级位置精度和亚度级旋转精度。在KUKA LBR iiwa机器人上实现实时扭矩控制，成功完成圆柱、方形和星形孔插入任务，成功率30/30。

Conclusion: 该框架是迈向物理AI的重要一步，将基于模型的物理交互控制与基于学习的轨迹生成方法相融合，所有代码已公开。

Abstract: Learning methods excel at motion generation in the information domain but are
not primarily designed for physical interaction in the energy domain. Impedance
Control shapes physical interaction but requires task-aware tuning by selecting
feasible impedance parameters. We present Diffusion-Based Impedance Learning, a
framework that combines both domains. A Transformer-based Diffusion Model with
cross-attention to external wrenches reconstructs a simulated Zero-Force
Trajectory (sZFT). This captures both translational and rotational task-space
behavior. For rotations, we introduce a novel SLERP-based quaternion noise
scheduler that ensures geometric consistency. The reconstructed sZFT is then
passed to an energy-based estimator that updates stiffness and damping
parameters. A directional rule is applied that reduces impedance along non task
axes while preserving rigidity along task directions. Training data were
collected for a parkour scenario and robotic-assisted therapy tasks using
teleoperation with Apple Vision Pro. With only tens of thousands of samples,
the model achieved sub-millimeter positional accuracy and sub-degree rotational
accuracy. Its compact model size enabled real-time torque control and
autonomous stiffness adaptation on a KUKA LBR iiwa robot. The controller
achieved smooth parkour traversal within force and velocity limits and 30/30
success rates for cylindrical, square, and star peg insertions without any
peg-specific demonstrations in the training data set. All code for the
Transformer-based Diffusion Model, the robot controller, and the Apple Vision
Pro telemanipulation framework is publicly available. These results mark an
important step towards Physical AI, fusing model-based control for physical
interaction with learning-based methods for trajectory generation.

</details>


### [25] [TopoCut: Learning Multi-Step Cutting with Spectral Rewards and Discrete Diffusion Policies](https://arxiv.org/abs/2509.19712)
*Liquan Wang,Jiangjie Bian,Eric Heiden,Animesh Garg*

Main category: cs.RO

TL;DR: TopoCut是一个用于多步骤机器人切割任务的综合基准，集成了高保真仿真环境和通用策略学习，解决了可变形物体切割中的拓扑行为复杂性、密集状态感知困难和切割结果评估问题。


<details>
  <summary>Details</summary>
Motivation: 可变形物体的机器人切割任务面临三大挑战：复杂的拓扑行为、密集物体状态感知困难，以及缺乏有效的切割结果评估方法。这些挑战限制了机器人切割技术的发展和应用。

Method: TopoCut包含三个核心组件：(1)基于粒子弹塑性求解器的高保真仿真环境，配备损伤驱动的拓扑发现机制；(2)结合拓扑发现和拉普拉斯-贝尔特拉米特征分析的姿态不变谱奖励模型；(3)基于粒子分数熵离散扩散策略(PDDP)的集成策略学习管道。

Result: 大量实验表明，TopoCut支持轨迹生成、可扩展学习、精确评估，并在不同物体几何形状、尺度、姿态和切割目标上展现出强大的泛化能力。

Conclusion: TopoCut为解决可变形物体切割任务提供了一个全面的基准平台，通过集成仿真、评估和策略学习，有效应对了该领域的核心挑战，为机器人切割技术的发展奠定了基础。

Abstract: Robotic manipulation tasks involving cutting deformable objects remain
challenging due to complex topological behaviors, difficulties in perceiving
dense object states, and the lack of efficient evaluation methods for cutting
outcomes. In this paper, we introduce TopoCut, a comprehensive benchmark for
multi-step robotic cutting tasks that integrates a cutting environment and
generalized policy learning. TopoCut is built upon three core components: (1)
We introduce a high-fidelity simulation environment based on a particle-based
elastoplastic solver with compliant von Mises constitutive models, augmented by
a novel damage-driven topology discovery mechanism that enables accurate
tracking of multiple cutting pieces. (2) We develop a comprehensive reward
design that integrates the topology discovery with a pose-invariant spectral
reward model based on Laplace-Beltrami eigenanalysis, facilitating consistent
and robust assessment of cutting quality. (3) We propose an integrated policy
learning pipeline, where a dynamics-informed perception module predicts
topological evolution and produces particle-wise, topology-aware embeddings to
support PDDP (Particle-based Score-Entropy Discrete Diffusion Policy) for
goal-conditioned policy learning. Extensive experiments demonstrate that
TopoCut supports trajectory generation, scalable learning, precise evaluation,
and strong generalization across diverse object geometries, scales, poses, and
cutting goals.

</details>


### [26] [Towards Autonomous Robotic Electrosurgery via Thermal Imaging](https://arxiv.org/abs/2509.19725)
*Naveed D. Riaziat,Joseph Chen,Axel Krieger,Jeremy D. Brown*

Main category: cs.RO

TL;DR: ThERMO系统通过热成像反馈智能控制电外科手术工具速度，在减少热损伤的同时平衡切割力，相比恒定速度方法显著提高了切割成功率并降低了峰值切割力。


<details>
  <summary>Details</summary>
Motivation: 电外科手术虽然能减少切割力和出血，但存在热损伤风险。专家依赖经验估计切割速度，缺乏量化参考。现有自主电外科手术主要使用恒定速度，无法适应组织特性、功率设置或工具类型的变化。

Method: 开发了ThERMO系统，利用热成像反馈信息，通过优化方法智能控制工具速度，在减少热损伤的同时平衡切割力。

Result: 在组织模型实验中，ThERMO将切割成功率提高了三倍，峰值切割力降低了一半。系统能响应环境变化，减少组织损伤，完成恒定速度方法会失败的任务。

Conclusion: ThERMO系统通过热成像反馈的智能速度控制，显著提升了电外科手术的安全性和有效性，为自主电外科手术提供了更可靠的解决方案。

Abstract: Electrosurgery is a surgical technique that can improve tissue cutting by
reducing cutting force and bleeding. However, electrosurgery adds a risk of
thermal injury to surrounding tissue. Expert surgeons estimate desirable
cutting velocities based on experience but have no quantifiable reference to
indicate if a particular velocity is optimal. Furthermore, prior demonstrations
of autonomous electrosurgery have primarily used constant tool velocity, which
is not robust to changes in electrosurgical tissue characteristics, power
settings, or tool type. Thermal imaging feedback provides information that can
be used to reduce thermal injury while balancing cutting force by controlling
tool velocity. We introduce Thermography for Electrosurgical Rate Modulation
via Optimization (ThERMO) to autonomously reduce thermal injury while balancing
cutting force by intelligently controlling tool velocity. We demonstrate ThERMO
in tissue phantoms and compare its performance to the constant velocity
approach. Overall, ThERMO improves cut success rate by a factor of three and
can reduce peak cutting force by a factor of two. ThERMO responds to varying
environmental disturbances, reduces damage to tissue, and completes cutting
tasks that would otherwise result in catastrophic failure for the constant
velocity approach.

</details>


### [27] [Simultaneous estimation of contact position and tool shape with high-dimensional parameters using force measurements and particle filtering](https://arxiv.org/abs/2509.19732)
*Kyo Kutsuzawa,Mitsuhiro Hayashibe*

Main category: cs.RO

TL;DR: 提出一种同时估计接触位置和工具形状的方法，使用网格表示高维工具形状，通过粒子滤波器避免直接处理高维参数空间。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多需要已知工具表面几何形状，而无需工具形状的方法要么估计时间过长，要么仅限于低维形状参数的工具。需要一种能处理高维工具形状的同时估计接触位置的方法。

Method: 使用粒子滤波器，每个粒子具有独立的工具形状参数，通过网格表示高维工具形状（超过1000维），避免直接处理高维参数空间。

Result: 通过曲面工具在平面上的仿真和实验验证，该方法能够同时估计工具形状和接触位置，提高了接触位置估计的准确性。

Conclusion: 所提出的方法能够有效同时估计接触位置和高维工具形状，为接触任务如装配和物体操作提供了更准确的接触状态估计能力。

Abstract: Estimating the contact state between a grasped tool and the environment is
essential for performing contact tasks such as assembly and object
manipulation. Force signals are valuable for estimating the contact state, as
they can be utilized even when the contact location is obscured by the tool.
Previous studies proposed methods for estimating contact positions using
force/torque signals; however, most methods require the geometry of the tool
surface to be known. Although several studies have proposed methods that do not
require the tool shape, these methods require considerable time for estimation
or are limited to tools with low-dimensional shape parameters. Here, we propose
a method for simultaneously estimating the contact position and tool shape,
where the tool shape is represented by a grid, which is high-dimensional (more
than 1000 dimensional). The proposed method uses a particle filter in which
each particle has individual tool shape parameters, thereby to avoid directly
handling a high-dimensional parameter space. The proposed method is evaluated
through simulations and experiments using tools with curved shapes on a plane.
Consequently, the proposed method can estimate the shape of the tool
simultaneously with the contact positions, making the contact-position
estimation more accurate.

</details>


### [28] [Trajectory Planning Using Safe Ellipsoidal Corridors as Projections of Orthogonal Trust Regions](https://arxiv.org/abs/2509.19734)
*Akshay Jaitly,Jon Arrizabalaga,Guanrui Li*

Main category: cs.RO

TL;DR: 提出了一种新的轨迹参数化方法，将非凸碰撞自由走廊表示为球的笛卡尔积的凸集，从而解耦问题规模与几何复杂度，并避免显式时间分配。


<details>
  <summary>Details</summary>
Motivation: 现有基于走廊的规划器在复杂环境中扩展性差，需要显式分配时间窗口给轨迹段，这限制了其在复杂环境中的性能。

Method: 引入新的轨迹参数化方法，将轨迹表示为椭球走廊内的凸笛卡尔积；基于此提出正交信任区域问题（Orth-TRP），开发利用并行结构和子问题特性的高效求解器。

Result: 在四旋翼轨迹规划基准测试中，该方法比现有最先进的走廊规划器产生更平滑的轨迹和更低的运行时间，特别是在高度复杂环境中。

Conclusion: 该方法通过创新的轨迹参数化和高效的优化求解器，显著提升了复杂环境中轨迹规划的性能和效率。

Abstract: Planning collision free trajectories in complex environments remains a core
challenge in robotics. Existing corridor based planners which rely on
decomposition of the free space into collision free subsets scale poorly with
environmental complexity and require explicit allocations of time windows to
trajectory segments. We introduce a new trajectory parameterization that
represents trajectories in a nonconvex collision free corridor as being in a
convex cartesian product of balls. This parameterization allows us to decouple
problem size from geometric complexity of the solution and naturally avoids
explicit time allocation by allowing trajectories to evolve continuously inside
ellipsoidal corridors. Building on this representation, we formulate the
Orthogonal Trust Region Problem (Orth-TRP), a specialized convex program with
separable block constraints, and develop a solver that exploits this parallel
structure and the unique structure of each parallel subproblem for efficient
optimization. Experiments on a quadrotor trajectory planning benchmark show
that our approach produces smoother trajectories and lower runtimes than
state-of-the-art corridor based planners, especially in highly complicated
environments.

</details>


### [29] [Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training](https://arxiv.org/abs/2509.19752)
*Rushuai Yang,Hangxing Wei,Ran Zhang,Zhiyuan Feng,Xiaoyu Chen,Tong Li,Chuheng Zhang,Li Zhao,Jiang Bian,Xiu Su,Yi Chen*

Main category: cs.RO

TL;DR: 本文提出了一种改进的扩散策略优化算法，用于生成高质量、低方差的轨迹，构建了扩散强化学习驱动的VLA训练管道，在130个长视野操作任务上表现优于人类演示和传统高斯RL策略。


<details>
  <summary>Details</summary>
Motivation: VLA模型依赖大规模人类演示数据，但人工收集成本高且难以扩展。传统RL算法在稀疏奖励的长视野操作任务中表现不佳，需要更有效的自主数据生成方法。

Method: 采用改进的扩散策略优化算法，利用扩散模型的高表达能力探索复杂多样行为，并通过迭代去噪过程的隐式正则化产生平滑一致的演示数据。

Result: 在LIBERO基准测试中，扩散RL生成的轨迹比人类演示和高斯RL策略更平滑一致。仅使用扩散RL生成数据训练的VLA模型平均成功率达81.9%，分别比人类数据和高斯RL数据高5.3%和12.6%。

Conclusion: 扩散RL是生成丰富、高质量、低方差VLA模型演示数据的有效替代方案，解决了人工数据收集的扩展性限制。

Abstract: Vision-language-action (VLA) models have shown strong generalization across
tasks and embodiments; however, their reliance on large-scale human
demonstrations limits their scalability owing to the cost and effort of manual
data collection. Reinforcement learning (RL) offers a potential alternative to
generate demonstrations autonomously, yet conventional RL algorithms often
struggle on long-horizon manipulation tasks with sparse rewards. In this paper,
we propose a modified diffusion policy optimization algorithm to generate
high-quality and low-variance trajectories, which contributes to a diffusion
RL-powered VLA training pipeline. Our algorithm benefits from not only the high
expressiveness of diffusion models to explore complex and diverse behaviors but
also the implicit regularization of the iterative denoising process to yield
smooth and consistent demonstrations. We evaluate our approach on the LIBERO
benchmark, which includes 130 long-horizon manipulation tasks, and show that
the generated trajectories are smoother and more consistent than both human
demonstrations and those from standard Gaussian RL policies. Further, training
a VLA model exclusively on the diffusion RL-generated data achieves an average
success rate of 81.9%, which outperforms the model trained on human data by
+5.3% and that on Gaussian RL-generated data by +12.6%. The results highlight
our diffusion RL as an effective alternative for generating abundant,
high-quality, and low-variance demonstrations for VLA models.

</details>


### [30] [DynaFlow: Dynamics-embedded Flow Matching for Physically Consistent Motion Generation from State-only Demonstrations](https://arxiv.org/abs/2509.19804)
*Sowoo Lee,Dongyun Kang,Jaehyun Park,Hae-Won Park*

Main category: cs.RO

TL;DR: DynaFlow是一个将可微分模拟器嵌入流匹配模型的新框架，通过生成动作空间轨迹并映射到动态可行的状态轨迹，确保所有输出在物理上一致。该框架支持仅基于状态演示的训练，能够同时生成物理一致的状态轨迹并推断所需的底层动作序列。


<details>
  <summary>Details</summary>
Motivation: 解决从仅状态演示中学习动态可行行为的问题，弥合运动学数据与实际执行之间的差距，使机器人能够从观察中学习并执行物理上可行的动作。

Method: 将可微分模拟器直接嵌入流匹配模型，在动作空间生成轨迹并通过模拟器映射到状态空间，构建端到端可微分架构，支持仅状态演示的训练。

Result: 在物理Go1四足机器人上成功部署，机器人能够重现数据集中的多种步态，执行长时程开环控制，并将不可行的运动学演示转换为动态可执行的风格化行为。

Conclusion: DynaFlow能够从仅状态演示中产生可部署、高效的运动，有效连接了运动学数据与实际硬件执行，在真实世界硬件上验证了其有效性。

Abstract: This paper introduces DynaFlow, a novel framework that embeds a
differentiable simulator directly into a flow matching model. By generating
trajectories in the action space and mapping them to dynamically feasible state
trajectories via the simulator, DynaFlow ensures all outputs are physically
consistent by construction. This end-to-end differentiable architecture enables
training on state-only demonstrations, allowing the model to simultaneously
generate physically consistent state trajectories while inferring the
underlying action sequences required to produce them. We demonstrate the
effectiveness of our approach through quantitative evaluations and showcase its
real-world applicability by deploying the generated actions onto a physical Go1
quadruped robot. The robot successfully reproduces diverse gait present in the
dataset, executes long-horizon motions in open-loop control and translates
infeasible kinematic demonstrations into dynamically executable, stylistic
behaviors. These hardware experiments validate that DynaFlow produces
deployable, highly effective motions on real-world hardware from state-only
demonstrations, effectively bridging the gap between kinematic data and
real-world execution.

</details>


### [31] [Where Did I Leave My Glasses? Open-Vocabulary Semantic Exploration in Real-World Semi-Static Environments](https://arxiv.org/abs/2509.19851)
*Benjamin Bogenberger,Oliver Harrison,Orrin Dahanaggamaarachchi,Lukas Brunke,Jingxing Qian,Siqi Zhou,Angela P. Schoellig*

Main category: cs.RO

TL;DR: 提出了一种用于半静态环境的开放词汇语义探索系统，通过概率模型跟踪对象实例稳定性，主动维护一致性语义地图，并利用LLM推理实现开放词汇目标导航


<details>
  <summary>Details</summary>
Motivation: 现有语义探索研究主要关注静态场景，缺乏对环境中对象动态变化（移除、重新引入、移动）的持续跟踪，而一致性地图对现实世界机器人应用至关重要

Method: 构建对象实例稳定性的概率模型，系统跟踪半静态变化，主动探索长时间未访问区域，并利用LLM推理进行开放词汇目标导航

Result: 在多个真实半静态环境中，系统平均检测到95%的地图变化，效率比随机和巡逻基线提高29%以上，映射精度与完全重建地图相差不到2%，目标导航任务完成速度比次优策略快约14%

Conclusion: 该系统能够有效处理半静态环境中的动态变化，通过主动地图维护和语义推理显著提高了机器人导航效率和适应性

Abstract: Robots deployed in real-world environments, such as homes, must not only
navigate safely but also understand their surroundings and adapt to environment
changes. To perform tasks efficiently, they must build and maintain a semantic
map that accurately reflects the current state of the environment. Existing
research on semantic exploration largely focuses on static scenes without
persistent object-level instance tracking. A consistent map is, however,
crucial for real-world robotic applications where objects in the environment
can be removed, reintroduced, or shifted over time. In this work, to close this
gap, we propose an open-vocabulary, semantic exploration system for semi-static
environments. Our system maintains a consistent map by building a probabilistic
model of object instance stationarity, systematically tracking semi-static
changes, and actively exploring areas that have not been visited for a
prolonged period of time. In addition to active map maintenance, our approach
leverages the map's semantic richness with LLM-based reasoning for
open-vocabulary object-goal navigation. This enables the robot to search more
efficiently by prioritizing contextually relevant areas. We evaluate our
approach across multiple real-world semi-static environments. Our system
detects 95% of map changes on average, improving efficiency by more than 29% as
compared to random and patrol baselines. Overall, our approach achieves a
mapping precision within 2% of a fully rebuilt map while requiring
substantially less exploration and further completes object goal navigation
tasks about 14% faster than the next-best tested strategy (coverage
patrolling). A video of our work can be found at
http://tiny.cc/sem-explor-semi-static .

</details>


### [32] [SAGE:State-Aware Guided End-to-End Policy for Multi-Stage Sequential Tasks via Hidden Markov Decision Process](https://arxiv.org/abs/2509.19853)
*BinXu Wu,TengFei Zhang,Chen Yang,JiaHao Wen,HaoCheng Li,JingTian Ma,Zhen Chen,JingYuan Wang*

Main category: cs.RO

TL;DR: SAGE是一个状态感知的引导模仿学习框架，通过将多阶段顺序机器人操作任务建模为隐马尔可夫决策过程，显式捕捉潜在任务阶段并解决状态模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 多阶段顺序机器人操作任务普遍存在状态模糊性，即视觉相似的观察对应不同的动作，需要一种能够明确区分任务阶段的方法。

Method: 使用隐马尔可夫决策过程建模，包含状态转移网络推断隐藏状态，以及状态感知动作策略基于观察和隐藏状态生成动作。采用半自动标注流程结合主动学习和软标签插值减少人工标注工作量。

Result: 在多个具有状态模糊性的复杂多阶段顺序任务中，SAGE在标准评估协议下实现了100%的任务成功率，显著超越基线方法。消融研究表明仅需约13%的状态手动标注即可维持性能。

Conclusion: SAGE框架有效解决了机器人操作任务中的状态模糊性问题，通过显式建模任务阶段实现了优异的性能，同时大幅减少了人工标注需求。

Abstract: Multi-stage sequential (MSS) robotic manipulation tasks are prevalent and
crucial in robotics. They often involve state ambiguity, where visually similar
observations correspond to different actions. We present SAGE, a state-aware
guided imitation learning framework that models tasks as a Hidden Markov
Decision Process (HMDP) to explicitly capture latent task stages and resolve
ambiguity. We instantiate the HMDP with a state transition network that infers
hidden states, and a state-aware action policy that conditions on both
observations and hidden states to produce actions, thereby enabling
disambiguation across task stages. To reduce manual annotation effort, we
propose a semi-automatic labeling pipeline combining active learning and soft
label interpolation. In real-world experiments across multiple complex MSS
tasks with state ambiguity, SAGE achieved 100% task success under the standard
evaluation protocol, markedly surpassing the baselines. Ablation studies
further show that such performance can be maintained with manual labeling for
only about 13% of the states, indicating its strong effectiveness.

</details>


### [33] [D3Grasp: Diverse and Deformable Dexterous Grasping for General Objects](https://arxiv.org/abs/2509.19892)
*Keyu Wang,Bingcong Lu,Zhengxue Cheng,Hengdi Zhang,Li Song*

Main category: cs.RO

TL;DR: D3Grasp是一个多模态感知引导的强化学习框架，用于实现多样化和稳定的灵巧抓取，特别针对可变形物体，在真实世界试验中达到95.1%的平均成功率。


<details>
  <summary>Details</summary>
Motivation: 解决灵巧抓取中高维动作空间和感知不确定性的挑战，特别是针对一般物体和可变形物体的多样化稳定抓取问题。

Method: 1）引入统一的多模态表示整合视觉和触觉感知；2）提出非对称强化学习架构利用特权信息；3）设计训练策略合成接触丰富、无穿透且运动学可行的抓取。

Result: 在大规模多样化物体类别上表现出高度鲁棒性，在可变形和柔性物体灵巧抓取方面显著推进了技术前沿，真实世界试验平均成功率95.1%。

Conclusion: D3Grasp框架有效解决了灵巧抓取的关键挑战，在感知不确定性和真实世界干扰下表现出卓越性能，优于现有方法。

Abstract: Achieving diverse and stable dexterous grasping for general and deformable
objects remains a fundamental challenge in robotics, due to high-dimensional
action spaces and uncertainty in perception. In this paper, we present D3Grasp,
a multimodal perception-guided reinforcement learning framework designed to
enable Diverse and Deformable Dexterous Grasping. We firstly introduce a
unified multimodal representation that integrates visual and tactile perception
to robustly grasp common objects with diverse properties. Second, we propose an
asymmetric reinforcement learning architecture that exploits privileged
information during training while preserving deployment realism, enhancing both
generalization and sample efficiency. Third, we meticulously design a training
strategy to synthesize contact-rich, penetration-free, and kinematically
feasible grasps with enhanced adaptability to deformable and contact-sensitive
objects. Extensive evaluations confirm that D3Grasp delivers highly robust
performance across large-scale and diverse object categories, and substantially
advances the state of the art in dexterous grasping for deformable and
compliant objects, even under perceptual uncertainty and real-world
disturbances. D3Grasp achieves an average success rate of 95.1% in real-world
trials,outperforming prior methods on both rigid and deformable objects
benchmarks.

</details>


### [34] [GUIDE: A Diffusion-Based Autonomous Robot Exploration Framework Using Global Graph Inference](https://arxiv.org/abs/2509.19916)
*Zijun Che,Yinghong Zhang,Shengyi Liang,Boyu Zhou,Jun Ma,Jinni Zhou*

Main category: cs.RO

TL;DR: GUIDE是一个新颖的自主探索框架，通过结合全局图推理和扩散决策，在结构化室内环境中实现高效探索，比现有方法快18.3%，冗余移动减少34.9%。


<details>
  <summary>Details</summary>
Motivation: 现有方法在结构化复杂室内环境中难以有效建模未观测空间和规划全局高效路径，存在局限性。

Method: 提出区域评估全局图表示，整合观测数据和未探索区域预测，采用扩散策略网络生成稳定的前瞻性动作序列。

Result: 在仿真和实际部署中，GUIDE始终优于最先进方法，覆盖完成速度提升18.3%，冗余移动减少34.9%。

Conclusion: GUIDE框架通过全局图推理与扩散决策的协同结合，有效解决了结构化室内环境中的自主探索挑战。

Abstract: Autonomous exploration in structured and complex indoor environments remains
a challenging task, as existing methods often struggle to appropriately model
unobserved space and plan globally efficient paths. To address these
limitations, we propose GUIDE, a novel exploration framework that
synergistically combines global graph inference with diffusion-based
decision-making. We introduce a region-evaluation global graph representation
that integrates both observed environmental data and predictions of unexplored
areas, enhanced by a region-level evaluation mechanism to prioritize reliable
structural inferences while discounting uncertain predictions. Building upon
this enriched representation, a diffusion policy network generates stable,
foresighted action sequences with significantly reduced denoising steps.
Extensive simulations and real-world deployments demonstrate that GUIDE
consistently outperforms state-of-the-art methods, achieving up to 18.3% faster
coverage completion and a 34.9% reduction in redundant movements.

</details>


### [35] [Robot Trajectron V2: A Probabilistic Shared Control Framework for Navigation](https://arxiv.org/abs/2509.19954)
*Pinhao Song,Yurui Du,Ophelie Saussus,Sofie De Schrijver,Irene Caprara,Peter Janssen,Renaud Detry*

Main category: cs.RO

TL;DR: RT-V2是一个概率共享控制导航解决方案，通过结合先验意图模型和后验更新来准确预测用户意图并提供安全有效的人机交互辅助


<details>
  <summary>Details</summary>
Motivation: 解决人机交互中准确意图预测和安全有效辅助的问题，平衡用户自主性和辅助干预

Method: 结合先验意图模型（使用RNN和条件变分自编码器捕捉多模态和历史依赖的用户意图）与后验更新（整合实时用户输入和环境上下文），通过概率建模、强化学习和安全优化统一方法

Result: 在合成基准测试、人机键盘交互实验和脑机接口实验中，RT-V2在意图估计方面优于现有技术，提供安全高效的导航支持

Conclusion: RT-V2为多样化辅助技术提供了一个原则性和可推广的共享控制方法

Abstract: We propose a probabilistic shared-control solution for navigation, called
Robot Trajectron V2 (RT-V2), that enables accurate intent prediction and safe,
effective assistance in human-robot interaction. RT-V2 jointly models a user's
long-term behavioral patterns and their noisy, low-dimensional control signals
by combining a prior intent model with a posterior update that accounts for
real-time user input and environmental context. The prior captures the
multimodal and history-dependent nature of user intent using recurrent neural
networks and conditional variational autoencoders, while the posterior
integrates this with uncertain user commands to infer desired actions. We
conduct extensive experiments to validate RT-V2 across synthetic benchmarks,
human-computer interaction studies with keyboard input, and brain-machine
interface experiments with non-human primates. Results show that RT-V2
outperforms the state of the art in intent estimation, provides safe and
efficient navigation support, and adequately balances user autonomy with
assistive intervention. By unifying probabilistic modeling, reinforcement
learning, and safe optimization, RT-V2 offers a principled and generalizable
approach to shared control for diverse assistive technologies.

</details>


### [36] [Generalist Robot Manipulation beyond Action Labeled Data](https://arxiv.org/abs/2509.19958)
*Alexander Spiridonov,Jan-Nico Zaech,Nikolay Nikolov,Luc Van Gool,Danda Pani Paudel*

Main category: cs.RO

TL;DR: 提出一种利用无动作标签视频数据的方法，通过提取手部/夹爪位置的密集动态3D点云，使用3D动态预测器进行自监督学习，然后使用少量标注数据进行动作对齐，从而提升通用机器人操作性能。


<details>
  <summary>Details</summary>
Motivation: 解决高质量动作标注机器人演示数据难以扩展的问题，利用无标签的人类和机器人动作视频来增强开放词汇性能并实现数据高效学习。

Method: 提取手部/夹爪位置的密集动态3D点云，设计3D动态预测器进行自监督学习，然后使用少量标注数据集将预测器调整为动作预测器进行动作对齐。

Result: 方法能够从无标签的人类和机器人演示中学习，提升下游通用机器人策略性能，并使机器人能够在真实世界和仿真环境中无需动作标签学习新任务。

Conclusion: 该方法有效解决了机器人操作数据标注的瓶颈问题，实现了从无标签视频中学习机器人操作技能，具有重要的实际应用价值。

Abstract: Recent advances in generalist robot manipulation leverage pre-trained
Vision-Language Models (VLMs) and large-scale robot demonstrations to tackle
diverse tasks in a zero-shot manner. A key challenge remains: scaling
high-quality, action-labeled robot demonstration data, which existing methods
rely on for robustness and generalization. To address this, we propose a method
that benefits from videos without action labels - featuring humans and/or
robots in action - enhancing open-vocabulary performance and enabling
data-efficient learning of new tasks. Our method extracts dense, dynamic 3D
point clouds at the hand or gripper location and uses a proposed 3D dynamics
predictor for self-supervision. This predictor is then tuned to an action
predictor using a smaller labeled dataset for action alignment. We show that
our method not only learns from unlabeled human and robot demonstrations -
improving downstream generalist robot policies - but also enables robots to
learn new tasks without action labels (i.e., out-of-action generalization) in
both real-world and simulated settings.

</details>


### [37] [An effective control of large systems of active particles: An application to evacuation problem](https://arxiv.org/abs/2509.19972)
*Albina Klepach,Egor E. Nuzhin,Alexey A. Tsukanov,Nikolay V. Brilliantov*

Main category: cs.RO

TL;DR: 本文提出了一种结合强化学习与人工力的领导者控制策略，用于大规模活性粒子系统的有效疏散，特别是在机器人救援人员引导人群撤离危险场所的应用中。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂场景下缺乏可扩展性和鲁棒性，特别是需要对每个个体进行单独控制。通过领导者控制整个系统可以解决这一问题。

Method: 引入了广义Vicsek模型来描述领导者对活性粒子的引导，结合强化学习与人工力开发了有效的领导者控制策略。

Result: 研究表明，直接应用强化学习会产生次优结果，而本文提出的方法能够提供鲁棒且高效的疏散策略。

Conclusion: 该方法为解决大规模活性粒子系统的控制问题提供了有效的解决方案，特别是在人群疏散等实际应用中具有重要价值。

Abstract: Manipulation of large systems of active particles is a serious challenge
across diverse domains, including crowd management, control of robotic swarms,
and coordinated material transport. The development of advanced control
strategies for complex scenarios is hindered, however, by the lack of
scalability and robustness of the existing methods, in particular, due to the
need of an individual control for each agent. One possible solution involves
controlling a system through a leader or a group of leaders, which other agents
tend to follow. Using such an approach we develop an effective control strategy
for a leader, combining reinforcement learning (RL) with artificial forces
acting on the system. To describe the guidance of active particles by a leader
we introduce the generalized Vicsek model. This novel method is then applied to
the problem of the effective evacuation by a robot-rescuer (leader) of large
groups of people from hazardous places. We demonstrate, that while a
straightforward application of RL yields suboptimal results, even for advanced
architectures, our approach provides a robust and efficient evacuation
strategy. The source code supporting this study is publicly available at:
https://github.com/cinemere/evacuation.

</details>


### [38] [Lidar-based Tracking of Traffic Participants with Sensor Nodes in Existing Urban Infrastructure](https://arxiv.org/abs/2509.20009)
*Simon Schäfer,Bassam Alrifaee,Ehsan Hashemi*

Main category: cs.RO

TL;DR: 本文提出了一种仅使用激光雷达的状态估计和跟踪框架，以及用于与现有城市基础设施集成的路边感知单元。该方案在CPU边缘硬件上实现实时高性能跟踪，适用于大规模城市部署。


<details>
  <summary>Details</summary>
Motivation: 城市部署需要可扩展的实时跟踪解决方案，但传统远程感知成本高且计算密集，特别是在感知条件恶化的情况下。需要开发计算效率高、无需GPU的解决方案。

Method: 传感器节点将单个激光雷达与边缘计算单元耦合，运行计算效率高的观测器，同时估计对象状态、类别、尺寸和存在概率。管道包括：(i)通过扩展卡尔曼滤波进行状态更新，(ii)使用1D网格图/贝叶斯更新进行尺寸估计，(iii)通过最可能足迹驱动的查找表进行类别更新，(iv)从跟踪年龄和边界框一致性估计存在概率。

Result: 在动态城市场景中的实验显示实时性能和高精度：完整端到端管道在99.88%的消息中在100毫秒内完成，具有出色的检测率。在模拟风和传感器振动下进一步确认了鲁棒性。

Conclusion: 可靠的路边实时跟踪在仅使用CPU的边缘硬件上是可行的，能够在现有城市基础设施中实现可扩展、隐私友好的部署，降低部署成本并简化大规模城市推广和维护工作。

Abstract: This paper presents a lidar-only state estimation and tracking framework,
along with a roadside sensing unit for integration with existing urban
infrastructure. Urban deployments demand scalable, real-time tracking
solutions, yet traditional remote sensing remains costly and computationally
intensive, especially under perceptually degraded conditions. Our sensor node
couples a single lidar with an edge computing unit and runs a computationally
efficient, GPU-free observer that simultaneously estimates object state, class,
dimensions, and existence probability. The pipeline performs: (i) state updates
via an extended Kalman filter, (ii) dimension estimation using a 1D
grid-map/Bayesian update, (iii) class updates via a lookup table driven by the
most probable footprint, and (iv) existence estimation from track age and
bounding-box consistency. Experiments in dynamic urban-like scenes with diverse
traffic participants demonstrate real-time performance and high precision: The
complete end-to-end pipeline finishes within \SI{100}{\milli\second} for
\SI{99.88}{\%} of messages, with an excellent detection rate. Robustness is
further confirmed under simulated wind and sensor vibration. These results
indicate that reliable, real-time roadside tracking is feasible on CPU-only
edge hardware, enabling scalable, privacy-friendly deployments within existing
city infrastructure. The framework integrates with existing poles, traffic
lights, and buildings, reducing deployment costs and simplifying large-scale
urban rollouts and maintenance efforts.

</details>


### [39] [MARG: MAstering Risky Gap Terrains for Legged Robots with Elevation Mapping](https://arxiv.org/abs/2509.20036)
*Yinzhao Dong,Ji Ma,Liu Zhao,Wanyue Li,Peng Lu*

Main category: cs.RO

TL;DR: 本文提出了一种名为MARG的深度强化学习控制器，通过整合地形图和本体感知来应对四足机器人在危险缝隙地形上的运动挑战，实现了零样本策略迁移。


<details>
  <summary>Details</summary>
Motivation: 现有盲运动控制器在危险缝隙地形上难以确保安全高效通行，而基于感知的控制器又存在多传感器部署复杂和计算资源需求高的问题。

Method: 提出MARG控制器，在训练阶段选择性利用仿真中的特权信息加速策略优化，设计三个足部相关奖励鼓励探索安全落脚点，并提出仅需单一LiDAR的地形图生成模型减少映射漂移。

Result: 实验结果表明MARG在各种危险地形任务中保持了稳定性。

Conclusion: MARG控制器成功解决了四足机器人在复杂缝隙地形上的运动控制问题，实现了高效稳定的零样本迁移。

Abstract: Deep Reinforcement Learning (DRL) controllers for quadrupedal locomotion have
demonstrated impressive performance on challenging terrains, allowing robots to
execute complex skills such as climbing, running, and jumping. However,
existing blind locomotion controllers often struggle to ensure safety and
efficient traversal through risky gap terrains, which are typically highly
complex, requiring robots to perceive terrain information and select
appropriate footholds during locomotion accurately. Meanwhile, existing
perception-based controllers still present several practical limitations,
including a complex multi-sensor deployment system and expensive computing
resource requirements. This paper proposes a DRL controller named MAstering
Risky Gap Terrains (MARG), which integrates terrain maps and proprioception to
dynamically adjust the action and enhance the robot's stability in these tasks.
During the training phase, our controller accelerates policy optimization by
selectively incorporating privileged information (e.g., center of mass,
friction coefficients) that are available in simulation but unmeasurable
directly in real-world deployments due to sensor limitations. We also designed
three foot-related rewards to encourage the robot to explore safe footholds.
More importantly, a terrain map generation (TMG) model is proposed to reduce
the drift existing in mapping and provide accurate terrain maps using only one
LiDAR, providing a foundation for zero-shot transfer of the learned policy. The
experimental results indicate that MARG maintains stability in various risky
terrain tasks.

</details>


### [40] [LLM Trainer: Automated Robotic Data Generating via Demonstration Augmentation using LLMs](https://arxiv.org/abs/2509.20070)
*Abraham George,Amir Barati Farimani*

Main category: cs.RO

TL;DR: LLM Trainer是一个全自动管道，利用大型语言模型的世界知识将少量人类演示转化为大规模机器人数据集用于模仿学习。该方法通过离线演示注释和在线关键姿势重定向生成新轨迹，并通过Thompson采样优化注释以提高成功率。


<details>
  <summary>Details</summary>
Motivation: 解决机器人模仿学习中需要大量人类演示数据的问题，通过利用LLM的知识将少量演示转化为大规模数据集，降低数据收集成本。

Method: 将演示生成分解为两步：离线演示注释（提取关键帧、显著物体和姿势-物体关系）和在线关键姿势重定向（根据初始观察适应新场景）。使用Thompson采样优化注释，提高生成成功率。

Result: 在多个任务上的评估显示，该方法的数据注释方法持续优于专家设计的基线。展示了结合优化LLM前馈规划和学习反馈模仿学习控制器的集成策略，并在Franka Emika Panda机器人上验证了硬件可行性。

Conclusion: LLM Trainer提供了一种有效的自动化方法，能够从少量人类演示生成大规模机器人数据集，显著提高了模仿学习的效率和可扩展性。

Abstract: We present LLM Trainer, a fully automated pipeline that leverages the world
knowledge of Large Language Models (LLMs) to transform a small number of human
demonstrations (as few as one) into a large robot dataset for imitation
learning. Our approach decomposes demonstration generation into two steps: (1)
offline demonstration annotation that extracts keyframes, salient objects, and
pose-object relations; and (2) online keypose retargeting that adapts those
keyframes to a new scene, given an initial observation. Using these modified
keypoints, our system warps the original demonstration to generate a new
trajectory, which is then executed, and the resulting demo, if successful, is
saved. Because the annotation is reusable across scenes, we use Thompson
sampling to optimize the annotation, significantly improving generation success
rate. We evaluate our method on a range of tasks, and find that our data
annotation method consistently outperforms expert-engineered baselines. We
further show an ensemble policy that combines the optimized LLM feed-forward
plan with a learned feedback imitation learning controller. Finally, we
demonstrate hardware feasibility on a Franka Emika Panda robot. For additional
materials and demonstration videos, please see the project website:
https://sites.google.com/andrew.cmu.edu/llm-trainer

</details>


### [41] [Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning](https://arxiv.org/abs/2509.20077)
*Xun Li,Rodrigo Santa Cruz,Mingze Xi,Hu Zhang,Madhawa Perera,Ziwei Wang,Ahalya Ravendran,Brandon J. Matthews,Feng Xu,Matt Adcock,Dadong Wang,Jiajun Liu*

Main category: cs.RO

TL;DR: 本文提出了3D可查询场景表示（3D QSR）框架，通过融合多模态3D表示来实现机器人对复杂3D环境的综合场景理解，将高级人类指令转化为精确的机器人任务规划。


<details>
  <summary>Details</summary>
Motivation: 为了让机器人理解高级人类指令并执行复杂任务，需要实现全面的场景理解：以有意义的方式解释和交互3D环境。这需要一种智能地图，将精确的几何结构与丰富的人类可理解语义相融合。

Method: 基于多媒体数据构建3D QSR框架，统一三种互补的3D表示：全景重建的3D一致新视角渲染和分割、3D点云的精确几何、3D场景图的结构化组织。采用以对象为中心的设计，与大型视觉语言模型集成，通过链接多模态对象嵌入实现语义可查询性。

Result: 在Unity模拟的机器人任务规划场景中评估，使用Replica室内公共数据集，并在真实湿实验室的数字副本中测试紧急响应场景。结果表明框架能够促进场景理解，整合空间和语义推理。

Conclusion: 3D QSR框架有效实现了将高级人类指令转化为复杂3D环境中的精确机器人任务规划，展示了在场景理解和语义推理方面的优势。

Abstract: To enable robots to comprehend high-level human instructions and perform
complex tasks, a key challenge lies in achieving comprehensive scene
understanding: interpreting and interacting with the 3D environment in a
meaningful way. This requires a smart map that fuses accurate geometric
structure with rich, human-understandable semantics. To address this, we
introduce the 3D Queryable Scene Representation (3D QSR), a novel framework
built on multimedia data that unifies three complementary 3D representations:
(1) 3D-consistent novel view rendering and segmentation from panoptic
reconstruction, (2) precise geometry from 3D point clouds, and (3) structured,
scalable organization via 3D scene graphs. Built on an object-centric design,
the framework integrates with large vision-language models to enable semantic
queryability by linking multimodal object embeddings, and supporting
object-level retrieval of geometric, visual, and semantic information. The
retrieved data are then loaded into a robotic task planner for downstream
execution. We evaluate our approach through simulated robotic task planning
scenarios in Unity, guided by abstract language instructions and using the
indoor public dataset Replica. Furthermore, we apply it in a digital duplicate
of a real wet lab environment to test QSR-supported robotic task planning for
emergency response. The results demonstrate the framework's ability to
facilitate scene understanding and integrate spatial and semantic reasoning,
effectively translating high-level human instructions into precise robotic task
planning in complex 3D environments.

</details>


### [42] [DB-TSDF: Directional Bitmask-based Truncated Signed Distance Fields for Efficient Volumetric Mapping](https://arxiv.org/abs/2509.20081)
*Jose E. Maese,Luis Merino,Fernando Caballero*

Main category: cs.RO

TL;DR: 提出了一种基于TSDF的高效CPU专用体素建图框架，通过方向性位掩码集成方案实现实时3D重建，处理时间与体素网格分辨率无关


<details>
  <summary>Details</summary>
Motivation: 大多数现有TSDF/ESDF方法依赖GPU加速，而本文旨在开发完全在CPU上运行的高效建图方法，实现与GPU方法竞争的性能

Method: 使用截断符号距离场(TSDF)，通过方向性位掩码集成方案将LiDAR点云数据增量融合到体素网格中

Result: 在真实世界开放数据集上的实验表明，生成的地图精度与当代建图技术相当，处理时间保持恒定

Conclusion: 该方法在CPU上实现了高效的三维建图，为不依赖GPU的实时3D重建提供了可行方案

Abstract: This paper presents a high-efficiency, CPU-only volumetric mapping framework
based on a Truncated Signed Distance Field (TSDF). The system incrementally
fuses raw LiDAR point-cloud data into a voxel grid using a directional
bitmask-based integration scheme, producing dense and consistent TSDF
representations suitable for real-time 3D reconstruction. A key feature of the
approach is that the processing time per point-cloud remains constant,
regardless of the voxel grid resolution, enabling high resolution mapping
without sacrificing runtime performance. In contrast to most recent TSDF/ESDF
methods that rely on GPU acceleration, our method operates entirely on CPU,
achieving competitive results in speed. Experiments on real-world open datasets
demonstrate that the generated maps attain accuracy on par with contemporary
mapping techniques.

</details>


### [43] [Orbital Stabilization and Time Synchronization of Unstable Periodic Motions in Underactuated Robots](https://arxiv.org/abs/2509.20082)
*Surov Maksim*

Main category: cs.RO

TL;DR: 提出一种控制方法，用于实现欠驱动机器人系统的轨道稳定性和时间同步，扩展了经典的横向线性化框架，结合LQR和滑模控制，并在六台蝴蝶机器人上验证了集中式和分散式控制策略。


<details>
  <summary>Details</summary>
Motivation: 解决欠驱动机器人系统在周期性轨迹跟踪中的时间同步问题，扩展传统控制框架以处理时间失同步动态。

Method: 扩展经典横向线性化框架以显式包含时间失同步动态，采用时变LQR和滑模控制相结合的方法来稳定扩展后的横向动态。

Result: 理论结果通过实验验证，在六台蝴蝶机器人上成功实现了集中式和分散式控制策略。

Conclusion: 所提出的控制方法能够有效实现欠驱动机器人系统的轨道稳定性和时间同步，为多机器人协同控制提供了可行的解决方案。

Abstract: This paper presents a control methodology for achieving orbital stabilization
with simultaneous time synchronization of periodic trajectories in
underactuated robotic systems. The proposed approach extends the classical
transverse linearization framework to explicitly incorporate
time-desynchronization dynamics. To stabilize the resulting extended transverse
dynamics, we employ a combination of time-varying LQR and sliding-mode control.
The theoretical results are validated experimentally through the implementation
of both centralized and decentralized control strategies on a group of six
Butterfly robots.

</details>


### [44] [C-3TO: Continuous 3D Trajectory Optimization on Neural Euclidean Signed Distance Fields](https://arxiv.org/abs/2509.20084)
*Guillermo Gil,Jose Antonio Cobano,Luis Merino,Fernando Caballero*

Main category: cs.RO

TL;DR: 本文提出了一种基于在线神经欧几里得符号距离场（ESDF）的连续3D轨迹优化框架C-3TO，用于在杂乱环境中进行连续轨迹优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖离散化ESDF网格和插值，无法提供精确的梯度信息。本文旨在通过连续神经ESDF直接优化平滑轨迹，确保整个轨迹的精确梯度计算。

Method: 采用五阶多项式表示连续轨迹，在连续神经ESDF上进行直接优化。集成两阶段非线性优化流程，平衡效率、安全性和平滑性。支持局部窗口大小和优化参数的灵活定义。

Result: 实验结果表明C-3TO能够生成碰撞感知且动态可行的轨迹。该框架具有良好的适应性，可在不牺牲性能的情况下满足不同用户需求。

Conclusion: 通过结合连续轨迹参数化和连续更新的神经ESDF，C-3TO为空中机器人的安全高效局部重规划建立了鲁棒且可泛化的基础。

Abstract: This paper introduces a novel framework for continuous 3D trajectory
optimization in cluttered environments, leveraging online neural Euclidean
Signed Distance Fields (ESDFs). Unlike prior approaches that rely on
discretized ESDF grids with interpolation, our method directly optimizes smooth
trajectories represented by fifth-order polynomials over a continuous neural
ESDF, ensuring precise gradient information throughout the entire trajectory.
The framework integrates a two-stage nonlinear optimization pipeline that
balances efficiency, safety and smoothness. Experimental results demonstrate
that C-3TO produces collision-aware and dynamically feasible trajectories.
Moreover, its flexibility in defining local window sizes and optimization
parameters enables straightforward adaptation to diverse user's needs without
compromising performance. By combining continuous trajectory parameterization
with a continuously updated neural ESDF, C-3TO establishes a robust and
generalizable foundation for safe and efficient local replanning in aerial
robotics.

</details>


### [45] [Hybrid Safety Verification of Multi-Agent Systems using $ψ$-Weighted CBFs and PAC Guarantees](https://arxiv.org/abs/2509.20093)
*Venkat Margapuri,Garik Kazanjian,Naren Kosaraju*

Main category: cs.RO

TL;DR: 提出了一种针对有界随机扰动下闭环多智能体系统的混合安全验证框架，结合控制屏障函数和蒙特卡洛验证来提供概率安全保证


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在有界随机扰动下的安全验证是一个挑战性问题，需要同时考虑确定性分析和概率性保证

Method: 采用混合方法：1）基于控制屏障函数的确定性可容许性分析；2）通过蒙特卡洛滚动进行经验验证；3）基于边界感知安全违规的PAC式概率保证

Result: 在不同有界随机扰动下的实验验证了所提方法的可行性

Conclusion: 该混合框架能够为多智能体系统提供有效的概率安全证书

Abstract: This study proposes a hybrid safety verification framework for closed-loop
multi-agent systems under bounded stochastic disturbances. The proposed
approach augments control barrier functions with a novel $\psi$-weighted
formulation that encodes directional control alignment between agents into the
safety constraints. Deterministic admissibility is combined with empirical
validation via Monte Carlo rollouts, and a PAC-style guarantee is derived based
on margin-aware safety violations to provide a probabilistic safety
certificate. The results from the experiments conducted under different bounded
stochastic disturbances validate the feasibility of the proposed approach.

</details>


### [46] [Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving](https://arxiv.org/abs/2509.20109)
*Pengxiang Li,Yinan Zheng,Yue Wang,Huimin Wang,Hang Zhao,Jingjing Liu,Xianyuan Zhan,Kun Zhan,Xianpeng Lang*

Main category: cs.RO

TL;DR: ReflectDrive是一个基于学习的框架，通过离散扩散和反射机制实现安全轨迹生成，解决了现有VLA模型在模仿学习中难以编码物理规则的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端自动驾驶方法受限于模仿学习的局限性，无法有效编码物理规则，通常需要复杂的后处理或计算昂贵的梯度计算。

Method: 首先将二维驾驶空间离散化构建动作码本，利用预训练扩散语言模型进行规划；核心是安全感知的反射机制，通过局部搜索识别不安全标记并确定可行解，然后进行基于修复的再生。

Result: 在NAVSIM基准测试中，ReflectDrive在安全关键轨迹生成方面表现出显著优势。

Conclusion: 该方法为自动驾驶系统提供了可扩展且可靠的解决方案。

Abstract: End-to-End (E2E) solutions have emerged as a mainstream approach for
autonomous driving systems, with Vision-Language-Action (VLA) models
representing a new paradigm that leverages pre-trained multimodal knowledge
from Vision-Language Models (VLMs) to interpret and interact with complex
real-world environments. However, these methods remain constrained by the
limitations of imitation learning, which struggles to inherently encode
physical rules during training. Existing approaches often rely on complex
rule-based post-refinement, employ reinforcement learning that remains largely
limited to simulation, or utilize diffusion guidance that requires
computationally expensive gradient calculations. To address these challenges,
we introduce ReflectDrive, a novel learning-based framework that integrates a
reflection mechanism for safe trajectory generation via discrete diffusion. We
first discretize the two-dimensional driving space to construct an action
codebook, enabling the use of pre-trained Diffusion Language Models for
planning tasks through fine-tuning. Central to our approach is a safety-aware
reflection mechanism that performs iterative self-correction without gradient
computation. Our method begins with goal-conditioned trajectory generation to
model multi-modal driving behaviors. Based on this, we apply local search
methods to identify unsafe tokens and determine feasible solutions, which then
serve as safe anchors for inpainting-based regeneration. Evaluated on the
NAVSIM benchmark, ReflectDrive demonstrates significant advantages in
safety-critical trajectory generation, offering a scalable and reliable
solution for autonomous driving systems.

</details>


### [47] [A Biomimetic Vertebraic Soft Robotic Tail for High-Speed, High-Force Dynamic Maneuvering](https://arxiv.org/abs/2509.20219)
*Sicong Liu,Jianhui Liu,Fang Chen,Wenjian Yang,Juan Yi,Yu Zheng,Zheng Wang,Wanchao Chi,Chaoyang Song*

Main category: cs.RO

TL;DR: 本文提出了一种仿生椎体软体机器人尾巴，通过结合刚性椎体和软体气动驱动，解决了传统刚性尾巴安全性差和软体尾巴动力不足的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器人尾巴设计存在刚性系统动力强但安全性差、软体系统安全但动力不足的权衡问题，需要一种既能提供强大惯性效应又能在非结构化环境中安全工作的解决方案。

Method: 采用仿生椎体结构设计，将被动关节椎柱与柔性气动体结合，实现负载承载和驱动的解耦。开发了包含椎体约束的专用运动学和动力学模型，并通过实验验证。

Result: BVSR尾巴实现了超过670度/秒的角速度，产生高达5.58N的惯性力和1.21Nm的扭矩，相比无椎体设计性能提升超过200%。在快速推车稳定、障碍物穿越、高速转向和四足机器人集成等应用中表现出色。

Conclusion: 这种混合设计成功解决了刚性尾巴和软体尾巴的权衡问题，为敏捷机器人平台提供了兼具高性能和安全性的尾巴解决方案。

Abstract: Robotic tails can enhance the stability and maneuverability of mobile robots,
but current designs face a trade-off between the power of rigid systems and the
safety of soft ones. Rigid tails generate large inertial effects but pose risks
in unstructured environments, while soft tails lack sufficient speed and force.
We present a Biomimetic Vertebraic Soft Robotic (BVSR) tail that resolves this
challenge through a compliant pneumatic body reinforced by a passively jointed
vertebral column inspired by musculoskeletal structures. This hybrid design
decouples load-bearing and actuation, enabling high-pressure actuation (up to 6
bar) for superior dynamics while preserving compliance. A dedicated kinematic
and dynamic model incorporating vertebral constraints is developed and
validated experimentally. The BVSR tail achieves angular velocities above
670{\deg}/s and generates inertial forces and torques up to 5.58 N and 1.21 Nm,
indicating over 200% improvement compared to non-vertebraic designs.
Demonstrations on rapid cart stabilization, obstacle negotiation, high-speed
steering, and quadruped integration confirm its versatility and practical
utility for agile robotic platforms.

</details>


### [48] [Techno-Economic analysis for Smart Hangar inspection operations through Sensing and Localisation at scale](https://arxiv.org/abs/2509.20229)
*Angelos Plastropoulos,Nicolas P. Avdelidis,Argyrios Zolotas*

Main category: cs.RO

TL;DR: 本文提出了首个技术经济路线图，在40x50米的机库环境中对运动捕捉、超宽带和天花板摄像头网络进行基准测试，并引入双层次优化框架来选择摄像头布局，以最小化硬件成本同时满足精度要求。


<details>
  <summary>Details</summary>
Motivation: 飞机维护机库环境具有高天花板、金属材料和GPS信号缺失等特点，存在严重的多径效应和严格的操作约束，需要针对特定领域的比较研究来指导可靠且可扩展的定位系统部署。

Method: 采用双层次优化框架，结合基于市场的摄像头镜头选择和优化求解器，生成最小化硬件成本同时满足精度目标的摄像头布局方案。

Result: 优化的视觉架构能够为下一代智能机库提供稳健且经济高效的传感解决方案，帮助MRO规划者在精度、覆盖范围和预算之间取得平衡。

Conclusion: 该路线图为MRO规划者提供了可操作的方法，展示了优化视觉架构在智能机库中实现可靠且经济高效定位的潜力。

Abstract: The accuracy, resilience, and affordability of localisation are fundamental
to autonomous robotic inspection within aircraft maintenance and overhaul (MRO)
hangars. Hangars typically feature tall ceilings and are often made of
materials such as metal. Due to its nature, it is considered a GPS-denied
environment, with extensive multipath effects and stringent operational
constraints that collectively create a uniquely challenging environment. This
persistent gap highlights the need for domain-specific comparative studies,
including rigorous cost, accuracy, and integration assessments, to inform a
reliable and scalable deployment of a localisation system in the Smart Hangar.
This paper presents the first techno-economic roadmap that benchmarks motion
capture (MoCap), ultra-wideband (UWB), and a ceiling-mounted camera network
across three operational scenarios: robot localisation, asset tracking, and
surface defect detection within a 40x50 m hangar bay. A dual-layer optimisation
for camera selection and positioning framework is introduced, which couples
market-based camera-lens selection with an optimisation solver, producing
camera layouts that minimise hardware while meeting accuracy targets. The
roadmap equips MRO planners with an actionable method to balance accuracy,
coverage, and budget, demonstrating that an optimised vision architecture has
the potential to unlock robust and cost-effective sensing for next-generation
Smart Hangars.

</details>


### [49] [AnchDrive: Bootstrapping Diffusion Policies with Hybrid Trajectory Anchors for End-to-End Driving](https://arxiv.org/abs/2509.20253)
*Jinhao Chai,Anqing Jiang,Hao Jiang,Shiyi Mu,Zichong Gu,Shugong Xu*

Main category: cs.RO

TL;DR: AnchDrive是一个端到端自动驾驶框架，通过混合轨迹锚点引导扩散策略，有效降低生成模型的计算成本，在NAVSIM基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决端到端多模态规划中的行为多模态性和长尾场景泛化挑战，同时降低传统生成模型的高计算成本。

Method: 提出基于混合轨迹锚点的扩散策略：使用静态驾驶先验词汇和动态上下文感知轨迹作为锚点，通过Transformer解码动态轨迹，扩散模型学习预测轨迹偏移分布进行细粒度优化。

Result: 在NAVSIM基准测试中达到新的最先进水平，展现出强大的泛化能力。

Conclusion: 锚点引导的扩散策略设计能够高效生成多样化、高质量的轨迹，为端到端自动驾驶提供了有效的解决方案。

Abstract: End-to-end multi-modal planning has become a transformative paradigm in
autonomous driving, effectively addressing behavioral multi-modality and the
generalization challenge in long-tail scenarios. We propose AnchDrive, a
framework for end-to-end driving that effectively bootstraps a diffusion policy
to mitigate the high computational cost of traditional generative models.
Rather than denoising from pure noise, AnchDrive initializes its planner with a
rich set of hybrid trajectory anchors. These anchors are derived from two
complementary sources: a static vocabulary of general driving priors and a set
of dynamic, context-aware trajectories. The dynamic trajectories are decoded in
real-time by a Transformer that processes dense and sparse perceptual features.
The diffusion model then learns to refine these anchors by predicting a
distribution of trajectory offsets, enabling fine-grained refinement. This
anchor-based bootstrapping design allows for efficient generation of diverse,
high-quality trajectories. Experiments on the NAVSIM benchmark confirm that
AnchDrive sets a new state-of-the-art and shows strong gen?eralizability

</details>


### [50] [HL-IK: A Lightweight Implementation of Human-Like Inverse Kinematics in Humanoid Arms](https://arxiv.org/abs/2509.20263)
*Bingjie Chen,Zihan Wang,Zhe Han,Guoping Pan,Yi Cheng,Houde Liu*

Main category: cs.RO

TL;DR: HL-IK是一个轻量级逆运动学框架，在保持末端执行器跟踪的同时，通过学习的肘部先验使整个手臂配置看起来更像人类，无需运行时全身感知。


<details>
  <summary>Details</summary>
Motivation: 传统的冗余人形机械臂逆运动学方法强调末端执行器跟踪，经常产生机械上有效但不类人的配置。

Method: 使用大规模人类运动数据训练FiLM调制的时空注意力网络(FiSTA)来预测下一步肘部姿态，并将其作为残差项与末端执行器和平滑度项一起整合到标准Levenberg-Marquardt优化器中。

Result: 在183k仿真步骤中，HL-IK将手臂相似性位置和方向误差平均降低了30.6%和35.4%，在最具挑战性的轨迹上降低了42.2%和47.4%。硬件遥操作进一步证实了拟人化的提升。

Conclusion: HL-IK易于集成，通过管道可跨平台适配，计算开销最小，为人形机器人实现了类人运动。

Abstract: Traditional IK methods for redundant humanoid manipulators emphasize
end-effector (EE) tracking, frequently producing configurations that are valid
mechanically but not human-like. We present Human-Like Inverse Kinematics
(HL-IK), a lightweight IK framework that preserves EE tracking while shaping
whole-arm configurations to appear human-like, without full-body sensing at
runtime. The key idea is a learned elbow prior: using large-scale human motion
data retargeted to the robot, we train a FiLM-modulated spatio-temporal
attention network (FiSTA) to predict the next-step elbow pose from the EE
target and a short history of EE-elbow states.This prediction is incorporated
as a small residual alongside EE and smoothness terms in a standard
Levenberg-Marquardt optimizer, making HL-IK a drop-in addition to numerical IK
stacks. Over 183k simulation steps, HL-IK reduces arm-similarity position and
direction error by 30.6% and 35.4% on average, and by 42.2% and 47.4% on the
most challenging trajectories. Hardware teleoperation on a robot distinct from
simulation further confirms the gains in anthropomorphism. HL-IK is simple to
integrate, adaptable across platforms via our pipeline, and adds minimal
computation, enabling human-like motions for humanoid robots. Project page:
https://hl-ik.github.io/

</details>


### [51] [Parse-Augment-Distill: Learning Generalizable Bimanual Visuomotor Policies from Single Human Video](https://arxiv.org/abs/2509.20286)
*Georgios Tziafas,Jiayun Zhang,Hamidreza Kasaei*

Main category: cs.RO

TL;DR: PAD框架通过解析人类视频演示、无模拟器的大规模演示增强和轨迹蒸馏，从单个人类视频学习可泛化的双手操作策略，在真实世界任务中实现一次性策略泛化。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法需要大量遥操作数据收集、难以泛化到分布外场景的问题，同时避免模拟器引入的sim-to-real差距。

Method: 三步骤框架：(a)解析人类视频为机器人可执行的关键点-动作轨迹；(b)使用双手任务和运动规划进行无模拟器的大规模演示增强；(c)将增强轨迹蒸馏为关键点条件策略。

Result: PAD在成功率和样本/成本效率上优于依赖图像策略和模拟器滚动的现有双手演示增强方法，在6个真实世界双手任务中实现一次性策略泛化。

Conclusion: PAD框架成功实现了从单个人类视频学习可泛化双手策略，在未见过的空间布置、物体实例和背景干扰下保持良好性能。

Abstract: Learning visuomotor policies from expert demonstrations is an important
frontier in modern robotics research, however, most popular methods require
copious efforts for collecting teleoperation data and struggle to generalize
out-ofdistribution. Scaling data collection has been explored through
leveraging human videos, as well as demonstration augmentation techniques. The
latter approach typically requires expensive simulation rollouts and trains
policies with synthetic image data, therefore introducing a sim-to-real gap. In
parallel, alternative state representations such as keypoints have shown great
promise for category-level generalization. In this work, we bring these avenues
together in a unified framework: PAD (Parse-AugmentDistill), for learning
generalizable bimanual policies from a single human video. Our method relies on
three steps: (a) parsing a human video demo into a robot-executable
keypoint-action trajectory, (b) employing bimanual task-and-motion-planning to
augment the demonstration at scale without simulators, and (c) distilling the
augmented trajectories into a keypoint-conditioned policy. Empirically, we
showcase that PAD outperforms state-ofthe-art bimanual demonstration
augmentation works relying on image policies with simulation rollouts, both in
terms of success rate and sample/cost efficiency. We deploy our framework in
six diverse real-world bimanual tasks such as pouring drinks, cleaning trash
and opening containers, producing one-shot policies that generalize in unseen
spatial arrangements, object instances and background distractors.
Supplementary material can be found in the project webpage
https://gtziafas.github.io/PAD_project/.

</details>


### [52] [mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies](https://arxiv.org/abs/2509.20297)
*Remo Steiner,Alexander Millane,David Tingdahl,Clemens Volk,Vikram Ramasamy,Xinjie Yao,Peter Du,Soha Pouya,Shiwei Sheng*

Main category: cs.RO

TL;DR: 本文提出了mindmap（深度特征图中的空间记忆用于3D动作策略），这是一种3D扩散策略，基于环境的语义3D重建生成机器人轨迹，旨在解决机器人学习系统中空间记忆机制的问题。


<details>
  <summary>Details</summary>
Motivation: 在机器人操作任务中，相关物体需要进出机器人的视野范围，因此空间记忆（记住场景空间构成的能力）是一个重要能力。然而，在机器人学习系统中构建这种机制仍然是一个开放的研究问题。

Method: 引入mindmap方法，这是一种3D扩散策略，通过语义3D重建环境来生成机器人轨迹。该方法利用深度特征图来维护空间记忆。

Result: 仿真实验表明，该方法在解决那些没有记忆机制的最先进方法难以处理的任务时非常有效。

Conclusion: mindmap方法为解决机器人学习中的空间记忆问题提供了有效方案，作者发布了重建系统、训练代码和评估任务以促进该方向的研究。

Abstract: End-to-end learning of robot control policies, structured as neural networks,
has emerged as a promising approach to robotic manipulation. To complete many
common tasks, relevant objects are required to pass in and out of a robot's
field of view. In these settings, spatial memory - the ability to remember the
spatial composition of the scene - is an important competency. However,
building such mechanisms into robot learning systems remains an open research
problem. We introduce mindmap (Spatial Memory in Deep Feature Maps for 3D
Action Policies), a 3D diffusion policy that generates robot trajectories based
on a semantic 3D reconstruction of the environment. We show in simulation
experiments that our approach is effective at solving tasks where
state-of-the-art approaches without memory mechanisms struggle. We release our
reconstruction system, training code, and evaluation tasks to spur research in
this direction.

</details>


### [53] [VisualMimic: Visual Humanoid Loco-Manipulation via Motion Tracking and Generation](https://arxiv.org/abs/2509.20322)
*Shaofeng Yin,Yanjie Ze,Hong-Xing Yu,C. Karen Liu,Jiajun Wu*

Main category: cs.RO

TL;DR: VisualMimic是一个视觉模拟到现实的框架，将自我中心视觉与分层全身控制相结合，使类人机器人能够在非结构化环境中完成各种运动操作任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖外部运动捕捉系统，要么无法在不同任务间泛化。需要开发一个能够整合自我中心感知和全身控制的统一框架。

Method: 采用教师-学生方案从人类运动数据训练任务无关的低级关键点跟踪器，结合任务特定的高级策略从视觉和本体感觉输入生成关键点命令。通过注入噪声和截断动作确保训练稳定性。

Result: 实现了从模拟到真实类人机器人的零样本迁移，完成了举箱、推箱、足球运球和踢球等多种运动操作任务，并在室外环境中表现出鲁棒泛化能力。

Conclusion: VisualMimic框架成功统一了自我中心视觉和分层全身控制，为类人机器人在非结构化环境中的运动操作提供了有效的解决方案。

Abstract: Humanoid loco-manipulation in unstructured environments demands tight
integration of egocentric perception and whole-body control. However, existing
approaches either depend on external motion capture systems or fail to
generalize across diverse tasks. We introduce VisualMimic, a visual sim-to-real
framework that unifies egocentric vision with hierarchical whole-body control
for humanoid robots. VisualMimic combines a task-agnostic low-level keypoint
tracker -- trained from human motion data via a teacher-student scheme -- with
a task-specific high-level policy that generates keypoint commands from visual
and proprioceptive input. To ensure stable training, we inject noise into the
low-level policy and clip high-level actions using human motion statistics.
VisualMimic enables zero-shot transfer of visuomotor policies trained in
simulation to real humanoid robots, accomplishing a wide range of
loco-manipulation tasks such as box lifting, pushing, football dribbling, and
kicking. Beyond controlled laboratory settings, our policies also generalize
robustly to outdoor environments. Videos are available at:
https://visualmimic.github.io .

</details>


### [54] [BBoE: Leveraging Bundle of Edges for Kinodynamic Bidirectional Motion Planning](https://arxiv.org/abs/2509.20333)
*Srikrishna Bangalore Raghu,Alessandro Roncone*

Main category: cs.RO

TL;DR: BBoE是一种双向、运动动力学的基于采样的运动规划器，能够在不同障碍物密度的环境中快速找到低成本解决方案。


<details>
  <summary>Details</summary>
Motivation: 为了解决在复杂障碍物环境中快速找到可行且低成本运动路径的问题，特别是在机器人运动动力学约束下的规划挑战。

Method: 结合探索和利用策略，依赖预计算的机器人状态遍历，通过排序和序列化预处理的前向传播来导航障碍物丰富的空间。

Result: 提出的框架相比之前方法减少了规划时间，降低了解决方案成本，并提高了成功率。

Conclusion: BBoE是一个鲁棒的双向运动动力学规划器，能够高效产生快速可行的解决方案。

Abstract: In this work, we introduce BBoE, a bidirectional, kinodynamic, sampling-based
motion planner that consistently and quickly finds low-cost solutions in
environments with varying obstacle clutter. The algorithm combines exploration
and exploitation while relying on precomputed robot state traversals, resulting
in efficient convergence towards the goal. Our key contributions include: i) a
strategy to navigate through obstacle-rich spaces by sorting and sequencing
preprocessed forward propagations; and ii) BBoE, a robust bidirectional
kinodynamic planner that utilizes this strategy to produce fast and feasible
solutions. The proposed framework reduces planning time, diminishes solution
cost and increases success rate in comparison to previous approaches.

</details>
