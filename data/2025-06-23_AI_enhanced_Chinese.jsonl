{"id": "2506.15788", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.15788", "abs": "https://arxiv.org/abs/2506.15788", "authors": ["Baxi Chong", "Juntao He", "Daniel Irvine", "Tianyu Wang", "Esteban Flores", "Daniel Soto", "Jianfeng Lin", "Zhaochen Xu", "Vincent R Nienhusser", "Grigoriy Blekherman", "Daniel I. Goldman"], "title": "Robust control for multi-legged elongate robots in noisy environments", "comment": null, "summary": "Modern two and four legged robots exhibit impressive mobility on complex\nterrain, largely attributed to advancement in learning algorithms. However,\nthese systems often rely on high-bandwidth sensing and onboard computation to\nperceive/respond to terrain uncertainties. Further, current locomotion\nstrategies typically require extensive robot-specific training, limiting their\ngeneralizability across platforms. Building on our prior research connecting\nrobot-environment interaction and communication theory, we develop a new\nparadigm to construct robust and simply controlled multi-legged elongate robots\n(MERs) capable of operating effectively in cluttered, unstructured\nenvironments. In this framework, each leg-ground contact is thought of as a\nbasic active contact (bac), akin to bits in signal transmission. Reliable\nlocomotion can be achieved in open-loop on \"noisy\" landscapes via sufficient\nredundancy in bacs. In such situations, robustness is achieved through passive\nmechanical responses. We term such processes as those displaying mechanical\nintelligence (MI) and analogize these processes to forward error correction\n(FEC) in signal transmission. To augment MI, we develop feedback control\nschemes, which we refer to as computational intelligence (CI) and such\nprocesses analogize automatic repeat request (ARQ) in signal transmission.\nIntegration of these analogies between locomotion and communication theory\nallow analysis, design, and prediction of embodied intelligence control schemes\n(integrating MI and CI) in MERs, showing effective and reliable performance\n(approximately half body lengths per cycle) on complex landscapes with terrain\n\"noise\" over twice the robot's height. Our work provides a foundation for\nsystematic development of MER control, paving the way for terrain-agnostic,\nagile, and resilient robotic systems capable of operating in extreme\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u68b0\u667a\u80fd\uff08MI\uff09\u548c\u8ba1\u7b97\u667a\u80fd\uff08CI\uff09\u7684\u65b0\u578b\u591a\u8db3\u673a\u5668\u4eba\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u901a\u4fe1\u7406\u8bba\u4e2d\u7684\u524d\u5411\u7ea0\u9519\uff08FEC\uff09\u548c\u81ea\u52a8\u91cd\u4f20\u8bf7\u6c42\uff08ARQ\uff09\uff0c\u5b9e\u73b0\u4e86\u5728\u590d\u6742\u5730\u5f62\u4e2d\u7684\u9c81\u68d2\u8fd0\u52a8\u3002", "motivation": "\u5f53\u524d\u591a\u8db3\u673a\u5668\u4eba\u4f9d\u8d56\u9ad8\u5e26\u5bbd\u4f20\u611f\u548c\u8ba1\u7b97\uff0c\u4e14\u8bad\u7ec3\u8fc7\u7a0b\u590d\u6742\uff0c\u7f3a\u4e4f\u8de8\u5e73\u53f0\u901a\u7528\u6027\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u7b80\u5355\u3001\u9c81\u68d2\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6742\u4e71\u3001\u975e\u7ed3\u6784\u5316\u73af\u5883\u3002", "method": "\u5c06\u6bcf\u6761\u817f\u4e0e\u5730\u9762\u7684\u63a5\u89e6\u89c6\u4e3a\u57fa\u672c\u4e3b\u52a8\u63a5\u89e6\uff08bac\uff09\uff0c\u6a21\u62df\u901a\u4fe1\u4e2d\u7684\u6bd4\u7279\u4f20\u8f93\u3002\u901a\u8fc7\u5197\u4f59bac\u5b9e\u73b0\u88ab\u52a8\u673a\u68b0\u54cd\u5e94\uff08MI\uff09\uff0c\u5e76\u7ed3\u5408\u53cd\u9988\u63a7\u5236\uff08CI\uff09\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5728\u590d\u6742\u5730\u5f62\uff08\u566a\u58f0\u9ad8\u5ea6\u8d85\u8fc7\u673a\u5668\u4eba\u9ad8\u5ea6\u7684\u4e24\u500d\uff09\u4e2d\uff0c\u673a\u5668\u4eba\u8868\u73b0\u51fa\u6709\u6548\u4e14\u53ef\u9760\u7684\u8fd0\u52a8\u6027\u80fd\uff08\u6bcf\u5468\u671f\u7ea6\u534a\u4e2a\u8eab\u957f\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u8db3\u673a\u5668\u4eba\u63a7\u5236\u7684\u7cfb\u7edf\u5316\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4e3a\u6781\u7aef\u73af\u5883\u4e2d\u7684\u654f\u6377\u3001\u9c81\u68d2\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.15799", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15799", "abs": "https://arxiv.org/abs/2506.15799", "authors": ["Andrew Wagenmaker", "Mitsuhiko Nakamoto", "Yunchu Zhang", "Seohong Park", "Waleed Yagoub", "Anusha Nagabandi", "Abhishek Gupta", "Sergey Levine"], "title": "Steering Your Diffusion Policy with Latent Space Reinforcement Learning", "comment": null, "summary": "Robotic control policies learned from human demonstrations have achieved\nimpressive results in many real-world applications. However, in scenarios where\ninitial performance is not satisfactory, as is often the case in novel\nopen-world settings, such behavioral cloning (BC)-learned policies typically\nrequire collecting additional human demonstrations to further improve their\nbehavior -- an expensive and time-consuming process. In contrast, reinforcement\nlearning (RL) holds the promise of enabling autonomous online policy\nimprovement, but often falls short of achieving this due to the large number of\nsamples it typically requires. In this work we take steps towards enabling fast\nautonomous adaptation of BC-trained policies via efficient real-world RL.\nFocusing in particular on diffusion policies -- a state-of-the-art BC\nmethodology -- we propose diffusion steering via reinforcement learning (DSRL):\nadapting the BC policy by running RL over its latent-noise space. We show that\nDSRL is highly sample efficient, requires only black-box access to the BC\npolicy, and enables effective real-world autonomous policy improvement.\nFurthermore, DSRL avoids many of the challenges associated with finetuning\ndiffusion policies, obviating the need to modify the weights of the base policy\nat all. We demonstrate DSRL on simulated benchmarks, real-world robotic tasks,\nand for adapting pretrained generalist policies, illustrating its sample\nefficiency and effective performance at real-world policy improvement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDSRL\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u6f5c\u5728\u566a\u58f0\u7a7a\u95f4\u4e2d\u8c03\u6574\u57fa\u4e8e\u884c\u4e3a\u514b\u9686\u7684\u6269\u6563\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u81ea\u9002\u5e94\u6539\u8fdb\u3002", "motivation": "\u884c\u4e3a\u514b\u9686\u7b56\u7565\u5728\u521d\u59cb\u6027\u80fd\u4e0d\u8db3\u65f6\u9700\u8981\u989d\u5916\u7684\u4eba\u7c7b\u6f14\u793a\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u81ea\u4e3b\u6539\u8fdb\u4f46\u6837\u672c\u6548\u7387\u4f4e\u3002DSRL\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u5feb\u901f\u81ea\u9002\u5e94\u3002", "method": "DSRL\u5728\u6269\u6563\u7b56\u7565\u7684\u6f5c\u5728\u566a\u58f0\u7a7a\u95f4\u4e2d\u8fd0\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u65e0\u9700\u4fee\u6539\u57fa\u7840\u7b56\u7565\u6743\u91cd\uff0c\u4ec5\u9700\u9ed1\u76d2\u8bbf\u95ee\u3002", "result": "DSRL\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6837\u672c\u6548\u7387\u548c\u6709\u6548\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "DSRL\u4e3a\u884c\u4e3a\u514b\u9686\u7b56\u7565\u7684\u5feb\u901f\u81ea\u9002\u5e94\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.15828", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15828", "abs": "https://arxiv.org/abs/2506.15828", "authors": ["Emanuele Musumeci", "Michele Brienza", "Francesco Argenziano", "Vincenzo Suriani", "Daniele Nardi", "Domenico D. Bloisi"], "title": "Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning", "comment": null, "summary": "Classical planning in AI and Robotics addresses complex tasks by shifting\nfrom imperative to declarative approaches (e.g., PDDL). However, these methods\noften fail in real scenarios due to limited robot perception and the need to\nground perceptions to planning predicates. This often results in heavily\nhard-coded behaviors that struggle to adapt, even with scenarios where goals\ncan be achieved through relaxed planning. Meanwhile, Large Language Models\n(LLMs) lead to planning systems that leverage commonsense reasoning but often\nat the cost of generating unfeasible and/or unsafe plans. To address these\nlimitations, we present an approach integrating classical planning with LLMs,\nleveraging their ability to extract commonsense knowledge and ground actions.\nWe propose a hierarchical formulation that enables robots to make unfeasible\ntasks tractable by defining functionally equivalent goals through gradual\nrelaxation. This mechanism supports partial achievement of the intended\nobjective, suited to the agent's specific context. Our method demonstrates its\nability to adapt and execute tasks effectively within environments modeled\nusing 3D Scene Graphs through comprehensive qualitative and quantitative\nevaluations. We also show how this method succeeds in complex scenarios where\nother benchmark methods are more likely to fail. Code, dataset, and additional\nmaterial are released to the community.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ecf\u5178\u89c4\u5212\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u9002\u5e94\u6027\u548c\u53ef\u884c\u6027\u95ee\u9898\u3002", "motivation": "\u7ecf\u5178\u89c4\u5212\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u56e0\u611f\u77e5\u9650\u5236\u548c\u96be\u4ee5\u5c06\u611f\u77e5\u6620\u5c04\u5230\u89c4\u5212\u8c13\u8bcd\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u800cLLMs\u867d\u80fd\u5229\u7528\u5e38\u8bc6\u63a8\u7406\u4f46\u5e38\u751f\u6210\u4e0d\u53ef\u884c\u6216\u4e0d\u5b89\u5168\u7684\u8ba1\u5212\u3002", "method": "\u901a\u8fc7\u5c42\u6b21\u5316\u65b9\u6cd5\u6574\u5408\u7ecf\u5178\u89c4\u5212\u4e0eLLMs\uff0c\u5229\u7528LLMs\u63d0\u53d6\u5e38\u8bc6\u77e5\u8bc6\u5e76\u9010\u6b65\u653e\u677e\u76ee\u6807\u5b9a\u4e49\uff0c\u4f7f\u4efb\u52a1\u53ef\u884c\u3002", "result": "\u65b9\u6cd5\u57283D\u573a\u666f\u56fe\u4e2d\u8868\u73b0\u51fa\u9002\u5e94\u6027\u548c\u9ad8\u6548\u6267\u884c\u80fd\u529b\uff0c\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.15847", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15847", "abs": "https://arxiv.org/abs/2506.15847", "authors": ["Arpit Bahety", "Arnav Balaji", "Ben Abbatematteo", "Roberto Mart\u00edn-Mart\u00edn"], "title": "SafeMimic: Towards Safe and Autonomous Human-to-Robot Imitation for Mobile Manipulation", "comment": null, "summary": "For robots to become efficient helpers in the home, they must learn to\nperform new mobile manipulation tasks simply by watching humans perform them.\nLearning from a single video demonstration from a human is challenging as the\nrobot needs to first extract from the demo what needs to be done and how,\ntranslate the strategy from a third to a first-person perspective, and then\nadapt it to be successful with its own morphology. Furthermore, to mitigate the\ndependency on costly human monitoring, this learning process should be\nperformed in a safe and autonomous manner. We present SafeMimic, a framework to\nlearn new mobile manipulation skills safely and autonomously from a single\nthird-person human video. Given an initial human video demonstration of a\nmulti-step mobile manipulation task, SafeMimic first parses the video into\nsegments, inferring both the semantic changes caused and the motions the human\nexecuted to achieve them and translating them to an egocentric reference. Then,\nit adapts the behavior to the robot's own morphology by sampling candidate\nactions around the human ones, and verifying them for safety before execution\nin a receding horizon fashion using an ensemble of safety Q-functions trained\nin simulation. When safe forward progression is not possible, SafeMimic\nbacktracks to previous states and attempts a different sequence of actions,\nadapting both the trajectory and the grasping modes when required for its\nmorphology. As a result, SafeMimic yields a strategy that succeeds in the\ndemonstrated behavior and learns task-specific actions that reduce exploration\nin future attempts. Our experiments show that our method allows robots to\nsafely and efficiently learn multi-step mobile manipulation behaviors from a\nsingle human demonstration, from different users, and in different\nenvironments, with improvements over state-of-the-art baselines across seven\ntasks", "AI": {"tldr": "SafeMimic\u6846\u67b6\u901a\u8fc7\u5355\u6b21\u4eba\u7c7b\u89c6\u9891\u6f14\u793a\uff0c\u8ba9\u673a\u5668\u4eba\u5b89\u5168\u81ea\u4e3b\u5730\u5b66\u4e60\u79fb\u52a8\u64cd\u4f5c\u6280\u80fd\uff0c\u51cf\u5c11\u63a2\u7d22\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u4ece\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u7684\u6311\u6218\uff0c\u5305\u62ec\u89c6\u89d2\u8f6c\u6362\u3001\u5f62\u6001\u9002\u5e94\u548c\u5b89\u5168\u81ea\u4e3b\u5b66\u4e60\u7684\u4f9d\u8d56\u3002", "method": "\u89e3\u6790\u89c6\u9891\u4e3a\u5206\u6bb5\uff0c\u63a8\u65ad\u8bed\u4e49\u53d8\u5316\u548c\u52a8\u4f5c\uff0c\u8f6c\u6362\u4e3a\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\uff0c\u91c7\u6837\u5019\u9009\u52a8\u4f5c\u5e76\u901a\u8fc7\u5b89\u5168Q\u51fd\u6570\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSafeMimic\u80fd\u5728\u4e0d\u540c\u73af\u5883\u548c\u7528\u6237\u4e2d\u5b89\u5168\u9ad8\u6548\u5b66\u4e60\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SafeMimic\u4e3a\u673a\u5668\u4eba\u4ece\u5355\u6b21\u4eba\u7c7b\u6f14\u793a\u4e2d\u5b66\u4e60\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.15849", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15849", "abs": "https://arxiv.org/abs/2506.15849", "authors": ["Kirill Muravyev", "Vasily Yuryev", "Oleg Bulichev", "Dmitry Yudin", "Konstantin Yakovlev"], "title": "PRISM-Loc: a Lightweight Long-range LiDAR Localization in Urban Environments with Topological Maps", "comment": "This version was submitted and rejected from IROS 2025 conference", "summary": "Localization in the environment is one of the crucial tasks of navigation of\na mobile robot or a self-driving vehicle. For long-range routes, performing\nlocalization within a dense global lidar map in real time may be difficult, and\nthe creation of such a map may require much memory. To this end, leveraging\ntopological maps may be useful. In this work, we propose PRISM-Loc -- a\ntopological map-based approach for localization in large environments. The\nproposed approach leverages a twofold localization pipeline, which consists of\nglobal place recognition and estimation of the local pose inside the found\nlocation. For local pose estimation, we introduce an original lidar scan\nmatching algorithm, which is based on 2D features and point-based optimization.\nWe evaluate the proposed method on the ITLP-Campus dataset on a 3 km route, and\ncompare it against the state-of-the-art metric map-based and place\nrecognition-based competitors. The results of the experiments show that the\nproposed method outperforms its competitors both quality-wise and\ncomputationally-wise.", "AI": {"tldr": "PRISM-Loc\u662f\u4e00\u79cd\u57fa\u4e8e\u62d3\u6251\u5730\u56fe\u7684\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u578b\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u5b9a\u4f4d\uff0c\u7ed3\u5408\u5168\u5c40\u5730\u70b9\u8bc6\u522b\u548c\u5c40\u90e8\u4f4d\u59ff\u4f30\u8ba1\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u957f\u8ddd\u79bb\u5bfc\u822a\u4e2d\uff0c\u4f7f\u7528\u5bc6\u96c6\u5168\u5c40\u6fc0\u5149\u96f7\u8fbe\u5730\u56fe\u8fdb\u884c\u5b9e\u65f6\u5b9a\u4f4d\u53ef\u80fd\u56f0\u96be\u4e14\u5360\u7528\u5185\u5b58\uff0c\u62d3\u6251\u5730\u56fe\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51faPRISM-Loc\u65b9\u6cd5\uff0c\u91c7\u7528\u53cc\u91cd\u5b9a\u4f4d\u6d41\u7a0b\uff1a\u5168\u5c40\u5730\u70b9\u8bc6\u522b\u548c\u5c40\u90e8\u4f4d\u59ff\u4f30\u8ba1\uff08\u57fa\u4e8e2D\u7279\u5f81\u548c\u70b9\u4f18\u5316\u7684\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u5339\u914d\u7b97\u6cd5\uff09\u3002", "result": "\u57283\u516c\u91cc\u8def\u7ebf\u7684ITLP-Campus\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cPRISM-Loc\u5728\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PRISM-Loc\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u62d3\u6251\u5730\u56fe\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u578b\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u5bfc\u822a\u3002"}}
{"id": "2506.15851", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15851", "abs": "https://arxiv.org/abs/2506.15851", "authors": ["Qiyuan Wu", "Mark Campbell"], "title": "Semantic and Feature Guided Uncertainty Quantification of Visual Localization for Autonomous Vehicles", "comment": "Accepted by ICRA 2025", "summary": "The uncertainty quantification of sensor measurements coupled with deep\nlearning networks is crucial for many robotics systems, especially for\nsafety-critical applications such as self-driving cars. This paper develops an\nuncertainty quantification approach in the context of visual localization for\nautonomous driving, where locations are selected based on images. Key to our\napproach is to learn the measurement uncertainty using light-weight sensor\nerror model, which maps both image feature and semantic information to\n2-dimensional error distribution. Our approach enables uncertainty estimation\nconditioned on the specific context of the matched image pair, implicitly\ncapturing other critical, unannotated factors (e.g., city vs highway, dynamic\nvs static scenes, winter vs summer) in a latent manner. We demonstrate the\naccuracy of our uncertainty prediction framework using the Ithaca365 dataset,\nwhich includes variations in lighting and weather (sunny, night, snowy). Both\nthe uncertainty quantification of the sensor+network is evaluated, along with\nBayesian localization filters using unique sensor gating method. Results show\nthat the measurement error does not follow a Gaussian distribution with poor\nweather and lighting conditions, and is better predicted by our Gaussian\nMixture model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u89c6\u89c9\u5b9a\u4f4d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4f20\u611f\u5668\u8bef\u5dee\u6a21\u578b\u5b66\u4e60\u6d4b\u91cf\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728Ithaca365\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u611f\u5668\u6d4b\u91cf\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5bf9\u673a\u5668\u4eba\u7cfb\u7edf\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\uff09\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u4f20\u611f\u5668\u8bef\u5dee\u6a21\u578b\uff0c\u5c06\u56fe\u50cf\u7279\u5f81\u548c\u8bed\u4e49\u4fe1\u606f\u6620\u5c04\u5230\u4e8c\u7ef4\u8bef\u5dee\u5206\u5e03\uff0c\u4ee5\u5b66\u4e60\u6d4b\u91cf\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6076\u52a3\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\u4e0b\uff0c\u6d4b\u91cf\u8bef\u5dee\u4e0d\u7b26\u5408\u9ad8\u65af\u5206\u5e03\uff0c\u800c\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u80fd\u66f4\u597d\u5730\u9884\u6d4b\u8bef\u5dee\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u5339\u914d\u56fe\u50cf\u5bf9\u7684\u7279\u5b9a\u4e0a\u4e0b\u6587\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5e76\u9690\u542b\u6355\u83b7\u5176\u4ed6\u5173\u952e\u56e0\u7d20\uff08\u5982\u57ce\u5e02\u4e0e\u9ad8\u901f\u516c\u8def\u3001\u52a8\u6001\u4e0e\u9759\u6001\u573a\u666f\u3001\u51ac\u5b63\u4e0e\u590f\u5b63\uff09\u3002"}}
{"id": "2506.15865", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15865", "abs": "https://arxiv.org/abs/2506.15865", "authors": ["Viral Rasik Galaiya"], "title": "Improving Robotic Manipulation: Techniques for Object Pose Estimation, Accommodating Positional Uncertainty, and Disassembly Tasks from Examples", "comment": "Thesis", "summary": "To use robots in more unstructured environments, we have to accommodate for\nmore complexities. Robotic systems need more awareness of the environment to\nadapt to uncertainty and variability. Although cameras have been predominantly\nused in robotic tasks, the limitations that come with them, such as occlusion,\nvisibility and breadth of information, have diverted some focus to tactile\nsensing. In this thesis, we explore the use of tactile sensing to determine the\npose of the object using the temporal features. We then use reinforcement\nlearning with tactile collisions to reduce the number of attempts required to\ngrasp an object resulting from positional uncertainty from camera estimates.\nFinally, we use information provided by these tactile sensors to a\nreinforcement learning agent to determine the trajectory to take to remove an\nobject from a restricted passage while reducing training time by pertaining\nfrom human examples.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u4f7f\u7528\u89e6\u89c9\u4f20\u611f\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u63d0\u5347\u673a\u5668\u4eba\u5bf9\u7269\u4f53\u59ff\u6001\u7684\u611f\u77e5\u548c\u6293\u53d6\u6548\u7387\u3002", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u66f4\u5f3a\u7684\u73af\u5883\u611f\u77e5\u80fd\u529b\u4ee5\u5e94\u5bf9\u4e0d\u786e\u5b9a\u6027\u3002\u5c3d\u7ba1\u6444\u50cf\u5934\u5e38\u7528\u4e8e\u673a\u5668\u4eba\u4efb\u52a1\uff0c\u4f46\u5176\u5c40\u9650\u6027\uff08\u5982\u906e\u6321\u3001\u89c6\u91ce\u548c\u4fe1\u606f\u5e7f\u5ea6\uff09\u4fc3\u4f7f\u8f6c\u5411\u89e6\u89c9\u4f20\u611f\u7684\u7814\u7a76\u3002", "method": "\u5229\u7528\u89e6\u89c9\u4f20\u611f\u7684\u65f6\u5e8f\u7279\u5f81\u786e\u5b9a\u7269\u4f53\u59ff\u6001\uff1b\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u89e6\u89c9\u78b0\u649e\u51cf\u5c11\u6293\u53d6\u5c1d\u8bd5\u6b21\u6570\uff1b\u901a\u8fc7\u89e6\u89c9\u4f20\u611f\u5668\u4fe1\u606f\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u89c4\u5212\u8f68\u8ff9\uff0c\u5e76\u4ece\u4eba\u7c7b\u793a\u4f8b\u4e2d\u8fc1\u79fb\u5b66\u4e60\u4ee5\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002", "result": "\u89e6\u89c9\u4f20\u611f\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\u80fd\u591f\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u5bf9\u7269\u4f53\u59ff\u6001\u7684\u611f\u77e5\u548c\u6293\u53d6\u6548\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "\u89e6\u89c9\u4f20\u611f\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\u4e3a\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.15868", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15868", "abs": "https://arxiv.org/abs/2506.15868", "authors": ["Mingyue Lei", "Zewei Zhou", "Hongchen Li", "Jia Hu", "Jiaqi Ma"], "title": "CooperRisk: A Driving Risk Quantification Pipeline with Multi-Agent Cooperative Perception and Prediction", "comment": "IROS2025", "summary": "Risk quantification is a critical component of safe autonomous driving,\nhowever, constrained by the limited perception range and occlusion of\nsingle-vehicle systems in complex and dense scenarios. Vehicle-to-everything\n(V2X) paradigm has been a promising solution to sharing complementary\nperception information, nevertheless, how to ensure the risk interpretability\nwhile understanding multi-agent interaction with V2X remains an open question.\nIn this paper, we introduce the first V2X-enabled risk quantification pipeline,\nCooperRisk, to fuse perception information from multiple agents and quantify\nthe scenario driving risk in future multiple timestamps. The risk is\nrepresented as a scenario risk map to ensure interpretability based on risk\nseverity and exposure, and the multi-agent interaction is captured by the\nlearning-based cooperative prediction model. We carefully design a\nrisk-oriented transformer-based prediction model with multi-modality and\nmulti-agent considerations. It aims to ensure scene-consistent future behaviors\nof multiple agents and avoid conflicting predictions that could lead to overly\nconservative risk quantification and cause the ego vehicle to become overly\nhesitant to drive. Then, the temporal risk maps could serve to guide a model\npredictive control planner. We evaluate the CooperRisk pipeline in a real-world\nV2X dataset V2XPnP, and the experiments demonstrate its superior performance in\nrisk quantification, showing a 44.35% decrease in conflict rate between the ego\nvehicle and background traffic participants.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eV2X\u7684\u98ce\u9669\u91cf\u5316\u6846\u67b6CooperRisk\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u611f\u77e5\u4fe1\u606f\u878d\u5408\u548c\u672a\u6765\u65f6\u95f4\u6233\u7684\u98ce\u9669\u91cf\u5316\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u573a\u666f\u98ce\u9669\u5730\u56fe\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u5b66\u4e60\u7684\u534f\u4f5c\u9884\u6d4b\u6a21\u578b\u6355\u6349\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u3002", "motivation": "\u5355\u8f66\u8f86\u7cfb\u7edf\u5728\u590d\u6742\u5bc6\u96c6\u573a\u666f\u4e2d\u7684\u611f\u77e5\u8303\u56f4\u548c\u906e\u6321\u9650\u5236\u98ce\u9669\u91cf\u5316\u80fd\u529b\uff0cV2X\u867d\u80fd\u5171\u4eab\u611f\u77e5\u4fe1\u606f\uff0c\u4f46\u5982\u4f55\u786e\u4fdd\u98ce\u9669\u53ef\u89e3\u91ca\u6027\u5e76\u7406\u89e3\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8eTransformer\u7684\u98ce\u9669\u5bfc\u5411\u9884\u6d4b\u6a21\u578b\uff0c\u8003\u8651\u591a\u6a21\u6001\u548c\u591a\u667a\u80fd\u4f53\u56e0\u7d20\uff0c\u786e\u4fdd\u573a\u666f\u4e00\u81f4\u7684\u672a\u6765\u884c\u4e3a\u9884\u6d4b\uff0c\u907f\u514d\u51b2\u7a81\u9884\u6d4b\u5bfc\u81f4\u4fdd\u5b88\u98ce\u9669\u91cf\u5316\u3002", "result": "\u5728V2XPnP\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cCooperRisk\u5728\u98ce\u9669\u91cf\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u51b2\u7a81\u7387\u964d\u4f4e\u4e8644.35%\u3002", "conclusion": "CooperRisk\u901a\u8fc7V2X\u6280\u672f\u6709\u6548\u63d0\u5347\u4e86\u98ce\u9669\u91cf\u5316\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.15870", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.15870", "abs": "https://arxiv.org/abs/2506.15870", "authors": ["Hossein Maghsoumi", "Yaser Fallah"], "title": "A Small-Scale Robot for Autonomous Driving: Design, Challenges, and Best Practices", "comment": null, "summary": "Small-scale autonomous vehicle platforms provide a cost-effective environment\nfor developing and testing advanced driving systems. However, specific\nconfigurations within this scale are underrepresented, limiting full awareness\nof their potential. This paper focuses on a one-sixth-scale setup, offering a\nhigh-level overview of its design, hardware and software integration, and\ntypical challenges encountered during development. We discuss methods for\naddressing mechanical and electronic issues common to this scale and propose\nguidelines for improving reliability and performance. By sharing these\ninsights, we aim to expand the utility of small-scale vehicles for testing\nautonomous driving algorithms and to encourage further research in this domain.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u516d\u5206\u4e4b\u4e00\u6bd4\u4f8b\u7684\u5c0f\u578b\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5e73\u53f0\u7684\u8bbe\u8ba1\u3001\u786c\u4ef6\u4e0e\u8f6f\u4ef6\u96c6\u6210\u53ca\u5176\u5f00\u53d1\u4e2d\u7684\u5e38\u89c1\u6311\u6218\uff0c\u65e8\u5728\u63d0\u5347\u5176\u53ef\u9760\u6027\u548c\u6027\u80fd\uff0c\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002", "motivation": "\u5c0f\u578b\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5e73\u53f0\u6210\u672c\u6548\u76ca\u9ad8\uff0c\u4f46\u7279\u5b9a\u914d\u7f6e\u7684\u7814\u7a76\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5bf9\u5176\u6f5c\u529b\u7684\u5168\u9762\u8ba4\u8bc6\u3002", "method": "\u901a\u8fc7\u5206\u6790\u516d\u5206\u4e4b\u4e00\u6bd4\u4f8b\u5e73\u53f0\u7684\u8bbe\u8ba1\u3001\u786c\u4ef6\u4e0e\u8f6f\u4ef6\u96c6\u6210\uff0c\u63d0\u51fa\u89e3\u51b3\u673a\u68b0\u548c\u7535\u5b50\u95ee\u9898\u7684\u65b9\u6848\uff0c\u5e76\u5236\u5b9a\u6539\u8fdb\u6307\u5357\u3002", "result": "\u63d0\u4f9b\u4e86\u63d0\u5347\u5c0f\u578b\u8f66\u8f86\u5e73\u53f0\u53ef\u9760\u6027\u548c\u6027\u80fd\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u7b97\u6cd5\u6d4b\u8bd5\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u901a\u8fc7\u5206\u4eab\u7ecf\u9a8c\uff0c\u672c\u6587\u65e8\u5728\u4fc3\u8fdb\u5c0f\u578b\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5e73\u53f0\u7684\u7814\u7a76\u548c\u5e94\u7528\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.15890", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15890", "abs": "https://arxiv.org/abs/2506.15890", "authors": ["Thomas Manzini", "Priyankari Perali", "Robin R. Murphy", "David Merrick"], "title": "Challenges and Research Directions from the Operational Use of a Machine Learning Damage Assessment System via Small Uncrewed Aerial Systems at Hurricanes Debby and Helene", "comment": "6 pages, 5 Figures, 1 Table", "summary": "This paper details four principal challenges encountered with machine\nlearning (ML) damage assessment using small uncrewed aerial systems (sUAS) at\nHurricanes Debby and Helene that prevented, degraded, or delayed the delivery\nof data products during operations and suggests three research directions for\nfuture real-world deployments. The presence of these challenges is not\nsurprising given that a review of the literature considering both datasets and\nproposed ML models suggests this is the first sUAS-based ML system for disaster\ndamage assessment actually deployed as a part of real-world operations. The\nsUAS-based ML system was applied by the State of Florida to Hurricanes Helene\n(2 orthomosaics, 3.0 gigapixels collected over 2 sorties by a Wintra WingtraOne\nsUAS) and Debby (1 orthomosaic, 0.59 gigapixels collected via 1 sortie by a\nWintra WingtraOne sUAS) in Florida. The same model was applied to crewed aerial\nimagery of inland flood damage resulting from post-tropical remnants of\nHurricane Debby in Pennsylvania (436 orthophotos, 136.5 gigapixels), providing\nfurther insights into the advantages and limitations of sUAS for disaster\nresponse. The four challenges (variationin spatial resolution of input imagery,\nspatial misalignment between imagery and geospatial data, wireless\nconnectivity, and data product format) lead to three recommendations that\nspecify research needed to improve ML model capabilities to accommodate the\nwide variation of potential spatial resolutions used in practice, handle\nspatial misalignment, and minimize the dependency on wireless connectivity.\nThese recommendations are expected to improve the effective operational use of\nsUAS and sUAS-based ML damage assessment systems for disaster response.", "AI": {"tldr": "\u8bba\u6587\u603b\u7ed3\u4e86\u5728\u98d3\u98ceDebby\u548cHelene\u4e2d\u4f7f\u7528\u5c0f\u578b\u65e0\u4eba\u673a\uff08sUAS\uff09\u8fdb\u884c\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u635f\u5bb3\u8bc4\u4f30\u65f6\u9047\u5230\u7684\u56db\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u4e2a\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5b9e\u9645\u90e8\u7f72\u4e2dsUAS\u548cML\u7cfb\u7edf\u5728\u707e\u5bb3\u635f\u5bb3\u8bc4\u4f30\u4e2d\u7684\u64cd\u4f5c\u6027\u95ee\u9898\uff0c\u586b\u8865\u4e86\u6587\u732e\u4e2d\u5b9e\u9645\u90e8\u7f72\u6848\u4f8b\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u98d3\u98ceDebby\u548cHelene\u4e2d\u6536\u96c6\u7684\u65e0\u4eba\u673a\u56fe\u50cf\u6570\u636e\uff08\u5305\u62ec\u4e0d\u540c\u5206\u8fa8\u7387\u548c\u5730\u7406\u7a7a\u95f4\u5bf9\u9f50\u95ee\u9898\uff09\uff0c\u8bc4\u4f30ML\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u8bc6\u522b\u4e86\u56db\u4e2a\u4e3b\u8981\u6311\u6218\uff08\u56fe\u50cf\u5206\u8fa8\u7387\u53d8\u5316\u3001\u5730\u7406\u7a7a\u95f4\u5bf9\u9f50\u95ee\u9898\u3001\u65e0\u7ebf\u8fde\u63a5\u4f9d\u8d56\u6027\u548c\u6570\u636e\u683c\u5f0f\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u4e2a\u6539\u8fdb\u5efa\u8bae\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u63d0\u9ad8ML\u6a21\u578b\u5bf9\u5206\u8fa8\u7387\u53d8\u5316\u7684\u9002\u5e94\u6027\u3001\u5904\u7406\u5730\u7406\u7a7a\u95f4\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u51cf\u5c11\u5bf9\u65e0\u7ebf\u8fde\u63a5\u7684\u4f9d\u8d56\uff0c\u4ee5\u63d0\u5347\u707e\u5bb3\u54cd\u5e94\u6548\u7387\u3002"}}
{"id": "2506.15899", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.15899", "abs": "https://arxiv.org/abs/2506.15899", "authors": ["Israel Charles", "Hossein Maghsoumi", "Yaser Fallah"], "title": "Advancing Autonomous Racing: A Comprehensive Survey of the RoboRacer (F1TENTH) Platform", "comment": null, "summary": "The RoboRacer (F1TENTH) platform has emerged as a leading testbed for\nadvancing autonomous driving research, offering a scalable, cost-effective, and\ncommunity-driven environment for experimentation. This paper presents a\ncomprehensive survey of the platform, analyzing its modular hardware and\nsoftware architecture, diverse research applications, and role in autonomous\nsystems education. We examine critical aspects such as bridging the\nsimulation-to-reality (Sim2Real) gap, integration with simulation environments,\nand the availability of standardized datasets and benchmarks. Furthermore, the\nsurvey highlights advancements in perception, planning, and control algorithms,\nas well as insights from global competitions and collaborative research\nefforts. By consolidating these contributions, this study positions RoboRacer\nas a versatile framework for accelerating innovation and bridging the gap\nbetween theoretical research and real-world deployment. The findings underscore\nthe platform's significance in driving forward developments in autonomous\nracing and robotics.", "AI": {"tldr": "\u672c\u6587\u5bf9RoboRacer\uff08F1TENTH\uff09\u5e73\u53f0\u8fdb\u884c\u4e86\u5168\u9762\u8c03\u67e5\uff0c\u5206\u6790\u4e86\u5176\u786c\u4ef6\u548c\u8f6f\u4ef6\u67b6\u6784\u3001\u7814\u7a76\u5e94\u7528\u53ca\u6559\u80b2\u4f5c\u7528\uff0c\u5f3a\u8c03\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "RoboRacer\u5e73\u53f0\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u7ecf\u6d4e\u9ad8\u6548\u4e14\u793e\u533a\u9a71\u52a8\u7684\u5b9e\u9a8c\u73af\u5883\uff0c\u672c\u6587\u65e8\u5728\u603b\u7ed3\u5176\u8d21\u732e\u548c\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5e73\u53f0\u7684\u6a21\u5757\u5316\u67b6\u6784\u3001Sim2Real\u6280\u672f\u3001\u4eff\u771f\u73af\u5883\u96c6\u6210\u3001\u6570\u636e\u96c6\u548c\u7b97\u6cd5\u8fdb\u5c55\uff0c\u4ee5\u53ca\u7ade\u8d5b\u548c\u5408\u4f5c\u7814\u7a76\u6210\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0RoboRacer\u662f\u52a0\u901f\u521b\u65b0\u548c\u8fde\u63a5\u7406\u8bba\u7814\u7a76\u4e0e\u5b9e\u9645\u90e8\u7f72\u7684\u591a\u529f\u80fd\u6846\u67b6\u3002", "conclusion": "RoboRacer\u5728\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u548c\u673a\u5668\u4eba\u9886\u57df\u7684\u53d1\u5c55\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2506.15920", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15920", "abs": "https://arxiv.org/abs/2506.15920", "authors": ["Liang Qin", "Weiwei Wan", "Jun Takahashi", "Ryo Negishi", "Masaki Matsushita", "Kensuke Harada"], "title": "Learning from Planned Data to Improve Robotic Pick-and-Place Planning Efficiency", "comment": null, "summary": "This work proposes a learning method to accelerate robotic pick-and-place\nplanning by predicting shared grasps. Shared grasps are defined as grasp poses\nfeasible to both the initial and goal object configurations in a pick-and-place\ntask. Traditional analytical methods for solving shared grasps evaluate grasp\ncandidates separately, leading to substantial computational overhead as the\ncandidate set grows. To overcome the limitation, we introduce an Energy-Based\nModel (EBM) that predicts shared grasps by combining the energies of feasible\ngrasps at both object poses. This formulation enables early identification of\npromising candidates and significantly reduces the search space. Experiments\nshow that our method improves grasp selection performance, offers higher data\nefficiency, and generalizes well to unseen grasps and similarly shaped objects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u6a21\u578b\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a0\u901f\u673a\u5668\u4eba\u62fe\u653e\u4efb\u52a1\u4e2d\u7684\u5171\u4eab\u6293\u53d6\u9884\u6d4b\u3002", "motivation": "\u4f20\u7edf\u5206\u6790\u65b9\u6cd5\u5728\u89e3\u51b3\u5171\u4eab\u6293\u53d6\u65f6\u9700\u5355\u72ec\u8bc4\u4f30\u6bcf\u4e2a\u5019\u9009\u6293\u53d6\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\u3002", "method": "\u5f15\u5165\u80fd\u91cf\u6a21\u578b\uff08EBM\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u521d\u59cb\u548c\u76ee\u6807\u7269\u4f53\u4f4d\u59ff\u7684\u53ef\u884c\u6293\u53d6\u80fd\u91cf\u6765\u9884\u6d4b\u5171\u4eab\u6293\u53d6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u6293\u53d6\u9009\u62e9\u6027\u80fd\uff0c\u6570\u636e\u6548\u7387\u66f4\u9ad8\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u6293\u53d6\u548c\u7c7b\u4f3c\u5f62\u72b6\u7269\u4f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u641c\u7d22\u7a7a\u95f4\uff0c\u63d0\u9ad8\u4e86\u62fe\u653e\u4efb\u52a1\u89c4\u5212\u7684\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.15945", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15945", "abs": "https://arxiv.org/abs/2506.15945", "authors": ["Kowndinya Boyalakuntla", "Abdeslam Boularias", "Jingjin Yu"], "title": "KARL: Kalman-Filter Assisted Reinforcement Learner for Dynamic Object Tracking and Grasping", "comment": null, "summary": "We present Kalman-filter Assisted Reinforcement Learner (KARL) for dynamic\nobject tracking and grasping over eye-on-hand (EoH) systems, significantly\nexpanding such systems capabilities in challenging, realistic environments. In\ncomparison to the previous state-of-the-art, KARL (1) incorporates a novel\nsix-stage RL curriculum that doubles the system's motion range, thereby greatly\nenhancing the system's grasping performance, (2) integrates a robust Kalman\nfilter layer between the perception and reinforcement learning (RL) control\nmodules, enabling the system to maintain an uncertain but continuous 6D pose\nestimate even when the target object temporarily exits the camera's\nfield-of-view or undergoes rapid, unpredictable motion, and (3) introduces\nmechanisms to allow retries to gracefully recover from unavoidable policy\nexecution failures. Extensive evaluations conducted in both simulation and\nreal-world experiments qualitatively and quantitatively corroborate KARL's\nadvantage over earlier systems, achieving higher grasp success rates and faster\nrobot execution speed. Source code and supplementary materials for KARL will be\nmade available at: https://github.com/arc-l/karl.", "AI": {"tldr": "KARL\u662f\u4e00\u79cd\u7ed3\u5408\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u7269\u4f53\u8ddf\u8e2a\u4e0e\u6293\u53d6\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u6269\u5c55\u773c\u5728\u624b\uff08EoH\uff09\u7cfb\u7edf\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u7684\u52a8\u6001\u7269\u4f53\u8ddf\u8e2a\u4e0e\u6293\u53d6\u80fd\u529b\u3002", "method": "1. \u516d\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u8bfe\u7a0b\uff1b2. \u5361\u5c14\u66fc\u6ee4\u6ce2\u5c42\u96c6\u6210\uff1b3. \u5931\u8d25\u6062\u590d\u673a\u5236\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\uff0cKARL\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6293\u53d6\u6210\u529f\u7387\u548c\u66f4\u5feb\u7684\u6267\u884c\u901f\u5ea6\u3002", "conclusion": "KARL\u5728\u52a8\u6001\u7269\u4f53\u8ddf\u8e2a\u4e0e\u6293\u53d6\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002"}}
{"id": "2506.15953", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15953", "abs": "https://arxiv.org/abs/2506.15953", "authors": ["Liang Heng", "Haoran Geng", "Kaifeng Zhang", "Pieter Abbeel", "Jitendra Malik"], "title": "ViTacFormer: Learning Cross-Modal Representation for Visuo-Tactile Dexterous Manipulation", "comment": null, "summary": "Dexterous manipulation is a cornerstone capability for robotic systems aiming\nto interact with the physical world in a human-like manner. Although\nvision-based methods have advanced rapidly, tactile sensing remains crucial for\nfine-grained control, particularly in unstructured or visually occluded\nsettings. We present ViTacFormer, a representation-learning approach that\ncouples a cross-attention encoder to fuse high-resolution vision and touch with\nan autoregressive tactile prediction head that anticipates future contact\nsignals. Building on this architecture, we devise an easy-to-challenging\ncurriculum that steadily refines the visual-tactile latent space, boosting both\naccuracy and robustness. The learned cross-modal representation drives\nimitation learning for multi-fingered hands, enabling precise and adaptive\nmanipulation. Across a suite of challenging real-world benchmarks, our method\nachieves approximately 50% higher success rates than prior state-of-the-art\nsystems. To our knowledge, it is also the first to autonomously complete\nlong-horizon dexterous manipulation tasks that demand highly precise control\nwith an anthropomorphic hand, successfully executing up to 11 sequential stages\nand sustaining continuous operation for 2.5 minutes.", "AI": {"tldr": "ViTacFormer\u662f\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u7f16\u7801\u5668\u548c\u81ea\u56de\u5f52\u89e6\u89c9\u9884\u6d4b\u5934\uff0c\u63d0\u5347\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u7075\u5de7\u64cd\u4f5c\u662f\u673a\u5668\u4eba\u7cfb\u7edf\u5b9e\u73b0\u7c7b\u4eba\u4ea4\u4e92\u7684\u5173\u952e\u80fd\u529b\uff0c\u800c\u89e6\u89c9\u611f\u77e5\u5728\u975e\u7ed3\u6784\u5316\u6216\u89c6\u89c9\u906e\u6321\u73af\u5883\u4e2d\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u63d0\u51faViTacFormer\u67b6\u6784\uff0c\u7ed3\u5408\u8de8\u6ce8\u610f\u529b\u7f16\u7801\u5668\u878d\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u4fe1\u606f\uff0c\u5e76\u8bbe\u8ba1\u7531\u6613\u5230\u96be\u7684\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u4f18\u5316\u8868\u793a\u7a7a\u95f4\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6210\u529f\u7387\u6bd4\u73b0\u6709\u6280\u672f\u9ad850%\uff0c\u9996\u6b21\u5b9e\u73b0\u957f\u5e8f\u5217\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\uff0811\u4e2a\u9636\u6bb5\uff0c\u6301\u7eed2.5\u5206\u949f\uff09\u3002", "conclusion": "ViTacFormer\u901a\u8fc7\u8de8\u6a21\u6001\u5b66\u4e60\u548c\u8bfe\u7a0b\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2506.15983", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15983", "abs": "https://arxiv.org/abs/2506.15983", "authors": ["Jianzhu Huai", "Yuxin Shao", "Yujia Zhang", "Alper Yilmaz"], "title": "A Low-Cost Portable Lidar-based Mobile Mapping System on an Android Smartphone", "comment": "ISPRS GSW2025 Dubai UAE", "summary": "The rapid advancement of the metaverse, digital twins, and robotics\nunderscores the demand for low-cost, portable mapping systems for reality\ncapture. Current mobile solutions, such as the Leica BLK2Go and lidar-equipped\nsmartphones, either come at a high cost or are limited in range and accuracy.\nLeveraging the proliferation and technological evolution of mobile devices\nalongside recent advancements in lidar technology, we introduce a novel,\nlow-cost, portable mobile mapping system. Our system integrates a lidar unit,\nan Android smartphone, and an RTK-GNSS stick. Running on the Android platform,\nit features lidar-inertial odometry built with the NDK, and logs data from the\nlidar, wide-angle camera, IMU, and GNSS. With a total bill of materials (BOM)\ncost under 2,000 USD and a weight of about 1 kilogram, the system achieves a\ngood balance between affordability and portability. We detail the system\ndesign, multisensor calibration, synchronization, and evaluate its performance\nfor tracking and mapping. To further contribute to the community, the system's\ndesign and software are made open source at:\nhttps://github.com/OSUPCVLab/marslogger_android/releases/tag/v2.1", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u4fbf\u643a\u5f0f\u7684\u79fb\u52a8\u6d4b\u7ed8\u7cfb\u7edf\uff0c\u7ed3\u5408\u6fc0\u5149\u96f7\u8fbe\u3001Android\u667a\u80fd\u624b\u673a\u548cRTK-GNSS\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u4ef7\u6bd4\u7684\u5b9e\u65f6\u6d4b\u7ed8\u3002", "motivation": "\u5f53\u524d\u79fb\u52a8\u6d4b\u7ed8\u7cfb\u7edf\u6210\u672c\u9ad8\u6216\u6027\u80fd\u6709\u9650\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5143\u5b87\u5b99\u3001\u6570\u5b57\u5b6a\u751f\u548c\u673a\u5668\u4eba\u7b49\u9886\u57df\u5bf9\u4f4e\u6210\u672c\u4fbf\u643a\u8bbe\u5907\u7684\u9700\u6c42\u3002", "method": "\u7cfb\u7edf\u96c6\u6210\u4e86\u6fc0\u5149\u96f7\u8fbe\u3001\u667a\u80fd\u624b\u673a\u548cRTK-GNSS\u6a21\u5757\uff0c\u5229\u7528Android\u5e73\u53f0\u5f00\u53d1\u6fc0\u5149\u96f7\u8fbe\u60ef\u6027\u91cc\u7a0b\u8ba1\uff0c\u5e76\u8bb0\u5f55\u591a\u4f20\u611f\u5668\u6570\u636e\u3002", "result": "\u7cfb\u7edf\u603b\u6210\u672c\u4f4e\u4e8e2000\u7f8e\u5143\uff0c\u91cd\u91cf\u7ea61\u5343\u514b\uff0c\u5728\u8ddf\u8e2a\u548c\u6d4b\u7ed8\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u4f4e\u6210\u672c\u4fbf\u643a\u6d4b\u7ed8\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u4e86\u8bbe\u8ba1\u548c\u8f6f\u4ef6\u3002"}}
{"id": "2506.16012", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16012", "abs": "https://arxiv.org/abs/2506.16012", "authors": ["Boyu Li", "Siyuan He", "Hang Xu", "Haoqi Yuan", "Yu Zang", "Liwei Hu", "Junpeng Yue", "Zhenxiong Jiang", "Pengbo Hu", "B\u00f6rje F. Karlsson", "Yehui Tang", "Zongqing Lu"], "title": "DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware Planning", "comment": null, "summary": "Developing embodied agents capable of performing complex interactive tasks in\nreal-world scenarios remains a fundamental challenge in embodied AI. Although\nrecent advances in simulation platforms have greatly enhanced task diversity to\ntrain embodied Vision Language Models (VLMs), most platforms rely on simplified\nrobot morphologies and bypass the stochastic nature of low-level execution,\nwhich limits their transferability to real-world robots. To address these\nissues, we present a physics-based simulation platform DualTHOR for complex\ndual-arm humanoid robots, built upon an extended version of AI2-THOR. Our\nsimulator includes real-world robot assets, a task suite for dual-arm\ncollaboration, and inverse kinematics solvers for humanoid robots. We also\nintroduce a contingency mechanism that incorporates potential failures through\nphysics-based low-level execution, bridging the gap to real-world scenarios.\nOur simulator enables a more comprehensive evaluation of the robustness and\ngeneralization of VLMs in household environments. Extensive evaluations reveal\nthat current VLMs struggle with dual-arm coordination and exhibit limited\nrobustness in realistic environments with contingencies, highlighting the\nimportance of using our simulator to develop more capable VLMs for embodied\ntasks. The code is available at https://github.com/ds199895/DualTHOR.git.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u4eff\u771f\u5e73\u53f0DualTHOR\uff0c\u7528\u4e8e\u590d\u6742\u53cc\u81c2\u4eba\u5f62\u673a\u5668\u4eba\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u5e73\u53f0\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u4eff\u771f\u5e73\u53f0\u4f9d\u8d56\u7b80\u5316\u7684\u673a\u5668\u4eba\u5f62\u6001\u5e76\u5ffd\u7565\u4f4e\u7ea7\u6267\u884c\u7684\u968f\u673a\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u53ef\u8fc1\u79fb\u6027\u3002", "method": "\u57fa\u4e8e\u6269\u5c55\u7248AI2-THOR\u6784\u5efaDualTHOR\uff0c\u5305\u542b\u771f\u5b9e\u673a\u5668\u4eba\u8d44\u4ea7\u3001\u53cc\u81c2\u534f\u4f5c\u4efb\u52a1\u5957\u4ef6\u548c\u4eba\u5f62\u673a\u5668\u4eba\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u5668\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u7269\u7406\u7684\u5e94\u6025\u673a\u5236\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u53cc\u81c2\u534f\u8c03\u548c\u5e94\u6025\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u51f8\u663e\u4e86DualTHOR\u7684\u91cd\u8981\u6027\u3002", "conclusion": "DualTHOR\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u4eff\u771f\u73af\u5883\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2506.16050", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16050", "abs": "https://arxiv.org/abs/2506.16050", "authors": ["Jiawen Yu", "Jieji Ren", "Yang Chang", "Qiaojun Yu", "Xuan Tong", "Boyang Wang", "Yan Song", "You Li", "Xinji Mai", "Wenqiang Zhang"], "title": "Noise Fusion-based Distillation Learning for Anomaly Detection in Complex Industrial Environments", "comment": "IROS 2025 Oral", "summary": "Anomaly detection and localization in automated industrial manufacturing can\nsignificantly enhance production efficiency and product quality. Existing\nmethods are capable of detecting surface defects in pre-defined or controlled\nimaging environments. However, accurately detecting workpiece defects in\ncomplex and unstructured industrial environments with varying views, poses and\nillumination remains challenging. We propose a novel anomaly detection and\nlocalization method specifically designed to handle inputs with perturbative\npatterns. Our approach introduces a new framework based on a collaborative\ndistillation heterogeneous teacher network (HetNet), an adaptive local-global\nfeature fusion module, and a local multivariate Gaussian noise generation\nmodule. HetNet can learn to model the complex feature distribution of normal\npatterns using limited information about local disruptive changes. We conducted\nextensive experiments on mainstream benchmarks. HetNet demonstrates superior\nperformance with approximately 10% improvement across all evaluation metrics on\nMSC-AD under industrial conditions, while achieving state-of-the-art results on\nother datasets, validating its resilience to environmental fluctuations and its\ncapability to enhance the reliability of industrial anomaly detection systems\nacross diverse scenarios. Tests in real-world environments further confirm that\nHetNet can be effectively integrated into production lines to achieve robust\nand real-time anomaly detection. Codes, images and videos are published on the\nproject website at: https://zihuatanejoyu.github.io/HetNet/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f02\u6784\u6559\u5e08\u7f51\u7edc\uff08HetNet\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u590d\u6742\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u3001\u975e\u7ed3\u6784\u5316\u7684\u5de5\u4e1a\u73af\u5883\u4e2d\u68c0\u6d4b\u5de5\u4ef6\u7f3a\u9677\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5f02\u6784\u6559\u5e08\u7f51\u7edc\uff08HetNet\uff09\u3001\u81ea\u9002\u5e94\u5c40\u90e8-\u5168\u5c40\u7279\u5f81\u878d\u5408\u6a21\u5757\u548c\u5c40\u90e8\u591a\u5143\u9ad8\u65af\u566a\u58f0\u751f\u6210\u6a21\u5757\u3002", "result": "\u5728\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cMSC-AD\u6307\u6807\u63d0\u5347\u7ea610%\uff0c\u5e76\u5728\u5176\u4ed6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u3002", "conclusion": "HetNet\u80fd\u6709\u6548\u5e94\u5bf9\u73af\u5883\u6ce2\u52a8\uff0c\u63d0\u5347\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\u7684\u53ef\u9760\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b\u3002"}}
{"id": "2506.16079", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16079", "abs": "https://arxiv.org/abs/2506.16079", "authors": ["Prakrut Kotecha", "Aditya Shirwatkar", "Shishir Kolathaya"], "title": "Investigating Lagrangian Neural Networks for Infinite Horizon Planning in Quadrupedal Locomotion", "comment": "6 pages, 5 figures, Accepted at Advances in Robotics (AIR) Conference\n  2025", "summary": "Lagrangian Neural Networks (LNNs) present a principled and interpretable\nframework for learning the system dynamics by utilizing inductive biases. While\ntraditional dynamics models struggle with compounding errors over long\nhorizons, LNNs intrinsically preserve the physical laws governing any system,\nenabling accurate and stable predictions essential for sustainable locomotion.\nThis work evaluates LNNs for infinite horizon planning in quadrupedal robots\nthrough four dynamics models: (1) full-order forward dynamics (FD) training and\ninference, (2) diagonalized representation of Mass Matrix in full order FD, (3)\nfull-order inverse dynamics (ID) training with FD inference, (4) reduced-order\nmodeling via torso centre-of-mass (CoM) dynamics. Experiments demonstrate that\nLNNs bring improvements in sample efficiency (10x) and superior prediction\naccuracy (up to 2-10x) compared to baseline methods. Notably, the\ndiagonalization approach of LNNs reduces computational complexity while\nretaining some interpretability, enabling real-time receding horizon control.\nThese findings highlight the advantages of LNNs in capturing the underlying\nstructure of system dynamics in quadrupeds, leading to improved performance and\nefficiency in locomotion planning and control. Additionally, our approach\nachieves a higher control frequency than previous LNN methods, demonstrating\nits potential for real-world deployment on quadrupeds.", "AI": {"tldr": "LNNs\u5229\u7528\u5f52\u7eb3\u504f\u7f6e\u5b66\u4e60\u7cfb\u7edf\u52a8\u529b\u5b66\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u9ad8\u6548\uff0810\u500d\u6837\u672c\u6548\u7387\uff09\u548c\u51c6\u786e\uff082-10\u500d\uff09\uff0c\u9002\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u65e0\u9650\u65f6\u57df\u89c4\u5212\u3002", "motivation": "\u4f20\u7edf\u52a8\u529b\u5b66\u6a21\u578b\u5728\u957f\u65f6\u57df\u9884\u6d4b\u4e2d\u8bef\u5dee\u7d2f\u79ef\uff0c\u800cLNNs\u80fd\u4fdd\u6301\u7269\u7406\u89c4\u5f8b\uff0c\u5b9e\u73b0\u7a33\u5b9a\u9884\u6d4b\uff0c\u9002\u7528\u4e8e\u53ef\u6301\u7eed\u8fd0\u52a8\u3002", "method": "\u8bc4\u4f30\u56db\u79cdLNNs\u52a8\u529b\u5b66\u6a21\u578b\uff1a\u5168\u9636\u6b63\u5411\u52a8\u529b\u5b66\u3001\u5bf9\u89d2\u5316\u8d28\u91cf\u77e9\u9635\u3001\u5168\u9636\u9006\u5411\u52a8\u529b\u5b66\u3001\u964d\u9636\u6a21\u578b\uff08CoM\u52a8\u529b\u5b66\uff09\u3002", "result": "LNNs\u5728\u6837\u672c\u6548\u7387\u548c\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5bf9\u89d2\u5316\u65b9\u6cd5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "LNNs\u80fd\u6709\u6548\u6355\u6349\u56db\u8db3\u673a\u5668\u4eba\u52a8\u529b\u5b66\u7ed3\u6784\uff0c\u63d0\u5347\u8fd0\u52a8\u89c4\u5212\u4e0e\u63a7\u5236\u6027\u80fd\uff0c\u5e76\u5b9e\u73b0\u66f4\u9ad8\u63a7\u5236\u9891\u7387\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2506.16143", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16143", "abs": "https://arxiv.org/abs/2506.16143", "authors": ["Stephane Ngnepiepaye Wembe", "Vincent Rousseau", "Johann Laconte", "Roland Lenain"], "title": "From Theory to Practice: Identifying the Optimal Approach for Offset Point Tracking in the Context of Agricultural Robotics", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "Modern agriculture faces escalating challenges: increasing demand for food,\nlabor shortages, and the urgent need to reduce environmental impact.\nAgricultural robotics has emerged as a promising response to these pressures,\nenabling the automation of precise and suitable field operations. In\nparticular, robots equipped with implements for tasks such as weeding or sowing\nmust interact delicately and accurately with the crops and soil. Unlike robots\nin other domains, these agricultural platforms typically use rigidly mounted\nimplements, where the implement's position is more critical than the robot's\ncenter in determining task success. Yet, most control strategies in the\nliterature focus on the vehicle body, often neglecting the acctual working\npoint of the system. This is particularly important when considering new\nagriculture practices where crops row are not necessary straights. This paper\npresents a predictive control strategy targeting the implement's reference\npoint. The method improves tracking performance by anticipating the motion of\nthe implement, which, due to its offset from the vehicle's center of rotation,\nis prone to overshooting during turns if not properly accounted for.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u519c\u4e1a\u673a\u5668\u4eba\u5de5\u5177\u7684\u9884\u6d4b\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u9884\u6d4b\u5de5\u5177\u7684\u8fd0\u52a8\u6765\u63d0\u5347\u8ddf\u8e2a\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u63a7\u5236\u7b56\u7565\u5ffd\u7565\u5de5\u5177\u5b9e\u9645\u5de5\u4f5c\u70b9\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u519c\u4e1a\u9762\u4e34\u52b3\u52a8\u529b\u77ed\u7f3a\u548c\u73af\u5883\u538b\u529b\uff0c\u519c\u4e1a\u673a\u5668\u4eba\u6210\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u63a7\u5236\u7b56\u7565\u591a\u5173\u6ce8\u673a\u5668\u4eba\u672c\u4f53\uff0c\u5ffd\u7565\u4e86\u5de5\u5177\u7684\u5b9e\u9645\u5de5\u4f5c\u70b9\uff0c\u5c24\u5176\u662f\u5728\u975e\u76f4\u7ebf\u4f5c\u7269\u884c\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u6d4b\u63a7\u5236\u7b56\u7565\uff0c\u4e13\u6ce8\u4e8e\u5de5\u5177\u7684\u53c2\u8003\u70b9\uff0c\u901a\u8fc7\u9884\u6d4b\u5de5\u5177\u7684\u8fd0\u52a8\u6765\u4f18\u5316\u8ddf\u8e2a\u6027\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u5de5\u5177\u5728\u8f6c\u5f2f\u65f6\u7684\u8fc7\u51b2\u73b0\u8c61\uff0c\u63d0\u5347\u4e86\u64cd\u4f5c\u7684\u7cbe\u786e\u6027\u3002", "conclusion": "\u8be5\u63a7\u5236\u7b56\u7565\u4e3a\u519c\u4e1a\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u5de5\u5177\u64cd\u4f5c\u80fd\u529b\uff0c\u9002\u5e94\u73b0\u4ee3\u519c\u4e1a\u7684\u975e\u76f4\u7ebf\u4f5c\u7269\u884c\u9700\u6c42\u3002"}}
{"id": "2506.16173", "categories": ["cs.RO", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.16173", "abs": "https://arxiv.org/abs/2506.16173", "authors": ["Jiang Wang", "Runwu Shi", "Benjamin Yen", "He Kong", "Kazuhiro Nakadai"], "title": "Single-Microphone-Based Sound Source Localization for Mobile Robots in Reverberant Environments", "comment": "This paper was accepted and going to appear in the 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS)", "summary": "Accurately estimating sound source positions is crucial for robot audition.\nHowever, existing sound source localization methods typically rely on a\nmicrophone array with at least two spatially preconfigured microphones. This\nrequirement hinders the applicability of microphone-based robot audition\nsystems and technologies. To alleviate these challenges, we propose an online\nsound source localization method that uses a single microphone mounted on a\nmobile robot in reverberant environments. Specifically, we develop a\nlightweight neural network model with only 43k parameters to perform real-time\ndistance estimation by extracting temporal information from reverberant\nsignals. The estimated distances are then processed using an extended Kalman\nfilter to achieve online sound source localization. To the best of our\nknowledge, this is the first work to achieve online sound source localization\nusing a single microphone on a moving robot, a gap that we aim to fill in this\nwork. Extensive experiments demonstrate the effectiveness and merits of our\napproach. To benefit the broader research community, we have open-sourced our\ncode at https://github.com/JiangWAV/single-mic-SSL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u9ea6\u514b\u98ce\u7684\u5728\u7ebf\u58f0\u6e90\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u5728\u6df7\u54cd\u73af\u5883\u4e2d\u3002", "motivation": "\u73b0\u6709\u58f0\u6e90\u5b9a\u4f4d\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u81f3\u5c11\u4e24\u4e2a\u9ea6\u514b\u98ce\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0843k\u53c2\u6570\uff09\uff0c\u901a\u8fc7\u63d0\u53d6\u6df7\u54cd\u4fe1\u53f7\u7684\u65f6\u57df\u4fe1\u606f\u8fdb\u884c\u5b9e\u65f6\u8ddd\u79bb\u4f30\u8ba1\uff0c\u5e76\u7ed3\u5408\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5b9e\u73b0\u5728\u7ebf\u5b9a\u4f4d\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u52bf\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5728\u79fb\u52a8\u673a\u5668\u4eba\u4e0a\u4f7f\u7528\u5355\u9ea6\u514b\u98ce\u5b9e\u73b0\u5728\u7ebf\u58f0\u6e90\u5b9a\u4f4d\u7684\u5de5\u4f5c\uff0c\u586b\u8865\u4e86\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u5f00\u6e90\u4e86\u4ee3\u7801\u3002"}}
{"id": "2506.16201", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16201", "abs": "https://arxiv.org/abs/2506.16201", "authors": ["Sen Wang", "Le Wang", "Sanping Zhou", "Jingyi Tian", "Jiayi Li", "Haowen Sun", "Wei Tang"], "title": "FlowRAM: Grounding Flow Matching Policy with Region-Aware Mamba Framework for Robotic Manipulation", "comment": null, "summary": "Robotic manipulation in high-precision tasks is essential for numerous\nindustrial and real-world applications where accuracy and speed are required.\nYet current diffusion-based policy learning methods generally suffer from low\ncomputational efficiency due to the iterative denoising process during\ninference. Moreover, these methods do not fully explore the potential of\ngenerative models for enhancing information exploration in 3D environments. In\nresponse, we propose FlowRAM, a novel framework that leverages generative\nmodels to achieve region-aware perception, enabling efficient multimodal\ninformation processing. Specifically, we devise a Dynamic Radius Schedule,\nwhich allows adaptive perception, facilitating transitions from global scene\ncomprehension to fine-grained geometric details. Furthermore, we integrate\nstate space models to integrate multimodal information, while preserving linear\ncomputational complexity. In addition, we employ conditional flow matching to\nlearn action poses by regressing deterministic vector fields, simplifying the\nlearning process while maintaining performance. We verify the effectiveness of\nthe FlowRAM in the RLBench, an established manipulation benchmark, and achieve\nstate-of-the-art performance. The results demonstrate that FlowRAM achieves a\nremarkable improvement, particularly in high-precision tasks, where it\noutperforms previous methods by 12.0% in average success rate. Additionally,\nFlowRAM is able to generate physically plausible actions for a variety of\nreal-world tasks in less than 4 time steps, significantly increasing inference\nspeed.", "AI": {"tldr": "FlowRAM\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u9ad8\u6548\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u534a\u5f84\u8c03\u5ea6\u548c\u6761\u4ef6\u6d41\u5339\u914d\u4f18\u5316\u611f\u77e5\u4e0e\u52a8\u4f5c\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cbe\u5ea6\u548c\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u6269\u6563\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u4f4e\u4e14\u672a\u5145\u5206\u5229\u7528\u751f\u6210\u6a21\u578b\u57283D\u73af\u5883\u4e2d\u7684\u6f5c\u529b\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u52a8\u6001\u534a\u5f84\u8c03\u5ea6\u5b9e\u73b0\u81ea\u9002\u5e94\u611f\u77e5\uff0c\u5229\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u6761\u4ef6\u6d41\u5339\u914d\u5b66\u4e60\u52a8\u4f5c\u4f4d\u59ff\u3002", "result": "\u5728RLBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u6210\u529f\u7387\u63d0\u534712%\uff0c\u4e14\u63a8\u7406\u901f\u5ea6\u663e\u8457\u52a0\u5feb\u3002", "conclusion": "FlowRAM\u5728\u9ad8\u6548\u6027\u548c\u7cbe\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u3002"}}
{"id": "2506.16211", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16211", "abs": "https://arxiv.org/abs/2506.16211", "authors": ["Puhao Li", "Yingying Wu", "Ziheng Xi", "Wanlin Li", "Yuzhe Huang", "Zhiyuan Zhang", "Yinghan Chen", "Jianan Wang", "Song-Chun Zhu", "Tengyu Liu", "Siyuan Huang"], "title": "ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models", "comment": "Website: https://controlvla.github.io", "summary": "Learning real-world robotic manipulation is challenging, particularly when\nlimited demonstrations are available. Existing methods for few-shot\nmanipulation often rely on simulation-augmented data or pre-built modules like\ngrasping and pose estimation, which struggle with sim-to-real gaps and lack\nextensibility. While large-scale imitation pre-training shows promise, adapting\nthese general-purpose policies to specific tasks in data-scarce settings\nremains unexplored. To achieve this, we propose ControlVLA, a novel framework\nthat bridges pre-trained VLA models with object-centric representations via a\nControlNet-style architecture for efficient fine-tuning. Specifically, to\nintroduce object-centric conditions without overwriting prior knowledge,\nControlVLA zero-initializes a set of projection layers, allowing them to\ngradually adapt the pre-trained manipulation policies. In real-world\nexperiments across 6 diverse tasks, including pouring cubes and folding\nclothes, our method achieves a 76.7% success rate while requiring only 10-20\ndemonstrations -- a significant improvement over traditional approaches that\nrequire more than 100 demonstrations to achieve comparable success. Additional\nexperiments highlight ControlVLA's extensibility to long-horizon tasks and\nrobustness to unseen objects and backgrounds.", "AI": {"tldr": "ControlVLA\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u7684VLA\u6a21\u578b\u548c\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u5728\u5c11\u91cf\u6f14\u793a\u4e0b\u9ad8\u6548\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5c11\u91cf\u6f14\u793a\u4e0b\u96be\u4ee5\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u4e14\u4f9d\u8d56\u4eff\u771f\u6570\u636e\u6216\u9884\u5efa\u6a21\u5757\uff0c\u5b58\u5728\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u548c\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faControlVLA\uff0c\u901a\u8fc7ControlNet\u98ce\u683c\u67b6\u6784\u5c06\u9884\u8bad\u7ec3VLA\u6a21\u578b\u4e0e\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\u7ed3\u5408\uff0c\u5e76\u96f6\u521d\u59cb\u5316\u6295\u5f71\u5c42\u4ee5\u9010\u6b65\u9002\u5e94\u4efb\u52a1\u3002", "result": "\u57286\u4e2a\u771f\u5b9e\u4efb\u52a1\u4e2d\uff0c\u4ec5\u970010-20\u6b21\u6f14\u793a\u5373\u8fbe\u523076.7%\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u6240\u9700\u7684100\u6b21\u6f14\u793a\u3002", "conclusion": "ControlVLA\u5728\u5c11\u91cf\u6f14\u793a\u4e0b\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u957f\u65f6\u4efb\u52a1\u548c\u672a\u77e5\u5bf9\u8c61\u53ca\u80cc\u666f\u3002"}}
{"id": "2506.16219", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16219", "abs": "https://arxiv.org/abs/2506.16219", "authors": ["Amine Tourki", "Paul Prevel", "Nils Einecke", "Tim Puphal", "Alexandre Alahi"], "title": "Probabilistic Collision Risk Estimation for Pedestrian Navigation", "comment": null, "summary": "Intelligent devices for supporting persons with vision impairment are\nbecoming more widespread, but they are lacking behind the advancements in\nintelligent driver assistant system. To make a first step forward, this work\ndiscusses the integration of the risk model technology, previously used in\nautonomous driving and advanced driver assistance systems, into an assistance\ndevice for persons with vision impairment. The risk model computes a\nprobabilistic collision risk given object trajectories which has previously\nbeen shown to give better indications of an object's collision potential\ncompared to distance or time-to-contact measures in vehicle scenarios. In this\nwork, we show that the risk model is also superior in warning persons with\nvision impairment about dangerous objects. Our experiments demonstrate that the\nwarning accuracy of the risk model is 67% while both distance and\ntime-to-contact measures reach only 51% accuracy for real-world data.", "AI": {"tldr": "\u5c06\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u98ce\u9669\u6a21\u578b\u6280\u672f\u5e94\u7528\u4e8e\u89c6\u89c9\u969c\u788d\u8f85\u52a9\u8bbe\u5907\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u8b66\u544a\u51c6\u786e\u6027\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u667a\u80fd\u89c6\u89c9\u969c\u788d\u8f85\u52a9\u8bbe\u5907\u53d1\u5c55\u6ede\u540e\u4e8e\u667a\u80fd\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\uff0c\u9700\u5f15\u5165\u66f4\u5148\u8fdb\u7684\u6280\u672f\u63d0\u5347\u5176\u6027\u80fd\u3002", "method": "\u5c06\u81ea\u52a8\u9a7e\u9a76\u4e2d\u4f7f\u7528\u7684\u98ce\u9669\u6a21\u578b\u6280\u672f\u6574\u5408\u5230\u89c6\u89c9\u969c\u788d\u8f85\u52a9\u8bbe\u5907\u4e2d\uff0c\u8ba1\u7b97\u7269\u4f53\u8f68\u8ff9\u7684\u78b0\u649e\u98ce\u9669\u6982\u7387\u3002", "result": "\u98ce\u9669\u6a21\u578b\u7684\u8b66\u544a\u51c6\u786e\u6027\u4e3a67%\uff0c\u800c\u8ddd\u79bb\u548c\u65f6\u95f4\u63a5\u89e6\u6d4b\u91cf\u7684\u51c6\u786e\u6027\u4ec5\u4e3a51%\u3002", "conclusion": "\u98ce\u9669\u6a21\u578b\u5728\u89c6\u89c9\u969c\u788d\u8f85\u52a9\u8bbe\u5907\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u4e3a\u672a\u6765\u6280\u672f\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2506.16263", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16263", "abs": "https://arxiv.org/abs/2506.16263", "authors": ["Xiting He", "Mingwu Su", "Xinqi Jiang", "Long Bai", "Jiewen Lai", "Hongliang Ren"], "title": "CapsDT: Diffusion-Transformer for Capsule Robot Manipulation", "comment": "IROS 2025", "summary": "Vision-Language-Action (VLA) models have emerged as a prominent research\narea, showcasing significant potential across a variety of applications.\nHowever, their performance in endoscopy robotics, particularly endoscopy\ncapsule robots that perform actions within the digestive system, remains\nunexplored. The integration of VLA models into endoscopy robots allows more\nintuitive and efficient interactions between human operators and medical\ndevices, improving both diagnostic accuracy and treatment outcomes. In this\nwork, we design CapsDT, a Diffusion Transformer model for capsule robot\nmanipulation in the stomach. By processing interleaved visual inputs, and\ntextual instructions, CapsDT can infer corresponding robotic control signals to\nfacilitate endoscopy tasks. In addition, we developed a capsule endoscopy robot\nsystem, a capsule robot controlled by a robotic arm-held magnet, addressing\ndifferent levels of four endoscopy tasks and creating corresponding capsule\nrobot datasets within the stomach simulator. Comprehensive evaluations on\nvarious robotic tasks indicate that CapsDT can serve as a robust\nvision-language generalist, achieving state-of-the-art performance in various\nlevels of endoscopy tasks while achieving a 26.25% success rate in real-world\nsimulation manipulation.", "AI": {"tldr": "CapsDT\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u80f6\u56ca\u673a\u5668\u4eba\u80c3\u5185\u64cd\u4f5c\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\u751f\u6210\u63a7\u5236\u4fe1\u53f7\uff0c\u63d0\u5347\u5185\u7aa5\u955c\u4efb\u52a1\u6548\u7387\u3002", "motivation": "\u63a2\u7d22VLA\u6a21\u578b\u5728\u5185\u7aa5\u955c\u80f6\u56ca\u673a\u5668\u4eba\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u6539\u5584\u4eba\u673a\u4ea4\u4e92\uff0c\u63d0\u9ad8\u8bca\u65ad\u548c\u6cbb\u7597\u6548\u679c\u3002", "method": "\u8bbe\u8ba1CapsDT\u6a21\u578b\uff0c\u5904\u7406\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\u751f\u6210\u63a7\u5236\u4fe1\u53f7\uff0c\u5e76\u5f00\u53d1\u80f6\u56ca\u673a\u5668\u4eba\u7cfb\u7edf\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "CapsDT\u5728\u591a\u79cd\u5185\u7aa5\u955c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u771f\u5b9e\u6a21\u62df\u64cd\u4f5c\u6210\u529f\u7387\u8fbe26.25%\u3002", "conclusion": "CapsDT\u4f5c\u4e3a\u89c6\u89c9\u8bed\u8a00\u901a\u7528\u6a21\u578b\uff0c\u5728\u5185\u7aa5\u955c\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2506.16301", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16301", "abs": "https://arxiv.org/abs/2506.16301", "authors": ["Nadine Imholz", "Maurice Brunner", "Nicolas Baumann", "Edoardo Ghignone", "Michele Magno"], "title": "M-Predictive Spliner: Enabling Spatiotemporal Multi-Opponent Overtaking for Autonomous Racing", "comment": null, "summary": "Unrestricted multi-agent racing presents a significant research challenge,\nrequiring decision-making at the limits of a robot's operational capabilities.\nWhile previous approaches have either ignored spatiotemporal information in the\ndecision-making process or been restricted to single-opponent scenarios, this\nwork enables arbitrary multi-opponent head-to-head racing while considering the\nopponents' future intent. The proposed method employs a KF-based multi-opponent\ntracker to effectively perform opponent ReID by associating them across\nobservations. Simultaneously, spatial and velocity GPR is performed on all\nobserved opponent trajectories, providing predictive information to compute the\novertaking maneuvers. This approach has been experimentally validated on a\nphysical 1:10 scale autonomous racing car, achieving an overtaking success rate\nof up to 91.65% and demonstrating an average 10.13%-point improvement in safety\nat the same speed as the previous SotA. These results highlight its potential\nfor high-performance autonomous racing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKF\u7684\u591a\u5bf9\u624b\u8ddf\u8e2a\u5668\u548cGPR\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u5bf9\u624b\u8d5b\u8f66\u573a\u666f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8d85\u8f66\u6210\u529f\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u591a\u5bf9\u624b\u8d5b\u8f66\u573a\u666f\u4e2d\uff0c\u4f20\u7edf\u65b9\u6cd5\u5ffd\u7565\u4e86\u65f6\u7a7a\u4fe1\u606f\u6216\u4ec5\u9002\u7528\u4e8e\u5355\u4e00\u5bf9\u624b\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528KF\u591a\u5bf9\u624b\u8ddf\u8e2a\u5668\u8fdb\u884c\u5bf9\u624b\u91cd\u8bc6\u522b\uff0c\u5e76\u7ed3\u5408GPR\u9884\u6d4b\u5bf9\u624b\u8f68\u8ff9\uff0c\u8ba1\u7b97\u8d85\u8f66\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\u8d85\u8f66\u6210\u529f\u7387\u8fbe91.65%\uff0c\u5b89\u5168\u6027\u63d0\u534710.13%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u9ad8\u6027\u80fd\u81ea\u4e3b\u8d5b\u8f66\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2506.16336", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.16336", "abs": "https://arxiv.org/abs/2506.16336", "authors": ["Yiou Huang"], "title": "Goal-conditioned Hierarchical Reinforcement Learning for Sample-efficient and Safe Autonomous Driving at Intersections", "comment": null, "summary": "Reinforcement learning (RL) exhibits remarkable potential in addressing\nautonomous driving tasks. However, it is difficult to train a sample-efficient\nand safe policy in complex scenarios. In this article, we propose a novel\nhierarchical reinforcement learning (HRL) framework with a goal-conditioned\ncollision prediction (GCCP) module. In the hierarchical structure, the GCCP\nmodule predicts collision risks according to different potential subgoals of\nthe ego vehicle. A high-level decision-maker choose the best safe subgoal. A\nlow-level motion-planner interacts with the environment according to the\nsubgoal. Compared to traditional RL methods, our algorithm is more\nsample-efficient, since its hierarchical structure allows reusing the policies\nof subgoals across similar tasks for various navigation scenarios. In\nadditional, the GCCP module's ability to predict both the ego vehicle's and\nsurrounding vehicles' future actions according to different subgoals, ensures\nthe safety of the ego vehicle throughout the decision-making process.\nExperimental results demonstrate that the proposed method converges to an\noptimal policy faster and achieves higher safety than traditional RL methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76ee\u6807\u6761\u4ef6\u78b0\u649e\u9884\u6d4b\uff08GCCP\uff09\u7684\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u6837\u672c\u6548\u7387\u548c\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u573a\u666f\u4e2d\u96be\u4ee5\u9ad8\u6548\u4e14\u5b89\u5168\u5730\u8bad\u7ec3\u7b56\u7565\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\uff08HRL\uff09\u6846\u67b6\uff0c\u5305\u542bGCCP\u6a21\u5757\u9884\u6d4b\u78b0\u649e\u98ce\u9669\uff0c\u9ad8\u5c42\u51b3\u7b56\u9009\u62e9\u5b89\u5168\u5b50\u76ee\u6807\uff0c\u4f4e\u5c42\u8fd0\u52a8\u89c4\u5212\u6267\u884c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edfRL\u65b9\u6cd5\u6536\u655b\u66f4\u5feb\u4e14\u5b89\u5168\u6027\u66f4\u9ad8\u3002", "conclusion": "\u63d0\u51fa\u7684HRL\u6846\u67b6\u5728\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2506.16356", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16356", "abs": "https://arxiv.org/abs/2506.16356", "authors": ["Aman Singh", "Deepak Kapa", "Prasham Chedda", "Shishir N. Y. Kolathaya"], "title": "Comparison between External and Internal Single Stage Planetary gearbox actuators for legged robots", "comment": "6 pages, 5 figures, Accepted at Advances in Robotics 2025", "summary": "Legged robots, such as quadrupeds and humanoids, require high-performance\nactuators for efficient locomotion. Quasi-Direct-Drive (QDD) actuators with\nsingle-stage planetary gearboxes offer low inertia, high efficiency, and\ntransparency. Among planetary gearbox architectures, Internal (ISSPG) and\nExternal Single-Stage Planetary Gearbox (ESSPG) are the two predominant\ndesigns. While ISSPG is often preferred for its compactness and high torque\ndensity at certain gear ratios, no objective comparison between the two\narchitectures exists. Additionally, existing designs rely on heuristics rather\nthan systematic optimization. This paper presents a design framework for\noptimally selecting actuator parameters based on given performance requirements\nand motor specifications. Using this framework, we generate and analyze various\noptimized gearbox designs for both architectures. Our results demonstrate that\nfor the T-motor U12, ISSPG is the superior choice within the lower gear ratio\nrange of 5:1 to 7:1, offering a lighter design. However, for gear ratios\nexceeding 7:1, ISSPG becomes infeasible, making ESSPG the better option in the\n7:1 to 11:1 range. To validate our approach, we designed and optimized two\nactuators for manufacturing: an ISSPG with a 6.0:1 gear ratio and an ESSPG with\na 7.2:1 gear ratio. Their respective masses closely align with our optimization\nmodel predictions, confirming the effectiveness of our methodology.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bbe\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u9009\u62e9\u6267\u884c\u5668\u53c2\u6570\uff0c\u6bd4\u8f83\u4e86ISSPG\u548cESSPG\u4e24\u79cd\u884c\u661f\u9f7f\u8f6e\u7bb1\u67b6\u6784\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7f3a\u4e4f\u5bf9ISSPG\u548cESSPG\u4e24\u79cd\u884c\u661f\u9f7f\u8f6e\u7bb1\u67b6\u6784\u7684\u5ba2\u89c2\u6bd4\u8f83\uff0c\u4e14\u73b0\u6709\u8bbe\u8ba1\u4f9d\u8d56\u542f\u53d1\u5f0f\u65b9\u6cd5\u800c\u975e\u7cfb\u7edf\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6027\u80fd\u9700\u6c42\u548c\u7535\u673a\u89c4\u683c\u7684\u8bbe\u8ba1\u6846\u67b6\uff0c\u751f\u6210\u5e76\u5206\u6790\u4e86\u4e24\u79cd\u67b6\u6784\u7684\u4f18\u5316\u9f7f\u8f6e\u7bb1\u8bbe\u8ba1\u3002", "result": "\u5bf9\u4e8eT-motor U12\uff0cISSPG\u57285:1\u81f37:1\u7684\u9f7f\u8f6e\u6bd4\u8303\u56f4\u5185\u66f4\u4f18\uff0c\u800cESSPG\u57287:1\u81f311:1\u8303\u56f4\u5185\u66f4\u4f18\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4f18\u5316\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8bbe\u8ba1\u6846\u67b6\u6709\u6548\uff0cISSPG\u548cESSPG\u5404\u6709\u4f18\u52bf\uff0c\u5177\u4f53\u9009\u62e9\u53d6\u51b3\u4e8e\u9f7f\u8f6e\u6bd4\u9700\u6c42\u3002"}}
{"id": "2506.16386", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16386", "abs": "https://arxiv.org/abs/2506.16386", "authors": ["Leesai Park", "Keunwoo Jang", "Sanghyun Kim"], "title": "CSC-MPPI: A Novel Constrained MPPI Framework with DBSCAN for Reliable Obstacle Avoidance", "comment": null, "summary": "This paper proposes Constrained Sampling Cluster Model Predictive Path\nIntegral (CSC-MPPI), a novel constrained formulation of MPPI designed to\nenhance trajectory optimization while enforcing strict constraints on system\nstates and control inputs. Traditional MPPI, which relies on a probabilistic\nsampling process, often struggles with constraint satisfaction and generates\nsuboptimal trajectories due to the weighted averaging of sampled trajectories.\nTo address these limitations, the proposed framework integrates a primal-dual\ngradient-based approach and Density-Based Spatial Clustering of Applications\nwith Noise (DBSCAN) to steer sampled input trajectories into feasible regions\nwhile mitigating risks associated with weighted averaging. First, to ensure\nthat sampled trajectories remain within the feasible region, the primal-dual\ngradient method is applied to iteratively shift sampled inputs while enforcing\nstate and control constraints. Then, DBSCAN groups the sampled trajectories,\nenabling the selection of representative control inputs within each cluster.\nFinally, among the representative control inputs, the one with the lowest cost\nis chosen as the optimal action. As a result, CSC-MPPI guarantees constraint\nsatisfaction, improves trajectory selection, and enhances robustness in complex\nenvironments. Simulation and real-world experiments demonstrate that CSC-MPPI\noutperforms traditional MPPI in obstacle avoidance, achieving improved\nreliability and efficiency. The experimental videos are available at\nhttps://cscmppi.github.io", "AI": {"tldr": "CSC-MPPI\u662f\u4e00\u79cd\u6539\u8fdb\u7684MPPI\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u539f\u59cb-\u5bf9\u5076\u68af\u5ea6\u65b9\u6cd5\u548cDBSCAN\u805a\u7c7b\uff0c\u589e\u5f3a\u8f68\u8ff9\u4f18\u5316\u5e76\u4e25\u683c\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6\u3002", "motivation": "\u4f20\u7edfMPPI\u5728\u7ea6\u675f\u6ee1\u8db3\u548c\u8f68\u8ff9\u4f18\u5316\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0cCSC-MPPI\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u539f\u59cb-\u5bf9\u5076\u68af\u5ea6\u65b9\u6cd5\u786e\u4fdd\u7ea6\u675f\u6ee1\u8db3\uff0c\u5e76\u4f7f\u7528DBSCAN\u805a\u7c7b\u9009\u62e9\u4ee3\u8868\u6027\u63a7\u5236\u8f93\u5165\u3002", "result": "CSC-MPPI\u5728\u907f\u969c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edfMPPI\uff0c\u63d0\u9ad8\u4e86\u53ef\u9760\u6027\u548c\u6548\u7387\u3002", "conclusion": "CSC-MPPI\u901a\u8fc7\u4e25\u683c\u7ea6\u675f\u548c\u4f18\u5316\u8f68\u8ff9\u9009\u62e9\uff0c\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.16427", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16427", "abs": "https://arxiv.org/abs/2506.16427", "authors": ["Mohamad Hachem", "Cl\u00e9ment Roos", "Thierry Miquel", "Murat Bronz"], "title": "Full-Pose Tracking via Robust Control for Over-Actuated Multirotors", "comment": null, "summary": "This paper presents a robust cascaded control architecture for over-actuated\nmultirotors. It extends the Incremental Nonlinear Dynamic Inversion (INDI)\ncontrol combined with structured H_inf control, initially proposed for\nunder-actuated multirotors, to a broader range of multirotor configurations. To\nachieve precise and robust attitude and position tracking, we employ a weighted\nleast-squares geometric guidance control allocation method, formulated as a\nquadratic optimization problem, enabling full-pose tracking. The proposed\napproach effectively addresses key challenges, such as preventing infeasible\npose references and enhancing robustness against disturbances, as well as\nconsidering multirotor's actual physical limitations. Numerical simulations\nwith an over-actuated hexacopter validate the method's effectiveness,\ndemonstrating its adaptability to diverse mission scenarios and its potential\nfor real-world aerial applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8fc7\u9a71\u52a8\u591a\u65cb\u7ffc\u7684\u7ea7\u8054\u63a7\u5236\u67b6\u6784\uff0c\u7ed3\u5408INDI\u548c\u7ed3\u6784\u5316H\u221e\u63a7\u5236\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u59ff\u6001\u4e0e\u4f4d\u7f6e\u8ddf\u8e2a\u3002", "motivation": "\u6269\u5c55INDI\u548c\u7ed3\u6784\u5316H\u221e\u63a7\u5236\u7684\u5e94\u7528\u8303\u56f4\uff0c\u89e3\u51b3\u8fc7\u9a71\u52a8\u591a\u65cb\u7ffc\u7684\u7cbe\u786e\u63a7\u5236\u548c\u9c81\u68d2\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58\u51e0\u4f55\u5f15\u5bfc\u63a7\u5236\u5206\u914d\u65b9\u6cd5\uff0c\u5c06\u5176\u8868\u8ff0\u4e3a\u4e8c\u6b21\u4f18\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u5168\u59ff\u6001\u8ddf\u8e2a\u3002", "result": "\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u9002\u5e94\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u4e0d\u53ef\u884c\u59ff\u6001\u53c2\u8003\u548c\u5e72\u6270\u9c81\u68d2\u6027\u7b49\u5173\u952e\u6311\u6218\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u4efb\u52a1\u573a\u666f\u3002"}}
{"id": "2506.16475", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16475", "abs": "https://arxiv.org/abs/2506.16475", "authors": ["Yaru Niu", "Yunzhe Zhang", "Mingyang Yu", "Changyi Lin", "Chenhao Li", "Yikai Wang", "Yuxiang Yang", "Wenhao Yu", "Tingnan Zhang", "Bingqing Chen", "Jonathan Francis", "Zhenzhen Li", "Jie Tan", "Ding Zhao"], "title": "Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining", "comment": null, "summary": "Quadrupedal robots have demonstrated impressive locomotion capabilities in\ncomplex environments, but equipping them with autonomous versatile manipulation\nskills in a scalable way remains a significant challenge. In this work, we\nintroduce a cross-embodiment imitation learning system for quadrupedal\nmanipulation, leveraging data collected from both humans and LocoMan, a\nquadruped equipped with multiple manipulation modes. Specifically, we develop a\nteleoperation and data collection pipeline, which unifies and modularizes the\nobservation and action spaces of the human and the robot. To effectively\nleverage the collected data, we propose an efficient modularized architecture\nthat supports co-training and pretraining on structured modality-aligned data\nacross different embodiments. Additionally, we construct the first manipulation\ndataset for the LocoMan robot, covering various household tasks in both\nunimanual and bimanual modes, supplemented by a corresponding human dataset. We\nvalidate our system on six real-world manipulation tasks, where it achieves an\naverage success rate improvement of 41.9% overall and 79.7% under\nout-of-distribution (OOD) settings compared to the baseline. Pretraining with\nhuman data contributes a 38.6% success rate improvement overall and 82.7% under\nOOD settings, enabling consistently better performance with only half the\namount of robot data. Our code, hardware, and data are open-sourced at:\nhttps://human2bots.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u5177\u8eab\u6a21\u4eff\u5b66\u4e60\u7cfb\u7edf\uff0c\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u7684\u591a\u529f\u80fd\u64cd\u4f5c\uff0c\u901a\u8fc7\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u6570\u636e\u8054\u5408\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u591a\u529f\u80fd\u81ea\u4e3b\u64cd\u4f5c\u7684\u6311\u6218\uff0c\u901a\u8fc7\u8de8\u5177\u8eab\u6570\u636e\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u7edf\u4e00\u7684\u9065\u64cd\u4f5c\u548c\u6570\u636e\u6536\u96c6\u6d41\u7a0b\uff0c\u63d0\u51fa\u6a21\u5757\u5316\u67b6\u6784\u652f\u6301\u8de8\u5177\u8eab\u9884\u8bad\u7ec3\u548c\u8054\u5408\u8bad\u7ec3\u3002", "result": "\u5728\u516d\u9879\u5b9e\u9645\u4efb\u52a1\u4e2d\uff0c\u5e73\u5747\u6210\u529f\u7387\u63d0\u534741.9%\uff0cOOD\u8bbe\u7f6e\u4e0b\u63d0\u534779.7%\uff1b\u4eba\u7c7b\u6570\u636e\u9884\u8bad\u7ec3\u8d21\u732e\u663e\u8457\u3002", "conclusion": "\u8de8\u5177\u8eab\u6a21\u4eff\u5b66\u4e60\u6709\u6548\u63d0\u5347\u56db\u8db3\u673a\u5668\u4eba\u64cd\u4f5c\u80fd\u529b\uff0c\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u4fc3\u8fdb\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2506.16493", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16493", "abs": "https://arxiv.org/abs/2506.16493", "authors": ["Mehreen Naeem", "Andrew Melnik", "Michael Beetz"], "title": "Grounding Language Models with Semantic Digital Twins for Robotic Planning", "comment": null, "summary": "We introduce a novel framework that integrates Semantic Digital Twins (SDTs)\nwith Large Language Models (LLMs) to enable adaptive and goal-driven robotic\ntask execution in dynamic environments. The system decomposes natural language\ninstructions into structured action triplets, which are grounded in contextual\nenvironmental data provided by the SDT. This semantic grounding allows the\nrobot to interpret object affordances and interaction rules, enabling action\nplanning and real-time adaptability. In case of execution failures, the LLM\nutilizes error feedback and SDT insights to generate recovery strategies and\niteratively revise the action plan. We evaluate our approach using tasks from\nthe ALFRED benchmark, demonstrating robust performance across various household\nscenarios. The proposed framework effectively combines high-level reasoning\nwith semantic environment understanding, achieving reliable task completion in\nthe face of uncertainty and failure.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u6570\u5b57\u5b6a\u751f\uff08SDT\uff09\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u73af\u5883\u4e2d\u673a\u5668\u4eba\u4efb\u52a1\u7684\u81ea\u9002\u5e94\u6267\u884c\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d\u673a\u5668\u4eba\u4efb\u52a1\u6267\u884c\u7684\u9ad8\u5c42\u63a8\u7406\u4e0e\u8bed\u4e49\u73af\u5883\u7406\u89e3\u7684\u7ed3\u5408\u95ee\u9898\u3002", "method": "\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u52a8\u4f5c\u4e09\u5143\u7ec4\uff0c\u5e76\u901a\u8fc7SDT\u63d0\u4f9b\u7684\u73af\u5883\u6570\u636e\u5b9e\u73b0\u8bed\u4e49\u63a5\u5730\uff0c\u652f\u6301\u52a8\u4f5c\u89c4\u5212\u548c\u5b9e\u65f6\u9002\u5e94\u3002", "result": "\u5728ALFRED\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u4e0d\u786e\u5b9a\u6027\u548c\u5931\u8d25\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u7ed3\u5408\u9ad8\u5c42\u63a8\u7406\u4e0e\u8bed\u4e49\u73af\u5883\u7406\u89e3\uff0c\u5b9e\u73b0\u53ef\u9760\u4efb\u52a1\u5b8c\u6210\u3002"}}
{"id": "2506.16535", "categories": ["cs.RO", "cs.MA", "cs.NI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16535", "abs": "https://arxiv.org/abs/2506.16535", "authors": ["Tyler Landle", "Jordan Rapp", "Dean Blank", "Chandramouli Amarnath", "Abhijit Chatterjee", "Alex Daglis", "Umakishore Ramachandran"], "title": "eCAV: An Edge-Assisted Evaluation Platform for Connected Autonomous Vehicles", "comment": null, "summary": "As autonomous vehicles edge closer to widespread adoption, enhancing road\nsafety through collision avoidance and minimization of collateral damage\nbecomes imperative. Vehicle-to-everything (V2X) technologies, which include\nvehicle-to-vehicle (V2V), vehicle-to-infrastructure (V2I), and vehicle-to-cloud\n(V2C), are being proposed as mechanisms to achieve this safety improvement.\n  Simulation-based testing is crucial for early-stage evaluation of Connected\nAutonomous Vehicle (CAV) control systems, offering a safer and more\ncost-effective alternative to real-world tests. However, simulating large 3D\nenvironments with many complex single- and multi-vehicle sensors and\ncontrollers is computationally intensive. There is currently no evaluation\nframework that can effectively evaluate realistic scenarios involving large\nnumbers of autonomous vehicles.\n  We propose eCAV -- an efficient, modular, and scalable evaluation platform to\nfacilitate both functional validation of algorithmic approaches to increasing\nroad safety, as well as performance prediction of algorithms of various V2X\ntechnologies, including a futuristic Vehicle-to-Edge control plane and\ncorrespondingly designed control algorithms. eCAV can model up to 256 vehicles\nrunning individual control algorithms without perception enabled, which is\n$8\\times$ more vehicles than what is possible with state-of-the-art\nalternatives. %faster than state-of-the-art alternatives that can simulate\n$8\\times$ fewer vehicles. With perception enabled, eCAV simulates up to 64\nvehicles with a step time under 800ms, which is $4\\times$ more and $1.5\\times$\nfaster than the state-of-the-art OpenCDA framework.", "AI": {"tldr": "\u63d0\u51faeCAV\u5e73\u53f0\uff0c\u7528\u4e8e\u9ad8\u6548\u3001\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63a7\u5236\u7b97\u6cd5\u8bc4\u4f30\uff0c\u652f\u6301256\u8f86\u8f66\u65e0\u611f\u77e5\u6a21\u62df\u548c64\u8f86\u8f66\u6709\u611f\u77e5\u6a21\u62df\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u666e\u53ca\uff0c\u63d0\u5347\u9053\u8def\u5b89\u5168\u53ca\u51cf\u5c11\u4e8b\u6545\u635f\u5bb3\u9700\u6c42\u8feb\u5207\uff0c\u4f46\u73b0\u6709\u6a21\u62df\u6846\u67b6\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u5927\u89c4\u6a21\u8f66\u8f86\u573a\u666f\u3002", "method": "\u5f00\u53d1eCAV\u5e73\u53f0\uff0c\u652f\u6301\u591a\u8f66\u8f86\u63a7\u5236\u7b97\u6cd5\u9a8c\u8bc1\uff0c\u5305\u62ecV2X\u6280\u672f\u53ca\u672a\u6765\u8f66-\u8fb9\u7f18\u63a7\u5236\u5e73\u9762\u3002", "result": "eCAV\u53ef\u6a21\u62df256\u8f86\u65e0\u611f\u77e5\u8f66\u8f86\uff088\u500d\u4e8e\u73b0\u6709\u65b9\u6848\uff09\u548c64\u8f86\u6709\u611f\u77e5\u8f66\u8f86\uff084\u500d\u4e8eOpenCDA\uff0c\u901f\u5ea6\u5feb1.5\u500d\uff09\u3002", "conclusion": "eCAV\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7b97\u6cd5\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u5927\u89c4\u6a21\u8f66\u8f86\u6a21\u62df\u7684\u7a7a\u767d\u3002"}}
{"id": "2506.16537", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16537", "abs": "https://arxiv.org/abs/2506.16537", "authors": ["Sreeja Roy-Singh", "Alan P. Li", "Vinay Ravindra", "Roderick Lammers", "Marc Sanchez Net"], "title": "Agile, Autonomous Spacecraft Constellations with Disruption Tolerant Networking to Monitor Precipitation and Urban Floods", "comment": null, "summary": "Fully re-orientable small spacecraft are now supported by commercial\ntechnologies, allowing them to point their instruments in any direction and\ncapture images, with short notice. When combined with improved onboard\nprocessing, and implemented on a constellation of inter-communicable\nsatellites, this intelligent agility can significantly increase responsiveness\nto transient or evolving phenomena. We demonstrate a ground-based and onboard\nalgorithmic framework that combines orbital mechanics, attitude control,\ninter-satellite communication, intelligent prediction and planning to schedule\nthe time-varying, re-orientation of agile, small satellites in a constellation.\nPlanner intelligence is improved by updating the predictive value of future\nspace-time observations based on shared observations of evolving episodic\nprecipitation and urban flood forecasts. Reliable inter-satellite communication\nwithin a fast, dynamic constellation topology is modeled in the physical,\naccess control and network layer. We apply the framework on a representative\n24-satellite constellation observing 5 global regions. Results show\nappropriately low latency in information exchange (average within 1/3rd\navailable time for implicit consensus), enabling the onboard scheduler to\nobserve ~7% more flood magnitude than a ground-based implementation. Both\nonboard and offline versions performed ~98% better than constellations without\nagility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u578b\u654f\u6377\u536b\u661f\u661f\u5ea7\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u7ed3\u5408\u8f68\u9053\u529b\u5b66\u3001\u59ff\u6001\u63a7\u5236\u548c\u661f\u95f4\u901a\u4fe1\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u77ac\u6001\u6216\u52a8\u6001\u73b0\u8c61\u7684\u54cd\u5e94\u80fd\u529b\u3002", "motivation": "\u5229\u7528\u5546\u4e1a\u6280\u672f\u652f\u6301\u7684\u5b8c\u5168\u53ef\u5b9a\u5411\u5c0f\u578b\u822a\u5929\u5668\uff0c\u7ed3\u5408\u661f\u95f4\u901a\u4fe1\u548c\u667a\u80fd\u9884\u6d4b\uff0c\u63d0\u9ad8\u5bf9\u52a8\u6001\u73b0\u8c61\uff08\u5982\u964d\u6c34\u4e0e\u57ce\u5e02\u6d2a\u6c34\uff09\u7684\u89c2\u6d4b\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e86\u5730\u9762\u548c\u661f\u8f7d\u7b97\u6cd5\u6846\u67b6\uff0c\u7ed3\u5408\u8f68\u9053\u529b\u5b66\u3001\u59ff\u6001\u63a7\u5236\u3001\u661f\u95f4\u901a\u4fe1\u548c\u667a\u80fd\u89c4\u5212\uff0c\u52a8\u6001\u8c03\u5ea6\u536b\u661f\u661f\u5ea7\u7684\u89c2\u6d4b\u4efb\u52a1\u3002", "result": "\u572824\u9897\u536b\u661f\u7684\u661f\u5ea7\u4e2d\uff0c\u661f\u8f7d\u8c03\u5ea6\u5668\u89c2\u6d4b\u5230\u7684\u6d2a\u6c34\u5e45\u5ea6\u6bd4\u5730\u9762\u5b9e\u73b0\u591a7%\uff0c\u4e14\u6bd4\u975e\u654f\u6377\u661f\u5ea7\u6027\u80fd\u63d0\u534798%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u667a\u80fd\u89c4\u5212\u548c\u661f\u95f4\u901a\u4fe1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u536b\u661f\u661f\u5ea7\u5bf9\u52a8\u6001\u73b0\u8c61\u7684\u89c2\u6d4b\u80fd\u529b\u548c\u54cd\u5e94\u901f\u5ea6\u3002"}}
{"id": "2506.16546", "categories": ["cs.RO", "cs.AI", "cs.ET", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16546", "abs": "https://arxiv.org/abs/2506.16546", "authors": ["Liyang Yu", "Tianyi Wang", "Junfeng Jiao", "Fengwu Shan", "Hongqing Chu", "Bingzhao Gao"], "title": "BIDA: A Bi-level Interaction Decision-making Algorithm for Autonomous Vehicles in Dynamic Traffic Scenarios", "comment": "6 pages, 3 figures, 4 tables, accepted for IEEE Intelligent Vehicles\n  (IV) Symposium 2025", "summary": "In complex real-world traffic environments, autonomous vehicles (AVs) need to\ninteract with other traffic participants while making real-time and\nsafety-critical decisions accordingly. The unpredictability of human behaviors\nposes significant challenges, particularly in dynamic scenarios, such as\nmulti-lane highways and unsignalized T-intersections. To address this gap, we\ndesign a bi-level interaction decision-making algorithm (BIDA) that integrates\ninteractive Monte Carlo tree search (MCTS) with deep reinforcement learning\n(DRL), aiming to enhance interaction rationality, efficiency and safety of AVs\nin dynamic key traffic scenarios. Specifically, we adopt three types of DRL\nalgorithms to construct a reliable value network and policy network, which\nguide the online deduction process of interactive MCTS by assisting in value\nupdate and node selection. Then, a dynamic trajectory planner and a trajectory\ntracking controller are designed and implemented in CARLA to ensure smooth\nexecution of planned maneuvers. Experimental evaluations demonstrate that our\nBIDA not only enhances interactive deduction and reduces computational costs,\nbut also outperforms other latest benchmarks, which exhibits superior safety,\nefficiency and interaction rationality under varying traffic conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42\u4ea4\u4e92\u51b3\u7b56\u7b97\u6cd5\uff08BIDA\uff09\uff0c\u7ed3\u5408\u4ea4\u4e92\u5f0f\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\uff0c\u4ee5\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u52a8\u6001\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u4ea4\u4e92\u7406\u6027\u3001\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e2d\u56e0\u4eba\u7c7b\u884c\u4e3a\u4e0d\u53ef\u9884\u6d4b\u6027\u800c\u9762\u4e34\u7684\u51b3\u7b56\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u591a\u8f66\u9053\u9ad8\u901f\u516c\u8def\u548c\u65e0\u4fe1\u53f7T\u578b\u4ea4\u53c9\u53e3\u7b49\u52a8\u6001\u573a\u666f\u3002", "method": "\u91c7\u7528\u4e09\u79cdDRL\u7b97\u6cd5\u6784\u5efa\u53ef\u9760\u7684\u4ef7\u503c\u7f51\u7edc\u548c\u7b56\u7565\u7f51\u7edc\uff0c\u6307\u5bfc\u4ea4\u4e92\u5f0fMCTS\u7684\u5728\u7ebf\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u7ed3\u5408\u52a8\u6001\u8f68\u8ff9\u89c4\u5212\u5668\u548c\u8f68\u8ff9\u8ddf\u8e2a\u63a7\u5236\u5668\u5728CARLA\u4e2d\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBIDA\u4e0d\u4ec5\u63d0\u5347\u4e86\u4ea4\u4e92\u63a8\u7406\u80fd\u529b\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u8fd8\u5728\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u4ea4\u4e92\u7406\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6700\u65b0\u57fa\u51c6\u3002", "conclusion": "BIDA\u5728\u52a8\u6001\u4ea4\u901a\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u4ea4\u4e92\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.16555", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16555", "abs": "https://arxiv.org/abs/2506.16555", "authors": ["Melih \u00d6zcan", "Ozgur S. Oguz"], "title": "An Optimization-Augmented Control Framework for Single and Coordinated Multi-Arm Robotic Manipulation", "comment": "8 pages, 8 figures, accepted for oral presentation at IROS 2025.\n  Supplementary site: https://sites.google.com/view/komo-force/home", "summary": "Robotic manipulation demands precise control over both contact forces and\nmotion trajectories. While force control is essential for achieving compliant\ninteraction and high-frequency adaptation, it is limited to operations in close\nproximity to the manipulated object and often fails to maintain stable\norientation during extended motion sequences. Conversely, optimization-based\nmotion planning excels in generating collision-free trajectories over the\nrobot's configuration space but struggles with dynamic interactions where\ncontact forces play a crucial role. To address these limitations, we propose a\nmulti-modal control framework that combines force control and\noptimization-augmented motion planning to tackle complex robotic manipulation\ntasks in a sequential manner, enabling seamless switching between control modes\nbased on task requirements. Our approach decomposes complex tasks into\nsubtasks, each dynamically assigned to one of three control modes: Pure\noptimization for global motion planning, pure force control for precise\ninteraction, or hybrid control for tasks requiring simultaneous trajectory\ntracking and force regulation. This framework is particularly advantageous for\nbimanual and multi-arm manipulation, where synchronous motion and coordination\namong arms are essential while considering both the manipulated object and\nenvironmental constraints. We demonstrate the versatility of our method through\na range of long-horizon manipulation tasks, including single-arm, bimanual, and\nmulti-arm applications, highlighting its ability to handle both free-space\nmotion and contact-rich manipulation with robustness and precision.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u529b\u63a7\u5236\u548c\u4f18\u5316\u8fd0\u52a8\u89c4\u5212\u7684\u591a\u6a21\u6001\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u529b\u63a7\u5236\u548c\u8fd0\u52a8\u89c4\u5212\u5404\u6709\u5c40\u9650\u6027\uff0c\u529b\u63a7\u5236\u96be\u4ee5\u7ef4\u6301\u7a33\u5b9a\u65b9\u5411\uff0c\u8fd0\u52a8\u89c4\u5212\u4e0d\u64c5\u957f\u52a8\u6001\u4ea4\u4e92\u3002", "method": "\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u52a8\u6001\u5206\u914d\u4e09\u79cd\u63a7\u5236\u6a21\u5f0f\uff1a\u7eaf\u4f18\u5316\u3001\u7eaf\u529b\u63a7\u5236\u6216\u6df7\u5408\u63a7\u5236\u3002", "result": "\u5728\u5355\u81c2\u3001\u53cc\u81c2\u548c\u591a\u81c2\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u7cbe\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u65e0\u7f1d\u5207\u6362\u63a7\u5236\u6a21\u5f0f\uff0c\u9002\u7528\u4e8e\u81ea\u7531\u7a7a\u95f4\u8fd0\u52a8\u548c\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u3002"}}
{"id": "2506.16565", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16565", "abs": "https://arxiv.org/abs/2506.16565", "authors": ["Yuxin Chen", "Jianglan Wei", "Chenfeng Xu", "Boyi Li", "Masayoshi Tomizuka", "Andrea Bajcsy", "Ran Tian"], "title": "Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control", "comment": null, "summary": "World models enable robots to \"imagine\" future observations given current\nobservations and planned actions, and have been increasingly adopted as\ngeneralized dynamics models to facilitate robot learning. Despite their\npromise, these models remain brittle when encountering novel visual distractors\nsuch as objects and background elements rarely seen during training.\nSpecifically, novel distractors can corrupt action outcome predictions, causing\ndownstream failures when robots rely on the world model imaginations for\nplanning or action verification. In this work, we propose Reimagination with\nObservation Intervention (ReOI), a simple yet effective test-time strategy that\nenables world models to predict more reliable action outcomes in open-world\nscenarios where novel and unanticipated visual distractors are inevitable.\nGiven the current robot observation, ReOI first detects visual distractors by\nidentifying which elements of the scene degrade in physically implausible ways\nduring world model prediction. Then, it modifies the current observation to\nremove these distractors and bring the observation closer to the training\ndistribution. Finally, ReOI \"reimagines\" future outcomes with the modified\nobservation and reintroduces the distractors post-hoc to preserve visual\nconsistency for downstream planning and verification. We validate our approach\non a suite of robotic manipulation tasks in the context of action verification,\nwhere the verifier needs to select desired action plans based on predictions\nfrom a world model. Our results show that ReOI is robust to both\nin-distribution and out-of-distribution visual distractors. Notably, it\nimproves task success rates by up to 3x in the presence of novel distractors,\nsignificantly outperforming action verification that relies on world model\npredictions without imagination interventions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faReOI\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u5e76\u79fb\u9664\u89c6\u89c9\u5e72\u6270\u7269\uff0c\u63d0\u5347\u4e16\u754c\u6a21\u578b\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u9884\u6d4b\u53ef\u9760\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u4e16\u754c\u6a21\u578b\u5728\u9047\u5230\u8bad\u7ec3\u4e2d\u7f55\u89c1\u7684\u89c6\u89c9\u5e72\u6270\u7269\u65f6\u8868\u73b0\u8106\u5f31\uff0c\u5bfc\u81f4\u9884\u6d4b\u5931\u6548\uff0c\u5f71\u54cd\u673a\u5668\u4eba\u89c4\u5212\u548c\u52a8\u4f5c\u9a8c\u8bc1\u3002", "method": "\u63d0\u51faReOI\u65b9\u6cd5\uff1a\u68c0\u6d4b\u5e72\u6270\u7269\u3001\u4fee\u6539\u89c2\u6d4b\u3001\u91cd\u65b0\u9884\u6d4b\u5e76\u6062\u590d\u5e72\u6270\u7269\u4ee5\u4fdd\u6301\u89c6\u89c9\u4e00\u81f4\u6027\u3002", "result": "ReOI\u5bf9\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u5e72\u6270\u7269\u5747\u6709\u6548\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u5347\u9ad8\u8fbe3\u500d\u3002", "conclusion": "ReOI\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e16\u754c\u6a21\u578b\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.16593", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16593", "abs": "https://arxiv.org/abs/2506.16593", "authors": ["Nicolas Samson", "William Larriv\u00e9e-Hardy", "William Dubois", "\u00c9lie Roy-Brouard", "Edith Brotherton", "Dominic Baril", "Julien L\u00e9pine", "Fran\u00e7ois Pomerleau"], "title": "DRIVE Through the Unpredictability:From a Protocol Investigating Slip to a Metric Estimating Command Uncertainty", "comment": "This version is the preprint of a journal article with the same\n  title, accepted in the IEEE Transactions on Field Robotics. To have a look at\n  the early access version, use the following link\n  https://ieeexplore.ieee.org/document/11037776", "summary": "Off-road autonomous navigation is a challenging task as it is mainly\ndependent on the accuracy of the motion model. Motion model performances are\nlimited by their ability to predict the interaction between the terrain and the\nUGV, which an onboard sensor can not directly measure. In this work, we propose\nusing the DRIVE protocol to standardize the collection of data for system\nidentification and characterization of the slip state space. We validated this\nprotocol by acquiring a dataset with two platforms (from 75 kg to 470 kg) on\nsix terrains (i.e., asphalt, grass, gravel, ice, mud, sand) for a total of 4.9\nhours and 14.7 km. Using this data, we evaluate the DRIVE protocol's ability to\nexplore the velocity command space and identify the reachable velocities for\nterrain-robot interactions. We investigated the transfer function between the\ncommand velocity space and the resulting steady-state slip for an SSMR. An\nunpredictability metric is proposed to estimate command uncertainty and help\nassess risk likelihood and severity in deployment. Finally, we share our\nlessons learned on running system identification on large UGV to help the\ncommunity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528DRIVE\u534f\u8bae\u6807\u51c6\u5316\u6570\u636e\u6536\u96c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc6\u522b\u548c\u6ed1\u79fb\u72b6\u6001\u7a7a\u95f4\u8868\u5f81\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8d8a\u91ce\u81ea\u4e3b\u5bfc\u822a\u4f9d\u8d56\u4e8e\u8fd0\u52a8\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u4f46\u8fd0\u52a8\u6a21\u578b\u53d7\u9650\u4e8e\u5176\u5bf9\u5730\u5f62\u4e0e\u65e0\u4eba\u5730\u9762\u8f66\u8f86\uff08UGV\uff09\u4ea4\u4e92\u7684\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u91c7\u7528DRIVE\u534f\u8bae\u6536\u96c6\u6570\u636e\uff0c\u9a8c\u8bc1\u5176\u5728\u63a2\u7d22\u901f\u5ea6\u547d\u4ee4\u7a7a\u95f4\u548c\u8bc6\u522b\u53ef\u8fbe\u901f\u5ea6\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e0d\u53ef\u9884\u6d4b\u6027\u5ea6\u91cf\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DRIVE\u534f\u8bae\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc4\u4f30\u90e8\u7f72\u98ce\u9669\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u5ea6\u91cf\u3002", "conclusion": "DRIVE\u534f\u8bae\u6709\u52a9\u4e8e\u7cfb\u7edf\u8bc6\u522b\uff0c\u5e76\u5206\u4eab\u4e86\u5728\u5927\u578bUGV\u4e0a\u8fd0\u884c\u7cfb\u7edf\u8bc6\u522b\u7684\u7ecf\u9a8c\u3002"}}
{"id": "2506.16623", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16623", "abs": "https://arxiv.org/abs/2506.16623", "authors": ["Mobin Habibpour", "Fatemeh Afghah"], "title": "History-Augmented Vision-Language Models for Frontier-Based Zero-Shot Object Navigation", "comment": null, "summary": "Object Goal Navigation (ObjectNav) challenges robots to find objects in\nunseen environments, demanding sophisticated reasoning. While Vision-Language\nModels (VLMs) show potential, current ObjectNav methods often employ them\nsuperficially, primarily using vision-language embeddings for object-scene\nsimilarity checks rather than leveraging deeper reasoning. This limits\ncontextual understanding and leads to practical issues like repetitive\nnavigation behaviors. This paper introduces a novel zero-shot ObjectNav\nframework that pioneers the use of dynamic, history-aware prompting to more\ndeeply integrate VLM reasoning into frontier-based exploration. Our core\ninnovation lies in providing the VLM with action history context, enabling it\nto generate semantic guidance scores for navigation actions while actively\navoiding decision loops. We also introduce a VLM-assisted waypoint generation\nmechanism for refining the final approach to detected objects. Evaluated on the\nHM3D dataset within Habitat, our approach achieves a 46% Success Rate (SR) and\n24.8% Success weighted by Path Length (SPL). These results are comparable to\nstate-of-the-art zero-shot methods, demonstrating the significant potential of\nour history-augmented VLM prompting strategy for more robust and context-aware\nrobotic navigation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u76ee\u6807\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5386\u53f2\u611f\u77e5\u63d0\u793a\u6df1\u5ea6\u6574\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u63a8\u7406\uff0c\u63d0\u5347\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u76ee\u6807\u5bfc\u822a\u65b9\u6cd5\u5bf9VLM\u7684\u5229\u7528\u8f83\u6d45\uff0c\u4ec5\u7528\u4e8e\u5bf9\u8c61-\u573a\u666f\u76f8\u4f3c\u6027\u68c0\u67e5\uff0c\u7f3a\u4e4f\u6df1\u5ea6\u63a8\u7406\uff0c\u5bfc\u81f4\u4e0a\u4e0b\u6587\u7406\u89e3\u4e0d\u8db3\u548c\u91cd\u590d\u5bfc\u822a\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u52a8\u6001\u5386\u53f2\u611f\u77e5\u63d0\u793a\uff0c\u4e3aVLM\u63d0\u4f9b\u52a8\u4f5c\u5386\u53f2\u4e0a\u4e0b\u6587\uff0c\u751f\u6210\u8bed\u4e49\u5bfc\u822a\u8bc4\u5206\uff0c\u5e76\u5f15\u5165VLM\u8f85\u52a9\u7684\u8def\u5f84\u70b9\u751f\u6210\u673a\u5236\u3002", "result": "\u5728HM3D\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8646%\u7684\u6210\u529f\u7387\u548c24.8%\u7684\u8def\u5f84\u52a0\u6743\u6210\u529f\u7387\uff08SPL\uff09\uff0c\u4e0e\u96f6\u6837\u672c\u65b9\u6cd5\u7684\u6700\u4f18\u7ed3\u679c\u76f8\u5f53\u3002", "conclusion": "\u5386\u53f2\u589e\u5f3a\u7684VLM\u63d0\u793a\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u7684\u9c81\u68d2\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2506.16643", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.16643", "abs": "https://arxiv.org/abs/2506.16643", "authors": ["Matthew Ebisu", "Hang Yu", "Reuben Aronson", "Elaine Short"], "title": "See What I Mean? Expressiveness and Clarity in Robot Display Design", "comment": null, "summary": "Nonverbal visual symbols and displays play an important role in communication\nwhen humans and robots work collaboratively. However, few studies have\ninvestigated how different types of non-verbal cues affect objective task\nperformance, especially in a dynamic environment that requires real time\ndecision-making. In this work, we designed a collaborative navigation task\nwhere the user and the robot only had partial information about the map on each\nend and thus the users were forced to communicate with a robot to complete the\ntask. We conducted our study in a public space and recruited 37 participants\nwho randomly passed by our setup. Each participant collaborated with a robot\nutilizing either animated anthropomorphic eyes and animated icons, or static\nanthropomorphic eyes and static icons. We found that participants that\ninteracted with a robot with animated displays reported the greatest level of\ntrust and satisfaction; that participants interpreted static icons the best;\nand that participants with a robot with static eyes had the highest completion\nsuccess. These results suggest that while animation can foster trust with\nrobots, human-robot communication can be optimized by the addition of familiar\nstatic icons that may be easier for users to interpret. We published our code,\ndesigned symbols, and collected results online at:\nhttps://github.com/mattufts/huamn_Cozmo_interaction.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u975e\u8bed\u8a00\u89c6\u89c9\u7b26\u53f7\u5728\u4eba\u4e0e\u673a\u5668\u4eba\u534f\u4f5c\u4e2d\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u52a8\u753b\u663e\u793a\u80fd\u589e\u5f3a\u4fe1\u4efb\uff0c\u800c\u9759\u6001\u56fe\u6807\u66f4\u6613\u7406\u89e3\u3002", "motivation": "\u63a2\u7d22\u4e0d\u540c\u7c7b\u578b\u975e\u8bed\u8a00\u63d0\u793a\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5bf9\u4efb\u52a1\u8868\u73b0\u7684\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e86\u534f\u4f5c\u5bfc\u822a\u4efb\u52a1\uff0c\u6bd4\u8f83\u52a8\u753b\u4e0e\u9759\u6001\u663e\u793a\u5bf9\u7528\u6237\u4fe1\u4efb\u548c\u4efb\u52a1\u5b8c\u6210\u7684\u5f71\u54cd\u3002", "result": "\u52a8\u753b\u663e\u793a\u63d0\u5347\u4fe1\u4efb\uff0c\u9759\u6001\u56fe\u6807\u66f4\u6613\u7406\u89e3\uff0c\u9759\u6001\u773c\u775b\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "\u52a8\u753b\u589e\u5f3a\u4fe1\u4efb\uff0c\u4f46\u9759\u6001\u56fe\u6807\u66f4\u4f18\uff0c\u9700\u7ed3\u5408\u4e24\u8005\u4f18\u5316\u4eba\u673a\u4ea4\u4e92\u3002"}}
{"id": "2506.16652", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.16652", "abs": "https://arxiv.org/abs/2506.16652", "authors": ["Guang Yin", "Yitong Li", "Yixuan Wang", "Dale McConachie", "Paarth Shah", "Kunimatsu Hashimoto", "Huan Zhang", "Katherine Liu", "Yunzhu Li"], "title": "CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity", "comment": "Accepted to Robotics: Science and Systems (RSS) 2025. The first three\n  authors contributed equally. Project Page:\n  https://robopil.github.io/code-diffuser/", "summary": "Natural language instructions for robotic manipulation tasks often exhibit\nambiguity and vagueness. For instance, the instruction \"Hang a mug on the mug\ntree\" may involve multiple valid actions if there are several mugs and branches\nto choose from. Existing language-conditioned policies typically rely on\nend-to-end models that jointly handle high-level semantic understanding and\nlow-level action generation, which can result in suboptimal performance due to\ntheir lack of modularity and interpretability. To address these challenges, we\nintroduce a novel robotic manipulation framework that can accomplish tasks\nspecified by potentially ambiguous natural language. This framework employs a\nVision-Language Model (VLM) to interpret abstract concepts in natural language\ninstructions and generates task-specific code - an interpretable and executable\nintermediate representation. The generated code interfaces with the perception\nmodule to produce 3D attention maps that highlight task-relevant regions by\nintegrating spatial and semantic information, effectively resolving ambiguities\nin instructions. Through extensive experiments, we identify key limitations of\ncurrent imitation learning methods, such as poor adaptation to language and\nenvironmental variations. We show that our approach excels across challenging\nmanipulation tasks involving language ambiguity, contact-rich manipulation, and\nmulti-object interactions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u89e3\u91ca\u7684\u4e2d\u95f4\u4ee3\u7801\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u6a21\u7cca\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6761\u4ef6\u7b56\u7565\u56e0\u7f3a\u4e4f\u6a21\u5757\u5316\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\uff0c\u9700\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u6a21\u7cca\u6027\u3002", "method": "\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u89e3\u6790\u6307\u4ee4\u5e76\u751f\u6210\u4efb\u52a1\u4ee3\u7801\uff0c\u7ed3\u5408\u611f\u77e5\u6a21\u5757\u751f\u62103D\u6ce8\u610f\u529b\u56fe\u4ee5\u6d88\u9664\u6b67\u4e49\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bed\u8a00\u6a21\u7cca\u6027\u3001\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u548c\u591a\u7269\u4f53\u4ea4\u4e92\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u4e2d\u95f4\u8868\u793a\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u3002"}}
{"id": "2506.16685", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16685", "abs": "https://arxiv.org/abs/2506.16685", "authors": ["Xiaomeng Xu", "Yifan Hou", "Zeyi Liu", "Shuran Song"], "title": "Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections", "comment": null, "summary": "We address key challenges in Dataset Aggregation (DAgger) for real-world\ncontact-rich manipulation: how to collect informative human correction data and\nhow to effectively update policies with this new data. We introduce Compliant\nResidual DAgger (CR-DAgger), which contains two novel components: 1) a\nCompliant Intervention Interface that leverages compliance control, allowing\nhumans to provide gentle, accurate delta action corrections without\ninterrupting the ongoing robot policy execution; and 2) a Compliant Residual\nPolicy formulation that learns from human corrections while incorporating force\nfeedback and force control. Our system significantly enhances performance on\nprecise contact-rich manipulation tasks using minimal correction data,\nimproving base policy success rates by over 50\\% on two challenging tasks (book\nflipping and belt assembly) while outperforming both retraining-from-scratch\nand finetuning approaches. Through extensive real-world experiments, we provide\npractical guidance for implementing effective DAgger in real-world robot\nlearning tasks. Result videos are available at:\nhttps://compliant-residual-dagger.github.io/", "AI": {"tldr": "CR-DAgger\u901a\u8fc7\u5408\u89c4\u5e72\u9884\u63a5\u53e3\u548c\u6b8b\u5dee\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3DAgger\u5728\u771f\u5b9e\u4e16\u754c\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u6570\u636e\u6536\u96c6\u548c\u7b56\u7565\u66f4\u65b0\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165\u5408\u89c4\u5e72\u9884\u63a5\u53e3\u548c\u5408\u89c4\u6b8b\u5dee\u7b56\u7565\uff0c\u7ed3\u5408\u529b\u53cd\u9988\u548c\u63a7\u5236\u3002", "result": "\u5728\u4e66\u7c4d\u7ffb\u9875\u548c\u76ae\u5e26\u7ec4\u88c5\u4efb\u52a1\u4e2d\uff0c\u6210\u529f\u7387\u63d0\u534750%\u4ee5\u4e0a\uff0c\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\u548c\u5fae\u8c03\u65b9\u6cd5\u3002", "conclusion": "CR-DAgger\u4e3a\u771f\u5b9e\u673a\u5668\u4eba\u5b66\u4e60\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684DAgger\u5b9e\u73b0\u6307\u5bfc\u3002"}}
{"id": "2506.16703", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16703", "abs": "https://arxiv.org/abs/2506.16703", "authors": ["Sinuo Cheng", "Ruyi Zhou", "Wenhao Feng", "Huaiguang Yang", "Haibo Gao", "Zongquan Deng", "Liang Ding"], "title": "VLM-Empowered Multi-Mode System for Efficient and Safe Planetary Navigation", "comment": "accepted by IROS 2025", "summary": "The increasingly complex and diverse planetary exploration environment\nrequires more adaptable and flexible rover navigation strategy. In this study,\nwe propose a VLM-empowered multi-mode system to achieve efficient while safe\nautonomous navigation for planetary rovers. Vision-Language Model (VLM) is used\nto parse scene information by image inputs to achieve a human-level\nunderstanding of terrain complexity. Based on the complexity classification,\nthe system switches to the most suitable navigation mode, composing of\nperception, mapping and planning modules designed for different terrain types,\nto traverse the terrain ahead before reaching the next waypoint. By integrating\nthe local navigation system with a map server and a global waypoint generation\nmodule, the rover is equipped to handle long-distance navigation tasks in\ncomplex scenarios. The navigation system is evaluated in various simulation\nenvironments. Compared to the single-mode conservative navigation method, our\nmulti-mode system is able to bootstrap the time and energy efficiency in a\nlong-distance traversal with varied type of obstacles, enhancing efficiency by\n79.5%, while maintaining its avoidance capabilities against terrain hazards to\nguarantee rover safety. More system information is shown at\nhttps://chengsn1234.github.io/multi-mode-planetary-navigation/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u591a\u6a21\u5f0f\u884c\u661f\u6f2b\u6e38\u8f66\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u5730\u5f62\u590d\u6742\u5ea6\u5206\u7c7b\u5207\u6362\u5bfc\u822a\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u884c\u661f\u63a2\u7d22\u73af\u5883\u590d\u6742\u591a\u6837\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u9002\u5e94\u7684\u5bfc\u822a\u7b56\u7565\u3002", "method": "\u5229\u7528VLM\u89e3\u6790\u573a\u666f\u4fe1\u606f\uff0c\u6839\u636e\u5730\u5f62\u590d\u6742\u5ea6\u5206\u7c7b\u5207\u6362\u5bfc\u822a\u6a21\u5f0f\uff08\u611f\u77e5\u3001\u5efa\u56fe\u3001\u89c4\u5212\uff09\uff0c\u5e76\u7ed3\u5408\u5168\u5c40\u8def\u5f84\u751f\u6210\u6a21\u5757\u3002", "result": "\u591a\u6a21\u5f0f\u7cfb\u7edf\u5728\u6a21\u62df\u73af\u5883\u4e2d\u6548\u7387\u63d0\u534779.5%\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5730\u5f62\u5371\u9669\u7684\u89c4\u907f\u80fd\u529b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u81ea\u4e3b\u5bfc\u822a\u3002"}}
{"id": "2506.16710", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.16710", "abs": "https://arxiv.org/abs/2506.16710", "authors": ["Aditya Bhatt", "Mary Katherine Corra", "Franklin Merlo", "Prajit KrisshnaKumar", "Souma Chowdhury"], "title": "Experimental Setup and Software Pipeline to Evaluate Optimization based Autonomous Multi-Robot Search Algorithms", "comment": "to be published in IDETC 2025 conference proceedings", "summary": "Signal source localization has been a problem of interest in the multi-robot\nsystems domain given its applications in search \\& rescue and hazard\nlocalization in various industrial and outdoor settings. A variety of\nmulti-robot search algorithms exist that usually formulate and solve the\nassociated autonomous motion planning problem as a heuristic model-free or\nbelief model-based optimization process. Most of these algorithms however\nremains tested only in simulation, thereby losing the opportunity to generate\nknowledge about how such algorithms would compare/contrast in a real physical\nsetting in terms of search performance and real-time computing performance. To\naddress this gap, this paper presents a new lab-scale physical setup and\nassociated open-source software pipeline to evaluate and benchmark multi-robot\nsearch algorithms. The presented physical setup innovatively uses an acoustic\nsource (that is safe and inexpensive) and small ground robots (e-pucks)\noperating in a standard motion-capture environment. This setup can be easily\nrecreated and used by most robotics researchers. The acoustic source also\npresents interesting uncertainty in terms of its noise-to-signal ratio, which\nis useful to assess sim-to-real gaps. The overall software pipeline is designed\nto readily interface with any multi-robot search algorithm with minimal effort\nand is executable in parallel asynchronous form. This pipeline includes a\nframework for distributed implementation of multi-robot or swarm search\nalgorithms, integrated with a ROS (Robotics Operating System)-based software\nstack for motion capture supported localization. The utility of this novel\nsetup is demonstrated by using it to evaluate two state-of-the-art multi-robot\nsearch algorithms, based on swarm optimization and batch-Bayesian Optimization\n(called Bayes-Swarm), as well as a random walk baseline.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b9e\u9a8c\u8bbe\u7f6e\u548c\u5f00\u6e90\u8f6f\u4ef6\u7ba1\u9053\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u673a\u5668\u4eba\u641c\u7d22\u7b97\u6cd5\uff0c\u586b\u8865\u4e86\u6a21\u62df\u4e0e\u7269\u7406\u6d4b\u8bd5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u4fe1\u53f7\u6e90\u5b9a\u4f4d\u5728\u641c\u7d22\u6551\u63f4\u548c\u5371\u9669\u5b9a\u4f4d\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u7b97\u6cd5\u591a\u9650\u4e8e\u6a21\u62df\u6d4b\u8bd5\uff0c\u7f3a\u4e4f\u7269\u7406\u73af\u5883\u4e0b\u7684\u6027\u80fd\u8bc4\u4f30\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u58f0\u6e90\u548c\u5c0f\u578b\u5730\u9762\u673a\u5668\u4eba\u7684\u5b9e\u9a8c\u8bbe\u7f6e\uff0c\u5f00\u53d1\u4e86\u5f00\u6e90\u8f6f\u4ef6\u7ba1\u9053\uff0c\u652f\u6301\u5206\u5e03\u5f0f\u591a\u673a\u5668\u4eba\u641c\u7d22\u7b97\u6cd5\u7684\u5e76\u884c\u5f02\u6b65\u6267\u884c\u3002", "result": "\u5b9e\u9a8c\u8bbe\u7f6e\u6210\u529f\u8bc4\u4f30\u4e86\u4e24\u79cd\u5148\u8fdb\u7684\u591a\u673a\u5668\u4eba\u641c\u7d22\u7b97\u6cd5\uff08\u7fa4\u4f53\u4f18\u5316\u548c\u8d1d\u53f6\u65af\u4f18\u5316\uff09\u4ee5\u53ca\u968f\u673a\u884c\u8d70\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u5b9e\u9a8c\u8bbe\u7f6e\u548c\u8f6f\u4ef6\u7ba1\u9053\u4e3a\u591a\u673a\u5668\u4eba\u641c\u7d22\u7b97\u6cd5\u7684\u7269\u7406\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u7f29\u5c0f\u6a21\u62df\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002"}}
{"id": "2506.16720", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16720", "abs": "https://arxiv.org/abs/2506.16720", "authors": ["Weitao Zhou", "Bo Zhang", "Zhong Cao", "Xiang Li", "Qian Cheng", "Chunyang Liu", "Yaqin Zhang", "Diange Yang"], "title": "DRARL: Disengagement-Reason-Augmented Reinforcement Learning for Efficient Improvement of Autonomous Driving Policy", "comment": null, "summary": "With the increasing presence of automated vehicles on open roads under driver\nsupervision, disengagement cases are becoming more prevalent. While some\ndata-driven planning systems attempt to directly utilize these disengagement\ncases for policy improvement, the inherent scarcity of disengagement data\n(often occurring as a single instances) restricts training effectiveness.\nFurthermore, some disengagement data should be excluded since the disengagement\nmay not always come from the failure of driving policies, e.g. the driver may\ncasually intervene for a while. To this end, this work proposes\ndisengagement-reason-augmented reinforcement learning (DRARL), which enhances\ndriving policy improvement process according to the reason of disengagement\ncases. Specifically, the reason of disengagement is identified by a\nout-of-distribution (OOD) state estimation model. When the reason doesn't\nexist, the case will be identified as a casual disengagement case, which\ndoesn't require additional policy adjustment. Otherwise, the policy can be\nupdated under a reason-augmented imagination environment, improving the policy\nperformance of disengagement cases with similar reasons. The method is\nevaluated using real-world disengagement cases collected by autonomous driving\nrobotaxi. Experimental results demonstrate that the method accurately\nidentifies policy-related disengagement reasons, allowing the agent to handle\nboth original and semantically similar cases through reason-augmented training.\nFurthermore, the approach prevents the agent from becoming overly conservative\nafter policy adjustments. Overall, this work provides an efficient way to\nimprove driving policy performance with disengagement cases.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8131\u94a9\u539f\u56e0\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08DRARL\uff09\uff0c\u901a\u8fc7\u8bc6\u522b\u8131\u94a9\u539f\u56e0\u4f18\u5316\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\uff0c\u907f\u514d\u65e0\u6548\u6570\u636e\u5e72\u6270\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5f00\u653e\u9053\u8def\u4e0a\u7684\u8131\u94a9\u6848\u4f8b\u589e\u591a\uff0c\u4f46\u6570\u636e\u7a00\u7f3a\u4e14\u90e8\u5206\u8131\u94a9\u5e76\u975e\u7b56\u7565\u5931\u8d25\u5bfc\u81f4\uff0c\u9700\u6709\u6548\u5229\u7528\u8131\u94a9\u6570\u636e\u63d0\u5347\u7b56\u7565\u3002", "method": "\u4f7f\u7528OOD\u72b6\u6001\u4f30\u8ba1\u6a21\u578b\u8bc6\u522b\u8131\u94a9\u539f\u56e0\uff0c\u533a\u5206\u65e0\u6548\u8131\u94a9\u6848\u4f8b\uff0c\u5e76\u5728\u539f\u56e0\u589e\u5f3a\u7684\u60f3\u8c61\u73af\u5883\u4e2d\u66f4\u65b0\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u8bc6\u522b\u7b56\u7565\u76f8\u5173\u8131\u94a9\u539f\u56e0\uff0c\u63d0\u5347\u7b56\u7565\u6027\u80fd\uff0c\u907f\u514d\u8fc7\u5ea6\u4fdd\u5b88\u8c03\u6574\u3002", "conclusion": "DRARL\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5229\u7528\u8131\u94a9\u6570\u636e\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.16748", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.16748", "abs": "https://arxiv.org/abs/2506.16748", "authors": ["Arjo Chakravarty", "Michael X. Grey", "M. A. Viraj J. Muthugala", "Mohan Rajesh Elara"], "title": "A Scalable Post-Processing Pipeline for Large-Scale Free-Space Multi-Agent Path Planning with PiBT", "comment": null, "summary": "Free-space multi-agent path planning remains challenging at large scales.\nMost existing methods either offer optimality guarantees but do not scale\nbeyond a few dozen agents, or rely on grid-world assumptions that do not\ngeneralize well to continuous space. In this work, we propose a hybrid,\nrule-based planning framework that combines Priority Inheritance with\nBacktracking (PiBT) with a novel safety-aware path smoothing method. Our\napproach extends PiBT to 8-connected grids and selectively applies\nstring-pulling based smoothing while preserving collision safety through local\ninteraction awareness and a fallback collision resolution step based on Safe\nInterval Path Planning (SIPP). This design allows us to reduce overall path\nlengths while maintaining real-time performance. We demonstrate that our method\ncan scale to over 500 agents in large free-space environments, outperforming\nexisting any-angle and optimal methods in terms of runtime, while producing\nnear-optimal trajectories in sparse domains. Our results suggest this framework\nis a promising building block for scalable, real-time multi-agent navigation in\nrobotics systems operating beyond grid constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f18\u5148\u7ea7\u7ee7\u627f\u56de\u6eaf\uff08PiBT\uff09\u548c\u5b89\u5168\u611f\u77e5\u8def\u5f84\u5e73\u6ed1\u7684\u6df7\u5408\u89c4\u5212\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u81ea\u7531\u7a7a\u95f4\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u6269\u5c55\u5230\u51e0\u5341\u4e2a\u667a\u80fd\u4f53\u4ee5\u4e0a\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u7f51\u683c\u4e16\u754c\u5047\u8bbe\uff0c\u96be\u4ee5\u63a8\u5e7f\u5230\u8fde\u7eed\u7a7a\u95f4\u3002", "method": "\u6269\u5c55PiBT\u52308\u8fde\u901a\u7f51\u683c\uff0c\u7ed3\u5408\u5b89\u5168\u611f\u77e5\u8def\u5f84\u5e73\u6ed1\u548c\u5c40\u90e8\u4ea4\u4e92\u611f\u77e5\uff0c\u4f7f\u7528SIPP\u8fdb\u884c\u78b0\u649e\u89e3\u51b3\u3002", "result": "\u65b9\u6cd5\u53ef\u6269\u5c55\u5230500\u4e2a\u667a\u80fd\u4f53\uff0c\u8fd0\u884c\u65f6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8def\u5f84\u63a5\u8fd1\u6700\u4f18\u3002", "conclusion": "\u8be5\u6846\u67b6\u662f\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u53ef\u6269\u5c55\u3001\u5b9e\u65f6\u591a\u667a\u80fd\u4f53\u5bfc\u822a\u7684\u6709\u524d\u666f\u7684\u6784\u5efa\u6a21\u5757\u3002"}}
{"id": "2506.16822", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16822", "abs": "https://arxiv.org/abs/2506.16822", "authors": ["Daniel Frau-Alfaro", "Julio Casta\u00f1o-Amoros", "Santiago Puente", "Pablo Gil", "Roberto Calandra"], "title": "Learning Dexterous Object Handover", "comment": "Paper accepted for presentation in RoMan 2025", "summary": "Object handover is an important skill that we use daily when interacting with\nother humans. To deploy robots in collaborative setting, like houses, being\nable to receive and handing over objects safely and efficiently becomes a\ncrucial skill. In this work, we demonstrate the use of Reinforcement Learning\n(RL) for dexterous object handover between two multi-finger hands. Key to this\ntask is the use of a novel reward function based on dual quaternions to\nminimize the rotation distance, which outperforms other rotation\nrepresentations such as Euler and rotation matrices. The robustness of the\ntrained policy is experimentally evaluated by testing w.r.t. objects that are\nnot included in the training distribution, and perturbations during the\nhandover process. The results demonstrate that the trained policy successfully\nperform this task, achieving a total success rate of 94% in the best-case\nscenario after 100 experiments, thereby showing the robustness of our policy\nwith novel objects. In addition, the best-case performance of the policy\ndecreases by only 13.8% when the other robot moves during the handover, proving\nthat our policy is also robust to this type of perturbation, which is common in\nreal-world object handovers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u6307\u624b\u95f4\u7269\u4f53\u4ea4\u63a5\u65b9\u6cd5\uff0c\u4f7f\u7528\u53cc\u56db\u5143\u6570\u5956\u52b1\u51fd\u6570\u4f18\u5316\u65cb\u8f6c\u8ddd\u79bb\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u534f\u4f5c\u73af\u5883\u4e2d\uff08\u5982\u5bb6\u5ead\uff09\uff0c\u673a\u5668\u4eba\u9700\u8981\u5b89\u5168\u9ad8\u6548\u5730\u5b8c\u6210\u7269\u4f53\u4ea4\u63a5\u4efb\u52a1\uff0c\u8fd9\u662f\u65e5\u5e38\u4eba\u673a\u4ea4\u4e92\u7684\u91cd\u8981\u6280\u80fd\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u56db\u5143\u6570\u7684\u5956\u52b1\u51fd\u6570\uff0c\u7528\u4e8e\u6700\u5c0f\u5316\u65cb\u8f6c\u8ddd\u79bb\uff0c\u4f18\u4e8e\u6b27\u62c9\u89d2\u548c\u65cb\u8f6c\u77e9\u9635\u7b49\u8868\u793a\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8bad\u7ec3\u7b56\u7565\u5728\u672a\u89c1\u8fc7\u7269\u4f53\u548c\u4ea4\u63a5\u6270\u52a8\u60c5\u51b5\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u6700\u4f73\u573a\u666f\u6210\u529f\u7387\u8fbe94%\uff0c\u6270\u52a8\u4e0b\u6027\u80fd\u4ec5\u4e0b\u964d13.8%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7269\u4f53\u4ea4\u63a5\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.16892", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16892", "abs": "https://arxiv.org/abs/2506.16892", "authors": ["Partha Chowdhury", "Harsha M", "Ayush Gupta", "Sanat K Biswas"], "title": "Orbital Collision: An Indigenously Developed Web-based Space Situational Awareness Platform", "comment": "This work has been already submitted for STEP-IPSC 2025 Conference\n  Proceedings", "summary": "This work presents an indigenous web based platform Orbital Collision (OrCo),\ncreated by the Space Systems Laboratory at IIIT Delhi, to enhance Space\nSituational Awareness (SSA) by predicting collision probabilities of space\nobjects using Two Line Elements (TLE) data. The work highlights the growing\nchallenges of congestion in the Earth's orbital environment, mainly due to\nspace debris and defunct satellites, which increase collision risks. It employs\nseveral methods for propagating orbital uncertainty and calculating the\ncollision probability. The performance of the platform is evaluated through\naccuracy assessments and efficiency metrics, in order to improve the tracking\nof space objects and ensure the safety of the satellite in congested space.", "AI": {"tldr": "Orbital Collision (OrCo) \u662f\u4e00\u4e2a\u57fa\u4e8e\u7f51\u7edc\u7684\u5e73\u53f0\uff0c\u7528\u4e8e\u901a\u8fc7 TLE \u6570\u636e\u9884\u6d4b\u7a7a\u95f4\u7269\u4f53\u7684\u78b0\u649e\u6982\u7387\uff0c\u4ee5\u589e\u5f3a\u7a7a\u95f4\u6001\u52bf\u611f\u77e5\u3002", "motivation": "\u5730\u7403\u8f68\u9053\u73af\u5883\u65e5\u76ca\u62e5\u6324\uff0c\u4e3b\u8981\u7531\u7a7a\u95f4\u788e\u7247\u548c\u5931\u6548\u536b\u661f\u5f15\u8d77\uff0c\u589e\u52a0\u4e86\u78b0\u649e\u98ce\u9669\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u65b9\u6cd5\u4f20\u64ad\u8f68\u9053\u4e0d\u786e\u5b9a\u6027\u5e76\u8ba1\u7b97\u78b0\u649e\u6982\u7387\u3002", "result": "\u901a\u8fc7\u51c6\u786e\u6027\u548c\u6548\u7387\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5e73\u53f0\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u5e73\u53f0\u6709\u52a9\u4e8e\u6539\u5584\u7a7a\u95f4\u7269\u4f53\u8ddf\u8e2a\uff0c\u786e\u4fdd\u536b\u661f\u5728\u62e5\u6324\u7a7a\u95f4\u4e2d\u7684\u5b89\u5168\u3002"}}
{"id": "2506.16936", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16936", "abs": "https://arxiv.org/abs/2506.16936", "authors": ["Shengpeng Wang", "Xin Luo", "Yulong Xie", "Wei Wang"], "title": "SDDiff: Boost Radar Perception via Spatial-Doppler Diffusion", "comment": null, "summary": "Point cloud extraction (PCE) and ego velocity estimation (EVE) are key\ncapabilities gaining attention in 3D radar perception. However, existing work\ntypically treats these two tasks independently, which may neglect the interplay\nbetween radar's spatial and Doppler domain features, potentially introducing\nadditional bias. In this paper, we observe an underlying correlation between 3D\npoints and ego velocity, which offers reciprocal benefits for PCE and EVE. To\nfully unlock such inspiring potential, we take the first step to design a\nSpatial-Doppler Diffusion (SDDiff) model for simultaneously dense PCE and\naccurate EVE. To seamlessly tailor it to radar perception, SDDiff improves the\nconventional latent diffusion process in three major aspects. First, we\nintroduce a representation that embodies both spatial occupancy and Doppler\nfeatures. Second, we design a directional diffusion with radar priors to\nstreamline the sampling. Third, we propose Iterative Doppler Refinement to\nenhance the model's adaptability to density variations and ghosting effects.\nExtensive evaluations show that SDDiff significantly outperforms\nstate-of-the-art baselines by achieving 59% higher in EVE accuracy, 4X greater\nin valid generation density while boosting PCE effectiveness and reliability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSDDiff\u7684\u6a21\u578b\uff0c\u9996\u6b21\u5c06\u70b9\u4e91\u63d0\u53d6\uff08PCE\uff09\u548c\u81ea\u8f66\u901f\u5ea6\u4f30\u8ba1\uff08EVE\uff09\u4efb\u52a1\u7ed3\u5408\uff0c\u5229\u7528\u7a7a\u95f4-\u591a\u666e\u52d2\u6269\u6563\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u72ec\u7acb\u5904\u7406PCE\u548cEVE\uff0c\u5ffd\u7565\u4e86\u96f7\u8fbe\u7a7a\u95f4\u4e0e\u591a\u666e\u52d2\u57df\u7279\u5f81\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u53ef\u80fd\u5f15\u5165\u504f\u5dee\u3002\u8bba\u6587\u53d1\u73b0\u4e24\u8005\u5b58\u5728\u6f5c\u5728\u5173\u8054\uff0c\u53ef\u76f8\u4e92\u4fc3\u8fdb\u3002", "method": "\u8bbe\u8ba1\u4e86SDDiff\u6a21\u578b\uff0c\u6539\u8fdb\u4f20\u7edf\u6f5c\u5728\u6269\u6563\u8fc7\u7a0b\uff1a1\uff09\u5f15\u5165\u5305\u542b\u7a7a\u95f4\u5360\u7528\u548c\u591a\u666e\u52d2\u7279\u5f81\u7684\u8868\u793a\uff1b2\uff09\u57fa\u4e8e\u96f7\u8fbe\u5148\u9a8c\u8bbe\u8ba1\u5b9a\u5411\u6269\u6563\uff1b3\uff09\u63d0\u51fa\u8fed\u4ee3\u591a\u666e\u52d2\u7ec6\u5316\u4ee5\u589e\u5f3a\u9002\u5e94\u6027\u3002", "result": "SDDiff\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0cEVE\u51c6\u786e\u7387\u63d0\u534759%\uff0c\u6709\u6548\u751f\u6210\u5bc6\u5ea6\u63d0\u9ad84\u500d\uff0c\u540c\u65f6\u589e\u5f3aPCE\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "SDDiff\u901a\u8fc7\u7ed3\u5408PCE\u548cEVE\u4efb\u52a1\uff0c\u5229\u7528\u7a7a\u95f4-\u591a\u666e\u52d2\u6269\u6563\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f7\u8fbe\u611f\u77e5\u6027\u80fd\u3002"}}
{"id": "2506.16986", "categories": ["cs.RO", "68T40, 93C85, 70E60", "I.2.9; I.2.10; I.2.8"], "pdf": "https://arxiv.org/pdf/2506.16986", "abs": "https://arxiv.org/abs/2506.16986", "authors": ["Yuntao Ma", "Yang Liu", "Kaixian Qu", "Marco Hutter"], "title": "Learning Accurate Whole-body Throwing with High-frequency Residual Policy and Pullback Tube Acceleration", "comment": "8 pages, IROS 2025", "summary": "Throwing is a fundamental skill that enables robots to manipulate objects in\nways that extend beyond the reach of their arms. We present a control framework\nthat combines learning and model-based control for prehensile whole-body\nthrowing with legged mobile manipulators. Our framework consists of three\ncomponents: a nominal tracking policy for the end-effector, a high-frequency\nresidual policy to enhance tracking accuracy, and an optimization-based module\nto improve end-effector acceleration control. The proposed controller achieved\nthe average of 0.28 m landing error when throwing at targets located 6 m away.\nFurthermore, in a comparative study with university students, the system\nachieved a velocity tracking error of 0.398 m/s and a success rate of 56.8%,\nhitting small targets randomly placed at distances of 3-5 m while throwing at a\nspecified speed of 6 m/s. In contrast, humans have a success rate of only\n15.2%. This work provides an early demonstration of prehensile throwing with\nquantified accuracy on hardware, contributing to progress in dynamic whole-body\nmanipulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b66\u4e60\u548c\u6a21\u578b\u63a7\u5236\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u817f\u5f0f\u79fb\u52a8\u673a\u68b0\u81c2\u7684\u6293\u63e1\u6295\u63b7\uff0c\u5c55\u793a\u4e86\u8f83\u9ad8\u7684\u6295\u63b7\u7cbe\u5ea6\u548c\u6210\u529f\u7387\u3002", "motivation": "\u6295\u63b7\u662f\u673a\u5668\u4eba\u6269\u5c55\u64cd\u4f5c\u8303\u56f4\u7684\u57fa\u672c\u6280\u80fd\uff0c\u4f46\u73b0\u6709\u6280\u672f\u96be\u4ee5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u52a8\u6001\u5168\u8eab\u64cd\u4f5c\u3002", "method": "\u6846\u67b6\u5305\u62ec\u672b\u7aef\u6267\u884c\u5668\u7684\u540d\u4e49\u8ddf\u8e2a\u7b56\u7565\u3001\u9ad8\u9891\u6b8b\u5dee\u7b56\u7565\u548c\u57fa\u4e8e\u4f18\u5316\u7684\u52a0\u901f\u5ea6\u63a7\u5236\u6a21\u5757\u3002", "result": "\u6295\u63b76\u7c73\u8fdc\u76ee\u6807\u65f6\u5e73\u5747\u7740\u9646\u8bef\u5dee0.28\u7c73\uff1b\u4e0e\u5927\u5b66\u751f\u5bf9\u6bd4\u4e2d\uff0c\u7cfb\u7edf\u6210\u529f\u738756.8%\uff0c\u4eba\u7c7b\u4ec515.2%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e86\u91cf\u5316\u7cbe\u5ea6\u7684\u6293\u63e1\u6295\u63b7\uff0c\u63a8\u52a8\u4e86\u52a8\u6001\u5168\u8eab\u64cd\u4f5c\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.17110", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17110", "abs": "https://arxiv.org/abs/2506.17110", "authors": ["Teng Guo", "Baichuan Huang", "Jingjin Yu"], "title": "Monocular One-Shot Metric-Depth Alignment for RGB-Based Robot Grasping", "comment": "Accepted to IROS 2025", "summary": "Accurate 6D object pose estimation is a prerequisite for successfully\ncompleting robotic prehensile and non-prehensile manipulation tasks. At\npresent, 6D pose estimation for robotic manipulation generally relies on depth\nsensors based on, e.g., structured light, time-of-flight, and stereo-vision,\nwhich can be expensive, produce noisy output (as compared with RGB cameras),\nand fail to handle transparent objects. On the other hand, state-of-the-art\nmonocular depth estimation models (MDEMs) provide only affine-invariant depths\nup to an unknown scale and shift. Metric MDEMs achieve some successful\nzero-shot results on public datasets, but fail to generalize. We propose a\nnovel framework, Monocular One-shot Metric-depth Alignment (MOMA), to recover\nmetric depth from a single RGB image, through a one-shot adaptation building on\nMDEM techniques. MOMA performs scale-rotation-shift alignments during camera\ncalibration, guided by sparse ground-truth depth points, enabling accurate\ndepth estimation without additional data collection or model retraining on the\ntesting setup. MOMA supports fine-tuning the MDEM on transparent objects,\ndemonstrating strong generalization capabilities. Real-world experiments on\ntabletop 2-finger grasping and suction-based bin-picking applications show MOMA\nachieves high success rates in diverse tasks, confirming its effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMOMA\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u5f20RGB\u56fe\u50cf\u6062\u590d\u5ea6\u91cf\u6df1\u5ea6\uff0c\u9002\u7528\u4e8e6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u5c24\u5176\u5728\u900f\u660e\u7269\u4f53\u5904\u7406\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d6D\u59ff\u6001\u4f30\u8ba1\u4f9d\u8d56\u6602\u8d35\u7684\u6df1\u5ea6\u4f20\u611f\u5668\uff0c\u4e14\u5bf9\u900f\u660e\u7269\u4f53\u6548\u679c\u4e0d\u4f73\uff1b\u73b0\u6709\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\uff08MDEMs\uff09\u65e0\u6cd5\u6cdb\u5316\u3002", "method": "MOMA\u901a\u8fc7\u76f8\u673a\u6821\u51c6\u4e2d\u7684\u5c3a\u5ea6-\u65cb\u8f6c-\u5e73\u79fb\u5bf9\u9f50\uff0c\u5229\u7528\u7a00\u758f\u771f\u5b9e\u6df1\u5ea6\u70b9\u6307\u5bfc\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6536\u96c6\u6216\u6a21\u578b\u91cd\u8bad\u7ec3\u3002", "result": "\u5728\u684c\u9762\u6293\u53d6\u548c\u5438\u76d8\u5f0f\u5206\u62e3\u4efb\u52a1\u4e2d\uff0cMOMA\u8868\u73b0\u51fa\u9ad8\u6210\u529f\u7387\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "MOMA\u4e3a6D\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u900f\u660e\u7269\u4f53\u3002"}}
{"id": "2506.17184", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.17184", "abs": "https://arxiv.org/abs/2506.17184", "authors": ["Albert H. Li", "Brandon Hung", "Aaron D. Ames", "Jiuguang Wang", "Simon Le Cleac'h", "Preston Culbertson"], "title": "Judo: A User-Friendly Open-Source Package for Sampling-Based Model Predictive Control", "comment": "Accepted at the 2025 RSS Workshop on Fast Motion Planning and Control\n  in the Era of Parallelism. 5 Pages", "summary": "Recent advancements in parallel simulation and successful robotic\napplications are spurring a resurgence in sampling-based model predictive\ncontrol. To build on this progress, however, the robotics community needs\ncommon tooling for prototyping, evaluating, and deploying sampling-based\ncontrollers. We introduce Judo, a software package designed to address this\nneed. To facilitate rapid prototyping and evaluation, Judo provides robust\nimplementations of common sampling-based MPC algorithms and standardized\nbenchmark tasks. It further emphasizes usability with simple but extensible\ninterfaces for controller and task definitions, asynchronous execution for\nstraightforward simulation-to-hardware transfer, and a highly customizable\ninteractive GUI for tuning controllers interactively. While written in Python,\nthe software leverages MuJoCo as its physics backend to achieve real-time\nperformance, which we validate across both consumer and server-grade hardware.\nCode at https://github.com/bdaiinstitute/judo.", "AI": {"tldr": "Judo\u662f\u4e00\u4e2a\u7528\u4e8e\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u548c\u8bc4\u4f30\u91c7\u6837\u57fa\u4e8eMPC\u7684\u8f6f\u4ef6\u5305\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u7b97\u6cd5\u3001\u4efb\u52a1\u548c\u4ea4\u4e92\u5f0fGUI\uff0c\u652f\u6301\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u673a\u5668\u4eba\u793e\u533a\u9700\u8981\u901a\u7528\u5de5\u5177\u6765\u652f\u6301\u91c7\u6837\u57fa\u4e8eMPC\u7684\u539f\u578b\u8bbe\u8ba1\u3001\u8bc4\u4f30\u548c\u90e8\u7f72\u3002", "method": "Judo\u63d0\u4f9b\u5e38\u89c1\u91c7\u6837\u57fa\u4e8eMPC\u7b97\u6cd5\u7684\u5b9e\u73b0\u3001\u6807\u51c6\u5316\u4efb\u52a1\u3001\u7b80\u5355\u53ef\u6269\u5c55\u7684\u63a5\u53e3\u3001\u5f02\u6b65\u6267\u884c\u548c\u4ea4\u4e92\u5f0fGUI\u3002", "result": "Judo\u5728\u6d88\u8d39\u7ea7\u548c\u670d\u52a1\u5668\u7ea7\u786c\u4ef6\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "Judo\u4e3a\u91c7\u6837\u57fa\u4e8eMPC\u7684\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u6613\u7528\u7684\u5de5\u5177\u3002"}}
{"id": "2506.17198", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17198", "abs": "https://arxiv.org/abs/2506.17198", "authors": ["Jianglong Ye", "Keyi Wang", "Chengjing Yuan", "Ruihan Yang", "Yiquan Li", "Jiyue Zhu", "Yuzhe Qin", "Xueyan Zou", "Xiaolong Wang"], "title": "Dex1B: Learning with 1B Demonstrations for Dexterous Manipulation", "comment": "Accepted to RSS 2025. Project page: https://jianglongye.com/dex1b", "summary": "Generating large-scale demonstrations for dexterous hand manipulation remains\nchallenging, and several approaches have been proposed in recent years to\naddress this. Among them, generative models have emerged as a promising\nparadigm, enabling the efficient creation of diverse and physically plausible\ndemonstrations. In this paper, we introduce Dex1B, a large-scale, diverse, and\nhigh-quality demonstration dataset produced with generative models. The dataset\ncontains one billion demonstrations for two fundamental tasks: grasping and\narticulation. To construct it, we propose a generative model that integrates\ngeometric constraints to improve feasibility and applies additional conditions\nto enhance diversity. We validate the model on both established and newly\nintroduced simulation benchmarks, where it significantly outperforms prior\nstate-of-the-art methods. Furthermore, we demonstrate its effectiveness and\nrobustness through real-world robot experiments. Our project page is at\nhttps://jianglongye.com/dex1b", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Dex1B\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u751f\u6210\u6a21\u578b\u521b\u5efa\u4e86\u5927\u89c4\u6a21\u3001\u591a\u6837\u4e14\u9ad8\u8d28\u91cf\u7684\u624b\u90e8\u64cd\u4f5c\u6f14\u793a\u6570\u636e\uff0c\u5305\u542b10\u4ebf\u4e2a\u6f14\u793a\uff0c\u5e76\u5728\u4eff\u771f\u548c\u5b9e\u9645\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u624b\u90e8\u64cd\u4f5c\u6f14\u793a\u6570\u636e\u751f\u6210\u7684\u6311\u6218\uff0c\u751f\u6210\u591a\u6837\u4e14\u7269\u7406\u53ef\u884c\u7684\u6f14\u793a\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u751f\u6210\u6a21\u578b\uff0c\u7ed3\u5408\u51e0\u4f55\u7ea6\u675f\u63d0\u5347\u53ef\u884c\u6027\uff0c\u5e76\u901a\u8fc7\u9644\u52a0\u6761\u4ef6\u589e\u5f3a\u591a\u6837\u6027\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "Dex1B\u6570\u636e\u96c6\u4e3a\u624b\u90e8\u64cd\u4f5c\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u8d44\u6e90\uff0c\u751f\u6210\u6a21\u578b\u65b9\u6cd5\u5177\u6709\u5e7f\u6cdb\u6f5c\u529b\u3002"}}
