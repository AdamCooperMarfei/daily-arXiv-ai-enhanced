<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 35]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Learning User Interaction Forces using Vision for a Soft Finger Exosuit](https://arxiv.org/abs/2508.02870)
*Mohamed Irfan Refai,Abdulaziz Y. Alkayas,Anup Teejo Mathew,Federico Renda,Thomas George Thuruthel*

Main category: cs.RO

TL;DR: 提出了一种基于图像和学习的框架，用于估计手指外骨骼系统的分布式接触力，并验证了其在闭环控制中的有效性。


<details>
  <summary>Details</summary>
Motivation: 软性可穿戴辅助设备的非线性与柔顺特性使得物理建模和嵌入式传感具有挑战性，需要一种非侵入式的方法实时估计接触力。

Method: 使用SoRoSim工具箱生成多样化的外骨骼几何形状和驱动场景数据集，通过低分辨率灰度图像训练学习模型。

Result: 模型能够准确估计多接触点的相互作用力，泛化能力强，对视觉噪声和对比度变化具有鲁棒性。

Conclusion: 该视觉估计器可作为闭环控制的替代力传感器，为非侵入式实时力估计提供了一种可行方案。

Abstract: Wearable assistive devices are increasingly becoming softer. Modelling their
interface with human tissue is necessary to capture transmission of dynamic
assistance. However, their nonlinear and compliant nature makes both physical
modeling and embedded sensing challenging. In this paper, we develop a
image-based, learning-based framework to estimate distributed contact forces
for a finger-exosuit system. We used the SoRoSim toolbox to generate a diverse
dataset of exosuit geometries and actuation scenarios for training. The method
accurately estimated interaction forces across multiple contact locations from
low-resolution grayscale images, was able to generalize to unseen shapes and
actuation levels, and remained robust under visual noise and contrast
variations. We integrated the model into a feedback controller, and found that
the vision-based estimator functions as a surrogate force sensor for
closed-loop control. This approach could be used as a non-intrusive alternative
for real-time force estimation for exosuits.

</details>


### [2] [Tunable Leg Stiffness in a Monopedal Hopper for Energy-Efficient Vertical Hopping Across Varying Ground Profiles](https://arxiv.org/abs/2508.02873)
*Rongqian Chen,Jun Kwon,Kefan Wu,Wei-Hsi Chen*

Main category: cs.RO

TL;DR: HASTA是一种具有实时可调腿部刚度的垂直跳跃机器人，旨在通过调整刚度优化不同地面条件下的能量效率。


<details>
  <summary>Details</summary>
Motivation: 研究目标是最大化跳跃高度以实现能量高效的运动，假设腿部刚度应根据地面刚度和阻尼条件动态调整。

Method: 通过实验和模拟，测试不同腿部刚度在各种地面条件下的表现，以找到最佳刚度组合。

Result: 实验支持假设，可调刚度能显著提高能量效率，模拟结果为未来控制器设计提供参考。

Conclusion: 可调刚度是优化跳跃机器人能量效率的有效方法，未来可进一步开发智能控制器。

Abstract: We present the design and implementation of HASTA (Hopper with Adjustable
Stiffness for Terrain Adaptation), a vertical hopping robot with real-time
tunable leg stiffness, aimed at optimizing energy efficiency across various
ground profiles (a pair of ground stiffness and damping conditions). By
adjusting leg stiffness, we aim to maximize apex hopping height, a key metric
for energy-efficient vertical hopping. We hypothesize that softer legs perform
better on soft, damped ground by minimizing penetration and energy loss, while
stiffer legs excel on hard, less damped ground by reducing limb deformation and
energy dissipation. Through experimental tests and simulations, we find the
best leg stiffness within our selection for each combination of ground
stiffness and damping, enabling the robot to achieve maximum steady-state
hopping height with a constant energy input. These results support our
hypothesis that tunable stiffness improves energy-efficient locomotion in
controlled experimental conditions. In addition, the simulation provides
insights that could aid in the future development of controllers for selecting
leg stiffness.

</details>


### [3] [Co-designing Zoomorphic Robot Concepts for Animal Welfare Education](https://arxiv.org/abs/2508.02898)
*Isobel Voysey,Lynne Baillie,Joanne Williams,Michael Herrmann*

Main category: cs.RO

TL;DR: 动物福利教育可通过定制机器人帮助儿童学习动物行为，促进安全互动。研究通过参与式设计工作坊，确定了机器人外观、行为和功能的关键需求，并提出了叙事概念。


<details>
  <summary>Details</summary>
Motivation: 利用机器人促进儿童对动物的理解和安全互动，提升动物福利教育效果。

Method: 通过参与式设计工作坊，结合动物福利教育者和儿童的意见，设计机器人的外观、行为和功能，并探索叙事概念。

Result: 发现机器人需具备负面行为反馈、面部和尾部状态提示、自然外观和毛发质感。同时提出新颖的参与式设计活动。

Conclusion: 研究为动物福利教育机器人的设计提供了关键需求和方法，并解决了教育者与儿童间共识的挑战。

Abstract: Animal welfare education could greatly benefit from customized robots to help
children learn about animals and their behavior, and thereby promote positive,
safe child-animal interactions. To this end, we ran Participatory Design
workshops with animal welfare educators and children to identify key
requirements for zoomorphic robots from their perspectives. Our findings
encompass a zoomorphic robot's appearance, behavior, and features, as well as
concepts for a narrative surrounding the robot. Through comparing and
contrasting the two groups, we find the importance of: negative reactions to
undesirable behavior from children; using the facial features and tail to
provide cues signaling an animal's internal state; and a natural, furry
appearance and texture. We also contribute some novel activities for
Participatory Design with children, including branching storyboards inspired by
thematic apperception tests and interactive narratives, and reflect on some of
the key design challenges of achieving consensus between the groups, despite
much overlap in their design concepts.

</details>


### [4] [Context-aware Risk Assessment and Its Application in Autonomous Driving](https://arxiv.org/abs/2508.02919)
*Boyang Tian,Weisong Shi*

Main category: cs.RO

TL;DR: 论文提出了一种轻量级模块化框架CRI，通过方向感知的空间分区和动态安全包络，实时量化风险并调整控制指令，显著提升了自动驾驶的安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有风险评估方法要么输出粗粒度的全局指标，缺乏可解释性，要么未具体集成到自动驾驶系统中，或仅针对特定场景。CRI旨在解决这些问题，提供实时、精确的风险评估。

Method: CRI采用方向感知的空间分区和动态安全包络，结合RSS原则、混合概率-最大融合策略和自适应控制策略，实时调整行为。

Result: 在Bench2Drive基准测试中，CRI显著降低了碰撞率（19%），提升了驾驶评分（17%），且运行时开销极低（3.6毫秒）。

Conclusion: CRI在复杂高风险环境中显著提升了安全性和鲁棒性，同时保持了模块化和低运行时开销。

Abstract: Ensuring safety in autonomous driving requires precise, real-time risk
assessment and adaptive behavior. Prior work on risk estimation either outputs
coarse, global scene-level metrics lacking interpretability, proposes
indicators without concrete integration into autonomous systems, or focuses
narrowly on specific driving scenarios. We introduce the Context-aware Risk
Index (CRI), a light-weight modular framework that quantifies directional risks
based on object kinematics and spatial relationships, dynamically adjusting
control commands in real time. CRI employs direction-aware spatial partitioning
within a dynamic safety envelope using Responsibility-Sensitive Safety (RSS)
principles, a hybrid probabilistic-max fusion strategy for risk aggregation,
and an adaptive control policy for real-time behavior modulation. We evaluate
CRI on the Bench2Drive benchmark comprising 220 safety-critical scenarios using
a state-of-the-art end-to-end model Transfuser++ on challenging routes. Our
collision-rate metrics show a 19\% reduction (p = 0.003) in vehicle collisions
per failed route, a 20\% reduction (p = 0.004) in collisions per kilometer, a
17\% increase (p = 0.016) in composed driving score, and a statistically
significant reduction in penalty scores (p = 0.013) with very low overhead (3.6
ms per decision cycle). These results demonstrate that CRI substantially
improves safety and robustness in complex, risk-intensive environments while
maintaining modularity and low runtime overhead.

</details>


### [5] [Model-agnostic Meta-learning for Adaptive Gait Phase and Terrain Geometry Estimation with Wearable Soft Sensors](https://arxiv.org/abs/2508.02930)
*Zenan Zhu,Wenxi Chen,Pei-Chun Kao,Janelle Clark,Lily Behnke,Rebecca Kramer-Bottiglio,Holly Yanco,Yan Gu*

Main category: cs.RO

TL;DR: 提出了一种基于MAML的框架，用于通过少量织物传感器准确估计步态和地形，适应性强且泛化能力高。


<details>
  <summary>Details</summary>
Motivation: 解决织物传感器因非线性特性（如滞后、放置误差）和有限校准数据带来的挑战，同时适应不同用户和地形。

Method: 将MAML集成到深度学习架构中，学习通用模型初始化，实现高效适应和强泛化。

Result: 在九名参与者的实验中，框架在步态、运动模式和倾斜角估计上优于基线方法。

Conclusion: 该框架在准确性和适应性上表现优异，适用于实际部署。

Abstract: This letter presents a model-agnostic meta-learning (MAML) based framework
for simultaneous and accurate estimation of human gait phase and terrain
geometry using a small set of fabric-based wearable soft sensors, with
efficient adaptation to unseen subjects and strong generalization across
different subjects and terrains. Compared to rigid alternatives such as
inertial measurement units, fabric-based soft sensors improve comfort but
introduce nonlinearities due to hysteresis, placement error, and fabric
deformation. Moreover, inter-subject and inter-terrain variability, coupled
with limited calibration data in real-world deployments, further complicate
accurate estimation. To address these challenges, the proposed framework
integrates MAML into a deep learning architecture to learn a generalizable
model initialization that captures subject- and terrain-invariant structure.
This initialization enables efficient adaptation (i.e., adaptation with only a
small amount of calibration data and a few fine-tuning steps) to new users,
while maintaining strong generalization (i.e., high estimation accuracy across
subjects and terrains). Experiments on nine participants walking at various
speeds over five terrain conditions demonstrate that the proposed framework
outperforms baseline approaches in estimating gait phase, locomotion mode, and
incline angle, with superior accuracy, adaptation efficiency, and
generalization.

</details>


### [6] [AeroSafe: Mobile Indoor Air Purification using Aerosol Residence Time Analysis and Robotic Cough Emulator Testbed](https://arxiv.org/abs/2508.02947)
*M Tanjid Hasan Tonmoy,Rahath Malladi,Kaustubh Singh,Forsad Al Hossain,Rajesh Gupta,Andrés E. Tejada-Martínez,Tauhidur Rahman*

Main category: cs.RO

TL;DR: AeroSafe通过机器人咳嗽模拟器和数字孪生技术提升室内空气净化效率，预测气溶胶浓度动态，减少病原体风险。


<details>
  <summary>Details</summary>
Motivation: 当前便携式空气过滤器忽视咳嗽产生的气溶胶浓度，尤其在医疗和公共场所存在高风险。

Method: 使用机器人双代理模拟器（模拟咳嗽和空气净化器响应）训练数字孪生模型，结合物理模型和LSTM网络。

Result: 模型预测气溶胶浓度动态，平均停留时间误差在35秒内，实时干预策略优于静态过滤器。

Conclusion: AeroSafe系统能有效减少空气传播病原体风险，适用于高风险环境。

Abstract: Indoor air quality plays an essential role in the safety and well-being of
occupants, especially in the context of airborne diseases. This paper
introduces AeroSafe, a novel approach aimed at enhancing the efficacy of indoor
air purification systems through a robotic cough emulator testbed and a
digital-twins-based aerosol residence time analysis. Current portable air
filters often overlook the concentrations of respiratory aerosols generated by
coughs, posing a risk, particularly in high-exposure environments like
healthcare facilities and public spaces. To address this gap, we present a
robotic dual-agent physical emulator comprising a maneuverable mannequin
simulating cough events and a portable air purifier autonomously responding to
aerosols. The generated data from this emulator trains a digital twins model,
combining a physics-based compartment model with a machine learning approach,
using Long Short-Term Memory (LSTM) networks and graph convolution layers.
Experimental results demonstrate the model's ability to predict aerosol
concentration dynamics with a mean residence time prediction error within 35
seconds. The proposed system's real-time intervention strategies outperform
static air filter placement, showcasing its potential in mitigating airborne
pathogen risks.

</details>


### [7] [A novel autonomous microplastics surveying robot for beach environments](https://arxiv.org/abs/2508.02952)
*Hassan Iqbal,Kobiny Rex,Joseph Shirley,Carlos Baiz,Christian Claudel*

Main category: cs.RO

TL;DR: 本文介绍了一种新型机器人平台，用于自动检测和分析海滩表面的微塑料，结合视觉和近红外光谱技术，实现了高精度的微塑料分类。


<details>
  <summary>Details</summary>
Motivation: 微塑料已成为普遍的环境污染物，但其检测和浓度映射仍是主要挑战。

Method: 采用移动机械臂系统，结合摄像头和近红外光谱传感器，实时检测和分析微塑料。

Result: 实验表明，系统在操控精度和微塑料分类准确性上表现优异。

Conclusion: 该机器人平台为解决微塑料污染问题提供了高效的技术支持。

Abstract: Microplastics, defined as plastic particles smaller than 5 millimeters, have
become a pervasive environmental contaminant that accumulates on beaches due to
wind patterns and tidal forcing. Detecting microplastics and mapping their
concentration in the wild remains one of the primary challenges in addressing
this environmental issue. This paper introduces a novel robotic platform that
automatically detects and chemically analyzes microplastics on beach surfaces.
This mobile manipulator system scans areas for microplastics using a camera
mounted on the robotic arm's end effector. The system effectively segments
candidate microplastic particles on sand surfaces even in the presence of
organic matter such as leaves and clams. Once a candidate microplastic particle
is detected, the system steers a near-infrared (NIR) spectroscopic sensor onto
the particle using both NIR and visual feedback to chemically analyze it in
real-time. Through experiments in lab and beach environments, the system is
shown to achieve an excellent positional precision in manipulation control and
high microplastic classification accuracy.

</details>


### [8] [Optimal Trajectory Planning in a Vertically Undulating Snake Locomotion using Contact-implicit Optimization](https://arxiv.org/abs/2508.02953)
*Adarsh Salagame,Eric Sihite,Alireza Ramezani*

Main category: cs.RO

TL;DR: 论文提出了一种基于简化动力学模型的蛇形机器人运动控制方法，填补了现有研究的空白，并通过仿真和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有蛇形机器人运动控制研究要么忽略复杂交互，要么专注于复杂交互（如挖掘运动），缺乏基于简单刚性动力学的中间范式模型。

Method: 引入基于Moreau微分包含数学的降阶模型，验证模型准确性，并进行实验验证。

Result: 通过仿真和实验验证了模型的准确性和有效性。

Conclusion: 该研究为蛇形机器人运动控制提供了一种新的简化动力学模型，填补了研究空白。

Abstract: Contact-rich problems, such as snake robot locomotion, offer unexplored yet
rich opportunities for optimization-based trajectory and acyclic contact
planning. So far, a substantial body of control research has focused on
emulating snake locomotion and replicating its distinctive movement patterns
using shape functions that either ignore the complexity of interactions or
focus on complex interactions with matter (e.g., burrowing movements). However,
models and control frameworks that lie in between these two paradigms and are
based on simple, fundamental rigid body dynamics, which alleviate the
challenging contact and control allocation problems in snake locomotion, remain
absent. This work makes meaningful contributions, substantiated by simulations
and experiments, in the following directions: 1) introducing a reduced-order
model based on Moreau's stepping-forward approach from differential inclusion
mathematics, 2) verifying model accuracy, 3) experimental validation.

</details>


### [9] [Robot builds a robot's brain: AI generated drone command and control station hosted in the sky](https://arxiv.org/abs/2508.02962)
*Peter Burke*

Main category: cs.RO

TL;DR: AI生成无人机控制系统，完全由AI编写代码，实现实时控制、任务规划和执行，性能优于人工编写代码，但存在模型上下文窗口和推理深度的限制。


<details>
  <summary>Details</summary>
Motivation: 探索AI在自主机器人设计和开发中的潜力，特别是通过AI生成代码实现无人机控制系统的可能性。

Method: 使用AI模型生成无人机控制系统的全部代码，包括实时映射、飞行遥测、任务规划和安全协议，并通过云端和实际无人机进行测试。

Result: AI生成的代码在开发速度和功能完整性上优于人工编写代码，但存在模型限制。

Conclusion: AI驱动的机器人控制系统开发具有潜力，未来可能成为机器人工程的新范式。

Abstract: Advances in artificial intelligence (AI) including large language models
(LLMs) and hybrid reasoning models present an opportunity to reimagine how
autonomous robots such as drones are designed, developed, and validated. Here,
we demonstrate a fully AI-generated drone control system: with minimal human
input, an artificial intelligence (AI) model authored all the code for a
real-time, self-hosted drone command and control platform, which was deployed
and demonstrated on a real drone in flight as well as a simulated virtual drone
in the cloud. The system enables real-time mapping, flight telemetry,
autonomous mission planning and execution, and safety protocolsall orchestrated
through a web interface hosted directly on the drone itself. Not a single line
of code was written by a human. We quantitatively benchmark system performance,
code complexity, and development speed against prior, human-coded
architectures, finding that AI-generated code can deliver functionally complete
command-and-control stacks at orders-of-magnitude faster development cycles,
though with identifiable current limitations related to specific model context
window and reasoning depth. Our analysis uncovers the practical boundaries of
AI-driven robot control code generation at current model scales, as well as
emergent strengths and failure modes in AI-generated robotics code. This work
sets a precedent for the autonomous creation of robot control systems and, more
broadly, suggests a new paradigm for robotics engineeringone in which future
robots may be largely co-designed, developed, and verified by artificial
intelligence. In this initial work, a robot built a robot's brain.

</details>


### [10] [Physics-informed Neural Time Fields for Prehensile Object Manipulation](https://arxiv.org/abs/2508.02976)
*Hanwen Ren,Ruiqi Ni,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: 提出了一种新型多模态物理信息神经网络（PINN），用于高效解决物体操纵任务，无需专家数据，并在复杂环境中快速生成轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有物体操纵方法效率低、依赖专家演示或试错学习，不适用于实际场景。

Method: 使用多模态物理信息神经网络（PINN）学习Eikonal方程，并在操纵过程中动态调整抓取策略。

Result: 在仿真和真实场景中验证，相比基线方法，训练高效、规划时间短、轨迹优、成功率高。

Conclusion: 该方法在多种物体上表现优异，适用于复杂环境，具有实际应用潜力。

Abstract: Object manipulation skills are necessary for robots operating in various
daily-life scenarios, ranging from warehouses to hospitals. They allow the
robots to manipulate the given object to their desired arrangement in the
cluttered environment. The existing approaches to solving object manipulations
are either inefficient sampling based techniques, require expert
demonstrations, or learn by trial and error, making them less ideal for
practical scenarios. In this paper, we propose a novel, multimodal
physics-informed neural network (PINN) for solving object manipulation tasks.
Our approach efficiently learns to solve the Eikonal equation without expert
data and finds object manipulation trajectories fast in complex, cluttered
environments. Our method is multimodal as it also reactively replans the
robot's grasps during manipulation to achieve the desired object poses. We
demonstrate our approach in both simulation and real-world scenarios and
compare it against state-of-the-art baseline methods. The results indicate that
our approach is effective across various objects, has efficient training
compared to previous learning-based methods, and demonstrates high performance
in planning time, trajectory length, and success rates. Our demonstration
videos can be found at https://youtu.be/FaQLkTV9knI.

</details>


### [11] [Multimodal Human-Intent Modeling for Contextual Robot-to-Human Handovers of Arbitrary Objects](https://arxiv.org/abs/2508.02982)
*Lucas Chen,Guna Avula,Hanwen Ren,Zixing Wang,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: 论文提出了一种统一方法，通过结合人类语言和非语言指令选择目标物体，并考虑人类偏好生成机器人抓取和柔顺的交接动作序列。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预选目标物体且未考虑人类偏好，限制了人机交互的自然性和流畅性。

Method: 结合人类语言和非语言指令选择目标物体，并基于人类偏好生成机器人抓取和交接动作序列。

Result: 通过真实实验和用户研究验证了方法的有效性，能够理解人类偏好并完成任务。

Conclusion: 提出的框架在理解人类偏好和处理物体交接任务方面表现优异。

Abstract: Human-robot object handover is a crucial element for assistive robots that
aim to help people in their daily lives, including elderly care, hospitals, and
factory floors. The existing approaches to solving these tasks rely on
pre-selected target objects and do not contextualize human implicit and
explicit preferences for handover, limiting natural and smooth interaction
between humans and robots. These preferences can be related to the target
object selection from the cluttered environment and to the way the robot should
grasp the selected object to facilitate desirable human grasping during
handovers. Therefore, this paper presents a unified approach that selects
target distant objects using human verbal and non-verbal commands and performs
the handover operation by contextualizing human implicit and explicit
preferences to generate robot grasps and compliant handover motion sequences.
We evaluate our integrated framework and its components through real-world
experiments and user studies with arbitrary daily-life objects. The results of
these evaluations demonstrate the effectiveness of our proposed pipeline in
handling object handover tasks by understanding human preferences. Our
demonstration videos can be found at https://youtu.be/6z27B2INl-s.

</details>


### [12] [Estimation of Aerodynamics Forces in Dynamic Morphing Wing Flight](https://arxiv.org/abs/2508.02984)
*Bibek Gupta,Mintae Kim,Albert Park,Eric Sihite,Koushil Sreenath,Alireza Ramezani*

Main category: cs.RO

TL;DR: 论文研究了两种方法（基于物理的观测器和神经网络回归模型）用于估计仿生扑翼机器人Aerobat的气动力，目标是为闭环飞行控制提供支持。


<details>
  <summary>Details</summary>
Motivation: 准确估计气动力对扑翼机器人的控制、建模和设计至关重要，尤其是在动态变形能力方面。

Method: 1. 基于哈密顿力学的物理观测器，利用共轭动量概念推断外力；2. 多层感知机（MLP）神经网络回归模型，从关节运动学和环境参数映射到气动力。

Result: 两种方法在三个力分量（Fx, Fy, Fz）上表现出高度一致性。

Conclusion: 两种方法均能有效估计气动力，为扑翼机器人的闭环控制提供了可靠工具。

Abstract: Accurate estimation of aerodynamic forces is essential for advancing the
control, modeling, and design of flapping-wing aerial robots with dynamic
morphing capabilities. In this paper, we investigate two distinct methodologies
for force estimation on Aerobat, a bio-inspired flapping-wing platform designed
to emulate the inertial and aerodynamic behaviors observed in bat flight. Our
goal is to quantify aerodynamic force contributions during tethered flight, a
crucial step toward closed-loop flight control. The first method is a
physics-based observer derived from Hamiltonian mechanics that leverages the
concept of conjugate momentum to infer external aerodynamic forces acting on
the robot. This observer builds on the system's reduced-order dynamic model and
utilizes real-time sensor data to estimate forces without requiring training
data. The second method employs a neural network-based regression model,
specifically a multi-layer perceptron (MLP), to learn a mapping from joint
kinematics, flapping frequency, and environmental parameters to aerodynamic
force outputs. We evaluate both estimators using a 6-axis load cell in a
high-frequency data acquisition setup that enables fine-grained force
measurements during periodic wingbeats. The conjugate momentum observer and the
regression model demonstrate strong agreement across three force components
(Fx, Fy, Fz).

</details>


### [13] [GACL: Grounded Adaptive Curriculum Learning with Active Task and Performance Monitoring](https://arxiv.org/abs/2508.02988)
*Linji Wang,Zifan Xu,Peter Stone,Xuesu Xiao*

Main category: cs.RO

TL;DR: 提出了Grounded Adaptive Curriculum Learning (GACL)框架，用于机器人任务的自适应课程学习，通过任务表示、性能跟踪和交替采样提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前机器人任务的课程学习依赖人工设计，效率低且可能不理想；自动课程学习在简单任务中有效，但机器人任务复杂且目标分布部分已知。

Method: GACL框架包括：1) 复杂任务表示；2) 自适应性能跟踪；3) 交替采样保持目标相关性。

Result: 在轮式导航和四足机器人3D受限空间任务中，GACL分别比现有方法成功率提高6.8%和6.1%。

Conclusion: GACL为复杂机器人任务提供了一种高效的自适应课程学习方法，显著优于现有技术。

Abstract: Curriculum learning has emerged as a promising approach for training complex
robotics tasks, yet current applications predominantly rely on manually
designed curricula, which demand significant engineering effort and can suffer
from subjective and suboptimal human design choices. While automated curriculum
learning has shown success in simple domains like grid worlds and games where
task distributions can be easily specified, robotics tasks present unique
challenges: they require handling complex task spaces while maintaining
relevance to target domain distributions that are only partially known through
limited samples. To this end, we propose Grounded Adaptive Curriculum Learning,
a framework specifically designed for robotics curriculum learning with three
key innovations: (1) a task representation that consistently handles complex
robot task design, (2) an active performance tracking mechanism that allows
adaptive curriculum generation appropriate for the robot's current
capabilities, and (3) a grounding approach that maintains target domain
relevance through alternating sampling between reference and synthetic tasks.
We validate GACL on wheeled navigation in constrained environments and
quadruped locomotion in challenging 3D confined spaces, achieving 6.8% and 6.1%
higher success rates, respectively, than state-of-the-art methods in each
domain.

</details>


### [14] [Thruster-Enhanced Locomotion: A Decoupled Model Predictive Control with Learned Contact Residuals](https://arxiv.org/abs/2508.03003)
*Chenghao Wang,Alireza Ramezani*

Main category: cs.RO

TL;DR: Husky Carbon机器人通过解耦控制架构（Raibert型控制器和MPC结合学习残差动力学）实现了稳定的推力辅助行走，优于传统统一MPC框架。


<details>
  <summary>Details</summary>
Motivation: 探索结合姿态操纵和推力矢量控制的机器人平台，解决传统统一MPC框架因低扭矩控制带宽而受限的问题。

Method: 采用解耦控制架构：Raibert型控制器负责腿部运动（位置控制），MPC调节推力器并学习接触残差动力学（CRD）以处理腿-地面碰撞。

Result: 实验表明，结合CRD的解耦控制器在抗干扰恢复和猫式步态上表现更稳定。

Conclusion: 解耦控制架构结合CRD有效解决了扭矩控制带宽限制，提升了机器人动态行为的稳定性。

Abstract: Husky Carbon, a robot developed by Northeastern University, serves as a
research platform to explore unification of posture manipulation and thrust
vectoring. Unlike conventional quadrupeds, its joint actuators and thrusters
enable enhanced control authority, facilitating thruster-assisted narrow-path
walking. While a unified Model Predictive Control (MPC) framework optimizing
both ground reaction forces and thruster forces could theoretically address
this control problem, its feasibility is limited by the low torque-control
bandwidth of the system's lightweight actuators. To overcome this challenge, we
propose a decoupled control architecture: a Raibert-type controller governs
legged locomotion using position-based control, while an MPC regulates the
thrusters augmented by learned Contact Residual Dynamics (CRD) to account for
leg-ground impacts. This separation bypasses the torque-control rate bottleneck
while retaining the thruster MPC to explicitly account for leg-ground impact
dynamics through learned residuals. We validate this approach through both
simulation and hardware experiments, showing that the decoupled control
architecture with CRD performs more stable behavior in terms of push recovery
and cat-like walking gait compared to the decoupled controller without CRD.

</details>


### [15] [LiGen: GAN-Augmented Spectral Fingerprinting for Indoor Positioning](https://arxiv.org/abs/2508.03024)
*Jie Lin,Hsun-Yu Lee,Ho-Ming Li,Fang-Jing Wu*

Main category: cs.RO

TL;DR: LiGen是一种新型室内定位系统，利用环境光的光谱强度模式作为指纹，通过GAN数据增强和MLP模型实现高精度定位。


<details>
  <summary>Details</summary>
Motivation: 现有Wi-Fi定位系统易受环境影响，LiGen提出利用更稳定的光信号解决这一问题。

Method: 设计基于GAN的数据增强框架（PointGAN和FreeGAN），并采用MLP模型训练合成数据。

Result: LiGen实现亚米级精度，性能优于Wi-Fi基线50%以上，且在复杂环境中表现稳健。

Conclusion: LiGen首次将光谱指纹与GAN数据增强结合，为室内定位提供了更稳定、高精度的解决方案。

Abstract: Accurate and robust indoor localization is critical for smart building
applications, yet existing Wi-Fi-based systems are often vulnerable to
environmental conditions. This work presents a novel indoor localization
system, called LiGen, that leverages the spectral intensity patterns of ambient
light as fingerprints, offering a more stable and infrastructure-free
alternative to radio signals. To address the limited spectral data, we design a
data augmentation framework based on generative adversarial networks (GANs),
featuring two variants: PointGAN, which generates fingerprints conditioned on
coordinates, and FreeGAN, which uses a weak localization model to label
unconditioned samples. Our positioning model, leveraging a Multi-Layer
Perceptron (MLP) architecture to train on synthesized data, achieves
submeter-level accuracy, outperforming Wi-Fi-based baselines by over 50\%.
LiGen also demonstrates strong robustness in cluttered environments. To the
best of our knowledge, this is the first system to combine spectral
fingerprints with GAN-based data augmentation for indoor localization.

</details>


### [16] [CogniPlan: Uncertainty-Guided Path Planning with Conditional Generative Layout Prediction](https://arxiv.org/abs/2508.03027)
*Yizhuo Wang,Haodong He,Jingsong Liang,Yuhong Cao,Ritabrata Chakraborty,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: 提出CogniPlan框架，结合生成式布局预测和图注意力路径规划，显著提升探索与导航性能。


<details>
  <summary>Details</summary>
Motivation: 解决移动机器人在未知环境中路径规划的两大挑战：自主探索和点目标导航。

Method: 利用条件生成修复模型预测多种可能布局，结合图注意力机制进行路径规划。

Result: 在多个数据集和实际部署中表现优异，超越现有先进规划器。

Conclusion: CogniPlan框架在未知环境中展现出高效、可扩展且实用的路径规划能力。

Abstract: Path planning in unknown environments is a crucial yet inherently challenging
capability for mobile robots, which primarily encompasses two coupled tasks:
autonomous exploration and point-goal navigation. In both cases, the robot must
perceive the environment, update its belief, and accurately estimate potential
information gain on-the-fly to guide planning. In this work, we propose
CogniPlan, a novel path planning framework that leverages multiple plausible
layouts predicted by a COnditional GeNerative Inpainting model, mirroring how
humans rely on cognitive maps during navigation. These predictions, based on
the partially observed map and a set of layout conditioning vectors, enable our
planner to reason effectively under uncertainty. We demonstrate strong synergy
between generative image-based layout prediction and graph-attention-based path
planning, allowing CogniPlan to combine the scalability of graph
representations with the fidelity and predictiveness of occupancy maps,
yielding notable performance gains in both exploration and navigation. We
extensively evaluate CogniPlan on two datasets (hundreds of maps and realistic
floor plans), consistently outperforming state-of-the-art planners. We further
deploy it in a high-fidelity simulator and on hardware, showcasing its
high-quality path planning and real-world applicability.

</details>


### [17] [Aerobatic maneuvers in insect-scale flapping-wing aerial robots via deep-learned robust tube model predictive control](https://arxiv.org/abs/2508.03043)
*Yi-Hsuan Hsiao,Andrea Tagliabue,Owen Matteson,Suhan Kim,Tong Zhao,Jonathan P. How,YuFeng Chen*

Main category: cs.RO

TL;DR: 论文提出了一种深度学习的鲁棒管模型预测控制器，使750毫克的扑翼机器人实现了类似昆虫的高敏捷飞行。


<details>
  <summary>Details</summary>
Motivation: 昆虫飞行的高敏捷性与现有扑翼机器人的性能差距，激发了研究如何通过控制器设计和计算优化实现类似性能。

Method: 设计了深度学习的鲁棒管模型预测控制器，并通过模仿学习方法训练神经网络，以在计算受限的实时系统中实现高反馈率。

Result: 机器人展示了类似昆虫的急转动作，速度和加速度分别提高了447%和255%，并能在强风干扰下完成高难度动作。

Conclusion: 该研究标志着在昆虫尺度飞行敏捷性上的重要进展，为未来感知和计算自主性研究提供了启发。

Abstract: Aerial insects exhibit highly agile maneuvers such as sharp braking,
saccades, and body flips under disturbance. In contrast, insect-scale aerial
robots are limited to tracking non-aggressive trajectories with small body
acceleration. This performance gap is contributed by a combination of low robot
inertia, fast dynamics, uncertainty in flapping-wing aerodynamics, and high
susceptibility to environmental disturbance. Executing highly dynamic maneuvers
requires the generation of aggressive flight trajectories that push against the
hardware limit and a high-rate feedback controller that accounts for model and
environmental uncertainty. Here, through designing a deep-learned robust tube
model predictive controller, we showcase insect-like flight agility and
robustness in a 750-millgram flapping-wing robot. Our model predictive
controller can track aggressive flight trajectories under disturbance. To
achieve a high feedback rate in a compute-constrained real-time system, we
design imitation learning methods to train a two-layer, fully connected neural
network, which resembles insect flight control architecture consisting of
central nervous system and motor neurons. Our robot demonstrates insect-like
saccade movements with lateral speed and acceleration of 197 centimeters per
second and 11.7 meters per second square, representing 447$\%$ and 255$\%$
improvement over prior results. The robot can also perform saccade maneuvers
under 160 centimeters per second wind disturbance and large command-to-force
mapping errors. Furthermore, it performs 10 consecutive body flips in 11
seconds - the most challenging maneuver among sub-gram flyers. These results
represent a milestone in achieving insect-scale flight agility and inspire
future investigations on sensing and compute autonomy.

</details>


### [18] [SkeNa: Learning to Navigate Unseen Environments Based on Abstract Hand-Drawn Maps](https://arxiv.org/abs/2508.03053)
*Haojun Xu,Jiaqi Xiang,Wu Wei,Jinyu Chen,Linqing Zhong,Linjiang Huang,Hongyu Yang,Si Liu*

Main category: cs.RO

TL;DR: SkeNa任务通过手绘草图导航，提出数据集SoR和框架SkeNavigator，显著提升导航性能。


<details>
  <summary>Details</summary>
Motivation: 受人类导航策略启发，研究如何利用手绘草图在未知环境中导航。

Method: 开发自动草图生成流程构建数据集SoR；提出SkeNavigator框架，结合RMD和DAGP技术对齐视觉与草图特征。

Result: SkeNavigator在高度抽象的验证集上相对提升SPL 105%，优于现有方法。

Conclusion: SkeNa任务和SkeNavigator为草图导航研究提供了新方向和工具。

Abstract: A typical human strategy for giving navigation guidance is to sketch route
maps based on the environmental layout. Inspired by this, we introduce Sketch
map-based visual Navigation (SkeNa), an embodied navigation task in which an
agent must reach a goal in an unseen environment using only a hand-drawn sketch
map as guidance. To support research for SkeNa, we present a large-scale
dataset named SoR, comprising 54k trajectory and sketch map pairs across 71
indoor scenes. In SoR, we introduce two navigation validation sets with varying
levels of abstraction in hand-drawn sketches, categorized based on their
preservation of spatial scales in the environment, to facilitate future
research. To construct SoR, we develop an automated sketch-generation pipeline
that efficiently converts floor plans into hand-drawn representations. To solve
SkeNa, we propose SkeNavigator, a navigation framework that aligns visual
observations with hand-drawn maps to estimate navigation targets. It employs a
Ray-based Map Descriptor (RMD) to enhance sketch map valid feature
representation using equidistant sampling points and boundary distances. To
improve alignment with visual observations, a Dual-Map Aligned Goal Predictor
(DAGP) leverages the correspondence between sketch map features and on-site
constructed exploration map features to predict goal position and guide
navigation. SkeNavigator outperforms prior floor plan navigation methods by a
large margin, improving SPL on the high-abstract validation set by 105%
relatively. Our code and dataset will be released.

</details>


### [19] [Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching](https://arxiv.org/abs/2508.03068)
*Sirui Chen,Yufei Ye,Zi-Ang Cao,Jennifer Lew,Pei Xu,C. Karen Liu*

Main category: cs.RO

TL;DR: HEAD框架通过模块化方法，从人类运动和视觉感知数据中学习人形机器人的导航、运动和抓取技能。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人在复杂环境中导航和抓取的挑战，通过模块化设计提高学习效率和可扩展性。

Method: 采用高低级策略结合：低级策略从大规模人类动作捕捉数据中学习全身控制，高级策略从Aria眼镜收集的人类数据中学习目标位置和方向。

Result: 在仿真和现实环境中验证了人形机器人在复杂环境中的导航和抓取能力。

Conclusion: 模块化方法有效分离了视觉感知与物理动作，提升了学习效率和场景适应性。

Abstract: We propose Hand-Eye Autonomous Delivery (HEAD), a framework that learns
navigation, locomotion, and reaching skills for humanoids, directly from human
motion and vision perception data. We take a modular approach where the
high-level planner commands the target position and orientation of the hands
and eyes of the humanoid, delivered by the low-level policy that controls the
whole-body movements. Specifically, the low-level whole-body controller learns
to track the three points (eyes, left hand, and right hand) from existing
large-scale human motion capture data while high-level policy learns from human
data collected by Aria glasses. Our modular approach decouples the ego-centric
vision perception from physical actions, promoting efficient learning and
scalability to novel scenes. We evaluate our method both in simulation and in
the real-world, demonstrating humanoid's capabilities to navigate and reach in
complex environments designed for humans.

</details>


### [20] [Optimizing Bipedal Locomotion for The 100m Dash With Comparison to Human Running](https://arxiv.org/abs/2508.03070)
*Devin Crowley,Jeremy Dao,Helei Duan,Kevin Green,Jonathan Hurst,Alan Fern*

Main category: cs.RO

TL;DR: 本文优化了双足机器人Cassie的步态效率，比较了其与人类跑步力学的相似性，并实现了100米短跑的世界纪录。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提高双足机器人在高速跑步时的步态效率，并与人类跑步力学进行对比。

Method: 通过优化步态效率，结合人类生物力学研究进行比较，并开发满足100米短跑任务要求的控制器。

Result: 优化后的步态与人类跑步力学高度相似，并在硬件上实现了100米短跑的世界纪录。

Conclusion: 研究表明双足机器人可以通过优化步态实现高效高速运动，并展示了实际应用的潜力。

Abstract: In this paper, we explore the space of running gaits for the bipedal robot
Cassie. Our first contribution is to present an approach for optimizing gait
efficiency across a spectrum of speeds with the aim of enabling extremely
high-speed running on hardware. This raises the question of how the resulting
gaits compare to human running mechanics, which are known to be highly
efficient in comparison to quadrupeds. Our second contribution is to conduct
this comparison based on established human biomechanical studies. We find that
despite morphological differences between Cassie and humans, key properties of
the gaits are highly similar across a wide range of speeds. Finally, our third
contribution is to integrate the optimized running gaits into a full controller
that satisfies the rules of the real-world task of the 100m dash, including
starting and stopping from a standing position. We demonstrate this controller
on hardware to establish the Guinness World Record for Fastest 100m by a
Bipedal Robot.

</details>


### [21] [Point2Act: Efficient 3D Distillation of Multimodal LLMs for Zero-Shot Context-Aware Grasping](https://arxiv.org/abs/2508.03099)
*Sang Min Kim,Hyeongjun Heo,Junho Kim,Yonghyeon Lee,Young Min Kim*

Main category: cs.RO

TL;DR: Point2Act利用多模态大语言模型（MLLMs）直接检索与任务相关的3D动作点，通过3D相关性场实现高效定位，支持零样本任务。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在2D图像中语义理解模糊、难以精确定位3D动作点的问题。

Method: 提出3D相关性场，结合多视角聚合补偿几何模糊性，生成精细的3D空间上下文。

Result: 系统在20秒内生成空间响应，支持实际操控任务。

Conclusion: Point2Act为通用机器人提供了高效、精确的3D动作定位解决方案。

Abstract: We propose Point2Act, which directly retrieves the 3D action point relevant
for a contextually described task, leveraging Multimodal Large Language Models
(MLLMs). Foundation models opened the possibility for generalist robots that
can perform a zero-shot task following natural language descriptions within an
unseen environment. While the semantics obtained from large-scale image and
language datasets provide contextual understanding in 2D images, the rich yet
nuanced features deduce blurry 2D regions and struggle to find precise 3D
locations for actions. Our proposed 3D relevancy fields bypass the
high-dimensional features and instead efficiently imbue lightweight 2D
point-level guidance tailored to the task-specific action. The multi-view
aggregation effectively compensates for misalignments due to geometric
ambiguities, such as occlusion, or semantic uncertainties inherent in the
language descriptions. The output region is highly localized, reasoning
fine-grained 3D spatial context that can directly transfer to an explicit
position for physical action at the on-the-fly reconstruction of the scene. Our
full-stack pipeline, which includes capturing, MLLM querying, 3D
reconstruction, and grasp pose extraction, generates spatially grounded
responses in under 20 seconds, facilitating practical manipulation tasks.
Project page: https://sangminkim-99.github.io/point2act/

</details>


### [22] [Safety-Aware Imitation Learning via MPC-Guided Disturbance Injection](https://arxiv.org/abs/2508.03129)
*Le Qiu,Yusuf Umut Ciftci,Somil Bansal*

Main category: cs.RO

TL;DR: MPC-SafeGIL通过在设计阶段注入对抗性扰动增强模仿学习的安全性，利用MPC近似最坏情况扰动，适用于高维和黑盒系统。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在安全关键应用中可能因策略错误导致安全问题，需要一种方法增强其安全性。

Method: 通过注入对抗性扰动扩展专家演示范围，利用采样MPC近似最坏情况扰动，直接整合安全考量。

Result: 在四足机器人运动和视觉导航仿真及四旋翼实验中验证，安全性和任务性能均有提升。

Conclusion: MPC-SafeGIL有效提升模仿学习的安全性，适用于复杂动态系统。

Abstract: Imitation Learning has provided a promising approach to learning complex
robot behaviors from expert demonstrations. However, learned policies can make
errors that lead to safety violations, which limits their deployment in
safety-critical applications. We propose MPC-SafeGIL, a design-time approach
that enhances the safety of imitation learning by injecting adversarial
disturbances during expert demonstrations. This exposes the expert to a broader
range of safety-critical scenarios and allows the imitation policy to learn
robust recovery behaviors. Our method uses sampling-based Model Predictive
Control (MPC) to approximate worst-case disturbances, making it scalable to
high-dimensional and black-box dynamical systems. In contrast to prior work
that relies on analytical models or interactive experts, MPC-SafeGIL integrates
safety considerations directly into data collection. We validate our approach
through extensive simulations including quadruped locomotion and visuomotor
navigation and real-world experiments on a quadrotor, demonstrating
improvements in both safety and task performance. See our website here:
https://leqiu2003.github.io/MPCSafeGIL/

</details>


### [23] [Language as Cost: Proactive Hazard Mapping using VLM for Robot Navigation](https://arxiv.org/abs/2508.03138)
*Mintaek Oh,Chan Kim,Seung-Woo Seo,Seong-Woo Kim*

Main category: cs.RO

TL;DR: 提出了一种基于视觉语言模型（VLM）的零样本语言成本映射框架，用于机器人主动预测和规避动态风险。


<details>
  <summary>Details</summary>
Motivation: 传统导航系统依赖静态地图，难以应对动态风险（如突然出现的行人），导致反应式而非预见性行为。

Method: 利用预训练的视觉语言模型解析视觉场景，评估动态风险，并预先分配风险感知的导航成本，结合几何障碍地图实现主动规划。

Result: 在模拟和多样化动态环境中的实验表明，该方法显著提高了导航成功率并减少了危险遭遇。

Conclusion: 该框架通过语言成本映射实现了对动态风险的预见性规避，优于传统反应式规划方法。

Abstract: Robots operating in human-centric or hazardous environments must proactively
anticipate and mitigate dangers beyond basic obstacle detection. Traditional
navigation systems often depend on static maps, which struggle to account for
dynamic risks, such as a person emerging from a suddenly opening door. As a
result, these systems tend to be reactive rather than anticipatory when
handling dynamic hazards. Recent advancements in pre-trained large language
models and vision-language models (VLMs) create new opportunities for proactive
hazard avoidance. In this work, we propose a zero-shot language-as-cost mapping
framework that leverages VLMs to interpret visual scenes, assess potential
dynamic risks, and assign risk-aware navigation costs preemptively, enabling
robots to anticipate hazards before they materialize. By integrating this
language-based cost map with a geometric obstacle map, the robot not only
identifies existing obstacles but also anticipates and proactively plans around
potential hazards arising from environmental dynamics. Experiments in simulated
and diverse dynamic environments demonstrate that the proposed method
significantly improves navigation success rates and reduces hazard encounters,
compared to reactive baseline planners. Code and supplementary materials are
available at https://github.com/Taekmino/LaC.

</details>


### [24] [CookBench: A Long-Horizon Embodied Planning Benchmark for Complex Cooking Scenarios](https://arxiv.org/abs/2508.03232)
*Muzhen Cai,Xiubo Chen,Yining An,Jiaxin Zhang,Xuesong Wang,Wang Xu,Weinan Zhang,Ting Liu*

Main category: cs.RO

TL;DR: 论文介绍了CookBench，一个用于复杂烹饪场景中长期规划任务的基准测试，解决了现有基准测试中任务短视和动作粗糙的问题。


<details>
  <summary>Details</summary>
Motivation: 现有体现规划基准测试多为短视任务和粗粒度动作，无法满足复杂物理世界中长期任务的需求。

Method: 通过高保真Unity模拟环境，设计两阶段任务（意图识别和体现交互），并提供统一API工具集。

Result: 提出了CookBench基准测试，揭示了当前大型语言模型和视觉语言模型在复杂长期任务中的不足。

Conclusion: CookBench将为未来研究提供支持，促进长期规划任务的发展。

Abstract: Embodied Planning is dedicated to the goal of creating agents capable of
executing long-horizon tasks in complex physical worlds. However, existing
embodied planning benchmarks frequently feature short-horizon tasks and
coarse-grained action primitives. To address this challenge, we introduce
CookBench, a benchmark for long-horizon planning in complex cooking scenarios.
By leveraging a high-fidelity simulation environment built upon the powerful
Unity game engine, we define frontier AI challenges in a complex, realistic
environment. The core task in CookBench is designed as a two-stage process.
First, in Intention Recognition, an agent needs to accurately parse a user's
complex intent. Second, in Embodied Interaction, the agent should execute the
identified cooking goal through a long-horizon, fine-grained sequence of
physical actions. Unlike existing embodied planning benchmarks, we refine the
action granularity to a spatial level that considers crucial operational
information while abstracting away low-level robotic control. Besides, We
provide a comprehensive toolset that encapsulates the simulator. Its unified
API supports both macro-level operations, such as placing orders and purchasing
ingredients, and a rich set of fine-grained embodied actions for physical
interaction, enabling researchers to focus on high-level planning and
decision-making. Furthermore, we present an in-depth analysis of
state-of-the-art, closed-source Large Language Model and Vision-Language Model,
revealing their major shortcomings and challenges posed by complex,
long-horizon tasks. The full benchmark will be open-sourced to facilitate
future research.

</details>


### [25] [Force-Compliance MPC and Robot-User CBFs for Interactive Navigation and User-Robot Safety in Hexapod Guide Robots](https://arxiv.org/abs/2508.03246)
*Zehua Fan,Feng Gao,Zhijun Chen,Yunpeng Yin,Limin Yang,Qingxing Xi,En Yang,Xuefeng Luo*

Main category: cs.RO

TL;DR: 提出了一种结合力-顺应模型预测控制（FC-MPC）和机器人-用户控制屏障函数（CBFs）的六足导盲机器人系统，实现实时双向交互和安全导航。


<details>
  <summary>Details</summary>
Motivation: 为视觉障碍者在复杂环境中提供实时、安全的导航辅助，需解决双向交互和动态避障问题。

Method: 采用FC-MPC估计用户施加的力并调整机器人运动，结合CBFs处理静态和动态障碍，使用八向连接DBSCAN聚类降低计算复杂度，MBE建模障碍物并通过卡尔曼滤波预测轨迹。

Result: 系统在HexGuide机器人上实现，实验表明其能适应用户力指令并确保导航安全。

Conclusion: 该方法有效解决了复杂环境中的实时导航和避障问题，提升了导盲机器人的实用性。

Abstract: Guiding the visually impaired in complex environments requires real-time
two-way interaction and safety assurance. We propose a Force-Compliance Model
Predictive Control (FC-MPC) and Robot-User Control Barrier Functions (CBFs) for
force-compliant navigation and obstacle avoidance in Hexapod guide robots.
FC-MPC enables two-way interaction by estimating user-applied forces and
moments using the robot's dynamic model and the recursive least squares (RLS)
method, and then adjusting the robot's movements accordingly, while Robot-User
CBFs ensure the safety of both the user and the robot by handling static and
dynamic obstacles, and employ weighted slack variables to overcome feasibility
issues in complex dynamic environments. We also adopt an Eight-Way Connected
DBSCAN method for obstacle clustering, reducing computational complexity from
O(n2) to approximately O(n), enabling real-time local perception on
resource-limited on-board robot computers. Obstacles are modeled using Minimum
Bounding Ellipses (MBEs), and their trajectories are predicted through Kalman
filtering. Implemented on the HexGuide robot, the system seamlessly integrates
force compliance, autonomous navigation, and obstacle avoidance. Experimental
results demonstrate the system's ability to adapt to user force commands while
guaranteeing user and robot safety simultaneously during navigation in complex
environments.

</details>


### [26] [UniFucGrasp: Human-Hand-Inspired Unified Functional Grasp Annotation Strategy and Dataset for Diverse Dexterous Hands](https://arxiv.org/abs/2508.03339)
*Haoran Lin,Wenrui Chen,Xianchi Chen,Fan Yang,Qiang Diao,Wenxin Xie,Sijie Wu,Kailun Yang,Maojun Li,Yaonan Wang*

Main category: cs.RO

TL;DR: 论文提出UniFucGrasp，一种通用的功能性抓取标注策略和数据集，解决了现有数据集忽略功能性抓取的问题，并通过生物启发方法实现低成本高效的数据收集。


<details>
  <summary>Details</summary>
Motivation: 现有灵巧抓取数据集过于关注稳定性，忽略了功能性抓取（如开瓶盖、握杯柄），且依赖昂贵难控的高自由度Shadow Hands。

Method: 基于生物启发，将人类自然动作映射到多种手型结构，利用几何力闭合确保功能性、稳定性和类人抓取。

Result: 实验表明，该方法提高了功能性操作精度和抓取稳定性，支持跨多种机械手的高效泛化，并克服了标注成本和泛化挑战。

Conclusion: UniFucGrasp为灵巧抓取提供了首个多手功能性数据集和合成模型，验证了其有效性。

Abstract: Dexterous grasp datasets are vital for embodied intelligence, but mostly
emphasize grasp stability, ignoring functional grasps needed for tasks like
opening bottle caps or holding cup handles. Most rely on bulky, costly, and
hard-to-control high-DOF Shadow Hands. Inspired by the human hand's
underactuated mechanism, we establish UniFucGrasp, a universal functional grasp
annotation strategy and dataset for multiple dexterous hand types. Based on
biomimicry, it maps natural human motions to diverse hand structures and uses
geometry-based force closure to ensure functional, stable, human-like grasps.
This method supports low-cost, efficient collection of diverse, high-quality
functional grasps. Finally, we establish the first multi-hand functional grasp
dataset and provide a synthesis model to validate its effectiveness.
Experiments on the UFG dataset, IsaacSim, and complex robotic tasks show that
our method improves functional manipulation accuracy and grasp stability,
enables efficient generalization across diverse robotic hands, and overcomes
annotation cost and generalization challenges in dexterous grasping. The
project page is at https://haochen611.github.io/UFG.

</details>


### [27] [Opti-Acoustic Scene Reconstruction in Highly Turbid Underwater Environments](https://arxiv.org/abs/2508.03408)
*Ivana Collado-Gonzalez,John McConnell,Paul Szenher,Brendan Englot*

Main category: cs.RO

TL;DR: 提出了一种实时光声场景重建方法，适用于浑浊水域，结合视觉和声纳数据以提高重建效果。


<details>
  <summary>Details</summary>
Motivation: 水下机器人在浑浊水域中导航时，单目视觉重建方法不可靠且缺乏深度信息，而声纳分辨率低且存在高度模糊问题。

Method: 通过识别视觉数据中的感兴趣区域并与声纳数据匹配，利用声纳的距离数据和相机的高度数据进行重建。

Result: 在不同浑浊度下的实验和码头环境中的实地测试验证了该方法的有效性。

Conclusion: 该方法在浑浊水域中表现优异，代码已开源以促进复现和社区参与。

Abstract: Scene reconstruction is an essential capability for underwater robots
navigating in close proximity to structures. Monocular vision-based
reconstruction methods are unreliable in turbid waters and lack depth scale
information. Sonars are robust to turbid water and non-uniform lighting
conditions, however, they have low resolution and elevation ambiguity. This
work proposes a real-time opti-acoustic scene reconstruction method that is
specially optimized to work in turbid water. Our strategy avoids having to
identify point features in visual data and instead identifies regions of
interest in the data. We then match relevant regions in the image to
corresponding sonar data. A reconstruction is obtained by leveraging range data
from the sonar and elevation data from the camera image. Experimental
comparisons against other vision-based and sonar-based approaches at varying
turbidity levels, and field tests conducted in marina environments, validate
the effectiveness of the proposed approach. We have made our code open-source
to facilitate reproducibility and encourage community engagement.

</details>


### [28] [Residual Neural Terminal Constraint for MPC-based Collision Avoidance in Dynamic Environments](https://arxiv.org/abs/2508.03428)
*Bojan Derajić,Mohamed-Khalil Bouzidi,Sebastian Bernhard,Wolfgang Hönig*

Main category: cs.RO

TL;DR: 提出了一种混合MPC局部规划器，通过学习近似时变安全集作为MPC终端约束，结合HJ可达性分析和神经网络残差，实现实时安全规划。


<details>
  <summary>Details</summary>
Motivation: 解决HJ可达性分析在实时应用中不可行的问题，同时确保规划的安全性和高效性。

Method: 利用HJ值函数可分解为符号距离函数（SDF）和非负残差函数的性质，用神经网络建模残差，并通过超网络参数化提高实时性和泛化能力。

Result: 在仿真和硬件实验中，相比三种先进方法，成功率提高30%，计算成本相近且生成高质量（低旅行时间）解。

Conclusion: 该方法在实时性和安全性上优于现有技术，适用于高效安全的局部规划。

Abstract: In this paper, we propose a hybrid MPC local planner that uses a
learning-based approximation of a time-varying safe set, derived from local
observations and applied as the MPC terminal constraint. This set can be
represented as a zero-superlevel set of the value function computed via
Hamilton-Jacobi (HJ) reachability analysis, which is infeasible in real-time.
We exploit the property that the HJ value function can be expressed as a
difference of the corresponding signed distance function (SDF) and a
non-negative residual function. The residual component is modeled as a neural
network with non-negative output and subtracted from the computed SDF,
resulting in a real-time value function estimate that is at least as safe as
the SDF by design. Additionally, we parametrize the neural residual by a
hypernetwork to improve real-time performance and generalization properties.
The proposed method is compared with three state-of-the-art methods in
simulations and hardware experiments, achieving up to 30\% higher success rates
compared to the best baseline while requiring a similar computational effort
and producing high-quality (low travel-time) solutions.

</details>


### [29] [Theatre in the Loop: A Rehearsal-Based, Collaborative Workflow for Expressive Robotic Behaviours](https://arxiv.org/abs/2508.03514)
*Pavlos Panagiotidis,Victor Zhi Heung Ngo,Sean Myatt,Roma Patel,Rachel Ramchurn,Alan Chamberlain,Ayse Kucukyilmaz*

Main category: cs.RO

TL;DR: 提出了一种名为“theatre-in-the-loop”的框架，通过导演引导的木偶工作流程开发艺术表演中的机器人行为。


<details>
  <summary>Details</summary>
Motivation: 通过戏剧方法为机器人行为赋予情感表达，探索人机交互的新模式。

Method: 利用叙事目标指导木偶师生成即兴机器人动作，捕捉并整理为可重用的动作模板。

Result: 初步试验验证了方法的可行性，展示了机器人动作的情感连贯性，但也揭示了机械限制带来的挑战。

Conclusion: 该框架为跨学科团队开发社交表达机器人行为提供了模型，同时促进了戏剧与人机交互的结合。

Abstract: In this paper, we propose theatre-in-the-loop, a framework for developing
expressive robot behaviours tailored to artistic performance through a
director-guided puppeteering workflow. Leveraging theatrical methods, we use
narrative objectives to direct a puppeteer in generating improvised robotic
gestures that convey specific emotions. These improvisations are captured and
curated to build a dataset of reusable movement templates for standalone
playback in future autonomous performances. Initial trials demonstrate the
feasibility of this approach, illustrating how the workflow enables precise
sculpting of robotic gestures into coherent emotional arcs while revealing
challenges posed by the robot's mechanical constraints. We argue that this
practice-led framework provides a model for interdisciplinary teams creating
socially expressive robot behaviours, contributing to (1) theatre as an
interactive training ground for human-robot interaction and (2) co-creation
methodologies between humans and machines.

</details>


### [30] [CollaBot: Vision-Language Guided Simultaneous Collaborative Manipulation](https://arxiv.org/abs/2508.03526)
*Kun Song,Shentao Ma,Gaoming Chen,Ninglong Jin,Guangbao Zhao,Mingyu Ding,Zhenhua Xiong,Jia Pan*

Main category: cs.RO

TL;DR: 提出了一种名为CollaBot的通用框架，用于多机器人协作操纵大型物体，解决了传统方法无法适应不同规模和任务的局限性。


<details>
  <summary>Details</summary>
Motivation: 工厂和家庭环境中需要移动大型物体，传统方法局限于小物体且缺乏通用性。

Method: 使用SEEM进行场景分割和目标物体点云提取，提出协作抓取框架，分解任务为局部抓取和全局协作，设计两阶段规划模块生成无碰撞轨迹。

Result: 实验显示在不同机器人数量、物体和任务中成功率为52%，验证了框架的有效性。

Conclusion: CollaBot框架在多机器人协作操纵任务中表现出通用性和可扩展性。

Abstract: A central research topic in robotics is how to use this system to interact
with the physical world. Traditional manipulation tasks primarily focus on
small objects. However, in factory or home environments, there is often a need
for the movement of large objects, such as moving tables. These tasks typically
require multi-robot systems to work collaboratively. Previous research lacks a
framework that can scale to arbitrary sizes of robots and generalize to various
kinds of tasks. In this work, we propose CollaBot, a generalist framework for
simultaneous collaborative manipulation. First, we use SEEM for scene
segmentation and point cloud extraction of the target object. Then, we propose
a collaborative grasping framework, which decomposes the task into local grasp
pose generation and global collaboration. Finally, we design a 2-stage planning
module that can generate collision-free trajectories to achieve this task.
Experiments show a success rate of 52% across different numbers of robots,
objects, and tasks, indicating the effectiveness of the proposed framework.

</details>


### [31] [Vision-based Perception System for Automated Delivery Robot-Pedestrians Interactions](https://arxiv.org/abs/2508.03541)
*Ergi Tushe,Bilal Farooq*

Main category: cs.RO

TL;DR: 论文提出了一种基于单视觉传感器的多行人检测与跟踪、姿态估计和深度感知的完整流程，通过结合人体姿态估计和深度信息，提升了行人轨迹预测和身份保持能力，在遮挡和密集人群场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决自动送货机器人（ADRs）在行人密集城市空间中安全、高效且社会可接受的导航问题。

Method: 利用MOT17数据集，开发了结合人体姿态估计和深度感知的多行人检测与跟踪系统。

Result: 身份保持（IDF1）提升10%，多目标跟踪准确率（MOTA）提升7%，检测精度超过85%，并能识别弱势行人群体。

Conclusion: 该系统显著提升了ADRs在复杂环境中的导航能力，并支持更具社会意识和包容性的机器人行为。

Abstract: The integration of Automated Delivery Robots (ADRs) into pedestrian-heavy
urban spaces introduces unique challenges in terms of safe, efficient, and
socially acceptable navigation. We develop the complete pipeline for a single
vision sensor based multi-pedestrian detection and tracking, pose estimation,
and monocular depth perception. Leveraging the real-world MOT17 dataset
sequences, this study demonstrates how integrating human-pose estimation and
depth cues enhances pedestrian trajectory prediction and identity maintenance,
even under occlusions and dense crowds. Results show measurable improvements,
including up to a 10% increase in identity preservation (IDF1), a 7%
improvement in multiobject tracking accuracy (MOTA), and consistently high
detection precision exceeding 85%, even in challenging scenarios. Notably, the
system identifies vulnerable pedestrian groups supporting more socially aware
and inclusive robot behaviour.

</details>


### [32] [Online Learning for Vibration Suppression in Physical Robot Interaction using Power Tools](https://arxiv.org/abs/2508.03559)
*Gokhan Solak,Arash Ajoudani*

Main category: cs.RO

TL;DR: 论文提出了一种改进的BMFLC算法（damped BMFLC），用于在线学习和抑制协作机器人在复杂环境中的振动，提高了收敛速度和抗噪能力。


<details>
  <summary>Details</summary>
Motivation: 协作机器人在建筑工地等复杂环境中工作时，振动抑制是一个重要能力。研究旨在主动抑制由外部源（如电动工具）引起的振动。

Method: 采用BMFLC算法在线学习振动，并通过前馈力控制进行抵消。提出damped BMFLC方法，引入自适应步长和改进的阻尼机制，以提升收敛速度和抗噪性。

Result: 仿真和实物实验表明，改进方法在抑制率和效率上优于原始BMFLC及其扩展算法（如递归最小二乘和卡尔曼滤波）。

Conclusion: damped BMFLC方法在振动抑制方面表现出色，适用于实际应用，如抛光实验。

Abstract: Vibration suppression is an important capability for collaborative robots
deployed in challenging environments such as construction sites. We study the
active suppression of vibration caused by external sources such as power tools.
We adopt the band-limited multiple Fourier linear combiner (BMFLC) algorithm to
learn the vibration online and counter it by feedforward force control. We
propose the damped BMFLC method, extending BMFLC with a novel adaptive
step-size approach that improves the convergence time and noise resistance. Our
logistic function-based damping mechanism reduces the effect of noise and
enables larger learning rates. We evaluate our method on extensive simulation
experiments with realistic time-varying multi-frequency vibration and
real-world physical interaction experiments. The simulation experiments show
that our method improves the suppression rate in comparison to the original
BMFLC and its recursive least squares and Kalman filter-based extensions.
Furthermore, our method is far more efficient than the latter two. We further
validate the effectiveness of our method in real-world polishing experiments. A
supplementary video is available at https://youtu.be/ms6m-6JyVAI.

</details>


### [33] [Why Evolve When You Can Adapt? Post-Evolution Adaptation of Genetic Memory for On-the-Fly Control](https://arxiv.org/abs/2508.03600)
*Hamze Hammami,Eva Denisa Barbulescu,Talal Shaikh,Mouayad Aldada,Muhammad Saad Munawar*

Main category: cs.RO

TL;DR: 提出了一种结合遗传算法和海布学习的零-shot适应机制，使机器人控制器能实时动态调整以应对突发挑战。


<details>
  <summary>Details</summary>
Motivation: 受生物系统启发，旨在解决机器人在不可预见环境变化中的实时适应问题。

Method: 将遗传算法控制器与在线海布可塑性结合，利用适应度函数动态调整突触权重。

Result: 在T型迷宫导航任务中验证了该方法的有效性，成功应对光线变化和障碍物。

Conclusion: 该方法为机器人提供了一种无需额外训练的实时适应能力，同时保留核心知识。

Abstract: Imagine a robot controller with the ability to adapt like human synapses,
dynamically rewiring itself to overcome unforeseen challenges in real time.
This paper proposes a novel zero-shot adaptation mechanism for evolutionary
robotics, merging a standard Genetic Algorithm (GA) controller with online
Hebbian plasticity. Inspired by biological systems, the method separates
learning and memory, with the genotype acting as memory and Hebbian updates
handling learning. In our approach, the fitness function is leveraged as a live
scaling factor for Hebbian learning, enabling the robot's neural controller to
adjust synaptic weights on-the-fly without additional training. This adds a
dynamic adaptive layer that activates only during runtime to handle unexpected
environmental changes. After the task, the robot 'forgets' the temporary
adjustments and reverts to the original weights, preserving core knowledge. We
validate this hybrid GA-Hebbian controller on an e-puck robot in a T-maze
navigation task with changing light conditions and obstacles.

</details>


### [34] [DiWA: Diffusion Policy Adaptation with World Models](https://arxiv.org/abs/2508.03645)
*Akshay L Chandra,Iman Nematollahi,Chenguang Huang,Tim Welschehold,Wolfram Burgard,Abhinav Valada*

Main category: cs.RO

TL;DR: DiWA框架通过离线世界模型优化扩散策略，显著提升样本效率，减少实际交互需求。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在强化学习（RL）中的微调面临奖励传播困难和大量实际交互需求的挑战。

Method: 提出DiWA框架，利用离线世界模型对扩散策略进行RL微调，避免大量环境交互。

Result: 在CALVIN基准测试中，DiWA仅通过离线适应就提升了八项任务的性能，且交互需求远低于基线方法。

Conclusion: DiWA首次实现了基于离线世界模型的扩散策略微调，为实际机器人学习提供了更高效、安全的方法。

Abstract: Fine-tuning diffusion policies with reinforcement learning (RL) presents
significant challenges. The long denoising sequence for each action prediction
impedes effective reward propagation. Moreover, standard RL methods require
millions of real-world interactions, posing a major bottleneck for practical
fine-tuning. Although prior work frames the denoising process in diffusion
policies as a Markov Decision Process to enable RL-based updates, its strong
dependence on environment interaction remains highly inefficient. To bridge
this gap, we introduce DiWA, a novel framework that leverages a world model for
fine-tuning diffusion-based robotic skills entirely offline with reinforcement
learning. Unlike model-free approaches that require millions of environment
interactions to fine-tune a repertoire of robot skills, DiWA achieves effective
adaptation using a world model trained once on a few hundred thousand offline
play interactions. This results in dramatically improved sample efficiency,
making the approach significantly more practical and safer for real-world robot
learning. On the challenging CALVIN benchmark, DiWA improves performance across
eight tasks using only offline adaptation, while requiring orders of magnitude
fewer physical interactions than model-free baselines. To our knowledge, this
is the first demonstration of fine-tuning diffusion policies for real-world
robotic skills using an offline world model. We make the code publicly
available at https://diwa.cs.uni-freiburg.de.

</details>


### [35] [Inland-LOAM: Voxel-Based Structural Semantic Mapping for Inland Waterways](https://arxiv.org/abs/2508.03672)
*Zhongbi Luo,Yunjia Wang,Jan Swevers,Peter Slaets,Herman Bruyninckx*

Main category: cs.RO

TL;DR: Inland-LOAM 是一种针对内陆水道的 LiDAR SLAM 框架，通过改进特征提取和水面平面约束减少垂直漂移，并生成语义地图以支持自主导航。


<details>
  <summary>Details</summary>
Motivation: 现有水道地图缺乏实时细节，传统 LiDAR SLAM 在水道环境中表现不佳，导致垂直漂移和非语义地图，阻碍自主导航。

Method: 提出 Inland-LOAM，结合改进的特征提取和水面平面约束，通过体素几何分析将 3D 点云转换为结构化 2D 语义地图，并自动提取岸线。

Result: 在真实数据集上验证，Inland-LOAM 的定位精度优于现有方法，生成的语义地图和岸线与实际情况一致。

Conclusion: Inland-LOAM 为水道自主导航提供了可靠的数据支持，代码和数据集将公开。

Abstract: Accurate geospatial information is crucial for safe, autonomous Inland
Waterway Transport (IWT), as existing charts (IENC) lack real-time detail and
conventional LiDAR SLAM fails in waterway environments. These challenges lead
to vertical drift and non-semantic maps, hindering autonomous navigation.
  This paper introduces Inland-LOAM, a LiDAR SLAM framework for waterways. It
uses an improved feature extraction and a water surface planar constraint to
mitigate vertical drift. A novel pipeline transforms 3D point clouds into
structured 2D semantic maps using voxel-based geometric analysis, enabling
real-time computation of navigational parameters like bridge clearances. An
automated module extracts shorelines and exports them into a lightweight,
IENC-compatible format.
  Evaluations on a real-world dataset show Inland-LOAM achieves superior
localization accuracy over state-of-the-art methods. The generated semantic
maps and shorelines align with real-world conditions, providing reliable data
for enhanced situational awareness. The code and dataset will be publicly
available

</details>
