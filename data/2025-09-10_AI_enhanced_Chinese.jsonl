{"id": "2509.07162", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07162", "abs": "https://arxiv.org/abs/2509.07162", "authors": ["Martin Matak", "Mohanraj Devendran Ashanti", "Karl Van Wyk", "Tucker Hermans"], "title": "First Plan Then Evaluate: Use a Vectorized Motion Planner for Grasping", "comment": null, "summary": "Autonomous multi-finger grasping is a fundamental capability in robotic\nmanipulation. Optimization-based approaches show strong performance, but tend\nto be sensitive to initialization and are potentially time-consuming. As an\nalternative, the generator-evaluator-planner framework has been proposed. A\ngenerator generates grasp candidates, an evaluator ranks the proposed grasps,\nand a motion planner plans a trajectory to the highest-ranked grasp. If the\nplanner doesn't find a trajectory, a new trajectory optimization is started\nwith the next-best grasp as the target and so on. However, executing\nlower-ranked grasps means a lower chance of grasp success, and multiple\ntrajectory optimizations are time-consuming. Alternatively, relaxing the\nthreshold for motion planning accuracy allows for easier computation of a\nsuccessful trajectory but implies lower accuracy in estimating grasp success\nlikelihood. It's a lose-lose proposition: either spend more time finding a\nsuccessful trajectory or have a worse estimate of grasp success. We propose a\nframework that plans trajectories to a set of generated grasp targets in\nparallel, the evaluator estimates the grasp success likelihood of the resulting\ntrajectories, and the robot executes the trajectory most likely to succeed. To\nplan trajectories to different targets efficiently, we propose the use of a\nvectorized motion planner. Our experiments show our approach improves over the\ntraditional generator-evaluator-planner framework across different objects,\ngenerators, and motion planners, and successfully generalizes to novel\nenvironments in the real world, including different shelves and table heights.\nProject website https://sites.google.com/view/fpte", "AI": {"tldr": "\u63d0\u51fa\u5e76\u884c\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5411\u91cf\u5316\u8fd0\u52a8\u89c4\u5212\u5668\u540c\u65f6\u4e3a\u591a\u4e2a\u6293\u53d6\u76ee\u6807\u89c4\u5212\u8f68\u8ff9\uff0c\u9009\u62e9\u6210\u529f\u7387\u6700\u9ad8\u7684\u8f68\u8ff9\u6267\u884c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u751f\u6210\u5668-\u8bc4\u4f30\u5668-\u89c4\u5212\u5668\u6846\u67b6\u4e2d\u65f6\u95f4\u6d88\u8017\u4e0e\u51c6\u786e\u6027\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u5bf9\u521d\u59cb\u5316\u654f\u611f\u4e14\u8017\u65f6\uff0c\u751f\u6210\u5668-\u8bc4\u4f30\u5668-\u89c4\u5212\u5668\u6846\u67b6\u5b58\u5728\u8981\u4e48\u82b1\u8d39\u66f4\u591a\u65f6\u95f4\u5bfb\u627e\u6210\u529f\u8f68\u8ff9\u3001\u8981\u4e48\u964d\u4f4e\u6293\u53d6\u6210\u529f\u6982\u7387\u4f30\u8ba1\u51c6\u786e\u6027\u7684\u4e24\u96be\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5e76\u884c\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff1a\u540c\u65f6\u4e3a\u591a\u4e2a\u6293\u53d6\u76ee\u6807\u89c4\u5212\u8f68\u8ff9\uff0c\u4f7f\u7528\u5411\u91cf\u5316\u8fd0\u52a8\u89c4\u5212\u5668\u9ad8\u6548\u89c4\u5212\uff0c\u8bc4\u4f30\u5668\u4f30\u8ba1\u5404\u8f68\u8ff9\u7684\u6293\u53d6\u6210\u529f\u6982\u7387\uff0c\u9009\u62e9\u6700\u53ef\u80fd\u6210\u529f\u7684\u8f68\u8ff9\u6267\u884c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u7269\u4f53\u3001\u751f\u6210\u5668\u548c\u8fd0\u52a8\u89c4\u5212\u5668\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u6846\u67b6\uff0c\u5e76\u80fd\u6210\u529f\u63a8\u5e7f\u5230\u771f\u5b9e\u4e16\u754c\u7684\u65b0\u73af\u5883\u4e2d\uff0c\u5305\u62ec\u4e0d\u540c\u8d27\u67b6\u548c\u684c\u5b50\u9ad8\u5ea6\u3002", "conclusion": "\u5e76\u884c\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u65f6\u95f4-\u51c6\u786e\u6027\u6743\u8861\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6293\u53d6\u6548\u7387\u548c\u6210\u529f\u7387\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.07216", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07216", "abs": "https://arxiv.org/abs/2509.07216", "authors": ["Hassen Nigatu", "Shi Gaokun", "Li Jituo", "Wang Jin", "Lu Guodong", "Howard Li"], "title": "Quantum Machine Learning and Grover's Algorithm for Quantum Optimization of Robotic Manipulators", "comment": null, "summary": "Optimizing high-degree of freedom robotic manipulators requires searching\ncomplex, high-dimensional configuration spaces, a task that is computationally\nchallenging for classical methods. This paper introduces a quantum native\nframework that integrates quantum machine learning with Grover's algorithm to\nsolve kinematic optimization problems efficiently. A parameterized quantum\ncircuit is trained to approximate the forward kinematics model, which then\nconstructs an oracle to identify optimal configurations. Grover's algorithm\nleverages this oracle to provide a quadratic reduction in search complexity.\nDemonstrated on 1-DoF, 2-DoF, and dual-arm manipulator tasks, the method\nachieves significant speedups-up to 93x over classical optimizers like Nelder\nMead as problem dimensionality increases. This work establishes a foundational,\nquantum-native framework for robot kinematic optimization, effectively bridging\nquantum computing and robotics problems.", "AI": {"tldr": "\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e0eGrover\u7b97\u6cd5\u7ed3\u5408\u7684\u91cf\u5b50\u539f\u751f\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u89e3\u51b3\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u4f18\u5316\u95ee\u9898\uff0c\u5728\u590d\u6742\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4e8c\u6b21\u641c\u7d22\u590d\u6742\u5ea6\u964d\u4f4e\uff0c\u76f8\u6bd4\u7ecf\u5178\u65b9\u6cd5\u6700\u9ad8\u8fbe\u523093\u500d\u52a0\u901f", "motivation": "\u4f18\u5316\u9ad8\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u9700\u8981\u5728\u590d\u6742\u9ad8\u7ef4\u914d\u7f6e\u7a7a\u95f4\u4e2d\u641c\u7d22\uff0c\u8fd9\u5bf9\u7ecf\u5178\u8ba1\u7b97\u65b9\u6cd5\u5177\u6709\u8ba1\u7b97\u6311\u6218\u6027\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5", "method": "\u4f7f\u7528\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u8bad\u7ec3\u524d\u5411\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u6784\u5efaoracle\u8bc6\u522b\u6700\u4f18\u914d\u7f6e\uff0c\u5229\u7528Grover\u7b97\u6cd5\u5b9e\u73b0\u641c\u7d22\u590d\u6742\u5ea6\u7684\u4e8c\u6b21\u964d\u4f4e", "result": "\u57281-DoF\u30012-DoF\u548c\u53cc\u81c2\u673a\u68b0\u81c2\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff0c\u968f\u7740\u95ee\u9898\u7ef4\u5ea6\u589e\u52a0\uff0c\u76f8\u6bd4Nelder Mead\u7b49\u7ecf\u5178\u4f18\u5316\u5668\u5b9e\u73b0\u6700\u9ad893\u500d\u7684\u52a0\u901f", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u57fa\u7840\u7684\u91cf\u5b50\u539f\u751f\u6846\u67b6\uff0c\u6709\u6548\u8fde\u63a5\u91cf\u5b50\u8ba1\u7b97\u548c\u673a\u5668\u4eba\u5b66\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.07239", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07239", "abs": "https://arxiv.org/abs/2509.07239", "authors": ["Max Asselmeier", "Abdel Zaro", "Dhruv Ahuja", "Ye Zhao", "Patricio A. Vela"], "title": "Safe Gap-based Planning in Dynamic Settings", "comment": "Accepted to Algorithms for Machine Vision in Navigation and Control -\n  Springer Publishing House", "summary": "This chapter extends the family of perception-informed gap-based local\nplanners to dynamic environments. Existing perception-informed local planners\nthat operate in dynamic environments often rely on emergent or empirical\nrobustness for collision avoidance as opposed to performing formal analysis of\ndynamic obstacles. This proposed planner, dynamic gap, explicitly addresses\ndynamic obstacles through several steps in the planning pipeline. First, polar\nregions of free space known as gaps are tracked and their dynamics are\nestimated in order to understand how the local environment evolves over time.\nThen, at planning time, gaps are propagated into the future through novel gap\npropagation algorithms to understand what regions are feasible for passage.\nLastly, pursuit guidance theory is leveraged to generate local trajectories\nthat are provably collision-free under ideal conditions. Additionally,\nobstacle-centric ungap processing is performed in situations where no gaps\nexist to robustify the overall planning framework. A set of gap-based planners\nare benchmarked against a series of classical and learned motion planners in\ndynamic environments, and dynamic gap is shown to outperform all other\nbaselines in all environments. Furthermore, dynamic gap is deployed on a\nTurtleBot2 platform in several real-world experiments to validate collision\navoidance behaviors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u52a8\u6001\u73af\u5883\u7684\u611f\u77e5\u4fe1\u606f\u95f4\u9699\u5c40\u90e8\u89c4\u5212\u5668dynamic gap\uff0c\u901a\u8fc7\u95f4\u9699\u8ddf\u8e2a\u3001\u52a8\u6001\u4f30\u8ba1\u3001\u95f4\u9699\u4f20\u64ad\u548c\u8ffd\u8e2a\u5236\u5bfc\u7406\u8bba\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u8bc1\u660e\u7684\u78b0\u649e\u907f\u514d", "motivation": "\u73b0\u6709\u52a8\u6001\u73af\u5883\u4e2d\u7684\u611f\u77e5\u4fe1\u606f\u5c40\u90e8\u89c4\u5212\u5668\u901a\u5e38\u4f9d\u8d56\u7ecf\u9a8c\u6027\u9c81\u68d2\u6027\u800c\u975e\u5bf9\u52a8\u6001\u969c\u788d\u7269\u7684\u5f62\u5f0f\u5316\u5206\u6790\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u52a8\u6001\u969c\u788d\u7269\u5904\u7406\u65b9\u6cd5", "method": "1) \u8ddf\u8e2a\u6781\u5750\u6807\u81ea\u7531\u7a7a\u95f4\u95f4\u9699\u5e76\u4f30\u8ba1\u5176\u52a8\u6001\uff1b2) \u901a\u8fc7\u65b0\u9896\u7684\u95f4\u9699\u4f20\u64ad\u7b97\u6cd5\u9884\u6d4b\u672a\u6765\u53ef\u884c\u533a\u57df\uff1b3) \u5229\u7528\u8ffd\u8e2a\u5236\u5bfc\u7406\u8bba\u751f\u6210\u7406\u60f3\u6761\u4ef6\u4e0b\u53ef\u8bc1\u660e\u65e0\u78b0\u649e\u7684\u5c40\u90e8\u8f68\u8ff9\uff1b4) \u5728\u65e0\u95f4\u9699\u60c5\u51b5\u4e0b\u6267\u884c\u969c\u788d\u7269\u4e2d\u5fc3\u5316\u5904\u7406", "result": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0cdynamic gap\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u4f20\u7edf\u548c\u5b66\u4e60\u578b\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u5e76\u5728TurtleBot2\u5e73\u53f0\u4e0a\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u78b0\u649e\u907f\u514d\u884c\u4e3a", "conclusion": "dynamic gap\u901a\u8fc7\u5f62\u5f0f\u5316\u5206\u6790\u52a8\u6001\u969c\u788d\u7269\u548c\u95f4\u9699\u4f20\u64ad\u7b97\u6cd5\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5c40\u90e8\u89c4\u5212\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.07321", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07321", "abs": "https://arxiv.org/abs/2509.07321", "authors": ["Casey D. Majhor", "Jeremy P. Bos"], "title": "Performance Characterization of a Point-Cloud-Based Path Planner in Off-Road Terrain", "comment": "This work has been published in the Journal of Field Robotics", "summary": "We present a comprehensive evaluation of a point-cloud-based navigation\nstack, MUONS, for autonomous off-road navigation. Performance is characterized\nby analyzing the results of 30,000 planning and navigation trials in simulation\nand validated through field testing. Our simulation campaign considers three\nkinematically challenging terrain maps and twenty combinations of seven\npath-planning parameters. In simulation, our MUONS-equipped AGV achieved a 0.98\nsuccess rate and experienced no failures in the field. By statistical and\ncorrelation analysis we determined that the Bi-RRT expansion radius used in the\ninitial planning stages is most correlated with performance in terms of\nplanning time and traversed path length. Finally, we observed that the\nproportional variation due to changes in the tuning parameters is remarkably\nwell correlated to performance in field testing. This finding supports the use\nof Monte-Carlo simulation campaigns for performance assessment and parameter\ntuning.", "AI": {"tldr": "MUONS\u70b9\u4e91\u5bfc\u822a\u7cfb\u7edf\u5728\u8d8a\u91ce\u81ea\u4e3b\u5bfc\u822a\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4eff\u771f\u6d4b\u8bd5\u6210\u529f\u738798%\uff0c\u5b9e\u5730\u6d4b\u8bd5\u96f6\u5931\u8d25\uff0c\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u53d1\u73b0Bi-RRT\u6269\u5c55\u534a\u5f84\u662f\u6700\u5173\u952e\u53c2\u6570", "motivation": "\u8bc4\u4f30\u70b9\u4e91\u5bfc\u822a\u7cfb\u7edf\u5728\u590d\u6742\u8d8a\u91ce\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u81ea\u4e3b\u5730\u9762\u8f66\u8f86(AGV)\u63d0\u4f9b\u53ef\u9760\u7684\u5bfc\u822a\u89e3\u51b3\u65b9\u6848", "method": "\u91c7\u752830,000\u6b21\u4eff\u771f\u8bd5\u9a8c\u548c\u5b9e\u5730\u6d4b\u8bd5\uff0c\u5206\u6790\u4e09\u79cd\u590d\u6742\u5730\u5f62\u56fe\u548c\u4e03\u4e2a\u8def\u5f84\u89c4\u5212\u53c2\u6570\u768420\u79cd\u7ec4\u5408\uff0c\u8fdb\u884c\u7edf\u8ba1\u548c\u76f8\u5173\u6027\u5206\u6790", "result": "\u4eff\u771f\u6210\u529f\u738798%\uff0c\u5b9e\u5730\u6d4b\u8bd5\u96f6\u5931\u8d25\uff0cBi-RRT\u6269\u5c55\u534a\u5f84\u4e0e\u89c4\u5212\u65f6\u95f4\u548c\u8def\u5f84\u957f\u5ea6\u76f8\u5173\u6027\u6700\u5f3a\uff0c\u53c2\u6570\u8c03\u6574\u4e0e\u5b9e\u5730\u6027\u80fd\u9ad8\u5ea6\u76f8\u5173", "conclusion": "\u8499\u7279\u5361\u6d1b\u4eff\u771f\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8e\u6027\u80fd\u8bc4\u4f30\u548c\u53c2\u6570\u8c03\u4f18\uff0c\u4e3a\u8d8a\u91ce\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u65b9\u6cd5\u8bba"}}
{"id": "2509.07362", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07362", "abs": "https://arxiv.org/abs/2509.07362", "authors": ["Yandi Yang", "Jianping Li", "Youqi Liao", "Yuhao Li", "Yizhe Zhang", "Zhen Dong", "Bisheng Yang", "Naser El-Sheimy"], "title": "Aerial-ground Cross-modal Localization: Dataset, Ground-truth, and Benchmark", "comment": null, "summary": "Accurate visual localization in dense urban environments poses a fundamental\ntask in photogrammetry, geospatial information science, and robotics. While\nimagery is a low-cost and widely accessible sensing modality, its effectiveness\non visual odometry is often limited by textureless surfaces, severe viewpoint\nchanges, and long-term drift. The growing public availability of airborne laser\nscanning (ALS) data opens new avenues for scalable and precise visual\nlocalization by leveraging ALS as a prior map. However, the potential of\nALS-based localization remains underexplored due to three key limitations: (1)\nthe lack of platform-diverse datasets, (2) the absence of reliable ground-truth\ngeneration methods applicable to large-scale urban environments, and (3)\nlimited validation of existing Image-to-Point Cloud (I2P) algorithms under\naerial-ground cross-platform settings. To overcome these challenges, we\nintroduce a new large-scale dataset that integrates ground-level imagery from\nmobile mapping systems with ALS point clouds collected in Wuhan, Hong Kong, and\nSan Francisco.", "AI": {"tldr": "\u57fa\u4e8e\u673a\u8f7d\u5149\u8f69\u626b\u63cf(ALS)\u70b9\u4e91\u7684\u7a7a\u4e2d-\u5730\u9762\u8de8\u5e73\u53f0\u89c6\u89c9\u5b9a\u4f4d\u65b0\u6570\u636e\u96c6", "motivation": "\u89e3\u51b3\u5bc6\u96c6\u57ce\u5e02\u73af\u5883\u4e2d\u56fe\u50cf\u5b9a\u4f4d\u7684\u6311\u6218\uff0c\u5982\u7f3a\u5c11\u7eb9\u7406\u8868\u9762\u3001\u4e25\u91cd\u89c6\u70b9\u53d8\u5316\u548c\u957f\u671f\u504f\u79fb\uff0c\u5229\u7528\u516c\u5f00\u7684ALS\u6570\u636e\u4f5c\u4e3a\u5148\u9a8c\u5730\u56fe", "method": "\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u6574\u5408\u6765\u81ea\u79fb\u52a8\u7ed8\u56fe\u7cfb\u7edf\u7684\u5730\u9762\u7ea7\u56fe\u50cf\u4e0e\u6b66\u6c49\u3001\u9999\u6e2f\u548c\u65e7\u91d1\u5c71\u7684ALS\u70b9\u4e91\u6570\u636e", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u5927\u89c4\u6a21\u8de8\u5e73\u53f0\u6570\u636e\u96c6\uff0c\u652f\u6301\u7a7a\u4e2d-\u5730\u9762\u8de8\u5e73\u53f0\u89c6\u89c9\u5b9a\u4f4d\u7b97\u6cd5\u7684\u9a8c\u8bc1", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u89e3\u51b3\u5bc6\u96c6\u57ce\u5e02\u89c6\u89c9\u5b9a\u4f4d\u7684\u4e09\u5927\u5173\u952e\u95ee\u9898\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdbALS\u57fa\u4e8e\u5b9a\u4f4d\u65b9\u6cd5\u7684\u7814\u7a76\u548c\u5e94\u7528"}}
{"id": "2509.07381", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07381", "abs": "https://arxiv.org/abs/2509.07381", "authors": ["Sichao Wu", "Jiang Wu", "Xingyu Cao", "Fawang Zhang", "Guangyuan Yu", "Junjie Zhao", "Yue Qu", "Fei Ma", "Jingliang Duan"], "title": "TransMPC: Transformer-based Explicit MPC with Variable Prediction Horizon", "comment": null, "summary": "Traditional online Model Predictive Control (MPC) methods often suffer from\nexcessive computational complexity, limiting their practical deployment.\nExplicit MPC mitigates online computational load by pre-computing control\npolicies offline; however, existing explicit MPC methods typically rely on\nsimplified system dynamics and cost functions, restricting their accuracy for\ncomplex systems. This paper proposes TransMPC, a novel Transformer-based\nexplicit MPC algorithm capable of generating highly accurate control sequences\nin real-time for complex dynamic systems. Specifically, we formulate the MPC\npolicy as an encoder-only Transformer leveraging bidirectional self-attention,\nenabling simultaneous inference of entire control sequences in a single forward\npass. This design inherently accommodates variable prediction horizons while\nensuring low inference latency. Furthermore, we introduce a direct policy\noptimization framework that alternates between sampling and learning phases.\nUnlike imitation-based approaches dependent on precomputed optimal\ntrajectories, TransMPC directly optimizes the true finite-horizon cost via\nautomatic differentiation. Random horizon sampling combined with a replay\nbuffer provides independent and identically distributed (i.i.d.) training\nsamples, ensuring robust generalization across varying states and horizon\nlengths. Extensive simulations and real-world vehicle control experiments\nvalidate the effectiveness of TransMPC in terms of solution accuracy,\nadaptability to varying horizons, and computational efficiency.", "AI": {"tldr": "TransMPC\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u663e\u5f0fMPC\u7b97\u6cd5\uff0c\u901a\u8fc7\u53cc\u5411\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u65f6\u751f\u6210\u9ad8\u7cbe\u5ea6\u63a7\u5236\u5e8f\u5217\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfMPC\u8ba1\u7b97\u590d\u6742\u548c\u73b0\u6709\u663e\u5f0fMPC\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5728\u7ebfMPC\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u5ea6\u8fc7\u9ad8\uff0c\u800c\u73b0\u6709\u663e\u5f0fMPC\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5316\u7684\u7cfb\u7edf\u52a8\u529b\u5b66\u548c\u6210\u672c\u51fd\u6570\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u7cbe\u5ea6\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u964d\u4f4e\u5728\u7ebf\u8ba1\u7b97\u8d1f\u62c5\u53c8\u80fd\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u7f16\u7801\u5668-only Transformer\u67b6\u6784\uff0c\u5229\u7528\u53cc\u5411\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u540c\u65f6\u63a8\u65ad\u6574\u4e2a\u63a7\u5236\u5e8f\u5217\u3002\u91c7\u7528\u76f4\u63a5\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5fae\u5206\u76f4\u63a5\u4f18\u5316\u771f\u5b9e\u6709\u9650\u65f6\u57df\u6210\u672c\uff0c\u7ed3\u5408\u968f\u673a\u65f6\u57df\u91c7\u6837\u548c\u56de\u653e\u7f13\u51b2\u533a\u786e\u4fdd\u8bad\u7ec3\u6837\u672c\u7684\u72ec\u7acb\u540c\u5206\u5e03\u3002", "result": "\u5927\u91cf\u4eff\u771f\u548c\u771f\u5b9e\u8f66\u8f86\u63a7\u5236\u5b9e\u9a8c\u9a8c\u8bc1\u4e86TransMPC\u5728\u89e3\u51b3\u65b9\u6848\u7cbe\u5ea6\u3001\u65f6\u57df\u9002\u5e94\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "TransMPC\u6210\u529f\u5730\u5c06Transformer\u67b6\u6784\u5e94\u7528\u4e8e\u663e\u5f0fMPC\uff0c\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u7684\u9ad8\u7cbe\u5ea6\u5b9e\u65f6\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f4e\u63a8\u7406\u5ef6\u8fdf\u548c\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.07412", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07412", "abs": "https://arxiv.org/abs/2509.07412", "authors": ["Zhen Tian", "Fujiang Yuan", "Yangfan He", "Qinghao Li", "Changlin Chen", "Huilin Chen", "Tianxiang Xu", "Jianyu Duan", "Yanhong Peng", "Zhihao Lin"], "title": "Attention and Risk-Aware Decision Framework for Safe Autonomous Driving", "comment": null, "summary": "Autonomous driving has attracted great interest due to its potential\ncapability in full-unsupervised driving. Model-based and learning-based methods\nare widely used in autonomous driving. Model-based methods rely on pre-defined\nmodels of the environment and may struggle with unforeseen events. Proximal\npolicy optimization (PPO), an advanced learning-based method, can adapt to the\nabove limits by learning from interactions with the environment. However,\nexisting PPO faces challenges with poor training results, and low training\nefficiency in long sequences. Moreover, the poor training results are\nequivalent to collisions in driving tasks. To solve these issues, this paper\ndevelops an improved PPO by introducing the risk-aware mechanism, a\nrisk-attention decision network, a balanced reward function, and a\nsafety-assisted mechanism. The risk-aware mechanism focuses on highlighting\nareas with potential collisions, facilitating safe-driving learning of the PPO.\nThe balanced reward function adjusts rewards based on the number of surrounding\nvehicles, promoting efficient exploration of the control strategy during\ntraining. Additionally, the risk-attention network enhances the PPO to hold\nchannel and spatial attention for the high-risk areas of input images.\nMoreover, the safety-assisted mechanism supervises and prevents the actions\nwith risks of collisions during the lane keeping and lane changing. Simulation\nresults on a physical engine demonstrate that the proposed algorithm\noutperforms benchmark algorithms in collision avoidance, achieving higher peak\nreward with less training time, and shorter driving time remaining on the risky\nareas among multiple testing traffic flow scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684PPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u98ce\u9669\u611f\u77e5\u673a\u5236\u3001\u98ce\u9669\u6ce8\u610f\u529b\u51b3\u7b56\u7f51\u7edc\u3001\u5e73\u8861\u5956\u52b1\u51fd\u6570\u548c\u5b89\u5168\u8f85\u52a9\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2dPPO\u7b97\u6cd5\u8bad\u7ec3\u6548\u679c\u5dee\u3001\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u610f\u5916\u4e8b\u4ef6\uff0c\u800c\u73b0\u6709\u7684PPO\u7b97\u6cd5\u5728\u957f\u5e8f\u5217\u8bad\u7ec3\u4e2d\u5b58\u5728\u8bad\u7ec3\u6548\u679c\u5dee\u3001\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u8bad\u7ec3\u6548\u679c\u5dee\u7b49\u540c\u4e8e\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u78b0\u649e\u98ce\u9669\u3002", "method": "\u5f00\u53d1\u6539\u8fdb\u7684PPO\u7b97\u6cd5\uff0c\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u98ce\u9669\u611f\u77e5\u673a\u5236\uff08\u7a81\u51fa\u6f5c\u5728\u78b0\u649e\u533a\u57df\uff09\u3001\u98ce\u9669\u6ce8\u610f\u529b\u51b3\u7b56\u7f51\u7edc\uff08\u5bf9\u9ad8\u98ce\u9669\u533a\u57df\u8fdb\u884c\u901a\u9053\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u5904\u7406\uff09\u3001\u5e73\u8861\u5956\u52b1\u51fd\u6570\uff08\u6839\u636e\u5468\u56f4\u8f66\u8f86\u6570\u91cf\u8c03\u6574\u5956\u52b1\uff09\u3001\u5b89\u5168\u8f85\u52a9\u673a\u5236\uff08\u5728\u8f66\u9053\u4fdd\u6301\u548c\u53d8\u9053\u65f6\u76d1\u7763\u548c\u9632\u6b62\u6709\u78b0\u649e\u98ce\u9669\u7684\u52a8\u4f5c\uff09\u3002", "result": "\u5728\u7269\u7406\u5f15\u64ce\u4e0a\u7684\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u78b0\u649e\u907f\u514d\u65b9\u9762\u4f18\u4e8e\u57fa\u51c6\u7b97\u6cd5\uff0c\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u5cf0\u503c\u5956\u52b1\u3001\u66f4\u5c11\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u4ee5\u53ca\u5728\u591a\u79cd\u6d4b\u8bd5\u4ea4\u901a\u6d41\u573a\u666f\u4e0b\u5728\u98ce\u9669\u533a\u57df\u505c\u7559\u65f6\u95f4\u66f4\u77ed\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6539\u8fdbPPO\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u4e3a\u89e3\u51b3PPO\u5728\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u4e2d\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.07413", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07413", "abs": "https://arxiv.org/abs/2509.07413", "authors": ["Yuhan Pang", "Bingyi Xia", "Zhe Zhang", "Zhirui Sun", "Peijia Xie", "Bike Zhu", "Wenjun Xu", "Jiankun Wang"], "title": "Robust Docking Maneuvers for Autonomous Trolley Collection: An Optimization-Based Visual Servoing Scheme", "comment": null, "summary": "Service robots have demonstrated significant potential for autonomous trolley\ncollection and redistribution in public spaces like airports or warehouses to\nimprove efficiency and reduce cost. Usually, a fully autonomous system for the\ncollection and transportation of multiple trolleys is based on a\nLeader-Follower formation of mobile manipulators, where reliable docking\nmaneuvers of the mobile base are essential to align trolleys into organized\nqueues. However, developing a vision-based robotic docking system faces\nsignificant challenges: high precision requirements, environmental\ndisturbances, and inherent robot constraints. To address these challenges, we\npropose an optimization-based Visual Servoing scheme that incorporates active\ninfrared markers for robust feature extraction across diverse lighting\nconditions. This framework explicitly models nonholonomic kinematics and\nvisibility constraints within the Hybrid Visual Servoing problem, augmented\nwith an observer for disturbance rejection to ensure precise and stable\ndocking. Experimental results across diverse environments demonstrate the\nrobustness of this system, with quantitative evaluations confirming high\ndocking accuracy.", "AI": {"tldr": "\u57fa\u4e8e\u89c6\u89c9\u670d\u52a1\u7684\u4f18\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u4e3b\u52a8\u7ea2\u5916\u6807\u8bb0\u548c\u5e72\u6270\u89c2\u6d4b\u5668\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u65e0\u4eba\u63a7\u5c0f\u8f66\u5bf9\u63a5", "motivation": "\u89e3\u51b3\u670d\u52a1\u673a\u5668\u4eba\u5728\u516c\u5171\u7a7a\u95f4\u81ea\u4e3b\u6536\u96c6\u548c\u91cd\u65b0\u5206\u914d\u624b\u63d0\u5c0f\u8f66\u65f6\u7684\u5bf9\u63a5\u6311\u6218\uff0c\u5305\u62ec\u9ad8\u7cbe\u5ea6\u8981\u6c42\u3001\u73af\u5883\u5e72\u6270\u548c\u673a\u5668\u4eba\u672c\u8eab\u7ea6\u675f", "method": "\u91c7\u7528\u4f18\u5316\u57fa\u4e89\u89c6\u89c9\u670d\u52a1\u65b9\u6848\uff0c\u7ed3\u5408\u4e3b\u52a8\u7ea2\u5916\u6807\u8bb0\u8fdb\u884c\u7a81\u51fa\u7279\u5f81\u63d0\u53d6\uff0c\u5728\u6df7\u5408\u89c6\u89c9\u670d\u52a1\u95ee\u9898\u4e2d\u663e\u5f0f\u5efa\u6a21\u975e\u5b8c\u6574\u52a8\u529b\u5b66\u548c\u53ef\u89c1\u6027\u7ea6\u675f\uff0c\u52a0\u5165\u5e72\u6270\u89c2\u6d4b\u5668\u63d0\u9ad8\u7a33\u5b9a\u6027", "result": "\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u7a33\u5065\u6027\uff0c\u5b9a\u91cf\u8bc4\u4f30\u786e\u8ba4\u4e86\u9ad8\u5bf9\u63a5\u7cbe\u5ea6", "conclusion": "\u8be5\u65b9\u6848\u80fd\u591f\u6709\u6548\u89e3\u51b3\u670d\u52a1\u673a\u5668\u4eba\u5bf9\u63a5\u624b\u63d0\u5c0f\u8f66\u7684\u6280\u672f\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u7a33\u5b9a\u7684\u81ea\u4e3b\u5bf9\u63a5\u529f\u80fd"}}
{"id": "2509.07438", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.07438", "abs": "https://arxiv.org/abs/2509.07438", "authors": ["Ya-Chuan Hsu", "Jonathan DeCastro", "Andrew Silva", "Guy Rosman"], "title": "Timing the Message: Language-Based Notifications for Time-Critical Assistive Settings", "comment": null, "summary": "In time-critical settings such as assistive driving, assistants often rely on\nalerts or haptic signals to prompt rapid human attention, but these cues\nusually leave humans to interpret situations and decide responses\nindependently, introducing potential delays or ambiguity in meaning.\nLanguage-based assistive systems can instead provide instructions backed by\ncontext, offering more informative guidance. However, current approaches (e.g.,\nsocial assistive robots) largely prioritize content generation while\noverlooking critical timing factors such as verbal conveyance duration, human\ncomprehension delays, and subsequent follow-through duration. These timing\nconsiderations are crucial in time-critical settings, where even minor delays\ncan substantially affect outcomes. We aim to study this inherent trade-off\nbetween timeliness and informativeness by framing the challenge as a sequential\ndecision-making problem using an augmented-state Markov Decision Process. We\ndesign a framework combining reinforcement learning and a generated offline\ntaxonomy dataset, where we balance the trade-off while enabling a scalable\ntaxonomy dataset generation pipeline. Empirical evaluation with synthetic\nhumans shows our framework improves success rates by over 40% compared to\nmethods that ignore time delays, while effectively balancing timeliness and\ninformativeness. It also exposes an often-overlooked trade-off between these\ntwo factors, opening new directions for optimizing communication in\ntime-critical human-AI assistance.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u65f6\u95f4\u6545\u611f\u8bbe\u5907\u4e2d\u8bed\u8a00\u6307\u4ee4\u7684\u53ca\u65f6\u6027\u4e0e\u4fe1\u606f\u91cf\u7684\u6279\u6263\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u63d0\u9ad840%\u6210\u529f\u7387", "motivation": "\u73b0\u6709\u8bed\u8a00\u8f85\u52a9\u7cfb\u7edf\u8fc7\u4e8e\u5173\u6ce8\u5185\u5bb9\u751f\u6210\u800c\u5ffd\u89c6\u4e86\u4e34\u754c\u65f6\u95f4\u56e0\u7d20\uff0c\u5982\u8bed\u8a00\u4f20\u8fbe\u65f6\u957f\u3001\u4eba\u7c7b\u7406\u89e3\u5ef6\u8fdf\u7b49\uff0c\u8fd9\u5728\u65f6\u95f4\u6545\u611f\u573a\u666f\u4e2d\u5bfc\u81f4\u5ef6\u8fdf\u6216\u610f\u4e49\u6a21\u7cca", "method": "\u4f7f\u7528\u589e\u5f3a\u72b6\u6001\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5efa\u6a21\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u751f\u6210\u7684\u79bb\u7ebf\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u53ef\u6269\u5c55\u7684\u5206\u7c7b\u6570\u636e\u751f\u6210\u6d41\u7a0b", "result": "\u901a\u8fc7\u7efc\u5408\u4eba\u7c7b\u6a21\u62df\u8bc4\u4f30\uff0c\u6846\u67b6\u6bd4\u5ffd\u89c6\u65f6\u95f4\u5ef6\u8fdf\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8d8510%\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u6709\u6548\u5e73\u8861\u4e86\u53ca\u65f6\u6027\u548c\u4fe1\u606f\u91cf", "conclusion": "\u8be5\u7814\u7a76\u66dd\u9732\u4e86\u8bed\u8a00\u8f85\u52a9\u4e2d\u53ca\u65f6\u6027\u4e0e\u4fe1\u606f\u91cf\u7684\u91cd\u8981\u6279\u6263\u5173\u7cfb\uff0c\u4e3a\u4f18\u5316\u65f6\u95f4\u6545\u611f\u573a\u666f\u4e2d\u7684\u4eba\u5de5\u667a\u80fd\u6c9f\u901a\u5f00\u542f\u4e86\u65b0\u65b9\u5411"}}
{"id": "2509.07445", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07445", "abs": "https://arxiv.org/abs/2509.07445", "authors": ["Harrison Field", "Max Yang", "Yijiong Lin", "Efi Psomopoulou", "David Barton", "Nathan F. Lepora"], "title": "Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward Functions", "comment": "Accepted at CoRL 2025", "summary": "Large language models (LLMs) are beginning to automate reward design for\ndexterous manipulation. However, no prior work has considered tactile sensing,\nwhich is known to be critical for human-like dexterity. We present Text2Touch,\nbringing LLM-crafted rewards to the challenging task of multi-axis in-hand\nobject rotation with real-world vision based tactile sensing in palm-up and\npalm-down configurations. Our prompt engineering strategy scales to over 70\nenvironment variables, and sim-to-real distillation enables successful policy\ntransfer to a tactile-enabled fully actuated four-fingered dexterous robot\nhand. Text2Touch significantly outperforms a carefully tuned human-engineered\nbaseline, demonstrating superior rotation speed and stability while relying on\nreward functions that are an order of magnitude shorter and simpler. These\nresults illustrate how LLM-designed rewards can significantly reduce the time\nfrom concept to deployable dexterous tactile skills, supporting more rapid and\nscalable multimodal robot learning. Project website:\nhttps://hpfield.github.io/text2touch-website", "AI": {"tldr": "Text2Touch\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\uff0c\u5728\u5177\u6709\u89e6\u89c9\u4f20\u611f\u7684\u591a\u8f74\u624b\u5185\u7269\u4f53\u65cb\u8f6c\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u57fa\u51c6\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u65cb\u8f6c\u901f\u5ea6\u548c\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5956\u52b1\u8bbe\u8ba1\u5de5\u4f5c\u672a\u8003\u8651\u89e6\u89c9\u4f20\u611f\uff0c\u800c\u89e6\u89c9\u5bf9\u4e8e\u7c7b\u4eba\u7075\u5de7\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u5904\u740670\u591a\u4e2a\u73af\u5883\u53d8\u91cf\uff0c\u901a\u8fc7\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u84b8\u998f\u5c06\u7b56\u7565\u8fc1\u79fb\u5230\u5177\u6709\u89e6\u89c9\u4f20\u611f\u7684\u56db\u6307\u7075\u5de7\u673a\u68b0\u624b\u3002", "result": "Text2Touch\u663e\u8457\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u57fa\u51c6\uff0c\u65cb\u8f6c\u901f\u5ea6\u66f4\u5feb\u3001\u7a33\u5b9a\u6027\u66f4\u597d\uff0c\u4e14\u5956\u52b1\u51fd\u6570\u66f4\u7b80\u6d01\uff08\u77ed\u4e00\u4e2a\u6570\u91cf\u7ea7\uff09\u3002", "conclusion": "LLM\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u53ef\u4ee5\u663e\u8457\u7f29\u77ed\u4ece\u6982\u5ff5\u5230\u53ef\u90e8\u7f72\u7075\u5de7\u89e6\u89c9\u6280\u80fd\u7684\u65f6\u95f4\uff0c\u652f\u6301\u66f4\u5feb\u901f\u548c\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u673a\u5668\u4eba\u5b66\u4e60\u3002"}}
{"id": "2509.07463", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.07463", "abs": "https://arxiv.org/abs/2509.07463", "authors": ["Sven Kirchner", "Nils Purschke", "Ross Greer", "Alois C. Knoll"], "title": "DepthVision: Robust Vision-Language Understanding through GAN-Based LiDAR-to-RGB Synthesis", "comment": null, "summary": "Ensuring reliable robot operation when visual input is degraded or\ninsufficient remains a central challenge in robotics. This letter introduces\nDepthVision, a framework for multimodal scene understanding designed to address\nthis problem. Unlike existing Vision-Language Models (VLMs), which use only\ncamera-based visual input alongside language, DepthVision synthesizes RGB\nimages from sparse LiDAR point clouds using a conditional generative\nadversarial network (GAN) with an integrated refiner network. These synthetic\nviews are then combined with real RGB data using a Luminance-Aware Modality\nAdaptation (LAMA), which blends the two types of data dynamically based on\nambient lighting conditions. This approach compensates for sensor degradation,\nsuch as darkness or motion blur, without requiring any fine-tuning of\ndownstream vision-language models. We evaluate DepthVision on real and\nsimulated datasets across various models and tasks, with particular attention\nto safety-critical tasks. The results demonstrate that our approach improves\nperformance in low-light conditions, achieving substantial gains over RGB-only\nbaselines while preserving compatibility with frozen VLMs. This work highlights\nthe potential of LiDAR-guided RGB synthesis for achieving robust robot\noperation in real-world environments.", "AI": {"tldr": "DepthVision\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u573a\u666f\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u4ece\u7a00\u758fLiDAR\u70b9\u4e91\u5408\u6210RGB\u56fe\u50cf\uff0c\u5e76\u7ed3\u5408\u771f\u5b9eRGB\u6570\u636e\uff0c\u5728\u5149\u7ebf\u4e0d\u8db3\u6761\u4ef6\u4e0b\u63d0\u5347\u673a\u5668\u4eba\u89c6\u89c9\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u89c6\u89c9\u8f93\u5165\u9000\u5316\u6216\u4e0d\u8db3\u65f6\u7684\u53ef\u9760\u64cd\u4f5c\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f4e\u5149\u7167\u7b49\u6076\u52a3\u6761\u4ef6\u4e0b\u786e\u4fdd\u5b89\u5168\u5173\u952e\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc(cGAN)\u4eceLiDAR\u70b9\u4e91\u5408\u6210RGB\u56fe\u50cf\uff0c\u901a\u8fc7Luminance-Aware Modality Adaptation(LAMA)\u52a8\u6001\u878d\u5408\u5408\u6210\u56fe\u50cf\u548c\u771f\u5b9eRGB\u6570\u636e\uff0c\u65e0\u9700\u5fae\u8c03\u4e0b\u6e38\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728\u771f\u5b9e\u548c\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cDepthVision\u5728\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528RGB\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u51bb\u7ed3VLMs\u7684\u517c\u5bb9\u6027\u3002", "conclusion": "LiDAR\u5f15\u5bfc\u7684RGB\u5408\u6210\u65b9\u6cd5\u4e3a\u5b9e\u73b0\u73b0\u5b9e\u73af\u5883\u4e2d\u9c81\u68d2\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.07464", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.07464", "abs": "https://arxiv.org/abs/2509.07464", "authors": ["Rui Yang", "Lei Zheng", "Shuzhi Sam Ge", "Jun Ma"], "title": "Safe and Non-Conservative Contingency Planning for Autonomous Vehicles via Online Learning-Based Reachable Set Barriers", "comment": "16 pages, 13 figures", "summary": "Autonomous vehicles must navigate dynamically uncertain environments while\nbalancing the safety and driving efficiency. This challenge is exacerbated by\nthe unpredictable nature of surrounding human-driven vehicles (HVs) and\nperception inaccuracies, which require planners to adapt to evolving\nuncertainties while maintaining safe trajectories. Overly conservative planners\ndegrade driving efficiency, while deterministic approaches may encounter\nserious issues and risks of failure when faced with sudden and unexpected\nmaneuvers. To address these issues, we propose a real-time contingency\ntrajectory optimization framework in this paper. By employing event-triggered\nonline learning of HV control-intent sets, our method dynamically quantifies\nmulti-modal HV uncertainties and refines the forward reachable set (FRS)\nincrementally. Crucially, we enforce invariant safety through FRS-based barrier\nconstraints that ensure safety without reliance on accurate trajectory\nprediction of HVs. These constraints are embedded in contingency trajectory\noptimization and solved efficiently through consensus alternative direction\nmethod of multipliers (ADMM). The system continuously adapts to the\nuncertainties in HV behaviors, preserving feasibility and safety without\nresorting to excessive conservatism. High-fidelity simulations on highway and\nurban scenarios, as well as a series of real-world experiments demonstrate\nsignificant improvements in driving efficiency and passenger comfort while\nmaintaining safety under uncertainty. The project page is available at\nhttps://pathetiue.github.io/frscp.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u5b9e\u65f6\u5e94\u6025\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\u63a7\u5236\u610f\u56fe\u96c6\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u7528\u57fa\u4e8e\u524d\u5411\u53ef\u8fbe\u96c6\u7684\u5c4f\u969c\u7ea6\u675f\u786e\u4fdd\u5b89\u5168\uff0c\u5728\u4fdd\u6301\u5b89\u5168\u7684\u540c\u65f6\u63d0\u9ad8\u9a7e\u9a76\u6548\u7387\u548c\u8212\u9002\u5ea6\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9700\u8981\u5728\u52a8\u6001\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u5e73\u8861\u5b89\u5168\u6027\u548c\u9a7e\u9a76\u6548\u7387\uff0c\u4f46\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u548c\u611f\u77e5\u4e0d\u51c6\u786e\u6027\u7ed9\u89c4\u5212\u5e26\u6765\u6311\u6218\u3002\u8fc7\u4e8e\u4fdd\u5b88\u7684\u89c4\u5212\u5668\u4f1a\u964d\u4f4e\u6548\u7387\uff0c\u800c\u786e\u5b9a\u6027\u65b9\u6cd5\u5728\u9762\u5bf9\u7a81\u53d1\u60c5\u51b5\u65f6\u53ef\u80fd\u5931\u8d25\u3002", "method": "\u91c7\u7528\u4e8b\u4ef6\u89e6\u53d1\u7684\u5728\u7ebf\u5b66\u4e60\u673a\u5236\u52a8\u6001\u91cf\u5316\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\u7684\u591a\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u7528\u57fa\u4e8e\u524d\u5411\u53ef\u8fbe\u96c6\u7684\u5c4f\u969c\u7ea6\u675f\u786e\u4fdd\u5b89\u5168\u4e0d\u53d8\u6027\uff0c\u901a\u8fc7\u5171\u8bc6ADMM\u7b97\u6cd5\u9ad8\u6548\u6c42\u89e3\u5e94\u6025\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\u3002", "result": "\u5728\u9ad8\u901f\u516c\u8def\u548c\u57ce\u5e02\u573a\u666f\u7684\u9ad8\u4fdd\u771f\u4eff\u771f\u4ee5\u53ca\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5b89\u5168\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u9a7e\u9a76\u6548\u7387\u548c\u4e58\u5ba2\u8212\u9002\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6301\u7eed\u9002\u5e94\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\u884c\u4e3a\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u4e0d\u4f9d\u8d56\u8fc7\u5ea6\u4fdd\u5b88\u7b56\u7565\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u53ef\u884c\u6027\u548c\u5b89\u5168\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.07496", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07496", "abs": "https://arxiv.org/abs/2509.07496", "authors": ["Ayano Miyamichi", "Moju Zhao", "Kazuki Sugihara", "Junichiro Sugihara", "Masanori Konishi", "Kunio Kojima", "Kei Okada", "Masayuki Inaba"], "title": "Flexible Morphing Aerial Robot with Inflatable Structure for Perching-based Human-Robot Interaction", "comment": null, "summary": "Birds in nature perform perching not only for rest but also for interaction\nwith human such as the relationship with falconers. Recently, researchers\nachieve perching-capable aerial robots as a way to save energy, and deformable\nstructure demonstrate significant advantages in efficiency of perching and\ncompactness of configuration. However, ensuring flight stability remains\nchallenging for deformable aerial robots due to the difficulty of controlling\nflexible arms. Furthermore, perching for human interaction requires high\ncompliance along with safety. Thus, this study aims to develop a deformable\naerial robot capable of perching on humans with high flexibility and grasping\nability. To overcome the challenges of stability of both flight and perching,\nwe propose a hybrid morphing structure that combines a unilateral flexible arm\nand a pneumatic inflatable actuators. This design allows the robot's arms to\nremain rigid during flight and soft while perching for more effective grasping.\nWe also develop a pneumatic control system that optimizes pressure regulation\nwhile integrating shock absorption and adjustable grasping forces, enhancing\ninteraction capabilities and energy efficiency. Besides, we focus on the\nstructural characteristics of the unilateral flexible arm and identify\nsufficient conditions under which standard quadrotor modeling and control\nremain effective in terms of flight stability. Finally, the developed prototype\ndemonstrates the feasibility of compliant perching maneuvers on humans, as well\nas the robust recovery even after arm deformation caused by thrust reductions\nduring flight. To the best of our knowledge, this work is the first to achieve\nan aerial robot capable of perching on humans for interaction.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u4ee5\u5728\u4eba\u4f53\u4e0a\u6816\u606f\u7684\u53d8\u5f62\u822a\u7a7a\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u6df7\u5408\u53d8\u5f62\u7ed3\u6784\u548c\u6c14\u52a8\u63a7\u5236\u7cfb\u7edf\u5b9e\u73b0\u4e86\u98de\u884c\u7a33\u5b9a\u6027\u548c\u6816\u606f\u6548\u679c\u7684\u5e73\u8861\u3002", "motivation": "\u4f5c\u8005\u60f3\u89e3\u51b3\u53ef\u53d8\u5f62\u822a\u7a7a\u673a\u5668\u4eba\u5728\u98de\u884c\u7a33\u5b9a\u6027\u548c\u4eba\u673a\u4ea4\u4e92\u5b89\u5168\u6027\u65b9\u9762\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5b9e\u73b0\u5728\u4eba\u4f53\u4e0a\u5b89\u5168\u6816\u606f\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u6df7\u5408\u53d8\u5f62\u7ed3\u6784\uff0c\u7ed3\u5408\u5355\u4fa7\u7075\u6d3b\u81c2\u548c\u6c14\u52a8\u80fd\u6269\u5f20\u5668\uff0c\u5728\u98de\u884c\u65f6\u4fdd\u6301\u7cbe\u786e\u63a7\u5236\u800c\u5728\u6816\u606f\u65f6\u53d8\u5f97\u67d4\u67d4\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u6c14\u52a8\u63a7\u5236\u7cfb\u7edf\u6765\u4f18\u5316\u538b\u529b\u8c03\u8282\u3001\u5438\u9707\u548c\u6293\u53d6\u529b\u63a7\u5236\u3002", "result": "\u539f\u578b\u673a\u6210\u529f\u5c55\u793a\u4e86\u5728\u4eba\u4f53\u4e0a\u8fdb\u884c\u67d4\u6027\u6816\u606f\u7684\u53ef\u884c\u6027\uff0c\u751a\u81f3\u5728\u98de\u884c\u4e2d\u53d7\u5230\u63a8\u529b\u51cf\u5c11\u5bfc\u81f4\u81c2\u90e8\u53d8\u5f62\u540e\u4ecd\u80fd\u7a33\u5b9a\u6062\u590d\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u80fd\u591f\u5728\u4eba\u4f53\u4e0a\u6816\u606f\u4ee5\u652f\u6301\u4eba\u673a\u4ea4\u4e92\u7684\u822a\u7a7a\u673a\u5668\u4eba\uff0c\u4e3a\u53ef\u53d8\u5f62\u822a\u7a7a\u673a\u5668\u4eba\u7684\u5b89\u5168\u4eba\u673a\u4ea4\u4e92\u5f00\u542f\u4e86\u65b0\u53ef\u80fd\u6027\u3002"}}
{"id": "2509.07500", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07500", "abs": "https://arxiv.org/abs/2509.07500", "authors": ["Yinan Deng", "Yufeng Yue", "Jianyu Dou", "Jingyu Zhao", "Jiahui Wang", "Yujie Tang", "Yi Yang", "Mengyin Fu"], "title": "OmniMap: A General Mapping Framework Integrating Optics, Geometry, and Semantics", "comment": "Accepted by IEEE Transactions on Robotics (TRO), project website:\n  https://omni-map.github.io/", "summary": "Robotic systems demand accurate and comprehensive 3D environment perception,\nrequiring simultaneous capture of photo-realistic appearance (optical), precise\nlayout shape (geometric), and open-vocabulary scene understanding (semantic).\nExisting methods typically achieve only partial fulfillment of these\nrequirements while exhibiting optical blurring, geometric irregularities, and\nsemantic ambiguities. To address these challenges, we propose OmniMap. Overall,\nOmniMap represents the first online mapping framework that simultaneously\ncaptures optical, geometric, and semantic scene attributes while maintaining\nreal-time performance and model compactness. At the architectural level,\nOmniMap employs a tightly coupled 3DGS-Voxel hybrid representation that\ncombines fine-grained modeling with structural stability. At the implementation\nlevel, OmniMap identifies key challenges across different modalities and\nintroduces several innovations: adaptive camera modeling for motion blur and\nexposure compensation, hybrid incremental representation with normal\nconstraints, and probabilistic fusion for robust instance-level understanding.\nExtensive experiments show OmniMap's superior performance in rendering\nfidelity, geometric accuracy, and zero-shot semantic segmentation compared to\nstate-of-the-art methods across diverse scenes. The framework's versatility is\nfurther evidenced through a variety of downstream applications, including\nmulti-domain scene Q&A, interactive editing, perception-guided manipulation,\nand map-assisted navigation.", "AI": {"tldr": "OmniMap\u662f\u9996\u4e2a\u80fd\u591f\u540c\u65f6\u6355\u83b7\u5149\u5b66\u3001\u51e0\u4f55\u548c\u8bed\u4e49\u573a\u666f\u5c5e\u6027\uff0c\u5e76\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u548c\u6a21\u578b\u7d27\u51d1\u6027\u7684\u5728\u7ebf\u5efa\u56fe\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u80fd\u90e8\u5206\u6ee1\u8db3\u673a\u5668\u4eba\u7cfb\u7edf\u5bf9\u73af\u5883\u611f\u77e5\u7684\u9700\u6c42\uff0c\u5b58\u5728\u5149\u5b66\u6a21\u7cca\u3001\u51e0\u4f55\u4e0d\u89c4\u5219\u548c\u8bed\u4e49\u6a21\u7cca\u7b49\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7d27\u5bc6\u8026\u5408\u76843DGS-Voxel\u6df7\u5408\u8868\u793a\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u76f8\u673a\u5efa\u6a21\u3001\u6df7\u5408\u589e\u91cf\u8868\u793a\u548c\u6982\u7387\u878d\u5408\u7b49\u521b\u65b0\u6280\u672f\u3002", "result": "\u5728\u6e32\u67d3\u4fdd\u771f\u5ea6\u3001\u51e0\u4f55\u7cbe\u5ea6\u548c\u96f6\u6837\u672c\u8bed\u4e49\u5206\u5272\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "OmniMap\u6846\u67b6\u5177\u6709\u591a\u529f\u80fd\u6027\uff0c\u53ef\u5e94\u7528\u4e8e\u591a\u9886\u57df\u573a\u666f\u95ee\u7b54\u3001\u4ea4\u4e92\u5f0f\u7f16\u8f91\u3001\u611f\u77e5\u5f15\u5bfc\u64cd\u4f5c\u548c\u5730\u56fe\u8f85\u52a9\u5bfc\u822a\u7b49\u4e0b\u6e38\u5e94\u7528\u3002"}}
{"id": "2509.07542", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07542", "abs": "https://arxiv.org/abs/2509.07542", "authors": ["Bartlomiej Kulecki", "Dominik Belter"], "title": "Improving Machine Learning-Based Robot Self-Collision Checking with Input Positional Encoding", "comment": null, "summary": "This manuscript investigates the integration of positional encoding -- a\ntechnique widely used in computer graphics -- into the input vector of a binary\nclassification model for self-collision detection. The results demonstrate the\nbenefits of incorporating positional encoding, which enhances classification\naccuracy by enabling the model to better capture high-frequency variations,\nleading to a more detailed and precise representation of complex collision\npatterns. The manuscript shows that machine learning-based techniques, such as\nlightweight multilayer perceptrons (MLPs) operating in a low-dimensional\nfeature space, offer a faster alternative for collision checking than\ntraditional methods that rely on geometric approaches, such as\ntriangle-to-triangle intersection tests and Bounding Volume Hierarchies (BVH)\nfor mesh-based models.", "AI": {"tldr": "\u5c06\u4f4d\u7f6e\u7f16\u7801\u6280\u672f\u96c6\u6210\u5230\u81ea\u78b0\u649e\u68c0\u6d4b\u7684\u4e8c\u5143\u5206\u7c7b\u6a21\u578b\u4e2d\uff0c\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u4f7f\u6a21\u578b\u80fd\u66f4\u597d\u5730\u6355\u6349\u9ad8\u9891\u53d8\u5316\uff0c\u4e3a\u590d\u6742\u78b0\u649e\u6a21\u5f0f\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u8868\u793a", "motivation": "\u63a2\u7d22\u4f4d\u7f6e\u7f16\u7801\u6280\u672f\u5728\u81ea\u78b0\u649e\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u5bfb\u6c42\u6bd4\u4f20\u7edf\u51e0\u4f55\u65b9\u6cd5\u66f4\u5feb\u901f\u7684\u78b0\u649e\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u591a\u5c42\u611f\u77e5\u5668(MLP)\u5728\u4f4e\u7ef4\u7279\u5f81\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u5c06\u4f4d\u7f6e\u7f16\u7801\u6280\u672f\u96c6\u6210\u5230\u4e8c\u5143\u5206\u7c7b\u6a21\u578b\u7684\u8f93\u5165\u5411\u91cf\u4e2d", "result": "\u96c6\u6210\u4f4d\u7f6e\u7f16\u7801\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u6a21\u578b\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u9ad8\u9891\u53d8\u5316\uff0c\u63d0\u4f9b\u66f4\u8be6\u7ec6\u548c\u7cbe\u786e\u7684\u590d\u6742\u78b0\u649e\u6a21\u5f0f\u8868\u793a", "conclusion": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u8f7b\u91cf\u7ea7MLP\u65b9\u6cd5\u5728\u81ea\u78b0\u649e\u68c0\u6d4b\u4e2d\u6bd4\u4f20\u7edf\u7684\u51e0\u4f55\u65b9\u6cd5\uff08\u5982\u4e09\u89d2\u5f62\u76f8\u4ea4\u6d4b\u8bd5\u548cBVH\uff09\u66f4\u5feb\u901f\u6709\u6548"}}
{"id": "2509.07593", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.SY", "eess.IV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.07593", "abs": "https://arxiv.org/abs/2509.07593", "authors": ["Gavin Tao", "Yinuo Wang", "Jinzhao Zhou"], "title": "Can SSD-Mamba2 Unlock Reinforcement Learning for End-to-End Motion Control?", "comment": "4 figures and 6 tables", "summary": "End-to-end reinforcement learning for motion control promises unified\nperception-action policies that scale across embodiments and tasks, yet most\ndeployed controllers are either blind (proprioception-only) or rely on fusion\nbackbones with unfavorable compute-memory trade-offs. Recurrent controllers\nstruggle with long-horizon credit assignment, and Transformer-based fusion\nincurs quadratic cost in token length, limiting temporal and spatial context.\nWe present a vision-driven cross-modal RL framework built on SSD-Mamba2, a\nselective state-space backbone that applies state-space duality (SSD) to enable\nboth recurrent and convolutional scanning with hardware-aware streaming and\nnear-linear scaling. Proprioceptive states and exteroceptive observations\n(e.g., depth tokens) are encoded into compact tokens and fused by stacked\nSSD-Mamba2 layers. The selective state-space updates retain long-range\ndependencies with markedly lower latency and memory use than quadratic\nself-attention, enabling longer look-ahead, higher token resolution, and stable\ntraining under limited compute. Policies are trained end-to-end under curricula\nthat randomize terrain and appearance and progressively increase scene\ncomplexity. A compact, state-centric reward balances task progress, energy\nefficiency, and safety. Across diverse motion-control scenarios, our approach\nconsistently surpasses strong state-of-the-art baselines in return, safety\n(collisions and falls), and sample efficiency, while converging faster at the\nsame compute budget. These results suggest that SSD-Mamba2 provides a practical\nfusion backbone for scalable, foresightful, and efficient end-to-end motion\ncontrol.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8eSSD-Mamba2\u7684\u89c6\u89c9\u9a71\u52a8\u8de8\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u7aef\u5230\u7aef\u8fd0\u52a8\u63a7\u5236\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u65b9\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u8fd0\u52a8\u63a7\u5236\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u5185\u5b58\u6743\u8861\u4e0d\u4f73\u3001\u957f\u65f6\u7a0b\u4fe1\u7528\u5206\u914d\u56f0\u96be\u3001Transformer\u4e8c\u6b21\u8ba1\u7b97\u6210\u672c\u9ad8\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u878d\u5408\u67b6\u6784", "method": "\u4f7f\u7528SSD-Mamba2\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u9aa8\u5e72\u7f51\u7edc\uff0c\u652f\u6301\u5faa\u73af\u548c\u5377\u79ef\u626b\u63cf\uff0c\u5c06\u672c\u4f53\u611f\u77e5\u548c\u5916\u90e8\u611f\u77e5\u7f16\u7801\u4e3a\u7d27\u51d1token\u8fdb\u884c\u878d\u5408\uff0c\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u548c\u72b6\u6001\u4e2d\u5fc3\u5956\u52b1\u51fd\u6570", "result": "\u5728\u591a\u6837\u5316\u8fd0\u52a8\u63a7\u5236\u573a\u666f\u4e2d\uff0c\u5728\u56de\u62a5\u7387\u3001\u5b89\u5168\u6027\u3001\u6837\u672c\u6548\u7387\u65b9\u9762\u8d85\u8d8a\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6536\u655b\u66f4\u5feb\u4e14\u8ba1\u7b97\u6210\u672c\u76f8\u540c", "conclusion": "SSD-Mamba2\u4e3a\u53ef\u6269\u5c55\u3001\u6709\u524d\u77bb\u6027\u4e14\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u8fd0\u52a8\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u878d\u5408\u9aa8\u5e72\u7f51\u7edc"}}
{"id": "2509.07646", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07646", "abs": "https://arxiv.org/abs/2509.07646", "authors": ["Yanlong Peng", "Zhigang Wang", "Ziwen He", "Pengxu Chang", "Chuangchuang Zhou", "Yu Yan", "Ming Chen"], "title": "Decoding RobKiNet: Insights into Efficient Training of Robotic Kinematics Informed Neural Network", "comment": null, "summary": "In robots task and motion planning (TAMP), it is crucial to sample within the\nrobot's configuration space to meet task-level global constraints and enhance\nthe efficiency of subsequent motion planning. Due to the complexity of joint\nconfiguration sampling under multi-level constraints, traditional methods often\nlack efficiency. This paper introduces the principle of RobKiNet, a\nkinematics-informed neural network, for end-to-end sampling within the\nContinuous Feasible Set (CFS) under multiple constraints in configuration\nspace, establishing its Optimization Expectation Model. Comparisons with\ntraditional sampling and learning-based approaches reveal that RobKiNet's\nkinematic knowledge infusion enhances training efficiency by ensuring stable\nand accurate gradient optimization.Visualizations and quantitative analyses in\na 2-DOF space validate its theoretical efficiency, while its application on a\n9-DOF autonomous mobile manipulator robot(AMMR) demonstrates superior\nwhole-body and decoupled control, excelling in battery disassembly tasks.\nRobKiNet outperforms deep reinforcement learning with a training speed 74.29\ntimes faster and a sampling accuracy of up to 99.25%, achieving a 97.33% task\ncompletion rate in real-world scenarios.", "AI": {"tldr": "RobKiNet\u662f\u4e00\u79cd\u57fa\u4e8e\u8fd0\u52a8\u5b66\u77e5\u8bc6\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u5728\u591a\u7ea6\u675f\u914d\u7f6e\u7a7a\u95f4\u4e2d\u8fdb\u884c\u9ad8\u6548\u91c7\u6837\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u534774.29\u500d\uff0c\u91c7\u6837\u7cbe\u5ea6\u8fbe99.25%\uff0c\u4efb\u52a1\u5b8c\u6210\u738797.33%\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212(TAMP)\u4e2d\uff0c\u5728\u591a\u7ea7\u7ea6\u675f\u4e0b\u8fdb\u884c\u5173\u8282\u914d\u7f6e\u7a7a\u95f4\u91c7\u6837\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u6ee1\u8db3\u4efb\u52a1\u7ea7\u5168\u5c40\u7ea6\u675f\u5e76\u63d0\u5347\u540e\u7eed\u8fd0\u52a8\u89c4\u5212\u6548\u7387\u3002", "method": "\u63d0\u51faRobKiNet\u8fd0\u52a8\u5b66\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u65b9\u5f0f\u5728\u8fde\u7eed\u53ef\u884c\u96c6(CFS)\u4e2d\u8fdb\u884c\u591a\u7ea6\u675f\u914d\u7f6e\u7a7a\u95f4\u91c7\u6837\uff0c\u5efa\u7acb\u4f18\u5316\u671f\u671b\u6a21\u578b\uff0c\u901a\u8fc7\u8fd0\u52a8\u5b66\u77e5\u8bc6\u6ce8\u5165\u786e\u4fdd\u7a33\u5b9a\u51c6\u786e\u7684\u68af\u5ea6\u4f18\u5316\u3002", "result": "\u57282-DOF\u7a7a\u95f4\u4e2d\u9a8c\u8bc1\u7406\u8bba\u6548\u7387\uff0c\u57289-DOF\u81ea\u4e3b\u79fb\u52a8\u673a\u68b0\u81c2\u4e0a\u5c55\u793a\u4f18\u8d8a\u7684\u5168\u8eab\u548c\u89e3\u8026\u63a7\u5236\u80fd\u529b\uff0c\u7535\u6c60\u62c6\u5378\u4efb\u52a1\u4e2d\u8bad\u7ec3\u901f\u5ea6\u6bd4\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5feb74.29\u500d\uff0c\u91c7\u6837\u7cbe\u5ea699.25%\uff0c\u5b9e\u9645\u573a\u666f\u4efb\u52a1\u5b8c\u6210\u738797.33%\u3002", "conclusion": "RobKiNet\u901a\u8fc7\u8fd0\u52a8\u5b66\u77e5\u8bc6\u6ce8\u5165\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u91c7\u6837\u7cbe\u5ea6\uff0c\u5728\u591a\u7ea6\u675f\u914d\u7f6e\u7a7a\u95f4\u91c7\u6837\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u673a\u5668\u4eba\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.07655", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07655", "abs": "https://arxiv.org/abs/2509.07655", "authors": ["Angelos Zacharia", "Mihir Dharmadhikari", "Kostas Alexis"], "title": "Collaborative Exploration with a Marsupial Ground-Aerial Robot Team through Task-Driven Map Compression", "comment": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)", "summary": "Efficient exploration of unknown environments is crucial for autonomous\nrobots, especially in confined and large-scale scenarios with limited\ncommunication. To address this challenge, we propose a collaborative\nexploration framework for a marsupial ground-aerial robot team that leverages\nthe complementary capabilities of both platforms. The framework employs a\ngraph-based path planning algorithm to guide exploration and deploy the aerial\nrobot in areas where its expected gain significantly exceeds that of the ground\nrobot, such as large open spaces or regions inaccessible to the ground\nplatform, thereby maximizing coverage and efficiency. To facilitate large-scale\nspatial information sharing, we introduce a bandwidth-efficient, task-driven\nmap compression strategy. This method enables each robot to reconstruct\nresolution-specific volumetric maps while preserving exploration-critical\ndetails, even at high compression rates. By selectively compressing and sharing\nkey data, communication overhead is minimized, ensuring effective map\nintegration for collaborative path planning. Simulation and real-world\nexperiments validate the proposed approach, demonstrating its effectiveness in\nimproving exploration efficiency while significantly reducing data\ntransmission.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5730\u9762-\u7a7a\u4e2d\u673a\u5668\u4eba\u534f\u4f5c\u63a2\u7d22\u6846\u67b6\uff0c\u5229\u7528\u56fe\u57fa\u8def\u5f84\u89c4\u5212\u548c\u5e26\u5bbd\u9ad8\u6548\u7684\u5730\u56fe\u538b\u7f29\u7b56\u7565\uff0c\u5728\u6709\u9650\u901a\u4fe1\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u6548\u73af\u5883\u63a2\u7d22\u3002", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u6709\u9650\u901a\u4fe1\u7684\u5bc6\u95ed\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u9ad8\u6548\u63a2\u7d22\u7684\u6311\u6218\uff0c\u5229\u7528\u5730\u9762\u548c\u7a7a\u4e2d\u5e73\u53f0\u7684\u4e92\u8865\u80fd\u529b\u3002", "method": "\u91c7\u7528\u56fe\u57fa\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u6307\u5bfc\u63a2\u7d22\uff0c\u5728\u9884\u671f\u6536\u76ca\u663e\u8457\u9ad8\u4e8e\u5730\u9762\u673a\u5668\u4eba\u7684\u533a\u57df\u90e8\u7f72\u7a7a\u4e2d\u673a\u5668\u4eba\uff1b\u5f15\u5165\u5e26\u5bbd\u9ad8\u6548\u7684\u4efb\u52a1\u9a71\u52a8\u5730\u56fe\u538b\u7f29\u7b56\u7565\uff0c\u9009\u62e9\u6027\u538b\u7f29\u548c\u5171\u4eab\u5173\u952e\u6570\u636e\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u663e\u8457\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u63a2\u7d22\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u534f\u4f5c\u63a2\u7d22\u6846\u67b6\u80fd\u591f\u6709\u6548\u5229\u7528\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u7684\u4e92\u8865\u80fd\u529b\uff0c\u5728\u6709\u9650\u901a\u4fe1\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u73af\u5883\u63a2\u7d22\u3002"}}
{"id": "2509.07674", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.07674", "abs": "https://arxiv.org/abs/2509.07674", "authors": ["Tamlin Love", "Antonio Andriella", "Guillem Aleny\u00e0"], "title": "Temporal Counterfactual Explanations of Behaviour Tree Decisions", "comment": "23 pages, 6 figures, submitted to Engineering Applications of\n  Artificial Intelligence", "summary": "Explainability is a critical tool in helping stakeholders understand robots.\nIn particular, the ability for robots to explain why they have made a\nparticular decision or behaved in a certain way is useful in this regard.\nBehaviour trees are a popular framework for controlling the decision-making of\nrobots and other software systems, and thus a natural question to ask is\nwhether or not a system driven by a behaviour tree is capable of answering\n\"why\" questions. While explainability for behaviour trees has seen some prior\nattention, no existing methods are capable of generating causal, counterfactual\nexplanations which detail the reasons for robot decisions and behaviour.\nTherefore, in this work, we introduce a novel approach which automatically\ngenerates counterfactual explanations in response to contrastive \"why\"\nquestions. Our method achieves this by first automatically building a causal\nmodel from the structure of the behaviour tree as well as domain knowledge\nabout the state and individual behaviour tree nodes. The resultant causal model\nis then queried and searched to find a set of diverse counterfactual\nexplanations. We demonstrate that our approach is able to correctly explain the\nbehaviour of a wide range of behaviour tree structures and states. By being\nable to answer a wide range of causal queries, our approach represents a step\ntowards more transparent, understandable and ultimately trustworthy robotic\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u751f\u6210\u884c\u4e3a\u6811\u9a71\u52a8\u7cfb\u7edf\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\uff0c\u7528\u4e8e\u56de\u7b54\u5bf9\u6c7d\u4eba\u51b3\u7b56\u884c\u4e3a\u7684\"\u4e3a\u4ec0\u4e48\"\u95ee\u9898", "motivation": "\u884c\u4e3a\u6811\u662f\u6c7d\u4eba\u51b3\u7b56\u63a7\u5236\u7684\u6d41\u884c\u6846\u67b6\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u751f\u6210\u56e0\u679c\u6027\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u5f71\u54cd\u7cfb\u7edf\u7684\u900f\u660e\u6027\u548c\u53ef\u4fe1\u8d56\u6027", "method": "\u81ea\u52a8\u4ece\u884c\u4e3a\u6811\u7ed3\u6784\u548c\u57df\u77e5\u8bc6\u6784\u5efa\u56e0\u679c\u6a21\u578b\uff0c\u7136\u540e\u67e5\u8be2\u548c\u641c\u7d22\u8be5\u6a21\u578b\u4ee5\u627e\u5230\u591a\u6837\u5316\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca", "result": "\u65b9\u6cd5\u80fd\u591f\u6b63\u786e\u89e3\u91ca\u5e7f\u6cdb\u884c\u4e3a\u6811\u7ed3\u6784\u548c\u72b6\u6001\u7684\u884c\u4e3a\uff0c\u80fd\u591f\u56de\u7b54\u591a\u79cd\u56e0\u679c\u67e5\u8be2", "conclusion": "\u8be5\u65b9\u6cd5\u5411\u66f4\u900f\u660e\u3001\u53ef\u7406\u89e3\u548c\u53ef\u4fe1\u8d56\u7684\u6c7d\u4eba\u7cfb\u7edf\u8fdb\u884c\u4e86\u91cd\u8981\u4e00\u6b65"}}
{"id": "2509.07683", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07683", "abs": "https://arxiv.org/abs/2509.07683", "authors": ["Luis Diener", "Jens Kalkkuhl", "Markus Enzweiler"], "title": "Robust Radar SLAM for Vehicle Parking Applications", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "We address ego-motion estimation for automated parking, where\ncentimeter-level accuracy is crucial due to tight spaces and nearby obstacles.\nTraditional methods using inertial-measurement units and wheel encoders require\ncalibration, making them costly and time-consuming. To overcome this, we\npropose a radar-based simultaneous localization and mapping (SLAM) approach\nthat leverages the robustness of radar to adverse weather and support for\nonline calibration. Our robocentric formulation fuses feature positions and\nDoppler velocities for robust data association and filter convergence. Key\ncontributions include a Doppler-augmented radar SLAM method, multi-radar\nsupport and an information-based feature-pruning strategy. Experiments\ndemonstrate high-accuracy localization and improved robustness over\nstate-of-the-art methods, meeting the demands of automated parking.", "AI": {"tldr": "\u96a8\u8ecc\u96f7\u9054SLAM\u65b9\u6848\uff0c\u901a\u904e\u591a\u666e\u52d2\u901f\u5ea6\u589e\u5f3a\u548c\u591a\u96f7\u9054\u878d\u5408\uff0c\u5b9e\u73b0\u81ea\u52d5\u505c\u8eca\u573a\u666f\u4e0b\u7684\u516c\u5206\u7ea7\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d", "motivation": "\u89e3\u51b3\u81ea\u52d5\u505c\u8eca\u573a\u666f\u4e0b\u5bf9\u516c\u5206\u7ea7\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u7684\u9700\u6c42\uff0c\u514d\u53bb\u4f20\u7edfIMU\u548c\u8f6e\u5b50\u7f16\u7801\u5668\u65b9\u6848\u7684\u68c0\u5b9a\u6210\u672c", "method": "\u63d0\u51fa\u4ee5\u96f7\u9054\u4e3a\u6838\u5fc3\u7684\u96a8\u8ecc\u5b9a\u4f4d\u4e0e\u5730\u56fe\u6784\u5efa(SLAM)\u65b9\u6cd5\uff0c\u878d\u5408\u7279\u5f81\u70b9\u4f4d\u7f6e\u548c\u591a\u666e\u52d2\u901f\u5ea6\uff0c\u652f\u6301\u591a\u96f7\u9054\u878d\u5408\u548c\u57fa\u4e8e\u4fe1\u606f\u7684\u7279\u5f81\u526a\u679d\u7b56\u7565", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\uff0c\u5728\u6025\u5267\u53d8\u5316\u548c\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u4f9b\u66f4\u597d\u7684\u7a33\u5065\u6027\uff0c\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5", "conclusion": "\u96f7\u9054\u57faSLAM\u65b9\u6848\u80fd\u591f\u6ee1\u8db3\u81ea\u52d5\u505c\u8eca\u5bf9\u516c\u5206\u7ea7\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u7684\u8981\u6c42\uff0c\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f"}}
{"id": "2509.07707", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.07707", "abs": "https://arxiv.org/abs/2509.07707", "authors": ["Muzaffar Habib", "Adnan Maqsood", "Adnan Fayyaz ud Din"], "title": "Fault Tolerant Control of a Quadcopter using Reinforcement Learning", "comment": "e-ISSN: 1946-3901, ISSN: 1946-3855,\n  https://www.sae.org/publications/technical-papers/content/01-18-01-0006/", "summary": "This study presents a novel reinforcement learning (RL)-based control\nframework aimed at enhancing the safety and robustness of the quadcopter, with\na specific focus on resilience to in-flight one propeller failure. Addressing\nthe critical need of a robust control strategy for maintaining a desired\naltitude for the quadcopter to safe the hardware and the payload in physical\napplications. The proposed framework investigates two RL methodologies Dynamic\nProgramming (DP) and Deep Deterministic Policy Gradient (DDPG), to overcome the\nchallenges posed by the rotor failure mechanism of the quadcopter. DP, a\nmodel-based approach, is leveraged for its convergence guarantees, despite high\ncomputational demands, whereas DDPG, a model-free technique, facilitates rapid\ncomputation but with constraints on solution duration. The research challenge\narises from training RL algorithms on large dimensions and action domains. With\nmodifications to the existing DP and DDPG algorithms, the controllers were\ntrained not only to cater for large continuous state and action domain and also\nachieve a desired state after an inflight propeller failure. To verify the\nrobustness of the proposed control framework, extensive simulations were\nconducted in a MATLAB environment across various initial conditions and\nunderscoring its viability for mission-critical quadcopter applications. A\ncomparative analysis was performed between both RL algorithms and their\npotential for applications in faulty aerial systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u56db\u65cb\u7ffc\u63a7\u5236\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u5355\u87ba\u65cb\u6868\u6545\u969c\u65f6\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\uff0c\u6bd4\u8f83\u4e86\u52a8\u6001\u89c4\u5212\u548cDDPG\u4e24\u79cd\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u56db\u65cb\u7ffc\u5728\u98de\u884c\u4e2d\u53d1\u751f\u87ba\u65cb\u6868\u6545\u969c\u65f6\u9700\u8981\u9c81\u68d2\u63a7\u5236\u7b56\u7565\u6765\u7ef4\u6301\u671f\u671b\u9ad8\u5ea6\uff0c\u4fdd\u62a4\u786c\u4ef6\u548c\u6709\u6548\u8f7d\u8377\u7684\u5173\u952e\u9700\u6c42\u3002", "method": "\u7814\u7a76\u4e24\u79cd\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff1a\u52a8\u6001\u89c4\u5212\uff08DP\uff0c\u57fa\u4e8e\u6a21\u578b\uff09\u548c\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08DDPG\uff0c\u65e0\u6a21\u578b\uff09\uff0c\u5e76\u5bf9\u7b97\u6cd5\u8fdb\u884c\u4fee\u6539\u4ee5\u9002\u5e94\u5927\u7ef4\u5ea6\u8fde\u7eed\u72b6\u6001\u548c\u52a8\u4f5c\u57df\u3002", "result": "\u901a\u8fc7MATLAB\u73af\u5883\u4e0b\u7684\u5e7f\u6cdb\u4eff\u771f\u9a8c\u8bc1\u4e86\u63a7\u5236\u6846\u67b6\u7684\u9c81\u68d2\u6027\uff0c\u5728\u4e0d\u540c\u521d\u59cb\u6761\u4ef6\u4e0b\u90fd\u80fd\u5728\u87ba\u65cb\u6868\u6545\u969c\u540e\u8fbe\u5230\u671f\u671b\u72b6\u6001\u3002", "conclusion": "\u8be5\u63a7\u5236\u6846\u67b6\u5728\u4efb\u52a1\u5173\u952e\u578b\u56db\u65cb\u7ffc\u5e94\u7528\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u4e24\u79cdRL\u7b97\u6cd5\u5728\u6545\u969c\u822a\u7a7a\u7cfb\u7edf\u4e2d\u90fd\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.07812", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07812", "abs": "https://arxiv.org/abs/2509.07812", "authors": ["Kristan Hilby", "Ian Hunter"], "title": "Unlocking Stopped-Rotor Flight: Development and Validation of SPERO, a Novel UAV Platform", "comment": "15 pages, 11 figures, 5 tables", "summary": "Stop-rotor aircraft have long been proposed as the ideal vertical takeoff and\nlanding (VTOL) aircraft for missions with equal time spent in both flight\nregimes, such as agricultural monitoring, search and rescue, and last-mile\ndelivery. Featuring a central lifting surface that rotates in VTOL to generate\nvertical thrust and locks in forward flight to generate passive lift, the\nstop-rotor offers the potential for high efficiency across both modes. However,\npractical implementation has remained infeasible due to aerodynamic and\nstability conflicts between flight modes. In this work, we present SPERO\n(Stopped-Penta Rotor), a stop-rotor uncrewed aerial vehicle (UAV) featuring a\nflipping and latching wing, an active center of pressure mechanism, thrust\nvectored counterbalances, a five-rotor architecture, and an eleven-state\nmachine flight controller coordinating geometric and controller\nreconfiguration. Furthermore, SPERO establishes a generalizable design and\ncontrol framework for stopped-rotor UAVs. Together, these innovations overcome\nlongstanding challenges in stop-rotor flight and enable the first stable,\nbidirectional transition between VTOL and forward flight.", "AI": {"tldr": "SPERO\u662f\u4e00\u79cd\u521b\u65b0\u7684\u505c\u6b62\u65cb\u7ffc\u65e0\u4eba\u673a\uff0c\u901a\u8fc7\u7ffb\u8f6c\u9501\u5b9a\u673a\u7ffc\u3001\u538b\u529b\u4e2d\u5fc3\u8c03\u8282\u673a\u5236\u3001\u63a8\u529b\u77e2\u91cf\u5e73\u8861\u7b49\u521b\u65b0\u8bbe\u8ba1\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5782\u76f4\u8d77\u964d\u548c\u5411\u524d\u98de\u884c\u7684\u7a33\u5b9a\u53cc\u5411\u8f6c\u6362", "motivation": "\u505c\u6b62\u65cb\u7ffc\u98de\u673a\u957f\u671f\u4ee5\u6765\u88ab\u8ba4\u4e3a\u662f\u5782\u76f4\u8d77\u964d\u548c\u5411\u524d\u98de\u884c\u65f6\u95f4\u76f8\u5f53\u7684\u7406\u60f3\u98de\u884c\u5668\uff0c\u4f46\u7531\u4e8e\u6c14\u52a8\u548c\u7a33\u5b9a\u6027\u51b2\u7a81\uff0c\u5b9e\u9645\u5e94\u7528\u4e00\u76f4\u4e0d\u53ef\u884c", "method": "\u91c7\u7528\u7ffb\u8f6c\u9501\u5b9a\u673a\u7ffc\u3001\u4e3b\u52a8\u538b\u529b\u4e2d\u5fc3\u673a\u5236\u3001\u63a8\u529b\u77e2\u91cf\u5e73\u8861\u3001\u4e94\u65cb\u7ffc\u67b6\u6784\u548c\u5341\u4e00\u72b6\u6001\u673a\u98de\u884c\u63a7\u5236\u5668\uff0c\u534f\u8c03\u51e0\u4f55\u548c\u63a7\u5236\u91cd\u6784", "result": "\u514b\u670d\u4e86\u505c\u6b62\u65cb\u7ffc\u98de\u884c\u7684\u957f\u671f\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9996\u4e2a\u7a33\u5b9a\u7684VTOL\u548c\u5411\u524d\u98de\u884c\u7684\u53cc\u5411\u8f6c\u6362", "conclusion": "SPERO\u4e3a\u505c\u6b62\u65cb\u7ffc\u65e0\u4eba\u673a\u5efa\u7acb\u4e86\u53ef\u63a8\u5e7f\u7684\u8bbe\u8ba1\u548c\u63a7\u5236\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u7684\u5173\u952e\u6280\u672f\u96be\u9898"}}
{"id": "2509.07916", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07916", "abs": "https://arxiv.org/abs/2509.07916", "authors": ["Jianshu Zhou", "Wei Chen", "Junda Huang", "Boyuan Liang", "Yunhui Liu", "Masayoshi Tomizuka"], "title": "Programmable Locking Cells (PLC) for Modular Robots with High Stiffness Tunability and Morphological Adaptability", "comment": null, "summary": "Robotic systems operating in unstructured environments require the ability to\nswitch between compliant and rigid states to perform diverse tasks such as\nadaptive grasping, high-force manipulation, shape holding, and navigation in\nconstrained spaces, among others. However, many existing variable stiffness\nsolutions rely on complex actuation schemes, continuous input power, or\nmonolithic designs, limiting their modularity and scalability. This paper\npresents the Programmable Locking Cell (PLC)-a modular, tendon-driven unit that\nachieves discrete stiffness modulation through mechanically interlocked joints\nactuated by cable tension. Each unit transitions between compliant and firm\nstates via structural engagement, and the assembled system exhibits high\nstiffness variation-up to 950% per unit-without susceptibility to damage under\nhigh payload in the firm state. Multiple PLC units can be assembled into\nreconfigurable robotic structures with spatially programmable stiffness. We\nvalidate the design through two functional prototypes: (1) a variable-stiffness\ngripper capable of adaptive grasping, firm holding, and in-hand manipulation;\nand (2) a pipe-traversing robot composed of serial PLC units that achieves\nshape adaptability and stiffness control in confined environments. These\nresults demonstrate the PLC as a scalable, structure-centric mechanism for\nprogrammable stiffness and motion, enabling robotic systems with reconfigurable\nmorphology and task-adaptive interaction.", "AI": {"tldr": "\u7a0b\u5e8f\u5316\u9501\u5b9a\u5355\u5143(PLC)\u662f\u4e00\u79cd\u6a21\u5757\u5316\u7684\u8179\u7eb3\u9a71\u52a8\u673a\u5236\uff0c\u901a\u8fc7\u673a\u68b0\u4e92\u9501\u5173\u8282\u5b9e\u73b0\u79bb\u6563\u7cbe\u5ea6\u8c03\u8282\uff0c\u5728\u4e0d\u9700\u6301\u7eed\u8f93\u5165\u529b\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4ece\u67d4\u6027\u5230\u786c\u6027\u7684\u72b6\u6001\u8f6c\u6362\uff0c\u5e76\u5c55\u73b0\u51fa\u9ad8\u8fbe950%\u7684\u521a\u5ea6\u53d8\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u53d8\u521a\u5ea6\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u4f9d\u8d56\u590d\u6742\u9a71\u52a8\u65b9\u6848\u3001\u6301\u7eed\u8f93\u5165\u529b\u6216\u5355\u4f53\u8bbe\u8ba1\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5728\u67d4\u6027\u548c\u786c\u6027\u72b6\u6001\u4e4b\u95f4\u5207\u6362\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "method": "\u8bbe\u8ba1\u4e86\u7a0b\u5e8f\u5316\u9501\u5b9a\u5355\u5143(PLC)\uff0c\u901a\u8fc7\u8179\u7eb3\u9a71\u52a8\u548c\u673a\u68b0\u4e92\u9501\u5173\u8282\u5b9e\u73b0\u79bb\u6563\u521a\u5ea6\u8c03\u8282\u3002\u6bcf\u4e2a\u5355\u5143\u901a\u8fc7\u7ed3\u6784\u54cd\u5e94\u5728\u67d4\u6027\u548c\u786c\u6027\u72b6\u6001\u4e4b\u95f4\u8f6c\u6362\uff0c\u591a\u4e2aPLC\u5355\u5143\u53ef\u7ec4\u88c5\u6210\u53ef\u91cd\u914d\u7f6e\u7684\u673a\u5668\u4eba\u7ed3\u6784\u3002", "result": "\u9a8c\u8bc1\u4e86\u4e24\u4e2a\u529f\u80fd\u539f\u578b\uff1a(1)\u53d8\u521a\u5ea6\u6293\u624b\uff0c\u80fd\u591f\u9002\u5e94\u6027\u6293\u53d6\u3001\u786c\u6027\u6301\u6709\u548c\u624b\u5185\u64cd\u4f5c\uff1b(2)\u7ba1\u9053\u904d\u5386\u673a\u5668\u4eba\uff0c\u5728\u5c01\u95ed\u73af\u5883\u4e2d\u5b9e\u73b0\u5f62\u72b6\u9002\u5e94\u6027\u548c\u521a\u5ea6\u63a7\u5236\u3002\u6bcf\u4e2a\u5355\u5143\u5c55\u73b0\u9ad8\u8fbe950%\u7684\u521a\u5ea6\u53d8\u5316\uff0c\u4e14\u5728\u9ad8\u8d1f\u8377\u786c\u6027\u72b6\u6001\u4e0b\u4e0d\u6613\u53d7\u635f\u574f\u3002", "conclusion": "PLC\u4f5c\u4e3a\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u4ee5\u7ed3\u6784\u4e3a\u4e2d\u5fc3\u7684\u673a\u5236\uff0c\u4e3a\u7a0b\u5e8f\u5316\u521a\u5ea6\u548c\u8fd0\u52a8\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u5f97\u673a\u5668\u4eba\u7cfb\u7edf\u80fd\u591f\u5b9e\u73b0\u53ef\u91cd\u914d\u7f6e\u5f62\u6001\u548c\u4efb\u52a1\u9002\u5e94\u6027\u4ea4\u4e92\u3002"}}
{"id": "2509.07953", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07953", "abs": "https://arxiv.org/abs/2509.07953", "authors": ["Zheyuan Hu", "Robyn Wu", "Naveen Enock", "Jasmine Li", "Riya Kadakia", "Zackory Erickson", "Aviral Kumar"], "title": "RaC: Robot Learning for Long-Horizon Tasks by Scaling Recovery and Correction", "comment": null, "summary": "Modern paradigms for robot imitation train expressive policy architectures on\nlarge amounts of human demonstration data. Yet performance on contact-rich,\ndeformable-object, and long-horizon tasks plateau far below perfect execution,\neven with thousands of expert demonstrations. This is due to the inefficiency\nof existing ``expert'' data collection procedures based on human teleoperation.\nTo address this issue, we introduce RaC, a new phase of training on\nhuman-in-the-loop rollouts after imitation learning pre-training. In RaC, we\nfine-tune a robotic policy on human intervention trajectories that illustrate\nrecovery and correction behaviors. Specifically, during a policy rollout, human\noperators intervene when failure appears imminent, first rewinding the robot\nback to a familiar, in-distribution state and then providing a corrective\nsegment that completes the current sub-task. Training on this data composition\nexpands the robotic skill repertoire to include retry and adaptation behaviors,\nwhich we show are crucial for boosting both efficiency and robustness on\nlong-horizon tasks. Across three real-world bimanual control tasks: shirt\nhanging, airtight container lid sealing, takeout box packing, and a simulated\nassembly task, RaC outperforms the prior state-of-the-art using 10$\\times$ less\ndata collection time and samples. We also show that RaC enables test-time\nscaling: the performance of the trained RaC policy scales linearly in the\nnumber of recovery maneuvers it exhibits. Videos of the learned policy are\navailable at https://rac-scaling-robot.github.io/.", "AI": {"tldr": "RaC\u662f\u4e00\u79cd\u5728\u6a21\u4eff\u5b66\u4e60\u9884\u8bad\u7ec3\u540e\u589e\u52a0\u4eba\u7c7b\u5e72\u9884\u56de\u653e\u8bad\u7ec3\u9636\u6bb5\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eba\u7c7b\u7ea0\u6b63\u884c\u4e3a\u6765\u63d0\u5347\u673a\u5668\u4eba\u7b56\u7565\u7684\u6062\u590d\u548c\u9002\u5e94\u80fd\u529b\uff0c\u663e\u8457\u51cf\u5c11\u6570\u636e\u6536\u96c6\u9700\u6c42\u5e76\u63d0\u9ad8\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4eba\u7c7b\u9065\u64cd\u4f5c\u7684\u4e13\u5bb6\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u5bfc\u81f4\u63a5\u89e6\u4e30\u5bcc\u3001\u53ef\u53d8\u5f62\u7269\u4f53\u548c\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u6027\u80fd\u8fdc\u4f4e\u4e8e\u5b8c\u7f8e\u6267\u884c\u6c34\u5e73\uff0c\u5373\u4f7f\u4f7f\u7528\u6570\u5343\u6761\u4e13\u5bb6\u6f14\u793a", "method": "\u5728\u6a21\u4eff\u5b66\u4e60\u9884\u8bad\u7ec3\u540e\uff0c\u5f15\u5165\u4eba\u7c7b\u5e72\u9884\u56de\u653e\u8bad\u7ec3\u9636\u6bb5\u3002\u4eba\u7c7b\u64cd\u4f5c\u5458\u5728\u7b56\u7565\u6267\u884c\u5373\u5c06\u5931\u8d25\u65f6\u8fdb\u884c\u5e72\u9884\uff0c\u9996\u5148\u5c06\u673a\u5668\u4eba\u56de\u9000\u5230\u719f\u6089\u7684\u5206\u5e03\u5185\u72b6\u6001\uff0c\u7136\u540e\u63d0\u4f9b\u5b8c\u6210\u5f53\u524d\u5b50\u4efb\u52a1\u7684\u7ea0\u6b63\u7247\u6bb5", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u53cc\u624b\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\uff08\u886c\u886b\u60ac\u6302\u3001\u5bc6\u5c01\u5bb9\u5668\u76d6\u3001\u5916\u5356\u76d2\u6253\u5305\uff09\u548c\u4e00\u4e2a\u6a21\u62df\u88c5\u914d\u4efb\u52a1\u4e2d\uff0cRaC\u4f7f\u752810\u500d\u5c11\u7684\u6570\u636e\u6536\u96c6\u65f6\u95f4\u548c\u6837\u672c\u5c31\u8d85\u8d8a\u4e86\u5148\u524d\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "RaC\u901a\u8fc7\u8bad\u7ec3\u6062\u590d\u548c\u7ea0\u6b63\u884c\u4e3a\u6269\u5c55\u4e86\u673a\u5668\u4eba\u6280\u80fd\u5e93\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u4e14\u8bad\u7ec3\u540e\u7684RaC\u7b56\u7565\u6027\u80fd\u968f\u6062\u590d\u64cd\u4f5c\u6b21\u6570\u7ebf\u6027\u6269\u5c55"}}
{"id": "2509.07957", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07957", "abs": "https://arxiv.org/abs/2509.07957", "authors": ["Shunlei Li", "Longsen Gao", "Jiuwen Cao", "Yingbai Hu"], "title": "Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm Robotic Manipulation", "comment": "This paper is submitted to IEEE IROS 2025 Workshop AIR4S", "summary": "Acquiring dexterous robotic skills from human video demonstrations remains a\nsignificant challenge, largely due to conventional reliance on low-level\ntrajectory replication, which often fails to generalize across varying objects,\nspatial layouts, and manipulator configurations. To address this limitation, we\nintroduce Graph-Fused Vision-Language-Action (GF-VLA), a unified framework that\nenables dual-arm robotic systems to perform task-level reasoning and execution\ndirectly from RGB-D human demonstrations. GF-VLA employs an\ninformation-theoretic approach to extract task-relevant cues, selectively\nhighlighting critical hand-object and object-object interactions. These cues\nare structured into temporally ordered scene graphs, which are subsequently\nintegrated with a language-conditioned transformer to produce hierarchical\nbehavior trees and interpretable Cartesian motion primitives. To enhance\nefficiency in bimanual execution, we propose a cross-arm allocation strategy\nthat autonomously determines gripper assignment without requiring explicit\ngeometric modeling. We validate GF-VLA on four dual-arm block assembly\nbenchmarks involving symbolic structure construction and spatial\ngeneralization. Empirical results demonstrate that the proposed representation\nachieves over 95% graph accuracy and 93% subtask segmentation, enabling the\nlanguage-action planner to generate robust, interpretable task policies. When\ndeployed on a dual-arm robot, these policies attain 94% grasp reliability, 89%\nplacement accuracy, and 90% overall task success across stacking,\nletter-formation, and geometric reconfiguration tasks, evidencing strong\ngeneralization and robustness under diverse spatial and semantic variations.", "AI": {"tldr": "GF-VLA\u662f\u4e00\u4e2a\u4ece\u4eba\u7c7b\u89c6\u9891\u6f14\u793a\u4e2d\u5b66\u4e60\u53cc\u624b\u673a\u5668\u4eba\u6280\u80fd\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u8bba\u65b9\u6cd5\u63d0\u53d6\u4efb\u52a1\u76f8\u5173\u7ebf\u7d22\uff0c\u6784\u5efa\u65f6\u5e8f\u573a\u666f\u56fe\uff0c\u7ed3\u5408\u8bed\u8a00\u6761\u4ef6\u53d8\u6362\u5668\u751f\u6210\u884c\u4e3a\u6811\u548c\u8fd0\u52a8\u57fa\u5143\uff0c\u5728\u53cc\u624b\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u8f68\u8ff9\u590d\u5236\u7684\u65b9\u6cd5\u5728\u6cdb\u5316\u5230\u4e0d\u540c\u7269\u4f53\u3001\u7a7a\u95f4\u5e03\u5c40\u548c\u673a\u68b0\u81c2\u914d\u7f6e\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u80fd\u591f\u8fdb\u884c\u4efb\u52a1\u7ea7\u63a8\u7406\u548c\u6267\u884c\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4fe1\u606f\u8bba\u65b9\u6cd5\u63d0\u53d6\u5173\u952e\u7684\u624b-\u7269\u4f53\u548c\u7269\u4f53-\u7269\u4f53\u4ea4\u4e92\u7ebf\u7d22\uff0c\u6784\u5efa\u65f6\u5e8f\u573a\u666f\u56fe\uff0c\u7ed3\u5408\u8bed\u8a00\u6761\u4ef6\u53d8\u6362\u5668\u751f\u6210\u5c42\u6b21\u884c\u4e3a\u6811\u548c\u53ef\u89e3\u91ca\u7684\u7b1b\u5361\u5c14\u8fd0\u52a8\u57fa\u5143\uff0c\u5e76\u63d0\u51fa\u8de8\u81c2\u5206\u914d\u7b56\u7565\u3002", "result": "\u5728\u56db\u4e2a\u53cc\u624b\u673a\u5668\u4eba\u79ef\u6728\u88c5\u914d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u56fe\u51c6\u786e\u7387\u8d85\u8fc795%\uff0c\u5b50\u4efb\u52a1\u5206\u5272\u51c6\u786e\u738793%\uff0c\u6293\u53d6\u53ef\u9760\u602794%\uff0c\u653e\u7f6e\u7cbe\u5ea689%\uff0c\u6574\u4f53\u4efb\u52a1\u6210\u529f\u738790%\u3002", "conclusion": "GF-VLA\u6846\u67b6\u80fd\u591f\u4eceRGB-D\u4eba\u7c7b\u6f14\u793a\u4e2d\u5b9e\u73b0\u4efb\u52a1\u7ea7\u63a8\u7406\u548c\u6267\u884c\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u7a7a\u95f4\u548c\u8bed\u4e49\u53d8\u5316\u573a\u666f\u3002"}}
{"id": "2509.07962", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07962", "abs": "https://arxiv.org/abs/2509.07962", "authors": ["Zongzheng Zhang", "Haobo Xu", "Zhuo Yang", "Chenghao Yue", "Zehao Lin", "Huan-ang Gao", "Ziwei Wang", "Hao Zhao"], "title": "TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models", "comment": "Accepted to CoRL 2025, project page:\n  \\url{https://zzongzheng0918.github.io/Torque-Aware-VLA.github.io/}", "summary": "Many robotic manipulation tasks require sensing and responding to force\nsignals such as torque to assess whether the task has been successfully\ncompleted and to enable closed-loop control. However, current\nVision-Language-Action (VLA) models lack the ability to integrate such subtle\nphysical feedback. In this work, we explore Torque-aware VLA models, aiming to\nbridge this gap by systematically studying the design space for incorporating\ntorque signals into existing VLA architectures. We identify and evaluate\nseveral strategies, leading to three key findings. First, introducing torque\nadapters into the decoder consistently outperforms inserting them into the\nencoder.Third, inspired by joint prediction and planning paradigms in\nautonomous driving, we propose predicting torque as an auxiliary output, which\nfurther improves performance. This strategy encourages the model to build a\nphysically grounded internal representation of interaction dynamics. Extensive\nquantitative and qualitative experiments across contact-rich manipulation\nbenchmarks validate our findings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u626d\u77e9\u611f\u77e5\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u7cfb\u7edf\u7814\u7a76\u626d\u77e9\u4fe1\u53f7\u96c6\u6210\u7b56\u7565\uff0c\u53d1\u73b0\u89e3\u7801\u5668\u5f15\u5165\u626d\u77e9\u9002\u914d\u5668\u4f18\u4e8e\u7f16\u7801\u5668\uff0c\u5e76\u63d0\u51fa\u9884\u6d4b\u626d\u77e9\u4f5c\u4e3a\u8f85\u52a9\u8f93\u51fa\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7f3a\u4e4f\u6574\u5408\u529b\u53cd\u9988\u4fe1\u53f7\u7684\u80fd\u529b\uff0c\u800c\u8bb8\u591a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u9700\u8981\u611f\u77e5\u548c\u54cd\u5e94\u626d\u77e9\u7b49\u529b\u4fe1\u53f7\u6765\u8bc4\u4f30\u4efb\u52a1\u5b8c\u6210\u60c5\u51b5\u548c\u5b9e\u73b0\u95ed\u73af\u63a7\u5236\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u626d\u77e9\u4fe1\u53f7\u96c6\u6210\u5230\u73b0\u6709VLA\u67b6\u6784\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u8bc4\u4f30\u591a\u79cd\u7b56\u7565\uff0c\u5305\u62ec\u5728\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e2d\u5f15\u5165\u626d\u77e9\u9002\u914d\u5668\uff0c\u5e76\u63d0\u51fa\u9884\u6d4b\u626d\u77e9\u4f5c\u4e3a\u8f85\u52a9\u8f93\u51fa\u7684\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5728\u89e3\u7801\u5668\u5f15\u5165\u626d\u77e9\u9002\u914d\u5668\u6027\u80fd\u66f4\u4f18\uff1b\u9884\u6d4b\u626d\u77e9\u4f5c\u4e3a\u8f85\u52a9\u8f93\u51fa\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\uff0c\u5e2e\u52a9\u6a21\u578b\u5efa\u7acb\u7269\u7406\u4ea4\u4e92\u52a8\u6001\u7684\u5185\u90e8\u8868\u5f81\u3002", "conclusion": "\u901a\u8fc7\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u626d\u77e9\u611f\u77e5VLA\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
