<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 64]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Maximizing UAV Cellular Connectivity with Reinforcement Learning for BVLoS Path Planning](https://arxiv.org/abs/2509.13336)
*Mehran Behjati,Rosdiadee Nordin,Nor Fadzilah Abdullah*

Main category: cs.RO

TL;DR: 基于强化学习的无人机超视距路径规划方法，通过最小化飞行距离和最大化蜂窝链路质量来确保安全可靠的飞行操作


<details>
  <summary>Details</summary>
Motivation: 解决超视距无人机操作中蜂窝通信限制带来的挑战，需要研究考虑实际空中覆盖约束和信道模型的路径规划方法

Method: 采用强化学习技术训练智能体，使用无人机与基站之间的通信链路质量作为奖励函数，结合经验空中信道模型

Result: 仿真结果表明该方法能有效训练智能体并生成可行的无人机路径规划，能够高效识别最优路径确保与地面基站的最大连接性

Conclusion: 该方法可作为离线路径规划模块集成到未来地面控制系统中，提升无人机操作能力和安全性，在复杂长距离无人机应用中有很大潜力

Abstract: This paper presents a reinforcement learning (RL) based approach for path
planning of cellular connected unmanned aerial vehicles (UAVs) operating beyond
visual line of sight (BVLoS). The objective is to minimize travel distance
while maximizing the quality of cellular link connectivity by considering real
world aerial coverage constraints and employing an empirical aerial channel
model. The proposed solution employs RL techniques to train an agent, using the
quality of communication links between the UAV and base stations (BSs) as the
reward function. Simulation results demonstrate the effectiveness of the
proposed method in training the agent and generating feasible UAV path plans.
The proposed approach addresses the challenges due to limitations in UAV
cellular communications, highlighting the need for investigations and
considerations in this area. The RL algorithm efficiently identifies optimal
paths, ensuring maximum connectivity with ground BSs to ensure safe and
reliable BVLoS flight operation. Moreover, the solution can be deployed as an
offline path planning module that can be integrated into future ground control
systems (GCS) for UAV operations, enhancing their capabilities and safety. The
method holds potential for complex long range UAV applications, advancing the
technology in the field of cellular connected UAV path planning.

</details>


### [2] [Real World Robotic Exploration using Deep Neural Networks Trained in Photorealistic Reconstructed Environments](https://arxiv.org/abs/2509.13342)
*Isaac Ronald Ward*

Main category: cs.RO

TL;DR: 通过修改深度神经网络损失函数，结合位置和旋转误差来提高机器人视觉定位精度，并使用光线测量数据构建数据集，实现了实时室内导航算法


<details>
  <summary>Details</summary>
Motivation: 解决机器人在室内场景中的准确定位问题，提高对视觉歧同的鲁棒性，并开发一个完整的实时导航流程

Method: 修改深度神经网络的损失函数，直观地结合位置和旋转误差；使用光线测量数据生成带有姿态标签的数据集；在TurtleBot进行实时测试

Result: 定位精度显著提升：位置误差降低9.64%，旋转误差降低2.99%；实现了0.11米和0.89度的高精度定位；只需330秒的图片收集时间

Conclusion: 该方法提供了一个完整的室内导航解决方案，在不影响训练难度的情况下显著提高了定位精度，具有良好的应用前景

Abstract: In this work, an existing deep neural network approach for determining a
robot's pose from visual information (RGB images) is modified, improving its
localization performance without impacting its ease of training. Explicitly,
the network's loss function is extended in a manner which intuitively combines
the positional and rotational error in order to increase robustness to
perceptual aliasing. An improvement in the localization accuracy for indoor
scenes is observed: with decreases of up to 9.64% and 2.99% in the median
positional and rotational error respectively, when compared to the unmodified
network.
  Additionally, photogrammetry data is used to produce a pose-labelled dataset
which allows the above model to be trained on a local environment, resulting in
localization accuracies of 0.11m & 0.89 degrees. This trained model forms the
basis of a navigation algorithm, which is tested in real-time on a TurtleBot (a
wheeled robotic device). As such, this work introduces a full pipeline for
creating a robust navigational algorithm for any given real world indoor scene;
the only requirement being a collection of images from the scene, which can be
captured in as little as 330 seconds of

</details>


### [3] [Label-Efficient Grasp Joint Prediction with Point-JEPA](https://arxiv.org/abs/2509.13349)
*Jed Guzelkabaagac,Boris Petrović*

Main category: cs.RO

TL;DR: Point-JEPA自监督预训练在低标签数据情况下显著提升抓取关节角度预测性能，在DLR-Hand II数据集上RMSE降低26%，达到全监督性能水平


<details>
  <summary>Details</summary>
Motivation: 研究3D自监督预训练是否能够实现标签高效的抓取关节角度预测，探索JEPA架构在数据高效抓取学习中的实用性

Method: 使用网格token化的点云数据，基于ShapeNet预训练的Point-JEPA编码器，训练轻量级多假设头网络，采用winner-takes-all策略和top-logit选择进行评估

Result: 在DLR-Hand II数据集的对象级分割上，Point-JEPA在低标签情况下RMSE降低达26%，且能达到全监督方法的性能水平

Conclusion: JEPA风格的预训练是数据高效抓取学习的一种实用方法，证明了自监督预训练在3D抓取任务中的有效性

Abstract: We investigate whether 3D self-supervised pretraining with a Joint-Embedding
Predictive Architecture (Point-JEPA) enables label-efficient grasp joint-angle
prediction. Using point clouds tokenized from meshes and a ShapeNet-pretrained
Point-JEPA encoder, we train a lightweight multi-hypothesis head with
winner-takes-all and evaluate by top-logit selection. On DLR-Hand II with
object-level splits, Point-JEPA reduces RMSE by up to 26% in low-label regimes
and reaches parity with full supervision. These results suggest JEPA-style
pretraining is a practical approach for data-efficient grasp learning.

</details>


### [4] [Using role-play and Hierarchical Task Analysis for designing human-robot interaction](https://arxiv.org/abs/2509.13378)
*Mattias Wingren,Sören Andersson,Sara Rosenberg,Malin Andtfolk,Susanne Hägglund,Prashani Jayasingha Arachchige,Linda Nyholm*

Main category: cs.RO

TL;DR: 设计社交机器人时使用角色扮演和层次任务分析两种方法，在社区药店场景中验证了其效果


<details>
  <summary>Details</summary>
Motivation: 在人机交互领域，角色扮演和层次任务分析还未得到充分利用，需要探索其在社交机器人设计中的潜力

Method: 通过在社区药店应用场景中进行角色扮演（药剂师模拟机器人行为）和层次任务分析，实现对用户需求的深入理解和行为模型的正确建模

Result: 角色扮演提供了可控可调整的环境来理解用户需求，药剂师可作为机器人行为模板；层次任务分析确保了行为模型的正确性并促进协同设计

Conclusion: 两种方法在社交机器人设计中显示出重要价值，未来应研发特别适用于社交机器交互的任务分析方法

Abstract: We present the use of two methods we believe warrant more use than they
currently have in the field of human-robot interaction: role-play and
Hierarchical Task Analysis. Some of its potential is showcased through our use
of them in an ongoing research project which entails developing a robot
application meant to assist at a community pharmacy. The two methods have
provided us with several advantages. The role-playing provided a controlled and
adjustable environment for understanding the customers' needs where pharmacists
could act as models for the robot's behavior; and the Hierarchical Task
Analysis ensured the behavior displayed was modelled correctly and aided
development through facilitating co-design. Future research could focus on
developing task analysis methods especially suited for social robot
interaction.

</details>


### [5] [ASTREA: Introducing Agentic Intelligence for Orbital Thermal Autonomy](https://arxiv.org/abs/2509.13380)
*Alejandro D. Mousist*

Main category: cs.RO

TL;DR: ASTREA是首个在飞行遗产硬件(TRL 9)上部署的自主航天器操作智能体系统，结合受限LLM和强化学习控制器，地面实验显示能改善热稳定性，但在轨验证发现推理延迟与LEO快速热循环不匹配导致性能下降


<details>
  <summary>Details</summary>
Motivation: 开发首个在飞行硬件上部署的自主航天器操作智能体系统，验证语义推理与自适应控制在硬件约束下的可行性

Method: 集成资源受限的大型语言模型(LLM)智能体与强化学习控制器，采用异步架构，专门针对空间认证平台设计，以热控制为代表用例

Result: 地面实验显示LLM引导的监督改善了热稳定性并减少了违规；但在国际空间站上的在轨验证显示，由于推理延迟与低地球轨道快速热循环不匹配，导致性能下降

Conclusion: 研究揭示了基于LLM的智能体系统在真实飞行环境中的机遇和当前局限性，为未来空间自主性提供了实用设计指南

Abstract: This paper presents ASTREA, the first agentic system deployed on
flight-heritage hardware (TRL 9) for autonomous spacecraft operations. Using
thermal control as a representative use case, we integrate a
resource-constrained Large Language Model (LLM) agent with a reinforcement
learning controller in an asynchronous architecture tailored for
space-qualified platforms. Ground experiments show that LLM-guided supervision
improves thermal stability and reduces violations, confirming the feasibility
of combining semantic reasoning with adaptive control under hardware
constraints. However, on-orbit validation aboard the International Space
Station (ISS) reveals performance degradation caused by inference latency
mismatched with the rapid thermal cycles characteristic of Low Earth Orbit
(LEO) satellites. These results highlight both the opportunities and current
limitations of agentic LLM-based systems in real flight environments, providing
practical design guidelines for future space autonomy.

</details>


### [6] [Cooperative Target Detection with AUVs: A Dual-Timescale Hierarchical MARDL Approach](https://arxiv.org/abs/2509.13381)
*Zhang Xueyao,Yang Bo,Yu Zhiwen,Cao Xuelin,George C. Alexandropoulos,Merouane Debbah,Chau Yuen*

Main category: cs.RO

TL;DR: 一种双时间尺度层次多自治体近端策略优化框架，用于水下自治潜器协同任务，在确保隐蔗操作的前提下实现高效协作。


<details>
  <summary>Details</summary>
Motivation: 水下自治潜器协同任务存在通信暴露风险，需要在对抗性环境中既保证隐蔗性又实现高效协作的解决方案。

Method: 提出双时间尺度层次H-MAPPO框架：高层通过中央AUV确定参与任务的个体，低层通过功率和轨迹控制降低暴露概率。

Result: 模拟结果显示该框架达到快速收敛，性能超过基准算法，在确保隐蔗操作的前提下最大化了长期协作效率。

Conclusion: 该框架有效解决了水下协同任务中隐蔗性与效率的平衡问题，为对抗性环境下的AUV协同探测提供了可靠解决方案。

Abstract: Autonomous Underwater Vehicles (AUVs) have shown great potential for
cooperative detection and reconnaissance. However, collaborative AUV
communications introduce risks of exposure. In adversarial environments,
achieving efficient collaboration while ensuring covert operations becomes a
key challenge for underwater cooperative missions. In this paper, we propose a
novel dual time-scale Hierarchical Multi-Agent Proximal Policy Optimization
(H-MAPPO) framework. The high-level component determines the individuals
participating in the task based on a central AUV, while the low-level component
reduces exposure probabilities through power and trajectory control by the
participating AUVs. Simulation results show that the proposed framework
achieves rapid convergence, outperforms benchmark algorithms in terms of
performance, and maximizes long-term cooperative efficiency while ensuring
covert operations.

</details>


### [7] [VEGA: Electric Vehicle Navigation Agent via Physics-Informed Neural Operator and Proximal Policy Optimization](https://arxiv.org/abs/2509.13386)
*Hansol Lim,Minhyeok Im,Jonathan Boyack,Jee Won Lee,Jongseong Brad Choi*

Main category: cs.RO

TL;DR: VEGA是一个基于强化学习的电动汽车充电感知导航系统，使用物理信息神经网络和PPO算法优化路径规划和充电策略，无需额外传感器即可实现个性化能效估算和路线优化。


<details>
  <summary>Details</summary>
Motivation: 随着软件定义车辆需求增长和电动汽车计算能力提升，需要开发能够根据车辆实时状态和环境条件进行充电感知路径优化的AI系统，以提升电动汽车的能效和用户体验。

Method: 采用两模块架构：1)物理信息神经网络算子(PINO)从车速日志学习车辆定制化动力学参数；2)强化学习代理使用PPO算法在充电站标注的道路图上进行预算A*师生指导下的SOC约束路径优化。

Result: 在长距离路线(如旧金山到纽约)测试中，VEGA的充电站选择、停留时间、SOC管理和总旅行时间与特斯拉行程规划器高度一致但更保守，且在法国和日本等未训练地区也表现出良好的泛化能力。

Conclusion: VEGA成功实现了物理信息学习与强化学习的实际集成，可作为电动汽车功率和效率的虚拟传感器，有望降低电动汽车成本并提升能效路由性能。

Abstract: Demands for software-defined vehicles (SDV) are rising and electric vehicles
(EVs) are increasingly being equipped with powerful computers. This enables
onboard AI systems to optimize charge-aware path optimization customized to
reflect vehicle's current condition and environment. We present VEGA, a
charge-aware EV navigation agent that plans over a charger-annotated road graph
using Proximal Policy Optimization (PPO) with budgeted A* teacher-student
guidance under state-of-charge (SoC) feasibility. VEGA consists of two modules.
First, a physics-informed neural operator (PINO), trained on real vehicle speed
and battery-power logs, uses recent vehicle speed logs to estimate aerodynamic
drag, rolling resistance, mass, motor and regenerative-braking efficiencies,
and auxiliary load by learning a vehicle-custom dynamics. Second, a
Reinforcement Learning (RL) agent uses these dynamics to optimize a path with
optimal charging stops and dwell times under SoC constraints. VEGA requires no
additional sensors and uses only vehicle speed signals. It may serve as a
virtual sensor for power and efficiency to potentially reduce EV cost. In
evaluation on long routes like San Francisco to New York, VEGA's stops, dwell
times, SoC management, and total travel time closely track Tesla Trip Planner
while being slightly more conservative, presumably due to real vehicle
conditions such as vehicle parameter drift due to deterioration. Although
trained only in U.S. regions, VEGA was able to compute optimal charge-aware
paths in France and Japan, demonstrating generalizability. It achieves
practical integration of physics-informed learning and RL for EV eco-routing.

</details>


### [8] [A Convex Formulation of Compliant Contact between Filaments and Rigid Bodies](https://arxiv.org/abs/2509.13434)
*Wei-Chen Li,Glen Chou*

Main category: cs.RO

TL;DR: 提出了一个计算框架，用于模拟细长丝状体与刚体之间的接触相互作用，通过统一离散弹性杆模型、压力场接触模型和凸接触公式，实现了精确的摩擦相互作用模拟。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常假设丝状体永久附着在刚体上，无法准确模拟细长丝状体与刚体之间的复杂摩擦相互作用，需要开发新的模拟框架来解决这一挑战。

Method: 结合离散弹性杆(DER)建模、压力场补丁接触模型和凸接触公式，通过凸优化方法确保每个时间步都能达到全局最优解，保证接触速度与冲量之间的互补性。

Result: 验证了摩擦力的准确性，与基准方法相比具有更高的物理保真度，在软机器人(如随机丝状抓手)和可变形物体操作(如鞋带系结)中展示了应用价值。

Conclusion: 该框架为涉及复杂丝状体-丝状体和丝状体-刚体相互作用的系统提供了一个通用的模拟器，解决了传统方法无法实现的精确摩擦相互作用模拟问题。

Abstract: We present a computational framework for simulating filaments interacting
with rigid bodies through contact. Filaments are challenging to simulate due to
their codimensionality, i.e., they are one-dimensional structures embedded in
three-dimensional space. Existing methods often assume that filaments remain
permanently attached to rigid bodies. Our framework unifies discrete elastic
rod (DER) modeling, a pressure field patch contact model, and a convex contact
formulation to accurately simulate frictional interactions between slender
filaments and rigid bodies - capabilities not previously achievable. Owing to
the convex formulation of contact, each time step can be solved to global
optimality, guaranteeing complementarity between contact velocity and impulse.
We validate the framework by assessing the accuracy of frictional forces and
comparing its physical fidelity against baseline methods. Finally, we
demonstrate its applicability in both soft robotics, such as a stochastic
filament-based gripper, and deformable object manipulation, such as shoelace
tying, providing a versatile simulator for systems involving complex
filament-filament and filament-rigid body interactions.

</details>


### [9] [Trajectory Tracking with Reachability-Guided Quadratic Programming and Freeze-Resume](https://arxiv.org/abs/2509.13501)
*Hossein Gholampour,Logan E. Beaver*

Main category: cs.RO

TL;DR: 这篇论文提出了一种输出空间方法，用于在人或物体干预时安全暂停和恢复路径跟踪，无需重新规划。


<details>
  <summary>Details</summary>
Motivation: 解决机器人系统在路径跟踪过程中遇到人或物体干预时的安全暂停和恢复问题，避免重新规划的计算成本。

Method: 方法包括两部分：离线进行达急性检查确认运动规划遵循速度和加速度限制，在线通过二次规划跟踪运动规划，使用一步达急性测试来约束系统能够拒绝的最大干扰。

Result: 当状态与参考路径一致时，系统能够在确定性情况下实现完美跟踪，并通过KKT受权权重来纠正错误。模拟实验显示该方法在安全停止和非计划偏移处理方面效率更高，性能超过纯追踪方法。

Conclusion: 该输出空间方法能够有效处理安全暂停和恢复问题，无需重新规划路径，在保持速度和加速度限制的同时提供更好的跟踪性能。

Abstract: Many robotic systems must follow planned paths yet pause safely and resume
when people or objects intervene. We present an output-space method for systems
whose tracked output can be feedback-linearized to a double integrator (e.g.,
manipulators). The approach has two parts. Offline, we perform a pre-run
reachability check to verify that the motion plan respects speed and
acceleration magnitude limits. Online, we apply a quadratic program to track
the motion plan under the same limits. We use a one-step reachability test to
bound the maximum disturbance the system is capable of rejecting. When the
state coincides with the reference path we recover perfect tracking in the
deterministic case, and we correct errors using a KKT-inspired weight. We
demonstrate that safety stops and unplanned deviations are handled efficiently,
and the system returns to the motion plan without replanning. We demonstrate
our system's improved performance over pure pursuit in simulation.

</details>


### [10] [Embracing Bulky Objects with Humanoid Robots: Whole-Body Manipulation with Reinforcement Learning](https://arxiv.org/abs/2509.13534)
*Chunxin Zheng,Kai Chen,Zhihai Bi,Yulin Li,Liang Pan,Jinni Zhou,Haoang Li,Jun Ma*

Main category: cs.RO

TL;DR: 提出基于强化学习和神经符号距离场的全身拥抱操作框架，通过预训练人体运动先验和教师-学生架构实现稳定多接触交互


<details>
  <summary>Details</summary>
Motivation: 传统末端执行器抓取在操作大体积物体时存在稳定性和负载限制，需要开发更鲁棒的全身操作方法来应对多接触和长时程任务

Method: 结合预训练人体运动先验和神经符号距离场表示，采用教师-学生架构从大规模人体运动数据中提取运动模式，实现协调的全身控制

Result: 在仿真和真实实验中表现出对多样化形状和尺寸物体的适应能力提升，成功实现从仿真到现实的迁移

Conclusion: 该框架为人形机器人的多接触和长时程全身操作任务提供了有效实用的解决方案

Abstract: Whole-body manipulation (WBM) for humanoid robots presents a promising
approach for executing embracing tasks involving bulky objects, where
traditional grasping relying on end-effectors only remains limited in such
scenarios due to inherent stability and payload constraints. This paper
introduces a reinforcement learning framework that integrates a pre-trained
human motion prior with a neural signed distance field (NSDF) representation to
achieve robust whole-body embracing. Our method leverages a teacher-student
architecture to distill large-scale human motion data, generating kinematically
natural and physically feasible whole-body motion patterns. This facilitates
coordinated control across the arms and torso, enabling stable multi-contact
interactions that enhance the robustness in manipulation and also the load
capacity. The embedded NSDF further provides accurate and continuous geometric
perception, improving contact awareness throughout long-horizon tasks. We
thoroughly evaluate the approach through comprehensive simulations and
real-world experiments. The results demonstrate improved adaptability to
diverse shapes and sizes of objects and also successful sim-to-real transfer.
These indicate that the proposed framework offers an effective and practical
solution for multi-contact and long-horizon WBM tasks of humanoid robots.

</details>


### [11] [Semantic 3D Reconstructions with SLAM for Central Airway Obstruction](https://arxiv.org/abs/2509.13541)
*Ayberk Acar,Fangjie Li,Hao Li,Lidia Al-Zogbi,Kanyifeechukwu Jane Oguine,Susheela Sharma Stern,Jesse F. d'Almeida,Robert J. Webster III,Ipek Oguz,Jie Ying Wu*

Main category: cs.RO

TL;DR: 一种新的内镜视频相机SLAM系统，结合语义分割模型，能够实时生成带有阻塞区域注释的中央气道三维重建，为自主机器人干预提供支持。


<details>
  <summary>Details</summary>
Motivation: 中央气道阻塞(CAO)是一种危及生命的病悥，传统治疗方法复杂性高。机器人干预为降低风险提供了可能，而场景理解和地图构建技术的结合为自动化干预打开了大门。

Method: 结合DROID-SLAM和训练识别阻塞组织的分割模型，SLAM模块实时重建气道三维几何，分割掩码指导在点云中注释阻塞区域。使用ex vivo模型评估重建质量。

Result: 定性和定量结果显示与真实CT扫描高度相似（Chamfer距离0.62mm）。系统能够实时生成标注临床相关区域的注释三维地图，重建速度更快且更准确反映手术场景。

Conclusion: 这是首次将语义分割与实时单目SLAM集成用于内镜CAO场景的研究。框架模块化且可扩展到其他解剖结构或手术，为自主机器人干预提供了有前景的步骤。

Abstract: Central airway obstruction (CAO) is a life-threatening condition with
increasing incidence, caused by tumors in and outside of the airway.
Traditional treatment methods such as bronchoscopy and electrocautery can be
used to remove the tumor completely; however, these methods carry a high risk
of complications. Recent advances allow robotic interventions with lesser risk.
The combination of robot interventions with scene understanding and mapping
also opens up the possibilities for automation. We present a novel pipeline
that enables real-time, semantically informed 3D reconstructions of the central
airway using monocular endoscopic video.
  Our approach combines DROID-SLAM with a segmentation model trained to
identify obstructive tissues. The SLAM module reconstructs the 3D geometry of
the airway in real time, while the segmentation masks guide the annotation of
obstruction regions within the reconstructed point cloud. To validate our
pipeline, we evaluate the reconstruction quality using ex vivo models.
  Qualitative and quantitative results show high similarity between ground
truth CT scans and the 3D reconstructions (0.62 mm Chamfer distance). By
integrating segmentation directly into the SLAM workflow, our system produces
annotated 3D maps that highlight clinically relevant regions in real time.
High-speed capabilities of the pipeline allows quicker reconstructions compared
to previous work, reflecting the surgical scene more accurately.
  To the best of our knowledge, this is the first work to integrate semantic
segmentation with real-time monocular SLAM for endoscopic CAO scenarios. Our
framework is modular and can generalize to other anatomies or procedures with
minimal changes, offering a promising step toward autonomous robotic
interventions.

</details>


### [12] [Using Visual Language Models to Control Bionic Hands: Assessment of Object Perception and Grasp Inference](https://arxiv.org/abs/2509.13572)
*Ozan Karaali,Hossam Farag,Strahinja Dosen,Cedomir Stefanovic*

Main category: cs.RO

TL;DR: 本研究探索使用视觉语言模型(VLMs)提升半自主假肢手的感知能力，建立统一基准评估VLM在物体识别和抓取参数推断方面的表现，发现VLM在物体识别方面表现优异，但在尺寸估计和抓取参数推断方面仍有局限。


<details>
  <summary>Details</summary>
Motivation: 传统假肢系统需要复杂的多模块流水线(物体检测、姿态估计、抓取规划)，本研究旨在探索单一VLM是否能统一完成这些任务，简化半自主假肢的感知系统。

Method: 建立包含34个常见物体快照的数据集，设计结构化JSON输出提示，评估8个当代VLM在物体属性识别(名称、形状、方向、尺寸)和抓取参数推断(抓取类型、手腕旋转、手部开合、手指数量)的性能。

Result: 大多数模型在物体识别和形状识别方面表现优异，但在尺寸估计和最优抓取参数推断(特别是手部旋转和开合)方面准确性差异较大，存在明显局限性。

Conclusion: VLM作为半自主仿生肢体控制的高级感知模块具有潜力，但目前在尺寸估计和抓取参数推断方面仍需改进，为未来假肢应用提供了发展方向。

Abstract: This study examines the potential of utilizing Vision Language Models (VLMs)
to improve the perceptual capabilities of semi-autonomous prosthetic hands. We
introduce a unified benchmark for end-to-end perception and grasp inference,
evaluating a single VLM to perform tasks that traditionally require complex
pipelines with separate modules for object detection, pose estimation, and
grasp planning. To establish the feasibility and current limitations of this
approach, we benchmark eight contemporary VLMs on their ability to perform a
unified task essential for bionic grasping. From a single static image, they
should (1) identify common objects and their key properties (name, shape,
orientation, and dimensions), and (2) infer appropriate grasp parameters (grasp
type, wrist rotation, hand aperture, and number of fingers). A corresponding
prompt requesting a structured JSON output was employed with a dataset of 34
snapshots of common objects. Key performance metrics, including accuracy for
categorical attributes (e.g., object name, shape) and errors in numerical
estimates (e.g., dimensions, hand aperture), along with latency and cost, were
analyzed. The results demonstrated that most models exhibited high performance
in object identification and shape recognition, while accuracy in estimating
dimensions and inferring optimal grasp parameters, particularly hand rotation
and aperture, varied more significantly. This work highlights the current
capabilities and limitations of VLMs as advanced perceptual modules for
semi-autonomous control of bionic limbs, demonstrating their potential for
effective prosthetic applications.

</details>


### [13] [Dense-Jump Flow Matching with Non-Uniform Time Scheduling for Robotic Policies: Mitigating Multi-Step Inference Degradation](https://arxiv.org/abs/2509.13574)
*Zidong Chen,Zihao Guo,Peng Wang,ThankGod Itua Egbe,Yan Lyu,Chenghao Qian*

Main category: cs.RO

TL;DR: 通过非均匀时间调度和密集跳跃集成策略，解决流匹配中推理步骤增加导致性能下降的问题，在多种机器人任务中实现了较现有方法较大的性能提升


<details>
  <summary>Details</summary>
Motivation: 发现流匹配框架中通用性在流轨迹早期就出现并饱和，且推理时增加欧拉积分步数反而降低政策性能，需要解决这些问题

Method: 提出使用非均匀时间调度（如U形）在训练中强调早期和晚期阶段，以正则化政策训练；在推理时使用密集跳跃集成调度，通过单步积分替换跳跃点之后的多步积分，避免时间接近1附近的不稳定区域

Result: 在多种机器人任务中实现了较现有最佳方法较大的性能提升，最高达到23.7%的性能增长

Conclusion: 该方法是一种高效的单步学习器，同时能够通过多步积分推动性能向前发展，有效解决了流匹配框架中的性能法循瓶颈问题

Abstract: Flow matching has emerged as a competitive framework for learning
high-quality generative policies in robotics; however, we find that
generalisation arises and saturates early along the flow trajectory, in
accordance with recent findings in the literature. We further observe that
increasing the number of Euler integration steps during inference
counter-intuitively and universally degrades policy performance. We attribute
this to (i) additional, uniformly spaced integration steps oversample the
late-time region, thereby constraining actions towards the training
trajectories and reducing generalisation; and (ii) the learned velocity field
becoming non-Lipschitz as integration time approaches 1, causing instability.
To address these issues, we propose a novel policy that utilises non-uniform
time scheduling (e.g., U-shaped) during training, which emphasises both early
and late temporal stages to regularise policy training, and a dense-jump
integration schedule at inference, which uses a single-step integration to
replace the multi-step integration beyond a jump point, to avoid unstable areas
around 1. Essentially, our policy is an efficient one-step learner that still
pushes forward performance through multi-step integration, yielding up to 23.7%
performance gains over state-of-the-art baselines across diverse robotic tasks.

</details>


### [14] [TreeIRL: Safe Urban Driving with Tree Search and Inverse Reinforcement Learning](https://arxiv.org/abs/2509.13579)
*Momchil S. Tomov,Sang Uk Lee,Hansford Hendrago,Jinwook Huh,Teawon Han,Forbes Howington,Rafael da Silva,Gianmarco Bernasconi,Marc Heim,Samuel Findler,Xiaonan Ji,Alexander Boule,Michael Napoli,Kuo Chen,Jesse Miller,Boaz Floor,Yunqing Hu*

Main category: cs.RO

TL;DR: TreeIRL结合蒙特卡洛树搜索和逆强化学习，在自动驾驶规划中实现最佳性能平衡安全、进度、舒适度和拟人性


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶规划中的瓶颈问题，需要平衡多种性能指标并在真实环境中验证

Method: 使用MCTS生成安全候选轨迹，通过深度IRL评分函数选择最拟人化的轨迹

Result: 在大规模仿真和500+英里真实道路测试中表现最佳，涵盖城市交通、自适应巡航等多种场景

Conclusion: 首次在公共道路上展示MCTS规划，强调多指标评估和真实环境测试的重要性，为经典与学习方法的结合提供框架

Abstract: We present TreeIRL, a novel planner for autonomous driving that combines
Monte Carlo tree search (MCTS) and inverse reinforcement learning (IRL) to
achieve state-of-the-art performance in simulation and in real-world driving.
The core idea is to use MCTS to find a promising set of safe candidate
trajectories and a deep IRL scoring function to select the most human-like
among them. We evaluate TreeIRL against both classical and state-of-the-art
planners in large-scale simulations and on 500+ miles of real-world autonomous
driving in the Las Vegas metropolitan area. Test scenarios include dense urban
traffic, adaptive cruise control, cut-ins, and traffic lights. TreeIRL achieves
the best overall performance, striking a balance between safety, progress,
comfort, and human-likeness. To our knowledge, our work is the first
demonstration of MCTS-based planning on public roads and underscores the
importance of evaluating planners across a diverse set of metrics and in
real-world environments. TreeIRL is highly extensible and could be further
improved with reinforcement learning and imitation learning, providing a
framework for exploring different combinations of classical and learning-based
approaches to solve the planning bottleneck in autonomous driving.

</details>


### [15] [Object Pose Estimation through Dexterous Touch](https://arxiv.org/abs/2509.13591)
*Amir-Hossein Shahidzadeh,Jiyue Zhu,Kezhou Chen,Sha Yi,Cornelia Fermüller,Yiannis Aloimonos,Xiaolong Wang*

Main category: cs.RO

TL;DR: 通过双手机器人主动触觉探索来估计物体姿态，使用强化学习收集触觉数据并迭代精炼物体形状和姿态


<details>
  <summary>Details</summary>
Motivation: 在视觉数据有限或受到照明、遮挡和外观影响的场景下，需要稳健的物体姿态估计方法来支持操作和交互任务

Method: 使用双手机器人配置（一只手持稳物体，另一只手进行主动探索），通过强化学习训练控制探索行动，收集触觉3D点云数据并迭代精炼物体形状和姿态

Result: 方法能够主动探索物体表面以识别关键姿态特征，无需先验知道物体的几何形状

Conclusion: 提出了一种基于触觉主动探索的新方法，能够在缺乏先验知识的情况下精确估计物体姿态，适用于视觉数据有限的环境

Abstract: Robust object pose estimation is essential for manipulation and interaction
tasks in robotics, particularly in scenarios where visual data is limited or
sensitive to lighting, occlusions, and appearances. Tactile sensors often offer
limited and local contact information, making it challenging to reconstruct the
pose from partial data. Our approach uses sensorimotor exploration to actively
control a robot hand to interact with the object. We train with Reinforcement
Learning (RL) to explore and collect tactile data. The collected 3D point
clouds are used to iteratively refine the object's shape and pose. In our
setup, one hand holds the object steady while the other performs active
exploration. We show that our method can actively explore an object's surface
to identify critical pose features without prior knowledge of the object's
geometry. Supplementary material and more demonstrations will be provided at
https://amirshahid.github.io/BimanualTactilePose .

</details>


### [16] [Leg-Arm Coordinated Operation for Curtain Wall Installation](https://arxiv.org/abs/2509.13595)
*Xiao Liu,Weijun Wang,Tianlun Huang,Zhiyong Wang,Wei Feng*

Main category: cs.RO

TL;DR: 基于六足帘墙安装机器人的分层优化整体控制框架，实现臂腿协调规划，有效解决传统帘墙安装中的效率低、安全风险高等问题


<details>
  <summary>Details</summary>
Motivation: 基于城市化加速背景，高层建筑和大型公共设施增多，帘墙应用普遍。传统安装方法面临现场地形复杂、劳动强度高、施工效率低、安全风险大等挑战

Method: 设计分层优化基础的整体控制框架，专门为墙体安装、天花板安装和地板铺设三种关键任务而设计的臂腿协调规划方法，集成六足腿部运动、折叠臂和串并联操纶器的运动

Result: 通过在六足帘墙安装机器人上进行实验，验证了控制方法的有效性，证明其能够完成帘墙安装任务

Conclusion: 分层优化基础的臂腿协调框架对六足机器人有效，为其在复杂施工环境中的进一步应用奠定了基础

Abstract: With the acceleration of urbanization, the number of high-rise buildings and
large public facilities is increasing, making curtain walls an essential
component of modern architecture with widespread applications. Traditional
curtain wall installation methods face challenges such as variable on-site
terrain, high labor intensity, low construction efficiency, and significant
safety risks. Large panels often require multiple workers to complete
installation. To address these issues, based on a hexapod curtain wall
installation robot, we design a hierarchical optimization-based whole-body
control framework for coordinated arm-leg planning tailored to three key tasks:
wall installation, ceiling installation, and floor laying. This framework
integrates the motion of the hexapod legs with the operation of the folding arm
and the serial-parallel manipulator. We conduct experiments on the hexapod
curtain wall installation robot to validate the proposed control method,
demonstrating its capability in performing curtain wall installation tasks. Our
results confirm the effectiveness of the hierarchical optimization-based
arm-leg coordination framework for the hexapod robot, laying the foundation for
its further application in complex construction site environments.

</details>


### [17] [Barometer-Aided Attitude Estimation](https://arxiv.org/abs/2509.13649)
*Méloné Nyoba Tchonkeu,Soulaimane Berkane,Tarek Hamel*

Main category: cs.RO

TL;DR: 基于气压计测高度的新题态估计方法，通过非线性观测器在SO(3)上推断垂直速度和体态，解决IMU在GNSS缺失环境中的加速度水平/垂直方向模糊性问题


<details>
  <summary>Details</summary>
Motivation: 解决自主车辆在GNSS缺失或高动态环境中的可靠态态估计挑战，IMU单独使用时因重力加速度和惯性加速度的模糊性而不可靠，而传统的辅助速度传感器又存在不可用、间断或成本高等问题

Method: 提出基于气压计测高度的态态估计架构，利用气压高度测量推断垂直速度和体态，采用确定性Riccati观测器与互补滤波器的级联设计，在SO(3)上实现非线性观测

Result: 设计确保了在均匀可观测条件下的几乎全局趋近稳定性(AGAS)，同时保持几何一致性，证明气压计辅助估计是一种轻量级且有效的互补方式

Conclusion: 气压计辅助态态估计提供了一种简单有效的解决方案，能够在不依赖高成本或易受干扰的辅助传感器的情况下，提高IMU在复杂环境中的态态估计性能

Abstract: Accurate and robust attitude estimation is a central challenge for autonomous
vehicles operating in GNSS-denied or highly dynamic environments. In such
cases, Inertial Measurement Units (IMUs) alone are insufficient for reliable
tilt estimation due to the ambiguity between gravitational and inertial
accelerations. While auxiliary velocity sensors, such as GNSS, Pitot tubes,
Doppler radar, or visual odometry, are often used, they can be unavailable,
intermittent, or costly. This work introduces a barometer-aided attitude
estimation architecture that leverages barometric altitude measurements to
infer vertical velocity and attitude within a nonlinear observer on SO(3). The
design cascades a deterministic Riccati observer with a complementary filter,
ensuring Almost Global Asymptotic Stability (AGAS) under a uniform
observability condition while maintaining geometric consistency. The analysis
highlights barometer-aided estimation as a lightweight and effective
complementary modality.

</details>


### [18] [DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater Monitoring](https://arxiv.org/abs/2509.13666)
*Zhenqi Wu,Abhinav Modi,Angelos Mavrogiannis,Kaustubh Joshi,Nikhil Chopra,Yiannis Aloimonos,Nare Karapetyan,Ioannis Rekleitis,Xiaomin Lin*

Main category: cs.RO

TL;DR: 提出了DREAM框架，一个基于视觉语言模型的自主系统，用于水下长期探索和栖息地监测，显著提高了目标检测效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 海洋变暖和酸化增加了贝类大规模死亡风险，需要长期监测系统。人工成本高且危险，需要机器人解决方案和智能决策能力。

Method: 开发了DREAM框架，使用视觉语言模型(VLM)指导自主决策，实现无先验位置信息的目标探索和监测。

Result: 在牡蛎监测任务中，比基线节省31.5%时间；比普通VLM减少23%步骤且覆盖更多牡蛎；在沉船场景中实现100%覆盖且减少27.5%步骤。

Conclusion: DREAM框架为水下长期监测提供了高效、自主的解决方案，显著优于现有方法，具有实际应用价值。

Abstract: The ocean is warming and acidifying, increasing the risk of mass mortality
events for temperature-sensitive shellfish such as oysters. This motivates the
development of long-term monitoring systems. However, human labor is costly and
long-duration underwater work is highly hazardous, thus favoring robotic
solutions as a safer and more efficient option. To enable underwater robots to
make real-time, environment-aware decisions without human intervention, we must
equip them with an intelligent "brain." This highlights the need for
persistent,wide-area, and low-cost benthic monitoring. To this end, we present
DREAM, a Vision Language Model (VLM)-guided autonomy framework for long-term
underwater exploration and habitat monitoring. The results show that our
framework is highly efficient in finding and exploring target objects (e.g.,
oysters, shipwrecks) without prior location information. In the
oyster-monitoring task, our framework takes 31.5% less time than the previous
baseline with the same amount of oysters. Compared to the vanilla VLM, it uses
23% fewer steps while covering 8.88% more oysters. In shipwreck scenes, our
framework successfully explores and maps the wreck without collisions,
requiring 27.5% fewer steps than the vanilla model and achieving 100% coverage,
while the vanilla model achieves 60.23% average coverage in our shipwreck
environments.

</details>


### [19] [SPAR: Scalable LLM-based PDDL Domain Generation for Aerial Robotics](https://arxiv.org/abs/2509.13691)
*Songhao Huang,Yuwei Wu,Guangyao Shi,Gaurav S. Sukhatme,Vijay Kumar*

Main category: cs.RO

TL;DR: 使用大型语言模型自动生成PDDL规划领域，特别是针对无人机任务，提出了SPAR框架从自然语言输入生成有效、多样且语义准确的PDDL领域。


<details>
  <summary>Details</summary>
Motivation: 手动设计PDDL领域对于无人机等多样化应用（如监视、交付、检查）既费时又容易出错，阻碍了实际部署和采用。

Method: 提出SPAR框架，利用LLM的生成能力从自然语言输入自动生成PDDL领域。首先构建系统验证的无人机规划数据集，然后设计提示框架生成高质量PDDL领域。

Result: 生成的领域通过语法验证、可执行性、可行性和可解释性评估，证明LLM能显著加速复杂规划领域的创建。

Conclusion: 该工作展示了LLM在自动规划领域的潜力，为无经验的领域专家提供了实用工具，推动了空中机器人和自动化规划的未来研究。

Abstract: We investigate the problem of automatic domain generation for the Planning
Domain Definition Language (PDDL) using Large Language Models (LLMs), with a
particular focus on unmanned aerial vehicle (UAV) tasks. Although PDDL is a
widely adopted standard in robotic planning, manually designing domains for
diverse applications such as surveillance, delivery, and inspection is
labor-intensive and error-prone, which hinders adoption and real-world
deployment. To address these challenges, we propose SPAR, a framework that
leverages the generative capabilities of LLMs to automatically produce valid,
diverse, and semantically accurate PDDL domains from natural language input. To
this end, we first introduce a systematically formulated and validated UAV
planning dataset, consisting of ground-truth PDDL domains and associated
problems, each paired with detailed domain and action descriptions. Building on
this dataset, we design a prompting framework that generates high-quality PDDL
domains from language input. The generated domains are evaluated through syntax
validation, executability, feasibility, and interpretability. Overall, this
work demonstrates that LLMs can substantially accelerate the creation of
complex planning domains, providing a reproducible dataset and evaluation
pipeline that enables application experts without prior experience to leverage
it for practical tasks and advance future research in aerial robotics and
automated planning.

</details>


### [20] [HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion](https://arxiv.org/abs/2509.13692)
*Yadan Zeng,Jiadong Zhou,Xiaohan Li,I-Ming Chen*

Main category: cs.RO

TL;DR: HGACNet是一个新颖的点云补全框架，通过分层图注意力编码器和多尺度跨模态融合模块，结合单视图RGB图像指导，实现了高质量的点云补全。


<details>
  <summary>Details</summary>
Motivation: 解决由于自遮挡和传感器限制导致的不完整几何问题，这些不完整的点云会严重影响下游的机器人感知、物体重建和交互任务。

Method: 使用分层图注意力编码器自适应选择关键局部点，通过图注意力下采样逐步细化分层几何特征；设计多尺度跨模态融合模块进行注意力特征对齐；提出对比损失来显式对齐跨模态特征分布。

Result: 在ShapeNet-ViPC基准和YCB-Complete数据集上的广泛实验证实了HGACNet的有效性，展示了最先进的性能以及在真实世界机器人操作任务中的强大适用性。

Conclusion: HGACNet通过分层几何特征编码和图像引导先验的融合，成功解决了点云补全问题，在多个基准测试中达到了state-of-the-art性能，并具有良好的实际应用价值。

Abstract: Point cloud completion is essential for robotic perception, object
reconstruction and supporting downstream tasks like grasp planning, obstacle
avoidance, and manipulation. However, incomplete geometry caused by
self-occlusion and sensor limitations can significantly degrade downstream
reasoning and interaction. To address these challenges, we propose HGACNet, a
novel framework that reconstructs complete point clouds of individual objects
by hierarchically encoding 3D geometric features and fusing them with
image-guided priors from a single-view RGB image. At the core of our approach,
the Hierarchical Graph Attention (HGA) encoder adaptively selects critical
local points through graph attention-based downsampling and progressively
refines hierarchical geometric features to better capture structural continuity
and spatial relationships. To strengthen cross-modal interaction, we further
design a Multi-Scale Cross-Modal Fusion (MSCF) module that performs
attention-based feature alignment between hierarchical geometric features and
structured visual representations, enabling fine-grained semantic guidance for
completion. In addition, we proposed the contrastive loss (C-Loss) to
explicitly align the feature distributions across modalities, improving
completion fidelity under modality discrepancy. Finally, extensive experiments
conducted on both the ShapeNet-ViPC benchmark and the YCB-Complete dataset
confirm the effectiveness of HGACNet, demonstrating state-of-the-art
performance as well as strong applicability in real-world robotic manipulation
tasks.

</details>


### [21] [EZREAL: Enhancing Zero-Shot Outdoor Robot Navigation toward Distant Targets under Varying Visibility](https://arxiv.org/abs/2509.13720)
*Tianle Zeng,Jianwei Peng,Hanjing Ye,Guangcheng Chen,Senzi Luo,Hong Zhang*

Main category: cs.RO

TL;DR: 提出了一种统一的轻量级闭环系统，用于解决户外零样本目标导航中的长距离目标和小投影问题，通过多尺度图像瓦片层次结构和层次化目标显著性融合来实现稳定的方向维持和目标检测。


<details>
  <summary>Details</summary>
Motivation: 解决户外零样本目标导航中长距离目标投影过小和间歇性可见性（部分或完全遮挡）的耦合挑战。

Method: 基于对齐的多尺度图像瓦片层次结构，通过层次化目标显著性融合将局部语义对比汇总为稳定的粗层区域显著性，提供目标方向并指示目标可见性。使用关键帧记忆、显著性加权历史航向融合和在临时不可见期间主动搜索来维持航向。

Result: 在仿真和真实户外试验中，系统能检测超过150米的语义目标，在可见性变化下以82.6%的概率维持正确航向，相比最先进方法整体任务成功率提高17.5%。

Conclusion: 该系统展示了针对远距离和间歇性可观测目标的鲁棒零样本目标导航能力，避免了整图重缩放，支持确定性自下而上聚合，并能高效运行在移动机器人上。

Abstract: Zero-shot object navigation (ZSON) in large-scale outdoor environments faces
many challenges; we specifically address a coupled one: long-range targets that
reduce to tiny projections and intermittent visibility due to partial or
complete occlusion. We present a unified, lightweight closed-loop system built
on an aligned multi-scale image tile hierarchy. Through hierarchical
target-saliency fusion, it summarizes localized semantic contrast into a stable
coarse-layer regional saliency that provides the target direction and indicates
target visibility. This regional saliency supports visibility-aware heading
maintenance through keyframe memory, saliency-weighted fusion of historical
headings, and active search during temporary invisibility. The system avoids
whole-image rescaling, enables deterministic bottom-up aggregation, supports
zero-shot navigation, and runs efficiently on a mobile robot. Across simulation
and real-world outdoor trials, the system detects semantic targets beyond 150m,
maintains a correct heading through visibility changes with 82.6% probability,
and improves overall task success by 17.5% compared with the SOTA methods,
demonstrating robust ZSON toward distant and intermittently observable targets.

</details>


### [22] [Reinforcement Learning for Robotic Insertion of Flexible Cables in Industrial Settings](https://arxiv.org/abs/2509.13731)
*Jeongwoo Park,Seabin Lee,Changmin Park,Wonjong Lee,Changjoo Nam*

Main category: cs.RO

TL;DR: 使用基础模型SAM2和VLM的实中任务学习方法，通过语义分割提高模拟到实际的转移效果，实现了灵活网线插入任务的零样本部署。


<details>
  <summary>Details</summary>
Motivation: 解决灵活网线插入任务中的以下挑战：需要次毫米精度操作、变形网线导致的非确定性、直接实际训练的安全风险、以及传统方法需要耗时的人工指导轨迹生成。

Method: 提出一种基于基础模型的实中任务学习算法：1）在模拟环境中完全进行训练，避免物理风险；2）通过语义分割提取网线和插座的几何和空间信息，实现模拟到实际的转移；3）使用Segment Anything Model 2（SAM2）获取分割掩码；4）使用视觉-语言模型（VLM）自动化SAM2的初始提示过程。

Result: 该方法在实验中展现出零样本能力，可以直接在实际环境中部署而无需微调，显著减少了训练时间并消除了物理风险。

Conclusion: 通过基础模型和语义分割的结合，成功实现了灵活网线插入任务的安全、高效自动化，为复杂实中任务提供了一种可靠的解决方案。

Abstract: The industrial insertion of flexible flat cables (FFCs) into receptacles
presents a significant challenge owing to the need for submillimeter precision
when handling the deformable cables. In manufacturing processes, FFC insertion
with robotic manipulators often requires laborious human-guided trajectory
generation. While Reinforcement Learning (RL) offers a solution to automate
this task without modeling complex properties of FFCs, the nondeterminism
caused by the deformability of FFCs requires significant efforts and time on
training. Moreover, training directly in a real environment is dangerous as
industrial robots move fast and possess no safety measure. We propose an RL
algorithm for FFC insertion that leverages a foundation model-based real-to-sim
approach to reduce the training time and eliminate the risk of physical damages
to robots and surroundings. Training is done entirely in simulation, allowing
for random exploration without the risk of physical damages. Sim-to-real
transfer is achieved through semantic segmentation masks which leave only those
visual features relevant to the insertion tasks such as the geometric and
spatial information of the cables and receptacles. To enhance generality, we
use a foundation model, Segment Anything Model 2 (SAM2). To eleminate human
intervention, we employ a Vision-Language Model (VLM) to automate the initial
prompting of SAM2 to find segmentation masks. In the experiments, our method
exhibits zero-shot capabilities, which enable direct deployments to real
environments without fine-tuning.

</details>


### [23] [FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph](https://arxiv.org/abs/2509.13733)
*Xiaolin Zhou,Tingyang Xiao,Liu Liu,Yucheng Wang,Maiyue Chen,Xinrui Meng,Xinjie Wang,Wei Feng,Wei Sui,Zhizhong Su*

Main category: cs.RO

TL;DR: FSR-VLN是一个视觉语言导航系统，通过分层多模态场景图和快慢导航推理机制，在长距离导航任务中实现了最先进的性能，同时大幅降低了响应时间。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉语言导航方法在长距离空间推理方面的局限性，包括低成功率高延迟的问题，特别是在长距离导航任务中。

Method: 结合分层多模态场景图(HMSG)和快慢导航推理(FSR)。HMSG提供多模态地图表示支持从粗粒度房间定位到细粒度目标视图和物体识别的渐进检索。FSR先进行快速匹配选择候选，然后使用VLM驱动的精炼进行最终目标选择。

Result: 在4个室内数据集上实现最先进的检索成功率(RSR)，相比基于VLM的方法响应时间减少82%。成功集成到Unitree-G1人形机器人上实现自然语言交互和实时导航。

Conclusion: FSR-VLN通过分层场景图和快慢推理机制有效解决了长距离视觉语言导航的挑战，在性能和效率方面都取得了显著提升，为机器人实际部署提供了可行方案。

Abstract: Visual-Language Navigation (VLN) is a fundamental challenge in robotic
systems, with broad applications for the deployment of embodied agents in
real-world environments. Despite recent advances, existing approaches are
limited in long-range spatial reasoning, often exhibiting low success rates and
high inference latency, particularly in long-range navigation tasks. To address
these limitations, we propose FSR-VLN, a vision-language navigation system that
combines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow
Navigation Reasoning (FSR). The HMSG provides a multi-modal map representation
supporting progressive retrieval, from coarse room-level localization to
fine-grained goal view and object identification. Building on HMSG, FSR first
performs fast matching to efficiently select candidate rooms, views, and
objects, then applies VLM-driven refinement for final goal selection. We
evaluated FSR-VLN across four comprehensive indoor datasets collected by
humanoid robots, utilizing 87 instructions that encompass a diverse range of
object categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all
datasets, measured by the retrieval success rate (RSR), while reducing the
response time by 82% compared to VLM-based methods on tour videos by activating
slow reasoning only when fast intuition fails. Furthermore, we integrate
FSR-VLN with speech interaction, planning, and control modules on a Unitree-G1
humanoid robot, enabling natural language interaction and real-time navigation.

</details>


### [24] [Motion Adaptation Across Users and Tasks for Exoskeletons via Meta-Learning](https://arxiv.org/abs/2509.13736)
*Muyuan Ma,Long Cheng,Lijun Han,Xiuze Xia,Houcheng Li*

Main category: cs.RO

TL;DR: 一种基于元模仿学习的方法，通过从公开RGB视频和动作捕获数据集中提取全身关键点运动，进行模拟重定向，训练任务特定神经网络预测手肘关节运动，并通过MAML框架快速适配新任务和新用户，实现了可穿戴外骨架系统的任务永性化和用户适配性。


<details>
  <summary>Details</summary>
Motivation: 解决可穿戴外骨架系统在个性化和任务永性化辅助算法方面的关键挑战，提高外骨架在新场景和新用户上的适配性和性能。

Method: 采用元模仿学习方法，从公开RGB视频和动作捕获数据集中提取全身关键点运动，通过模拟重定向生成手肘屈曲轨迹，在MAML框架下训练任务特定神经网络，并使用重力补偿PD控制器进行稳定辅助。

Result: 实验结果显示，外骨架在新用户执行未经训练任务时，与无辅助情况相比，显著减少了肌肉激活和代谢成本。

Conclusion: 该框架能够有效提高可穿戴外骨架系统的任务永性化和用户适配性。

Abstract: Wearable exoskeletons can augment human strength and reduce muscle fatigue
during specific tasks. However, developing personalized and task-generalizable
assistance algorithms remains a critical challenge. To address this, a
meta-imitation learning approach is proposed. This approach leverages a
task-specific neural network to predict human elbow joint movements, enabling
effective assistance while enhancing generalization to new scenarios. To
accelerate data collection, full-body keypoint motions are extracted from
publicly available RGB video and motion-capture datasets across multiple tasks,
and subsequently retargeted in simulation. Elbow flexion trajectories generated
in simulation are then used to train the task-specific neural network within
the model-agnostic meta-learning (MAML) framework, which allows the network to
rapidly adapt to novel tasks and unseen users with only a few gradient updates.
The adapted network outputs personalized references tracked by a
gravity-compensated PD controller to ensure stable assistance. Experimental
results demonstrate that the exoskeleton significantly reduces both muscle
activation and metabolic cost for new users performing untrained tasks,
compared to performing without exoskeleton assistance. These findings suggest
that the proposed framework effectively improves task generalization and user
adaptability for wearable exoskeleton systems.

</details>


### [25] [Dynamic Adaptive Legged Locomotion Policy via Decoupling Reaction Force Control and Gait Control](https://arxiv.org/abs/2509.13737)
*Renjie Wang,Shangke Lyu,Donglin Wang*

Main category: cs.RO

TL;DR: 提出解耦框架分离支撑腿和摆动腿控制，通过在线快速适应能力解决强化学习在腿式运动控制中的分布外性能下降和仿真到现实差距问题


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在腿式运动控制中面临分布外条件性能下降和仿真现实差距问题，主要依赖域随机化方法覆盖真实环境，但效果有限

Method: 提出解耦框架，将支撑腿控制和摆动腿控制分离，获得快速在线适应能力，缓解在陌生环境中的仿真到现实问题

Result: 在各种仿真和真实世界实验中证明了对水平力干扰、不平坦地形、重载和偏置载荷以及仿真现实差距的有效性

Conclusion: 解耦控制框架为解决腿式机器人的仿真到现实迁移和鲁棒性问题提供了有效的新途径

Abstract: While Reinforcement Learning (RL) has achieved remarkable progress in legged
locomotion control, it often suffers from performance degradation in
out-of-distribution (OOD) conditions and discrepancies between the simulation
and the real environments. Instead of mainly relying on domain randomization
(DR) to best cover the real environments and thereby close the sim-to-real gap
and enhance robustness, this work proposes an emerging decoupled framework that
acquires fast online adaptation ability and mitigates the sim-to-real problems
in unfamiliar environments by isolating stance-leg control and swing-leg
control. Various simulation and real-world experiments demonstrate its
effectiveness against horizontal force disturbances, uneven terrains, heavy and
biased payloads, and sim-to-real gap.

</details>


### [26] [CDFlow: Generative Gradient Flows for Configuration Space Distance Fields via Neural ODEs](https://arxiv.org/abs/2509.13771)
*Mengzhu Li,Yunyu Zhou,He Ying,F. Richard Yu*

Main category: cs.RO

TL;DR: CDFlow是一个基于神经ODE的配置空间距离场框架，解决了高自由度机器人中现有CDF方法的梯度模糊和几何失真问题，通过建模多模态碰撞配置分布来提升运动规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有配置空间距离场(CDF)方法在高自由度机器人中存在两个主要问题：1)只返回单个最近碰撞配置，忽略了多模态特性导致梯度模糊；2)依赖稀疏采样，无法识别真正最近配置，产生过度平滑的近似和几何失真。

Method: 提出CDFlow框架，使用神经ODE学习配置空间中的连续流，将问题重新定义为建模最小距离碰撞配置的分布。引入自适应细化采样策略生成高质量训练数据，神经ODE隐式建模多模态分布并产生平滑一致的梯度场。

Result: 在高自由度运动规划任务上的大量实验表明，CDFlow相比现有CDF方法显著提高了规划效率、轨迹质量和鲁棒性。

Conclusion: CDFlow能够为碰撞感知机器人在复杂环境中实现更鲁棒和高效的运动规划，通过建模多模态分布解决了梯度模糊问题并保持了尖锐的几何特征。

Abstract: Signed Distance Fields (SDFs) are a fundamental representation in robot
motion planning. Their configuration-space counterpart, the Configuration Space
Distance Field (CDF), directly encodes distances in joint space, offering a
unified representation for optimization and control. However, existing CDF
formulations face two major challenges in high-degree-of-freedom (DoF) robots:
(1) they effectively return only a single nearest collision configuration,
neglecting the multi-modal nature of minimal-distance collision configurations
and leading to gradient ambiguity; and (2) they rely on sparse sampling of the
collision boundary, which often fails to identify the true closest
configurations, producing oversmoothed approximations and geometric distortion
in high-dimensional spaces. We propose CDFlow, a novel framework that addresses
these limitations by learning a continuous flow in configuration space via
Neural Ordinary Differential Equations (Neural ODEs). We redefine the problem
from finding a single nearest point to modeling the distribution of
minimal-distance collision configurations. We also introduce an adaptive
refinement sampling strategy to generate high-fidelity training data for this
distribution. The resulting Neural ODE implicitly models this multi-modal
distribution and produces a smooth, consistent gradient field-derived as the
expected direction towards the distribution-that mitigates gradient ambiguity
and preserves sharp geometric features. Extensive experiments on high-DoF
motion planning tasks demonstrate that CDFlow significantly improves planning
efficiency, trajectory quality, and robustness compared to existing CDF-based
methods, enabling more robust and efficient planning for collision-aware robots
in complex environments.

</details>


### [27] [Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach](https://arxiv.org/abs/2509.13774)
*Piaopiao Jin,Qi Wang,Guokang Sun,Ziwen Cai,Pinjia He,Yangwei You*

Main category: cs.RO

TL;DR: 一种基于强化学习的人在循环双演员精细调整框架，通过语义基础的人类纠正生成新数据集，在多任务和长期限任务中展现出艰非凡的性能和可扩展性


<details>
  <summary>Details</summary>
Motivation: 解决视觉-语言-动作(VLA)模型在复杂真实任务中的挑战，克服监督精细调整对数据质量的依赖，利用强化学习作为替代方案

Method: 提出人在循环双演员精细调整框架：主演员负责稳健多任务性能，精细演员进行潜在空间适应。重点介绍请谈和调整方案，将人类纠正转换为语义基础的语言命令，生成新的策略学习数据集

Result: 在真实世界多任务实验中，在101分钟的在线精细调整内完成三个任务的100%成功率。在长期限任务中，能够在12个连续操作中保持50%的成功率。多机器人训练效率提升2倍

Conclusion: 该框架通过人类互动精细调整有效提升了VLA模型在复杂真实任务中的性能，具有良好的可扩展性和实用价值

Abstract: Vision-language-action (VLA) models demonstrate strong generalization in
robotic manipulation but face challenges in complex, real-world tasks. While
supervised fine-tuning with demonstrations is constrained by data quality,
reinforcement learning (RL) offers a promising alternative. We propose a
human-in-the-loop dual-actor fine-tuning framework grounded in RL. The
framework integrates a primary actor for robust multi-task performance with a
refinement actor for latent-space adaptation. Beyond standard physical
interventions, we introduce a lightweight talk-and-tweak scheme that converts
human corrections into semantically grounded language commands, thereby
generating a new dataset for policy learning. In real-world multi-task
experiments, our approach achieves 100% success across three tasks within 101
minutes of online fine-tuning. For long-horizon tasks, it sustains a 50%
success rate over 12 consecutive operations. Furthermore, the framework scales
effectively to multi-robot training, achieving up to a 2 times improvement in
efficiency when using dual robots. The experiment videos are available at
https://sites.google.com/view/hil-daft/.

</details>


### [28] [Behavior Foundation Model for Humanoid Robots](https://arxiv.org/abs/2509.13780)
*Weishuai Zeng,Shunlin Lu,Kangning Yin,Xiaojie Niu,Minyue Dai,Jingbo Wang,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 提出了行为基础模型（BFM），这是一个基于大规模行为数据集预训练的生成模型，用于人形机器人的全身控制，能够跨多种控制模式灵活操作并快速适应新行为。


<details>
  <summary>Details</summary>
Motivation: 现有的全身控制框架主要针对特定任务，依赖大量人工奖励工程，跨任务和技能的泛化能力有限，阻碍了在复杂现实场景中的部署。

Method: 采用掩码在线蒸馏框架与条件变分自编码器（CVAE）相结合，建模行为分布，无需从头开始重新训练即可高效获取新行为。

Result: 在仿真和物理人形平台上的大量实验表明，BFM能够稳健地泛化到不同的全身控制任务，同时快速适应新行为。

Conclusion: BFM为通用人形控制的基础模型迈出了有希望的一步，展示了在多样化控制模式下的灵活操作能力。

Abstract: Whole-body control (WBC) of humanoid robots has witnessed remarkable progress
in skill versatility, enabling a wide range of applications such as locomotion,
teleoperation, and motion tracking. Despite these achievements, existing WBC
frameworks remain largely task-specific, relying heavily on labor-intensive
reward engineering and demonstrating limited generalization across tasks and
skills. These limitations hinder their response to arbitrary control modes and
restrict their deployment in complex, real-world scenarios. To address these
challenges, we revisit existing WBC systems and identify a shared objective
across diverse tasks: the generation of appropriate behaviors that guide the
robot toward desired goal states. Building on this insight, we propose the
Behavior Foundation Model (BFM), a generative model pretrained on large-scale
behavioral datasets to capture broad, reusable behavioral knowledge for
humanoid robots. BFM integrates a masked online distillation framework with a
Conditional Variational Autoencoder (CVAE) to model behavioral distributions,
thereby enabling flexible operation across diverse control modes and efficient
acquisition of novel behaviors without retraining from scratch. Extensive
experiments in both simulation and on a physical humanoid platform demonstrate
that BFM generalizes robustly across diverse WBC tasks while rapidly adapting
to new behaviors. These results establish BFM as a promising step toward a
foundation model for general-purpose humanoid control.

</details>


### [29] [Shell-Type Soft Jig for Holding Objects during Disassembly](https://arxiv.org/abs/2509.13802)
*Takuya Kiyokawa,Ryunosuke Takebayashi,Kensuke Harada*

Main category: cs.RO

TL;DR: 这篇论文提出了一种壶型软头具，用于机器人分解任务中的灵活头持工具，通过气球基机制实现对多种形状物体的安全稳定头持，减少了对精确感知和控制的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决传统头持工具在机器人分解中存在的问题：需要专门设计头具、高精度感知、精确抓取和精细调整轨迹规划，而且容易造成组件损坏。

Method: 设计了一种壶型软头具，采用气球基头持机制，能够安全地且通用地头持物体，适应不同形状，并对识别、规划和控制错误具有强锐性。

Result: 通过与臂卡和受塞位放器启发的软头具进行性能对比实验，证明了该头具的实际可行性。在10种不同物体上的测试显示了代表性成功和失败案例，明确了头具的局限性和发展前景。

Conclusion: 该壶型软头具为机器人分解任务提供了一种有效的解决方案，能够在保证物体安全的同时适应多样化形状，并降低对精确控制的要求，为软性头持技术的发展提供了新的思路。

Abstract: This study addresses a flexible holding tool for robotic disassembly. We
propose a shell-type soft jig that securely and universally holds objects,
mitigating the risk of component damage and adapting to diverse shapes while
enabling soft fixation that is robust to recognition, planning, and control
errors. The balloon-based holding mechanism ensures proper alignment and stable
holding performance, thereby reducing the need for dedicated jig design, highly
accurate perception, precise grasping, and finely tuned trajectory planning
that are typically required with conventional fixtures. Our experimental
results demonstrate the practical feasibility of the proposed jig through
performance comparisons with a vise and a jamming-gripper-inspired soft jig.
Tests on ten different objects further showed representative successes and
failures, clarifying the jig's limitations and outlook.

</details>


### [30] [Soft Regrasping Tool Inspired by Jamming Gripper](https://arxiv.org/abs/2509.13815)
*Takuya Kiyokawa,Zhengtao Hu,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: 这篇论文提出了一种基于软件夹具的重新抓取方法，通过软件夹具的变形能力来减少机器人组装中的位姿不确定性，实现了高成功率的精确放置。


<details>
  <summary>Details</summary>
Motivation: 传统硬件夹具缺乏适应性且需要专门设计，无法适应多样化的工件形状。需要一种更灵活的方案来提高机器人组装的通用性和准确性。

Method: 使用受粘聚过渡现象受灵感的软件夹具，通过将三角锥形工具压入膜内并排空空气来形成稳定的定位空间。优化压刷深度以平衡放置稳定性和抓取器可达性。

Result: 在10种不同形状的机械部件上进行的放置实验中，大多数对象的放置成功率超过80%，圆柱形部件达到90%以上。失败主要由于几何约束和膜性质导致。

Conclusion: 该软件夹具能够实现通用、准确且可重复的重新抓取，显示了作为硬件夹具实用替代方案的潜力，同时也明确了当前的限制和未来发展方向。

Abstract: Regrasping on fixtures is a promising approach to reduce pose uncertainty in
robotic assembly, but conventional rigid fixtures lack adaptability and require
dedicated designs for each part. To overcome this limitation, we propose a soft
jig inspired by the jamming transition phenomenon, which can be continuously
deformed to accommodate diverse object geometries. By pressing a
triangular-pyramid-shaped tool into the membrane and evacuating the enclosed
air, a stable cavity is formed as a placement space. We further optimize the
stamping depth to balance placement stability and gripper accessibility. In
soft-jig-based regrasping, the key challenge lies in optimizing the cavity size
to achieve precise dropping; once the part is reliably placed, subsequent
grasping can be performed with reduced uncertainty. Accordingly, we conducted
drop experiments on ten mechanical parts of varying shapes, which achieved
placement success rates exceeding 80% for most objects and above 90% for
cylindrical ones, while failures were mainly caused by geometric constraints
and membrane properties. These results demonstrate that the proposed jig
enables general-purpose, accurate, and repeatable regrasping, while also
clarifying its current limitations and future potential as a practical
alternative to rigid fixtures in assembly automation.

</details>


### [31] [Agile in the Face of Delay: Asynchronous End-to-End Learning for Real-World Aerial Navigation](https://arxiv.org/abs/2509.13816)
*Yude Li,Zhexuan Zhou,Huizhe Li,Youmin Gong,Jie Mei*

Main category: cs.RO

TL;DR: 通过异步加强学习框架解耦感知与控制，解决了自主航空器在复杂环境中高频控制与低频感知的矛盾，实现了100Hz的高频控制和稳健的实际航行。


<details>
  <summary>Details</summary>
Motivation: 现代端到端导航系统中，高频控制循环与低频感知流之间存在明显矛盾，导致传统同步模型只能以较低控制频率运行，影响了自主航空器的灵活性和反应能力。

Method: 提出异步加强学习框架，将感知与控制解耦；设计时间编码模块(TEM)显式处理感知延迟；采用两阶段课程学习确保训练稳定性。

Result: 在模拟环境中验证成功，通过零样本模拟到实际转移部署在板载NUC上，维持100Hz控制频率，在杂乱实际环境中展现出稳健灵活的导航能力。

Conclusion: 该异步框架有效解决了高频控制与低频感知的矛盾，为自主航空器在复杂环境中的稳健灵活导航提供了可行解决方案，具有良好的实际部署效果。

Abstract: Robust autonomous navigation for Autonomous Aerial Vehicles (AAVs) in complex
environments is a critical capability. However, modern end-to-end navigation
faces a key challenge: the high-frequency control loop needed for agile flight
conflicts with low-frequency perception streams, which are limited by sensor
update rates and significant computational cost. This mismatch forces
conventional synchronous models into undesirably low control rates. To resolve
this, we propose an asynchronous reinforcement learning framework that
decouples perception and control, enabling a high-frequency policy to act on
the latest IMU state for immediate reactivity, while incorporating perception
features asynchronously. To manage the resulting data staleness, we introduce a
theoretically-grounded Temporal Encoding Module (TEM) that explicitly
conditions the policy on perception delays, a strategy complemented by a
two-stage curriculum to ensure stable and efficient training. Validated in
extensive simulations, our method was successfully deployed in zero-shot
sim-to-real transfer on an onboard NUC, where it sustains a 100~Hz control rate
and demonstrates robust, agile navigation in cluttered real-world environments.
Our source code will be released for community reference.

</details>


### [32] [How Fly Neural Perception Mechanisms Enhance Visuomotor Control of Micro Robots](https://arxiv.org/abs/2509.13827)
*Renyuan Liu,Haoting Zhou,Chuankai Fang,Qinbing Fu*

Main category: cs.RO

TL;DR: 本文提出了一种受苍蝇视觉神经元LPLC2启发的注意力驱动视觉运动控制策略，首次在物理移动机器人上实现了LPLC2神经模型，实现了碰撞感知和反应性规避。


<details>
  <summary>Details</summary>
Motivation: 自主机器人在复杂环境中需要类似苍蝇的敏捷性，但受到计算成本和性能之间权衡的限制。昆虫启发的智能为低功耗、计算高效的框架提供了简洁的途径。

Method: 简化并优化了LPLC2神经模型（仅70KB内存），适用于视觉微型机器人Colias的计算约束，同时保留了关键的神经感知机制。还加入了多注意力机制来模拟LPLC2响应的分布式特性。

Result: 与最先进的蝗虫启发的碰撞检测模型相比，苍蝇启发的视觉运动模型实现了相当的鲁棒性，碰撞检测成功率达到96.1%，同时产生更自适应和优雅的规避动作。

Conclusion: 这项工作不仅展示了一种有效的碰撞规避策略，还突显了苍蝇启发的神经模型在推进昆虫智能集体行为研究方面的潜力。

Abstract: Anyone who has tried to swat a fly has likely been frustrated by its
remarkable agility.This ability stems from its visual neural perception system,
particularly the collision-selective neurons within its small brain.For
autonomous robots operating in complex and unfamiliar environments, achieving
similar agility is highly desirable but often constrained by the trade-off
between computational cost and performance.In this context, insect-inspired
intelligence offers a parsimonious route to low-power, computationally
efficient frameworks.In this paper, we propose an attention-driven visuomotor
control strategy inspired by a specific class of fly visual projection
neurons-the lobula plate/lobula column type-2 (LPLC2)-and their associated
escape behaviors.To our knowledge, this represents the first embodiment of an
LPLC2 neural model in the embedded vision of a physical mobile robot, enabling
collision perception and reactive evasion.The model was simplified and
optimized at 70KB in memory to suit the computational constraints of a
vision-based micro robot, the Colias, while preserving key neural perception
mechanisms.We further incorporated multi-attention mechanisms to emulate the
distributed nature of LPLC2 responses, allowing the robot to detect and react
to approaching targets both rapidly and selectively.We systematically evaluated
the proposed method against a state-of-the-art locust-inspired collision
detection model.Results showed that the fly-inspired visuomotor model achieved
comparable robustness, at success rate of 96.1% in collision detection while
producing more adaptive and elegant evasive maneuvers.Beyond demonstrating an
effective collision-avoidance strategy, this work highlights the potential of
fly-inspired neural models for advancing research into collective behaviors in
insect intelligence.

</details>


### [33] [UltraHiT: A Hierarchical Transformer Architecture for Generalizable Internal Carotid Artery Robotic Ultrasonography](https://arxiv.org/abs/2509.13832)
*Teng Wang,Haojun Jiang,Yuxuan Wang,Zhenguo Sun,Xiangjie Yan,Xiang Li,Gao Huang*

Main category: cs.RO

TL;DR: 提出基于分层Transformer的UltraHiT架构，用于自动化颈动脉超声扫描中具有挑战性的颈内动脉(ICA)定位，通过高层变异评估和低层动作决策的整合，在未见个体上达到95%的成功率。


<details>
  <summary>Details</summary>
Motivation: 颈内动脉(ICA)由于位置深、走行曲折且个体变异大，传统方法难以自动化扫描。研究将个体血管结构概念化为标准血管模型的形态变异，旨在解决ICA扫描的复杂性问题。

Method: 采用分层Transformer决策架构：高层模块识别血管变异并切换两个低层模块（适应校正器处理变异情况，标准执行器处理正常情况）。高层模块和适应校正器均实现为因果Transformer，基于历史扫描序列生成预测。

Result: 在包含28名受试者、164条轨迹和72K样本的大规模ICA扫描数据集上测试，方法在未见个体上定位ICA的成功率达到95%，优于基线方法。

Conclusion: UltraHiT架构有效解决了颈内动脉自动化超声扫描的挑战，通过分层决策和Transformer技术实现了高精度的血管定位，展现了良好的泛化能力。

Abstract: Carotid ultrasound is crucial for the assessment of cerebrovascular health,
particularly the internal carotid artery (ICA). While previous research has
explored automating carotid ultrasound, none has tackled the challenging ICA.
This is primarily due to its deep location, tortuous course, and significant
individual variations, which greatly increase scanning complexity. To address
this, we propose a Hierarchical Transformer-based decision architecture, namely
UltraHiT, that integrates high-level variation assessment with low-level action
decision. Our motivation stems from conceptualizing individual vascular
structures as morphological variations derived from a standard vascular model.
The high-level module identifies variation and switches between two low-level
modules: an adaptive corrector for variations, or a standard executor for
normal cases. Specifically, both the high-level module and the adaptive
corrector are implemented as causal transformers that generate predictions
based on the historical scanning sequence. To ensure generalizability, we
collected the first large-scale ICA scanning dataset comprising 164
trajectories and 72K samples from 28 subjects of both genders. Based on the
above innovations, our approach achieves a 95% success rate in locating the ICA
on unseen individuals, outperforming baselines and demonstrating its
effectiveness. Our code will be released after acceptance.

</details>


### [34] [Track Any Motions under Any Disturbances](https://arxiv.org/abs/2509.13833)
*Zhikai Zhang,Jun Guo,Chao Chen,Jilong Wang,Chenghuai Lin,Yunrui Lian,Han Xue,Zhenrong Wang,Maoqi Liu,Huaping Liu,He Wang,Li Yi*

Main category: cs.RO

TL;DR: Any2Track是一个两阶段强化学习框架，用于在真实世界中跟踪各种运动并抵抗多种动态干扰，包括地形变化、外力和物理属性变化。


<details>
  <summary>Details</summary>
Motivation: 开发能够在真实世界场景中稳定运行、抵抗各种动态干扰的基础人形运动跟踪器，实现通用实用价值。

Method: 采用两阶段RL框架：AnyTracker（通用运动跟踪器）和AnyAdapter（历史信息适应模块），前者跟踪各种运动，后者提供在线动态适应能力以克服sim2real差距和真实世界干扰。

Result: 在Unitree G1硬件上成功实现零样本sim2real迁移，在各种真实世界干扰下表现出色的运动跟踪性能。

Conclusion: Any2Track框架有效解决了人形机器人在真实环境中跟踪多样化、高动态、接触丰富运动时的动态适应问题，为实际应用提供了可靠解决方案。

Abstract: A foundational humanoid motion tracker is expected to be able to track
diverse, highly dynamic, and contact-rich motions. More importantly, it needs
to operate stably in real-world scenarios against various dynamics
disturbances, including terrains, external forces, and physical property
changes for general practical use. To achieve this goal, we propose Any2Track
(Track Any motions under Any disturbances), a two-stage RL framework to track
various motions under multiple disturbances in the real world. Any2Track
reformulates dynamics adaptability as an additional capability on top of basic
action execution and consists of two key components: AnyTracker and AnyAdapter.
AnyTracker is a general motion tracker with a series of careful designs to
track various motions within a single policy. AnyAdapter is a history-informed
adaptation module that endows the tracker with online dynamics adaptability to
overcome the sim2real gap and multiple real-world disturbances. We deploy
Any2Track on Unitree G1 hardware and achieve a successful sim2real transfer in
a zero-shot manner. Any2Track performs exceptionally well in tracking various
motions under multiple real-world disturbances.

</details>


### [35] [Pre-Manipulation Alignment Prediction with Parallel Deep State-Space and Transformer Models](https://arxiv.org/abs/2509.13839)
*Motonari Kambara,Komei Sugiura*

Main category: cs.RO

TL;DR: 提出了一种预测开放式词汇物体操作任务未来成功率的模型，通过多级轨迹融合模块分析末端执行器轨迹的自相关性，在实验中获得优于现有方法的表现


<details>
  <summary>Details</summary>
Motivation: 传统方法只能在操作完成后判断成功与否，难以预防潜在危险且依赖失败触发重规划，降低了物体操作序列的效率

Method: 提出预测预操作图像与规划轨迹及自然语言指令对齐度的模型，采用多级轨迹融合模块（结合深度状态空间模型和Transformer编码器）捕捉末端执行器轨迹的多层次时间序列自相关性

Result: 实验结果表明该方法优于现有方法，包括基础模型

Conclusion: 所提出的方法能够有效预测开放式词汇物体操作任务的未来成功率，提高了操作效率和安全性

Abstract: In this work, we address the problem of predicting the future success of
open-vocabulary object manipulation tasks. Conventional approaches typically
determine success or failure after the action has been carried out. However,
they make it difficult to prevent potential hazards and rely on failures to
trigger replanning, thereby reducing the efficiency of object manipulation
sequences. To overcome these challenges, we propose a model, which predicts the
alignment between a pre-manipulation egocentric image with the planned
trajectory and a given natural language instruction. We introduce a Multi-Level
Trajectory Fusion module, which employs a state-of-the-art deep state-space
model and a transformer encoder in parallel to capture multi-level time-series
self-correlation within the end effector trajectory. Our experimental results
indicate that the proposed method outperformed existing methods, including
foundation models.

</details>


### [36] [InterKey: Cross-modal Intersection Keypoints for Global Localization on OpenStreetMap](https://arxiv.org/abs/2509.13857)
*Nguyen Hoang Khoi Tran,Julie Stephany Berrio,Mao Shan,Stewart Worrall*

Main category: cs.RO

TL;DR: InterKey是一个利用道路交叉口作为地标的跨模态全局定位框架，通过联合编码点云和OSM数据构建二进制描述符，在GNSS失效环境下实现高精度车辆定位


<details>
  <summary>Details</summary>
Motivation: 解决GNSS信号退化环境（如城市峡谷和隧道）中的可靠全局定位问题，同时克服高精地图成本高和OpenStreetMap数据粗糙的挑战

Method: 提出跨模态框架，利用道路交叉口作为显著地标，通过联合编码点云和OSM中的道路和建筑印记构建紧凑二进制描述符，采用差异缓解、方向确定和区域均衡采样策略来弥合模态差距

Result: 在KITTI数据集上实现了最先进的精度，大幅超越现有基线方法

Conclusion: 该框架可推广到能产生密集结构点云的传感器，为稳健车辆定位提供了可扩展且经济高效的解决方案

Abstract: Reliable global localization is critical for autonomous vehicles, especially
in environments where GNSS is degraded or unavailable, such as urban canyons
and tunnels. Although high-definition (HD) maps provide accurate priors, the
cost of data collection, map construction, and maintenance limits scalability.
OpenStreetMap (OSM) offers a free and globally available alternative, but its
coarse abstraction poses challenges for matching with sensor data. We propose
InterKey, a cross-modal framework that leverages road intersections as
distinctive landmarks for global localization. Our method constructs compact
binary descriptors by jointly encoding road and building imprints from point
clouds and OSM. To bridge modality gaps, we introduce discrepancy mitigation,
orientation determination, and area-equalized sampling strategies, enabling
robust cross-modal matching. Experiments on the KITTI dataset demonstrate that
InterKey achieves state-of-the-art accuracy, outperforming recent baselines by
a large margin. The framework generalizes to sensors that can produce dense
structural point clouds, offering a scalable and cost-effective solution for
robust vehicle localization.

</details>


### [37] [Using Petri Nets for Context-Adaptive Robot Explanations](https://arxiv.org/abs/2509.13861)
*Görkem Kılınç Soylu,Neziha Akalin,Maria Riveiro*

Main category: cs.RO

TL;DR: 使用匹特鲁网模型建模上下文信息，支持机器人根据用户注意力和存在状态等上下文线索进行适应性解释


<details>
  <summary>Details</summary>
Motivation: 在人机交互中，机器人需要以自然透明的方式沟通，以促进信任，这要求根据上下文适应其沟通方式

Method: 提出使用匹特鲁网(Petri nets)模型上下文信息，匹特鲁网提供了形式化的图形表示方法，可以模型并发动作、因果依赖关系和系统状态

Result: 通过包含用户注意力和存在状态等上下文线索的场景进行实验，模型分析确认了关键属性：无死锁、上下文敏感可达性、有界性和活性

Conclusion: 匹特鲁网在设计和验证人机交互中上下文适应性解释方面具有稳健性和灵活性

Abstract: In human-robot interaction, robots must communicate in a natural and
transparent manner to foster trust, which requires adapting their communication
to the context. In this paper, we propose using Petri nets (PNs) to model
contextual information for adaptive robot explanations. PNs provide a formal,
graphical method for representing concurrent actions, causal dependencies, and
system states, making them suitable for analyzing dynamic interactions between
humans and robots. We demonstrate this approach through a scenario involving a
robot that provides explanations based on contextual cues such as user
attention and presence. Model analysis confirms key properties, including
deadlock-freeness, context-sensitive reachability, boundedness, and liveness,
showing the robustness and flexibility of PNs for designing and verifying
context-adaptive explanations in human-robot interactions.

</details>


### [38] [Repulsive Trajectory Modification and Conflict Resolution for Efficient Multi-Manipulator Motion Planning](https://arxiv.org/abs/2509.13882)
*Junhwa Hong,Beomjoon Lee,Woojin Lee,Changjoo Nam*

Main category: cs.RO

TL;DR: 一种基于冲突搜索算法的高效多操纵器运动规划方法，通过人工力场梯度下降修改轨迹来减少冲突，提高规划效率和成功率


<details>
  <summary>Details</summary>
Motivation: 多操纵器系统在高维配置空间中协调运动的计算挑战，传统冲突搜索算法存在约束树指数增长问题

Method: 在CBS两层结构中集成排斥力场梯度下降方法，通过产生排斥力引导操纵器轨迹避免冲突，并在特定条件下直接尝试单步求解

Result: 经过广泛测试包括物理机器人实验，证明方法能够减少约束树扩展节点数量，提高成功率，更快找到解决方案

Conclusion: 该方法通过排斥力场梯度下降优化，有效觤解了CBS算法的约束树暴长问题，为多操纵器运动规划提供了更高效的解决方案

Abstract: We propose an efficient motion planning method designed to efficiently find
collision-free trajectories for multiple manipulators. While multi-manipulator
systems offer significant advantages, coordinating their motions is
computationally challenging owing to the high dimensionality of their composite
configuration space. Conflict-Based Search (CBS) addresses this by decoupling
motion planning, but suffers from subsequent conflicts incurred by resolving
existing conflicts, leading to an exponentially growing constraint tree of CBS.
Our proposed method is based on repulsive trajectory modification within the
two-level structure of CBS. Unlike conventional CBS variants, the low-level
planner applies a gradient descent approach using an Artificial Potential
Field. This field generates repulsive forces that guide the trajectory of the
conflicting manipulator away from those of other robots. As a result,
subsequent conflicts are less likely to occur. Additionally, we develop a
strategy that, under a specific condition, directly attempts to find a
conflict-free solution in a single step without growing the constraint tree.
Through extensive tests including physical robot experiments, we demonstrate
that our method consistently reduces the number of expanded nodes in the
constraint tree, achieves a higher success rate, and finds a solution faster
compared to Enhanced CBS and other state-of-the-art algorithms.

</details>


### [39] [PhysicalAgent: Towards General Cognitive Robotics with Foundation World Models](https://arxiv.org/abs/2509.13903)
*Artem Lykov,Jeffrin Sam,Hung Khang Nguyen,Vladislav Kozlovskiy,Yara Mahmoud,Valerii Serpiva,Miguel Altamirano Cabrera,Mikhail Konenkov,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: PhysicalAgent是一个机器人操作框架，通过迭代推理、扩散视频生成和闭环执行实现鲁棒操作，在多种平台和感知模态下达到80%的成功率


<details>
  <summary>Details</summary>
Motivation: 为了解决机器人操作中的执行错误恢复问题，开发一个能够通过迭代规划和视频生成来适应不同环境和任务的通用框架

Method: 结合文本指令生成候选轨迹的短视频演示，在机器人上执行并基于失败情况进行迭代重规划，使用扩散模型进行视频生成和闭环执行

Result: 在多种机器人平台（UR3、G1人形、GR1仿真）和感知模态下，首次尝试成功率20-30%，但通过迭代修正后整体成功率提升至80%，优于现有方法

Conclusion: 基于视频的生成推理和迭代执行对于通用机器人操作具有重要潜力，为可扩展、自适应和鲁棒的机器人控制铺平了道路

Abstract: We introduce PhysicalAgent, an agentic framework for robotic manipulation
that integrates iterative reasoning, diffusion-based video generation, and
closed-loop execution. Given a textual instruction, our method generates short
video demonstrations of candidate trajectories, executes them on the robot, and
iteratively re-plans in response to failures. This approach enables robust
recovery from execution errors. We evaluate PhysicalAgent across multiple
perceptual modalities (egocentric, third-person, and simulated) and robotic
embodiments (bimanual UR3, Unitree G1 humanoid, simulated GR1), comparing
against state-of-the-art task-specific baselines. Experiments demonstrate that
our method consistently outperforms prior approaches, achieving up to 83%
success on human-familiar tasks. Physical trials reveal that first-attempt
success is limited (20-30%), yet iterative correction increases overall success
to 80% across platforms. These results highlight the potential of video-based
generative reasoning for general-purpose robotic manipulation and underscore
the importance of iterative execution for recovering from initial failures. Our
framework paves the way for scalable, adaptable, and robust robot control.

</details>


### [40] [MAP: End-to-End Autonomous Driving with Map-Assisted Planning](https://arxiv.org/abs/2509.13926)
*Huilin Yin,Yiming Kan,Daniel Watzenig*

Main category: cs.RO

TL;DR: MAP是一个新颖的端到端轨迹规划框架，通过显式整合语义地图特征和当前ego状态，显著提升了自动驾驶轨迹规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法未能充分利用在线地图模块的潜力来增强轨迹规划能力。

Method: 提出MAP框架，包含三个核心模块：规划增强在线地图模块、ego状态引导规划模块，以及基于当前ego状态的权重适配器。

Result: 在DAIR-V2X-seq-SPD数据集上，相比UniV2X基线，L2位移误差降低16.6%，脱轨率降低56.2%，整体评分提升44.5%。在CVPR2025挑战赛中排名第一，比第二名模型整体评分高39.5%。

Conclusion: 显式利用语义地图特征在规划中非常有效，为改进端到端自动驾驶系统结构设计提供了新方向。

Abstract: In recent years, end-to-end autonomous driving has attracted increasing
attention for its ability to jointly model perception, prediction, and planning
within a unified framework. However, most existing approaches underutilize the
online mapping module, leaving its potential to enhance trajectory planning
largely untapped. This paper proposes MAP (Map-Assisted Planning), a novel
map-assisted end-to-end trajectory planning framework. MAP explicitly
integrates segmentation-based map features and the current ego status through a
Plan-enhancing Online Mapping module, an Ego-status-guided Planning module, and
a Weight Adapter based on current ego status. Experiments conducted on the
DAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6%
reduction in L2 displacement error, a 56.2% reduction in off-road rate, and a
44.5% improvement in overall score compared to the UniV2X baseline, even
without post-processing. Furthermore, it achieves top ranking in Track 2 of the
End-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS
Workshop @CVPR2025, outperforming the second-best model by 39.5% in terms of
overall score. These results highlight the effectiveness of explicitly
leveraging semantic map features in planning and suggest new directions for
improving structure design in end-to-end autonomous driving systems. Our code
is available at https://gitee.com/kymkym/map.git

</details>


### [41] [Reinforcement Learning for Autonomous Point-to-Point UAV Navigation](https://arxiv.org/abs/2509.13943)
*Salim Oyinlola,Nitesh Subedi,Soumik Sarkar*

Main category: cs.RO

TL;DR: 开发基于强化学习的无人机自主导航系统，通过试错学习实现点对点导航，无需人工干预


<details>
  <summary>Details</summary>
Motivation: 无人机在自动化检查、配送和导航任务中需要可靠的自主性，现有方法需要手动干预或不够灵活

Method: 使用强化学习方法，设计自定义奖励函数鼓励高效到达目标并惩罚碰撞和不安全行为，集成ROS和Gym兼容训练环境

Result: 训练后的策略在实际无人机平台上成功部署，能够在实际条件下实现自主导航，只需最少的人工监督

Conclusion: 基于强化学习的控制方法在现实世界点对点无人机操作中具有可行性，证明了RL在无人机自主导航中的应用价值

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used in automated
inspection, delivery, and navigation tasks that require reliable autonomy. This
project develops a reinforcement learning (RL) approach to enable a single UAV
to autonomously navigate between predefined points without manual intervention.
The drone learns navigation policies through trial-and-error interaction, using
a custom reward function that encourages goal-reaching efficiency while
penalizing collisions and unsafe behavior. The control system integrates ROS
with a Gym-compatible training environment, enabling flexible deployment and
testing. After training, the learned policy is deployed on a real UAV platform
and evaluated under practical conditions. Results show that the UAV can
successfully perform autonomous navigation with minimal human oversight,
demonstrating the viability of RL-based control for point-to-point drone
operations in real-world scenarios.

</details>


### [42] [The Influence of Facial Features on the Perceived Trustworthiness of a Social Robot](https://arxiv.org/abs/2509.13948)
*Benedict Barrow,Roger K. Moore*

Main category: cs.RO

TL;DR: 研究发现社交机器人的眼睛形状和大小对人类信任感知有显著影响，骄子脸特征能增进信任


<details>
  <summary>Details</summary>
Motivation: 信任在人机交互中至关重要，但对于社交机器人面部特征如何影响信任感知的理解仍不充分

Method: 通过操控Furhat机器人的后投影面部，测试不同眼睛形状和大小对信任感知的影响

Result: 眼睛形状和大小对信任质感知有显著影响，骄子脸特征能够增进人类对机器人的信任

Conclusion: 这些发现为社交机器人设计提供了重要指导，有助于优化人机交互效果

Abstract: Trust and the perception of trustworthiness play an important role in
decision-making and our behaviour towards others, and this is true not only of
human-human interactions but also of human-robot interactions. While
significant advances have been made in recent years in the field of social
robotics, there is still some way to go before we fully understand the factors
that influence human trust in robots. This paper presents the results of a
study into the first impressions created by a social robot's facial features,
based on the hypothesis that a `babyface' engenders trust. By manipulating the
back-projected face of a Furhat robot, the study confirms that eye shape and
size have a significant impact on the perception of trustworthiness. The work
thus contributes to an understanding of the design choices that need to be made
when developing social robots so as to optimise the effectiveness of
human-robot interaction.

</details>


### [43] [SHaRe-RL: Structured, Interactive Reinforcement Learning for Contact-Rich Industrial Assembly Tasks](https://arxiv.org/abs/2509.13949)
*Jannick Stranghöner,Philipp Hartmann,Marco Braun,Sebastian Wrede,Klaus Neumann*

Main category: cs.RO

TL;DR: SHaRe-RL是一个强化学习框架，通过整合多种先验知识（技能结构化、人类演示、力约束）来解决HMLV工业装配中的安全高效学习问题，在0.2-0.4mm间隙的工业连接器插入任务中实现了可靠性能。


<details>
  <summary>Details</summary>
Motivation: 解决中小企业在高混合低产量(HMLV)工业装配中面临的挑战：需要高精度、安全性和可靠性，同时保持对产品变化和环境不确定性的灵活性。现有机器人系统难以满足这些需求，手动编程脆弱且成本高，而基于学习的方法在接触密集型任务中样本效率低且探索不安全。

Method: SHaRe-RL强化学习框架整合三种先验知识：(i)将技能结构化为操作原语，(ii)融入人类演示和在线修正，(iii)通过轴向柔顺性约束交互力，实现接触密集型工业装配任务的安全高效在线学习。

Result: 在工业Harting连接器模块插入实验（0.2-0.4mm间隙）中，SHaRe-RL在实用时间预算内实现了可靠性能，证明过程专业知识（无需机器人或RL知识）能够有效促进学习。

Conclusion: SHaRe-RL实现了更安全、更鲁棒、更经济可行的强化学习在工业装配中的部署，使领域专业知识能够有意义地贡献于学习过程。

Abstract: High-mix low-volume (HMLV) industrial assembly, common in small and
medium-sized enterprises (SMEs), requires the same precision, safety, and
reliability as high-volume automation while remaining flexible to product
variation and environmental uncertainty. Current robotic systems struggle to
meet these demands. Manual programming is brittle and costly to adapt, while
learning-based methods suffer from poor sample efficiency and unsafe
exploration in contact-rich tasks. To address this, we present SHaRe-RL, a
reinforcement learning framework that leverages multiple sources of prior
knowledge. By (i) structuring skills into manipulation primitives, (ii)
incorporating human demonstrations and online corrections, and (iii) bounding
interaction forces with per-axis compliance, SHaRe-RL enables efficient and
safe online learning for long-horizon, contact-rich industrial assembly tasks.
Experiments on the insertion of industrial Harting connector modules with
0.2-0.4 mm clearance demonstrate that SHaRe-RL achieves reliable performance
within practical time budgets. Our results show that process expertise, without
requiring robotics or RL knowledge, can meaningfully contribute to learning,
enabling safer, more robust, and more economically viable deployment of RL for
industrial assembly.

</details>


### [44] [SEG-Parking: Towards Safe, Efficient, and Generalizable Autonomous Parking via End-to-End Offline Reinforcement Learning](https://arxiv.org/abs/2509.13956)
*Zewei Yang,Zengqi Peng,Jun Ma*

Main category: cs.RO

TL;DR: SEG-Parking是一个基于离线强化学习的端到端自动驾驶停车框架，通过构建专门的停车数据集和保守正则化方法，在复杂交互场景中实现高成功率和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决非结构化环境和动态交互对自动驾驶停车任务带来的挑战，特别是在与对向车辆交互的复杂场景中实现安全高效的自主停车。

Method: 构建专门的停车数据集（包含无干扰和复杂交互场景），预训练目标条件状态编码器将感知信息映射到潜在空间，使用保守正则化优化离线强化学习策略以惩罚分布外动作。

Result: 在CARLA模拟器中进行的闭环实验显示，该框架具有最高的成功率和强大的泛化能力，能够有效处理分布外停车场景。

Conclusion: SEG-Parking框架通过端到端离线强化学习方法，成功解决了自动驾驶停车中的交互感知问题，在复杂场景中表现出优异的性能和泛化能力。

Abstract: Autonomous parking is a critical component for achieving safe and efficient
urban autonomous driving. However, unstructured environments and dynamic
interactions pose significant challenges to autonomous parking tasks. To
address this problem, we propose SEG-Parking, a novel end-to-end offline
reinforcement learning (RL) framework to achieve interaction-aware autonomous
parking. Notably, a specialized parking dataset is constructed for parking
scenarios, which include those without interference from the opposite vehicle
(OV) and complex ones involving interactions with the OV. Based on this
dataset, a goal-conditioned state encoder is pretrained to map the fused
perception information into the latent space. Then, an offline RL policy is
optimized with a conservative regularizer that penalizes out-of-distribution
actions. Extensive closed-loop experiments are conducted in the high-fidelity
CARLA simulator. Comparative results demonstrate the superior performance of
our framework with the highest success rate and robust generalization to
out-of-distribution parking scenarios. The related dataset and source code will
be made publicly available after the paper is accepted.

</details>


### [45] [MetricNet: Recovering Metric Scale in Generative Navigation Policies](https://arxiv.org/abs/2509.13965)
*Abhijeet Nayak,Débora N. P. Oliveira,Samiran Gode,Cordelia Schmid,Wolfram Burgard*

Main category: cs.RO

TL;DR: 提出MetricNet方法解决生成式导航策略的两个结构性问题：缺乏度量基础和短视控制策略，通过预测路径点间的度量距离来改进导航性能


<details>
  <summary>Details</summary>
Motivation: 生成式导航策略存在两个结构性问题：1）采样轨迹存在于抽象无标度的空间中，缺乏度量基础；2）控制策略丢弃完整路径，只朝单个路径点移动，导致短视和不安全的动作

Method: 提出MetricNet作为生成式导航的附加模块，预测路径点间的度量距离，将策略输出锚定到真实世界坐标中。进一步提出MetricNav，将MetricNet集成到导航策略中，引导机器人避开障碍物同时朝向目标移动

Result: 在仿真环境中使用新的基准框架评估，执行MetricNet缩放的路径点显著提高了导航和探索性能。在真实世界实验中进一步验证了方法的有效性

Conclusion: MetricNet能够有效解决生成式导航的度量基础和短视问题，MetricNav集成方法能够在避开障碍物的同时保持向目标移动，显著提升导航性能

Abstract: Generative navigation policies have made rapid progress in improving
end-to-end learned navigation. Despite their promising results, this paradigm
has two structural problems. First, the sampled trajectories exist in an
abstract, unscaled space without metric grounding. Second, the control strategy
discards the full path, instead moving directly towards a single waypoint. This
leads to short-sighted and unsafe actions, moving the robot towards obstacles
that a complete and correctly scaled path would circumvent. To address these
issues, we propose MetricNet, an effective add-on for generative navigation
that predicts the metric distance between waypoints, grounding policy outputs
in real-world coordinates. We evaluate our method in simulation with a new
benchmarking framework and show that executing MetricNet-scaled waypoints
significantly improves both navigation and exploration performance. Beyond
simulation, we further validate our approach in real-world experiments.
Finally, we propose MetricNav, which integrates MetricNet into a navigation
policy to guide the robot away from obstacles while still moving towards the
goal.

</details>


### [46] [BIM Informed Visual SLAM for Construction Monitoring](https://arxiv.org/abs/2509.13972)
*Asier Bikandi,Miguel Fernandez-Cortizas,Muhammad Shaheer,Ali Tourani,Holger Voos,Jose Luis Sanchez-Lopez*

Main category: cs.RO

TL;DR: 基于BIM结构前知的RGB-D SLAM系统，通过将检测到的墙体与BIM模型对应关系作为约束优化，在建筑环境中提升定位和地图精度


<details>
  <summary>Details</summary>
Motivation: 解决建筑环境中视觉SLAM遇到的挑战：重复布局、遮挡、低纹理结构导致轨迹偏移，而LiDAR SLAM又存在体积大、耗电高的问题

Method: 提出一种融合BIM结构前知的RGB-D SLAM系统，连续建立检测到的墙体与BIM模型的对应关系，并将这些对应关系作为约束加入到后端优化中

Result: 在实际建筑工地验证，轨迹误差平均减少23.71%，地图RMSE减少7.14%，较传统视觉SLAM基线显著提升

Conclusion: BIM约束能够在部分建造条件下供可靠的数字计划与实际场景对齐，为建筑环境监控提供更准确的SLAM解决方案

Abstract: Simultaneous Localization and Mapping (SLAM) is a key tool for monitoring
construction sites, where aligning the evolving as-built state with the
as-planned design enables early error detection and reduces costly rework.
LiDAR-based SLAM achieves high geometric precision, but its sensors are
typically large and power-demanding, limiting their use on portable platforms.
Visual SLAM offers a practical alternative with lightweight cameras already
embedded in most mobile devices. however, visually mapping construction
environments remains challenging: repetitive layouts, occlusions, and
incomplete or low-texture structures often cause drift in the trajectory map.
To mitigate this, we propose an RGB-D SLAM system that incorporates the
Building Information Model (BIM) as structural prior knowledge. Instead of
relying solely on visual cues, our system continuously establishes
correspondences between detected wall and their BIM counterparts, which are
then introduced as constraints in the back-end optimization. The proposed
method operates in real time and has been validated on real construction sites,
reducing trajectory error by an average of 23.71% and map RMSE by 7.14%
compared to visual SLAM baselines. These results demonstrate that BIM
constraints enable reliable alignment of the digital plan with the as-built
scene, even under partially constructed conditions.

</details>


### [47] [Flexible and Foldable: Workspace Analysis and Object Manipulation Using a Soft, Interconnected, Origami-Inspired Actuator Array](https://arxiv.org/abs/2509.13998)
*Bailey Dacre,Rodrigo Moreno,Serhat Demirtas,Ziqiao Wang,Yuhao Jiang,Jamie Paik,Kasper Stoy,Andrés Faíña*

Main category: cs.RO

TL;DR: 一种新型分布式操纳系统，采用算纳含级激光器数组和复合表面层，在低激光器密度下实现更大的操纳面积


<details>
  <summary>Details</summary>
Motivation: 解决传统分布式操纳系统密度高、对象比例限制多、适应性差的问题

Method: 使用3-DoF算纳含级激光器线性数组，通过复合表面层连接，形成连续可控操纳表面

Result: 操纳面积增加了1.84倍，无需增加激光器数量，能够实现简单几何形状物体的位移操纱

Conclusion: 该设计提供了低成本、低复杂性的替代方案，为利用连接表面灵活性的新型操纱策略开启了新机会

Abstract: Object manipulation is a fundamental challenge in robotics, where systems
must balance trade-offs among manipulation capabilities, system complexity, and
throughput. Distributed manipulator systems (DMS) use the coordinated motion of
actuator arrays to perform complex object manipulation tasks, seeing widespread
exploration within the literature and in industry. However, existing DMS
designs typically rely on high actuator densities and impose constraints on
object-to-actuator scale ratios, limiting their adaptability. We present a
novel DMS design utilizing an array of 3-DoF, origami-inspired robotic tiles
interconnected by a compliant surface layer. Unlike conventional DMS, our
approach enables manipulation not only at the actuator end effectors but also
across a flexible surface connecting all actuators; creating a continuous,
controllable manipulation surface. We analyse the combined workspace of such a
system, derive simple motion primitives, and demonstrate its capabilities to
translate simple geometric objects across an array of tiles. By leveraging the
inter-tile connective material, our approach significantly reduces actuator
density, increasing the area over which an object can be manipulated by x1.84
without an increase in the number of actuators. This design offers a lower cost
and complexity alternative to traditional high-density arrays, and introduces
new opportunities for manipulation strategies that leverage the flexibility of
the interconnected surface.

</details>


### [48] [Whole-body Motion Control of an Omnidirectional Wheel-Legged Mobile Manipulator via Contact-Aware Dynamic Optimization](https://arxiv.org/abs/2509.14010)
*Zong Chen,Shaoyang Li,Ben Liu,Min Li,Zhouping Yin,Yiqun Li*

Main category: cs.RO

TL;DR: 提出了一种轮腿式四足机器人及其全身运动控制框架，解决了轮腿机器人统一控制的挑战，包括冗余自由度、复杂轮地接触动力学和运动操作协调问题。


<details>
  <summary>Details</summary>
Motivation: 轮腿式机器人结合机械臂在物流、工业自动化和人机协作中具有巨大潜力，但由于自由度冗余、轮地接触动力学复杂以及运动操作协调需求，统一控制仍然具有挑战性。

Method: 开发了接触感知的全身动态优化框架，集成了操作的点接触建模和轮地交互的线接触建模；引入预热启动策略加速在线优化；建立了针对4WIS-4WID驱动方案的统一运动学模型。

Result: 仿真和实验结果验证了框架的有效性，展示了敏捷地形穿越、高速全向移动和精确操作能力，在多样化场景下表现优异。

Conclusion: 该系统在工厂自动化、城市物流和半结构化环境中的服务机器人领域具有巨大应用潜力，提出的控制框架为轮腿式机器人的统一控制提供了有效解决方案。

Abstract: Wheel-legged robots with integrated manipulators hold great promise for
mobile manipulation in logistics, industrial automation, and human-robot
collaboration. However, unified control of such systems remains challenging due
to the redundancy in degrees of freedom, complex wheel-ground contact dynamics,
and the need for seamless coordination between locomotion and manipulation. In
this work, we present the design and whole-body motion control of an
omnidirectional wheel-legged quadrupedal robot equipped with a dexterous
manipulator. The proposed platform incorporates independently actuated steering
modules and hub-driven wheels, enabling agile omnidirectional locomotion with
high maneuverability in structured environments. To address the challenges of
contact-rich interaction, we develop a contact-aware whole-body dynamic
optimization framework that integrates point-contact modeling for manipulation
with line-contact modeling for wheel-ground interactions. A warm-start strategy
is introduced to accelerate online optimization, ensuring real-time feasibility
for high-dimensional control. Furthermore, a unified kinematic model tailored
for the robot's 4WIS-4WID actuation scheme eliminates the need for mode
switching across different locomotion strategies, improving control consistency
and robustness. Simulation and experimental results validate the effectiveness
of the proposed framework, demonstrating agile terrain traversal, high-speed
omnidirectional mobility, and precise manipulation under diverse scenarios,
underscoring the system's potential for factory automation, urban logistics,
and service robotics in semi-structured environments.

</details>


### [49] [TransforMARS: Fault-Tolerant Self-Reconfiguration for Arbitrarily Shaped Modular Aerial Robot Systems](https://arxiv.org/abs/2509.14025)
*Rui Huang,Zhiyu Gao,Siyu Tang,Jialin Zhang,Lei He,Ziqian Zhang,Lin Zhao*

Main category: cs.RO

TL;DR: TransforMARS是一个通用的故障容忍重构框架，能够处理任意形状的模块化空中机器人系统在多个转子和单元故障情况下的重构，同时确保空中稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的模块化空中机器人系统重构方法仅关注矩形形状系统，且只能容忍单个转子或单元故障。需要更通用的框架来处理任意形状配置和多重故障情况。

Method: 开发算法首先识别并构建包含故障单元的最小可控组件，然后规划可行的拆卸-装配序列来运输系统单元或子组件以形成目标配置。

Result: 在具有挑战性的任意形状配置中验证了TransforMARS，在处理多样化配置和容忍故障数量方面相比先前工作有显著改进。

Conclusion: 提出的方法实现了更灵活和实用的可行重构，能够处理多重转子和单元故障，同时保持连续空中稳定性。

Abstract: Modular Aerial Robot Systems (MARS) consist of multiple drone modules that
are physically bound together to form a single structure for flight. Exploiting
structural redundancy, MARS can be reconfigured into different formations to
mitigate unit or rotor failures and maintain stable flight. Prior work on MARS
self-reconfiguration has solely focused on maximizing controllability margins
to tolerate a single rotor or unit fault for rectangular-shaped MARS. We
propose TransforMARS, a general fault-tolerant reconfiguration framework that
transforms arbitrarily shaped MARS under multiple rotor and unit faults while
ensuring continuous in-air stability. Specifically, we develop algorithms to
first identify and construct minimum controllable assemblies containing faulty
units. We then plan feasible disassembly-assembly sequences to transport MARS
units or subassemblies to form target configuration. Our approach enables more
flexible and practical feasible reconfiguration. We validate TransforMARS in
challenging arbitrarily shaped MARS configurations, demonstrating substantial
improvements over prior works in both the capacity of handling diverse
configurations and the number of faults tolerated. The videos and source code
of this work are available at the anonymous repository:
https://anonymous.4open.science/r/TransforMARS-1030/

</details>


### [50] [Prompt2Auto: From Motion Prompt to Automated Control via Geometry-Invariant One-Shot Gaussian Process Learning](https://arxiv.org/abs/2509.14040)
*Zewen Yang,Xiaobing Dai,Dongfa Zhang,Yu Li,Ziyang Meng,Bingkun Huang,Hamid Sadeghian,Sami Haddadin*

Main category: cs.RO

TL;DR: Prompt2Auto是一个几何不变的单次高斯过程学习框架，通过单个运动提示实现机器人的人类引导自动控制，具有平移、旋转和缩放不变性，显著减少演示负担。


<details>
  <summary>Details</summary>
Motivation: 传统示教学习方法需要大量数据集且难以跨坐标变换泛化，需要开发能够从单个演示中学习并具有几何不变性的方法。

Method: 提出基于坐标变换的数据集构建策略，使用几何不变高斯过程(GeoGP)实现平移、旋转和缩放不变性，支持多步预测和多技能自主。

Result: 通过数值模拟和两个真实机器人实验验证，证明该方法有效、跨任务泛化能力强，显著降低了演示需求。

Conclusion: Prompt2Auto框架成功实现了从单个运动提示中学习几何不变控制策略，为机器人学习提供了高效且通用的解决方案。

Abstract: Learning from demonstration allows robots to acquire complex skills from
human demonstrations, but conventional approaches often require large datasets
and fail to generalize across coordinate transformations. In this paper, we
propose Prompt2Auto, a geometry-invariant one-shot Gaussian process (GeoGP)
learning framework that enables robots to perform human-guided automated
control from a single motion prompt. A dataset-construction strategy based on
coordinate transformations is introduced that enforces invariance to
translation, rotation, and scaling, while supporting multi-step predictions.
Moreover, GeoGP is robust to variations in the user's motion prompt and
supports multi-skill autonomy. We validate the proposed approach through
numerical simulations with the designed user graphical interface and two
real-world robotic experiments, which demonstrate that the proposed method is
effective, generalizes across tasks, and significantly reduces the
demonstration burden. Project page is available at:
https://prompt2auto.github.io

</details>


### [51] [Language Conditioning Improves Accuracy of Aircraft Goal Prediction in Untowered Airspace](https://arxiv.org/abs/2509.14063)
*Sundhar Vinodh Sangeetha,Chih-Yuan Chiu,Sarah H. Q. Li,Shreyas Kousik*

Main category: cs.RO

TL;DR: 这篇论文提出了一种多模态框架，通过结合自然语言理解和空间推理来预测无塔台空域中飞机的目标位置，以支持自主航空决策。


<details>
  <summary>Details</summary>
Motivation: 在无塔台空域中，自主飞机需要预测其他飞机的意图和目标位置来确保安全飞行，而传统上依靠人类飞行员通过语音通信进行协调。

Method: 结合自动语音识别和大语言模型进行广播调用转写和解释，识别飞机并提取意图标签。这些标签与观测到的轨迹融合，用条件化时间卷积网络和高斯混合模型进行概率性目标预测。

Result: 与仅依赖运动历史的基准方法相比，本方法显著减少了目标预测误差，证明语言条件化预测能够提高预测准确性。

Conclusion: 通过在真实无塔台机场数据集上的实验验证，该方法显示了其在实现社交意识、语言条件化的机器人运动规划方面的潜力。

Abstract: Autonomous aircraft must safely operate in untowered airspace, where
coordination relies on voice-based communication among human pilots. Safe
operation requires an aircraft to predict the intent, and corresponding goal
location, of other aircraft. This paper introduces a multimodal framework for
aircraft goal prediction that integrates natural language understanding with
spatial reasoning to improve autonomous decision-making in such environments.
We leverage automatic speech recognition and large language models to
transcribe and interpret pilot radio calls, identify aircraft, and extract
discrete intent labels. These intent labels are fused with observed
trajectories to condition a temporal convolutional network and Gaussian mixture
model for probabilistic goal prediction. Our method significantly reduces goal
prediction error compared to baselines that rely solely on motion history,
demonstrating that language-conditioned prediction increases prediction
accuracy. Experiments on a real-world dataset from an untowered airport
validate the approach and highlight its potential to enable socially aware,
language-conditioned robotic motion planning.

</details>


### [52] [Constraint-Consistent Control of Task-Based and Kinematic RCM Constraints for Surgical Robots](https://arxiv.org/abs/2509.14075)
*Yu Li,Hamid Sadeghian,Zewen Yang,Valentin Le Mesle,Sami Haddadin*

Main category: cs.RO

TL;DR: 提出一种约束一致的扭矩控制器，通过投影逆动力学框架将远程运动中心约束作为流形约束嵌入，在保证工具尖端精确跟踪的同时实现平滑扭矩控制，提高了手术机器人的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助微创手术需要精确执行远程运动中心约束以确保手术工具通过套管的安全操作，现有控制方法在扭矩级别缺乏鲁棒性或无法保证一致的约束满足。

Method: 将远程运动中心约束视为流形约束，嵌入到基于投影的逆动力学框架中，统一任务级和运动学表述，实现精确的工具尖端跟踪和平滑扭矩行为。

Result: 在仿真和RAMIS训练平台上验证，相比最先进方法，显示改进的RCM约束满足、降低的所需扭矩，通过一致性表述在临床相关场景下提高关节扭矩平滑度和鲁棒性能。

Conclusion: 约束一致的扭矩控制方法有潜力增强手术机器人的安全性和可靠性，为动态交互条件下的精确手术操作提供了有效解决方案。

Abstract: Robotic-assisted minimally invasive surgery (RAMIS) requires precise
enforcement of the remote center of motion (RCM) constraint to ensure safe tool
manipulation through a trocar. Achieving this constraint under dynamic and
interactive conditions remains challenging, as existing control methods either
lack robustness at the torque level or do not guarantee consistent RCM
constraint satisfaction. This paper proposes a constraint-consistent torque
controller that treats the RCM as a rheonomic holonomic constraint and embeds
it into a projection-based inverse-dynamics framework. The method unifies
task-level and kinematic formulations, enabling accurate tool-tip tracking
while maintaining smooth and efficient torque behavior. The controller is
validated both in simulation and on a RAMIS training platform, and is
benchmarked against state-of-the-art approaches. Results show improved RCM
constraint satisfaction, reduced required torque, and robust performance by
improving joint torque smoothness through the consistency formulation under
clinically relevant scenarios, including spiral trajectories, variable
insertion depths, moving trocars, and human interaction. These findings
demonstrate the potential of constraint-consistent torque control to enhance
safety and reliability in surgical robotics. The project page is available at:
https://rcmpc-cube.github.io

</details>


### [53] [FlightDiffusion: Revolutionising Autonomous Drone Training with Diffusion Models Generating FPV Video](https://arxiv.org/abs/2509.14082)
*Valerii Serpiva,Artem Lykov,Faryal Batool,Vladislav Kozlovskiy,Miguel Altamirano Cabrera,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: FlightDiffusion是一个基于扩散模型的框架，用于从第一人称视角视频训练自主无人机，能够生成逼真的视频序列和对应的动作空间，支持推理驱动的导航和大规模训练数据集合成。


<details>
  <summary>Details</summary>
Motivation: 解决无人机自主导航训练中真实数据收集成本高的问题，通过生成模型合成多样化的FPV轨迹和状态-动作对，降低对真实世界数据收集的依赖。

Method: 采用扩散模型框架，从单帧图像生成逼真的视频序列，并同时生成对应的动作空间，支持推理驱动的动态环境导航和大规模训练数据集合成。

Result: 生成的轨迹物理上合理且可执行，平均位置误差0.25m，平均方向误差0.19rad。仿真和现实性能无显著差异（p=0.541），成功率分别为0.628和0.617，表现出优秀的sim-to-real迁移能力。

Conclusion: 扩散基推理为统一导航、动作生成和数据合成提供了有前景的范式，生成的数据集为未来无人机研究提供了宝贵资源，在仿真环境中展现出增强的鲁棒性、平滑轨迹规划和适应未知条件的能力。

Abstract: We present FlightDiffusion, a diffusion-model-based framework for training
autonomous drones from first-person view (FPV) video. Our model generates
realistic video sequences from a single frame, enriched with corresponding
action spaces to enable reasoning-driven navigation in dynamic environments.
Beyond direct policy learning, FlightDiffusion leverages its generative
capabilities to synthesize diverse FPV trajectories and state-action pairs,
facilitating the creation of large-scale training datasets without the high
cost of real-world data collection. Our evaluation demonstrates that the
generated trajectories are physically plausible and executable, with a mean
position error of 0.25 m (RMSE 0.28 m) and a mean orientation error of 0.19 rad
(RMSE 0.24 rad). This approach enables improved policy learning and dataset
scalability, leading to superior performance in downstream navigation tasks.
Results in simulated environments highlight enhanced robustness, smoother
trajectory planning, and adaptability to unseen conditions. An ANOVA revealed
no statistically significant difference between performance in simulation and
reality (F(1, 16) = 0.394, p = 0.541), with success rates of M = 0.628 (SD =
0.162) and M = 0.617 (SD = 0.177), respectively, indicating strong sim-to-real
transfer. The generated datasets provide a valuable resource for future UAV
research. This work introduces diffusion-based reasoning as a promising
paradigm for unifying navigation, action generation, and data synthesis in
aerial robotics.

</details>


### [54] [GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model](https://arxiv.org/abs/2509.14117)
*Ali Abouzeid,Malak Mansour,Zezhou Sun,Dezhen Song*

Main category: cs.RO

TL;DR: GeoAware-VLA通过集成几何先验知识增强VLA模型的视角不变性，在未见过的相机视角下实现2倍以上的成功率提升，并在真实机器人上验证了有效性


<details>
  <summary>Details</summary>
Motivation: 现有的Vision-Language-Action模型难以从2D图像推断鲁棒的3D几何结构，导致对新相机视角的泛化能力不足

Method: 利用冻结的预训练几何视觉模型作为特征提取器，通过可训练的投影层将几何丰富的特征适配到策略解码器，避免从头学习3D一致性

Result: 在LIBERO基准测试中，零样本泛化到新相机姿态的成功率提升超过2倍，在真实机器人上从未见过的相机角度评估也显示出显著性能增益

Conclusion: 鲁棒的几何基础是创建更具泛化能力的机器人智能体的关键组件，该方法在连续和离散动作空间中都有效

Abstract: Vision-Language-Action (VLA) models often fail to generalize to novel camera
viewpoints, a limitation stemming from their difficulty in inferring robust 3D
geometry from 2D images. We introduce GeoAware-VLA, a simple yet effective
approach that enhances viewpoint invariance by integrating strong geometric
priors into the vision backbone. Instead of training a visual encoder or
relying on explicit 3D data, we leverage a frozen, pretrained geometric vision
model as a feature extractor. A trainable projection layer then adapts these
geometrically-rich features for the policy decoder, relieving it of the burden
of learning 3D consistency from scratch. Through extensive evaluations on
LIBERO benchmark subsets, we show GeoAware-VLA achieves substantial
improvements in zero-shot generalization to novel camera poses, boosting
success rates by over 2x in simulation. Crucially, these benefits translate to
the physical world; our model shows a significant performance gain on a real
robot, especially when evaluated from unseen camera angles. Our approach proves
effective across both continuous and discrete action spaces, highlighting that
robust geometric grounding is a key component for creating more generalizable
robotic agents.

</details>


### [55] [CrazyMARL: Decentralized Direct Motor Control Policies for Cooperative Aerial Transport of Cable-Suspended Payloads](https://arxiv.org/abs/2509.14126)
*Viktor Lorentz,Khaled Wahba,Sayantan Auddy,Marc Toussaint,Wolfgang Hönig*

Main category: cs.RO

TL;DR: CrazyMARL是一个去中心化强化学习框架，用于多无人机协同运输缆绳悬挂载荷，解决了缆绳松弛-紧绷模式转换的挑战，在干扰抑制和跟踪精度方面优于传统方法，并实现了零样本仿真到现实的迁移。


<details>
  <summary>Details</summary>
Motivation: 多无人机协同运输缆绳悬挂载荷具有提升载荷能力、适应不同载荷形状和内置合规性的潜力，但在干扰、非线性载荷动力学和缆绳松弛-紧绷模式转换下的协调控制仍是一个挑战。

Method: 提出了CrazyMARL，一个去中心化强化学习(RL)框架，专门处理多无人机缆绳悬挂载荷运输问题，特别是缆绳模式转换的挑战。

Result: 仿真结果显示，学习到的策略在干扰抑制和跟踪精度方面优于经典去中心化控制器，从恶劣条件下的恢复率达到80%（基线方法为44%）。成功实现了零样本仿真到现实迁移，策略在风力、随机外部干扰和缆绳动力学模式转换等恶劣条件下表现出高度鲁棒性。

Conclusion: 这项工作为能够在非结构化环境中执行复杂载荷任务的自主、弹性无人机团队铺平了道路。

Abstract: Collaborative transportation of cable-suspended payloads by teams of Unmanned
Aerial Vehicles (UAVs) has the potential to enhance payload capacity, adapt to
different payload shapes, and provide built-in compliance, making it attractive
for applications ranging from disaster relief to precision logistics. However,
multi-UAV coordination under disturbances, nonlinear payload dynamics, and
slack--taut cable modes remains a challenging control problem. To our
knowledge, no prior work has addressed these cable mode transitions in the
multi-UAV context, instead relying on simplifying rigid-link assumptions. We
propose CrazyMARL, a decentralized Reinforcement Learning (RL) framework for
multi-UAV cable-suspended payload transport. Simulation results demonstrate
that the learned policies can outperform classical decentralized controllers in
terms of disturbance rejection and tracking precision, achieving an 80%
recovery rate from harsh conditions compared to 44% for the baseline method. We
also achieve successful zero-shot sim-to-real transfer and demonstrate that our
policies are highly robust under harsh conditions, including wind, random
external disturbances, and transitions between slack and taut cable dynamics.
This work paves the way for autonomous, resilient UAV teams capable of
executing complex payload missions in unstructured environments.

</details>


### [56] [Energy Efficient Multi Robot Package Delivery under Capacity-Constraints via Voronoi-Constrained Networks](https://arxiv.org/abs/2509.14127)
*Alkesh K. Srivastava,Jared Michael Levin,Philip Dames*

Main category: cs.RO

TL;DR: VCST-RCP框架通过Voronoi约束的Steiner树优化构建稀疏中继主干，将中继从附带产物转变为协调核心，相比传统直接运输方法提升达34%的效率


<details>
  <summary>Details</summary>
Motivation: 解决多机器人有限运载能力下的包裹配送问题，传统直接运输方法效率有限，需要将中继作为协调核心元素来提升配送效率

Method: 提出Voronoi约束的Steiner树中继协调规划框架，构建稀疏中继主干网络，并合成机器人级别的取件、中继和配送调度计划

Result: 实验显示相比传统基线方法有高达34%的持续改进，显著提升了容量约束下多机器人配送的能源效率

Conclusion: 该框架为中继协调提供了可扩展的解决方案，证明了将中继纳入配送过程的好处，适用于现实世界的物流应用

Abstract: We consider the problem of delivering multiple packages from a single pickup
depot to distinct goal locations using a homogeneous fleet of robots with
limited carrying capacity. We propose VCST-RCP, a Voronoi-Constrained Steiner
Tree Relay Coordination Planning framework that constructs sparse relay trunks
using Steiner tree optimization and then synthesizes robot-level pickup, relay,
and delivery schedules. This framework reframes relays from incidental
byproducts into central elements of coordination, offering a contrast with
traditional delivery methods that rely on direct source-to-destination
transport. Extensive experiments show consistent improvements of up to 34%
compared to conventional baselines, underscoring the benefits of incorporating
relays into the delivery process. These improvements translate directly to
enhanced energy efficiency in multi-robot delivery under capacity constraints,
providing a scalable framework for real-world logistics.

</details>


### [57] [SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model](https://arxiv.org/abs/2509.14138)
*Ran Yang,Zijian An,Lifeng ZHou,Yiming Feng*

Main category: cs.RO

TL;DR: SeqVLA是一个基于π₀的视觉-语言-动作模型扩展，通过添加轻量级检测头来感知子任务完成状态，从而在长时程多阶段机器人操作任务中实现自主子任务转换，显著提高了序列操作的鲁棒性和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型如π₀擅长连续低级控制，但缺乏检测子任务完成的内在信号，导致在需要严格序列执行的多阶段任务中容易因错误累积而失败。

Method: 在π₀基础架构上增加轻量级检测头，形成双头设计（动作生成+完成检测），研究了四种微调策略（联合vs顺序微调，全微调vs冻结主干网络），在沙拉打包（7个子任务）和糖果打包（4个子任务）任务上进行实验。

Result: SeqVLA显著优于基线π₀和其他强基线，特别是联合微调且不冻结主干网络的策略能够产生最准确可靠的完成预测，消除了序列相关故障，实现了鲁棒的长时程执行。

Conclusion: 将动作生成与子任务感知检测相结合对于可扩展的顺序操作至关重要，双头设计能够有效解决长时程多阶段机器人操作中的序列执行问题。

Abstract: Long-horizon robotic manipulation tasks require executing multiple
interdependent subtasks in strict sequence, where errors in detecting subtask
completion can cascade into downstream failures. Existing
Vision-Language-Action (VLA) models such as $\pi_0$ excel at continuous
low-level control but lack an internal signal for identifying when a subtask
has finished, making them brittle in sequential settings. We propose SeqVLA, a
completion-aware extension of $\pi_0$ that augments the base architecture with
a lightweight detection head perceiving whether the current subtask is
complete. This dual-head design enables SeqVLA not only to generate
manipulation actions but also to autonomously trigger transitions between
subtasks. We investigate four finetuning strategies that vary in how the action
and detection heads are optimized (joint vs. sequential finetuning) and how
pretrained knowledge is preserved (full finetuning vs. frozen backbone).
Experiments are performed on two multi-stage tasks: salad packing with seven
distinct subtasks and candy packing with four distinct subtasks. Results show
that SeqVLA significantly outperforms the baseline $\pi_0$ and other strong
baselines in overall success rate. In particular, joint finetuning with an
unfrozen backbone yields the most decisive and statistically reliable
completion predictions, eliminating sequence-related failures and enabling
robust long-horizon execution. Our results highlight the importance of coupling
action generation with subtask-aware detection for scalable sequential
manipulation.

</details>


### [58] [CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping](https://arxiv.org/abs/2509.14143)
*Zijian An,Ran Yang,Yiming Feng,Lifeng Zhou*

Main category: cs.RO

TL;DR: CLAW框架通过解耦条件评估和动作生成，使用微调CLIP模型监控数字读数并生成基于重量阈值的离散指令，结合VLA策略实现精确的重量感知机器人控制


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型在满足精确任务约束（如基于数值阈值的停止）方面存在困难，因为其观测到动作的映射是隐式训练的，缺乏明确的条件监控机制

Method: 提出CLAW框架：1）使用微调CLIP模型作为轻量级提示生成器，持续监控秤的数字读数并基于任务特定重量阈值产生离散指令；2）由π₀流式VLA策略整合提示和多视角相机观测，生成连续机器人动作

Result: 在单物体抓取和需要双臂操作的混合物体任务三个实验设置中，CLAW可靠执行重量感知行为，性能优于原始π₀和微调π₀模型

Conclusion: CLAW成功将符号化重量推理与高频视觉运动控制相结合，为满足精确任务约束的机器人控制提供了有效解决方案

Abstract: Vision-language-action (VLA) models have recently emerged as a promising
paradigm for robotic control, enabling end-to-end policies that ground natural
language instructions into visuomotor actions. However, current VLAs often
struggle to satisfy precise task constraints, such as stopping based on numeric
thresholds, since their observation-to-action mappings are implicitly shaped by
training data and lack explicit mechanisms for condition monitoring. In this
work, we propose CLAW (CLIP-Language-Action for Weight), a framework that
decouples condition evaluation from action generation. CLAW leverages a
fine-tuned CLIP model as a lightweight prompt generator, which continuously
monitors the digital readout of a scale and produces discrete directives based
on task-specific weight thresholds. These prompts are then consumed by $\pi_0$,
a flow-based VLA policy, which integrates the prompts with multi-view camera
observations to produce continuous robot actions. This design enables CLAW to
combine symbolic weight reasoning with high-frequency visuomotor control. We
validate CLAW on three experimental setups: single-object grasping and
mixed-object tasks requiring dual-arm manipulation. Across all conditions, CLAW
reliably executes weight-aware behaviors and outperforms both raw-$\pi_0$ and
fine-tuned $\pi_0$ models. We have uploaded the videos as supplementary
materials.

</details>


### [59] [StableTracker: Learning to Stably Track Target via Differentiable Simulation](https://arxiv.org/abs/2509.14147)
*Fanxing Li,Shengyang Wang,Fangyu Sun,Shuyu Wu,Dexin Zuo,Wenxian Yu,Danping Zou*

Main category: cs.RO

TL;DR: StableTracker是一个基于学习的控制策略，通过可微分模拟训练，使四旋翼无人机能够从任意视角稳定跟踪移动目标，作为自主空中相机使用。


<details>
  <summary>Details</summary>
Motivation: 传统FPV目标跟踪方法依赖手工模块化设计，存在硬件过载和累积误差问题，特别是在目标快速加减速时性能严重下降。

Method: 使用通过可微分模拟的反向传播时间训练学习型控制策略，使无人机在水平和垂直方向保持目标在视野中心，同时保持固定相对距离。

Result: 仿真实验显示该策略在精度、稳定性和泛化性方面优于传统算法和学习基线，真实世界实验验证了其实用性。

Conclusion: StableTracker提供了一种有效的学习型解决方案，能够实现鲁棒的目标跟踪，克服了传统方法的局限性。

Abstract: FPV object tracking methods heavily rely on handcraft modular designs,
resulting in hardware overload and cumulative error, which seriously degrades
the tracking performance, especially for rapidly accelerating or decelerating
targets. To address these challenges, we present \textbf{StableTracker}, a
learning-based control policy that enables quadrotors to robustly follow the
moving target from arbitrary perspectives. The policy is trained using
backpropagation-through-time via differentiable simulation, allowing the
quadrotor to maintain the target at the center of the visual field in both
horizontal and vertical directions, while keeping a fixed relative distance,
thereby functioning as an autonomous aerial camera. We compare StableTracker
against both state-of-the-art traditional algorithms and learning baselines.
Simulation experiments demonstrate that our policy achieves superior accuracy,
stability and generalization across varying safe distances, trajectories, and
target velocities. Furthermore, a real-world experiment on a quadrotor with an
onboard computer validated practicality of the proposed approach.

</details>


### [60] [MIMIC-D: Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies](https://arxiv.org/abs/2509.14159)
*Dayi Dong,Maulik Bhatt,Seoyeon Choi,Negar Mehr*

Main category: cs.RO

TL;DR: MIMIC-D是一种基于扩散策略的多模态多智能体模仿学习方法，采用集中训练分散执行(CTDE)范式，能够在没有显式通信的情况下实现智能体间的隐式协调。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在社会中更广泛的应用，需要学习多模态任务中的协调行为。传统模仿学习方法难以处理多模态专家演示，而现有扩散模型方法通常需要集中规划或显式通信，这在现实场景中不可行。

Method: 提出MIMIC-D方法，使用扩散策略进行多智能体模仿学习。采用CTDE范式：集中训练时使用完整信息，分散执行时仅使用局部信息，实现隐式协调。

Result: 在仿真和硬件实验中证明，该方法能够恢复智能体间的多模态协调行为，在多种任务和环境中优于现有最先进的基线方法。

Conclusion: MIMIC-D成功解决了多模态多智能体模仿学习中的协调问题，无需显式通信即可实现有效的隐式协调，为现实世界机器人协调提供了实用解决方案。

Abstract: As robots become more integrated in society, their ability to coordinate with
other robots and humans on multi-modal tasks (those with multiple valid
solutions) is crucial. We propose to learn such behaviors from expert
demonstrations via imitation learning (IL). However, when expert demonstrations
are multi-modal, standard IL approaches can struggle to capture the diverse
strategies, hindering effective coordination. Diffusion models are known to be
effective at handling complex multi-modal trajectory distributions in
single-agent systems. Diffusion models have also excelled in multi-agent
scenarios where multi-modality is more common and crucial to learning
coordinated behaviors. Typically, diffusion-based approaches require a
centralized planner or explicit communication among agents, but this assumption
can fail in real-world scenarios where robots must operate independently or
with agents like humans that they cannot directly communicate with. Therefore,
we propose MIMIC-D, a Centralized Training, Decentralized Execution (CTDE)
paradigm for multi-modal multi-agent imitation learning using diffusion
policies. Agents are trained jointly with full information, but execute
policies using only local information to achieve implicit coordination. We
demonstrate in both simulation and hardware experiments that our method
recovers multi-modal coordination behavior among agents in a variety of tasks
and environments, while improving upon state-of-the-art baselines.

</details>


### [61] [\textsc{Gen2Real}: Towards Demo-Free Dexterous Manipulation by Harnessing Generated Video](https://arxiv.org/abs/2509.14178)
*Kai Ye,Yuhang Wu,Shuyuan Hu,Junliang Li,Meng Liu,Yongquan Chen,Rui Huang*

Main category: cs.RO

TL;DR: Gen2Real使用生成视频替代昂贵的人类演示，通过视频生成、轨迹优化和学习策略实现机器人灵巧操作，在仿真中达到77.3%的成功率，并能迁移到真实机器人。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作是机器人领域的挑战性难题，主要因为收集大量人类演示数据成本高昂。本文旨在通过生成视频来替代人类演示，降低数据收集成本。

Method: 结合三个组件：1)利用视频生成、姿态和深度估计生成手-物体轨迹的演示生成；2)使用物理感知交互优化模型(PIOM)确保物理一致性的轨迹优化；3)基于锚点的残差PPO策略，将人类动作重定向到机器人手并稳定控制。

Result: 仅使用生成视频，学习策略在仿真抓取任务中达到77.3%的成功率，并在真实机器人上展示连贯执行。消融研究验证了各组件贡献，并展示了使用自然语言直接指定任务的能力。

Conclusion: Gen2Real展示了从想象视频到真实世界执行的泛化能力，具有灵活性和鲁棒性，为降低机器人学习成本提供了有效解决方案。

Abstract: Dexterous manipulation remains a challenging robotics problem, largely due to
the difficulty of collecting extensive human demonstrations for learning. In
this paper, we introduce \textsc{Gen2Real}, which replaces costly human demos
with one generated video and drives robot skill from it: it combines
demonstration generation that leverages video generation with pose and depth
estimation to yield hand-object trajectories, trajectory optimization that uses
Physics-aware Interaction Optimization Model (PIOM) to impose physics
consistency, and demonstration learning that retargets human motions to a robot
hand and stabilizes control with an anchor-based residual Proximal Policy
Optimization (PPO) policy. Using only generated videos, the learned policy
achieves a 77.3\% success rate on grasping tasks in simulation and demonstrates
coherent executions on a real robot. We also conduct ablation studies to
validate the contribution of each component and demonstrate the ability to
directly specify tasks using natural language, highlighting the flexibility and
robustness of \textsc{Gen2Real} in generalizing grasping skills from imagined
videos to real-world execution.

</details>


### [62] [MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping](https://arxiv.org/abs/2509.14191)
*Zhihao Cao,Hanyu Wu,Li Wa Tang,Zizhou Luo,Zihan Zhu,Wei Zhang,Marc Pollefeys,Martin R. Oswald*

Main category: cs.RO

TL;DR: MCGS-SLAM是首个基于纯RGB输入的多相机3D高斯溅射SLAM系统，通过多视角融合实现实时高精度建图和轨迹估计


<details>
  <summary>Details</summary>
Motivation: 现有的密集SLAM方法主要针对单目设置，往往牺牲了鲁棒性和几何覆盖范围。多相机系统可以提供更宽的视野和更完整的场景重建，这对于自动驾驶等安全关键应用至关重要

Method: 使用多相机束调整(MCBA)通过密集光度学和几何残差联合优化位姿和深度，采用尺度一致性模块通过低秩先验强制多视图间的度量对齐，构建统一的连续优化高斯地图

Result: 在合成和真实数据集上的实验表明，MCGS-SLAM能够产生准确的轨迹和照片级真实感重建，通常优于单目基线方法，能够重建单目系统遗漏的侧视区域

Conclusion: 多相机高斯溅射SLAM在机器人和自动驾驶的高保真建图中具有巨大潜力，能够提供更完整和安全的场景重建

Abstract: Recent progress in dense SLAM has primarily targeted monocular setups, often
at the expense of robustness and geometric coverage. We present MCGS-SLAM, the
first purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting
(3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM
fuses dense RGB inputs from multiple viewpoints into a unified, continuously
optimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines
poses and depths via dense photometric and geometric residuals, while a scale
consistency module enforces metric alignment across views using low-rank
priors. The system supports RGB input and maintains real-time performance at
large scale. Experiments on synthetic and real-world datasets show that
MCGS-SLAM consistently yields accurate trajectories and photorealistic
reconstructions, usually outperforming monocular baselines. Notably, the wide
field of view from multi-camera input enables reconstruction of side-view
regions that monocular setups miss, critical for safe autonomous operation.
These results highlight the promise of multi-camera Gaussian Splatting SLAM for
high-fidelity mapping in robotics and autonomous driving.

</details>


### [63] [GLIDE: A Coordinated Aerial-Ground Framework for Search and Rescue in Unknown Environments](https://arxiv.org/abs/2509.14210)
*Seth Farrell,Chenghao Li,Hongzhan Yu,Hesam Mojtahedi,Sicun Gao,Henrik I. Christensen*

Main category: cs.RO

TL;DR: GLIDE框架使用两个无人机和一个地面车辆协同工作，通过无人机进行目标搜索和地形侦察，地面车辆融合空中信息进行路径规划，提高搜救任务效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 在未知环境中实现快速受害者定位和障碍物感知导航，解决传统搜救任务中时间紧迫和环境不确定性带来的挑战。

Method: 使用两个无人机（目标搜索无人机和地形侦察无人机）与一个地面车辆协同工作，目标搜索无人机进行实时受害者检测和地理参考，地形侦察无人机提供地形可通行性信息，地面车辆融合空中信息进行A*路径规划和持续重规划。

Result: 硬件演示和仿真实验表明，明确的角色分工、地形侦察和引导规划相结合，显著提高了时间紧迫搜救任务中的到达时间和导航安全性。

Conclusion: GLIDE框架通过空中-地面协同和多无人机角色分工，有效提升了搜救任务的效率和安全性，为未知环境中的搜救行动提供了实用解决方案。

Abstract: We present a cooperative aerial-ground search-and-rescue (SAR) framework that
pairs two unmanned aerial vehicles (UAVs) with an unmanned ground vehicle (UGV)
to achieve rapid victim localization and obstacle-aware navigation in unknown
environments. We dub this framework Guided Long-horizon Integrated Drone Escort
(GLIDE), highlighting the UGV's reliance on UAV guidance for long-horizon
planning. In our framework, a goal-searching UAV executes real-time onboard
victim detection and georeferencing to nominate goals for the ground platform,
while a terrain-scouting UAV flies ahead of the UGV's planned route to provide
mid-level traversability updates. The UGV fuses aerial cues with local sensing
to perform time-efficient A* planning and continuous replanning as information
arrives. Additionally, we present a hardware demonstration (using a GEM e6 golf
cart as the UGV and two X500 UAVs) to evaluate end-to-end SAR mission
performance and include simulation ablations to assess the planning stack in
isolation from detection. Empirical results demonstrate that explicit role
separation across UAVs, coupled with terrain scouting and guided planning,
improves reach time and navigation safety in time-critical SAR missions.

</details>


### [64] [Multi-robot Multi-source Localization in Complex Flows with Physics-Preserving Environment Models](https://arxiv.org/abs/2509.14228)
*Benjamin Shaffer,Victoria Edwards,Brooks Kinch,Nathaniel Trask,M. Ani Hsieh*

Main category: cs.RO

TL;DR: 多机器人分布式源定位框架，通过机器学习有限元模型指导信息基采样策略，在复杂流动环境中实现更快速和准确的源头定位


<details>
  <summary>Details</summary>
Motivation: 复杂流动环境中的源头定位面临时变、混沌的流动动力学、间息性传感器读数和复杂地形等挑战，而传统数值模型计算费用高，难以在机器人本地运行

Method: 每个机器人携带机器学习的有限元环境模型，使用近似相互信息准则来驱动infotaxis控制策略，选择预期信息量最大的采样区域

Result: 方法实现了比基准采样策略更快的错误减少，以及比基准机器学习方法更准确的源头定位结果

Conclusion: 分布式机器学习有限元模型结合infotaxis控制策略，为复杂流动环境中的源头定位提供了高效、准确的解决方案

Abstract: Source localization in a complex flow poses a significant challenge for
multi-robot teams tasked with localizing the source of chemical leaks or
tracking the dispersion of an oil spill. The flow dynamics can be time-varying
and chaotic, resulting in sporadic and intermittent sensor readings, and
complex environmental geometries further complicate a team's ability to model
and predict the dispersion. To accurately account for the physical processes
that drive the dispersion dynamics, robots must have access to computationally
intensive numerical models, which can be difficult when onboard computation is
limited. We present a distributed mobile sensing framework for source
localization in which each robot carries a machine-learned, finite element
model of its environment to guide information-based sampling. The models are
used to evaluate an approximate mutual information criterion to drive an
infotaxis control strategy, which selects sensing regions that are expected to
maximize informativeness for the source localization objective. Our approach
achieves faster error reduction compared to baseline sensing strategies and
results in more accurate source localization compared to baseline machine
learning approaches.

</details>
