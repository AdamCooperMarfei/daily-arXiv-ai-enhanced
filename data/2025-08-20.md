<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Diff-MSM: Differentiable MusculoSkeletal Model for Simultaneous Identification of Human Muscle and Bone Parameters](https://arxiv.org/abs/2508.13303)
*Yingfan Zhou,Philip Sanderink,Sigurd Jager Lemming,Cheng Fang*

Main category: cs.RO

TL;DR: 提出可微分肌肉骨骼模型(Diff-MSM)，通过端到端自动微分技术同时识别肌肉和骨骼参数，无需测量内部关节扭矩，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高保真个性化人体肌肉骨骼模型对于人机交互系统仿真和安全关键应用验证至关重要，但传统方法难以直接测量内部生物力学变量特别是关节扭矩。

Method: 使用可微分肌肉骨骼模型(Diff-MSM)，通过自动微分技术从可测量的肌肉激活到可观察的运动进行端到端参数识别，无需测量内部关节扭矩。

Result: 仿真结果表明该方法显著优于现有基线方法，肌肉参数估计误差可低至0.05%，初始猜测从均值为真实值、标准差为10%的正态分布采样。

Conclusion: Diff-MSM参数识别技术在肌肉骨骼建模和仿真方面具有巨大潜力，可应用于肌肉健康监测、康复和运动科学等领域。

Abstract: High-fidelity personalized human musculoskeletal models are crucial for
simulating realistic behavior of physically coupled human-robot interactive
systems and verifying their safety-critical applications in simulations before
actual deployment, such as human-robot co-transportation and rehabilitation
through robotic exoskeletons. Identifying subject-specific Hill-type muscle
model parameters and bone dynamic parameters is essential for a personalized
musculoskeletal model, but very challenging due to the difficulty of measuring
the internal biomechanical variables in vivo directly, especially the joint
torques. In this paper, we propose using Differentiable MusculoSkeletal Model
(Diff-MSM) to simultaneously identify its muscle and bone parameters with an
end-to-end automatic differentiation technique differentiating from the
measurable muscle activation, through the joint torque, to the resulting
observable motion without the need to measure the internal joint torques.
Through extensive comparative simulations, the results manifested that our
proposed method significantly outperformed the state-of-the-art baseline
methods, especially in terms of accurate estimation of the muscle parameters
(i.e., initial guess sampled from a normal distribution with the mean being the
ground truth and the standard deviation being 10% of the ground truth could end
up with an average of the percentage errors of the estimated values as low as
0.05%). In addition to human musculoskeletal modeling and simulation, the new
parameter identification technique with the Diff-MSM has great potential to
enable new applications in muscle health monitoring, rehabilitation, and sports
science.

</details>


### [2] [A Surveillance Based Interactive Robot](https://arxiv.org/abs/2508.13319)
*Kshitij Kavimandan,Pooja Mangal,Devanshi Mehta*

Main category: cs.RO

TL;DR: 基于树莓派4的移动监控机器人系统，支持实时视频流、语音控制和多语言交互，通过YOLOv3进行物体检测和自主导航


<details>
  <summary>Details</summary>
Motivation: 构建一个使用普通硬件和开源软件的可复现移动监控机器人，支持远程监控和语音控制，提供自动化监视功能

Method: 使用2台Raspberry Pi 4：前端单元负责移动、摄像头和音频，中央单元负责视频流和视觉处理。采用FFmpeg传输视频，YOLOv3进行物体检测，Python语音库实现语音识别和翻译，Kinect RGB-D传感器提供深度信息

Result: 室内测试显示机器人能在CPU上以交互帧率检测常见物体，可靠识别命令并自动执行动作，无需手动控制

Conclusion: 设计依靠商用硬件和开源软件，易于复现，并提出了传感器融合、GPU加速、人脸和文本识别等未来扩展方向

Abstract: We build a mobile surveillance robot that streams video in real time and
responds to speech so a user can monitor and steer it from a phone or browser.
The system uses two Raspberry Pi 4 units: a front unit on a differential drive
base with camera, mic, and speaker, and a central unit that serves the live
feed and runs perception. Video is sent with FFmpeg. Objects in the scene are
detected using YOLOv3 to support navigation and event awareness. For voice
interaction, we use Python libraries for speech recognition, multilingual
translation, and text-to-speech, so the robot can take spoken commands and read
back responses in the requested language. A Kinect RGB-D sensor provides visual
input and obstacle cues. In indoor tests the robot detects common objects at
interactive frame rates on CPU, recognises commands reliably, and translates
them to actions without manual control. The design relies on off-the-shelf
hardware and open software, making it easy to reproduce. We discuss limits and
practical extensions, including sensor fusion with ultrasonic range data, GPU
acceleration, and adding face and text recognition.

</details>


### [3] [Incremental Generalized Hybrid A*](https://arxiv.org/abs/2508.13392)
*Sidharth Talia,Oren Salzman,Siddhartha Srinivasa*

Main category: cs.RO

TL;DR: 提出了Incremental Generalized Hybrid A* (IGHA*)算法，通过动态组织顶点扩展而非刚性剪枝，在运动规划中比传统Hybrid A*减少6倍扩展次数，实现实时性能


<details>
  <summary>Details</summary>
Motivation: 解决复杂动力学下运动规划的高效搜索问题，传统Hybrid A*的网格分辨率选择困难：太粗可能导致失败，太细则导致过度扩展和规划缓慢

Method: 开发了IGHA*算法，这是一个随时树搜索框架，动态组织顶点扩展而不使用刚性剪枝，可证明匹配或优于HA*

Result: 在汽车类机器人的道路和非道路规划查询中，IGHA*变体比优化版HA*减少6倍扩展到最佳解的扩展次数；在高保真模拟器中，IGHA*在模型预测控制循环中优于HA*M

Conclusion: IGHA*在仿真和小型越野车辆上展示了实时性能，能够在复杂动力学下实现快速、鲁棒的规划

Abstract: We address the problem of efficiently organizing search over very large
trees, which arises in many applications ranging from autonomous driving to
aerial vehicles. Here, we are motivated by off-road autonomy, where real-time
planning is essential. Classical approaches use graphs of motion primitives and
exploit dominance to mitigate the curse of dimensionality and prune expansions
efficiently. However, for complex dynamics, repeatedly solving two-point
boundary-value problems makes graph construction too slow for fast kinodynamic
planning. Hybrid A* (HA*) addressed this challenge by searching over a tree of
motion primitives and introducing approximate pruning using a grid-based
dominance check. However, choosing the grid resolution is difficult: too coarse
risks failure, while too fine leads to excessive expansions and slow planning.
We propose Incremental Generalized Hybrid A* (IGHA*), an anytime tree-search
framework that dynamically organizes vertex expansions without rigid pruning.
IGHA* provably matches or outperforms HA*. For both on-road kinematic and
off-road kinodynamic planning queries for a car-like robot, variants of IGHA*
use 6x fewer expansions to the best solution compared to an optimized version
of HA*. In simulated off-road experiments in a high fidelity simulator, IGHA*
outperforms HA*M when both are used in the loop with a model predictive
controller. We demonstrate real-time performance both in simulation and on a
small-scale off-road vehicle, enabling fast, robust planning under complex
dynamics. Code: https://github.com/personalrobotics/IGHAStar

</details>


### [4] [Accelerating Signal-Temporal-Logic-Based Task and Motion Planning of Bipedal Navigation using Benders Decomposition](https://arxiv.org/abs/2508.13407)
*Jiming Ren,Xuan Lin,Roman Mineyev,Karen M. Feigh,Samuel Coogan,Ye Zhao*

Main category: cs.RO

TL;DR: 基于Benders分解的方法，逐步解决双足行走中的任务与动作规划问题，避免整体最优化的计算复杂性


<details>
  <summary>Details</summary>
Motivation: 双足行走中的任务与动作规划问题具有高计算复杂性，特别是引入非凸约束后的混合整数规划问题更加难以解决

Method: 采用Benders分解技术，将问题分解为主问题（计划原型）和子问题（动力学和运动学可行性检查），逐步迭代添加切割平面

Result: 实验结果显示，该方法在处理非线性约束的优化问题时，规划速度更快于其他算法

Conclusion: Benders分解技术能够有效地处理双足行走中的复杂规划问题，提高解决效率而不影响规划质量

Abstract: Task and motion planning under Signal Temporal Logic constraints is known to
be NP-hard. A common class of approaches formulates these hybrid problems,
which involve discrete task scheduling and continuous motion planning, as
mixed-integer programs (MIP). However, in applications for bipedal locomotion,
introduction of non-convex constraints such as kinematic reachability and
footstep rotation exacerbates the computational complexity of MIPs. In this
work, we present a method based on Benders Decomposition to address scenarios
where solving the entire monolithic optimization problem is prohibitively
intractable. Benders Decomposition proposes an iterative cutting-plane
technique that partitions the problem into a master problem to prototype a plan
that meets the task specification, and a series of subproblems for kinematics
and dynamics feasibility checks. Our experiments demonstrate that this method
achieves faster planning compared to alternative algorithms for solving the
resulting optimization program with nonlinear constraints.

</details>


### [5] [Switch4EAI: Leveraging Console Game Platform for Benchmarking Robotic Athletics](https://arxiv.org/abs/2508.13444)
*Tianyu Li,Jeonghwan Kim,Wontaek Kim,Donghoon Baek,Seungeun Rho,Sehoon Ha*

Main category: cs.RO

TL;DR: Switch4EAI是一个利用体感游戏（如Just Dance）来评估全身机器人控制策略的低成本系统，通过捕捉、重建和重定向游戏中的舞蹈动作给机器人执行，为机器人性能提供量化基准。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏在真实环境中评估人形机器人运动性能并与人类进行直接比较的标准化基准测试方法。

Method: 使用Nintendo Switch的Just Dance游戏捕捉舞蹈动作，通过运动重建和重定向技术让Unitree G1人形机器人执行这些动作，并建立量化评估基准。

Result: 系统在Unitree G1人形机器人上成功验证，建立了机器人相对于人类玩家的性能量化基准，证明了使用商业游戏平台作为物理基准的可行性。

Conclusion: 商业游戏平台可以作为物理基础的基准测试工具，为具身AI的基准测试提供了新的方向和方法。

Abstract: Recent advances in whole-body robot control have enabled humanoid and legged
robots to execute increasingly agile and coordinated movements. However,
standardized benchmarks for evaluating robotic athletic performance in
real-world settings and in direct comparison to humans remain scarce. We
present Switch4EAI(Switch-for-Embodied-AI), a low-cost and easily deployable
pipeline that leverages motion-sensing console games to evaluate whole-body
robot control policies. Using Just Dance on the Nintendo Switch as a
representative example, our system captures, reconstructs, and retargets
in-game choreography for robotic execution. We validate the system on a Unitree
G1 humanoid with an open-source whole-body controller, establishing a
quantitative baseline for the robot's performance against a human player. In
the paper, we discuss these results, which demonstrate the feasibility of using
commercial games platform as physically grounded benchmarks and motivate future
work to for benchmarking embodied AI.

</details>


### [6] [CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models](https://arxiv.org/abs/2508.13446)
*Catherine Glossop,William Chen,Arjun Bhorkar,Dhruv Shah,Sergey Levine*

Main category: cs.RO

TL;DR: 通过视觉语言模型生成假想反事实标签，提升机器人数据集语言基础的细粒度和多样性，从而显著提高视觉-语言-动作模型的指令遵循能力


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型在遵循细粒度指令方面表现不佳，主要原因是现有机器人数据集缺乏语义多样性和语言基础，尤其是类似观测情境下的细粒度任务多样性不足

Method: 提出一种新的数据增帽方法，利用视觉语言模型为现有机器人数据集创建假想反事实标签，通过生成假想反事实的语言和动作来提高语言基础的多样性和细粒度

Result: 在3种不同室内外环境中进行视视语言导航实验，证明假想反事实重标注方法在不需额外数据收集的情况下显著提高了VLA策略的指令遵循能力，在导航任务上成功率提高27%，达到了领先方法的竞争力

Conclusion: 假想反事实标签是一种有效的数据增帽方法，能够显著提升视觉-语言-动作模型的语言指令遵循能力，为构建更好的通用机器人系统提供了有力的技术支撑

Abstract: Generalist robots should be able to understand and follow user instructions,
but current vision-language-action (VLA) models struggle with following
fine-grained commands despite providing a powerful architecture for mapping
open-vocabulary natural language instructions to robot actions. One cause for
this is a lack of semantic diversity and language grounding in existing robot
datasets and, specifically, a lack of fine-grained task diversity for similar
observations. To address this, we present a novel method to augment existing
robot datasets by leveraging vision language models to create counterfactual
labels. Our method improves the language-following capabilities of VLAs by
increasing the diversity and granularity of language grounding for robot
datasets by generating counterfactual language and actions. We evaluate the
resulting model's ability to follow language instructions, ranging from simple
object-centric commands to complex referential tasks, by conducting visual
language navigation experiments in 3 different indoor and outdoor environments.
Our experiments demonstrate that counterfactual relabeling, without any
additional data collection, significantly improves instruction-following in VLA
policies, making them competitive with state-of-the-art methods and increasing
success rate by 27% on navigation tasks.

</details>


### [7] [Modeling and Control of AWOISV: A Filtered Tube-Based MPC Approach for Simultaneous Tracking of Lateral Position and Heading Angle](https://arxiv.org/abs/2508.13457)
*Xu Yang,Jun Ni,Hengyang Feng,Feiyu Wang,Tiezhen Wang*

Main category: cs.RO

TL;DR: 基于新的转向半径角和侧滑角表示法，提出了一种筛波管基线性时变MPC策略，实现了全轮全向独立轮转车辆的高精度位置和方向控制


<details>
  <summary>Details</summary>
Motivation: 解决全轮全向独立轮转车辆在所有运动模式下的平滑过渡问题，并提高对模型不确定性的鲁棒性

Method: 开发了一种统一的v-β-r动力学模型，并设计了筛波管基线性时变MPC控制策略，通过轮转中心相对位置来定义运动模式和切换准则

Result: 串联模拟和硬件在环实验证明，FT-LTVMPC策略能够同时达到高精度位置和任意方向角跟踪，并保证优秀的实时性能

Conclusion: 该研究为AWOISV提供了一种有效的控制方案，能够在各种运动模式下实现平滑过渡和鲁棒性控制，为这类特殊车辆的实际应用奠定了基础

Abstract: An all-wheel omni-directional independent steering vehicle (AWOISV) is a
specialized all-wheel independent steering vehicle with each wheel capable of
steering up to 90{\deg}, enabling unique maneuvers like yaw and diagonal
movement. This paper introduces a theoretical steering radius angle and
sideslip angle (\( \theta_R \)-\(\beta_R \)) representation, based on the
position of the instantaneous center of rotation relative to the wheel rotation
center, defining the motion modes and switching criteria for AWOISVs. A
generalized \( v\)-\(\beta\)-\(r \) dynamic model is developed with forward
velocity \(v\), sideslip angle \(\beta\), and yaw rate \(r\) as states, and
\(\theta_R\) and \(\beta_R\) as control inputs. This model decouples
longitudinal and lateral motions into forward and rotational motions, allowing
seamless transitions across all motion modes under specific conditions. A
filtered tube-based linear time-varying MPC (FT-LTVMPC) strategy is proposed,
achieving simultaneous tracking of lateral position and arbitrary heading
angles, with robustness to model inaccuracies and parameter uncertainties.
Co-simulation and hardware-in-loop (HIL) experiments confirm that FT-LTVMPC
enables high-precision control of both position and heading while ensuring
excellent real-time performance.

</details>


### [8] [Multi-Robot Navigation in Social Mini-Games: Definitions, Taxonomy, and Algorithms](https://arxiv.org/abs/2508.13459)
*Rohan Chandra,Shubham Singh,Abhishek Jha,Dannon Andrade,Hriday Sainathuni,Katia Sycara*

Main category: cs.RO

TL;DR: 本文首次提出社会迷你游戏(SMG)导航的统一分类法，系统梳理了专门解决机器人"最后一英里"挑战的导航方法，填补了该领域缺乏标准化评估体系的空白。


<details>
  <summary>Details</summary>
Motivation: 传统多机器人导航方法在受限拥挤环境(SMG)中表现不佳，而现有SMG导航研究缺乏统一假设和评估标准，导致难以建立基准比较和实际应用选择困难。

Method: 通过定义SMG的独特特征和专用评估指标，建立统一的分类体系，对现有SMG求解器进行系统分类和整理。

Result: 创建了首个SMG导航方法的标准化分类框架，为研究者提供了清晰的领域地图和比较基准。

Conclusion: SMG导航研究需要专门的分类法、定义和评估协议来推动有效研究，本调查为该领域建立了必要的理论基础和标准化框架。

Abstract: The ``Last Mile Challenge'' has long been considered an important, yet
unsolved, challenge for autonomous vehicles, public service robots, and
delivery robots. A central issue in this challenge is the ability of robots to
navigate constrained and cluttered environments (e.g., doorways, hallways,
corridor intersections), often while competing for space with other robots and
humans. We refer to these environments as ``Social Mini-Games'' (SMGs). SMGs
are tightly coupled, high-agency interactions that arise within general
multi-robot navigation (MRN) scenarios. They are identified through certain
distinct characteristics and require specialized metrics to evaluate them.
Traditional navigation approaches designed for MRN do not perform well in SMGs,
which has led to focused research on dedicated SMG solvers (navigation methods
specialized to navigate in SMGs), which has flourished in recent years.
However, publications on SMG navigation research make different assumptions (on
centralized versus decentralized, observability, communication, cooperation,
etc.), and have different objective functions (safety versus liveness). These
assumptions and objectives are sometimes implicitly assumed or described
informally. This makes it difficult to establish appropriate baselines for
comparison in research papers, as well as making it difficult for practitioners
to find the papers relevant to their concrete application. Such ad-hoc
representation of the field also presents a barrier to new researchers wanting
to start research in this area. SMG navigation research requires its own
taxonomy, definitions, and evaluation protocols to guide effective research
moving forward. This survey is the first to catalog SMG solvers using a
well-defined and unified taxonomy and to classify existing methods accordingly.

</details>


### [9] [ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments](https://arxiv.org/abs/2508.13488)
*Jingwen Yu,Jiayi Yang,Anjun Hu,Jiankun Wang,Ping Tan,Hong Zhang*

Main category: cs.RO

TL;DR: ROVER是一种利用历史轨迹作为先验约束的闭环验证方法，专门针对重复环境中外观特征失效的问题，通过轨迹优化和评分机制来拒绝错误闭环检测。


<details>
  <summary>Details</summary>
Motivation: 在重复性环境中，基于外观特征的闭环检测容易产生误判，现有方法忽视了机器人时空运动轨迹这一重要先验知识，需要一种能够利用轨迹信息进行验证的方法。

Method: 提出ROVER方法：对于每个闭环候选，首先通过位姿图优化估计机器人轨迹，然后通过评分方案评估该轨迹与无闭环时的轨迹先验的符合程度，决定是否接受该闭环候选。

Result: 基准测试和真实世界实验证明了该方法的有效性，集成到最先进的SLAM系统中验证了其鲁棒性和效率。

Conclusion: 利用历史轨迹作为先验约束能够有效拒绝重复环境中的错误闭环检测，提高SLAM系统的可靠性。

Abstract: Loop closure detection is important for simultaneous localization and mapping
(SLAM), which associates current observations with historical keyframes,
achieving drift correction and global relocalization. However, a falsely
detected loop can be fatal, and this is especially difficult in repetitive
environments where appearance-based features fail due to the high similarity.
Therefore, verification of a loop closure is a critical step in avoiding false
positive detections. Existing works in loop closure verification predominantly
focus on learning invariant appearance features, neglecting the prior knowledge
of the robot's spatial-temporal motion cue, i.e., trajectory. In this letter,
we propose ROVER, a loop closure verification method that leverages the
historical trajectory as a prior constraint to reject false loops in
challenging repetitive environments. For each loop candidate, it is first used
to estimate the robot trajectory with pose-graph optimization. This trajectory
is then submitted to a scoring scheme that assesses its compliance with the
trajectory without the loop, which we refer to as the trajectory prior, to
determine if the loop candidate should be accepted. Benchmark comparisons and
real-world experiments demonstrate the effectiveness of the proposed method.
Furthermore, we integrate ROVER into state-of-the-art SLAM systems to verify
its robustness and efficiency. Our source code and self-collected dataset are
available at https://github.com/jarvisyjw/ROVER.

</details>


### [10] [Unified Hierarchical MPC in Task Executing for Modular Manipulators across Diverse Morphologies](https://arxiv.org/abs/2508.13513)
*Maolin Lei,Edoardo Romiti,Arturo Laurenzi,Cheng Zhou,Wanli Xing,Liang Lu,Nikos G. Tsagarakis*

Main category: cs.RO

TL;DR: 统一的层次模型预测控制方法，可适配不同槍式的模块化操纳机械手，无需深度调参即可执行任务


<details>
  <summary>Details</summary>
Motivation: 解决传统控制器在不同槍式模块化操纳机械手上需要重新调整参数的问题，提高控制系统的适应性和灵活性

Method: 采用两层次MPC结构：高层MPC预测未来状态和提供轨迹信息，低层MPC基于这些信息精炼控制动作并更新预测模型，包含二次线性化技术

Result: 在多种操纳机械手槍式上进行了广泛评估，实际场景中成功执行了摘放任务，显示了控制精度和可靠性的提升

Conclusion: H-MPC方法能够有效处理运动学约束，确保平滑的关节空间轨迹，在保持线性模型简洁性的同时提高了运动学表示的准确性

Abstract: This work proposes a unified Hierarchical Model Predictive Control (H-MPC)
for modular manipulators across various morphologies, as the controller can
adapt to different configurations to execute the given task without extensive
parameter tuning in the controller. The H-MPC divides the control process into
two levels: a high-level MPC and a low-level MPC. The high-level MPC predicts
future states and provides trajectory information, while the low-level MPC
refines control actions by updating the predictive model based on this
high-level information. This hierarchical structure allows for the integration
of kinematic constraints and ensures smooth joint-space trajectories, even near
singular configurations. Moreover, the low-level MPC incorporates secondary
linearization by leveraging predictive information from the high-level MPC,
effectively capturing the second-order Taylor expansion information of the
kinematic model while still maintaining a linearized model formulation. This
approach not only preserves the simplicity of a linear control model but also
enhances the accuracy of the kinematic representation, thereby improving
overall control precision and reliability. To validate the effectiveness of the
control policy, we conduct extensive evaluations across different manipulator
morphologies and demonstrate the execution of pick-and-place tasks in
real-world scenarios.

</details>


### [11] [A Three-Level Whole-Body Disturbance Rejection Control Framework for Dynamic Motions in Legged Robots](https://arxiv.org/abs/2508.13531)
*Bolin Li,Gewei Zuo,Zhixiang Wang,Xiaotian Ke,Lijun Zhu,Han Ding*

Main category: cs.RO

TL;DR: 这篇论文提出了一种三级全身干扰抱抗控制框架(T-WB-DRC)，通过新题移动水平扩展状态观测器(MH-ESO)估计不确定性，显著提升了腿式机器人在模型不确定性、外部干扰和故障条件下的稳定性和强壁性。


<details>
  <summary>Details</summary>
Motivation: 解决腿式机器人在存在模型不确定性、外部干扰和故障时的稳定性和强壁性问题，充分利用全状态反馈估计器来估计和补偿腿式机器人全身动力学中的不确定性。

Method: 首先提出新题移动水平扩展状态观测器(MH-ESO)来估计不确定性并减少噪声，然后提出三级全身干扰抱抗控制框架(T-WB-DRC)，考虑了无不确定性和有不确定性两种情况下的全身动力学规划。

Result: 在Gazebo模拟器中对人形和四足机器人进行了模拟，验证了T-WB-DRC的有效性和通用性。在四足机器人上进行了广泛的实验测试，验证了系统在各种干扰条件下使用T-WB-DRC时的强壁性和稳定性。

Conclusion: 该控制框架显著提升了腿式机器人的负载运输能力、外部干扰抱抗能力和故障宽容性，为腿式机器人在复杂环境中的稳定运行提供了有效解决方案。

Abstract: This paper presents a control framework designed to enhance the stability and
robustness of legged robots in the presence of uncertainties, including model
uncertainties, external disturbances, and faults. The framework enables the
full-state feedback estimator to estimate and compensate for uncertainties in
whole-body dynamics of the legged robots. First, we propose a novel moving
horizon extended state observer (MH-ESO) to estimate uncertainties and mitigate
noise in legged systems, which can be integrated into the framework for
disturbance compensation. Second, we introduce a three-level whole-body
disturbance rejection control framework (T-WB-DRC). Unlike the previous
two-level approach, this three-level framework considers both the plan based on
whole-body dynamics without uncertainties and the plan based on dynamics with
uncertainties, significantly improving payload transportation, external
disturbance rejection, and fault tolerance. Third, simulations of both humanoid
and quadruped robots in the Gazebo simulator demonstrate the effectiveness and
versatility of T-WB-DRC. Finally, extensive experimental trials on a quadruped
robot validate the robustness and stability of the system when using T-WB-DRC
under various disturbance conditions.

</details>


### [12] [MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence](https://arxiv.org/abs/2508.13534)
*Chao Tang,Anxing Xiao,Yuhong Deng,Tianrun Hu,Wenlong Dong,Hanbo Zhang,David Hsu,Hong Zhang*

Main category: cs.RO

TL;DR: MimicFunc是一个通过功能帧建立功能对应关系的框架，使机器人能够从单个人类RGB-D视频中学习工具操作技能，并泛化到新工具上，无需繁琐的遥操作数据收集。


<details>
  <summary>Details</summary>
Motivation: 人类能够通过观察一次工具操作就模仿并泛化到功能等效的不同工具上，而现有机器人难以达到这种泛化水平。主要挑战在于处理功能相似工具之间的几何差异（功能内变异）。

Method: 提出MimicFunc框架，使用基于关键点的抽象构建功能中心局部坐标系（功能帧），建立功能级别的对应关系来模仿工具操作技能。

Result: 实验表明MimicFunc能有效让机器人从单个人类视频泛化到操作新工具执行功能等效任务，生成的rollout可用于训练视觉运动策略。

Conclusion: 该框架为解决工具操作技能模仿中的功能对应问题提供了有效方案，展示了单次泛化能力，为减少遥操作数据收集需求提供了有前景的替代方案。

Abstract: Imitating tool manipulation from human videos offers an intuitive approach to
teaching robots, while also providing a promising and scalable alternative to
labor-intensive teleoperation data collection for visuomotor policy learning.
While humans can mimic tool manipulation behavior by observing others perform a
task just once and effortlessly transfer the skill to diverse tools for
functionally equivalent tasks, current robots struggle to achieve this level of
generalization. A key challenge lies in establishing function-level
correspondences, considering the significant geometric variations among
functionally similar tools, referred to as intra-function variations. To
address this challenge, we propose MimicFunc, a framework that establishes
functional correspondences with function frame, a function-centric local
coordinate frame constructed with keypoint-based abstraction, for imitating
tool manipulation skills. Experiments demonstrate that MimicFunc effectively
enables the robot to generalize the skill from a single RGB-D human video to
manipulating novel tools for functionally equivalent tasks. Furthermore,
leveraging MimicFunc's one-shot generalization capability, the generated
rollouts can be used to train visuomotor policies without requiring
labor-intensive teleoperation data collection for novel objects. Our code and
video are available at https://sites.google.com/view/mimicfunc.

</details>


### [13] [Assessing Pedestrian Behavior Around Autonomous Cleaning Robots in Public Spaces: Findings from a Field Observation](https://arxiv.org/abs/2508.13699)
*Maren Raab,Linda Miller,Zhe Zeng,Pascal Jansen,Martin Baumann,Johannes Kraus*

Main category: cs.RO

TL;DR: 研究调查了自主清扫机器人类型和运动模式对分心和未分心行人运动行为的影响，发现机器人大小和运动模式比行人是否分心更重要


<details>
  <summary>Details</summary>
Motivation: 随着自主机器人在公共空间的普及，需要研究如何通过通信策略提高透明度和减少危险情况，而当前对分心行人在机器人身边的行为研究较少

Method: 在现场设置中，对N=498名未知情的行人进行视频录制，观察他们穿过两个自主清扫机器人时的运动行为，其中约8%行人使用手机分心

Result: 分心和未分心行人在机器人附近的运动行为没有显著差异；更大的清扫机器人和偏移矩形运动模式显著增加了行人的横向调整次数；偏移矩形模式还导致更多的近距离横向调整

Conclusion: 研究为公共空间中行人在自主清扫机器人附近的运动行为提供了初步见解，对HRI研究领域做出了贡献

Abstract: As autonomous robots become more common in public spaces, spontaneous
encounters with laypersons are more frequent. For this, robots need to be
equipped with communication strategies that enhance momentary transparency and
reduce the probability of critical situations. Adapting these robotic
strategies requires consideration of robot movements, environmental conditions,
and user characteristics and states. While numerous studies have investigated
the impact of distraction on pedestrians' movement behavior, limited research
has examined this behavior in the presence of autonomous robots. This research
addresses the impact of robot type and robot movement pattern on distracted and
undistracted pedestrians' movement behavior. In a field setting, unaware
pedestrians were videotaped while moving past two working, autonomous cleaning
robots. Out of N=498 observed pedestrians, approximately 8% were distracted by
smartphones. Distracted and undistracted pedestrians did not exhibit
significant differences in their movement behaviors around the robots. Instead,
both the larger sweeping robot and the offset rectangular movement pattern
significantly increased the number of lateral adaptations compared to the
smaller cleaning robot and the circular movement pattern. The offset
rectangular movement pattern also led to significantly more close lateral
adaptations. Depending on the robot type, the movement patterns led to
differences in the distances of lateral adaptations. The study provides initial
insights into pedestrian movement behavior around an autonomous cleaning robot
in public spaces, contributing to the growing field of HRI research.

</details>


### [14] [Blast Hole Seeking and Dipping -- The Navigation and Perception Framework in a Mine Site Inspection Robot](https://arxiv.org/abs/2508.13785)
*Liyang Liu,Ehsan Mihankhah,Nathan Wallace,Javier Martinez,Andrew J. Hill*

Main category: cs.RO

TL;DR: 开采矿山爆破孔自主检测机器人DIPPeR，利用LiDAR点云数据进行凸台探测和孔漏定位，实现精准下孔位置控制和自主寻孔导航


<details>
  <summary>Details</summary>
Motivation: 传统人工孔漏检测效率低、成本高，无法全面获取孔漏几何和地质特性信息，下游材料处理成本较高

Method: 通过LiDAR收集点云数据，提取地面凸台体积，将3D点投影到虚拟深度图进行2D分割，识别圆形孔漏中心，使用非最大倾制模块确保精准位置，自动调整投影参数适应不同点云密度和孔径

Result: 在高保真模拟环境和现场实际测试中验证了导航和感知系统的有效性，实现了准确的孔漏定位和连续跟踪

Conclusion: DIPPeR机器人系统能够有效解决开采矿山孔漏自动检测问题，提高检测效率和准确度，为下游材料处理节省成本提供技术支撑

Abstract: In open-pit mining, holes are drilled into the surface of the excavation site
and detonated with explosives to facilitate digging. These blast holes need to
be inspected internally for investigation of downhole material types and
properties. Knowing these properties can lead to significant savings in
material handling costs in downstream processes. Manual hole inspection is slow
and expensive, with major limitations in revealing the geometric and geological
properties of the holes and their contents. This has been the motivation for
the development of our autonomous mine-site inspection robot - "DIPPeR". In
this paper, the automation aspect of the project is explained. We present a
robust blast hole seeking and detection framework that enables target-based
navigation and accurate down-hole sensor positioning. The pipeline first
processes point-cloud data collected by the on-board LiDAR sensors, extracting
the cone-shaped volume of drill-waste above the ground. By projecting the 3D
cone points into a virtual depth image, segmentation is achieved in the 2D
domain, yielding a circular hole at the image centre and a collared cone face.
We then identify the hole centre using a robust detection module while
suppressing non-maximum candidates, ensuring precise sensor placement for
down-hole inspection and avoiding collisions with the cavity wall. To enable
autonomous hole-seeking, the pipeline automatically adjusts its projection
parameters during robot navigation to account for variations in point sparsity
and hole opening size, ensuring a consistent hole appearance in 2D images. This
allows continuous tracking of the target hole as the robot approaches the goal
point. We demonstrate the effectiveness of our navigation and perception system
in both high-fidelity simulation environments and on-site field tests. A
demonstration video is available at
"https://www.youtube.com/watch?v=fRNbcBcaSqE".

</details>


### [15] [Trajectory Tracking and Stabilization of Quadrotors Using Deep Koopman Model Predictive Control](https://arxiv.org/abs/2508.13795)
*Haitham El-Hussieny*

Main category: cs.RO

TL;DR: 深度Koopman算子统一深度学习和模型预测控制，通过线性化方式处理四旋翼非线性动力学，实现高精度轨迹跟随和快速计算。


<details>
  <summary>Details</summary>
Motivation: 解决四旋翼系统非线性动力学导致的模型预测控制（NMPC）计算复杂度高、实时性差的问题，寻求新的控制框架来简化模型并保持控制性能。

Method: 使甦深度Koopman算子从飞行数据中学习构建高维潜在空间，在该空间中将非线性动力学近似为线性模型，然后应用模型预测控制（MPC）进行有限预测路径优化。

Result: 数值实验表明，该方法在轨迹跟随和点稳定任务中展现出更高的跟随精度和显著更低的计算时间，较传统非线性MPC有显著优势。

Conclusion: 深度Koopman算子统一MPC框架能够高效处理四旋翼复杂动力学，满足嵌入式飞行控制的实时要求，为灵活飞行场景和干扰强化研究指明了方向。

Abstract: This paper presents a data-driven control framework for quadrotor systems
that integrates a deep Koopman operator with model predictive control (DK-MPC).
The deep Koopman operator is trained on sampled flight data to construct a
high-dimensional latent representation in which the nonlinear quadrotor
dynamics are approximated by linear models. This linearization enables the
application of MPC to efficiently optimize control actions over a finite
prediction horizon, ensuring accurate trajectory tracking and stabilization.
The proposed DK-MPC approach is validated through a series of
trajectory-following and point-stabilization numerical experiments, where it
demonstrates superior tracking accuracy and significantly lower computation
time compared to conventional nonlinear MPC. These results highlight the
potential of Koopman-based learning methods to handle complex quadrotor
dynamics while meeting the real-time requirements of embedded flight control.
Future work will focus on extending the framework to more agile flight
scenarios and improving robustness against external disturbances.

</details>


### [16] [Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided Decision Transformer](https://arxiv.org/abs/2508.13877)
*Rathnam Vidushika Rasanji,Jin Wei-Kocsis,Jiansong Zhang,Dongming Gan,Ragu Athinarayanan,Paul Asunda*

Main category: cs.RO

TL;DR: 提出了Symbolically-Guided Decision Transformer (SGDT)框架，结合神经符号机制和因果变换器，用于可部署的多机器人协作操作任务


<details>
  <summary>Details</summary>
Motivation: 强化学习在机器人操作中数据密集且依赖MDP假设，难以处理复杂动态和长期时间依赖的多机器人操作任务。决策变换器作为离线替代方案在多机器人操作中的应用尚未充分探索

Method: SGDT框架包含神经符号规划器生成符号子目标的高级任务导向计划，以及目标条件决策变换器进行低级顺序决策的层次化架构

Result: 在零样本和少样本场景下评估了SGDT的性能表现

Conclusion: 这是首个探索基于决策变换器技术用于多机器人操作的工作，实现了结构化、可解释和可泛化的复杂多机器人协作决策

Abstract: Reinforcement learning (RL) has demonstrated great potential in robotic
operations. However, its data-intensive nature and reliance on the Markov
Decision Process (MDP) assumption limit its practical deployment in real-world
scenarios involving complex dynamics and long-term temporal dependencies, such
as multi-robot manipulation. Decision Transformers (DTs) have emerged as a
promising offline alternative by leveraging causal transformers for sequence
modeling in RL tasks. However, their applications to multi-robot manipulations
still remain underexplored. To address this gap, we propose a novel framework,
Symbolically-Guided Decision Transformer (SGDT), which integrates a
neuro-symbolic mechanism with a causal transformer to enable deployable
multi-robot collaboration. In the proposed SGDT framework, a neuro-symbolic
planner generates a high-level task-oriented plan composed of symbolic
subgoals. Guided by these subgoals, a goal-conditioned decision transformer
(GCDT) performs low-level sequential decision-making for multi-robot
manipulation. This hierarchical architecture enables structured, interpretable,
and generalizable decision making in complex multi-robot collaboration tasks.
We evaluate the performance of SGDT across a range of task scenarios, including
zero-shot and few-shot scenarios. To our knowledge, this is the first work to
explore DT-based technology for multi-robot manipulation.

</details>


### [17] [Driving Style Recognition Like an Expert Using Semantic Privileged Information from Large Language Models](https://arxiv.org/abs/2508.13881)
*Zhaokun Chen,Chaopeng Zhang,Xiaohan Li,Wenshuo Wang,Gentiane Venture,Junqiang Xi*

Main category: cs.RO

TL;DR: 使用大语言模型生成语义特权信息(SPI)来对齐驾驶风格识别算法与人类专家判断，提高识别准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 现有驾驶风格识别系统主要依赖低级传感器特征，忽视了人类专家的语义推理能力，导致算法分类与专家判断之间的对不齐

Method: 首先开发DriBehavGPT模块生成自然语言驾驶行为描述，通过文本嵌入和降维转换为机器学习可处理表征，然后作为特权信息集成到SVM+中进行训练

Result: 在多种实际驾驶场景中，SPI增强框架超越传统方法，F1分数提升8.6%（跟车）和7.9%（变道），且仅在训练使用SPI，推理仅需传感器数据

Conclusion: 语义行为表征在提高识别准确性和推进可解释性、以人为本的驾驶系统方面发挥了关键作用

Abstract: Existing driving style recognition systems largely depend on low-level
sensor-derived features for training, neglecting the rich semantic reasoning
capability inherent to human experts. This discrepancy results in a fundamental
misalignment between algorithmic classifications and expert judgments. To
bridge this gap, we propose a novel framework that integrates Semantic
Privileged Information (SPI) derived from large language models (LLMs) to align
recognition outcomes with human-interpretable reasoning. First, we introduce
DriBehavGPT, an interactive LLM-based module that generates natural-language
descriptions of driving behaviors. These descriptions are then encoded into
machine learning-compatible representations via text embedding and
dimensionality reduction. Finally, we incorporate them as privileged
information into Support Vector Machine Plus (SVM+) for training, enabling the
model to approximate human-like interpretation patterns. Experiments across
diverse real-world driving scenarios demonstrate that our SPI-enhanced
framework outperforms conventional methods, achieving F1-score improvements of
7.6% (car-following) and 7.9% (lane-changing). Importantly, SPI is exclusively
used during training, while inference relies solely on sensor data, ensuring
computational efficiency without sacrificing performance. These results
highlight the pivotal role of semantic behavioral representations in improving
recognition accuracy while advancing interpretable, human-centric driving
systems.

</details>


### [18] [Multimodal Data Storage and Retrieval for Embodied AI: A Survey](https://arxiv.org/abs/2508.13901)
*Yihao Lu,Hao Tang*

Main category: cs.RO

TL;DR: 本论文系统评估了五种存储架构和五种检索范式在体现式AI数据管理中的适用性，指出了长期语义一致性与实时响应能力之间的核心矛盾，并提出了物理感知数据模型、适应性存储检索协同优化等未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 体现式AI产生的市场、多模态数据流对传统数据管理系统构成挑战，需要找到能够满足物理基础、低延迟访问和动态扩展性等核心需求的数据管理方案。

Method: 通过系统性评估5种存储架构（图数据库、多模型数据库、数据湖、向量数据库、时序数据库）和5种检索范式（融合策略、表征对齐、图结构、生成模型、高效检索优化），基于对180多份相关研究的全面分析。

Result: 识别了从基础的物理基础间隔到系统性挑战的关键瓶颈，包括跨模态集成、动态适应和开放世界普适性等问题，揭示了语义一致性与实时性之间的核心矛盾。

Conclusion: 提出了包含物理感知数据模型、适应性存储检索协同优化和标准化测试基准在内的前瞻性研究议程，为下一代自主体现系统的稳健高性能数据管理框架设计提供了严谨的路线图。

Abstract: Embodied AI (EAI) agents continuously interact with the physical world,
generating vast, heterogeneous multimodal data streams that traditional
management systems are ill-equipped to handle. In this survey, we first
systematically evaluate five storage architectures (Graph Databases,
Multi-Model Databases, Data Lakes, Vector Databases, and Time-Series
Databases), focusing on their suitability for addressing EAI's core
requirements, including physical grounding, low-latency access, and dynamic
scalability. We then analyze five retrieval paradigms (Fusion Strategy-Based
Retrieval, Representation Alignment-Based Retrieval, Graph-Structure-Based
Retrieval, Generation Model-Based Retrieval, and Efficient Retrieval-Based
Optimization), revealing a fundamental tension between achieving long-term
semantic coherence and maintaining real-time responsiveness. Based on this
comprehensive analysis, we identify key bottlenecks, spanning from the
foundational Physical Grounding Gap to systemic challenges in cross-modal
integration, dynamic adaptation, and open-world generalization. Finally, we
outline a forward-looking research agenda encompassing physics-aware data
models, adaptive storage-retrieval co-optimization, and standardized
benchmarking, to guide future research toward principled data management
solutions for EAI. Our survey is based on a comprehensive review of more than
180 related studies, providing a rigorous roadmap for designing the robust,
high-performance data management frameworks essential for the next generation
of autonomous embodied systems.

</details>


### [19] [Augmenting cobots for sheet-metal SMEs with 3D object recognition and localisation](https://arxiv.org/abs/2508.13964)
*Martijn Cramer,Yanming Wu,David De Schepper,Eric Demeester*

Main category: cs.RO

TL;DR: 探讨在工业环境中通过3D物体识别和定位技术增强协作机器人系统的机遇与挑战，基于COOCK+ ROBUST项目将协作机器人转变为移动可重构生产助手


<details>
  <summary>Details</summary>
Motivation: 小批量多品种生产模式下，中小企业面临标准自动化解决方案不足的问题，导致重复性体力劳动增加生产成本，技术熟练劳动力未能充分发挥潜力

Method: 集成现有技术（包括3D物体识别和定位），将协作机器人转变为移动可重构生产助手，并通过ACRO研究单位与工业合作伙伴的实际项目案例进行验证

Result: 项目旨在解决中小企业在高混合低产量生产环境中的自动化挑战，通过增强协作机器人系统提升生产灵活性和效率

Conclusion: 通过集成3D识别和定位技术增强协作机器人系统，可以为中小企业提供可行的自动化解决方案，但实施过程中需要克服技术和集成方面的挑战

Abstract: Due to high-mix-low-volume production, sheet-metal workshops today are
challenged by small series and varying orders. As standard automation solutions
tend to fall short, SMEs resort to repetitive manual labour impacting
production costs and leading to tech-skilled workforces not being used to their
full potential. The COOCK+ ROBUST project aims to transform cobots into mobile
and reconfigurable production assistants by integrating existing technologies,
including 3D object recognition and localisation. This article explores both
the opportunities and challenges of enhancing cobotic systems with these
technologies in an industrial setting, outlining the key steps involved in the
process. Additionally, insights from a past project, carried out by the ACRO
research unit in collaboration with an industrial partner, serves as a concrete
implementation example throughout.

</details>


### [20] [Toward an Interaction-Centered Approach to Robot Trustworthiness](https://arxiv.org/abs/2508.13976)
*Carlo Mazzola,Hassan Ali,Kristína Malinovská,Igor Farkaš*

Main category: cs.RO

TL;DR: 本文提出了一个基于交互的框架，通过人机相互理解来建立信任，强调人类意识和透明度两个支柱，旨在使机器人行为符合人类期望并提供控制权。


<details>
  <summary>Details</summary>
Motivation: 随着机器人更深入融入人类环境，建立可信赖的具身机器人代理对于有效和安全的人机交互至关重要，需要避免错误信任或过度信任带来的安全风险和伦理问题。

Method: 提出了基于交互的信任建立框架，包含两个主要支柱：人类意识（机器人准确解释人类行为的能力）和透明度（清晰传达机器人意图和目标）。还引入了四个重要组件来弥合人类感知信任与机器人真实能力之间的差距。

Result: 该框架旨在使机器人行为与人类期望和需求保持一致，同时为人类伙伴提供对其行为的理解和控制权。

Conclusion: 通过整合人类意识和透明度这两个关键要素，可以促进人机之间基于相互理解的信任关系建立，实现更安全和有效的人机交互。

Abstract: As robots get more integrated into human environments, fostering
trustworthiness in embodied robotic agents becomes paramount for an effective
and safe human-robot interaction (HRI). To achieve that, HRI applications must
promote human trust that aligns with robot skills and avoid misplaced trust or
overtrust, which can pose safety risks and ethical concerns. To achieve that,
HRI applications must promote human trust that aligns with robot skills and
avoid misplaced trust or overtrust, which can pose safety risks and ethical
concerns. In this position paper, we outline an interaction-based framework for
building trust through mutual understanding between humans and robots. We
emphasize two main pillars: human awareness and transparency, referring to the
robot ability to interpret human actions accurately and to clearly communicate
its intentions and goals, respectively. By integrating these two pillars,
robots can behave in a manner that aligns with human expectations and needs
while providing their human partners with both comprehension and control over
their actions. We also introduce four components that we think are important
for bridging the gap between a human-perceived sense of trust and a robot true
capabilities.

</details>


### [21] [The Social Context of Human-Robot Interactions](https://arxiv.org/abs/2508.13982)
*Sydney Thompson,Kate Candon,Marynel Vázquez*

Main category: cs.RO

TL;DR: 本文对HRI领域中"社交情境"术语的使用混乱问题进行了系统梳理，提出了一个概念模型来统一描述人机交互的社交情境，并讨论了该模型的应用价值和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: HRI研究社区在设计和评估机器人行为时经常强调社交情境的重要性，但研究人员对"社交情境"这一术语的使用方式各不相同，导致沟通障碍和研究成果难以整合。

Method: 通过文献综述梳理HRI领域中现有的"社交情境"定义和使用方式，然后提出一个概念模型来描述人机交互的社交情境，并将该模型应用于现有研究工作。

Result: 提出了一个能够帮助研究人员规划交互、开发机器人行为模型以及在交互发生后获得洞察的社交情境属性框架，为HRI研究提供了统一的概念工具。

Conclusion: 虽然提出了概念模型来解决术语混乱问题，但在理解和建模人机交互社交情境方面仍存在许多开放的研究问题需要进一步探索。

Abstract: The Human-Robot Interaction (HRI) community often highlights the social
context of an interaction as a key consideration when designing, implementing,
and evaluating robot behavior. Unfortunately, researchers use the term "social
context" in varied ways. This can lead to miscommunication, making it
challenging to draw connections between related work on understanding and
modeling the social contexts of human-robot interactions. To address this gap,
we survey the HRI literature for existing definitions and uses of the term
"social context". Then, we propose a conceptual model for describing the social
context of a human-robot interaction. We apply this model to existing work, and
we discuss a range of attributes of social contexts that can help researchers
plan for interactions, develop behavior models for robots, and gain insights
after interactions have taken place. We conclude with a discussion of open
research questions in relation to understanding and modeling the social
contexts of human-robot interactions.

</details>


### [22] [Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation](https://arxiv.org/abs/2508.13998)
*Yifu Yuan,Haiqin Cui,Yaoting Huang,Yibin Chen,Fei Ni,Zibin Dong,Pengyi Li,Yan Zheng,Jianye Hao*

Main category: cs.RO

TL;DR: 这篇论文提出了一种以"指向"为统一中间表示的方法，通过Embodied-R1模型和两阶段强化学习训练方案，有效解决了体现式AI中的"视觉到动作间隔"问题，在多个基准测试中达到独创性能并显示出强夫的零样本演续能力。


<details>
  <summary>Details</summary>
Motivation: 解决体现式AI中的"视觉到动作间隔"问题，该问题来自于数据稀缺和体现异质性，限制了模型的普适性。

Method: 提出以"指向"作为统一的体现无关中间表示，定义四种核心体现指向能力；构建大规模数据集Embodied-Points-200K；设计了专门的3B视觉-语言模型Embodied-R1，采用两阶段强化学习调优(RFT)训练计划和专门的多任务奖励设计。

Result: 在11个体现空间和指向基准测试中达到最佳性能；在SIMPLEREnv中达到56.2%成功率，在8个实际XArm任务中达到87.5%成功率，较强基准提升62%；显示出对多样视觉干扰的高稳健性。

Conclusion: 指向中心表示结合RFT训练范式为闭合机器人学中的感知-动作间隔提供了一条高效且可普适的途径。

Abstract: Generalization in embodied AI is hindered by the "seeing-to-doing gap," which
stems from data scarcity and embodiment heterogeneity. To address this, we
pioneer "pointing" as a unified, embodiment-agnostic intermediate
representation, defining four core embodied pointing abilities that bridge
high-level vision-language comprehension with low-level action primitives. We
introduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed
for embodied reasoning and pointing. We use a wide range of embodied and
general visual reasoning datasets as sources to construct a large-scale
dataset, Embodied-Points-200K, which supports key embodied pointing
capabilities. We then train Embodied-R1 using a two-stage Reinforced
Fine-tuning (RFT) curriculum with a specialized multi-task reward design.
Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and
pointing benchmarks. Critically, it demonstrates robust zero-shot
generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5%
across 8 real-world XArm tasks without any task-specific fine-tuning,
representing a 62% improvement over strong baselines. Furthermore, the model
exhibits high robustness against diverse visual disturbances. Our work shows
that a pointing-centric representation, combined with an RFT training paradigm,
offers an effective and generalizable pathway to closing the perception-action
gap in robotics.

</details>


### [23] [Train Once, Deploy Anywhere: Realize Data-Efficient Dynamic Object Manipulation](https://arxiv.org/abs/2508.14042)
*Zhuoling Li,Xiaoyang Wu,Zhenhua Xu,Hengshuang Zhao*

Main category: cs.RO

TL;DR: 本文提出了一个基于熵的理论框架和GEM系统，仅需少量演示就能实现动态物体操作的强泛化能力，在真实食堂场景中取得了97%的成功率


<details>
  <summary>Details</summary>
Motivation: 解决动态物体操作中演示数据收集成本高的问题，探索是否能用少量演示实现强泛化能力

Method: 开发基于熵的理论框架量化模仿学习优化，提出Generalizable Entropy-based Manipulation (GEM)系统

Result: 在仿真和真实任务中广泛实验，GEM能够泛化到不同的环境背景、机器人形态、运动动力学和物体几何形状。在真实食堂餐具收集任务中，无需场景内演示，超过10,000次操作的成功率超过97%

Conclusion: GEM系统证明了仅用少量演示就能实现动态物体操作的强泛化能力，为制造业效率提升提供了有效解决方案

Abstract: Realizing generalizable dynamic object manipulation is important for
enhancing manufacturing efficiency, as it eliminates specialized engineering
for various scenarios. To this end, imitation learning emerges as a promising
paradigm, leveraging expert demonstrations to teach a policy manipulation
skills. Although the generalization of an imitation learning policy can be
improved by increasing demonstrations, demonstration collection is
labor-intensive. To address this problem, this paper investigates whether
strong generalization in dynamic object manipulation is achievable with only a
few demonstrations. Specifically, we develop an entropy-based theoretical
framework to quantify the optimization of imitation learning. Based on this
framework, we propose a system named Generalizable Entropy-based Manipulation
(GEM). Extensive experiments in simulated and real tasks demonstrate that GEM
can generalize across diverse environment backgrounds, robot embodiments,
motion dynamics, and object geometries. Notably, GEM has been deployed in a
real canteen for tableware collection. Without any in-scene demonstration, it
achieves a success rate of over 97% across more than 10,000 operations.

</details>
