{"id": "2511.11616", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11616", "abs": "https://arxiv.org/abs/2511.11616", "authors": ["Rathin Chandra Shit", "Sharmila Subudhi"], "title": "Hierarchical Federated Graph Attention Networks for Scalable and Resilient UAV Collision Avoidance", "comment": "Accepted and scheduled for conference presentation", "summary": "The real-time performance, adversarial resiliency, and privacy preservation are the most important metrics that need to be balanced to practice collision avoidance in large-scale multi-UAV (Unmanned Aerial Vehicle) systems. Current frameworks tend to prescribe monolithic solutions that are not only prohibitively computationally complex with a scaling cost of $O(n^2)$ but simply do not offer Byzantine fault tolerance. The proposed hierarchical framework presented in this paper tries to eliminate such trade-offs by stratifying a three-layered architecture. We spread the intelligence into three layers: an immediate collision avoiding local layer running on dense graph attention with latency of $<10 ms$, a regional layer using sparse attention with $O(nk)$ computational complexity and asynchronous federated learning with coordinate-wise trimmed mean aggregation, and lastly, a global layer using a lightweight Hashgraph-inspired protocol. We have proposed an adaptive differential privacy mechanism, wherein the noise level $(\u03b5\\in [0.1, 1.0])$ is dynamically reduced based on an evaluation of the measured real-time threat that in turn maximized the privacy-utility tradeoff. Through the use of Distributed Hash Table (DHT)-based lightweight audit logging instead of heavyweight blockchain consensus, the median cost of getting a $95^{th}$ percentile decision within 50ms is observed across all tested swarm sizes. This architecture provides a scalable scenario of 500 UAVs with a collision rate of $< 2.0\\%$ and the Byzantine fault tolerance of $f < n/3$.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6846\u67b6\u6765\u89e3\u51b3\u5927\u89c4\u6a21\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u7684\u5b9e\u65f6\u6027\u80fd\u3001\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u4e09\u5c42\u67b6\u6784\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u78b0\u649e\u907f\u514d\u3002", "motivation": "\u73b0\u6709\u6846\u67b6\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff08O(n\u00b2)\uff09\u4e14\u7f3a\u4e4f\u62dc\u5360\u5ead\u5bb9\u9519\u80fd\u529b\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u5728\u5927\u89c4\u6a21\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u5e73\u8861\u5b9e\u65f6\u6027\u80fd\u3001\u5b89\u5168\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "method": "\u91c7\u7528\u4e09\u5c42\u5206\u5c42\u67b6\u6784\uff1a\u672c\u5730\u5c42\u4f7f\u7528\u5bc6\u96c6\u56fe\u6ce8\u610f\u529b\u8fdb\u884c\u5373\u65f6\u78b0\u649e\u907f\u514d\uff08\u5ef6\u8fdf<10ms\uff09\uff1b\u533a\u57df\u5c42\u4f7f\u7528\u7a00\u758f\u6ce8\u610f\u529b\uff08O(nk)\u590d\u6742\u5ea6\uff09\u548c\u5f02\u6b65\u8054\u90a6\u5b66\u4e60\uff1b\u5168\u5c40\u5c42\u4f7f\u7528\u8f7b\u91cf\u7ea7Hashgraph\u534f\u8bae\u548c\u81ea\u9002\u5e94\u5dee\u5206\u9690\u79c1\u673a\u5236\u3002", "result": "\u5728500\u67b6\u65e0\u4eba\u673a\u89c4\u6a21\u4e0b\u5b9e\u73b0\u78b0\u649e\u7387<2.0%\uff0c\u62dc\u5360\u5ead\u5bb9\u9519f<n/3\uff0c95%\u51b3\u7b56\u572850ms\u5185\u5b8c\u6210\uff0c\u9690\u79c1\u53c2\u6570\u03b5\u2208[0.1,1.0]\u52a8\u6001\u8c03\u6574\u3002", "conclusion": "\u8be5\u5206\u5c42\u6846\u67b6\u6210\u529f\u6d88\u9664\u4e86\u5927\u89c4\u6a21\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u5b9e\u65f6\u6027\u80fd\u3001\u5b89\u5168\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11634", "categories": ["cs.RO", "cs.CV", "cs.HC", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.11634", "abs": "https://arxiv.org/abs/2511.11634", "authors": ["Michikuni Eguchi", "Takekazu Kitagishi", "Yuichi Hiroi", "Takefumi Hiraki"], "title": "Tactile Data Recording System for Clothing with Motion-Controlled Robotic Sliding", "comment": "3 pages, 2 figures, 1 table. Presented at SIGGRAPH Asia 2025 Posters (SA Posters '25), December 15-18, 2025, Hong Kong, Hong Kong", "summary": "The tactile sensation of clothing is critical to wearer comfort. To reveal physical properties that make clothing comfortable, systematic collection of tactile data during sliding motion is required. We propose a robotic arm-based system for collecting tactile data from intact garments. The system performs stroking measurements with a simulated fingertip while precisely controlling speed and direction, enabling creation of motion-labeled, multimodal tactile databases. Machine learning evaluation showed that including motion-related parameters improved identification accuracy for audio and acceleration data, demonstrating the efficacy of motion-related labels for characterizing clothing tactile sensation. This system provides a scalable, non-destructive method for capturing tactile data of clothing, contributing to future studies on fabric perception and reproduction.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u673a\u68b0\u81c2\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u62df\u6307\u5c16\u6ed1\u52a8\u8fd0\u52a8\u6536\u96c6\u670d\u88c5\u89e6\u89c9\u6570\u636e\uff0c\u521b\u5efa\u5e26\u8fd0\u52a8\u6807\u7b7e\u7684\u591a\u6a21\u6001\u89e6\u89c9\u6570\u636e\u5e93\uff0c\u673a\u5668\u5b66\u4e60\u8bc4\u4f30\u663e\u793a\u8fd0\u52a8\u53c2\u6570\u80fd\u63d0\u9ad8\u8bc6\u522b\u51c6\u786e\u7387", "motivation": "\u670d\u88c5\u89e6\u89c9\u5bf9\u7a7f\u7740\u8212\u9002\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u7cfb\u7edf\u6536\u96c6\u6ed1\u52a8\u8fd0\u52a8\u4e2d\u7684\u89e6\u89c9\u6570\u636e\u6765\u63ed\u793a\u8212\u9002\u670d\u88c5\u7684\u7269\u7406\u7279\u6027", "method": "\u4f7f\u7528\u673a\u68b0\u81c2\u7cfb\u7edf\u5bf9\u5b8c\u6574\u670d\u88c5\u8fdb\u884c\u89e6\u89c9\u6570\u636e\u91c7\u96c6\uff0c\u901a\u8fc7\u6a21\u62df\u6307\u5c16\u8fdb\u884c\u6ed1\u52a8\u6d4b\u91cf\uff0c\u7cbe\u786e\u63a7\u5236\u901f\u5ea6\u548c\u65b9\u5411", "result": "\u673a\u5668\u5b66\u4e60\u8bc4\u4f30\u8868\u660e\uff0c\u5305\u542b\u8fd0\u52a8\u76f8\u5173\u53c2\u6570\u63d0\u9ad8\u4e86\u97f3\u9891\u548c\u52a0\u901f\u5ea6\u6570\u636e\u7684\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u8fd0\u52a8\u76f8\u5173\u6807\u7b7e\u5728\u8868\u5f81\u670d\u88c5\u89e6\u89c9\u611f\u53d7\u65b9\u9762\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u975e\u7834\u574f\u6027\u7684\u670d\u88c5\u89e6\u89c9\u6570\u636e\u91c7\u96c6\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u7ec7\u7269\u611f\u77e5\u548c\u518d\u73b0\u7684\u7814\u7a76"}}
{"id": "2511.11639", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11639", "abs": "https://arxiv.org/abs/2511.11639", "authors": ["Jie Fan", "Francesco Visentin", "Barbara Mazzolai", "Emanuela Del Dottore"], "title": "Image-based Morphological Characterization of Filamentous Biological Structures with Non-constant Curvature Shape Feature", "comment": "This manuscript is a preprint version of the article currently under peer review at International Journal of Computer Vision (IJCV)", "summary": "Tendrils coil their shape to anchor the plant to supporting structures, allowing vertical growth toward light. Although climbing plants have been studied for a long time, extracting information regarding the relationship between the temporal shape change, the event that triggers it, and the contact location is still challenging. To help build this relation, we propose an image-based method by which it is possible to analyze shape changes over time in tendrils when mechano-stimulated in different portions of their body. We employ a geometric approach using a 3D Piece-Wise Clothoid-based model to reconstruct the configuration taken by a tendril after mechanical rubbing. The reconstruction shows high robustness and reliability with an accuracy of R2 > 0.99. This method demonstrates distinct advantages over deep learning-based approaches, including reduced data requirements, lower computational costs, and interpretability. Our analysis reveals higher responsiveness in the apical segment of tendrils, which might correspond to higher sensitivity and tissue flexibility in that region of the organs. Our study provides a methodology for gaining new insights into plant biomechanics and offers a foundation for designing and developing novel intelligent robotic systems inspired by climbing plants.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u76843D\u51e0\u4f55\u5efa\u6a21\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u85e4\u8513\u89e6\u987b\u5728\u673a\u68b0\u523a\u6fc0\u4e0b\u7684\u5f62\u72b6\u53d8\u5316\uff0c\u8be5\u65b9\u6cd5\u6bd4\u6df1\u5ea6\u5b66\u4e60\u66f4\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u3002", "motivation": "\u7814\u7a76\u85e4\u8513\u89e6\u987b\u5f62\u72b6\u53d8\u5316\u4e0e\u673a\u68b0\u523a\u6fc0\u4f4d\u7f6e\u3001\u89e6\u53d1\u4e8b\u4ef6\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ee5\u7406\u89e3\u690d\u7269\u751f\u7269\u529b\u5b66\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e3D\u5206\u6bb5\u56de\u65cb\u66f2\u7ebf\u7684\u51e0\u4f55\u65b9\u6cd5\u91cd\u5efa\u85e4\u8513\u89e6\u987b\u5728\u673a\u68b0\u6469\u64e6\u540e\u7684\u5f62\u72b6\u914d\u7f6e\u3002", "result": "\u91cd\u5efa\u65b9\u6cd5\u5177\u6709\u9ad8\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\uff08R2 > 0.99\uff09\uff0c\u53d1\u73b0\u85e4\u8513\u9876\u7aef\u90e8\u5206\u5bf9\u523a\u6fc0\u54cd\u5e94\u66f4\u5f3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u690d\u7269\u751f\u7269\u529b\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u5e76\u4e3a\u53d7\u6500\u63f4\u690d\u7269\u542f\u53d1\u7684\u667a\u80fd\u673a\u5668\u4eba\u7cfb\u7edf\u8bbe\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.11740", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11740", "abs": "https://arxiv.org/abs/2511.11740", "authors": ["Haowen Jiang", "Xinyu Huang", "You Lu", "Dingji Wang", "Yuheng Cao", "Chaofeng Sha", "Bihuan Chen", "Keyu Chen", "Xin Peng"], "title": "ExpertAD: Enhancing Autonomous Driving Systems with Mixture of Experts", "comment": "The paper has been accepted by the Fortieth AAAI Conference on Artificial Intelligence. AAAI 2026", "summary": "Recent advancements in end-to-end autonomous driving systems (ADSs) underscore their potential for perception and planning capabilities. However, challenges remain. Complex driving scenarios contain rich semantic information, yet ambiguous or noisy semantics can compromise decision reliability, while interference between multiple driving tasks may hinder optimal planning. Furthermore, prolonged inference latency slows decision-making, increasing the risk of unsafe driving behaviors. To address these challenges, we propose ExpertAD, a novel framework that enhances the performance of ADS with Mixture of Experts (MoE) architecture. We introduce a Perception Adapter (PA) to amplify task-critical features, ensuring contextually relevant scene understanding, and a Mixture of Sparse Experts (MoSE) to minimize task interference during prediction, allowing for effective and efficient planning. Our experiments show that ExpertAD reduces average collision rates by up to 20% and inference latency by 25% compared to prior methods. We further evaluate its multi-skill planning capabilities in rare scenarios (e.g., accidents, yielding to emergency vehicles) and demonstrate strong generalization to unseen urban environments. Additionally, we present a case study that illustrates its decision-making process in complex driving scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86ExpertAD\u6846\u67b6\uff0c\u4f7f\u7528\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6027\u80fd\uff0c\u901a\u8fc7\u611f\u77e5\u9002\u914d\u5668\u548c\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u51cf\u5c11\u4efb\u52a1\u5e72\u6270\uff0c\u964d\u4f4e\u78b0\u649e\u7387\u548c\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u5f53\u524d\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9762\u4e34\u8bed\u4e49\u4fe1\u606f\u6a21\u7cca\u3001\u591a\u4efb\u52a1\u5e72\u6270\u548c\u63a8\u7406\u5ef6\u8fdf\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u51b3\u7b56\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u5f15\u5165\u611f\u77e5\u9002\u914d\u5668\u589e\u5f3a\u4efb\u52a1\u5173\u952e\u7279\u5f81\uff0c\u4f7f\u7528\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u6700\u5c0f\u5316\u9884\u6d4b\u65f6\u7684\u4efb\u52a1\u5e72\u6270\uff0c\u5b9e\u73b0\u9ad8\u6548\u89c4\u5212\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0cExpertAD\u5c06\u5e73\u5747\u78b0\u649e\u7387\u964d\u4f4e\u8fbe20%\uff0c\u63a8\u7406\u5ef6\u8fdf\u51cf\u5c1125%\uff0c\u5728\u7f55\u89c1\u573a\u666f\u4e2d\u5c55\u73b0\u591a\u6280\u80fd\u89c4\u5212\u80fd\u529b\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u57ce\u5e02\u73af\u5883\u3002", "conclusion": "ExpertAD\u6846\u67b6\u901a\u8fc7\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8bed\u4e49\u6a21\u7cca\u3001\u4efb\u52a1\u5e72\u6270\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2511.11777", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11777", "abs": "https://arxiv.org/abs/2511.11777", "authors": ["Vinit Mehta", "Charu Sharma", "Karthick Thiyagarajan"], "title": "Large Language Models and 3D Vision for Intelligent Robotic Perception and Autonomy: A Review", "comment": "45 pages, 15 figures, MDPI Sensors Journal", "summary": "With the rapid advancement of artificial intelligence and robotics, the integration of Large Language Models (LLMs) with 3D vision is emerging as a transformative approach to enhancing robotic sensing technologies. This convergence enables machines to perceive, reason and interact with complex environments through natural language and spatial understanding, bridging the gap between linguistic intelligence and spatial perception. This review provides a comprehensive analysis of state-of-the-art methodologies, applications and challenges at the intersection of LLMs and 3D vision, with a focus on next-generation robotic sensing technologies. We first introduce the foundational principles of LLMs and 3D data representations, followed by an in-depth examination of 3D sensing technologies critical for robotics. The review then explores key advancements in scene understanding, text-to-3D generation, object grounding and embodied agents, highlighting cutting-edge techniques such as zero-shot 3D segmentation, dynamic scene synthesis and language-guided manipulation. Furthermore, we discuss multimodal LLMs that integrate 3D data with touch, auditory and thermal inputs, enhancing environmental comprehension and robotic decision-making. To support future research, we catalog benchmark datasets and evaluation metrics tailored for 3D-language and vision tasks. Finally, we identify key challenges and future research directions, including adaptive model architectures, enhanced cross-modal alignment and real-time processing capabilities, which pave the way for more intelligent, context-aware and autonomous robotic sensing systems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e3D\u89c6\u89c9\u5728\u673a\u5668\u4eba\u611f\u77e5\u6280\u672f\u4e2d\u7684\u878d\u5408\uff0c\u5206\u6790\u4e86\u5f53\u524d\u65b9\u6cd5\u3001\u5e94\u7528\u548c\u6311\u6218\uff0c\u63a2\u8ba8\u4e86\u573a\u666f\u7406\u89e3\u3001\u6587\u672c\u52303D\u751f\u6210\u3001\u7269\u4f53\u5b9a\u4f4d\u7b49\u5173\u952e\u6280\u672f\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u4eba\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e3D\u89c6\u89c9\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u548c\u7a7a\u95f4\u7406\u89e3\u4f7f\u673a\u5668\u611f\u77e5\u3001\u63a8\u7406\u548c\u4ea4\u4e92\u590d\u6742\u73af\u5883\uff0c\u5f25\u5408\u8bed\u8a00\u667a\u80fd\u4e0e\u7a7a\u95f4\u611f\u77e5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u9996\u5148\u4ecb\u7ecdLLMs\u548c3D\u6570\u636e\u8868\u793a\u7684\u57fa\u7840\u539f\u7406\uff0c\u6df1\u5165\u5206\u6790\u673a\u5668\u4eba\u5173\u952e\u76843D\u611f\u77e5\u6280\u672f\uff0c\u7136\u540e\u63a2\u8ba8\u573a\u666f\u7406\u89e3\u3001\u6587\u672c\u52303D\u751f\u6210\u3001\u7269\u4f53\u5b9a\u4f4d\u548c\u5177\u8eab\u667a\u80fd\u4f53\u7b49\u5173\u952e\u8fdb\u5c55\uff0c\u5305\u62ec\u96f6\u6837\u672c3D\u5206\u5272\u3001\u52a8\u6001\u573a\u666f\u5408\u6210\u548c\u8bed\u8a00\u5f15\u5bfc\u64cd\u4f5c\u7b49\u524d\u6cbf\u6280\u672f\u3002", "result": "\u8ba8\u8bba\u4e86\u6574\u54083D\u6570\u636e\u4e0e\u89e6\u89c9\u3001\u542c\u89c9\u548c\u70ed\u8f93\u5165\u7684\u591a\u6a21\u6001LLMs\uff0c\u589e\u5f3a\u4e86\u73af\u5883\u7406\u89e3\u548c\u673a\u5668\u4eba\u51b3\u7b56\u80fd\u529b\u3002\u4e3a\u652f\u6301\u672a\u6765\u7814\u7a76\uff0c\u6574\u7406\u4e86\u9488\u5bf93D\u8bed\u8a00\u548c\u89c6\u89c9\u4efb\u52a1\u7684\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "\u786e\u5b9a\u4e86\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u6a21\u578b\u67b6\u6784\u3001\u589e\u5f3a\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u5b9e\u65f6\u5904\u7406\u80fd\u529b\uff0c\u8fd9\u4e9b\u5c06\u4e3a\u66f4\u667a\u80fd\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u81ea\u4e3b\u7684\u673a\u5668\u4eba\u611f\u77e5\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2511.11840", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11840", "abs": "https://arxiv.org/abs/2511.11840", "authors": ["Shuangyu Xie", "Kaiyuan Chen", "Wenjing Chen", "Chengyuan Qian", "Christian Juette", "Liu Ren", "Dezhen Song", "Ken Goldberg"], "title": "LAVQA: A Latency-Aware Visual Question Answering Framework for Shared Autonomy in Self-Driving Vehicles", "comment": null, "summary": "When uncertainty is high, self-driving vehicles may halt for safety and benefit from the access to remote human operators who can provide high-level guidance. This paradigm, known as {shared autonomy}, enables autonomous vehicle and remote human operators to jointly formulate appropriate responses. To address critical decision timing with variable latency due to wireless network delays and human response time, we present LAVQA, a latency-aware shared autonomy framework that integrates Visual Question Answering (VQA) and spatiotemporal risk visualization. LAVQA augments visual queries with Latency-Induced COllision Map (LICOM), a dynamically evolving map that represents both temporal latency and spatial uncertainty. It enables remote operator to observe as the vehicle safety regions vary over time in the presence of dynamic obstacles and delayed responses. Closed-loop simulations in CARLA, the de-facto standard for autonomous vehicle simulator, suggest that that LAVQA can reduce collision rates by over 8x compared to latency-agnostic baselines.", "AI": {"tldr": "LAVQA\u662f\u4e00\u4e2a\u5ef6\u8fdf\u611f\u77e5\u7684\u5171\u4eab\u81ea\u4e3b\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u95ee\u7b54\u548c\u65f6\u7a7a\u98ce\u9669\u53ef\u89c6\u5316\u6765\u5e94\u5bf9\u65e0\u7ebf\u7f51\u7edc\u5ef6\u8fdf\u548c\u4eba\u7c7b\u54cd\u5e94\u65f6\u95f4\u5e26\u6765\u7684\u51b3\u7b56\u65f6\u673a\u6311\u6218\u3002", "motivation": "\u5f53\u4e0d\u786e\u5b9a\u6027\u8f83\u9ad8\u65f6\uff0c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u53ef\u80fd\u9700\u8981\u505c\u8f66\u7b49\u5f85\u5b89\u5168\uff0c\u5e76\u53d7\u76ca\u4e8e\u8fdc\u7a0b\u4eba\u7c7b\u64cd\u4f5c\u5458\u63d0\u4f9b\u7684\u9ad8\u7ea7\u6307\u5bfc\u3002\u5171\u4eab\u81ea\u4e3b\u6027\u8303\u5f0f\u5141\u8bb8\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u548c\u8fdc\u7a0b\u4eba\u7c7b\u64cd\u4f5c\u5458\u5171\u540c\u5236\u5b9a\u9002\u5f53\u7684\u54cd\u5e94\u7b56\u7565\u3002", "method": "LAVQA\u6846\u67b6\u6574\u5408\u4e86\u89c6\u89c9\u95ee\u7b54\u548c\u65f6\u7a7a\u98ce\u9669\u53ef\u89c6\u5316\uff0c\u901a\u8fc7\u5ef6\u8fdf\u8bf1\u5bfc\u78b0\u649e\u5730\u56fe\u52a8\u6001\u8868\u793a\u65f6\u95f4\u5ef6\u8fdf\u548c\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u8fdc\u7a0b\u64cd\u4f5c\u5458\u80fd\u591f\u89c2\u5bdf\u8f66\u8f86\u5b89\u5168\u533a\u57df\u5728\u52a8\u6001\u969c\u788d\u7269\u548c\u5ef6\u8fdf\u54cd\u5e94\u60c5\u51b5\u4e0b\u7684\u53d8\u5316\u3002", "result": "\u5728CARLA\u81ea\u52a8\u9a7e\u9a76\u6a21\u62df\u5668\u4e2d\u7684\u95ed\u73af\u4eff\u771f\u8868\u660e\uff0c\u4e0e\u4e0d\u8003\u8651\u5ef6\u8fdf\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cLAVQA\u53ef\u4ee5\u5c06\u78b0\u649e\u7387\u964d\u4f4e8\u500d\u4ee5\u4e0a\u3002", "conclusion": "LAVQA\u6846\u67b6\u901a\u8fc7\u5ef6\u8fdf\u611f\u77e5\u7684\u5171\u4eab\u81ea\u4e3b\u6027\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7531\u4e8e\u7f51\u7edc\u5ef6\u8fdf\u548c\u4eba\u7c7b\u54cd\u5e94\u65f6\u95f4\u5e26\u6765\u7684\u51b3\u7b56\u65f6\u673a\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u3002"}}
{"id": "2511.11845", "categories": ["cs.RO", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.11845", "abs": "https://arxiv.org/abs/2511.11845", "authors": ["K. A. I. N Jayarathne", "R. M. N. M. Rathnayaka", "D. P. S. S. Peiris"], "title": "Autonomous Underwater Cognitive System for Adaptive Navigation: A SLAM-Integrated Cognitive Architecture", "comment": "6 pages, 2 figures", "summary": "Deep-sea exploration poses significant challenges, including disorientation, communication loss, and navigational failures in dynamic underwater environments. This paper presents an Autonomous Underwater Cognitive System (AUCS) that integrates Simultaneous Localization and Mapping (SLAM) with a Soar-based cognitive architecture to enable adaptive navigation in complex oceanic conditions. The system fuses multi-sensor data from SONAR, LiDAR, IMU, and DVL with cognitive reasoning modules for perception, attention, planning, and learning. Unlike conventional SLAM systems, AUCS incorporates semantic understanding, adaptive sensor management, and memory-based learning to differentiate between dynamic and static objects, reducing false loop closures and enhancing long-term map consistency. The proposed architecture demonstrates a complete perception-cognition-action-learning loop, allowing autonomous underwater vehicles to sense, reason, and adapt intelligently. This work lays a foundation for next-generation cognitive submersible systems, improving safety, reliability, and autonomy in deep-sea exploration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210SLAM\u4e0eSoar\u8ba4\u77e5\u67b6\u6784\u7684\u81ea\u4e3b\u6c34\u4e0b\u8ba4\u77e5\u7cfb\u7edf(AUCS)\uff0c\u901a\u8fc7\u591a\u4f20\u611f\u5668\u6570\u636e\u878d\u5408\u548c\u8ba4\u77e5\u63a8\u7406\u6a21\u5757\uff0c\u5b9e\u73b0\u590d\u6742\u6d77\u6d0b\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u5bfc\u822a\u3002", "motivation": "\u6df1\u6d77\u63a2\u7d22\u9762\u4e34\u8ff7\u5931\u65b9\u5411\u3001\u901a\u4fe1\u4e2d\u65ad\u548c\u5bfc\u822a\u5931\u8d25\u7b49\u6311\u6218\uff0c\u9700\u8981\u80fd\u591f\u9002\u5e94\u52a8\u6001\u6c34\u4e0b\u73af\u5883\u7684\u667a\u80fd\u5bfc\u822a\u7cfb\u7edf\u3002", "method": "\u878d\u5408SONAR\u3001LiDAR\u3001IMU\u548cDVL\u7b49\u591a\u4f20\u611f\u5668\u6570\u636e\uff0c\u7ed3\u5408Soar\u8ba4\u77e5\u67b6\u6784\u7684\u611f\u77e5\u3001\u6ce8\u610f\u529b\u3001\u89c4\u5212\u548c\u5b66\u4e60\u6a21\u5757\uff0c\u5b9e\u73b0\u8bed\u4e49\u7406\u89e3\u3001\u81ea\u9002\u5e94\u4f20\u611f\u5668\u7ba1\u7406\u548c\u57fa\u4e8e\u8bb0\u5fc6\u7684\u5b66\u4e60\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u533a\u5206\u52a8\u6001\u548c\u9759\u6001\u7269\u4f53\uff0c\u51cf\u5c11\u9519\u8bef\u95ed\u73af\u68c0\u6d4b\uff0c\u589e\u5f3a\u957f\u671f\u5730\u56fe\u4e00\u81f4\u6027\uff0c\u5c55\u793a\u4e86\u5b8c\u6574\u7684\u611f\u77e5-\u8ba4\u77e5-\u884c\u52a8-\u5b66\u4e60\u5faa\u73af\u3002", "conclusion": "\u4e3a\u4e0b\u4e00\u4ee3\u8ba4\u77e5\u6f5c\u6c34\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u9ad8\u4e86\u6df1\u6d77\u63a2\u7d22\u7684\u5b89\u5168\u6027\u3001\u53ef\u9760\u6027\u548c\u81ea\u4e3b\u6027\u3002"}}
{"id": "2511.11931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11931", "abs": "https://arxiv.org/abs/2511.11931", "authors": ["Saida Liu", "Nikolay Atanasov", "Shumon Koga"], "title": "MATT-Diff: Multimodal Active Target Tracking by Diffusion Policy", "comment": "14 pages, 3 figures. Submitted to L4DC 2026", "summary": "This paper proposes MATT-Diff: Multi-Modal Active Target Tracking by Diffusion Policy, a control policy that captures multiple behavioral modes - exploration, dedicated tracking, and target reacquisition - for active multi-target tracking. The policy enables agent control without prior knowledge of target numbers, states, or dynamics. Effective target tracking demands balancing exploration for undetected or lost targets with following the motion of detected but uncertain ones. We generate a demonstration dataset from three expert planners including frontier-based exploration, an uncertainty-based hybrid planner switching between frontier-based exploration and RRT* tracking based on target uncertainty, and a time-based hybrid planner switching between exploration and tracking based on target detection time. We design a control policy utilizing a vision transformer for egocentric map tokenization and an attention mechanism to integrate variable target estimates represented by Gaussian densities. Trained as a diffusion model, the policy learns to generate multi-modal action sequences through a denoising process. Evaluations demonstrate MATT-Diff's superior tracking performance against expert and behavior cloning baselines across multiple target motions, empirically validating its advantages in target tracking.", "AI": {"tldr": "MATT-Diff\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7b56\u7565\u7684\u591a\u6a21\u6001\u4e3b\u52a8\u76ee\u6807\u8ddf\u8e2a\u63a7\u5236\u7b56\u7565\uff0c\u80fd\u591f\u6355\u6349\u63a2\u7d22\u3001\u4e13\u6ce8\u8ddf\u8e2a\u548c\u76ee\u6807\u91cd\u6355\u83b7\u7b49\u591a\u79cd\u884c\u4e3a\u6a21\u5f0f\uff0c\u65e0\u9700\u76ee\u6807\u6570\u91cf\u3001\u72b6\u6001\u6216\u52a8\u6001\u7684\u5148\u9a8c\u77e5\u8bc6\u3002", "motivation": "\u6709\u6548\u7684\u76ee\u6807\u8ddf\u8e2a\u9700\u8981\u5728\u63a2\u7d22\u672a\u68c0\u6d4b\u6216\u4e22\u5931\u76ee\u6807\u4e0e\u8ddf\u968f\u5df2\u68c0\u6d4b\u4f46\u4e0d\u786e\u5b9a\u76ee\u6807\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5904\u7406\u8fd9\u4e9b\u4e0d\u540c\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "method": "\u4f7f\u7528\u4e09\u4e2a\u4e13\u5bb6\u89c4\u5212\u5668\u751f\u6210\u6f14\u793a\u6570\u636e\u96c6\uff0c\u5305\u62ec\u57fa\u4e8e\u8fb9\u754c\u7684\u63a2\u7d22\u3001\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u6df7\u5408\u89c4\u5212\u5668\u548c\u57fa\u4e8e\u65f6\u95f4\u7684\u6df7\u5408\u89c4\u5212\u5668\u3002\u91c7\u7528\u89c6\u89c9\u53d8\u6362\u5668\u8fdb\u884c\u81ea\u4e2d\u5fc3\u5730\u56fe\u6807\u8bb0\u5316\uff0c\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u9ad8\u65af\u5bc6\u5ea6\u8868\u793a\u7684\u53ef\u53d8\u76ee\u6807\u4f30\u8ba1\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u751f\u6210\u591a\u6a21\u6001\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u8bc4\u4f30\u663e\u793aMATT-Diff\u5728\u591a\u79cd\u76ee\u6807\u8fd0\u52a8\u573a\u666f\u4e0b\uff0c\u76f8\u6bd4\u4e13\u5bb6\u548c\u884c\u4e3a\u514b\u9686\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u66f4\u4f18\u8d8a\u7684\u8ddf\u8e2a\u6027\u80fd\u3002", "conclusion": "MATT-Diff\u901a\u8fc7\u6269\u6563\u7b56\u7565\u6210\u529f\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u4e3b\u52a8\u76ee\u6807\u8ddf\u8e2a\uff0c\u5728\u76ee\u6807\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2511.11958", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11958", "abs": "https://arxiv.org/abs/2511.11958", "authors": ["Derek Chen", "Zoe Samuels", "Lizzie Peiros", "Sujaan Mukherjee", "Michael C. Yip"], "title": "Characterization and Evaluation of Screw-Based Locomotion Across Aquatic, Granular, and Transitional Media", "comment": null, "summary": "Screw-based propulsion systems offer promising capabilities for amphibious mobility, yet face significant challenges in optimizing locomotion across water, granular materials, and transitional environments. This study presents a systematic investigation into the locomotion performance of various screw configurations in media such as dry sand, wet sand, saturated sand, and water. Through a principles-first approach to analyze screw performance, it was found that certain parameters are dominant in their impact on performance. Depending on the media, derived parameters inspired from optimizing heat sink design help categorize performance within the dominant design parameters. Our results provide specific insights into screw shell design and adaptive locomotion strategies to enhance the performance of screw-based propulsion systems for versatile amphibious applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u5206\u6790\u4e86\u4e0d\u540c\u87ba\u65cb\u6868\u914d\u7f6e\u5728\u5e72\u6c99\u3001\u6e7f\u6c99\u3001\u9971\u548c\u6c99\u548c\u6c34\u4e2d\u7684\u8fd0\u52a8\u6027\u80fd\uff0c\u53d1\u73b0\u67d0\u4e9b\u53c2\u6570\u5bf9\u6027\u80fd\u6709\u4e3b\u5bfc\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u70ed\u6c89\u8bbe\u8ba1\u4f18\u5316\u7684\u53c2\u6570\u5206\u7c7b\u65b9\u6cd5\u3002", "motivation": "\u87ba\u65cb\u63a8\u8fdb\u7cfb\u7edf\u5728\u5b9e\u73b0\u4e24\u6816\u673a\u52a8\u6027\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u4f18\u5316\u6c34\u3001\u9897\u7c92\u6750\u6599\u548c\u8fc7\u6e21\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u6027\u80fd\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528\u539f\u7406\u4f18\u5148\u7684\u65b9\u6cd5\u5206\u6790\u87ba\u65cb\u6868\u6027\u80fd\uff0c\u4f7f\u7528\u4ece\u70ed\u6c89\u8bbe\u8ba1\u4f18\u5316\u4e2d\u542f\u53d1\u7684\u53c2\u6570\u6765\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63d0\u4f9b\u4e86\u5173\u4e8e\u87ba\u65cb\u58f3\u8bbe\u8ba1\u548c\u81ea\u9002\u5e94\u8fd0\u52a8\u7b56\u7565\u7684\u5177\u4f53\u89c1\u89e3\uff0c\u4ee5\u63d0\u5347\u87ba\u65cb\u63a8\u8fdb\u7cfb\u7edf\u5728\u591a\u6837\u5316\u4e24\u6816\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u87ba\u65cb\u63a8\u8fdb\u7cfb\u7edf\u7684\u4f18\u5316\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u5177\u9002\u5e94\u6027\u7684\u4e24\u6816\u79fb\u52a8\u7cfb\u7edf\u3002"}}
{"id": "2511.11967", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11967", "abs": "https://arxiv.org/abs/2511.11967", "authors": ["Mani Amani", "Behrad Beheshti", "Reza Akhavian"], "title": "Bootstrapped LLM Semantics for Context-Aware Path Planning", "comment": null, "summary": "Prompting robots with natural language (NL) has largely been studied as what task to execute (goal selection, skill sequencing) rather than how to execute that task safely and efficiently in semantically rich, human-centric spaces. We address this gap with a framework that turns a large language model (LLM) into a stochastic semantic sensor whose outputs modulate a classical planner. Given a prompt and a semantic map, we draw multiple LLM \"danger\" judgments and apply a Bayesian bootstrap to approximate a posterior over per-class risk. Using statistics from the posterior, we create a potential cost to formulate a path planning problem. Across simulated environments and a BIM-backed digital twin, our method adapts how the robot moves in response to explicit prompts and implicit contextual information. We present qualitative and quantitative results.", "AI": {"tldr": "\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u8f6c\u5316\u4e3a\u968f\u673a\u8bed\u4e49\u4f20\u611f\u5668\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u5f15\u5bfc\u65b9\u6cd5\u8bc4\u4f30\u73af\u5883\u98ce\u9669\uff0c\u5e76\u57fa\u4e8e\u6b64\u5236\u5b9a\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u5b89\u5168\u9ad8\u6548\u5730\u79fb\u52a8\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u4eba\u6267\u884c\u4ec0\u4e48\u4efb\u52a1\uff0c\u800c\u5ffd\u7565\u4e86\u5982\u4f55\u5728\u8bed\u4e49\u4e30\u5bcc\u7684\u4eba\u7c7b\u4e2d\u5fc3\u7a7a\u95f4\u4e2d\u5b89\u5168\u9ad8\u6548\u5730\u6267\u884c\u4efb\u52a1\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8ba9\u673a\u5668\u4eba\u7406\u89e3\u73af\u5883\u98ce\u9669\u5e76\u76f8\u5e94\u8c03\u6574\u79fb\u52a8\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06LLM\u8f6c\u5316\u4e3a\u968f\u673a\u8bed\u4e49\u4f20\u611f\u5668\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u5f15\u5bfc\u65b9\u6cd5\u4ece\u591a\u4e2aLLM\"\u5371\u9669\"\u5224\u65ad\u4e2d\u8fd1\u4f3c\u5f97\u5230\u6bcf\u7c7b\u98ce\u9669\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u7136\u540e\u57fa\u4e8e\u540e\u9a8c\u7edf\u8ba1\u521b\u5efa\u52bf\u80fd\u6210\u672c\u6765\u5236\u5b9a\u8def\u5f84\u89c4\u5212\u95ee\u9898\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u548cBIM\u652f\u6301\u7684\u6570\u5b57\u5b6a\u751f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u663e\u5f0f\u63d0\u793a\u548c\u9690\u5f0f\u4e0a\u4e0b\u6587\u4fe1\u606f\u81ea\u9002\u5e94\u5730\u8c03\u6574\u673a\u5668\u4eba\u79fb\u52a8\u65b9\u5f0f\uff0c\u63d0\u4f9b\u4e86\u5b9a\u6027\u548c\u5b9a\u91cf\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5730\u5c06LLM\u4e0e\u7ecf\u5178\u89c4\u5212\u5668\u7ed3\u5408\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5728\u8bed\u4e49\u4e30\u5bcc\u7684\u73af\u5883\u4e2d\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8fdb\u884c\u5b89\u5168\u9ad8\u6548\u7684\u8def\u5f84\u89c4\u5212\u3002"}}
{"id": "2511.11970", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11970", "abs": "https://arxiv.org/abs/2511.11970", "authors": ["Sara Wickenhiser", "Lizzie Peiros", "Calvin Joyce", "Peter Gavrilrov", "Sujaan Mukherjee", "Syler Sylvester", "Junrong Zhou", "Mandy Cheung", "Jason Lim", "Florian Richter", "Michael C. Yip"], "title": "ARCSnake V2: An Amphibious Multi-Domain Screw-Propelled Snake-Like Robot", "comment": "8 pages, 9 figures, ICRA", "summary": "Robotic exploration in extreme environments such as caves, oceans, and planetary surfaces pose significant challenges, particularly in locomotion across diverse terrains. Conventional wheeled or legged robots often struggle in these contexts due to surface variability. This paper presents ARCSnake V2, an amphibious, screw propelled, snake like robot designed for teleoperated or autonomous locomotion across land, granular media, and aquatic environments. ARCSnake V2 combines the high mobility of hyper redundant snake robots with the terrain versatility of Archimedean screw propulsion. Key contributions include a water sealed mechanical design with serially linked screw and joint actuation, an integrated buoyancy control system, and teleoperation via a kinematically matched handheld controller. The robots design and control architecture enable multiple locomotion modes screwing, wheeling, and sidewinding with smooth transitions between them. Extensive experiments validate its underwater maneuverability, communication robustness, and force regulated actuation. These capabilities position ARCSnake V2 as a versatile platform for exploration, search and rescue, and environmental monitoring in multi domain settings.", "AI": {"tldr": "ARCSnake V2\u662f\u4e00\u6b3e\u4e24\u6816\u87ba\u65cb\u63a8\u8fdb\u86c7\u5f62\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u4e86\u86c7\u5f62\u673a\u5668\u4eba\u7684\u9ad8\u673a\u52a8\u6027\u548c\u963f\u57fa\u7c73\u5fb7\u87ba\u65cb\u63a8\u8fdb\u7684\u5730\u5f62\u9002\u5e94\u6027\uff0c\u80fd\u5728\u9646\u5730\u3001\u9897\u7c92\u4ecb\u8d28\u548c\u6c34\u57df\u4e2d\u5b9e\u73b0\u8fdc\u7a0b\u6216\u81ea\u4e3b\u79fb\u52a8\u3002", "motivation": "\u4f20\u7edf\u8f6e\u5f0f\u6216\u817f\u5f0f\u673a\u5668\u4eba\u5728\u6d1e\u7a74\u3001\u6d77\u6d0b\u548c\u884c\u661f\u8868\u9762\u7b49\u6781\u7aef\u73af\u5883\u4e2d\u56e0\u5730\u5f62\u591a\u53d8\u800c\u79fb\u52a8\u56f0\u96be\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9002\u5e94\u591a\u53d8\u5730\u5f62\u7684\u673a\u5668\u4eba\u5e73\u53f0\u3002", "method": "\u91c7\u7528\u6c34\u5bc6\u673a\u68b0\u8bbe\u8ba1\uff0c\u7ed3\u5408\u4e32\u8054\u87ba\u65cb\u548c\u5173\u8282\u9a71\u52a8\uff0c\u96c6\u6210\u6d6e\u529b\u63a7\u5236\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fd0\u52a8\u5b66\u5339\u914d\u7684\u624b\u6301\u63a7\u5236\u5668\u8fdb\u884c\u8fdc\u7a0b\u64cd\u4f5c\uff0c\u652f\u6301\u87ba\u65cb\u63a8\u8fdb\u3001\u8f6e\u5f0f\u548c\u4fa7\u5411\u6ed1\u52a8\u7b49\u591a\u79cd\u8fd0\u52a8\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u673a\u5668\u4eba\u7684\u6c34\u4e0b\u673a\u52a8\u6027\u3001\u901a\u4fe1\u9c81\u68d2\u6027\u548c\u529b\u8c03\u8282\u9a71\u52a8\u80fd\u529b\uff0c\u80fd\u591f\u5e73\u6ed1\u5207\u6362\u4e0d\u540c\u8fd0\u52a8\u6a21\u5f0f\u3002", "conclusion": "ARCSnake V2\u4f5c\u4e3a\u4e00\u4e2a\u591a\u529f\u80fd\u5e73\u53f0\uff0c\u9002\u7528\u4e8e\u591a\u57df\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u3001\u641c\u6551\u548c\u73af\u5883\u76d1\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2511.12022", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12022", "abs": "https://arxiv.org/abs/2511.12022", "authors": ["Anh-Quan Pham", "Kabir Ram Puri", "Shreyas Raorane"], "title": "SBAMP: Sampling Based Adaptive Motion Planning", "comment": "8 pages, 13 figures", "summary": "Autonomous robotic systems must navigate complex, dynamic environments in real time, often facing unpredictable obstacles and rapidly changing conditions. Traditional sampling-based methods, such as RRT*, excel at generating collision-free paths but struggle to adapt to sudden changes without extensive replanning. Conversely, learning-based dynamical systems, such as the Stable Estimator of Dynamical Systems (SEDS), offer smooth, adaptive trajectory tracking but typically rely on pre-collected demonstration data, limiting their generalization to novel scenarios. This paper introduces Sampling-Based Adaptive Motion Planning (SBAMP), a novel framework that overcomes these limitations by integrating RRT* for global path planning with a SEDS-based local controller for continuous, adaptive trajectory adjustment. Our approach requires no pre-trained datasets and ensures smooth transitions between planned waypoints, maintaining stability through Lyapunov-based guarantees. We validate SBAMP in both simulated environments and real hardware using the RoboRacer platform, demonstrating superior performance in dynamic obstacle scenarios, rapid recovery from perturbations, and robust handling of sharp turns. Experimental results highlight SBAMP's ability to adapt in real time without sacrificing global path optimality, providing a scalable solution for dynamic, unstructured environments.", "AI": {"tldr": "SBAMP\u7ed3\u5408RRT*\u5168\u5c40\u8def\u5f84\u89c4\u5212\u548cSEDS\u5c40\u90e8\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u65e0\u9700\u9884\u8bad\u7ec3\u6570\u636e\u7684\u81ea\u9002\u5e94\u8fd0\u52a8\u89c4\u5212\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4fdd\u6301\u8def\u5f84\u6700\u4f18\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u91c7\u6837\u65b9\u6cd5\u5982RRT*\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u53d8\u5316\uff0c\u800c\u5b66\u4e60\u578b\u52a8\u6001\u7cfb\u7edf\u5982SEDS\u4f9d\u8d56\u9884\u6536\u96c6\u6570\u636e\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5168\u5c40\u89c4\u5212\u53c8\u80fd\u5b9e\u65f6\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u96c6\u6210RRT*\u8fdb\u884c\u5168\u5c40\u8def\u5f84\u89c4\u5212\uff0c\u4f7f\u7528SEDS\u57fa\u7684\u5c40\u90e8\u63a7\u5236\u5668\u8fdb\u884c\u8fde\u7eed\u81ea\u9002\u5e94\u8f68\u8ff9\u8c03\u6574\uff0c\u901a\u8fc7Lyapunov\u4fdd\u8bc1\u7a33\u5b9a\u6027\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "\u5728\u4eff\u771f\u548cRoboRacer\u5e73\u53f0\u4e0a\u9a8c\u8bc1\uff0cSBAMP\u5728\u52a8\u6001\u969c\u788d\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u5feb\u901f\u4ece\u6270\u52a8\u4e2d\u6062\u590d\uff0c\u7a33\u5065\u5904\u7406\u6025\u8f6c\u5f2f\uff0c\u5b9e\u65f6\u9002\u5e94\u800c\u4e0d\u727a\u7272\u5168\u5c40\u8def\u5f84\u6700\u4f18\u6027\u3002", "conclusion": "SBAMP\u4e3a\u52a8\u6001\u975e\u7ed3\u6784\u5316\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u7ed3\u5408\u4e86\u91c7\u6837\u89c4\u5212\u548c\u5b66\u4e60\u63a7\u5236\u7684\u4f18\u52bf\u3002"}}
{"id": "2511.12101", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12101", "abs": "https://arxiv.org/abs/2511.12101", "authors": ["Jian Zhou", "Sihao Lin", "Shuai Fu", "Qi WU"], "title": "Decoupled Action Head: Confining Task Knowledge to Conditioning Layers", "comment": null, "summary": "Behavior Cloning (BC) is a data-driven supervised learning approach that has gained increasing attention with the success of scaling laws in language and vision domains. Among its implementations in robotic manipulation, Diffusion Policy (DP), with its two variants DP-CNN (DP-C) and DP-Transformer (DP-T), is one of the most effective and widely adopted models, demonstrating the advantages of predicting continuous action sequences. However, both DP and other BC methods remain constrained by the scarcity of paired training data, and the internal mechanisms underlying DP's effectiveness remain insufficiently understood, leading to limited generalization and a lack of principled design in model development. In this work, we propose a decoupled training recipe that leverages nearly cost-free kinematics-generated trajectories as observation-free data to pretrain a general action head (action generator). The pretrained action head is then frozen and adapted to novel tasks through feature modulation. Our experiments demonstrate the feasibility of this approach in both in-distribution and out-of-distribution scenarios. As an additional benefit, decoupling improves training efficiency; for instance, DP-C achieves up to a 41% speedup. Furthermore, the confinement of task-specific knowledge to the conditioning components under decoupling, combined with the near-identical performance of DP-C in both normal and decoupled training, indicates that the action generation backbone plays a limited role in robotic manipulation. Motivated by this observation, we introduce DP-MLP, which replaces the 244M-parameter U-Net backbone of DP-C with only 4M parameters of simple MLP blocks, achieving a 83.9% faster training speed under normal training and 89.1% under decoupling.", "AI": {"tldr": "\u63d0\u51fa\u89e3\u8026\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u4f4e\u6210\u672c\u8fd0\u52a8\u5b66\u751f\u6210\u8f68\u8ff9\u9884\u8bad\u7ec3\u52a8\u4f5c\u5934\uff0c\u7136\u540e\u901a\u8fc7\u7279\u5f81\u8c03\u5236\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u5e76\u51cf\u5c11\u6a21\u578b\u53c2\u6570\u3002", "motivation": "\u73b0\u6709\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u53d7\u9650\u4e8e\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\uff0c\u4e14\u6269\u6563\u7b56\u7565\u7b49\u6a21\u578b\u5185\u90e8\u673a\u5236\u7406\u89e3\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u6709\u9650\u548c\u6a21\u578b\u8bbe\u8ba1\u7f3a\u4e4f\u539f\u5219\u6027\u3002", "method": "\u4f7f\u7528\u8fd0\u52a8\u5b66\u751f\u6210\u8f68\u8ff9\u9884\u8bad\u7ec3\u901a\u7528\u52a8\u4f5c\u5934\uff0c\u51bb\u7ed3\u540e\u901a\u8fc7\u7279\u5f81\u8c03\u5236\u9002\u5e94\u65b0\u4efb\u52a1\uff1b\u63d0\u51faDP-MLP\u7528\u7b80\u5355MLP\u5757\u66ff\u6362\u590d\u6742U-Net\u9aa8\u5e72\u7f51\u7edc\u3002", "result": "\u89e3\u8026\u8bad\u7ec3\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u573a\u666f\u5747\u53ef\u884c\uff0cDP-C\u8bad\u7ec3\u901f\u5ea6\u63d0\u534741%\uff1bDP-MLP\u4ec5\u75284M\u53c2\u6570\u5b9e\u73b083.9%-89.1%\u7684\u8bad\u7ec3\u52a0\u901f\uff0c\u6027\u80fd\u63a5\u8fd1\u539f\u59cb\u6a21\u578b\u3002", "conclusion": "\u52a8\u4f5c\u751f\u6210\u9aa8\u5e72\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u4f5c\u7528\u6709\u9650\uff0c\u89e3\u8026\u8bad\u7ec3\u548c\u8f7b\u91cf\u5316\u8bbe\u8ba1\u80fd\u663e\u8457\u63d0\u5347\u6548\u7387\u800c\u4e0d\u727a\u7272\u6027\u80fd\u3002"}}
{"id": "2511.12148", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12148", "abs": "https://arxiv.org/abs/2511.12148", "authors": ["Advik Sinha", "Akshay Arjun", "Abhijit Das", "Joyjit Mukherjee"], "title": "Towards Obstacle-Avoiding Control of Planar Snake Robots Exploring Neuro-Evolution of Augmenting Topologies", "comment": "9 pages, 6 figures", "summary": "This work aims to develop a resource-efficient solution for obstacle-avoiding tracking control of a planar snake robot in a densely cluttered environment with obstacles. Particularly, Neuro-Evolution of Augmenting Topologies (NEAT) has been employed to generate dynamic gait parameters for the serpenoid gait function, which is implemented on the joint angles of the snake robot, thus controlling the robot on a desired dynamic path. NEAT is a single neural-network based evolutionary algorithm that is known to work extremely well when the input layer is of significantly higher dimension and the output layer is of a smaller size. For the planar snake robot, the input layer consists of the joint angles, link positions, head link position as well as obstacle positions in the vicinity. However, the output layer consists of only the frequency and offset angle of the serpenoid gait that control the speed and heading of the robot, respectively. Obstacle data from a LiDAR and the robot data from various sensors, along with the location of the end goal and time, are employed to parametrize a reward function that is maximized over iterations by selective propagation of superior neural networks. The implementation and experimental results showcase that the proposed approach is computationally efficient, especially for large environments with many obstacles. The proposed framework has been verified through a physics engine simulation study on PyBullet. The approach shows superior results to existing state-of-the-art methodologies and comparable results to the very recent CBRL approach with significantly lower computational overhead. The video of the simulation can be found here: https://sites.google.com/view/neatsnakerobot", "AI": {"tldr": "\u4f7f\u7528NEAT\u7b97\u6cd5\u4e3a\u86c7\u5f62\u673a\u5668\u4eba\u5f00\u53d1\u4e86\u8d44\u6e90\u9ad8\u6548\u7684\u907f\u969c\u8ddf\u8e2a\u63a7\u5236\u65b9\u6848\uff0c\u901a\u8fc7\u8fdb\u5316\u795e\u7ecf\u7f51\u7edc\u52a8\u6001\u751f\u6210\u6b65\u6001\u53c2\u6570\uff0c\u5728\u5bc6\u96c6\u969c\u788d\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u5bfc\u822a\u3002", "motivation": "\u5728\u5bc6\u96c6\u969c\u788d\u73af\u5883\u4e2d\u4e3a\u86c7\u5f62\u673a\u5668\u4eba\u5f00\u53d1\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u907f\u969c\u8ddf\u8e2a\u63a7\u5236\u65b9\u6848\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528NEAT\uff08\u795e\u7ecf\u8fdb\u5316\u589e\u5f3a\u62d3\u6251\uff09\u7b97\u6cd5\uff0c\u4ee5\u673a\u5668\u4eba\u5173\u8282\u89d2\u5ea6\u3001\u4f4d\u7f6e\u3001\u969c\u788d\u7269\u4f4d\u7f6e\u7b49\u4e3a\u8f93\u5165\uff0c\u8f93\u51fa\u63a7\u5236\u86c7\u5f62\u673a\u5668\u4eba\u901f\u5ea6\u548c\u822a\u5411\u7684\u6b65\u6001\u53c2\u6570\uff0c\u901a\u8fc7\u5956\u52b1\u51fd\u6570\u6700\u5927\u5316\u5b9e\u73b0\u8fdb\u5316\u5b66\u4e60\u3002", "result": "\u8be5\u65b9\u6cd5\u5728PyBullet\u7269\u7406\u5f15\u64ce\u4eff\u771f\u4e2d\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e0e\u6700\u65b0CBRL\u65b9\u6cd5\u7ed3\u679c\u76f8\u5f53\u4f46\u8ba1\u7b97\u5f00\u9500\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "NEAT\u65b9\u6cd5\u4e3a\u86c7\u5f62\u673a\u5668\u4eba\u5728\u5bc6\u96c6\u969c\u788d\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u63d0\u4f9b\u4e86\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5927\u578b\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u63a7\u5236\u3002"}}
{"id": "2511.12160", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12160", "abs": "https://arxiv.org/abs/2511.12160", "authors": ["Wenbin Mai", "Minghui Liwang", "Xinlei Yi", "Xiaoyu Xia", "Seyyedali Hosseinalipour", "Xianbin Wang"], "title": "Game-Theoretic Safe Multi-Agent Motion Planning with Reachability Analysis for Dynamic and Uncertain Environments (Extended Version)", "comment": "12 pages, 9 figures", "summary": "Ensuring safe, robust, and scalable motion planning for multi-agent systems in dynamic and uncertain environments is a persistent challenge, driven by complex inter-agent interactions, stochastic disturbances, and model uncertainties. To overcome these challenges, particularly the computational complexity of coupled decision-making and the need for proactive safety guarantees, we propose a Reachability-Enhanced Dynamic Potential Game (RE-DPG) framework, which integrates game-theoretic coordination into reachability analysis. This approach formulates multi-agent coordination as a dynamic potential game, where the Nash equilibrium (NE) defines optimal control strategies across agents. To enable scalability and decentralized execution, we develop a Neighborhood-Dominated iterative Best Response (ND-iBR) scheme, built upon an iterated $\\varepsilon$-BR (i$\\varepsilon$-BR) process that guarantees finite-step convergence to an $\\varepsilon$-NE. This allows agents to compute strategies based on local interactions while ensuring theoretical convergence guarantees. Furthermore, to ensure safety under uncertainty, we integrate a Multi-Agent Forward Reachable Set (MA-FRS) mechanism into the cost function, explicitly modeling uncertainty propagation and enforcing collision avoidance constraints. Through both simulations and real-world experiments in 2D and 3D environments, we validate the effectiveness of RE-DPG across diverse operational scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u53ef\u8fbe\u6027\u5206\u6790\u548c\u535a\u5f08\u8bba\u7684\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u89c4\u5212\u6846\u67b6RE-DPG\uff0c\u901a\u8fc7\u52a8\u6001\u52bf\u535a\u5f08\u548c\u90bb\u57df\u4e3b\u5bfc\u8fed\u4ee3\u6700\u4f73\u54cd\u5e94\u7b97\u6cd5\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u5206\u6563\u5f0f\u51b3\u7b56\uff0c\u5e76\u96c6\u6210\u591a\u667a\u80fd\u4f53\u524d\u5411\u53ef\u8fbe\u96c6\u786e\u4fdd\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u52a8\u6001\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u5b89\u5168\u3001\u9c81\u68d2\u548c\u53ef\u6269\u5c55\u8fd0\u52a8\u89c4\u5212\u7684\u6311\u6218\uff0c\u5305\u62ec\u590d\u6742\u667a\u80fd\u4f53\u95f4\u4ea4\u4e92\u3001\u968f\u673a\u6270\u52a8\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u5e26\u6765\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5b89\u5168\u6027\u95ee\u9898\u3002", "method": "\u5c06\u591a\u667a\u80fd\u4f53\u534f\u8c03\u5efa\u6a21\u4e3a\u52a8\u6001\u52bf\u535a\u5f08\uff0c\u7eb3\u4ec0\u5747\u8861\u5b9a\u4e49\u6700\u4f18\u63a7\u5236\u7b56\u7565\uff1b\u5f00\u53d1\u90bb\u57df\u4e3b\u5bfc\u8fed\u4ee3\u6700\u4f73\u54cd\u5e94(ND-iBR)\u65b9\u6848\u5b9e\u73b0\u5206\u6563\u6267\u884c\uff1b\u96c6\u6210\u591a\u667a\u80fd\u4f53\u524d\u5411\u53ef\u8fbe\u96c6(MA-FRS)\u673a\u5236\u5230\u4ee3\u4ef7\u51fd\u6570\u4e2d\uff0c\u663e\u5f0f\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u5e76\u5f3a\u5236\u6267\u884c\u78b0\u649e\u907f\u514d\u7ea6\u675f\u3002", "result": "\u901a\u8fc72D\u548c3D\u73af\u5883\u7684\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86RE-DPG\u5728\u5404\u79cd\u64cd\u4f5c\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u80fd\u591f\u5b9e\u73b0\u5b89\u5168\u3001\u53ef\u6269\u5c55\u7684\u591a\u667a\u80fd\u4f53\u534f\u8c03\u3002", "conclusion": "RE-DPG\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5b89\u5168\u6027\u6311\u6218\uff0c\u901a\u8fc7\u535a\u5f08\u8bba\u4e0e\u53ef\u8fbe\u6027\u5206\u6790\u7684\u7ed3\u5408\uff0c\u4e3a\u52a8\u6001\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12184", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12184", "abs": "https://arxiv.org/abs/2511.12184", "authors": ["Jun Huo", "Kehan Xu", "Chengyao Li", "Yu Cao", "Jie Zuo", "Xinxing Chen", "Jian Huang"], "title": "Variable Impedance Control for Floating-Base Supernumerary Robotic Leg in Walking Assistance", "comment": null, "summary": "In human-robot systems, ensuring safety during force control in the presence of both internal and external disturbances is crucial. As a typical loosely coupled floating-base robot system, the supernumerary robotic leg (SRL) system is particularly susceptible to strong internal disturbances. To address the challenge posed by floating base, we investigated the dynamics model of the loosely coupled SRL and designed a hybrid position/force impedance controller to fit dynamic torque input. An efficient variable impedance control (VIC) method is developed to enhance human-robot interaction, particularly in scenarios involving external force disturbances. By dynamically adjusting impedance parameters, VIC improves the dynamic switching between rigidity and flexibility, so that it can adapt to unknown environmental disturbances in different states. An efficient real-time stability guaranteed impedance parameters generating network is specifically designed for the proposed SRL, to achieve shock mitigation and high rigidity supporting. Simulations and experiments validate the system's effectiveness, demonstrating its ability to maintain smooth signal transitions in flexible states while providing strong support forces in rigid states. This approach provides a practical solution for accommodating individual gait variations in interaction, and significantly advances the safety and adaptability of human-robot systems.", "AI": {"tldr": "\u4e3a\u5e94\u5bf9\u6d6e\u52a8\u57fa\u5ea7\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5185\u90e8\u548c\u5916\u90e8\u5e72\u6270\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u4f4d\u7f6e/\u529b\u963b\u6297\u63a7\u5236\u5668\u548c\u53ef\u53d8\u963b\u6297\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u963b\u6297\u53c2\u6570\u5b9e\u73b0\u521a\u6027\u548c\u67d4\u6027\u72b6\u6001\u7684\u5e73\u6ed1\u5207\u6362\uff0c\u63d0\u9ad8\u4eba\u673a\u4ea4\u4e92\u5b89\u5168\u6027\u3002", "motivation": "\u5728\u6d6e\u52a8\u57fa\u5ea7\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\uff0c\u786e\u4fdd\u529b\u63a7\u5236\u7684\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8d85\u9650\u673a\u5668\u4eba\u817f\u7cfb\u7edf\uff0c\u5176\u5bb9\u6613\u53d7\u5230\u5f3a\u5185\u90e8\u5e72\u6270\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u4e86\u677e\u6563\u8026\u5408SRL\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e86\u6df7\u5408\u4f4d\u7f6e/\u529b\u963b\u6297\u63a7\u5236\u5668\uff0c\u5e76\u5f00\u53d1\u4e86\u53ef\u53d8\u963b\u6297\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u65f6\u7a33\u5b9a\u6027\u4fdd\u8bc1\u7684\u963b\u6297\u53c2\u6570\u751f\u6210\u7f51\u7edc\u52a8\u6001\u8c03\u6574\u963b\u6297\u53c2\u6570\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5728\u67d4\u6027\u72b6\u6001\u4e0b\u4fdd\u6301\u5e73\u6ed1\u4fe1\u53f7\u8fc7\u6e21\uff0c\u5728\u521a\u6027\u72b6\u6001\u4e0b\u63d0\u4f9b\u5f3a\u652f\u6491\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u4e2a\u4f53\u6b65\u6001\u53d8\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4eba\u673a\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2511.12186", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12186", "abs": "https://arxiv.org/abs/2511.12186", "authors": ["Jun Huo", "Jian Huang", "Jie Zuo", "Bo Yang", "Zhongzheng Fu", "Xi Li", "Samer Mohammed"], "title": "Innovative Design of Multi-functional Supernumerary Robotic Limbs with Ellipsoid Workspace Optimization", "comment": null, "summary": "Supernumerary robotic limbs (SRLs) offer substantial potential in both the rehabilitation of hemiplegic patients and the enhancement of functional capabilities for healthy individuals. Designing a general-purpose SRL device is inherently challenging, particularly when developing a unified theoretical framework that meets the diverse functional requirements of both upper and lower limbs. In this paper, we propose a multi-objective optimization (MOO) design theory that integrates grasping workspace similarity, walking workspace similarity, braced force for sit-to-stand (STS) movements, and overall mass and inertia. A geometric vector quantification method is developed using an ellipsoid to represent the workspace, aiming to reduce computational complexity and address quantification challenges. The ellipsoid envelope transforms workspace points into ellipsoid attributes, providing a parametric description of the workspace. Furthermore, the STS static braced force assesses the effectiveness of force transmission. The overall mass and inertia restricts excessive link length. To facilitate rapid and stable convergence of the model to high-dimensional irregular Pareto fronts, we introduce a multi-subpopulation correction firefly algorithm. This algorithm incorporates a strategy involving attractive and repulsive domains to effectively handle the MOO task. The optimized solution is utilized to redesign the prototype for experimentation to meet specified requirements. Six healthy participants and two hemiplegia patients participated in real experiments. Compared to the pre-optimization results, the average grasp success rate improved by 7.2%, while the muscle activity during walking and STS tasks decreased by an average of 12.7% and 25.1%, respectively. The proposed design theory offers an efficient option for the design of multi-functional SRL mechanisms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8d85\u6570\u673a\u5668\u4eba\u80a2\u4f53(SRL)\u7684\u591a\u76ee\u6807\u4f18\u5316\u8bbe\u8ba1\u7406\u8bba\uff0c\u901a\u8fc7\u51e0\u4f55\u5411\u91cf\u91cf\u5316\u65b9\u6cd5\u548c\u6539\u8fdb\u7684\u8424\u706b\u866b\u7b97\u6cd5\u4f18\u5316\u6293\u53d6/\u884c\u8d70\u5de5\u4f5c\u7a7a\u95f4\u3001\u5750\u7acb\u652f\u6491\u529b\u548c\u8d28\u91cf\u60ef\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u6293\u53d6\u6210\u529f\u7387\u63d0\u9ad87.2%\uff0c\u808c\u8089\u6d3b\u52a8\u5ea6\u964d\u4f4e12.7%-25.1%\u3002", "motivation": "\u8bbe\u8ba1\u901a\u7528SRL\u8bbe\u5907\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u6ee1\u8db3\u4e0a\u4e0b\u80a2\u7684\u591a\u6837\u5316\u529f\u80fd\u9700\u6c42\uff0c\u5305\u62ec\u5eb7\u590d\u6cbb\u7597\u548c\u529f\u80fd\u589e\u5f3a\u3002", "method": "\u5f00\u53d1\u591a\u76ee\u6807\u4f18\u5316\u8bbe\u8ba1\u7406\u8bba\uff0c\u96c6\u6210\u5de5\u4f5c\u7a7a\u95f4\u76f8\u4f3c\u6027\u3001\u5750\u7acb\u652f\u6491\u529b\u3001\u8d28\u91cf\u60ef\u6027\u7b49\u6307\u6807\uff1b\u4f7f\u7528\u692d\u7403\u4f53\u8868\u793a\u5de5\u4f5c\u7a7a\u95f4\u4ee5\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b\u5f15\u5165\u591a\u5b50\u7fa4\u4fee\u6b63\u8424\u706b\u866b\u7b97\u6cd5\u5904\u7406\u9ad8\u7ef4\u4e0d\u89c4\u5219\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "result": "\u4f18\u5316\u540e\u539f\u578b\u5b9e\u9a8c\u663e\u793a\uff1a\u6293\u53d6\u6210\u529f\u7387\u5e73\u5747\u63d0\u9ad87.2%\uff0c\u884c\u8d70\u548c\u5750\u7acb\u4efb\u52a1\u4e2d\u7684\u808c\u8089\u6d3b\u52a8\u5ea6\u5206\u522b\u964d\u4f4e12.7%\u548c25.1%\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u7406\u8bba\u4e3a\u591a\u529f\u80fdSRL\u673a\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u9009\u62e9\uff0c\u80fd\u591f\u540c\u65f6\u6ee1\u8db3\u5eb7\u590d\u548c\u529f\u80fd\u589e\u5f3a\u7684\u9700\u6c42\u3002"}}
{"id": "2511.12203", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12203", "abs": "https://arxiv.org/abs/2511.12203", "authors": ["Antony Thomas", "Fulvio Mastrogiovanni", "Marco Baglietto"], "title": "Locally Optimal Solutions to Constraint Displacement Problems via Path-Obstacle Overlaps", "comment": "Robotics and Autonomous Systems", "summary": "We present a unified approach for constraint displacement problems in which a robot finds a feasible path by displacing constraints or obstacles. To this end, we propose a two stage process that returns locally optimal obstacle displacements to enable a feasible path for the robot. The first stage proceeds by computing a trajectory through the obstacles while minimizing an appropriate objective function. In the second stage, these obstacles are displaced to make the computed robot trajectory feasible, that is, collision-free. Several examples are provided that successfully demonstrate our approach on two distinct classes of constraint displacement problems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u7ea6\u675f\u4f4d\u79fb\u95ee\u9898\u7684\u7edf\u4e00\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc7\u7a0b\u8ba1\u7b97\u5c40\u90e8\u6700\u4f18\u7684\u969c\u788d\u7269\u4f4d\u79fb\uff0c\u4e3a\u673a\u5668\u4eba\u627e\u5230\u53ef\u884c\u8def\u5f84\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u6709\u969c\u788d\u7269\u73af\u5883\u4e2d\u5bfb\u627e\u53ef\u884c\u8def\u5f84\u65f6\uff0c\u901a\u8fc7\u4f4d\u79fb\u7ea6\u675f\u6216\u969c\u788d\u7269\u6765\u521b\u9020\u53ef\u884c\u8def\u5f84\u7684\u95ee\u9898\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u8ba1\u7b97\u901a\u8fc7\u969c\u788d\u7269\u7684\u8f68\u8ff9\u5e76\u6700\u5c0f\u5316\u76ee\u6807\u51fd\u6570\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f4d\u79fb\u969c\u788d\u7269\u4f7f\u673a\u5668\u4eba\u8f68\u8ff9\u53ef\u884c\u4e14\u65e0\u78b0\u649e\u3002", "result": "\u6210\u529f\u6f14\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u4e24\u7c7b\u7ea6\u675f\u4f4d\u79fb\u95ee\u9898\u4e0a\u7684\u5e94\u7528\uff0c\u63d0\u4f9b\u4e86\u591a\u4e2a\u793a\u4f8b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7ea6\u675f\u4f4d\u79fb\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u627e\u5230\u53ef\u884c\u8def\u5f84\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.12232", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12232", "abs": "https://arxiv.org/abs/2511.12232", "authors": ["Lingfeng Zhang", "Erjia Xiao", "Xiaoshuai Hao", "Haoxiang Fu", "Zeying Gong", "Long Chen", "Xiaojun Liang", "Renjing Xu", "Hangjun Ye", "Wenbo Ding"], "title": "SocialNav-Map: Dynamic Mapping with Human Trajectory Prediction for Zero-Shot Social Navigation", "comment": null, "summary": "Social navigation in densely populated dynamic environments poses a significant challenge for autonomous mobile robots, requiring advanced strategies for safe interaction. Existing reinforcement learning (RL)-based methods require over 2000+ hours of extensive training and often struggle to generalize to unfamiliar environments without additional fine-tuning, limiting their practical application in real-world scenarios. To address these limitations, we propose SocialNav-Map, a novel zero-shot social navigation framework that combines dynamic human trajectory prediction with occupancy mapping, enabling safe and efficient navigation without the need for environment-specific training. Specifically, SocialNav-Map first transforms the task goal position into the constructed map coordinate system. Subsequently, it creates a dynamic occupancy map that incorporates predicted human movements as dynamic obstacles. The framework employs two complementary methods for human trajectory prediction: history prediction and orientation prediction. By integrating these predicted trajectories into the occupancy map, the robot can proactively avoid potential collisions with humans while efficiently navigating to its destination. Extensive experiments on the Social-HM3D and Social-MP3D datasets demonstrate that SocialNav-Map significantly outperforms state-of-the-art (SOTA) RL-based methods, which require 2,396 GPU hours of training. Notably, it reduces human collision rates by over 10% without necessitating any training in novel environments. By eliminating the need for environment-specific training, SocialNav-Map achieves superior navigation performance, paving the way for the deployment of social navigation systems in real-world environments characterized by diverse human behaviors. The code is available at: https://github.com/linglingxiansen/SocialNav-Map.", "AI": {"tldr": "SocialNav-Map\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u793e\u4ea4\u5bfc\u822a\u6846\u67b6\uff0c\u7ed3\u5408\u52a8\u6001\u4eba\u7c7b\u8f68\u8ff9\u9884\u6d4b\u548c\u5360\u636e\u5730\u56fe\uff0c\u65e0\u9700\u73af\u5883\u7279\u5b9a\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u5bfc\u822a\uff0c\u663e\u8457\u4f18\u4e8e\u9700\u89812396 GPU\u5c0f\u65f6\u8bad\u7ec3\u7684\u73b0\u6709RL\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u793e\u4ea4\u5bfc\u822a\u65b9\u6cd5\u9700\u89812000+\u5c0f\u65f6\u8bad\u7ec3\u4e14\u96be\u4ee5\u6cdb\u5316\u5230\u964c\u751f\u73af\u5883\u7684\u95ee\u9898\uff0c\u63a8\u52a8\u793e\u4ea4\u5bfc\u822a\u7cfb\u7edf\u5728\u771f\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u3002", "method": "\u5c06\u4efb\u52a1\u76ee\u6807\u4f4d\u7f6e\u8f6c\u6362\u5230\u5730\u56fe\u5750\u6807\u7cfb\uff0c\u521b\u5efa\u5305\u542b\u9884\u6d4b\u4eba\u7c7b\u8fd0\u52a8\u7684\u52a8\u6001\u5360\u636e\u5730\u56fe\uff0c\u4f7f\u7528\u5386\u53f2\u9884\u6d4b\u548c\u65b9\u5411\u9884\u6d4b\u4e24\u79cd\u4e92\u8865\u65b9\u6cd5\u8fdb\u884c\u4eba\u7c7b\u8f68\u8ff9\u9884\u6d4b\u3002", "result": "\u5728Social-HM3D\u548cSocial-MP3D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSocialNav-Map\u663e\u8457\u4f18\u4e8eSOTA RL\u65b9\u6cd5\uff0c\u4eba\u7c7b\u78b0\u649e\u7387\u964d\u4f4e\u8d85\u8fc710%\uff0c\u4e14\u65e0\u9700\u5728\u65b0\u73af\u5883\u4e2d\u8bad\u7ec3\u3002", "conclusion": "\u901a\u8fc7\u6d88\u9664\u73af\u5883\u7279\u5b9a\u8bad\u7ec3\u9700\u6c42\uff0cSocialNav-Map\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u5bfc\u822a\u6027\u80fd\uff0c\u4e3a\u793e\u4ea4\u5bfc\u822a\u7cfb\u7edf\u5728\u771f\u5b9e\u591a\u6837\u5316\u4eba\u7c7b\u884c\u4e3a\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.12237", "categories": ["cs.RO", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12237", "abs": "https://arxiv.org/abs/2511.12237", "authors": ["Alysson Ribeiro da Silva", "Luiz Chaimowicz"], "title": "Intermittent Rendezvous Plans with Mixed Integer Linear Program for Large-Scale Multi-Robot Exploration", "comment": "9 pages, 9 figures, International Conference on Advanced Robotics", "summary": "Multi-Robot Exploration (MRE) systems with communication constraints have proven efficient in accomplishing a variety of tasks, including search-and-rescue, stealth, and military operations. While some works focus on opportunistic approaches for efficiency, others concentrate on pre-planned trajectories or scheduling for increased interpretability. However, scheduling usually requires knowledge of the environment beforehand, which prevents its deployment in several domains due to related uncertainties (e.g., underwater exploration). In our previous work, we proposed an intermittent communications framework for MRE under communication constraints that uses scheduled rendezvous events to mitigate such limitations. However, the system was unable to generate optimal plans and had no mechanisms to follow the plan considering realistic trajectories, which is not suited for real-world deployments. In this work, we further investigate the problem by formulating the Multi-Robot Exploration with Communication Constraints and Intermittent Connectivity (MRE-CCIC) problem. We propose a Mixed-Integer Linear Program (MILP) formulation to generate rendezvous plans and a policy to follow them based on the Rendezvous Tracking for Unknown Scenarios (RTUS) mechanism. The RTUS is a simple rule to allow robots to follow the assigned plan, considering unknown conditions. Finally, we evaluated our method in a large-scale environment configured in Gazebo simulations. The results suggest that our method can follow the plan promptly and accomplish the task efficiently. We provide an open-source implementation of both the MILP plan generator and the large-scale MRE-CCIC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u673a\u5668\u4eba\u63a2\u7d22\u901a\u4fe1\u7ea6\u675f\u548c\u95f4\u6b47\u8fde\u63a5\u95ee\u9898(MRE-CCIC)\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ecMILP\u89c4\u5212\u751f\u6210\u5668\u548cRTUS\u8ddf\u8e2a\u673a\u5236\uff0c\u5728Gazebo\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u901a\u4fe1\u53d7\u9650\u73af\u5883\u4e0b\u7684\u63a2\u7d22\u95ee\u9898\uff0c\u7279\u522b\u662f\u5f53\u73af\u5883\u672a\u77e5\u65f6\u65e0\u6cd5\u9884\u5148\u89c4\u5212\u7684\u95ee\u9898\uff0c\u6539\u8fdb\u4e4b\u524d\u95f4\u6b47\u901a\u4fe1\u6846\u67b6\u7684\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212(MILP)\u751f\u6210\u4f1a\u5408\u8ba1\u5212\uff0c\u5e76\u57fa\u4e8eRTUS(\u672a\u77e5\u573a\u666f\u4f1a\u5408\u8ddf\u8e2a)\u673a\u5236\u8ba9\u673a\u5668\u4eba\u9075\u5faa\u8ba1\u5212\uff0c\u8003\u8651\u672a\u77e5\u73af\u5883\u6761\u4ef6\u3002", "result": "\u5728Gazebo\u4eff\u771f\u7684\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u80fd\u591f\u53ca\u65f6\u9075\u5faa\u8ba1\u5212\u5e76\u9ad8\u6548\u5b8c\u6210\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u7684MILP\u89c4\u5212\u751f\u6210\u5668\u548cRTUS\u673a\u5236\u80fd\u591f\u6709\u6548\u89e3\u51b3\u901a\u4fe1\u7ea6\u675f\u4e0b\u7684\u591a\u673a\u5668\u4eba\u63a2\u7d22\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u3002"}}
{"id": "2511.12361", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12361", "abs": "https://arxiv.org/abs/2511.12361", "authors": ["Leroy D'Souza", "Akash Karthikeyan", "Yash Vardhan Pant", "Sebastian Fischmeister"], "title": "SAC-MoE: Reinforcement Learning with Mixture-of-Experts for Control of Hybrid Dynamical Systems with Uncertainty", "comment": null, "summary": "Hybrid dynamical systems result from the interaction of continuous-variable dynamics with discrete events and encompass various systems such as legged robots, vehicles and aircrafts. Challenges arise when the system's modes are characterized by unobservable (latent) parameters and the events that cause system dynamics to switch between different modes are also unobservable. Model-based control approaches typically do not account for such uncertainty in the hybrid dynamics, while standard model-free RL methods fail to account for abrupt mode switches, leading to poor generalization.\n  To overcome this, we propose SAC-MoE which models the actor of the Soft Actor-Critic (SAC) framework as a Mixture-of-Experts (MoE) with a learned router that adaptively selects among learned experts. To further improve robustness, we develop a curriculum-based training algorithm to prioritize data collection in challenging settings, allowing better generalization to unseen modes and switching locations. Simulation studies in hybrid autonomous racing and legged locomotion tasks show that SAC-MoE outperforms baselines (up to 6x) in zero-shot generalization to unseen environments. Our curriculum strategy consistently improves performance across all evaluated policies. Qualitative analysis shows that the interpretable MoE router activates different experts for distinct latent modes.", "AI": {"tldr": "SAC-MoE\u5c06Soft Actor-Critic\u6846\u67b6\u4e2d\u7684actor\u5efa\u6a21\u4e3a\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff0c\u901a\u8fc7\u5b66\u4e60\u7684\u8def\u7531\u5668\u81ea\u9002\u5e94\u9009\u62e9\u4e13\u5bb6\uff0c\u5e76\u91c7\u7528\u8bfe\u7a0b\u8bad\u7ec3\u7b97\u6cd5\u63d0\u9ad8\u5728\u6df7\u5408\u52a8\u529b\u7cfb\u7edf\u4e2d\u5bf9\u4e0d\u53ef\u89c2\u6d4b\u6a21\u5f0f\u548c\u5207\u6362\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6df7\u5408\u52a8\u529b\u7cfb\u7edf\u4e2d\u5b58\u5728\u4e0d\u53ef\u89c2\u6d4b\u7684\u6f5c\u5728\u53c2\u6570\u548c\u6a21\u5f0f\u5207\u6362\u4e8b\u4ef6\uff0c\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u6807\u51c6\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u7a81\u7136\u7684\u6a21\u5f0f\u5207\u6362\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51faSAC-MoE\u65b9\u6cd5\uff0c\u5c06SAC\u6846\u67b6\u4e2d\u7684actor\u5efa\u6a21\u4e3a\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff0c\u5305\u542b\u5b66\u4e60\u5230\u7684\u8def\u7531\u5668\u81ea\u9002\u5e94\u9009\u62e9\u4e13\u5bb6\uff1b\u5f00\u53d1\u57fa\u4e8e\u8bfe\u7a0b\u7684\u8bad\u7ec3\u7b97\u6cd5\uff0c\u4f18\u5148\u5728\u6311\u6218\u6027\u8bbe\u7f6e\u4e2d\u6536\u96c6\u6570\u636e\u3002", "result": "\u5728\u6df7\u5408\u81ea\u4e3b\u8d5b\u8f66\u548c\u817f\u5f0f\u8fd0\u52a8\u4efb\u52a1\u7684\u4eff\u771f\u7814\u7a76\u4e2d\uff0cSAC-MoE\u5728\u96f6\u6837\u672c\u6cdb\u5316\u5230\u672a\u89c1\u73af\u5883\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u6700\u591a6\u500d\uff09\uff1b\u8bfe\u7a0b\u7b56\u7565\u5728\u6240\u6709\u8bc4\u4f30\u7b56\u7565\u4e2d\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "SAC-MoE\u80fd\u591f\u6709\u6548\u5904\u7406\u6df7\u5408\u52a8\u529b\u7cfb\u7edf\u4e2d\u7684\u6f5c\u5728\u6a21\u5f0f\u548c\u5207\u6362\u4e0d\u786e\u5b9a\u6027\uff0cMoE\u8def\u7531\u5668\u80fd\u591f\u4e3a\u4e0d\u540c\u6f5c\u5728\u6a21\u5f0f\u6fc0\u6d3b\u4e0d\u540c\u4e13\u5bb6\uff0c\u5177\u6709\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2511.12380", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12380", "abs": "https://arxiv.org/abs/2511.12380", "authors": ["Nicholas Gunter", "Heiko Kabutz", "Kaushik Jayaram"], "title": "Multilaminate piezoelectric PVDF actuators to enhance performance of soft micro robots", "comment": null, "summary": "Multilayer piezoelectric polyvinylidene fluoride (PVDF) actuators are a promising approach to enhance performance of soft microrobotic systems. In this work, we develop and characterize multilayer PVDF actuators with parallel voltage distribution across each layer, bridging a unique design space between brittle high-force PZT stacks and compliant but lower-bandwidth soft polymer actuators. We show the effects of layer thickness and number of layers in actuator performance and their agreement with a first principles model. By varying these parameters, we demonstrate actuators capable of >3 mm of free deflection, >20 mN of blocked force, and >=500 Hz, while operating at voltages as low as 150 volts. To illustrate their potential for robotic integration, we integrate our actuators into a planar, translating microrobot that leverages resonance to achieve locomotion with robustness to large perturbations.", "AI": {"tldr": "\u5f00\u53d1\u5e76\u8868\u5f81\u4e86\u591a\u5c42PVDF\u538b\u7535\u81f4\u52a8\u5668\uff0c\u5728\u8106\u6027\u9ad8\u529bPZT\u5806\u6808\u548c\u67d4\u987a\u4f46\u4f4e\u5e26\u5bbd\u8f6f\u805a\u5408\u7269\u81f4\u52a8\u5668\u4e4b\u95f4\u586b\u8865\u4e86\u72ec\u7279\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5c55\u793a\u4e86\u5728\u4f4e\u7535\u538b\u4e0b\u5b9e\u73b0\u5927\u53d8\u5f62\u3001\u9ad8\u529b\u548c\u9ad8\u9891\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u5c42PVDF\u538b\u7535\u81f4\u52a8\u5668\u662f\u63d0\u5347\u8f6f\u5fae\u673a\u5668\u4eba\u7cfb\u7edf\u6027\u80fd\u7684\u6709\u524d\u666f\u65b9\u6cd5\uff0c\u65e8\u5728\u586b\u8865\u8106\u6027\u9ad8\u529bPZT\u5806\u6808\u548c\u67d4\u987a\u4f46\u4f4e\u5e26\u5bbd\u8f6f\u805a\u5408\u7269\u81f4\u52a8\u5668\u4e4b\u95f4\u7684\u8bbe\u8ba1\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u591a\u5c42PVDF\u81f4\u52a8\u5668\uff0c\u91c7\u7528\u5e76\u884c\u7535\u538b\u5206\u5e03\u8bbe\u8ba1\uff0c\u7814\u7a76\u5c42\u539a\u5ea6\u548c\u5c42\u6570\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u4e0e\u57fa\u672c\u539f\u7406\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u9a8c\u8bc1\u3002", "result": "\u81f4\u52a8\u5668\u5728150\u4f0f\u4f4e\u7535\u538b\u4e0b\u53ef\u5b9e\u73b0>3\u6beb\u7c73\u81ea\u7531\u53d8\u5f62\u3001>20\u6beb\u725b\u963b\u585e\u529b\u548c\u2265500\u8d6b\u5179\u9891\u7387\uff0c\u5e76\u96c6\u6210\u5230\u5e73\u9762\u79fb\u52a8\u5fae\u673a\u5668\u4eba\u4e2d\u5229\u7528\u5171\u632f\u5b9e\u73b0\u9c81\u68d2\u8fd0\u52a8\u3002", "conclusion": "\u591a\u5c42PVDF\u81f4\u52a8\u5668\u5728\u4f4e\u7535\u538b\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\uff0c\u6210\u529f\u96c6\u6210\u5230\u5fae\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\uff0c\u5c55\u793a\u4e86\u5728\u8f6f\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.12383", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12383", "abs": "https://arxiv.org/abs/2511.12383", "authors": ["Sanjar Atamuradov"], "title": "Evaluating Model-Agnostic Meta-Learning on MetaWorld ML10 Benchmark: Fast Adaptation in Robotic Manipulation Tasks", "comment": "7 pages, 5 figures", "summary": "Meta-learning algorithms enable rapid adaptation to new tasks with minimal data, a critical capability for real-world robotic systems. This paper evaluates Model-Agnostic Meta-Learning (MAML) combined with Trust Region Policy Optimization (TRPO) on the MetaWorld ML10 benchmark, a challenging suite of ten diverse robotic manipulation tasks. We implement and analyze MAML-TRPO's ability to learn a universal initialization that facilitates few-shot adaptation across semantically different manipulation behaviors including pushing, picking, and drawer manipulation. Our experiments demonstrate that MAML achieves effective one-shot adaptation with clear performance improvements after a single gradient update, reaching final success rates of 21.0% on training tasks and 13.2% on held-out test tasks. However, we observe a generalization gap that emerges during meta-training, where performance on test tasks plateaus while training task performance continues to improve. Task-level analysis reveals high variance in adaptation effectiveness, with success rates ranging from 0% to 80% across different manipulation skills. These findings highlight both the promise and current limitations of gradient-based meta-learning for diverse robotic manipulation, and suggest directions for future work in task-aware adaptation and structured policy architectures.", "AI": {"tldr": "\u8bc4\u4f30MAML-TRPO\u5728MetaWorld ML10\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u5c55\u793a\u5176\u5728\u591a\u6837\u5316\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5143\u5b66\u4e60\u80fd\u529b\uff0c\u5b9e\u73b0\u5355\u6b21\u68af\u5ea6\u66f4\u65b0\u7684\u6709\u6548\u9002\u5e94\uff0c\u4f46\u5b58\u5728\u6cdb\u5316\u5dee\u8ddd\u548c\u4efb\u52a1\u95f4\u6027\u80fd\u5dee\u5f02\u5927\u7684\u95ee\u9898\u3002", "motivation": "\u5143\u5b66\u4e60\u7b97\u6cd5\u80fd\u591f\u4f7f\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5c11\u91cf\u6570\u636e\u4e0b\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u8fd9\u5bf9\u4e8e\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u8bc4\u4f30MAML\u4e0eTRPO\u7ed3\u5408\u5728\u591a\u6837\u5316\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u6a21\u578b\u65e0\u5173\u5143\u5b66\u4e60(MAML)\u7ed3\u5408\u4fe1\u4efb\u57df\u7b56\u7565\u4f18\u5316(TRPO)\u5728MetaWorld ML10\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5b9e\u73b0\uff0c\u8be5\u57fa\u51c6\u5305\u542b10\u79cd\u4e0d\u540c\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u5305\u62ec\u63a8\u52a8\u3001\u6293\u53d6\u548c\u62bd\u5c49\u64cd\u4f5c\u7b49\u3002", "result": "MAML\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u5355\u6b21\u9002\u5e94\uff0c\u5355\u6b21\u68af\u5ea6\u66f4\u65b0\u540e\u6027\u80fd\u660e\u663e\u63d0\u5347\uff0c\u8bad\u7ec3\u4efb\u52a1\u6700\u7ec8\u6210\u529f\u7387\u8fbe\u523021.0%\uff0c\u6d4b\u8bd5\u4efb\u52a1\u8fbe\u523013.2%\u3002\u4f46\u5b58\u5728\u6cdb\u5316\u5dee\u8ddd\uff0c\u6d4b\u8bd5\u4efb\u52a1\u6027\u80fd\u505c\u6ede\u800c\u8bad\u7ec3\u4efb\u52a1\u6027\u80fd\u6301\u7eed\u63d0\u5347\uff0c\u4e0d\u540c\u64cd\u4f5c\u6280\u80fd\u7684\u6210\u529f\u7387\u5dee\u5f02\u5f88\u5927(0%\u523080%)\u3002", "conclusion": "\u57fa\u4e8e\u68af\u5ea6\u7684\u5143\u5b66\u4e60\u5728\u591a\u6837\u5316\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\u4f46\u4e5f\u5b58\u5728\u5c40\u9650\u6027\uff0c\u672a\u6765\u5de5\u4f5c\u9700\u8981\u5173\u6ce8\u4efb\u52a1\u611f\u77e5\u9002\u5e94\u548c\u7ed3\u6784\u5316\u7b56\u7565\u67b6\u6784\u3002"}}
{"id": "2511.12390", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12390", "abs": "https://arxiv.org/abs/2511.12390", "authors": ["Sanjar Atamuradov"], "title": "Learning Adaptive Neural Teleoperation for Humanoid Robots: From Inverse Kinematics to End-to-End Control", "comment": "9 pages, 5 figures", "summary": "Virtual reality (VR) teleoperation has emerged as a promising approach for controlling humanoid robots in complex manipulation tasks. However, traditional teleoperation systems rely on inverse kinematics (IK) solvers and hand-tuned PD controllers, which struggle to handle external forces, adapt to different users, and produce natural motions under dynamic conditions. In this work, we propose a learning-based neural teleoperation framework that replaces the conventional IK+PD pipeline with learned policies trained via reinforcement learning. Our approach learns to directly map VR controller inputs to robot joint commands while implicitly handling force disturbances, producing smooth trajectories, and adapting to user preferences. We train our policies in simulation using demonstrations collected from IK-based teleoperation as initialization, then fine-tune them with force randomization and trajectory smoothness rewards. Experiments on the Unitree G1 humanoid robot demonstrate that our learned policies achieve 34% lower tracking error, 45% smoother motions, and superior force adaptation compared to the IK baseline, while maintaining real-time performance (50Hz control frequency). We validate our approach on manipulation tasks including object pick-and-place, door opening, and bimanual coordination. These results suggest that learning-based approaches can significantly improve the naturalness and robustness of humanoid teleoperation systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5b66\u4e60\u7684\u795e\u7ecf\u9065\u64cd\u4f5c\u6846\u67b6\uff0c\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u7b56\u7565\u66ff\u4ee3\u4f20\u7edf\u7684IK+PD\u63a7\u5236\u7ba1\u9053\uff0c\u76f4\u63a5\u6620\u5c04VR\u63a7\u5236\u5668\u8f93\u5165\u5230\u673a\u5668\u4eba\u5173\u8282\u547d\u4ee4\uff0c\u663e\u8457\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u7684\u81ea\u7136\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u9065\u64cd\u4f5c\u7cfb\u7edf\u4f9d\u8d56\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u5668\u548c\u624b\u52a8\u8c03\u6574\u7684PD\u63a7\u5236\u5668\uff0c\u96be\u4ee5\u5904\u7406\u5916\u529b\u5e72\u6270\u3001\u9002\u5e94\u4e0d\u540c\u7528\u6237\uff0c\u4ee5\u53ca\u5728\u52a8\u6001\u6761\u4ef6\u4e0b\u4ea7\u751f\u81ea\u7136\u8fd0\u52a8\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u4eff\u771f\u73af\u5883\u4e2d\u6536\u96c6\u57fa\u4e8eIK\u7684\u9065\u64cd\u4f5c\u6f14\u793a\u4f5c\u4e3a\u521d\u59cb\u5316\uff0c\u7136\u540e\u901a\u8fc7\u529b\u968f\u673a\u5316\u548c\u8f68\u8ff9\u5e73\u6ed1\u5ea6\u5956\u52b1\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b66\u4e60\u7b56\u7565\u76f8\u6bd4IK\u57fa\u7ebf\u5b9e\u73b0\u4e8634%\u66f4\u4f4e\u7684\u8ddf\u8e2a\u8bef\u5dee\u300145%\u66f4\u5e73\u6ed1\u7684\u8fd0\u52a8\u548c\u66f4\u4f18\u7684\u529b\u9002\u5e94\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\uff0850Hz\u63a7\u5236\u9891\u7387\uff09\u3002", "conclusion": "\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u6539\u5584\u4eba\u5f62\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u7cfb\u7edf\u7684\u81ea\u7136\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5728\u7269\u4f53\u6293\u53d6\u653e\u7f6e\u3001\u5f00\u95e8\u548c\u53cc\u624b\u534f\u8c03\u7b49\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002"}}
{"id": "2511.12436", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12436", "abs": "https://arxiv.org/abs/2511.12436", "authors": ["Xiaoshuai Hao", "Yingbo Tang", "Lingfeng Zhang", "Yanbiao Ma", "Yunfeng Diao", "Ziyu Jia", "Wenbo Ding", "Hangjun Ye", "Long Chen"], "title": "RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation", "comment": null, "summary": "Robotic manipulation and navigation are fundamental capabilities of embodied intelligence, enabling effective robot interactions with the physical world. Achieving these capabilities requires a cohesive understanding of the environment, including object recognition to localize target objects, object affordances to identify potential interaction areas and spatial affordances to discern optimal areas for both object placement and robot movement. While Vision-Language Models (VLMs) excel at high-level task planning and scene understanding, they often struggle to infer actionable positions for physical interaction, such as functional grasping points and permissible placement regions. This limitation stems from the lack of fine-grained annotations for object and spatial affordances in their training datasets. To tackle this challenge, we introduce RoboAfford++, a generative AI-enhanced dataset for multimodal affordance learning for both robotic manipulation and navigation. Our dataset comprises 869,987 images paired with 2.0 million question answering (QA) annotations, covering three critical tasks: object affordance recognition to identify target objects based on attributes and spatial relationships, object affordance prediction to pinpoint functional parts for manipulation, and spatial affordance localization to identify free space for object placement and robot navigation. Complementing this dataset, we propose RoboAfford-Eval, a comprehensive benchmark for assessing affordance-aware prediction in real-world scenarios, featuring 338 meticulously annotated samples across the same three tasks. Extensive experimental results reveal the deficiencies of existing VLMs in affordance learning, while fine-tuning on the RoboAfford++ dataset significantly enhances their ability to reason about object and spatial affordances, validating the dataset's effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86RoboAfford++\u6570\u636e\u96c6\u548cRoboAfford-Eval\u57fa\u51c6\uff0c\u7528\u4e8e\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u5bfc\u822a\u4e2d\u96be\u4ee5\u63a8\u65ad\u53ef\u64cd\u4f5c\u4f4d\u7f6e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u7ea7\u4efb\u52a1\u89c4\u5212\u548c\u573a\u666f\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u63a8\u65ad\u7269\u7406\u4ea4\u4e92\u53ef\u64cd\u4f5c\u4f4d\u7f6e\uff08\u5982\u529f\u80fd\u6027\u6293\u53d6\u70b9\u548c\u5141\u8bb8\u653e\u7f6e\u533a\u57df\uff09\u7684\u80fd\u529b\uff0c\u8fd9\u6e90\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u7269\u4f53\u548c\u7a7a\u95f4\u53ef\u4f9b\u6027\u6807\u6ce8\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b869,987\u5f20\u56fe\u50cf\u548c200\u4e07\u95ee\u7b54\u6807\u6ce8\u7684RoboAfford++\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u7269\u4f53\u53ef\u4f9b\u6027\u8bc6\u522b\u3001\u7269\u4f53\u53ef\u4f9b\u6027\u9884\u6d4b\u548c\u7a7a\u95f4\u53ef\u4f9b\u6027\u5b9a\u4f4d\u4e09\u4e2a\u5173\u952e\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86RoboAfford-Eval\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u4f9b\u6027\u5b66\u4e60\u65b9\u9762\u5b58\u5728\u7f3a\u9677\uff0c\u800c\u5728RoboAfford++\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u540e\uff0c\u6a21\u578b\u5728\u63a8\u7406\u7269\u4f53\u548c\u7a7a\u95f4\u53ef\u4f9b\u6027\u65b9\u9762\u7684\u80fd\u529b\u663e\u8457\u63d0\u5347\u3002", "conclusion": "RoboAfford++\u6570\u636e\u96c6\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u5bfc\u822a\u4e2d\u7684\u53ef\u4f9b\u6027\u63a8\u7406\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.12479", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12479", "abs": "https://arxiv.org/abs/2511.12479", "authors": ["Navin Sriram Ravie", "Keerthi Vasan M", "Bijo Sebastian"], "title": "ClutterNav: Gradient-Guided Search for Efficient 3D Clutter Removal with Learned Costmaps", "comment": null, "summary": "Dense clutter removal for target object retrieval presents a challenging problem, especially when targets are embedded deep within densely-packed configurations. It requires foresight to minimize overall changes to the clutter configuration while accessing target objects, avoiding stack destabilization and reducing the number of object removals required. Rule-based planners when applied to this problem, rely on rigid heuristics, leading to high computational overhead. End-to-end reinforcement learning approaches struggle with interpretability and generalizability over different conditions. To address these issues, we present ClutterNav, a novel decision-making framework that can identify the next best object to be removed so as to access a target object in a given clutter, while minimising stack disturbances. ClutterNav formulates the problem as a continuous reinforcement learning task, where each object removal dynamically updates the understanding of the scene. A removability critic, trained from demonstrations, estimates the cost of removing any given object based on geometric and spatial features. This learned cost is complemented by integrated gradients that assess how the presence or removal of surrounding objects influences the accessibility of the target. By dynamically prioritizing actions that balance immediate removability against long-term target exposure, ClutterNav achieves near human-like strategic sequencing, without predefined heuristics. The proposed approach is validated extensively in simulation and over real-world experiments. The results demonstrate real-time, occlusion-aware decision-making in partially observable environments.", "AI": {"tldr": "ClutterNav\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u51b3\u7b56\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e2d\u68c0\u7d22\u76ee\u6807\u7269\u4f53\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5e73\u8861\u5373\u65f6\u53ef\u79fb\u9664\u6027\u548c\u957f\u671f\u76ee\u6807\u66b4\u9732\uff0c\u5b9e\u73b0\u63a5\u8fd1\u4eba\u7c7b\u7b56\u7565\u7684\u5e8f\u5217\u89c4\u5212\u3002", "motivation": "\u89e3\u51b3\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e2d\u76ee\u6807\u7269\u4f53\u68c0\u7d22\u7684\u6311\u6218\u6027\u95ee\u9898\uff0c\u9700\u8981\u9884\u89c1\u6027\u4ee5\u6700\u5c0f\u5316\u5bf9\u6742\u4e71\u914d\u7f6e\u7684\u603b\u4f53\u6539\u53d8\uff0c\u907f\u514d\u5806\u6808\u5931\u7a33\u5e76\u51cf\u5c11\u7269\u4f53\u79fb\u9664\u6b21\u6570\u3002\u57fa\u4e8e\u89c4\u5219\u7684\u89c4\u5212\u5668\u4f9d\u8d56\u521a\u6027\u542f\u53d1\u5f0f\u65b9\u6cd5\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u5f00\u9500\uff0c\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u8fde\u7eed\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u901a\u8fc7\u53ef\u79fb\u9664\u6027\u8bc4\u4f30\u5668\uff08\u57fa\u4e8e\u6f14\u793a\u8bad\u7ec3\uff09\u4f30\u8ba1\u79fb\u9664\u4efb\u4f55\u7ed9\u5b9a\u7269\u4f53\u7684\u6210\u672c\uff0c\u7ed3\u5408\u79ef\u5206\u68af\u5ea6\u8bc4\u4f30\u5468\u56f4\u7269\u4f53\u5bf9\u76ee\u6807\u53ef\u8bbf\u95ee\u6027\u7684\u5f71\u54cd\uff0c\u52a8\u6001\u5e73\u8861\u5373\u65f6\u53ef\u79fb\u9664\u6027\u548c\u957f\u671f\u76ee\u6807\u66b4\u9732\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u3001\u906e\u6321\u611f\u77e5\u51b3\u7b56\uff0c\u8868\u73b0\u51fa\u63a5\u8fd1\u4eba\u7c7b\u7b56\u7565\u7684\u5e8f\u5217\u89c4\u5212\u80fd\u529b\u3002", "conclusion": "ClutterNav\u6846\u67b6\u65e0\u9700\u9884\u5b9a\u4e49\u542f\u53d1\u5f0f\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u7684\u76ee\u6807\u7269\u4f53\u68c0\u7d22\uff0c\u5728\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.12526", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12526", "abs": "https://arxiv.org/abs/2511.12526", "authors": ["Davide De Benedittis", "Giovanni Di Lorenzo", "Franco Angelini", "Barbara Valle", "Marina Serena Borgatti", "Paolo Remagnino", "Marco Caccianiga", "Manolo Garabini"], "title": "Botany Meets Robotics in Alpine Scree Monitoring", "comment": "Published as Early Access in IEEE Transactions on Field Robotics. 19 pages, 13 figures", "summary": "According to the European Union's Habitat Directive, habitat monitoring plays a critical role in response to the escalating problems posed by biodiversity loss and environmental degradation. Scree habitats, hosting unique and often endangered species, face severe threats from climate change due to their high-altitude nature. Traditionally, their monitoring has required highly skilled scientists to conduct extensive fieldwork in remote, potentially hazardous locations, making the process resource-intensive and time-consuming. This paper presents a novel approach for scree habitat monitoring using a legged robot to assist botanists in data collection and species identification. Specifically, we deployed the ANYmal C robot in the Italian Alpine bio-region in two field campaigns spanning two years and leveraged deep learning to detect and classify key plant species of interest. Our results demonstrate that agile legged robots can navigate challenging terrains and increase the frequency and efficiency of scree monitoring. When paired with traditional phytosociological surveys performed by botanists, this robotics-assisted protocol not only streamlines field operations but also enhances data acquisition, storage, and usage. The outcomes of this research contribute to the evolving landscape of robotics in environmental science, paving the way for a more comprehensive and sustainable approach to habitat monitoring and preservation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u817f\u5f0f\u673a\u5668\u4eba\u76d1\u6d4b\u77f3\u6ee9\u6816\u606f\u5730\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7ANYmal C\u673a\u5668\u4eba\u5728\u610f\u5927\u5229\u963f\u5c14\u5351\u65af\u5c71\u533a\u8fdb\u884c\u5b9e\u5730\u90e8\u7f72\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u68c0\u6d4b\u548c\u5206\u7c7b\u5173\u952e\u690d\u7269\u7269\u79cd\uff0c\u63d0\u9ad8\u4e86\u76d1\u6d4b\u6548\u7387\u548c\u9891\u7387\u3002", "motivation": "\u77f3\u6ee9\u6816\u606f\u5730\u627f\u8f7d\u7740\u72ec\u7279\u4e14\u6fd2\u5371\u7684\u7269\u79cd\uff0c\u4f46\u7531\u4e8e\u9ad8\u6d77\u62d4\u7279\u6027\u9762\u4e34\u4e25\u91cd\u7684\u6c14\u5019\u53d8\u5316\u5a01\u80c1\u3002\u4f20\u7edf\u76d1\u6d4b\u65b9\u6cd5\u9700\u8981\u79d1\u5b66\u5bb6\u5728\u504f\u8fdc\u5371\u9669\u5730\u533a\u8fdb\u884c\u5927\u91cf\u5b9e\u5730\u8003\u5bdf\uff0c\u8fc7\u7a0b\u8d44\u6e90\u5bc6\u96c6\u4e14\u8017\u65f6\u3002", "method": "\u5728\u610f\u5927\u5229\u963f\u5c14\u5351\u65af\u751f\u7269\u533a\u90e8\u7f72ANYmal C\u817f\u5f0f\u673a\u5668\u4eba\u8fdb\u884c\u4e24\u5e74\u5b9e\u5730\u6d3b\u52a8\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u68c0\u6d4b\u548c\u5206\u7c7b\u5173\u952e\u690d\u7269\u7269\u79cd\uff0c\u5e76\u4e0e\u690d\u7269\u5b66\u5bb6\u8fdb\u884c\u7684\u4f20\u7edf\u690d\u7269\u793e\u4f1a\u5b66\u8c03\u67e5\u76f8\u7ed3\u5408\u3002", "result": "\u7075\u6d3b\u7684\u817f\u5f0f\u673a\u5668\u4eba\u80fd\u591f\u5bfc\u822a\u5177\u6709\u6311\u6218\u6027\u7684\u5730\u5f62\uff0c\u63d0\u9ad8\u77f3\u6ee9\u76d1\u6d4b\u7684\u9891\u7387\u548c\u6548\u7387\u3002\u673a\u5668\u4eba\u8f85\u52a9\u65b9\u6848\u4e0d\u4ec5\u7b80\u5316\u4e86\u5b9e\u5730\u64cd\u4f5c\uff0c\u8fd8\u589e\u5f3a\u4e86\u6570\u636e\u91c7\u96c6\u3001\u5b58\u50a8\u548c\u4f7f\u7528\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u73af\u5883\u79d1\u5b66\u4e2d\u673a\u5668\u4eba\u6280\u672f\u7684\u5e94\u7528\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u4e3a\u66f4\u5168\u9762\u548c\u53ef\u6301\u7eed\u7684\u6816\u606f\u5730\u76d1\u6d4b\u4e0e\u4fdd\u62a4\u65b9\u6cd5\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.12618", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12618", "abs": "https://arxiv.org/abs/2511.12618", "authors": ["Jordan Leyva", "Nahim J. Moran Vera", "Yihan Xu", "Adrien Durasno", "Christopher U. Romero", "Tendai Chimuka", "Gabriel O. Huezo Ramirez", "Ziqian Dong", "Roberto Rojas-Cessa"], "title": "EcoFlight: Finding Low-Energy Paths Through Obstacles for Autonomous Sensing Drones", "comment": "Autonomous drone, A* algorithm, 3D environments, path planning, obstacle avoidance, energy efficiency, MIT Conference", "summary": "Obstacle avoidance path planning for uncrewed aerial vehicles (UAVs), or drones, is rarely addressed in most flight path planning schemes, despite obstacles being a realistic condition. Obstacle avoidance can also be energy-intensive, making it a critical factor in efficient point-to-point drone flights. To address these gaps, we propose EcoFlight, an energy-efficient pathfinding algorithm that determines the lowest-energy route in 3D space with obstacles. The algorithm models energy consumption based on the drone propulsion system and flight dynamics. We conduct extensive evaluations, comparing EcoFlight with direct-flight and shortest-distance schemes. The simulation results across various obstacle densities show that EcoFlight consistently finds paths with lower energy consumption than comparable algorithms, particularly in high-density environments. We also demonstrate that a suitable flying speed can further enhance energy savings.", "AI": {"tldr": "\u63d0\u51fa\u4e86EcoFlight\u7b97\u6cd5\uff0c\u4e00\u79cd\u8003\u8651\u969c\u788d\u7269\u7684\u65e0\u4eba\u673a\u8282\u80fd\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u57283D\u7a7a\u95f4\u4e2d\u5bfb\u627e\u80fd\u8017\u6700\u4f4e\u7684\u8def\u5f84", "motivation": "\u5927\u591a\u6570\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u65b9\u6848\u5f88\u5c11\u8003\u8651\u969c\u788d\u7269\u89c4\u907f\uff0c\u800c\u969c\u788d\u7269\u89c4\u907f\u53c8\u662f\u80fd\u8017\u5bc6\u96c6\u7684\uff0c\u8fd9\u5bf9\u9ad8\u6548\u7684\u70b9\u5bf9\u70b9\u65e0\u4eba\u673a\u98de\u884c\u81f3\u5173\u91cd\u8981", "method": "\u57fa\u4e8e\u65e0\u4eba\u673a\u63a8\u8fdb\u7cfb\u7edf\u548c\u98de\u884c\u52a8\u529b\u5b66\u5efa\u6a21\u80fd\u8017\uff0c\u63d0\u51faEcoFlight\u8282\u80fd\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff0c\u57283D\u7a7a\u95f4\u4e2d\u5bfb\u627e\u80fd\u8017\u6700\u4f4e\u7684\u8def\u5f84", "result": "\u5728\u5404\u79cd\u969c\u788d\u7269\u5bc6\u5ea6\u4e0b\u7684\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0cEcoFlight\u59cb\u7ec8\u627e\u5230\u6bd4\u76f4\u63a5\u98de\u884c\u548c\u6700\u77ed\u8ddd\u79bb\u65b9\u6848\u80fd\u8017\u66f4\u4f4e\u7684\u8def\u5f84\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5bc6\u5ea6\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5408\u9002\u7684\u98de\u884c\u901f\u5ea6\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u8282\u80fd\u6548\u679c", "conclusion": "EcoFlight\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u964d\u4f4e\u65e0\u4eba\u673a\u5728\u969c\u788d\u73af\u5883\u4e2d\u7684\u80fd\u8017\uff0c\u4e3a\u8282\u80fd\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2511.12650", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12650", "abs": "https://arxiv.org/abs/2511.12650", "authors": ["Arvind Kumar Mishra", "Sohom Chakrabarty"], "title": "Task-Aware Morphology Optimization of Planar Manipulators via Reinforcement Learning", "comment": "10 pages, 11 figures, It is submitted as a journal option paper associated with the IFAC World Congress 2026", "summary": "In this work, Yoshikawa's manipulability index is used to investigate reinforcement learning (RL) as a framework for morphology optimization in planar robotic manipulators. A 2R manipulator tracking a circular end-effector path is first examined because this case has a known analytical optimum: equal link lengths and the second joint orthogonal to the first. This serves as a validation step to test whether RL can rediscover the optimum using reward feedback alone, without access to the manipulability expression or the Jacobian. Three RL algorithms (SAC, DDPG, and PPO) are compared with grid search and black-box optimizers, with morphology represented by a single action parameter phi that maps to the link lengths. All methods converge to the analytical solution, showing that numerical recovery of the optimum is possible without supplying analytical structure.\n  Most morphology design tasks have no closed-form solutions, and grid or heuristic search becomes expensive as dimensionality increases. RL is therefore explored as a scalable alternative. The formulation used for the circular path is extended to elliptical and rectangular paths by expanding the action space to the full morphology vector (L1, L2, theta2). In these non-analytical settings, RL continues to converge reliably, whereas grid and black-box methods require far larger evaluation budgets. These results indicate that RL is effective for both recovering known optima and solving morphology optimization problems without analytical solutions.", "AI": {"tldr": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u673a\u5668\u4eba\u5f62\u6001\u4f18\u5316\uff0c\u9a8c\u8bc1\u4e86RL\u80fd\u591f\u4ec5\u901a\u8fc7\u5956\u52b1\u53cd\u9988\u91cd\u65b0\u53d1\u73b0\u5df2\u77e5\u89e3\u6790\u6700\u4f18\u89e3\uff0c\u5e76\u5728\u65e0\u89e3\u6790\u89e3\u7684\u60c5\u51b5\u4e0b\u53ef\u9760\u6536\u655b\u3002", "motivation": "\u5927\u591a\u6570\u5f62\u6001\u8bbe\u8ba1\u4efb\u52a1\u6ca1\u6709\u95ed\u5f0f\u89e3\uff0c\u968f\u7740\u7ef4\u5ea6\u589e\u52a0\uff0c\u7f51\u683c\u6216\u542f\u53d1\u5f0f\u641c\u7d22\u53d8\u5f97\u6602\u8d35\u3002RL\u88ab\u63a2\u7d22\u4e3a\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Yoshikawa\u53ef\u64cd\u4f5c\u6027\u6307\u6570\uff0c\u901a\u8fc7\u4e09\u79cdRL\u7b97\u6cd5\uff08SAC\u3001DDPG\u3001PPO\uff09\u4e0e\u7f51\u683c\u641c\u7d22\u548c\u9ed1\u76d2\u4f18\u5316\u5668\u6bd4\u8f83\uff0c\u5f62\u6001\u7531\u52a8\u4f5c\u53c2\u6570phi\u6216\u5b8c\u6574\u5f62\u6001\u5411\u91cf\u8868\u793a\u3002", "result": "\u6240\u6709\u65b9\u6cd5\u90fd\u6536\u655b\u5230\u89e3\u6790\u89e3\uff0c\u8868\u660e\u65e0\u9700\u63d0\u4f9b\u89e3\u6790\u7ed3\u6784\u5373\u53ef\u6570\u503c\u6062\u590d\u6700\u4f18\u89e3\u3002\u5728\u975e\u89e3\u6790\u8bbe\u7f6e\u4e2d\uff0cRL\u7ee7\u7eed\u53ef\u9760\u6536\u655b\uff0c\u800c\u7f51\u683c\u548c\u9ed1\u76d2\u65b9\u6cd5\u9700\u8981\u66f4\u5927\u7684\u8bc4\u4f30\u9884\u7b97\u3002", "conclusion": "RL\u5bf9\u4e8e\u6062\u590d\u5df2\u77e5\u6700\u4f18\u89e3\u548c\u89e3\u51b3\u65e0\u89e3\u6790\u89e3\u7684\u5f62\u6001\u4f18\u5316\u95ee\u9898\u90fd\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2511.12755", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12755", "abs": "https://arxiv.org/abs/2511.12755", "authors": ["Aleesha Khurram", "Amir Moeini", "Shangtong Zhang", "Rohan Chandra"], "title": "Prompt-Driven Domain Adaptation for End-to-End Autonomous Driving via In-Context RL", "comment": null, "summary": "Despite significant progress and advances in autonomous driving, many end-to-end systems still struggle with domain adaptation (DA), such as transferring a policy trained under clear weather to adverse weather conditions. Typical DA strategies in the literature include collecting additional data in the target domain or re-training the model, or both. Both these strategies quickly become impractical as we increase scale and complexity of driving. These limitations have encouraged investigation into few-shot and zero-shot prompt-driven DA at inference time involving LLMs and VLMs. These methods work by adding a few state-action trajectories during inference to the prompt (similar to in-context learning). However, there are two limitations of such an approach: $(i)$ prompt-driven DA methods are currently restricted to perception tasks such as detection and segmentation and $(ii)$ they require expert few-shot data. In this work, we present a new approach to inference-time few-shot prompt-driven DA for closed-loop autonomous driving in adverse weather condition using in-context reinforcement learning (ICRL). Similar to other prompt-driven DA methods, our approach does not require any updates to the model parameters nor does it require additional data collection in adversarial weather regime. Furthermore, our approach advances the state-of-the-art in prompt-driven DA by extending to closed driving using general trajectories observed during inference. Our experiments using the CARLA simulator show that ICRL results in safer, more efficient, and more comfortable driving policies in the target domain compared to state-of-the-art prompt-driven DA baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60(ICRL)\u7684\u63a8\u7406\u65f6\u5c11\u6837\u672c\u63d0\u793a\u9a71\u52a8\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u7528\u4e8e\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u95ed\u73af\u81ea\u52a8\u9a7e\u9a76\uff0c\u65e0\u9700\u6a21\u578b\u53c2\u6570\u66f4\u65b0\u6216\u989d\u5916\u6570\u636e\u6536\u96c6\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u9886\u57df\u81ea\u9002\u5e94\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u4ece\u6674\u6717\u5929\u6c14\u5230\u6076\u52a3\u5929\u6c14\u7684\u7b56\u7565\u8fc1\u79fb\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u6536\u96c6\u76ee\u6807\u57df\u6570\u636e\u6216\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u6210\u672c\u9ad8\u6602\u4e14\u4e0d\u5b9e\u7528\u3002\u73b0\u6709\u7684\u63d0\u793a\u9a71\u52a8DA\u65b9\u6cd5\u4ec5\u9650\u4e8e\u611f\u77e5\u4efb\u52a1\u4e14\u9700\u8981\u4e13\u5bb6\u5c11\u6837\u672c\u6570\u636e\u3002", "method": "\u4f7f\u7528\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60(ICRL)\u8fdb\u884c\u63a8\u7406\u65f6\u5c11\u6837\u672c\u63d0\u793a\u9a71\u52a8\u9886\u57df\u81ea\u9002\u5e94\uff0c\u5728\u63a8\u7406\u65f6\u5411\u63d0\u793a\u4e2d\u6dfb\u52a0\u72b6\u6001-\u52a8\u4f5c\u8f68\u8ff9\uff08\u7c7b\u4f3c\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\uff09\uff0c\u4f46\u6269\u5c55\u5230\u95ed\u73af\u9a7e\u9a76\u5e76\u4f7f\u7528\u63a8\u7406\u8fc7\u7a0b\u4e2d\u89c2\u5bdf\u5230\u7684\u4e00\u822c\u8f68\u8ff9\u3002", "result": "\u5728CARLA\u6a21\u62df\u5668\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u63d0\u793a\u9a71\u52a8DA\u57fa\u7ebf\u76f8\u6bd4\uff0cICRL\u5728\u76ee\u6807\u57df\u4e2d\u4ea7\u751f\u4e86\u66f4\u5b89\u5168\u3001\u66f4\u9ad8\u6548\u3001\u66f4\u8212\u9002\u7684\u9a7e\u9a76\u7b56\u7565\u3002", "conclusion": "ICRL\u65b9\u6cd5\u6269\u5c55\u4e86\u63d0\u793a\u9a71\u52a8DA\u7684\u8303\u56f4\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u95ed\u73af\u81ea\u52a8\u9a7e\u9a76\uff0c\u4e14\u4e0d\u9700\u8981\u6a21\u578b\u66f4\u65b0\u6216\u989d\u5916\u6570\u636e\u6536\u96c6\uff0c\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.12778", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12778", "abs": "https://arxiv.org/abs/2511.12778", "authors": ["Vignesh Rajagopal", "Kasun Weerakoon Kulathun Mudiyanselage", "Gershom Devake Seneviratne", "Pon Aswin Sankaralingam", "Mohamed Elnoor", "Jing Liang", "Rohan Chandra", "Dinesh Manocha"], "title": "DR. Nav: Semantic-Geometric Representations for Proactive Dead-End Recovery and Navigation", "comment": null, "summary": "We present DR. Nav (Dead-End Recovery-aware Navigation), a novel approach to autonomous navigation in scenarios where dead-end detection and recovery are critical, particularly in unstructured environments where robots must handle corners, vegetation occlusions, and blocked junctions. DR. Nav introduces a proactive strategy for navigation in unmapped environments without prior assumptions. Our method unifies dead-end prediction and recovery by generating a single, continuous, real-time semantic cost map. Specifically, DR. Nav leverages cross-modal RGB-LiDAR fusion with attention-based filtering to estimate per-cell dead-end likelihoods and recovery points, which are continuously updated through Bayesian inference to enhance robustness. Unlike prior mapping methods that only encode traversability, DR. Nav explicitly incorporates recovery-aware risk into the navigation cost map, enabling robots to anticipate unsafe regions and plan safer alternative trajectories. We evaluate DR. Nav across multiple dense indoor and outdoor scenarios and demonstrate an increase of 83.33% in accuracy in detection, a 52.4% reduction in time-to-goal (path efficiency), compared to state-of-the-art planners such as DWA, MPPI, and Nav2 DWB. Furthermore, the dead-end classifier functions", "AI": {"tldr": "DR.Nav\u662f\u4e00\u79cd\u7528\u4e8e\u81ea\u4e3b\u5bfc\u822a\u7684\u65b0\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u6b7b\u80e1\u540c\u68c0\u6d4b\u548c\u6062\u590d\uff0c\u7279\u522b\u9002\u7528\u4e8e\u975e\u7ed3\u6784\u5316\u73af\u5883\u3002\u5b83\u901a\u8fc7RGB-LiDAR\u878d\u5408\u548c\u8d1d\u53f6\u65af\u63a8\u7406\u751f\u6210\u5b9e\u65f6\u7684\u8bed\u4e49\u6210\u672c\u5730\u56fe\uff0c\u7edf\u4e00\u4e86\u6b7b\u80e1\u540c\u9884\u6d4b\u548c\u6062\u590d\u529f\u80fd\u3002", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u7ecf\u5e38\u9762\u4e34\u89d2\u843d\u3001\u690d\u88ab\u906e\u6321\u548c\u963b\u585e\u8def\u53e3\u7b49\u6b7b\u80e1\u540c\u60c5\u51b5\uff0c\u73b0\u6709\u5bfc\u822a\u65b9\u6cd5\u7f3a\u4e4f\u6709\u6548\u7684\u6b7b\u80e1\u540c\u68c0\u6d4b\u548c\u6062\u590d\u673a\u5236\uff0c\u5bfc\u81f4\u5bfc\u822a\u6548\u7387\u4f4e\u4e0b\u548c\u5b89\u5168\u98ce\u9669\u3002", "method": "\u91c7\u7528RGB-LiDAR\u8de8\u6a21\u6001\u878d\u5408\u4e0e\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8fc7\u6ee4\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u7406\u6301\u7eed\u66f4\u65b0\u6bcf\u4e2a\u5355\u5143\u7684\u6b7b\u4ea1\u6982\u7387\u548c\u6062\u590d\u70b9\uff0c\u5c06\u6062\u590d\u611f\u77e5\u98ce\u9669\u660e\u786e\u7eb3\u5165\u5bfc\u822a\u6210\u672c\u5730\u56fe\u3002", "result": "\u5728\u5bc6\u96c6\u5ba4\u5185\u5916\u573a\u666f\u4e2d\uff0c\u76f8\u6bd4DWA\u3001MPPI\u548cNav2 DWB\u7b49\u5148\u8fdb\u89c4\u5212\u5668\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u9ad883.33%\uff0c\u5230\u8fbe\u76ee\u6807\u65f6\u95f4\u51cf\u5c1152.4%\uff08\u8def\u5f84\u6548\u7387\u63d0\u5347\uff09\u3002", "conclusion": "DR.Nav\u901a\u8fc7\u7edf\u4e00\u6b7b\u80e1\u540c\u9884\u6d4b\u548c\u6062\u590d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u4e3b\u5bfc\u822a\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2511.12795", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12795", "abs": "https://arxiv.org/abs/2511.12795", "authors": ["Boshu Lei", "Wen Jiang", "Kostas Daniilidis"], "title": "ActiveGrasp: Information-Guided Active Grasping with Calibrated Energy-based Model", "comment": "under review", "summary": "Grasping in a densely cluttered environment is a challenging task for robots. Previous methods tried to solve this problem by actively gathering multiple views before grasp pose generation. However, they either overlooked the importance of the grasp distribution for information gain estimation or relied on the projection of the grasp distribution, which ignores the structure of grasp poses on the SE(3) manifold. To tackle these challenges, we propose a calibrated energy-based model for grasp pose generation and an active view selection method that estimates information gain from grasp distribution. Our energy-based model captures the multi-modality nature of grasp distribution on the SE(3) manifold. The energy level is calibrated to the success rate of grasps so that the predicted distribution aligns with the real distribution. The next best view is selected by estimating the information gain for grasp from the calibrated distribution conditioned on the reconstructed environment, which could efficiently drive the robot to explore affordable parts of the target object. Experiments on simulated environments and real robot setups demonstrate that our model could successfully grasp objects in a cluttered environment with limited view budgets compared to previous state-of-the-art models. Our simulated environment can serve as a reproducible platform for future research on active grasping. The source code of our paper will be made public when the paper is released to the public.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u6821\u51c6\u7684\u6293\u53d6\u59ff\u6001\u751f\u6210\u6a21\u578b\u548c\u4e3b\u52a8\u89c6\u89d2\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u6293\u53d6\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6821\u51c6\u80fd\u91cf\u6a21\u578b\u6355\u6349SE(3)\u6d41\u5f62\u4e0a\u7684\u591a\u6a21\u6001\u6293\u53d6\u5206\u5e03\uff0c\u5e76\u57fa\u4e8e\u4fe1\u606f\u589e\u76ca\u9009\u62e9\u6700\u4f18\u89c6\u89d2\u3002", "motivation": "\u89e3\u51b3\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u6293\u53d6\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u4e86\u6293\u53d6\u5206\u5e03\u5728\u4fe1\u606f\u589e\u76ca\u4f30\u8ba1\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u6293\u53d6\u5206\u5e03\u7684\u6295\u5f71\u800c\u5ffd\u7565\u4e86SE(3)\u6d41\u5f62\u4e0a\u7684\u7ed3\u6784\u7279\u6027\u3002", "method": "1) \u57fa\u4e8e\u80fd\u91cf\u6821\u51c6\u7684\u6293\u53d6\u59ff\u6001\u751f\u6210\u6a21\u578b\uff0c\u6355\u6349SE(3)\u6d41\u5f62\u4e0a\u7684\u591a\u6a21\u6001\u6293\u53d6\u5206\u5e03\uff0c\u5e76\u5c06\u80fd\u91cf\u6c34\u5e73\u6821\u51c6\u4e3a\u6293\u53d6\u6210\u529f\u7387\uff1b2) \u4e3b\u52a8\u89c6\u89d2\u9009\u62e9\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6821\u51c6\u5206\u5e03\u4f30\u8ba1\u6293\u53d6\u7684\u4fe1\u606f\u589e\u76ca\u6765\u9009\u62e9\u4e0b\u4e00\u4e2a\u6700\u4f73\u89c6\u89d2\u3002", "result": "\u5728\u4eff\u771f\u73af\u5883\u548c\u771f\u5b9e\u673a\u5668\u4eba\u8bbe\u7f6e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u6709\u9650\u89c6\u89d2\u9884\u7b97\u4e0b\u6210\u529f\u6293\u53d6\u6742\u4e71\u73af\u5883\u4e2d\u7684\u7269\u4f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e2d\u7684\u6293\u53d6\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u4eff\u771f\u73af\u5883\u53ef\u4f5c\u4e3a\u672a\u6765\u4e3b\u52a8\u6293\u53d6\u7814\u7a76\u7684\u53ef\u590d\u73b0\u5e73\u53f0\u3002"}}
{"id": "2511.12848", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12848", "abs": "https://arxiv.org/abs/2511.12848", "authors": ["Max M. Sun", "Todd Murphey"], "title": "Structured Imitation Learning of Interactive Policies through Inverse Games", "comment": "Presented at the \"Workshop on Generative Modeling Meets Human-Robot Interaction\" at Robotics: Science and Systems 2025. Workshop website: https://sites.google.com/view/gai-hri/", "summary": "Generative model-based imitation learning methods have recently achieved strong results in learning high-complexity motor skills from human demonstrations. However, imitation learning of interactive policies that coordinate with humans in shared spaces without explicit communication remains challenging, due to the significantly higher behavioral complexity in multi-agent interactions compared to non-interactive tasks. In this work, we introduce a structured imitation learning framework for interactive policies by combining generative single-agent policy learning with a flexible yet expressive game-theoretic structure. Our method explicitly separates learning into two steps: first, we learn individual behavioral patterns from multi-agent demonstrations using standard imitation learning; then, we structurally learn inter-agent dependencies by solving an inverse game problem. Preliminary results in a synthetic 5-agent social navigation task show that our method significantly improves non-interactive policies and performs comparably to the ground truth interactive policy using only 50 demonstrations. These results highlight the potential of structured imitation learning in interactive settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u751f\u6210\u5f0f\u5355\u667a\u80fd\u4f53\u7b56\u7565\u5b66\u4e60\u548c\u535a\u5f08\u8bba\u7ed3\u6784\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u4ea4\u4e92\u5f0f\u7b56\u7565\uff0c\u57285\u667a\u80fd\u4f53\u793e\u4ea4\u5bfc\u822a\u4efb\u52a1\u4e2d\u4ec5\u752850\u4e2a\u6f14\u793a\u5c31\u53d6\u5f97\u4e86\u4e0e\u771f\u5b9e\u4ea4\u4e92\u7b56\u7565\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u4ece\u4eba\u7c7b\u6f14\u793a\u4e2d\u5b66\u4e60\u9ad8\u590d\u6742\u5ea6\u8fd0\u52a8\u6280\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u4f46\u5728\u65e0\u663e\u5f0f\u901a\u4fe1\u7684\u5171\u4eab\u7a7a\u95f4\u4e2d\u4e0e\u4eba\u7c7b\u534f\u8c03\u7684\u4ea4\u4e92\u5f0f\u7b56\u7565\u5b66\u4e60\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u884c\u4e3a\u590d\u6742\u5ea6\u8fdc\u9ad8\u4e8e\u975e\u4ea4\u4e92\u4efb\u52a1\u3002", "method": "\u5c06\u5b66\u4e60\u660e\u786e\u5206\u4e3a\u4e24\u4e2a\u6b65\u9aa4\uff1a\u9996\u5148\u4f7f\u7528\u6807\u51c6\u6a21\u4eff\u5b66\u4e60\u4ece\u591a\u667a\u80fd\u4f53\u6f14\u793a\u4e2d\u5b66\u4e60\u4e2a\u4f53\u884c\u4e3a\u6a21\u5f0f\uff0c\u7136\u540e\u901a\u8fc7\u89e3\u51b3\u9006\u535a\u5f08\u95ee\u9898\u6765\u7ed3\u6784\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u5408\u6210\u76845\u667a\u80fd\u4f53\u793e\u4ea4\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u975e\u4ea4\u4e92\u7b56\u7565\uff0c\u4ec5\u4f7f\u752850\u4e2a\u6f14\u793a\u5c31\u53d6\u5f97\u4e86\u4e0e\u771f\u5b9e\u4ea4\u4e92\u7b56\u7565\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u51f8\u663e\u4e86\u7ed3\u6784\u5316\u6a21\u4eff\u5b66\u4e60\u5728\u4ea4\u4e92\u5f0f\u8bbe\u7f6e\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.12882", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12882", "abs": "https://arxiv.org/abs/2511.12882", "authors": ["Taiyi Su", "Jian Zhu", "Yaxuan Li", "Chong Ma", "Zitai Huang", "Yichen Zhu", "Hanli Wang", "Yi Xu"], "title": "Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos", "comment": "11 pages, 5 figures", "summary": "Embodied world models aim to predict and interact with the physical world through visual observations and actions. However, existing models struggle to accurately translate low-level actions (e.g., joint positions) into precise robotic movements in predicted frames, leading to inconsistencies with real-world physical interactions. To address these limitations, we propose MTV-World, an embodied world model that introduces Multi-view Trajectory-Video control for precise visuomotor prediction. Specifically, instead of directly using low-level actions for control, we employ trajectory videos obtained through camera intrinsic and extrinsic parameters and Cartesian-space transformation as control signals. However, projecting 3D raw actions onto 2D images inevitably causes a loss of spatial information, making a single view insufficient for accurate interaction modeling. To overcome this, we introduce a multi-view framework that compensates for spatial information loss and ensures high-consistency with physical world. MTV-World forecasts future frames based on multi-view trajectory videos as input and conditioning on an initial frame per view. Furthermore, to systematically evaluate both robotic motion precision and object interaction accuracy, we develop an auto-evaluation pipeline leveraging multimodal large models and referring video object segmentation models. To measure spatial consistency, we formulate it as an object location matching problem and adopt the Jaccard Index as the evaluation metric. Extensive experiments demonstrate that MTV-World achieves precise control execution and accurate physical interaction modeling in complex dual-arm scenarios.", "AI": {"tldr": "MTV-World\u662f\u4e00\u4e2a\u5177\u8eab\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u8f68\u8ff9\u89c6\u9891\u63a7\u5236\u5b9e\u73b0\u7cbe\u786e\u7684\u89c6\u89c9\u8fd0\u52a8\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5c06\u4f4e\u7ea7\u52a8\u4f5c\u8f6c\u6362\u4e3a\u7cbe\u786e\u673a\u5668\u4eba\u8fd0\u52a8\u65f6\u7684\u7269\u7406\u4e00\u81f4\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5177\u8eab\u4e16\u754c\u6a21\u578b\u96be\u4ee5\u5c06\u4f4e\u7ea7\u52a8\u4f5c\uff08\u5982\u5173\u8282\u4f4d\u7f6e\uff09\u51c6\u786e\u8f6c\u6362\u4e3a\u9884\u6d4b\u5e27\u4e2d\u7684\u7cbe\u786e\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u5bfc\u81f4\u4e0e\u73b0\u5b9e\u4e16\u754c\u7269\u7406\u4ea4\u4e92\u4e0d\u4e00\u81f4\u3002", "method": "\u4f7f\u7528\u901a\u8fc7\u76f8\u673a\u5185\u5916\u53c2\u548c\u7b1b\u5361\u5c14\u7a7a\u95f4\u53d8\u6362\u83b7\u5f97\u7684\u8f68\u8ff9\u89c6\u9891\u4f5c\u4e3a\u63a7\u5236\u4fe1\u53f7\uff0c\u5f15\u5165\u591a\u89c6\u89d2\u6846\u67b6\u8865\u507f\u7a7a\u95f4\u4fe1\u606f\u635f\u5931\uff0c\u57fa\u4e8e\u591a\u89c6\u89d2\u8f68\u8ff9\u89c6\u9891\u9884\u6d4b\u672a\u6765\u5e27\u3002", "result": "\u5728\u590d\u6742\u53cc\u81c2\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u63a7\u5236\u6267\u884c\u548c\u51c6\u786e\u7684\u7269\u7406\u4ea4\u4e92\u5efa\u6a21\uff0c\u901a\u8fc7\u81ea\u52a8\u8bc4\u4f30\u6d41\u7a0b\u9a8c\u8bc1\u4e86\u8fd0\u52a8\u7cbe\u5ea6\u548c\u4ea4\u4e92\u51c6\u786e\u6027\u3002", "conclusion": "MTV-World\u901a\u8fc7\u591a\u89c6\u89d2\u8f68\u8ff9\u89c6\u9891\u63a7\u5236\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5177\u8eab\u4e16\u754c\u6a21\u578b\u5728\u7269\u7406\u4ea4\u4e92\u4e2d\u7684\u7cbe\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2511.12896", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12896", "abs": "https://arxiv.org/abs/2511.12896", "authors": ["Jun Huo", "Hongge Ru", "Bo Yang", "Xingjian Chen", "Xi Li", "Jian Huang"], "title": "Air-Chamber Based Soft Six-Axis Force/Torque Sensor for Human-Robot Interaction", "comment": null, "summary": "Soft multi-axis force/torque sensors provide safe and precise force interaction. Capturing the complete degree-of-freedom of force is imperative for accurate force measurement with six-axis force/torque sensors. However, cross-axis coupling can lead to calibration issues and decreased accuracy. In this instance, developing a soft and accurate six-axis sensor is a challenging task. In this paper, a soft air-chamber type six-axis force/torque sensor with 16-channel barometers is introduced, which housed in hyper-elastic air chambers made of silicone rubber. Additionally, an effective decoupling method is proposed, based on a rigid-soft hierarchical structure, which reduces the six-axis decoupling problem to two three-axis decoupling problems. Finite element model simulation and experiments demonstrate the compatibility of the proposed approach with reality. The prototype's sensing performance is quantitatively measured in terms of static load response, dynamic load response and dynamic response characteristic. It possesses a measuring range of 50 N force and 1 Nm torque, and the average deviation, repeatability, non-linearity and hysteresis are 4.9$\\%$, 2.7$\\%$, 5.8$\\%$ and 6.7$\\%$, respectively. The results indicate that the prototype exhibits satisfactory sensing performance while maintaining its softness due to the presence of soft air chambers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e16\u901a\u9053\u6c14\u538b\u8ba1\u7684\u8f6f\u6c14\u5ba4\u578b\u516d\u8f74\u529b/\u529b\u77e9\u4f20\u611f\u5668\uff0c\u91c7\u7528\u521a\u67d4\u5206\u5c42\u7ed3\u6784\u8fdb\u884c\u6709\u6548\u89e3\u8026\uff0c\u5c06\u516d\u8f74\u89e3\u8026\u95ee\u9898\u7b80\u5316\u4e3a\u4e24\u4e2a\u4e09\u8f74\u89e3\u8026\u95ee\u9898\u3002", "motivation": "\u8f6f\u591a\u8f74\u529b/\u529b\u77e9\u4f20\u611f\u5668\u80fd\u63d0\u4f9b\u5b89\u5168\u7cbe\u786e\u7684\u529b\u4ea4\u4e92\uff0c\u4f46\u4ea4\u53c9\u8f74\u8026\u5408\u4f1a\u5bfc\u81f4\u6821\u51c6\u95ee\u9898\u548c\u7cbe\u5ea6\u4e0b\u964d\uff0c\u5f00\u53d1\u67d4\u8f6f\u4e14\u7cbe\u786e\u7684\u516d\u8f74\u4f20\u611f\u5668\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u7845\u6a61\u80f6\u5236\u6210\u7684\u8d85\u5f39\u6027\u6c14\u5ba4\u5bb9\u7eb316\u901a\u9053\u6c14\u538b\u8ba1\uff0c\u63d0\u51fa\u57fa\u4e8e\u521a\u67d4\u5206\u5c42\u7ed3\u6784\u7684\u89e3\u8026\u65b9\u6cd5\uff0c\u901a\u8fc7\u6709\u9650\u5143\u6a21\u62df\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u539f\u578b\u4f20\u611f\u5668\u6d4b\u91cf\u8303\u56f4\u4e3a50N\u529b\u548c1Nm\u626d\u77e9\uff0c\u5e73\u5747\u504f\u5dee4.9%\u3001\u91cd\u590d\u60272.7%\u3001\u975e\u7ebf\u60275.8%\u3001\u8fdf\u6ede6.7%\uff0c\u5728\u4fdd\u6301\u67d4\u8f6f\u6027\u7684\u540c\u65f6\u8868\u73b0\u51fa\u4ee4\u4eba\u6ee1\u610f\u7684\u4f20\u611f\u6027\u80fd\u3002", "conclusion": "\u8be5\u8f6f\u6c14\u5ba4\u578b\u516d\u8f74\u529b/\u529b\u77e9\u4f20\u611f\u5668\u901a\u8fc7\u6709\u6548\u7684\u89e3\u8026\u65b9\u6cd5\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u4f20\u611f\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6750\u6599\u7684\u67d4\u8f6f\u7279\u6027\u3002"}}
{"id": "2511.12910", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12910", "abs": "https://arxiv.org/abs/2511.12910", "authors": ["Yong Li", "Yujun Huang", "Yi Chen", "Hui Cheng"], "title": "TOPP-DWR: Time-Optimal Path Parameterization of Differential-Driven Wheeled Robots Considering Piecewise-Constant Angular Velocity Constraints", "comment": null, "summary": "Differential-driven wheeled robots (DWR) represent the quintessential type of mobile robots and find extensive appli- cations across the robotic field. Most high-performance control approaches for DWR explicitly utilize the linear and angular velocities of the trajectory as control references. However, existing research on time-optimal path parameterization (TOPP) for mobile robots usually neglects the angular velocity and joint vel- ocity constraints, which can result in degraded control perfor- mance in practical applications. In this article, a systematic and practical TOPP algorithm named TOPP-DWR is proposed for DWR and other mobile robots. First, the non-uniform B-spline is adopted to represent the initial trajectory in the task space. Second, the piecewise-constant angular velocity, as well as joint velocity, linear velocity, and linear acceleration constraints, are incorporated into the TOPP problem. During the construction of the optimization problem, the aforementioned constraints are uniformly represented as linear velocity constraints. To boost the numerical computational efficiency, we introduce a slack variable to reformulate the problem into second-order-cone programming (SOCP). Subsequently, comparative experiments are conducted to validate the superiority of the proposed method. Quantitative performance indexes show that TOPP-DWR achieves TOPP while adhering to all constraints. Finally, field autonomous navigation experiments are carried out to validate the practicability of TOPP-DWR in real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86TOPP-DWR\u7b97\u6cd5\uff0c\u4e3a\u5dee\u901f\u9a71\u52a8\u8f6e\u5f0f\u673a\u5668\u4eba\u63d0\u4f9b\u7cfb\u7edf\u5b9e\u7528\u7684\u65f6\u95f4\u6700\u4f18\u8def\u5f84\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u8003\u8651\u4e86\u89d2\u901f\u5ea6\u3001\u5173\u8282\u901f\u5ea6\u7b49\u7ea6\u675f\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u4e8c\u9636\u9525\u89c4\u5212\u95ee\u9898\u6c42\u89e3\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u6700\u4f18\u8def\u5f84\u53c2\u6570\u5316\u7814\u7a76\u901a\u5e38\u5ffd\u7565\u89d2\u901f\u5ea6\u548c\u5173\u8282\u901f\u5ea6\u7ea6\u675f\uff0c\u5bfc\u81f4\u5b9e\u9645\u5e94\u7528\u4e2d\u63a7\u5236\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5168\u9762\u8003\u8651\u8fd9\u4e9b\u7ea6\u675f\u7684\u5b9e\u7528\u7b97\u6cd5\u3002", "method": "\u4f7f\u7528\u975e\u5747\u5300B\u6837\u6761\u8868\u793a\u8f68\u8ff9\uff0c\u5c06\u89d2\u901f\u5ea6\u3001\u5173\u8282\u901f\u5ea6\u3001\u7ebf\u901f\u5ea6\u548c\u7ebf\u52a0\u901f\u5ea6\u7ea6\u675f\u7edf\u4e00\u8868\u793a\u4e3a\u7ebf\u901f\u5ea6\u7ea6\u675f\uff0c\u5f15\u5165\u677e\u5f1b\u53d8\u91cf\u5c06\u95ee\u9898\u91cd\u6784\u4e3a\u4e8c\u9636\u9525\u89c4\u5212\u95ee\u9898\u3002", "result": "\u6bd4\u8f83\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5b9a\u91cf\u6027\u80fd\u6307\u6807\u663e\u793aTOPP-DWR\u80fd\u5728\u6ee1\u8db3\u6240\u6709\u7ea6\u675f\u6761\u4ef6\u4e0b\u5b9e\u73b0\u65f6\u95f4\u6700\u4f18\u8def\u5f84\u53c2\u6570\u5316\uff0c\u73b0\u573a\u81ea\u4e3b\u5bfc\u822a\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b9e\u9645\u5e94\u7528\u53ef\u884c\u6027\u3002", "conclusion": "TOPP-DWR\u7b97\u6cd5\u4e3a\u5dee\u901f\u9a71\u52a8\u8f6e\u5f0f\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5b9e\u7528\u7684\u65f6\u95f4\u6700\u4f18\u8def\u5f84\u53c2\u6570\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u6709\u6548\u5904\u7406\u591a\u79cd\u8fd0\u52a8\u7ea6\u675f\u5e76\u63d0\u5347\u63a7\u5236\u6027\u80fd\u3002"}}
{"id": "2511.12912", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12912", "abs": "https://arxiv.org/abs/2511.12912", "authors": ["Yingting Zhou", "Wenbo Cui", "Weiheng Liu", "Guixing Chen", "Haoran Li", "Dongbin Zhao"], "title": "DiffuDepGrasp: Diffusion-based Depth Noise Modeling Empowers Sim2Real Robotic Grasping", "comment": null, "summary": "Transferring the depth-based end-to-end policy trained in simulation to physical robots can yield an efficient and robust grasping policy, yet sensor artifacts in real depth maps like voids and noise establish a significant sim2real gap that critically impedes policy transfer. Training-time strategies like procedural noise injection or learned mappings suffer from data inefficiency due to unrealistic noise simulation, which is often ineffective for grasping tasks that require fine manipulation or dependency on paired datasets heavily. Furthermore, leveraging foundation models to reduce the sim2real gap via intermediate representations fails to mitigate the domain shift fully and adds computational overhead during deployment. This work confronts dual challenges of data inefficiency and deployment complexity. We propose DiffuDepGrasp, a deploy-efficient sim2real framework enabling zero-shot transfer through simulation-exclusive policy training. Its core innovation, the Diffusion Depth Generator, synthesizes geometrically pristine simulation depth with learned sensor-realistic noise via two synergistic modules. The first Diffusion Depth Module leverages temporal geometric priors to enable sample-efficient training of a conditional diffusion model that captures complex sensor noise distributions, while the second Noise Grafting Module preserves metric accuracy during perceptual artifact injection. With only raw depth inputs during deployment, DiffuDepGrasp eliminates computational overhead and achieves a 95.7% average success rate on 12-object grasping with zero-shot transfer and strong generalization to unseen objects.Project website: https://diffudepgrasp.github.io/.", "AI": {"tldr": "DiffuDepGrasp\u662f\u4e00\u4e2a\u53ef\u90e8\u7f72\u7684sim2real\u6846\u67b6\uff0c\u901a\u8fc7\u4ec5\u4f7f\u7528\u6a21\u62df\u6570\u636e\u8fdb\u884c\u7b56\u7565\u8bad\u7ec3\u5b9e\u73b0\u96f6\u6837\u672c\u8f6c\u79fb\uff0c\u6838\u5fc3\u521b\u65b0\u662f\u6269\u6563\u6df1\u5ea6\u751f\u6210\u5668\uff0c\u80fd\u591f\u5408\u6210\u51e0\u4f55\u7eaf\u51c0\u7684\u6a21\u62df\u6df1\u5ea6\u5e76\u5b66\u4e60\u4f20\u611f\u5668\u771f\u5b9e\u566a\u58f0\u3002", "motivation": "\u89e3\u51b3\u5c06\u6a21\u62df\u8bad\u7ec3\u7684\u6df1\u5ea6\u7b56\u7565\u8f6c\u79fb\u5230\u7269\u7406\u673a\u5668\u4eba\u65f6\u7684sim2real\u5dee\u8ddd\u95ee\u9898\uff0c\u7279\u522b\u662f\u6df1\u5ea6\u56fe\u4e2d\u7684\u7a7a\u6d1e\u548c\u566a\u58f0\u7b49\u4f20\u611f\u5668\u4f2a\u5f71\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7684\u6570\u636e\u6548\u7387\u4f4e\u548c\u90e8\u7f72\u590d\u6742\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faDiffuDepGrasp\u6846\u67b6\uff0c\u5305\u542b\u6269\u6563\u6df1\u5ea6\u751f\u6210\u5668\uff0c\u7531\u6269\u6563\u6df1\u5ea6\u6a21\u5757\u548c\u566a\u58f0\u5ac1\u63a5\u6a21\u5757\u7ec4\u6210\u3002\u6269\u6563\u6df1\u5ea6\u6a21\u5757\u5229\u7528\u65f6\u95f4\u51e0\u4f55\u5148\u9a8c\u8bad\u7ec3\u6761\u4ef6\u6269\u6563\u6a21\u578b\u6355\u83b7\u590d\u6742\u4f20\u611f\u5668\u566a\u58f0\u5206\u5e03\uff0c\u566a\u58f0\u5ac1\u63a5\u6a21\u5757\u5728\u6ce8\u5165\u611f\u77e5\u4f2a\u5f71\u65f6\u4fdd\u6301\u5ea6\u91cf\u7cbe\u5ea6\u3002", "result": "\u5728\u90e8\u7f72\u65f6\u4ec5\u9700\u539f\u59cb\u6df1\u5ea6\u8f93\u5165\uff0c\u6d88\u9664\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u572812\u4e2a\u7269\u4f53\u6293\u53d6\u4efb\u52a1\u4e2d\u8fbe\u523095.7%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u8f6c\u79fb\u5e76\u5bf9\u672a\u89c1\u7269\u4f53\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DiffuDepGrasp\u6210\u529f\u89e3\u51b3\u4e86sim2real\u8f6c\u79fb\u4e2d\u7684\u6570\u636e\u6548\u7387\u4f4e\u548c\u90e8\u7f72\u590d\u6742\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u8f6c\u79fb\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u673a\u5668\u4eba\u6293\u53d6\u7b56\u7565\u90e8\u7f72\u3002"}}
{"id": "2511.12941", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12941", "abs": "https://arxiv.org/abs/2511.12941", "authors": ["Chunyong Hu", "Qi Luo", "Jianyun Xu", "Song Wang", "Qiang Li", "Sheng Yang"], "title": "GUIDE: Gaussian Unified Instance Detection for Enhanced Obstacle Perception in Autonomous Driving", "comment": null, "summary": "In the realm of autonomous driving, accurately detecting surrounding obstacles is crucial for effective decision-making. Traditional methods primarily rely on 3D bounding boxes to represent these obstacles, which often fail to capture the complexity of irregularly shaped, real-world objects. To overcome these limitations, we present GUIDE, a novel framework that utilizes 3D Gaussians for instance detection and occupancy prediction. Unlike conventional occupancy prediction methods, GUIDE also offers robust tracking capabilities. Our framework employs a sparse representation strategy, using Gaussian-to-Voxel Splatting to provide fine-grained, instance-level occupancy data without the computational demands associated with dense voxel grids. Experimental validation on the nuScenes dataset demonstrates GUIDE's performance, with an instance occupancy mAP of 21.61, marking a 50\\% improvement over existing methods, alongside competitive tracking capabilities. GUIDE establishes a new benchmark in autonomous perception systems, effectively combining precision with computational efficiency to better address the complexities of real-world driving environments.", "AI": {"tldr": "GUIDE\u662f\u4e00\u4e2a\u57fa\u4e8e3D\u9ad8\u65af\u7684\u65b0\u578b\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u4f8b\u68c0\u6d4b\u548c\u5360\u636e\u9884\u6d4b\uff0c\u76f8\u6bd4\u4f20\u7edf3D\u8fb9\u754c\u6846\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u5904\u7406\u4e0d\u89c4\u5219\u5f62\u72b6\u7269\u4f53\uff0c\u5e76\u5177\u5907\u8ddf\u8e2a\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u9a7e\u9a76\u969c\u788d\u7269\u68c0\u6d4b\u4e3b\u8981\u4f9d\u8d563D\u8fb9\u754c\u6846\uff0c\u65e0\u6cd5\u51c6\u786e\u8868\u793a\u73b0\u5b9e\u4e16\u754c\u4e2d\u4e0d\u89c4\u5219\u5f62\u72b6\u7269\u4f53\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u91c7\u75283D\u9ad8\u65af\u8fdb\u884c\u7a00\u758f\u8868\u793a\uff0c\u901a\u8fc7\u9ad8\u65af\u5230\u4f53\u7d20\u6295\u5f71(Gaussian-to-Voxel Splatting)\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u5b9e\u4f8b\u7ea7\u5360\u636e\u6570\u636e\uff0c\u907f\u514d\u5bc6\u96c6\u4f53\u7d20\u7f51\u683c\u7684\u8ba1\u7b97\u8d1f\u62c5\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u5b9e\u4f8b\u5360\u636emAP\u8fbe\u523021.61\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534750%\uff0c\u540c\u65f6\u5177\u5907\u7ade\u4e89\u529b\u7684\u8ddf\u8e2a\u80fd\u529b\u3002", "conclusion": "GUIDE\u5728\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u4e2d\u5efa\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u6709\u6548\u7ed3\u5408\u4e86\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u80fd\u66f4\u597d\u5730\u5e94\u5bf9\u73b0\u5b9e\u9a7e\u9a76\u73af\u5883\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2511.12972", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12972", "abs": "https://arxiv.org/abs/2511.12972", "authors": ["Siddarth Narasimhan", "Matthew Lisondra", "Haitong Wang", "Goldie Nejat"], "title": "SplatSearch: Instance Image Goal Navigation for Mobile Robots using 3D Gaussian Splatting and Diffusion Models", "comment": "Project Page: https://splat-search.github.io/", "summary": "The Instance Image Goal Navigation (IIN) problem requires mobile robots deployed in unknown environments to search for specific objects or people of interest using only a single reference goal image of the target. This problem can be especially challenging when: 1) the reference image is captured from an arbitrary viewpoint, and 2) the robot must operate with sparse-view scene reconstructions. In this paper, we address the IIN problem, by introducing SplatSearch, a novel architecture that leverages sparse-view 3D Gaussian Splatting (3DGS) reconstructions. SplatSearch renders multiple viewpoints around candidate objects using a sparse online 3DGS map, and uses a multi-view diffusion model to complete missing regions of the rendered images, enabling robust feature matching against the goal image. A novel frontier exploration policy is introduced which uses visual context from the synthesized viewpoints with semantic context from the goal image to evaluate frontier locations, allowing the robot to prioritize frontiers that are semantically and visually relevant to the goal image. Extensive experiments in photorealistic home and real-world environments validate the higher performance of SplatSearch against current state-of-the-art methods in terms of Success Rate and Success Path Length. An ablation study confirms the design choices of SplatSearch.", "AI": {"tldr": "SplatSearch\u662f\u4e00\u79cd\u89e3\u51b3\u5b9e\u4f8b\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u95ee\u9898\u7684\u65b0\u67b6\u6784\uff0c\u5229\u7528\u7a00\u758f\u89c6\u56fe3D\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\uff0c\u901a\u8fc7\u591a\u89c6\u56fe\u6269\u6563\u6a21\u578b\u8865\u5168\u6e32\u67d3\u56fe\u50cf\uff0c\u5e76\u7ed3\u5408\u8bed\u4e49\u4e0a\u4e0b\u6587\u8fdb\u884c\u524d\u6cbf\u63a2\u7d22\u3002", "motivation": "\u89e3\u51b3\u5b9e\u4f8b\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u95ee\u9898\u4e2d\u7684\u6311\u6218\uff1a\u53c2\u8003\u56fe\u50cf\u89c6\u89d2\u4efb\u610f\u4e14\u673a\u5668\u4eba\u5fc5\u987b\u5728\u7a00\u758f\u89c6\u56fe\u573a\u666f\u91cd\u5efa\u4e0b\u64cd\u4f5c\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u5728\u7ebf3DGS\u5730\u56fe\u6e32\u67d3\u5019\u9009\u5bf9\u8c61\u5468\u56f4\u591a\u4e2a\u89c6\u89d2\uff0c\u7528\u591a\u89c6\u56fe\u6269\u6563\u6a21\u578b\u8865\u5168\u7f3a\u5931\u533a\u57df\uff0c\u5b9e\u73b0\u4e0e\u76ee\u6807\u56fe\u50cf\u7684\u9c81\u68d2\u7279\u5f81\u5339\u914d\uff1b\u5f15\u5165\u65b0\u9896\u7684\u524d\u6cbf\u63a2\u7d22\u7b56\u7565\uff0c\u7ed3\u5408\u5408\u6210\u89c6\u89d2\u7684\u89c6\u89c9\u4e0a\u4e0b\u6587\u548c\u76ee\u6807\u56fe\u50cf\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u6765\u8bc4\u4f30\u524d\u6cbf\u4f4d\u7f6e\u3002", "result": "\u5728\u771f\u5b9e\u611f\u5bb6\u5ead\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SplatSearch\u5728\u6210\u529f\u7387\u548c\u6210\u529f\u8def\u5f84\u957f\u5ea6\u65b9\u9762\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86SplatSearch\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9e\u4f8b\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.12984", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12984", "abs": "https://arxiv.org/abs/2511.12984", "authors": ["Miryeong Park", "Dongjin Cho", "Sanghyun Kim", "Younggun Cho"], "title": "CUTE-Planner: Confidence-aware Uneven Terrain Exploration Planner", "comment": "Accepted in International Conference on Space Robotics 2025", "summary": "Planetary exploration robots must navigate uneven terrain while building reliable maps for space missions. However, most existing methods incorporate traversability constraints but may not handle high uncertainty in elevation estimates near complex features like craters, do not consider exploration strategies for uncertainty reduction, and typically fail to address how elevation uncertainty affects navigation safety and map quality. To address the problems, we propose a framework integrating safe path generation, adaptive confidence updates, and confidence-aware exploration strategies. Using Kalman-based elevation estimation, our approach generates terrain traversability and confidence scores, then incorporates them into Graph-Based exploration Planner (GBP) to prioritize exploration of traversable low-confidence regions. We evaluate our framework through simulated lunar experiments using a novel low-confidence region ratio metric, achieving 69% uncertainty reduction compared to baseline GBP. In terms of mission success rate, our method achieves 100% while baseline GBP achieves 0%, demonstrating improvements in exploration safety and map reliability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210\u5b89\u5168\u8def\u5f84\u751f\u6210\u3001\u81ea\u9002\u5e94\u7f6e\u4fe1\u5ea6\u66f4\u65b0\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u63a2\u7d22\u7b56\u7565\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u884c\u661f\u63a2\u6d4b\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e2d\u7684\u5bfc\u822a\u548c\u5efa\u56fe\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u9668\u77f3\u5751\u7b49\u590d\u6742\u7279\u5f81\u9644\u8fd1\u7684\u9ad8\u7a0b\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u51cf\u5c11\u7684\u63a2\u7d22\u7b56\u7565\uff0c\u4e14\u672a\u5145\u5206\u8003\u8651\u9ad8\u7a0b\u4e0d\u786e\u5b9a\u6027\u5bf9\u5bfc\u822a\u5b89\u5168\u548c\u5730\u56fe\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u9ad8\u7a0b\u4f30\u8ba1\u65b9\u6cd5\u751f\u6210\u5730\u5f62\u53ef\u901a\u884c\u6027\u548c\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u5c06\u5176\u6574\u5408\u5230\u57fa\u4e8e\u56fe\u7684\u63a2\u7d22\u89c4\u5212\u5668\u4e2d\uff0c\u4f18\u5148\u63a2\u7d22\u53ef\u901a\u884c\u7684\u4f4e\u7f6e\u4fe1\u5ea6\u533a\u57df\u3002", "result": "\u5728\u6a21\u62df\u6708\u7403\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e8669%\u7684\u4e0d\u786e\u5b9a\u6027\u51cf\u5c11\uff0c\u4efb\u52a1\u6210\u529f\u7387\u4ece0%\u63d0\u5347\u5230100%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u63a2\u7d22\u5b89\u5168\u6027\u548c\u5730\u56fe\u53ef\u9760\u6027\uff0c\u4e3a\u884c\u661f\u63a2\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5bfc\u822a\u548c\u5efa\u56fe\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13042", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13042", "abs": "https://arxiv.org/abs/2511.13042", "authors": ["Yong Li", "Hui Cheng"], "title": "APP: A* Post-Processing Algorithm for Robots with Bidirectional Shortcut and Path Perturbation", "comment": null, "summary": "Paths generated by A* and other graph-search-based planners are widely used in the robotic field. Due to the restricted node-expansion directions, the resulting paths are usually not the shortest. Besides, unnecessary heading changes, or zig-zag patterns, exist even when no obstacle is nearby, which is inconsistent with the human intuition that the path segments should be straight in wide-open space due to the absence of obstacles. This article puts forward a general and systematic post-processing algorithm for A* and other graph-search-based planners. The A* post-processing algorithm, called APP, is developed based on the costmap, which is widely used in commercial service robots. First, a bidirectional vertices reduction algorithm is proposed to tackle the asymm- etry of the path and the environments. During the forward and backward vertices reduction, a thorough shortcut strategy is put forward to improve the path-shortening performance and avoid unnecessary heading changes. Second, an iterative path perturbation algorithm is adopted to locally reduce the number of unnecessary heading changes and improve the path smooth- ness. Comparative experiments are then carried out to validate the superiority of the proposed method. Quantitative performance indexes show that APP outperforms the existing methods in planning time, path length as well as the number of unnecessary heading changes. Finally, field navigation experiments are carried out to verify the practicability of APP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAPP\u7684A*\u8def\u5f84\u89c4\u5212\u540e\u5904\u7406\u7b97\u6cd5\uff0c\u901a\u8fc7\u53cc\u5411\u9876\u70b9\u7f29\u51cf\u548c\u8def\u5f84\u6270\u52a8\u6765\u4f18\u5316\u8def\u5f84\u957f\u5ea6\u548c\u5e73\u6ed1\u5ea6\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8f6c\u5411\u3002", "motivation": "A*\u7b49\u56fe\u641c\u7d22\u89c4\u5212\u5668\u751f\u6210\u7684\u8def\u5f84\u901a\u5e38\u4e0d\u662f\u6700\u77ed\u7684\uff0c\u4e14\u5728\u65e0\u969c\u788d\u7269\u533a\u57df\u5b58\u5728\u4e0d\u5fc5\u8981\u7684\u8f6c\u5411\u548c\u952f\u9f7f\u6a21\u5f0f\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u76f4\u89c9\u4e0d\u7b26\u3002", "method": "\u57fa\u4e8e\u6210\u672c\u56fe\u5f00\u53d1APP\u7b97\u6cd5\uff1a1\uff09\u53cc\u5411\u9876\u70b9\u7f29\u51cf\u7b97\u6cd5\u5904\u7406\u8def\u5f84\u548c\u73af\u5883\u4e0d\u5bf9\u79f0\u6027\uff1b2\uff09\u5f7b\u5e95\u6377\u5f84\u7b56\u7565\u6539\u8fdb\u8def\u5f84\u7f29\u77ed\u6027\u80fd\uff1b3\uff09\u8fed\u4ee3\u8def\u5f84\u6270\u52a8\u7b97\u6cd5\u51cf\u5c11\u4e0d\u5fc5\u8981\u8f6c\u5411\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAPP\u5728\u89c4\u5212\u65f6\u95f4\u3001\u8def\u5f84\u957f\u5ea6\u548c\u4e0d\u5fc5\u8981\u8f6c\u5411\u6b21\u6570\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u73b0\u573a\u5bfc\u822a\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "APP\u7b97\u6cd5\u80fd\u6709\u6548\u4f18\u5316A*\u7b49\u89c4\u5212\u5668\u751f\u6210\u7684\u8def\u5f84\uff0c\u63d0\u9ad8\u8def\u5f84\u8d28\u91cf\u548c\u5e73\u6ed1\u5ea6\uff0c\u9002\u7528\u4e8e\u5546\u4e1a\u670d\u52a1\u673a\u5668\u4eba\u3002"}}
{"id": "2511.13048", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13048", "abs": "https://arxiv.org/abs/2511.13048", "authors": ["Yong Li", "Hui Cheng"], "title": "Unidirectional-Road-Network-Based Global Path Planning for Cleaning Robots in Semi-Structured Environments", "comment": "2023 IEEE International Conference on Robotics and Automation (ICRA)", "summary": "Practical global path planning is critical for commercializing cleaning robots working in semi-structured environments. In the literature, global path planning methods for free space usually focus on path length and neglect the traffic rule constraints of the environments, which leads to high-frequency re-planning and increases collision risks. In contrast, those for structured environments are developed mainly by strictly complying with the road network representing the traffic rule constraints, which may result in an overlong path that hinders the overall navigation efficiency. This article proposes a general and systematic approach to improve global path planning performance in semi-structured environments. A unidirectional road network is built to represent the traffic constraints in semi-structured environments and a hybrid strategy is proposed to achieve a guaranteed planning result.Cutting across the road at the starting and the goal points are allowed to achieve a shorter path. Especially, a two-layer potential map is proposed to achieve a guaranteed performance when the starting and the goal points are in complex intersections. Comparative experiments are carried out to validate the effectiveness of the proposed method. Quantitative experimental results show that, compared with the state-of-art, the proposed method guarantees a much better balance between path length and the consistency with the road network.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u534a\u7ed3\u6784\u5316\u73af\u5883\u6e05\u6d01\u673a\u5668\u4eba\u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u5355\u5411\u9053\u8def\u7f51\u7edc\u8868\u793a\u4ea4\u901a\u7ea6\u675f\uff0c\u91c7\u7528\u6df7\u5408\u7b56\u7565\u4fdd\u8bc1\u89c4\u5212\u7ed3\u679c\uff0c\u5141\u8bb8\u5728\u8d77\u70b9\u548c\u7ec8\u70b9\u7a7f\u8d8a\u9053\u8def\u4ee5\u83b7\u5f97\u66f4\u77ed\u8def\u5f84\uff0c\u7279\u522b\u8bbe\u8ba1\u4e86\u53cc\u5c42\u52bf\u80fd\u56fe\u5904\u7406\u590d\u6742\u4ea4\u53c9\u53e3\u60c5\u51b5\u3002", "motivation": "\u73b0\u6709\u81ea\u7531\u7a7a\u95f4\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5ffd\u89c6\u4ea4\u901a\u89c4\u5219\u7ea6\u675f\u5bfc\u81f4\u9891\u7e41\u91cd\u65b0\u89c4\u5212\u548c\u78b0\u649e\u98ce\u9669\uff0c\u800c\u7ed3\u6784\u5316\u73af\u5883\u8def\u5f84\u89c4\u5212\u4e25\u683c\u9075\u5faa\u9053\u8def\u7f51\u7edc\u5bfc\u81f4\u8def\u5f84\u8fc7\u957f\u5f71\u54cd\u5bfc\u822a\u6548\u7387\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5728\u534a\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5e73\u8861\u8def\u5f84\u957f\u5ea6\u548c\u4ea4\u901a\u89c4\u5219\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u5355\u5411\u9053\u8def\u7f51\u7edc\u8868\u793a\u4ea4\u901a\u7ea6\u675f\uff0c\u63d0\u51fa\u6df7\u5408\u7b56\u7565\u4fdd\u8bc1\u89c4\u5212\u7ed3\u679c\uff0c\u5141\u8bb8\u8d77\u70b9\u548c\u7ec8\u70b9\u7a7f\u8d8a\u9053\u8def\uff0c\u7279\u522b\u8bbe\u8ba1\u53cc\u5c42\u52bf\u80fd\u56fe\u5904\u7406\u590d\u6742\u4ea4\u53c9\u53e3\uff0c\u5b9e\u73b0\u6027\u80fd\u4fdd\u8bc1\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6280\u672f\uff0c\u8be5\u65b9\u6cd5\u5728\u8def\u5f84\u957f\u5ea6\u548c\u9053\u8def\u7f51\u7edc\u4e00\u81f4\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u5b9a\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u534a\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u7cfb\u7edf\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u6539\u5584\u4e86\u8def\u5f84\u89c4\u5212\u6027\u80fd\u3002"}}
{"id": "2511.13071", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13071", "abs": "https://arxiv.org/abs/2511.13071", "authors": ["Michal Levin", "Itzik Klein"], "title": "Orientation-Free Neural Network-Based Bias Estimation for Low-Cost Stationary Accelerometers", "comment": "22 pages, 10 figures", "summary": "Low-cost micro-electromechanical accelerometers are widely used in navigation, robotics, and consumer devices for motion sensing and position estimation. However, their performance is often degraded by bias errors. To eliminate deterministic bias terms a calibration procedure is applied under stationary conditions. It requires accelerom- eter leveling or complex orientation-dependent calibration procedures. To overcome those requirements, in this paper we present a model-free learning-based calibration method that estimates accelerometer bias under stationary conditions, without requiring knowledge of the sensor orientation and without the need to rotate the sensors. The proposed approach provides a fast, practical, and scalable solution suitable for rapid field deployment. Experimental validation on a 13.39-hour dataset collected from six accelerometers shows that the proposed method consistently achieves error levels more than 52% lower than traditional techniques. On a broader scale, this work contributes to the advancement of accurate calibration methods in orientation-free scenarios. As a consequence, it improves the reliability of low-cost inertial sensors in diverse scientific and industrial applications and eliminates the need for leveled calibration.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u4f20\u611f\u5668\u65b9\u5411\u4fe1\u606f\u7684\u6a21\u578b\u65e0\u5173\u5b66\u4e60\u6821\u51c6\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u9759\u6b62\u6761\u4ef6\u4e0b\u4f30\u8ba1\u52a0\u901f\u5ea6\u8ba1\u504f\u7f6e\uff0c\u65e0\u9700\u65cb\u8f6c\u4f20\u611f\u5668\u5373\u53ef\u5b9e\u73b0\u5feb\u901f\u73b0\u573a\u90e8\u7f72\u3002", "motivation": "\u4f4e\u6210\u672cMEMS\u52a0\u901f\u5ea6\u8ba1\u5728\u5bfc\u822a\u3001\u673a\u5668\u4eba\u7b49\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u6027\u80fd\u5e38\u53d7\u504f\u7f6e\u8bef\u5dee\u5f71\u54cd\u3002\u4f20\u7edf\u6821\u51c6\u65b9\u6cd5\u9700\u8981\u52a0\u901f\u5ea6\u8ba1\u8c03\u5e73\u6216\u590d\u6742\u7684\u5b9a\u5411\u76f8\u5173\u7a0b\u5e8f\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u5b66\u4e60\u6821\u51c6\u65b9\u6cd5\uff0c\u5728\u9759\u6b62\u6761\u4ef6\u4e0b\u4f30\u8ba1\u52a0\u901f\u5ea6\u8ba1\u504f\u7f6e\uff0c\u65e0\u9700\u4f20\u611f\u5668\u65b9\u5411\u4fe1\u606f\uff0c\u4e5f\u65e0\u9700\u65cb\u8f6c\u4f20\u611f\u5668\u3002", "result": "\u572813.39\u5c0f\u65f6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u6280\u672f\u8bef\u5dee\u964d\u4f4e52%\u4ee5\u4e0a\uff0c\u63d0\u4f9b\u5feb\u901f\u3001\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63a8\u8fdb\u4e86\u65e0\u5b9a\u5411\u573a\u666f\u4e0b\u7684\u7cbe\u786e\u6821\u51c6\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u4f4e\u6210\u672c\u60ef\u6027\u4f20\u611f\u5668\u5728\u79d1\u5b66\u548c\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u6d88\u9664\u4e86\u8c03\u5e73\u6821\u51c6\u7684\u9700\u6c42\u3002"}}
{"id": "2511.13096", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13096", "abs": "https://arxiv.org/abs/2511.13096", "authors": ["Guy Damari", "Itzik Klein"], "title": "ResAlignNet: A Data-Driven Approach for INS/DVL Alignment", "comment": null, "summary": "Autonomous underwater vehicles rely on precise navigation systems that combine the inertial navigation system and the Doppler velocity log for successful missions in challenging environments where satellite navigation is unavailable. The effectiveness of this integration critically depends on accurate alignment between the sensor reference frames. Standard model-based alignment methods between these sensor systems suffer from lengthy convergence times, dependence on prescribed motion patterns, and reliance on external aiding sensors, significantly limiting operational flexibility. To address these limitations, this paper presents ResAlignNet, a data-driven approach using the 1D ResNet-18 architecture that transforms the alignment problem into deep neural network optimization, operating as an in-situ solution that requires only sensors on board without external positioning aids or complex vehicle maneuvers, while achieving rapid convergence in seconds. Additionally, the approach demonstrates the learning capabilities of Sim2Real transfer, enabling training in synthetic data while deploying in operational sensor measurements. Experimental validation using the Snapir autonomous underwater vehicle demonstrates that ResAlignNet achieves alignment accuracy within 0.8\u00b0 using only 25 seconds of data collection, representing a 65\\% reduction in convergence time compared to standard velocity-based methods. The trajectory-independent solution eliminates motion pattern requirements and enables immediate vehicle deployment without lengthy pre-mission procedures, advancing underwater navigation capabilities through robust sensor-agnostic alignment that scales across different operational scenarios and sensor specifications.", "AI": {"tldr": "ResAlignNet\u4f7f\u75281D ResNet-18\u67b6\u6784\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5c06\u4f20\u611f\u5668\u5bf9\u51c6\u95ee\u9898\u8f6c\u5316\u4e3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\uff0c\u65e0\u9700\u5916\u90e8\u5b9a\u4f4d\u8f85\u52a9\u6216\u590d\u6742\u673a\u52a8\uff0c\u4ec5\u970025\u79d2\u6570\u636e\u5373\u53ef\u57280.8\u00b0\u7cbe\u5ea6\u5185\u5b8c\u6210\u5bf9\u51c6\uff0c\u6536\u655b\u65f6\u95f4\u6bd4\u6807\u51c6\u65b9\u6cd5\u51cf\u5c1165%\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684INS/DVL\u4f20\u611f\u5668\u5bf9\u51c6\u65b9\u6cd5\u5b58\u5728\u6536\u655b\u65f6\u95f4\u957f\u3001\u4f9d\u8d56\u9884\u8bbe\u8fd0\u52a8\u6a21\u5f0f\u548c\u5916\u90e8\u8f85\u52a9\u4f20\u611f\u5668\u7b49\u95ee\u9898\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u6c34\u4e0b\u81ea\u4e3b\u822a\u884c\u5668\u7684\u64cd\u4f5c\u7075\u6d3b\u6027\u3002", "method": "\u91c7\u75281D ResNet-18\u67b6\u6784\u7684\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\uff0c\u5c06\u4f20\u611f\u5668\u5bf9\u51c6\u95ee\u9898\u8f6c\u5316\u4e3a\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\uff0c\u652f\u6301Sim2Real\u8fc1\u79fb\u5b66\u4e60\uff0c\u53ef\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u5e76\u5728\u5b9e\u9645\u4f20\u611f\u5668\u6570\u636e\u4e0a\u90e8\u7f72\u3002", "result": "\u5728Snapir\u6c34\u4e0b\u81ea\u4e3b\u822a\u884c\u5668\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cResAlignNet\u4ec5\u970025\u79d2\u6570\u636e\u6536\u96c6\u5373\u53ef\u5b9e\u73b00.8\u00b0\u4ee5\u5185\u7684\u5bf9\u51c6\u7cbe\u5ea6\uff0c\u6536\u655b\u65f6\u95f4\u6bd4\u6807\u51c6\u901f\u5ea6\u65b9\u6cd5\u51cf\u5c1165%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u8f68\u8ff9\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6d88\u9664\u4e86\u8fd0\u52a8\u6a21\u5f0f\u8981\u6c42\uff0c\u65e0\u9700\u5197\u957f\u7684\u4efb\u52a1\u524d\u7a0b\u5e8f\u5373\u53ef\u7acb\u5373\u90e8\u7f72\uff0c\u901a\u8fc7\u9c81\u68d2\u7684\u4f20\u611f\u5668\u65e0\u5173\u5bf9\u51c6\u63d0\u5347\u4e86\u6c34\u4e0b\u5bfc\u822a\u80fd\u529b\u3002"}}
{"id": "2511.13100", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13100", "abs": "https://arxiv.org/abs/2511.13100", "authors": ["Xuecheng Chen", "Jingao Xu", "Wenhua Ding", "Haoyang Wang", "Xinyu Luo", "Ruiyang Duan", "Jialong Chen", "Xueqian Wang", "Yunhao Liu", "Xinlei Chen"], "title": "Count Every Rotation and Every Rotation Counts: Exploring Drone Dynamics via Propeller Sensing", "comment": null, "summary": "As drone-based applications proliferate, paramount contactless sensing of airborne drones from the ground becomes indispensable. This work demonstrates concentrating on propeller rotational speed will substantially improve drone sensing performance and proposes an event-camera-based solution, \\sysname. \\sysname features two components: \\textit{Count Every Rotation} achieves accurate, real-time propeller speed estimation by mitigating ultra-high sensitivity of event cameras to environmental noise. \\textit{Every Rotation Counts} leverages these speeds to infer both internal and external drone dynamics. Extensive evaluations in real-world drone delivery scenarios show that \\sysname achieves a sensing latency of 3$ms$ and a rotational speed estimation error of merely 0.23\\%. Additionally, \\sysname infers drone flight commands with 96.5\\% precision and improves drone tracking accuracy by over 22\\% when combined with other sensing modalities. \\textit{ Demo: {\\color{blue}https://eventpro25.github.io/EventPro/.} }", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u65e0\u4eba\u673a\u87ba\u65cb\u6868\u8f6c\u901f\u611f\u77e5\u7cfb\u7edfEventPro\uff0c\u901a\u8fc7\u7cbe\u786e\u6d4b\u91cf\u87ba\u65cb\u6868\u8f6c\u901f\u6765\u63d0\u5347\u65e0\u4eba\u673a\u611f\u77e5\u6027\u80fd\uff0c\u5728\u771f\u5b9e\u65e0\u4eba\u673a\u914d\u9001\u573a\u666f\u4e2d\u5b9e\u73b0\u4e863ms\u7684\u4f4e\u5ef6\u8fdf\u548c0.23%\u7684\u8f6c\u901f\u4f30\u8ba1\u8bef\u5dee\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u5e94\u7528\u6fc0\u589e\uff0c\u4ece\u5730\u9762\u8fdb\u884c\u975e\u63a5\u89e6\u5f0f\u65e0\u4eba\u673a\u611f\u77e5\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u4e13\u6ce8\u4e8e\u87ba\u65cb\u6868\u8f6c\u901f\u611f\u77e5\u80fd\u663e\u8457\u63d0\u5347\u65e0\u4eba\u673a\u611f\u77e5\u6027\u80fd\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1aCount Every Rotation\u901a\u8fc7\u51cf\u8f7b\u4e8b\u4ef6\u76f8\u673a\u5bf9\u73af\u5883\u566a\u58f0\u7684\u8d85\u9ad8\u654f\u611f\u6027\uff0c\u5b9e\u73b0\u51c6\u786e\u3001\u5b9e\u65f6\u7684\u87ba\u65cb\u6868\u8f6c\u901f\u4f30\u8ba1\uff1bEvery Rotation Counts\u5229\u7528\u8fd9\u4e9b\u8f6c\u901f\u63a8\u65ad\u65e0\u4eba\u673a\u7684\u5185\u5916\u52a8\u529b\u5b66\u7279\u6027\u3002", "result": "\u5728\u771f\u5b9e\u65e0\u4eba\u673a\u914d\u9001\u573a\u666f\u8bc4\u4f30\u4e2d\uff0c\u7cfb\u7edf\u8fbe\u52303ms\u7684\u611f\u77e5\u5ef6\u8fdf\u548c0.23%\u7684\u8f6c\u901f\u4f30\u8ba1\u8bef\u5dee\uff0c\u80fd\u591f\u4ee596.5%\u7684\u7cbe\u5ea6\u63a8\u65ad\u65e0\u4eba\u673a\u98de\u884c\u6307\u4ee4\uff0c\u4e0e\u5176\u4ed6\u611f\u77e5\u6a21\u6001\u7ed3\u5408\u65f6\u63d0\u5347\u65e0\u4eba\u673a\u8ddf\u8e2a\u7cbe\u5ea6\u8d85\u8fc722%\u3002", "conclusion": "\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u87ba\u65cb\u6868\u8f6c\u901f\u611f\u77e5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u65e0\u4eba\u673a\u611f\u77e5\u6027\u80fd\uff0c\u4e3a\u65e0\u4eba\u673a\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u975e\u63a5\u89e6\u5f0f\u611f\u77e5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13120", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13120", "abs": "https://arxiv.org/abs/2511.13120", "authors": ["Trevor Exley", "Anderson Brazil Nardin", "Petr Trunin", "Diana Cafiso", "Lucia Beccai"], "title": "Monolithic Units: Actuation, Sensing, and Simulation for Integrated Soft Robot Design", "comment": "8 pages, 6 figures, 1 algorithm, 1 table", "summary": "This work introduces the Monolithic Unit (MU), an actuator-lattice-sensor building block for soft robotics. The MU integrates pneumatic actuation, a compliant lattice envelope, and candidate sites for optical waveguide sensing into a single printed body. In order to study reproducibility and scalability, a parametric design framework establishes deterministic rules linking actuator chamber dimensions to lattice unit cell size. Experimental homogenization of lattice specimens provides effective material properties for finite element simulation. Within this simulation environment, sensor placement is treated as a discrete optimization problem, where a finite set of candidate waveguide paths derived from lattice nodes is evaluated by introducing local stiffening, and the configuration minimizing deviation from baseline mechanical response is selected. Optimized models are fabricated and experimentally characterized, validating the preservation of mechanical performance while enabling embedded sensing. The workflow is further extended to scaled units and a two-finger gripper, demonstrating generality of the MU concept. This approach advances monolithic soft robotic design by combining reproducible co-design rules with simulation-informed sensor integration.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5355\u4f53\u5355\u5143(MU)\u6982\u5ff5\uff0c\u5c06\u6c14\u52a8\u9a71\u52a8\u3001\u67d4\u6027\u6676\u683c\u5916\u58f3\u548c\u5149\u5b66\u6ce2\u5bfc\u4f20\u611f\u96c6\u6210\u5230\u5355\u4e00\u6253\u5370\u4f53\u4e2d\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u8bbe\u8ba1\u6846\u67b6\u548c\u4eff\u771f\u4f18\u5316\u5b9e\u73b0\u4f20\u611f\u96c6\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u673a\u68b0\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u96c6\u9a71\u52a8\u3001\u7ed3\u6784\u548c\u4f20\u611f\u4e8e\u4e00\u4f53\u7684\u5355\u4f53\u8f6f\u4f53\u673a\u5668\u4eba\u6784\u5efa\u6a21\u5757\uff0c\u89e3\u51b3\u8f6f\u4f53\u673a\u5668\u4eba\u4e2d\u9a71\u52a8\u3001\u4f20\u611f\u548c\u7ed3\u6784\u5206\u79bb\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u53ef\u91cd\u590d\u5236\u9020\u548c\u89c4\u6a21\u5316\u5e94\u7528\u3002", "method": "\u91c7\u7528\u53c2\u6570\u5316\u8bbe\u8ba1\u6846\u67b6\u5efa\u7acb\u9a71\u52a8\u8154\u5c3a\u5bf8\u4e0e\u6676\u683c\u5355\u5143\u5c3a\u5bf8\u7684\u786e\u5b9a\u6027\u5173\u7cfb\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5747\u8d28\u5316\u83b7\u53d6\u6750\u6599\u5c5e\u6027\u7528\u4e8e\u6709\u9650\u5143\u4eff\u771f\uff0c\u5c06\u4f20\u611f\u5668\u5e03\u7f6e\u4f5c\u4e3a\u79bb\u6563\u4f18\u5316\u95ee\u9898\u5904\u7406\uff0c\u9009\u62e9\u6700\u5c0f\u5316\u673a\u68b0\u54cd\u5e94\u504f\u5dee\u7684\u6ce2\u5bfc\u8def\u5f84\u914d\u7f6e\u3002", "result": "\u4f18\u5316\u6a21\u578b\u7ecf\u8fc7\u5236\u9020\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5728\u4fdd\u6301\u673a\u68b0\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5d4c\u5165\u5f0f\u4f20\u611f\uff0c\u8be5\u5de5\u4f5c\u6d41\u7a0b\u6210\u529f\u6269\u5c55\u5230\u7f29\u653e\u5355\u5143\u548c\u53cc\u6307\u5939\u722a\uff0c\u8bc1\u660e\u4e86MU\u6982\u5ff5\u7684\u901a\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u53ef\u91cd\u590d\u7684\u534f\u540c\u8bbe\u8ba1\u89c4\u5219\u548c\u57fa\u4e8e\u4eff\u771f\u7684\u4f20\u611f\u5668\u96c6\u6210\uff0c\u63a8\u8fdb\u4e86\u5355\u4f53\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u53d1\u5c55\u3002"}}
{"id": "2511.13188", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13188", "abs": "https://arxiv.org/abs/2511.13188", "authors": ["Osama Al Sheikh Ali", "Sotiris Koutsoftas", "Ze Zhang", "Knut Akesson", "Emmanuel Dean"], "title": "Collision-Free Navigation of Mobile Robots via Quadtree-Based Model Predictive Control", "comment": "This paper has been accepted by IEEE SII 2026", "summary": "This paper presents an integrated navigation framework for Autonomous Mobile Robots (AMRs) that unifies environment representation, trajectory generation, and Model Predictive Control (MPC). The proposed approach incorporates a quadtree-based method to generate structured, axis-aligned collision-free regions from occupancy maps. These regions serve as both a basis for developing safe corridors and as linear constraints within the MPC formulation, enabling efficient and reliable navigation without requiring direct obstacle encoding. The complete pipeline combines safe-area extraction, connectivity graph construction, trajectory generation, and B-spline smoothing into one coherent system. Experimental results demonstrate consistent success and superior performance compared to baseline approaches across complex environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u5bfc\u822a\u6846\u67b6\uff0c\u5c06\u73af\u5883\u8868\u793a\u3001\u8f68\u8ff9\u751f\u6210\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7edf\u4e00\u8d77\u6765\uff0c\u4f7f\u7528\u56db\u53c9\u6811\u65b9\u6cd5\u4ece\u5360\u636e\u5730\u56fe\u751f\u6210\u7ed3\u6784\u5316\u3001\u8f74\u5bf9\u9f50\u7684\u65e0\u78b0\u649e\u533a\u57df\u4f5c\u4e3a\u5b89\u5168\u8d70\u5eca\u548cMPC\u7ea6\u675f\u3002", "motivation": "\u4e3a\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u5bfc\u822a\u6846\u67b6\uff0c\u907f\u514d\u76f4\u63a5\u7f16\u7801\u969c\u788d\u7269\uff0c\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u7684\u5bfc\u822a\u3002", "method": "\u7ed3\u5408\u5b89\u5168\u533a\u57df\u63d0\u53d6\u3001\u8fde\u901a\u56fe\u6784\u5efa\u3001\u8f68\u8ff9\u751f\u6210\u548cB\u6837\u6761\u5e73\u6ed1\u7684\u5b8c\u6574\u6d41\u6c34\u7ebf\uff0c\u4f7f\u7528\u56db\u53c9\u6811\u751f\u6210\u8f74\u5bf9\u9f50\u65e0\u78b0\u649e\u533a\u57df\u4f5c\u4e3aMPC\u7ebf\u6027\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u590d\u6742\u73af\u5883\u4e2d\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u4e00\u81f4\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u96c6\u6210\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u7684\u5bfc\u822a\uff0c\u65e0\u9700\u76f4\u63a5\u5904\u7406\u969c\u788d\u7269\u7f16\u7801\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2511.13207", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13207", "abs": "https://arxiv.org/abs/2511.13207", "authors": ["Cheng Peng", "Zhenzhe Zhang", "Cheng Chi", "Xiaobao Wei", "Yanhao Zhang", "Heng Wang", "Pengwei Wang", "Zhongyuan Wang", "Jing Liu", "Shanghang Zhang"], "title": "PIGEON: VLM-Driven Object Navigation via Points of Interest Selection", "comment": null, "summary": "Navigating to a specified object in an unknown environment is a fundamental yet challenging capability of embodied intelligence. However, current methods struggle to balance decision frequency with intelligence, resulting in decisions lacking foresight or discontinuous actions. In this work, we propose PIGEON: Point of Interest Guided Exploration for Object Navigation with VLM, maintaining a lightweight and semantically aligned snapshot memory during exploration as semantic input for the exploration strategy. We use a large Visual-Language Model (VLM), named PIGEON-VL, to select Points of Interest (PoI) formed during exploration and then employ a lower-level planner for action output, increasing the decision frequency. Additionally, this PoI-based decision-making enables the generation of Reinforcement Learning with Verifiable Reward (RLVR) data suitable for simulators. Experiments on classic object navigation benchmarks demonstrate that our zero-shot transfer method achieves state-of-the-art performance, while RLVR further enhances the model's semantic guidance capabilities, enabling deep reasoning during real-time navigation.", "AI": {"tldr": "PIGEON\u662f\u4e00\u79cd\u57fa\u4e8e\u5174\u8da3\u70b9\u5f15\u5bfc\u7684\u7269\u4f53\u5bfc\u822a\u65b9\u6cd5\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9009\u62e9\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u7684\u5174\u8da3\u70b9\uff0c\u901a\u8fc7\u5206\u5c42\u89c4\u5212\u63d0\u9ad8\u51b3\u7b56\u9891\u7387\uff0c\u5e76\u5728\u96f6\u6837\u672c\u8fc1\u79fb\u548c\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u5956\u52b1\u65b9\u9762\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7269\u4f53\u5bfc\u822a\u65b9\u6cd5\u5728\u51b3\u7b56\u9891\u7387\u4e0e\u667a\u80fd\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u5bfc\u81f4\u51b3\u7b56\u7f3a\u4e4f\u524d\u77bb\u6027\u6216\u52a8\u4f5c\u4e0d\u8fde\u7eed\u3002", "method": "\u63d0\u51faPIGEON\u6846\u67b6\uff0c\u5728\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u7ef4\u62a4\u8f7b\u91cf\u7ea7\u8bed\u4e49\u5bf9\u9f50\u7684\u5feb\u7167\u8bb0\u5fc6\u4f5c\u4e3a\u63a2\u7d22\u7b56\u7565\u7684\u8bed\u4e49\u8f93\u5165\uff0c\u4f7f\u7528PIGEON-VL\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9009\u62e9\u5174\u8da3\u70b9\uff0c\u7136\u540e\u901a\u8fc7\u4f4e\u5c42\u89c4\u5212\u5668\u8f93\u51fa\u52a8\u4f5c\u3002", "result": "\u5728\u7ecf\u5178\u7269\u4f53\u5bfc\u822a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u96f6\u6837\u672c\u8fc1\u79fb\u65b9\u6cd5\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0cRLVR\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u8bed\u4e49\u5f15\u5bfc\u80fd\u529b\u3002", "conclusion": "PIGEON\u65b9\u6cd5\u6210\u529f\u5e73\u8861\u4e86\u51b3\u7b56\u9891\u7387\u4e0e\u667a\u80fd\u6027\uff0c\u5174\u8da3\u70b9\u51b3\u7b56\u673a\u5236\u80fd\u591f\u751f\u6210\u9002\u7528\u4e8e\u6a21\u62df\u5668\u7684\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u5956\u52b1\u6570\u636e\uff0c\u5b9e\u73b0\u5b9e\u65f6\u5bfc\u822a\u4e2d\u7684\u6df1\u5ea6\u63a8\u7406\u3002"}}
{"id": "2511.13216", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13216", "abs": "https://arxiv.org/abs/2511.13216", "authors": ["Chiyun Noh", "Sangwoo Jung", "Hanjun Kim", "Yafei Hu", "Laura Herlant", "Ayoung Kim"], "title": "GaRLILEO: Gravity-aligned Radar-Leg-Inertial Enhanced Odometry", "comment": null, "summary": "Deployment of legged robots for navigating challenging terrains (e.g., stairs, slopes, and unstructured environments) has gained increasing preference over wheel-based platforms. In such scenarios, accurate odometry estimation is a preliminary requirement for stable locomotion, localization, and mapping. Traditional proprioceptive approaches, which rely on leg kinematics sensor modalities and inertial sensing, suffer from irrepressible vertical drift caused by frequent contact impacts, foot slippage, and vibrations, particularly affected by inaccurate roll and pitch estimation. Existing methods incorporate exteroceptive sensors such as LiDAR or cameras. Further enhancement has been introduced by leveraging gravity vector estimation to add additional observations on roll and pitch, thereby increasing the accuracy of vertical pose estimation. However, these approaches tend to degrade in feature-sparse or repetitive scenes and are prone to errors from double-integrated IMU acceleration. To address these challenges, we propose GaRLILEO, a novel gravity-aligned continuous-time radar-leg-inertial odometry framework. GaRLILEO decouples velocity from the IMU by building a continuous-time ego-velocity spline from SoC radar Doppler and leg kinematics information, enabling seamless sensor fusion which mitigates odometry distortion. In addition, GaRLILEO can reliably capture accurate gravity vectors leveraging a novel soft S2-constrained gravity factor, improving vertical pose accuracy without relying on LiDAR or cameras. Evaluated on a self-collected real-world dataset with diverse indoor-outdoor trajectories, GaRLILEO demonstrates state-of-the-art accuracy, particularly in vertical odometry estimation on stairs and slopes. We open-source both our dataset and algorithm to foster further research in legged robot odometry and SLAM. https://garlileo.github.io/GaRLILEO", "AI": {"tldr": "GaRLILEO\u662f\u4e00\u4e2a\u91cd\u529b\u5bf9\u9f50\u7684\u8fde\u7eed\u65f6\u95f4\u96f7\u8fbe-\u817f-\u60ef\u6027\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026IMU\u901f\u5ea6\u5e76\u4f7f\u7528\u96f7\u8fbe\u591a\u666e\u52d2\u548c\u817f\u90e8\u8fd0\u52a8\u5b66\u6784\u5efa\u8fde\u7eed\u65f6\u95f4\u81ea\u901f\u5ea6\u6837\u6761\uff0c\u63d0\u9ad8\u817f\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e2d\u7684\u5782\u76f4\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u817f\u90e8\u8fd0\u52a8\u5b66\u548c\u60ef\u6027\u4f20\u611f\u7684\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u5b58\u5728\u4e0d\u53ef\u6291\u5236\u7684\u5782\u76f4\u6f02\u79fb\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u697c\u68af\u3001\u659c\u5761\u7b49\u590d\u6742\u5730\u5f62\u4e2d\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528LiDAR\u6216\u76f8\u673a\uff0c\u4f46\u5728\u7279\u5f81\u7a00\u758f\u6216\u91cd\u590d\u573a\u666f\u4e2d\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faGaRLILEO\u6846\u67b6\uff0c\u901a\u8fc7\u8fde\u7eed\u65f6\u95f4\u81ea\u901f\u5ea6\u6837\u6761\u89e3\u8026IMU\u901f\u5ea6\uff0c\u878d\u5408\u96f7\u8fbe\u591a\u666e\u52d2\u548c\u817f\u90e8\u8fd0\u52a8\u5b66\u4fe1\u606f\uff0c\u5e76\u4f7f\u7528\u8f6fS2\u7ea6\u675f\u91cd\u529b\u56e0\u5b50\u53ef\u9760\u6355\u83b7\u91cd\u529b\u5411\u91cf\u3002", "result": "\u5728\u81ea\u6536\u96c6\u7684\u771f\u5b9e\u4e16\u754c\u5ba4\u5185\u5916\u8f68\u8ff9\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cGaRLILEO\u5728\u697c\u68af\u548c\u659c\u5761\u4e0a\u7684\u5782\u76f4\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u8fbe\u5230\u6700\u5148\u8fdb\u7cbe\u5ea6\u3002", "conclusion": "GaRLILEO\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56LiDAR\u6216\u76f8\u673a\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u5782\u76f4\u59ff\u6001\u7cbe\u5ea6\uff0c\u4e3a\u817f\u5f0f\u673a\u5668\u4eba\u91cc\u7a0b\u8ba1\u548cSLAM\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13312", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13312", "abs": "https://arxiv.org/abs/2511.13312", "authors": ["Jonas Bode", "Raphael Memmesheimer", "Sven Behnke"], "title": "EL3DD: Extended Latent 3D Diffusion for Language Conditioned Multitask Manipulation", "comment": "10 pages; 2 figures; 1 table. Prprint submitted to the European Robotics Forum 2026", "summary": "Acting in human environments is a crucial capability for general-purpose robots, necessitating a robust understanding of natural language and its application to physical tasks. This paper seeks to harness the capabilities of diffusion models within a visuomotor policy framework that merges visual and textual inputs to generate precise robotic trajectories. By employing reference demonstrations during training, the model learns to execute manipulation tasks specified through textual commands within the robot's immediate environment. The proposed research aims to extend an existing model by leveraging improved embeddings, and adapting techniques from diffusion models for image generation. We evaluate our methods on the CALVIN dataset, proving enhanced performance on various manipulation tasks and an increased long-horizon success rate when multiple tasks are executed in sequence. Our approach reinforces the usefulness of diffusion models and contributes towards general multitask manipulation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\u7684\u6269\u6563\u6a21\u578b\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff0c\u7528\u4e8e\u751f\u6210\u7cbe\u786e\u7684\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u5728CALVIN\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u591a\u4efb\u52a1\u64cd\u4f5c\u6027\u80fd\u7684\u63d0\u5347\u3002", "motivation": "\u8ba9\u673a\u5668\u4eba\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u6267\u884c\u4efb\u52a1\u9700\u8981\u5f3a\u5927\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u7269\u7406\u4efb\u52a1\u6267\u884c\u80fd\u529b\uff0c\u672c\u6587\u65e8\u5728\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u80fd\u529b\u6765\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\u6784\u5efa\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u6846\u67b6\uff0c\u878d\u5408\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\uff0c\u901a\u8fc7\u53c2\u8003\u6f14\u793a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b66\u4e60\u6839\u636e\u6587\u672c\u6307\u4ee4\u5728\u673a\u5668\u4eba\u73af\u5883\u4e2d\u6267\u884c\u64cd\u4f5c\u4efb\u52a1\u3002", "result": "\u5728CALVIN\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u64cd\u4f5c\u4efb\u52a1\u4e0a\u6027\u80fd\u63d0\u5347\uff0c\u591a\u4efb\u52a1\u987a\u5e8f\u6267\u884c\u65f6\u7684\u957f\u65f6\u57df\u6210\u529f\u7387\u589e\u52a0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5f3a\u5316\u4e86\u6269\u6563\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u901a\u7528\u591a\u4efb\u52a1\u64cd\u4f5c\u63d0\u4f9b\u4e86\u8d21\u732e\u3002"}}
{"id": "2511.13327", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13327", "abs": "https://arxiv.org/abs/2511.13327", "authors": ["Juntao Jian", "Yi-Lin Wei", "Chengjie Mou", "Yuhao Lin", "Xing Zhu", "Yujun Shen", "Wei-Shi Zheng", "Ruizhen Hu"], "title": "ZeroDexGrasp: Zero-Shot Task-Oriented Dexterous Grasp Synthesis with Prompt-Based Multi-Stage Semantic Reasoning", "comment": null, "summary": "Task-oriented dexterous grasping holds broad application prospects in robotic manipulation and human-object interaction. However, most existing methods still struggle to generalize across diverse objects and task instructions, as they heavily rely on costly labeled data to ensure task-specific semantic alignment. In this study, we propose \\textbf{ZeroDexGrasp}, a zero-shot task-oriented dexterous grasp synthesis framework integrating Multimodal Large Language Models with grasp refinement to generate human-like grasp poses that are well aligned with specific task objectives and object affordances. Specifically, ZeroDexGrasp employs prompt-based multi-stage semantic reasoning to infer initial grasp configurations and object contact information from task and object semantics, then exploits contact-guided grasp optimization to refine these poses for physical feasibility and task alignment. Experimental results demonstrate that ZeroDexGrasp enables high-quality zero-shot dexterous grasping on diverse unseen object categories and complex task requirements, advancing toward more generalizable and intelligent robotic grasping.", "AI": {"tldr": "ZeroDexGrasp\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u4efb\u52a1\u5bfc\u5411\u7075\u5de7\u6293\u53d6\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6293\u53d6\u4f18\u5316\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u5373\u53ef\u751f\u6210\u7b26\u5408\u4efb\u52a1\u76ee\u6807\u548c\u7269\u4f53\u529f\u80fd\u7684\u7075\u5de7\u6293\u53d6\u59ff\u52bf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u6807\u6ce8\u6570\u636e\u6765\u786e\u4fdd\u4efb\u52a1\u7279\u5b9a\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u4e0d\u540c\u7269\u4f53\u548c\u4efb\u52a1\u6307\u4ee4\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u63d0\u793a\u7684\u591a\u9636\u6bb5\u8bed\u4e49\u63a8\u7406\u63a8\u65ad\u521d\u59cb\u6293\u53d6\u914d\u7f6e\u548c\u7269\u4f53\u63a5\u89e6\u4fe1\u606f\uff0c\u7136\u540e\u5229\u7528\u63a5\u89e6\u5f15\u5bfc\u7684\u6293\u53d6\u4f18\u5316\u6765\u4f18\u5316\u59ff\u52bf\u7684\u7269\u7406\u53ef\u884c\u6027\u548c\u4efb\u52a1\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cZeroDexGrasp\u80fd\u591f\u5728\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u7c7b\u522b\u548c\u590d\u6742\u4efb\u52a1\u9700\u6c42\u4e0a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u96f6\u6837\u672c\u7075\u5de7\u6293\u53d6\u3002", "conclusion": "\u8be5\u6846\u67b6\u671d\u7740\u66f4\u53ef\u6cdb\u5316\u548c\u667a\u80fd\u7684\u673a\u5668\u4eba\u6293\u53d6\u65b9\u5411\u8fc8\u8fdb\u3002"}}
{"id": "2511.13459", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13459", "abs": "https://arxiv.org/abs/2511.13459", "authors": ["Bingkun Huang", "Yuhe Gong", "Zewen Yang", "Tianyu Ren", "Luis Figueredo"], "title": "Contact-Safe Reinforcement Learning with ProMP Reparameterization and Energy Awareness", "comment": null, "summary": "Reinforcement learning (RL) approaches based on Markov Decision Processes (MDPs) are predominantly applied in the robot joint space, often relying on limited task-specific information and partial awareness of the 3D environment. In contrast, episodic RL has demonstrated advantages over traditional MDP-based methods in terms of trajectory consistency, task awareness, and overall performance in complex robotic tasks. Moreover, traditional step-wise and episodic RL methods often neglect the contact-rich information inherent in task-space manipulation, especially considering the contact-safety and robustness. In this work, contact-rich manipulation tasks are tackled using a task-space, energy-safe framework, where reliable and safe task-space trajectories are generated through the combination of Proximal Policy Optimization (PPO) and movement primitives. Furthermore, an energy-aware Cartesian Impedance Controller objective is incorporated within the proposed framework to ensure safe interactions between the robot and the environment. Our experimental results demonstrate that the proposed framework outperforms existing methods in handling tasks on various types of surfaces in 3D environments, achieving high success rates as well as smooth trajectories and energy-safe interactions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4efb\u52a1\u7a7a\u95f4\u548c\u80fd\u91cf\u5b89\u5168\u6846\u67b6\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408PPO\u548c\u8fd0\u52a8\u57fa\u5143\u6765\u5904\u7406\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u786e\u4fdd\u5b89\u5168\u4ea4\u4e92\u548c\u8f68\u8ff9\u5e73\u6ed1\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eMDP\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u5173\u8282\u7a7a\u95f4\u4e2d\u5e94\u7528\uff0c\u7f3a\u4e4f\u5bf93D\u73af\u5883\u7684\u5145\u5206\u8ba4\u77e5\u548c\u4efb\u52a1\u7279\u5b9a\u4fe1\u606f\u3002\u4f20\u7edf\u6b65\u8fdb\u5f0f\u548c\u60c5\u666f\u5f0fRL\u65b9\u6cd5\u4e5f\u5ffd\u89c6\u4e86\u4efb\u52a1\u7a7a\u95f4\u64cd\u4f5c\u4e2d\u4e30\u5bcc\u7684\u63a5\u89e6\u4fe1\u606f\uff0c\u7279\u522b\u662f\u63a5\u89e6\u5b89\u5168\u548c\u9c81\u68d2\u6027\u65b9\u9762\u3002", "method": "\u91c7\u7528\u4efb\u52a1\u7a7a\u95f4\u3001\u80fd\u91cf\u5b89\u5168\u6846\u67b6\uff0c\u7ed3\u5408\u8fd1\u7aef\u7b56\u7565\u4f18\u5316(PPO)\u548c\u8fd0\u52a8\u57fa\u5143\u751f\u6210\u53ef\u9760\u5b89\u5168\u7684\u4efb\u52a1\u7a7a\u95f4\u8f68\u8ff9\uff0c\u5e76\u878d\u5165\u80fd\u91cf\u611f\u77e5\u7684\u7b1b\u5361\u5c14\u963b\u6297\u63a7\u5236\u5668\u76ee\u6807\u4ee5\u786e\u4fdd\u673a\u5668\u4eba\u4e0e\u73af\u5883\u7684\u5b89\u5168\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u6846\u67b6\u57283D\u73af\u5883\u4e2d\u5904\u7406\u5404\u79cd\u8868\u9762\u7c7b\u578b\u7684\u4efb\u52a1\u65f6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6210\u529f\u7387\u3001\u5e73\u6ed1\u8f68\u8ff9\u548c\u80fd\u91cf\u5b89\u5168\u4ea4\u4e92\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4efb\u52a1\u6027\u80fd\u3001\u8f68\u8ff9\u4e00\u81f4\u6027\u548c\u5b89\u5168\u4ea4\u4e92\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2511.13530", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13530", "abs": "https://arxiv.org/abs/2511.13530", "authors": ["Vesna Poprcova", "Iulia Lefter", "Matthias Wieser", "Martijn Warnier", "Frances Brazier"], "title": "Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety", "comment": "Accepted at the Workshop on Benefits of pErsonalization and behAvioral adaptation in assistive Robots (BEAR 2025), held at the IEEE RO-MAN Conference 2025", "summary": "Social anxiety is a prevalent condition that affects interpersonal interactions and social functioning. Recent advances in artificial intelligence and social robotics offer new opportunities to examine social anxiety in the human-robot interaction context. Accurate detection of affective states and behaviours associated with social anxiety requires multimodal datasets, where each signal modality provides complementary insights into its manifestations. However, such datasets remain scarce, limiting progress in both research and applications. To address this, this paper presents a protocol for multimodal dataset collection designed to reflect social anxiety in a human-robot interaction context. The dataset will consist of synchronised audio, video, and physiological recordings acquired from at least 70 participants, grouped according to their level of social anxiety, as they engage in approximately 10-minute interactive Wizard-of-Oz role-play scenarios with the Furhat social robot under controlled experimental conditions. In addition to multimodal data, the dataset will be enriched with contextual data providing deeper insight into individual variability in social anxiety responses. This work can contribute to research on affect-adaptive human-robot interaction by providing support for robust multimodal detection of social anxiety.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u6536\u96c6\u793e\u4ea4\u7126\u8651\u591a\u6a21\u6001\u6570\u636e\u96c6\u7684\u534f\u8bae\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u97f3\u9891\u3001\u89c6\u9891\u548c\u751f\u7406\u8bb0\u5f55\uff0c\u65e8\u5728\u652f\u6301\u4eba\u673a\u4ea4\u4e92\u4e2d\u793e\u4ea4\u7126\u8651\u7684\u7a33\u5065\u68c0\u6d4b\u3002", "motivation": "\u793e\u4ea4\u7126\u8651\u662f\u4e00\u79cd\u666e\u904d\u5b58\u5728\u7684\u72b6\u51b5\uff0c\u5f71\u54cd\u4eba\u9645\u4e92\u52a8\u548c\u793e\u4f1a\u529f\u80fd\u3002\u76ee\u524d\u7f3a\u4e4f\u53cd\u6620\u793e\u4ea4\u7126\u8651\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u76f8\u5173\u7814\u7a76\u548c\u5e94\u7528\u7684\u53d1\u5c55\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u6536\u96c6\u534f\u8bae\uff0c\u4ece\u81f3\u5c1170\u540d\u53c2\u4e0e\u8005\u4e2d\u83b7\u53d6\u540c\u6b65\u7684\u97f3\u9891\u3001\u89c6\u9891\u548c\u751f\u7406\u8bb0\u5f55\uff0c\u53c2\u4e0e\u8005\u5728\u53d7\u63a7\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u4e0eFurhat\u793e\u4ea4\u673a\u5668\u4eba\u8fdb\u884c\u7ea610\u5206\u949f\u7684Wizard-of-Oz\u89d2\u8272\u626e\u6f14\u4e92\u52a8\u3002", "result": "\u5c06\u6784\u5efa\u4e00\u4e2a\u5305\u542b\u591a\u6a21\u6001\u6570\u636e\u548c\u60c5\u5883\u6570\u636e\u7684\u4e30\u5bcc\u6570\u636e\u96c6\uff0c\u6309\u53c2\u4e0e\u8005\u793e\u4ea4\u7126\u8651\u6c34\u5e73\u5206\u7ec4\uff0c\u4e3a\u7814\u7a76\u63d0\u4f9b\u652f\u6301\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u53ef\u4ee5\u4e3a\u60c5\u611f\u9002\u5e94\u6027\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u505a\u51fa\u8d21\u732e\uff0c\u901a\u8fc7\u63d0\u4f9b\u652f\u6301\u793e\u4ea4\u7126\u8651\u7684\u7a33\u5065\u591a\u6a21\u6001\u68c0\u6d4b\u3002"}}
{"id": "2511.13707", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13707", "abs": "https://arxiv.org/abs/2511.13707", "authors": ["Xiaoyu Liang", "Ziang Liu", "Kelvin Lin", "Edward Gu", "Ruolin Ye", "Tam Nguyen", "Cynthia Hsu", "Zhanxin Wu", "Xiaoman Yang", "Christy Sum Yu Cheung", "Harold Soh", "Katherine Dimitropoulou", "Tapomayukh Bhattacharjee"], "title": "OpenRoboCare: A Multimodal Multi-Task Expert Demonstration Dataset for Robot Caregiving", "comment": "IROS 2025", "summary": "We present OpenRoboCare, a multimodal dataset for robot caregiving, capturing expert occupational therapist demonstrations of Activities of Daily Living (ADLs). Caregiving tasks involve complex physical human-robot interactions, requiring precise perception under occlusions, safe physical contact, and long-horizon planning. While recent advances in robot learning from demonstrations have shown promise, there is a lack of a large-scale, diverse, and expert-driven dataset that captures real-world caregiving routines. To address this gap, we collect data from 21 occupational therapists performing 15 ADL tasks on two manikins. The dataset spans five modalities: RGB-D video, pose tracking, eye-gaze tracking, task and action annotations, and tactile sensing, providing rich multimodal insights into caregiver movement, attention, force application, and task execution strategies. We further analyze expert caregiving principles and strategies, offering insights to improve robot efficiency and task feasibility. Additionally, our evaluations demonstrate that OpenRoboCare presents challenges for state-of-the-art robot perception and human activity recognition methods, both critical for developing safe and adaptive assistive robots, highlighting the value of our contribution. See our website for additional visualizations: https://emprise.cs.cornell.edu/robo-care/.", "AI": {"tldr": "OpenRoboCare\u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u4eba\u62a4\u7406\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b\u804c\u4e1a\u6cbb\u7597\u5e08\u6267\u884c\u65e5\u5e38\u751f\u6d3b\u6d3b\u52a8\u4efb\u52a1\u7684\u4e13\u5bb6\u6f14\u793a\uff0c\u6db5\u76d6RGB-D\u89c6\u9891\u3001\u59ff\u6001\u8ddf\u8e2a\u3001\u773c\u52a8\u8ffd\u8e2a\u3001\u4efb\u52a1\u6ce8\u91ca\u548c\u89e6\u89c9\u611f\u77e5\u4e94\u79cd\u6a21\u6001\u3002", "motivation": "\u62a4\u7406\u4efb\u52a1\u6d89\u53ca\u590d\u6742\u7684\u4eba\u673a\u7269\u7406\u4ea4\u4e92\uff0c\u9700\u8981\u7cbe\u786e\u7684\u906e\u6321\u611f\u77e5\u3001\u5b89\u5168\u7684\u7269\u7406\u63a5\u89e6\u548c\u957f\u671f\u89c4\u5212\u3002\u5f53\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u4e14\u7531\u4e13\u5bb6\u9a71\u52a8\u7684\u771f\u5b9e\u4e16\u754c\u62a4\u7406\u6570\u636e\u96c6\u3002", "method": "\u6536\u96c6\u4e8621\u540d\u804c\u4e1a\u6cbb\u7597\u5e08\u5728\u4e24\u4e2a\u4eba\u4f53\u6a21\u578b\u4e0a\u6267\u884c15\u9879\u65e5\u5e38\u751f\u6d3b\u6d3b\u52a8\u4efb\u52a1\u7684\u6570\u636e\uff0c\u6db5\u76d6\u4e94\u79cd\u6a21\u6001\uff1aRGB-D\u89c6\u9891\u3001\u59ff\u6001\u8ddf\u8e2a\u3001\u773c\u52a8\u8ffd\u8e2a\u3001\u4efb\u52a1\u548c\u52a8\u4f5c\u6ce8\u91ca\u3001\u89e6\u89c9\u611f\u77e5\u3002", "result": "\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u62a4\u7406\u4eba\u5458\u8fd0\u52a8\u3001\u6ce8\u610f\u529b\u3001\u65bd\u529b\u548c\u4efb\u52a1\u6267\u884c\u7b56\u7565\u7684\u4e30\u5bcc\u591a\u6a21\u6001\u6d1e\u5bdf\uff0c\u5206\u6790\u5c55\u793a\u4e86\u4e13\u5bb6\u62a4\u7406\u539f\u5219\u548c\u7b56\u7565\u3002\u8bc4\u4f30\u8868\u660e\u8be5\u6570\u636e\u96c6\u5bf9\u6700\u5148\u8fdb\u7684\u673a\u5668\u4eba\u611f\u77e5\u548c\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u65b9\u6cd5\u63d0\u51fa\u4e86\u6311\u6218\u3002", "conclusion": "OpenRoboCare\u586b\u8865\u4e86\u673a\u5668\u4eba\u62a4\u7406\u9886\u57df\u7684\u6570\u636e\u7a7a\u767d\uff0c\u4e3a\u5f00\u53d1\u5b89\u5168\u548c\u81ea\u9002\u5e94\u7684\u8f85\u52a9\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u7a81\u663e\u4e86\u5176\u8d21\u732e\u4ef7\u503c\u3002"}}
{"id": "2511.13710", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13710", "abs": "https://arxiv.org/abs/2511.13710", "authors": ["Jianglong Ye", "Lai Wei", "Guangqi Jiang", "Changwei Jing", "Xueyan Zou", "Xiaolong Wang"], "title": "From Power to Precision: Learning Fine-grained Dexterity for Multi-fingered Robotic Hands", "comment": "Project page: https://jianglongye.com/power-to-precision", "summary": "Human grasps can be roughly categorized into two types: power grasps and precision grasps. Precision grasping enables tool use and is believed to have influenced human evolution. Today's multi-fingered robotic hands are effective in power grasps, but for tasks requiring precision, parallel grippers are still more widely adopted. This contrast highlights a key limitation in current robotic hand design: the difficulty of achieving both stable power grasps and precise, fine-grained manipulation within a single, versatile system. In this work, we bridge this gap by jointly optimizing the control and hardware design of a multi-fingered dexterous hand, enabling both power and precision manipulation. Rather than redesigning the entire hand, we introduce a lightweight fingertip geometry modification, represent it as a contact plane, and jointly optimize its parameters along with the corresponding control. Our control strategy dynamically switches between power and precision manipulation and simplifies precision control into parallel thumb-index motions, which proves robust for sim-to-real transfer. On the design side, we leverage large-scale simulation to optimize the fingertip geometry using a differentiable neural-physics surrogate model. We validate our approach through extensive experiments in both sim-to-real and real-to-real settings. Our method achieves an 82.5% zero-shot success rate on unseen objects in sim-to-real precision grasping, and a 93.3% success rate in challenging real-world tasks involving bread pinching. These results demonstrate that our co-design framework can significantly enhance the fine-grained manipulation ability of multi-fingered hands without reducing their ability for power grasps. Our project page is at https://jianglongye.com/power-to-precision", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u4f18\u5316\u591a\u6307\u7075\u5de7\u624b\u63a7\u5236\u548c\u786c\u4ef6\u8bbe\u8ba1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6307\u5c16\u51e0\u4f55\u4fee\u6539\u5b9e\u73b0\u529f\u7387\u6293\u53d6\u548c\u7cbe\u786e\u64cd\u4f5c\u7684\u7edf\u4e00\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u624b\u8bbe\u8ba1\u96be\u4ee5\u5728\u5355\u4e00\u7cfb\u7edf\u4e2d\u540c\u65f6\u5b9e\u73b0\u7a33\u5b9a\u7684\u529f\u7387\u6293\u53d6\u548c\u7cbe\u786e\u7684\u7cbe\u7ec6\u64cd\u4f5c\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165\u8f7b\u91cf\u7ea7\u6307\u5c16\u51e0\u4f55\u4fee\u6539\uff0c\u5c06\u5176\u8868\u793a\u4e3a\u63a5\u89e6\u5e73\u9762\uff0c\u5e76\u8054\u5408\u4f18\u5316\u5176\u53c2\u6570\u548c\u76f8\u5e94\u63a7\u5236\u7b56\u7565\uff1b\u63a7\u5236\u7b56\u7565\u5728\u529f\u7387\u548c\u7cbe\u786e\u64cd\u4f5c\u95f4\u52a8\u6001\u5207\u6362\uff0c\u5c06\u7cbe\u786e\u63a7\u5236\u7b80\u5316\u4e3a\u5e73\u884c\u62c7\u6307-\u98df\u6307\u8fd0\u52a8\u3002", "result": "\u5728\u672a\u89c1\u7269\u4f53\u7684sim-to-real\u7cbe\u786e\u6293\u53d6\u4e2d\u8fbe\u523082.5%\u7684\u96f6\u6837\u672c\u6210\u529f\u7387\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u9762\u5305\u634f\u53d6\u4efb\u52a1\u4e2d\u8fbe\u523093.3%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\u663e\u8457\u589e\u5f3a\u4e86\u591a\u6307\u624b\u7684\u7cbe\u7ec6\u64cd\u4f5c\u80fd\u529b\uff0c\u540c\u65f6\u4e0d\u964d\u4f4e\u5176\u529f\u7387\u6293\u53d6\u80fd\u529b\u3002"}}
