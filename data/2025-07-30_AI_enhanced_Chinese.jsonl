{"id": "2507.21225", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.21225", "abs": "https://arxiv.org/abs/2507.21225", "authors": ["Annan Zhang", "Miguel Flores-Acton", "Andy Yu", "Anshul Gupta", "Maggie Yao", "Daniela Rus"], "title": "Fluidically Innervated Lattices Make Versatile and Durable Tactile Sensors", "comment": "Accepted for publication in the proceedings of the 2025 International\n  Symposium on Experimental Robotics (ISER)", "summary": "Tactile sensing plays a fundamental role in enabling robots to navigate\ndynamic and unstructured environments, particularly in applications such as\ndelicate object manipulation, surface exploration, and human-robot interaction.\nIn this paper, we introduce a passive soft robotic fingertip with integrated\ntactile sensing, fabricated using a 3D-printed elastomer lattice with embedded\nair channels. This sensorization approach, termed fluidic innervation,\ntransforms the lattice into a tactile sensor by detecting pressure changes\nwithin sealed air channels, providing a simple yet robust solution to tactile\nsensing in robotics. Unlike conventional methods that rely on complex materials\nor designs, fluidic innervation offers a simple, scalable, single-material\nfabrication process. We characterize the sensors' response, develop a geometric\nmodel to estimate tip displacement, and train a neural network to accurately\npredict contact location and contact force. Additionally, we integrate the\nfingertip with an admittance controller to emulate spring-like behavior,\ndemonstrate its capability for environment exploration through tactile\nfeedback, and validate its durability under high impact and cyclic loading\nconditions. This tactile sensing technique offers advantages in terms of\nsimplicity, adaptability, and durability and opens up new opportunities for\nversatile robotic manipulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u6253\u5370\u5f39\u6027\u4f53\u6676\u683c\u548c\u5d4c\u5165\u5f0f\u7a7a\u6c14\u901a\u9053\u7684\u88ab\u52a8\u8f6f\u673a\u5668\u4eba\u6307\u5c16\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u901a\u8fc7\u68c0\u6d4b\u5bc6\u5c01\u7a7a\u6c14\u901a\u9053\u5185\u7684\u538b\u529b\u53d8\u5316\u5b9e\u73b0\u89e6\u89c9\u611f\u77e5\u3002", "motivation": "\u89e6\u89c9\u611f\u77e5\u5bf9\u673a\u5668\u4eba\u5728\u52a8\u6001\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u7cbe\u7ec6\u7269\u4f53\u64cd\u4f5c\u3001\u8868\u9762\u63a2\u7d22\u548c\u4eba\u673a\u4ea4\u4e92\u7b49\u5e94\u7528\u4e2d\u3002", "method": "\u91c7\u7528\u6d41\u4f53\u795e\u7ecf\u5316\u6280\u672f\uff0c\u901a\u8fc73D\u6253\u5370\u5f39\u6027\u4f53\u6676\u683c\u548c\u5d4c\u5165\u5f0f\u7a7a\u6c14\u901a\u9053\u5b9e\u73b0\u89e6\u89c9\u4f20\u611f\uff0c\u5f00\u53d1\u4e86\u51e0\u4f55\u6a21\u578b\u548c\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u63a5\u89e6\u4f4d\u7f6e\u548c\u529b\uff0c\u5e76\u7ed3\u5408\u5bfc\u7eb3\u63a7\u5236\u5668\u6a21\u62df\u5f39\u7c27\u884c\u4e3a\u3002", "result": "\u4f20\u611f\u5668\u5728\u73af\u5883\u63a2\u7d22\u3001\u9ad8\u51b2\u51fb\u548c\u5faa\u73af\u52a0\u8f7d\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u7684\u8010\u4e45\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u89e6\u89c9\u4f20\u611f\u6280\u672f\u5177\u6709\u7b80\u5355\u3001\u9002\u5e94\u6027\u5f3a\u548c\u8010\u7528\u7684\u4f18\u52bf\uff0c\u4e3a\u591a\u529f\u80fd\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.21245", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21245", "abs": "https://arxiv.org/abs/2507.21245", "authors": ["Gershy Ben-Arie", "Daniel Engelsman", "Rotem Dror", "Itzik Klein"], "title": "Diffusion Denoiser-Aided Gyrocompassing", "comment": "8 pages, 8 figures", "summary": "An accurate initial heading angle is essential for efficient and safe\nnavigation across diverse domains. Unlike magnetometers, gyroscopes can provide\naccurate heading reference independent of the magnetic disturbances in a\nprocess known as gyrocompassing. Yet, accurate and timely gyrocompassing, using\nlow-cost gyroscopes, remains a significant challenge in scenarios where\nexternal navigation aids are unavailable. Such challenges are commonly\naddressed in real-world applications such as autonomous vehicles, where size,\nweight, and power limitations restrict sensor quality, and noisy measurements\nseverely degrade gyrocompassing performance. To cope with this challenge, we\npropose a novel diffusion denoiser-aided gyrocompass approach. It integrates a\ndiffusion-based denoising framework with an enhanced learning-based heading\nestimation model. The diffusion denoiser processes raw inertial sensor signals\nbefore input to the deep learning model, resulting in accurate gyrocompassing.\nExperiments using both simulated and real sensor data demonstrate that our\nproposed approach improves gyrocompassing accuracy by 26% compared to\nmodel-based gyrocompassing and by 15% compared to other learning-driven\napproaches. This advancement holds particular significance for ensuring\naccurate and robust navigation in autonomous platforms that incorporate\nlow-cost gyroscopes within their navigation systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53bb\u566a\u7684\u9640\u87ba\u7f57\u76d8\u65b9\u6cd5\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f4e\u6210\u672c\u9640\u87ba\u4eea\u7684\u5bfc\u822a\u7cbe\u5ea6\u3002", "motivation": "\u4f4e\u6210\u672c\u9640\u87ba\u4eea\u5728\u65e0\u5916\u90e8\u5bfc\u822a\u8f85\u52a9\u7684\u573a\u666f\u4e0b\uff0c\u5bfc\u822a\u7cbe\u5ea6\u53d7\u9650\u4e8e\u566a\u58f0\u548c\u4f20\u611f\u5668\u8d28\u91cf\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u5c06\u6269\u6563\u53bb\u566a\u6846\u67b6\u4e0e\u5b66\u4e60\u578b\u822a\u5411\u4f30\u8ba1\u6a21\u578b\u7ed3\u5408\uff0c\u9884\u5904\u7406\u4f20\u611f\u5668\u4fe1\u53f7\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u6a21\u578b\u63d0\u9ad826%\u7cbe\u5ea6\uff0c\u6bd4\u5176\u4ed6\u5b66\u4e60\u65b9\u6cd5\u63d0\u9ad815%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f4e\u6210\u672c\u9640\u87ba\u4eea\u5728\u81ea\u4e3b\u5e73\u53f0\u4e2d\u7684\u5bfc\u822a\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.21259", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.21259", "abs": "https://arxiv.org/abs/2507.21259", "authors": ["Van Chung Nguyen", "Pratik Walunj", "Chuong Le", "An Duy Nguyen", "Hung Manh La"], "title": "NMPCM: Nonlinear Model Predictive Control on Resource-Constrained Microcontrollers", "comment": null, "summary": "Nonlinear Model Predictive Control (NMPC) is a powerful approach for\ncontrolling highly dynamic robotic systems, as it accounts for system dynamics\nand optimizes control inputs at each step. However, its high computational\ncomplexity makes implementation on resource-constrained microcontrollers\nimpractical. While recent studies have demonstrated the feasibility of Model\nPredictive Control (MPC) with linearized dynamics on microcontrollers, applying\nfull NMPC remains a significant challenge. This work presents an efficient\nsolution for generating and deploying NMPC on microcontrollers (NMPCM) to\ncontrol quadrotor UAVs. The proposed method optimizes computational efficiency\nwhile maintaining high control accuracy. Simulations in Gazebo/ROS and\nreal-world experiments validate the effectiveness of the approach,\ndemonstrating its capability to achieve high-frequency NMPC execution in\nreal-time systems. The code is available at:\nhttps://github.com/aralab-unr/NMPCM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5fae\u63a7\u5236\u5668\u4e0a\u9ad8\u6548\u90e8\u7f72\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63a7\u5236\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u3002", "motivation": "\u5c3d\u7ba1NMPC\u5728\u63a7\u5236\u52a8\u6001\u673a\u5668\u4eba\u7cfb\u7edf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u4f7f\u5176\u96be\u4ee5\u5728\u5fae\u63a7\u5236\u5668\u4e0a\u5b9e\u73b0\u3002", "method": "\u4f18\u5316\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u63a7\u5236\u7cbe\u5ea6\uff0c\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u9891NMPC\u5b9e\u65f6\u6267\u884c\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u5fae\u63a7\u5236\u5668\u4e0a\u5b9e\u73b0NMPC\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.21338", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.21338", "abs": "https://arxiv.org/abs/2507.21338", "authors": ["Yuman Gao", "Ruibin Zhang", "Tiancheng Lai", "Yanjun Cao", "Chao Xu", "Fei Gao"], "title": "Autonomous Exploration with Terrestrial-Aerial Bimodal Vehicles", "comment": null, "summary": "Terrestrial-aerial bimodal vehicles, which integrate the high mobility of\naerial robots with the long endurance of ground robots, offer significant\npotential for autonomous exploration. Given the inherent energy and time\nconstraints in practical exploration tasks, we present a hierarchical framework\nfor the bimodal vehicle to utilize its flexible locomotion modalities for\nexploration. Beginning with extracting environmental information to identify\ninformative regions, we generate a set of potential bimodal viewpoints. To\nadaptively manage energy and time constraints, we introduce an extended Monte\nCarlo Tree Search approach that strategically optimizes both modality selection\nand viewpoint sequencing. Combined with an improved bimodal vehicle motion\nplanner, we present a complete bimodal energy- and time-aware exploration\nsystem. Extensive simulations and deployment on a customized real-world\nplatform demonstrate the effectiveness of our system.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6846\u67b6\uff0c\u7528\u4e8e\u9646\u5730-\u7a7a\u4e2d\u53cc\u6a21\u6001\u8f66\u8f86\u5728\u81ea\u4e3b\u63a2\u7d22\u4e2d\u4f18\u5316\u80fd\u91cf\u548c\u65f6\u95f4\u7ea6\u675f\u3002", "motivation": "\u7ed3\u5408\u7a7a\u4e2d\u673a\u5668\u4eba\u7684\u9ad8\u673a\u52a8\u6027\u548c\u5730\u9762\u673a\u5668\u4eba\u7684\u957f\u7eed\u822a\u80fd\u529b\uff0c\u89e3\u51b3\u5b9e\u9645\u63a2\u7d22\u4efb\u52a1\u4e2d\u7684\u80fd\u91cf\u548c\u65f6\u95f4\u9650\u5236\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6846\u67b6\uff0c\u5305\u62ec\u73af\u5883\u4fe1\u606f\u63d0\u53d6\u3001\u53cc\u6a21\u6001\u89c6\u70b9\u751f\u6210\u3001\u6269\u5c55\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u4f18\u5316\u6a21\u6001\u9009\u62e9\u548c\u89c6\u70b9\u5e8f\u5217\uff0c\u4ee5\u53ca\u6539\u8fdb\u7684\u53cc\u6a21\u6001\u8fd0\u52a8\u89c4\u5212\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u5e73\u53f0\u90e8\u7f72\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u7ba1\u7406\u80fd\u91cf\u548c\u65f6\u95f4\u7ea6\u675f\uff0c\u63d0\u5347\u53cc\u6a21\u6001\u8f66\u8f86\u7684\u81ea\u4e3b\u63a2\u7d22\u80fd\u529b\u3002"}}
{"id": "2507.21384", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.21384", "abs": "https://arxiv.org/abs/2507.21384", "authors": ["I-Chieh Lee", "He Huang"], "title": "Projecting the New Body: How Body Image Evolves During Learning to Walk with a Wearable Robot", "comment": null, "summary": "Advances in wearable robotics challenge the traditional definition of human\nmotor systems, as wearable robots redefine body structure, movement capability,\nand perception of their own bodies. We measured gait performance and perceived\nbody images via Selected Coefficient of Perceived Motion, SCoMo, after each\ntraining session. Based on human motor learning theory extended to wearer-robot\nsystems, we hypothesized that learning the perceived body image when walking\nwith a robotic leg co-evolves with the actual gait improvement and becomes more\ncertain and more accurate to the actual motion. Our result confirmed that motor\nlearning improved both physical and perceived gait pattern towards normal,\nindicating that via practice the wearers incorporated the robotic leg into\ntheir sensorimotor systems to enable wearer-robot movement coordination.\nHowever, a persistent discrepancy between perceived and actual motion remained,\nlikely due to the absence of direct sensation and control of the prosthesis\nfrom wearers. Additionally, the perceptual overestimation at the later training\nsessions might limit further motor improvement. These findings suggest that\nenhancing the human sense of wearable robots and frequent calibrating\nperception of body image are essential for effective training with lower limb\nwearable robots and for developing more embodied assistive technologies.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u7a7f\u6234\u5f0f\u673a\u5668\u4eba\u5982\u4f55\u5f71\u54cd\u4eba\u4f53\u8fd0\u52a8\u7cfb\u7edf\u548c\u8eab\u4f53\u611f\u77e5\uff0c\u53d1\u73b0\u8fd0\u52a8\u5b66\u4e60\u548c\u611f\u77e5\u8eab\u4f53\u56fe\u50cf\u7684\u5171\u540c\u8fdb\u5316\uff0c\u4f46\u611f\u77e5\u4e0e\u5b9e\u9645\u8fd0\u52a8\u95f4\u4ecd\u5b58\u5728\u5dee\u8ddd\u3002", "motivation": "\u63a2\u7d22\u7a7f\u6234\u5f0f\u673a\u5668\u4eba\u5982\u4f55\u91cd\u65b0\u5b9a\u4e49\u4eba\u4f53\u8fd0\u52a8\u7cfb\u7edf\uff0c\u5305\u62ec\u8eab\u4f53\u7ed3\u6784\u3001\u8fd0\u52a8\u80fd\u529b\u548c\u8eab\u4f53\u611f\u77e5\u3002", "method": "\u901a\u8fc7SCoMo\u6d4b\u91cf\u6b65\u6001\u8868\u73b0\u548c\u611f\u77e5\u8eab\u4f53\u56fe\u50cf\uff0c\u57fa\u4e8e\u8fd0\u52a8\u5b66\u4e60\u7406\u8bba\u6269\u5c55\u5230\u7a7f\u6234\u8005-\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "result": "\u8fd0\u52a8\u5b66\u4e60\u6539\u5584\u4e86\u5b9e\u9645\u548c\u611f\u77e5\u7684\u6b65\u6001\u6a21\u5f0f\uff0c\u4f46\u611f\u77e5\u4e0e\u5b9e\u9645\u8fd0\u52a8\u95f4\u5b58\u5728\u6301\u7eed\u5dee\u8ddd\uff0c\u53ef\u80fd\u9650\u5236\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "conclusion": "\u589e\u5f3a\u7a7f\u6234\u8005\u5bf9\u673a\u5668\u4eba\u7684\u611f\u77e5\u5e76\u6821\u51c6\u8eab\u4f53\u56fe\u50cf\u611f\u77e5\u5bf9\u6709\u6548\u8bad\u7ec3\u548c\u5f00\u53d1\u66f4\u81ea\u7136\u7684\u8f85\u52a9\u6280\u672f\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.21431", "categories": ["cs.RO", "cs.HC", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.21431", "abs": "https://arxiv.org/abs/2507.21431", "authors": ["Victor Liu", "Timothy Du", "Jordy Sehn", "Jack Collier", "Fran\u00e7ois Grondin"], "title": "Sound Source Localization for Human-Robot Interaction in Outdoor Environments", "comment": null, "summary": "This paper presents a sound source localization strategy that relies on a\nmicrophone array embedded in an unmanned ground vehicle and an asynchronous\nclose-talking microphone near the operator. A signal coarse alignment strategy\nis combined with a time-domain acoustic echo cancellation algorithm to estimate\na time-frequency ideal ratio mask to isolate the target speech from\ninterferences and environmental noise. This allows selective sound source\nlocalization, and provides the robot with the direction of arrival of sound\nfrom the active operator, which enables rich interaction in noisy scenarios.\nResults demonstrate an average angle error of 4 degrees and an accuracy within\n5 degrees of 95\\% at a signal-to-noise ratio of 1dB, which is significantly\nsuperior to the state-of-the-art localization methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ea6\u514b\u98ce\u9635\u5217\u548c\u5f02\u6b65\u8fd1\u573a\u9ea6\u514b\u98ce\u7684\u58f0\u6e90\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7ed3\u5408\u4fe1\u53f7\u7c97\u5bf9\u9f50\u548c\u65f6\u57df\u58f0\u5b66\u56de\u58f0\u6d88\u9664\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u566a\u58f0\u73af\u5883\u4e0b\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u5728\u5608\u6742\u73af\u5883\u4e2d\u673a\u5668\u4eba\u5982\u4f55\u51c6\u786e\u8bc6\u522b\u64cd\u4f5c\u8005\u58f0\u6e90\u65b9\u5411\u7684\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u66f4\u4e30\u5bcc\u7684\u4ea4\u4e92\u3002", "method": "\u4f7f\u7528\u9ea6\u514b\u98ce\u9635\u5217\u548c\u5f02\u6b65\u8fd1\u573a\u9ea6\u514b\u98ce\uff0c\u7ed3\u5408\u4fe1\u53f7\u7c97\u5bf9\u9f50\u548c\u65f6\u57df\u58f0\u5b66\u56de\u58f0\u6d88\u9664\u7b97\u6cd5\uff0c\u4f30\u8ba1\u65f6\u9891\u7406\u60f3\u6bd4\u7387\u63a9\u7801\u4ee5\u5206\u79bb\u76ee\u6807\u8bed\u97f3\u3002", "result": "\u57281dB\u4fe1\u566a\u6bd4\u4e0b\uff0c\u5e73\u5747\u89d2\u5ea6\u8bef\u5dee\u4e3a4\u5ea6\uff0c95%\u7684\u60c5\u51b5\u4e0b\u8bef\u5dee\u57285\u5ea6\u4ee5\u5185\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u566a\u58f0\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u58f0\u6e90\u5b9a\u4f4d\uff0c\u4e3a\u673a\u5668\u4eba\u4ea4\u4e92\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u65b9\u5411\u4fe1\u606f\u3002"}}
{"id": "2507.21496", "categories": ["cs.RO", "cs.LG", "nlin.CD"], "pdf": "https://arxiv.org/pdf/2507.21496", "abs": "https://arxiv.org/abs/2507.21496", "authors": ["Ryo Terajima", "Katsuma Inoue", "Kohei Nakajima", "Yasuo Kuniyoshi"], "title": "Multifunctional physical reservoir computing in soft tensegrity robots", "comment": "25 pages, 12 figures. The following article has been accepted by\n  Chaos: An Interdisciplinary Journal of Nonlinear Science", "summary": "Recent studies have demonstrated that the dynamics of physical systems can be\nutilized for the desired information processing under the framework of physical\nreservoir computing (PRC). Robots with soft bodies are examples of such\nphysical systems, and their nonlinear body-environment dynamics can be used to\ncompute and generate the motor signals necessary for the control of their own\nbehavior. In this simulation study, we extend this approach to control and\nembed not only one but also multiple behaviors into a type of soft robot called\na tensegrity robot. The resulting system, consisting of the robot and the\nenvironment, is a multistable dynamical system that converges to different\nattractors from varying initial conditions. Furthermore, attractor analysis\nreveals that there exist \"untrained attractors\" in the state space of the\nsystem outside the training data. These untrained attractors reflect the\nintrinsic properties and structures of the tensegrity robot and its\ninteractions with the environment. The impacts of these recent findings in PRC\nremain unexplored in embodied AI research. We here illustrate their potential\nto understand various features of embodied cognition that have not been fully\naddressed to date.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7269\u7406\u50a8\u5c42\u8ba1\u7b97\uff08PRC\uff09\u6846\u67b6\uff0c\u5229\u7528\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u5b9e\u73b0\u591a\u884c\u4e3a\u63a7\u5236\uff0c\u53d1\u73b0\u672a\u8bad\u7ec3\u5438\u5f15\u5b50\uff0c\u63ed\u793a\u4e86\u5176\u5728\u5177\u8eab\u8ba4\u77e5\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u63a2\u7d22\u8f6f\u4f53\u673a\u5668\u4eba\uff08\u5982\u5f20\u62c9\u6574\u4f53\u673a\u5668\u4eba\uff09\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u5982\u4f55\u7528\u4e8e\u591a\u884c\u4e3a\u63a7\u5236\uff0c\u5e76\u7814\u7a76\u5176\u672a\u8bad\u7ec3\u5438\u5f15\u5b50\u5bf9\u5177\u8eab\u8ba4\u77e5\u7684\u542f\u793a\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u7814\u7a76\uff0c\u5229\u7528PRC\u6846\u67b6\u5c06\u591a\u4e2a\u884c\u4e3a\u5d4c\u5165\u5f20\u62c9\u6574\u4f53\u673a\u5668\u4eba\uff0c\u5206\u6790\u5176\u591a\u7a33\u6001\u52a8\u529b\u5b66\u548c\u5438\u5f15\u5b50\u7279\u6027\u3002", "result": "\u7cfb\u7edf\u8868\u73b0\u51fa\u591a\u7a33\u6001\u7279\u6027\uff0c\u5b58\u5728\u672a\u8bad\u7ec3\u5438\u5f15\u5b50\uff0c\u53cd\u6620\u4e86\u673a\u5668\u4eba\u4e0e\u73af\u5883\u4ea4\u4e92\u7684\u56fa\u6709\u7279\u6027\u3002", "conclusion": "\u672a\u8bad\u7ec3\u5438\u5f15\u5b50\u7684\u53d1\u73b0\u4e3a\u5177\u8eab\u8ba4\u77e5\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5c55\u793a\u4e86PRC\u5728\u7406\u89e3\u590d\u6742\u884c\u4e3a\u63a7\u5236\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.21506", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21506", "abs": "https://arxiv.org/abs/2507.21506", "authors": ["Chang-Hun Ji", "SiWoon Song", "Youn-Hee Han", "SungTae Moon"], "title": "Decision Transformer-Based Drone Trajectory Planning with Dynamic Safety-Efficiency Trade-Offs", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. \\c{opyright} 2025 IEEE. Personal use of this\n  material is permitted. \\c{opyright} 2025 IEEE. Personal use of this material\n  is permitted. Permission from IEEE must be obtained for all other uses", "summary": "A drone trajectory planner should be able to dynamically adjust the\nsafety-efficiency trade-off according to varying mission requirements in\nunknown environments. Although traditional polynomial-based planners offer\ncomputational efficiency and smooth trajectory generation, they require expert\nknowledge to tune multiple parameters to adjust this trade-off. Moreover, even\nwith careful tuning, the resulting adjustment may fail to achieve the desired\ntrade-off. Similarly, although reinforcement learning-based planners are\nadaptable in unknown environments, they do not explicitly address the\nsafety-efficiency trade-off. To overcome this limitation, we introduce a\nDecision Transformer-based trajectory planner that leverages a single\nparameter, Return-to-Go (RTG), as a \\emph{temperature parameter} to dynamically\nadjust the safety-efficiency trade-off. In our framework, since RTG intuitively\nmeasures the safety and efficiency of a trajectory, RTG tuning does not require\nexpert knowledge. We validate our approach using Gazebo simulations in both\nstructured grid and unstructured random environments. The experimental results\ndemonstrate that our planner can dynamically adjust the safety-efficiency\ntrade-off by simply tuning the RTG parameter. Furthermore, our planner\noutperforms existing baseline methods across various RTG settings, generating\nsafer trajectories when tuned for safety and more efficient trajectories when\ntuned for efficiency. Real-world experiments further confirm the reliability\nand practicality of our proposed planner.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51b3\u7b56\u53d8\u6362\u5668\u7684\u65e0\u4eba\u673a\u8f68\u8ff9\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5355\u4e00\u53c2\u6570RTG\u52a8\u6001\u8c03\u6574\u5b89\u5168\u4e0e\u6548\u7387\u7684\u6743\u8861\uff0c\u65e0\u9700\u4e13\u5bb6\u77e5\u8bc6\uff0c\u5e76\u5728\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u591a\u9879\u5f0f\u89c4\u5212\u5668\u9700\u8981\u4e13\u5bb6\u8c03\u53c2\u4e14\u96be\u4ee5\u5b9e\u73b0\u7406\u60f3\u6743\u8861\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u89c4\u5212\u5668\u672a\u660e\u786e\u89e3\u51b3\u5b89\u5168\u4e0e\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u91c7\u7528\u51b3\u7b56\u53d8\u6362\u5668\u6846\u67b6\uff0c\u5f15\u5165RTG\u4f5c\u4e3a\u6e29\u5ea6\u53c2\u6570\uff0c\u76f4\u89c2\u8861\u91cf\u8f68\u8ff9\u7684\u5b89\u5168\u4e0e\u6548\u7387\uff0c\u5b9e\u73b0\u52a8\u6001\u8c03\u6574\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u89c4\u5212\u5668\u901a\u8fc7\u8c03\u6574RTG\u80fd\u7075\u6d3b\u4f18\u5316\u5b89\u5168\u6216\u6548\u7387\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u7b80\u5316\u4e86\u6743\u8861\u8c03\u6574\u8fc7\u7a0b\uff0c\u9002\u7528\u4e8e\u672a\u77e5\u73af\u5883\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.21517", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.21517", "abs": "https://arxiv.org/abs/2507.21517", "authors": ["Junhao Chen", "Zhen Zhang", "Chengrui Zhu", "Xiaojun Hou", "Tianyang Hu", "Huifeng Wu", "Yong Liu"], "title": "LITE: A Learning-Integrated Topological Explorer for Multi-Floor Indoor Environments", "comment": "IROS2025", "summary": "This work focuses on multi-floor indoor exploration, which remains an open\narea of research. Compared to traditional methods, recent learning-based\nexplorers have demonstrated significant potential due to their robust\nenvironmental learning and modeling capabilities, but most are restricted to 2D\nenvironments. In this paper, we proposed a learning-integrated topological\nexplorer, LITE, for multi-floor indoor environments. LITE decomposes the\nenvironment into a floor-stair topology, enabling seamless integration of\nlearning or non-learning-based 2D exploration methods for 3D exploration. As we\nincrementally build floor-stair topology in exploration using YOLO11-based\ninstance segmentation model, the agent can transition between floors through a\nfinite state machine. Additionally, we implement an attention-based 2D\nexploration policy that utilizes an attention mechanism to capture spatial\ndependencies between different regions, thereby determining the next global\ngoal for more efficient exploration. Extensive comparison and ablation studies\nconducted on the HM3D and MP3D datasets demonstrate that our proposed 2D\nexploration policy significantly outperforms all baseline explorers in terms of\nexploration efficiency. Furthermore, experiments in several 3D multi-floor\nenvironments indicate that our framework is compatible with various 2D\nexploration methods, facilitating effective multi-floor indoor exploration.\nFinally, we validate our method in the real world with a quadruped robot,\nhighlighting its strong generalization capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u96c6\u6210\u62d3\u6251\u63a2\u7d22\u5668LITE\uff0c\u7528\u4e8e\u591a\u697c\u5c42\u5ba4\u5185\u73af\u5883\u63a2\u7d22\uff0c\u901a\u8fc7\u5206\u89e3\u73af\u5883\u4e3a\u697c\u5c42-\u697c\u68af\u62d3\u6251\uff0c\u7ed3\u54082D\u63a2\u7d22\u65b9\u6cd5\u5b9e\u73b03D\u63a2\u7d22\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u591a\u697c\u5c42\u5ba4\u5185\u63a2\u7d22\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u7814\u7a76\u9886\u57df\uff0c\u73b0\u6709\u5b66\u4e60\u578b\u63a2\u7d22\u5668\u591a\u5c40\u9650\u4e8e2D\u73af\u5883\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faLITE\u6846\u67b6\uff0c\u5c06\u73af\u5883\u5206\u89e3\u4e3a\u697c\u5c42-\u697c\u68af\u62d3\u6251\uff0c\u7ed3\u5408YOLO11\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\u548c\u6709\u9650\u72b6\u6001\u673a\u5b9e\u73b0\u697c\u5c42\u5207\u6362\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u76842D\u63a2\u7d22\u7b56\u7565\u3002", "result": "\u5728HM3D\u548cMP3D\u6570\u636e\u96c6\u4e0a\uff0cLITE\u76842D\u63a2\u7d22\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u5728\u771f\u5b9e\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "LITE\u6846\u67b6\u6709\u6548\u652f\u6301\u591a\u697c\u5c42\u5ba4\u5185\u63a2\u7d22\uff0c\u517c\u5bb9\u591a\u79cd2D\u63a2\u7d22\u65b9\u6cd5\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2507.21533", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21533", "abs": "https://arxiv.org/abs/2507.21533", "authors": ["Tyler Han", "Yanda Bao", "Bhaumik Mehta", "Gabriel Guo", "Anubhav Vishwakarma", "Emily Kang", "Sanghun Jung", "Rosario Scalise", "Jason Zhou", "Bryan Xu", "Byron Boots"], "title": "Model Predictive Adversarial Imitation Learning for Planning from Observation", "comment": "Open-source code in process of being cleaned and documented for\n  release. Please contact directly in the meantime for code. Under Review", "summary": "Human demonstration data is often ambiguous and incomplete, motivating\nimitation learning approaches that also exhibit reliable planning behavior. A\ncommon paradigm to perform planning-from-demonstration involves learning a\nreward function via Inverse Reinforcement Learning (IRL) then deploying this\nreward via Model Predictive Control (MPC). Towards unifying these methods, we\nderive a replacement of the policy in IRL with a planning-based agent. With\nconnections to Adversarial Imitation Learning, this formulation enables\nend-to-end interactive learning of planners from observation-only\ndemonstrations. In addition to benefits in interpretability, complexity, and\nsafety, we study and observe significant improvements on sample efficiency,\nout-of-distribution generalization, and robustness. The study includes\nevaluations in both simulated control benchmarks and real-world navigation\nexperiments using few-to-single observation-only demonstrations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9006\u5411\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c4\u5212\u4ee3\u7406\u66ff\u4ee3IRL\u4e2d\u7684\u7b56\u7565\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u89c2\u5bdf\u6f14\u793a\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u3001\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4eba\u7c7b\u6f14\u793a\u6570\u636e\u901a\u5e38\u6a21\u7cca\u4e14\u4e0d\u5b8c\u6574\uff0c\u9700\u8981\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u540c\u65f6\u5177\u5907\u53ef\u9760\u7684\u89c4\u5212\u884c\u4e3a\u3002", "method": "\u7528\u89c4\u5212\u4ee3\u7406\u66ff\u4ee3IRL\u4e2d\u7684\u7b56\u7565\uff0c\u7ed3\u5408\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u89c2\u5bdf\u6f14\u793a\u5b66\u4e60\u3002", "result": "\u5728\u6a21\u62df\u63a7\u5236\u548c\u771f\u5b9e\u5bfc\u822a\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u3001\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u3001\u590d\u6742\u6027\u3001\u5b89\u5168\u6027\u7b49\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.21545", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.21545", "abs": "https://arxiv.org/abs/2507.21545", "authors": ["Haoming Ye", "Yunxiao Xiao", "Cewu Lu", "Panpan Cai"], "title": "Pretraining a Unified PDDL Domain from Real-World Demonstrations for Generalizable Robot Task Planning", "comment": "Preprint. Under review", "summary": "Robotic task planning in real-world environments requires reasoning over\nimplicit constraints from language and vision. While LLMs and VLMs offer strong\npriors, they struggle with long-horizon structure and symbolic grounding.\nExisting methods that combine LLMs with symbolic planning often rely on\nhandcrafted or narrow domains, limiting generalization. We propose UniDomain, a\nframework that pre-trains a PDDL domain from robot manipulation demonstrations\nand applies it for online robotic task planning. It extracts atomic domains\nfrom 12,393 manipulation videos to form a unified domain with 3137 operators,\n2875 predicates, and 16481 causal edges. Given a target class of tasks, it\nretrieves relevant atomics from the unified domain and systematically fuses\nthem into high-quality meta-domains to support compositional generalization in\nplanning. Experiments on diverse real-world tasks show that UniDomain solves\ncomplex, unseen tasks in a zero-shot manner, achieving up to 58% higher task\nsuccess and 160% improvement in plan optimality over state-of-the-art LLM and\nLLM-PDDL baselines.", "AI": {"tldr": "UniDomain\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3PDDL\u57df\u6765\u652f\u6301\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\uff0c\u89e3\u51b3\u4e86LLM\u548cVLM\u5728\u957f\u671f\u7ed3\u6784\u548c\u7b26\u53f7\u57fa\u7840\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u6216\u72ed\u7a84\u9886\u57df\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002UniDomain\u65e8\u5728\u901a\u8fc7\u4ece\u6f14\u793a\u4e2d\u63d0\u53d6\u7edf\u4e00\u57df\u6765\u652f\u6301\u7ec4\u5408\u6cdb\u5316\u3002", "method": "\u4ece12,393\u4e2a\u64cd\u7eb5\u89c6\u9891\u4e2d\u63d0\u53d6\u539f\u5b50\u57df\uff0c\u5f62\u6210\u7edf\u4e00\u57df\uff0c\u5e76\u68c0\u7d22\u548c\u878d\u5408\u76f8\u5173\u539f\u5b50\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u5143\u57df\u3002", "result": "\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\uff0cUniDomain\u96f6\u6837\u672c\u89e3\u51b3\u590d\u6742\u4efb\u52a1\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad858%\uff0c\u8ba1\u5212\u6700\u4f18\u6027\u63d0\u5347160%\u3002", "conclusion": "UniDomain\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.21553", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.21553", "abs": "https://arxiv.org/abs/2507.21553", "authors": ["Federica Di Lauro", "Domenico G. Sorrenti", "Miguel Angel Sotelo"], "title": "Multi-robot LiDAR SLAM: a practical case study in underground tunnel environments", "comment": "14 pages, 14 figures", "summary": "Multi-robot SLAM aims at localizing and building a map with multiple robots,\ninteracting with each other. In the work described in this article, we analyze\nthe pipeline of a decentralized LiDAR SLAM system to study the current\nlimitations of the state of the art, and we discover a significant source of\nfailures, i.e., that the loop detection is the source of too many false\npositives. We therefore develop and propose a new heuristic to overcome these\nlimitations. The environment taken as reference in this work is the highly\nchallenging case of underground tunnels. We also highlight potential new\nresearch areas still under-explored.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u591a\u673a\u5668\u4ebaSLAM\u7cfb\u7edf\u4e2d\u53bb\u4e2d\u5fc3\u5316LiDAR SLAM\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u73af\u8def\u68c0\u6d4b\u662f\u5bfc\u81f4\u8bef\u62a5\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u591a\u673a\u5668\u4ebaSLAM\u7cfb\u7edf\u5728\u53bb\u4e2d\u5fc3\u5316LiDAR SLAM\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u73af\u8def\u68c0\u6d4b\u65b9\u9762\u7684\u9ad8\u8bef\u62a5\u95ee\u9898\u3002", "method": "\u5206\u6790\u73b0\u6709SLAM\u6d41\u7a0b\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u4ee5\u6539\u8fdb\u73af\u8def\u68c0\u6d4b\u3002", "result": "\u65b0\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u73af\u8def\u68c0\u6d4b\u4e2d\u7684\u8bef\u62a5\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\uff08\u5982\u5730\u4e0b\u96a7\u9053\uff09\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u4e3a\u89e3\u51b3\u591a\u673a\u5668\u4ebaSLAM\u4e2d\u7684\u73af\u8def\u68c0\u6d4b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u53ef\u80fd\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.21610", "categories": ["cs.RO", "cs.CV", "I.4.9"], "pdf": "https://arxiv.org/pdf/2507.21610", "abs": "https://arxiv.org/abs/2507.21610", "authors": ["Ruiyang Hao", "Haibao Yu", "Jiaru Zhong", "Chuanye Wang", "Jiahao Wang", "Yiming Kan", "Wenxian Yang", "Siqi Fan", "Huilin Yin", "Jianing Qiu", "Yao Mu", "Jiankai Sun", "Li Chen", "Walter Zimmer", "Dandan Zhang", "Shanghang Zhang", "Mac Schwager", "Wei Huang", "Xiaobo Zhang", "Ping Luo", "Zaiqing Nie"], "title": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition", "comment": "10 pages, 4 figures, accepted by ICCVW", "summary": "With the rapid advancement of autonomous driving technology,\nvehicle-to-everything (V2X) communication has emerged as a key enabler for\nextending perception range and enhancing driving safety by providing visibility\nbeyond the line of sight. However, integrating multi-source sensor data from\nboth ego-vehicles and infrastructure under real-world constraints, such as\nlimited communication bandwidth and dynamic environments, presents significant\ntechnical challenges. To facilitate research in this area, we organized the\nEnd-to-End Autonomous Driving through V2X Cooperation Challenge, which features\ntwo tracks: cooperative temporal perception and cooperative end-to-end\nplanning. Built on the UniV2X framework and the V2X-Seq-SPD dataset, the\nchallenge attracted participation from over 30 teams worldwide and established\na unified benchmark for evaluating cooperative driving systems. This paper\ndescribes the design and outcomes of the challenge, highlights key research\nproblems including bandwidth-aware fusion, robust multi-agent planning, and\nheterogeneous sensor integration, and analyzes emerging technical trends among\ntop-performing solutions. By addressing practical constraints in communication\nand data fusion, the challenge contributes to the development of scalable and\nreliable V2X-cooperative autonomous driving systems.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86V2X\u901a\u4fe1\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u7ec4\u7ec7\u4e86\u4e00\u9879\u6311\u6218\u8d5b\u4ee5\u63a8\u52a8\u591a\u6e90\u4f20\u611f\u5668\u6570\u636e\u878d\u5408\u548c\u89c4\u5212\u7684\u7814\u7a76\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u591a\u6e90\u4f20\u611f\u5668\u6570\u636e\u878d\u5408\u548c\u52a8\u6001\u73af\u5883\u4e0b\u7684\u901a\u4fe1\u5e26\u5bbd\u9650\u5236\u95ee\u9898\u3002", "method": "\u57fa\u4e8eUniV2X\u6846\u67b6\u548cV2X-Seq-SPD\u6570\u636e\u96c6\uff0c\u7ec4\u7ec7\u4e86\u5305\u542b\u4e24\u4e2a\u8d5b\u9053\u7684\u6311\u6218\u8d5b\u3002", "result": "\u5438\u5f15\u4e86\u5168\u740330\u591a\u4e2a\u56e2\u961f\u53c2\u4e0e\uff0c\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u5206\u6790\u4e86\u9ad8\u6027\u80fd\u89e3\u51b3\u65b9\u6848\u7684\u6280\u672f\u8d8b\u52bf\u3002", "conclusion": "\u6311\u6218\u8d5b\u4e3a\u89e3\u51b3\u901a\u4fe1\u548c\u6570\u636e\u878d\u5408\u7684\u5b9e\u9645\u7ea6\u675f\u63d0\u4f9b\u4e86\u8d21\u732e\uff0c\u63a8\u52a8\u4e86\u53ef\u6269\u5c55\u548c\u53ef\u9760\u7684V2X\u534f\u4f5c\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2507.21709", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.21709", "abs": "https://arxiv.org/abs/2507.21709", "authors": ["Haolan Zhang", "Thanh Nguyen Canh", "Chenghao Li", "Nak Young Chong"], "title": "Adaptive Prior Scene-Object SLAM for Dynamic Environments", "comment": "Accepted by IEEE The 2025 IEEE International Conference on Real-time\n  Computing and Robotics", "summary": "Visual Simultaneous Localization and Mapping (SLAM) plays a vital role in\nreal-time localization for autonomous systems. However, traditional SLAM\nmethods, which assume a static environment, often suffer from significant\nlocalization drift in dynamic scenarios. While recent advancements have\nimproved SLAM performance in such environments, these systems still struggle\nwith localization drift, particularly due to abrupt viewpoint changes and\npoorly characterized moving objects. In this paper, we propose a novel\nscene-object-based reliability assessment framework that comprehensively\nevaluates SLAM stability through both current frame quality metrics and scene\nchanges relative to reliable reference frames. Furthermore, to tackle the lack\nof error correction mechanisms in existing systems when pose estimation becomes\nunreliable, we employ a pose refinement strategy that leverages information\nfrom reliable frames to optimize camera pose estimation, effectively mitigating\nthe adverse effects of dynamic interference. Extensive experiments on the TUM\nRGB-D datasets demonstrate that our approach achieves substantial improvements\nin localization accuracy and system robustness under challenging dynamic\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u573a\u666f-\u5bf9\u8c61\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u6846\u67b6\u548c\u59ff\u6001\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u89c6\u89c9SLAM\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edfSLAM\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u56e0\u5047\u8bbe\u9759\u6001\u73af\u5883\u800c\u5b58\u5728\u663e\u8457\u5b9a\u4f4d\u6f02\u79fb\uff0c\u73b0\u6709\u7cfb\u7edf\u5728\u89c6\u89d2\u7a81\u53d8\u548c\u52a8\u6001\u7269\u4f53\u5904\u7406\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5f53\u524d\u5e27\u8d28\u91cf\u6307\u6807\u548c\u573a\u666f\u53d8\u5316\u8bc4\u4f30SLAM\u7a33\u5b9a\u6027\uff0c\u5e76\u5229\u7528\u53ef\u9760\u5e27\u4fe1\u606f\u4f18\u5316\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u3002", "result": "\u5728TUM RGB-D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u7684SLAM\u5b9a\u4f4d\u6f02\u79fb\u95ee\u9898\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.21772", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.21772", "abs": "https://arxiv.org/abs/2507.21772", "authors": ["Yuda Chen", "Shuaikang Wang", "Jie Li", "Meng Guo"], "title": "Multi-UAV Deployment in Obstacle-Cluttered Environments with LOS Connectivity", "comment": "iros2025", "summary": "A reliable communication network is essential for multiple UAVs operating\nwithin obstacle-cluttered environments, where limited communication due to\nobstructions often occurs. A common solution is to deploy intermediate UAVs to\nrelay information via a multi-hop network, which introduces two challenges: (i)\nhow to design the structure of multihop networks; and (ii) how to maintain\nconnectivity during collaborative motion. To this end, this work first proposes\nan efficient constrained search method based on the minimumedge RRT? algorithm,\nto find a spanning-tree topology that requires a less number of UAVs for the\ndeployment task. Then, to achieve this deployment, a distributed model\npredictive control strategy is proposed for the online motion coordination. It\nexplicitly incorporates not only the inter-UAV and UAVobstacle distance\nconstraints, but also the line-of-sight (LOS) connectivity constraint. These\nconstraints are well-known to be nonlinear and often tackled by various\napproximations. In contrast, this work provides a theoretical guarantee that\nall agent trajectories are ensured to be collision-free with a teamwise LOS\nconnectivity at all time. Numerous simulations are performed in 3D valley-like\nenvironments, while hardware experiments validate its dynamic adaptation when\nthe deployment position changes online.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5c0f\u8fb9RRT*\u7b97\u6cd5\u7684\u9ad8\u6548\u7ea6\u675f\u641c\u7d22\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bbe\u8ba1\u591a\u8df3\u7f51\u7edc\u7ed3\u6784\uff0c\u5e76\u91c7\u7528\u5206\u5e03\u5f0f\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7b56\u7565\u5b9e\u73b0\u65e0\u4eba\u673a\u5728\u7ebf\u8fd0\u52a8\u534f\u8c03\uff0c\u786e\u4fdd\u65e0\u78b0\u649e\u548c\u89c6\u7ebf\u8fde\u63a5\u3002", "motivation": "\u5728\u969c\u788d\u7269\u5bc6\u96c6\u73af\u5883\u4e2d\uff0c\u591a\u65e0\u4eba\u673a\u534f\u540c\u4f5c\u4e1a\u5e38\u56e0\u901a\u4fe1\u53d7\u9650\u800c\u9762\u4e34\u6311\u6218\uff0c\u9700\u89e3\u51b3\u591a\u8df3\u7f51\u7edc\u8bbe\u8ba1\u548c\u8fd0\u52a8\u4e2d\u7684\u8fde\u63a5\u7ef4\u62a4\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6700\u5c0f\u8fb9RRT*\u7b97\u6cd5\u7684\u7ea6\u675f\u641c\u7d22\u65b9\u6cd5\u8bbe\u8ba1\u7f51\u7edc\u62d3\u6251\uff0c\u7ed3\u5408\u5206\u5e03\u5f0f\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7b56\u7565\uff0c\u8003\u8651\u65e0\u4eba\u673a\u95f4\u3001\u65e0\u4eba\u673a\u4e0e\u969c\u788d\u7269\u8ddd\u79bb\u53ca\u89c6\u7ebf\u8fde\u63a5\u7ea6\u675f\u3002", "result": "\u7406\u8bba\u4fdd\u8bc1\u65e0\u4eba\u673a\u8f68\u8ff9\u65e0\u78b0\u649e\u4e14\u59cb\u7ec8\u4fdd\u6301\u89c6\u7ebf\u8fde\u63a5\uff0c\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u52a8\u6001\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u969c\u788d\u73af\u5883\u4e2d\u591a\u65e0\u4eba\u673a\u901a\u4fe1\u7f51\u7edc\u7684\u90e8\u7f72\u4e0e\u7ef4\u62a4\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.21796", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21796", "abs": "https://arxiv.org/abs/2507.21796", "authors": ["Yuying Zhang", "Kevin Sebastian Luck", "Francesco Verdoja", "Ville Kyrki", "Joni Pajarinen"], "title": "MoDeSuite: Robot Learning Task Suite for Benchmarking Mobile Manipulation with Deformable Objects", "comment": null, "summary": "Mobile manipulation is a critical capability for robots operating in diverse,\nreal-world environments. However, manipulating deformable objects and materials\nremains a major challenge for existing robot learning algorithms. While various\nbenchmarks have been proposed to evaluate manipulation strategies with rigid\nobjects, there is still a notable lack of standardized benchmarks that address\nmobile manipulation tasks involving deformable objects.\n  To address this gap, we introduce MoDeSuite, the first Mobile Manipulation\nDeformable Object task suite, designed specifically for robot learning.\nMoDeSuite consists of eight distinct mobile manipulation tasks covering both\nelastic objects and deformable objects, each presenting a unique challenge\ninspired by real-world robot applications. Success in these tasks requires\neffective collaboration between the robot's base and manipulator, as well as\nthe ability to exploit the deformability of the objects. To evaluate and\ndemonstrate the use of the proposed benchmark, we train two state-of-the-art\nreinforcement learning algorithms and two imitation learning algorithms,\nhighlighting the difficulties encountered and showing their performance in\nsimulation. Furthermore, we demonstrate the practical relevance of the suite by\ndeploying the trained policies directly into the real world with the Spot\nrobot, showcasing the potential for sim-to-real transfer. We expect that\nMoDeSuite will open a novel research domain in mobile manipulation involving\ndeformable objects. Find more details, code, and videos at\nhttps://sites.google.com/view/modesuite/home.", "AI": {"tldr": "MoDeSuite\u662f\u4e00\u4e2a\u9488\u5bf9\u673a\u5668\u4eba\u5b66\u4e60\u7684\u79fb\u52a8\u64cd\u4f5c\u53ef\u53d8\u5f62\u7269\u4f53\u4efb\u52a1\u5957\u4ef6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6807\u51c6\u5316\u57fa\u51c6\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u5b66\u4e60\u7b97\u6cd5\u5728\u5904\u7406\u53ef\u53d8\u5f62\u7269\u4f53\u65f6\u9762\u4e34\u6311\u6218\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u3002", "method": "\u63d0\u51faMoDeSuite\uff0c\u5305\u542b\u516b\u4e2a\u4efb\u52a1\uff0c\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u6d4b\u8bd5\u3002", "result": "\u5c55\u793a\u4e86\u7b97\u6cd5\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5957\u4ef6\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "MoDeSuite\u6709\u671b\u63a8\u52a8\u6d89\u53ca\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u79fb\u52a8\u64cd\u4f5c\u7814\u7a76\u3002"}}
{"id": "2507.21814", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.21814", "abs": "https://arxiv.org/abs/2507.21814", "authors": ["Yicheng Guo", "Chengkai Xu", "Jiaqi Liu", "Hao Zhang", "Peng Hang", "Jian Sun"], "title": "Interactive Adversarial Testing of Autonomous Vehicles with Adjustable Confrontation Intensity", "comment": null, "summary": "Scientific testing techniques are essential for ensuring the safe operation\nof autonomous vehicles (AVs), with high-risk, highly interactive scenarios\nbeing a primary focus. To address the limitations of existing testing methods,\nsuch as their heavy reliance on high-quality test data, weak interaction\ncapabilities, and low adversarial robustness, this paper proposes ExamPPO, an\ninteractive adversarial testing framework that enables scenario-adaptive and\nintensity-controllable evaluation of autonomous vehicles. The framework models\nthe Surrounding Vehicle (SV) as an intelligent examiner, equipped with a\nmulti-head attention-enhanced policy network, enabling context-sensitive and\nsustained behavioral interventions. A scalar confrontation factor is introduced\nto modulate the intensity of adversarial behaviors, allowing continuous,\nfine-grained adjustment of test difficulty. Coupled with structured evaluation\nmetrics, ExamPPO systematically probes AV's robustness across diverse scenarios\nand strategies. Extensive experiments across multiple scenarios and AV\nstrategies demonstrate that ExamPPO can effectively modulate adversarial\nbehavior, expose decision-making weaknesses in tested AVs, and generalize\nacross heterogeneous environments, thereby offering a unified and reproducible\nsolution for evaluating the safety and intelligence of autonomous\ndecision-making systems.", "AI": {"tldr": "\u63d0\u51faExamPPO\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u9002\u5e94\u548c\u53ef\u63a7\u5f3a\u5ea6\u7684\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u6d4b\u8bd5\uff0c\u901a\u8fc7\u667a\u80fd\u8003\u5b98\u6a21\u578b\u548c\u5bf9\u6297\u884c\u4e3a\u8c03\u8282\uff0c\u7cfb\u7edf\u6027\u8bc4\u4f30AV\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6d4b\u8bd5\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6570\u636e\u3001\u4ea4\u4e92\u80fd\u529b\u5f31\u548c\u5bf9\u6297\u9c81\u68d2\u6027\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u6ce8\u610f\u529b\u5934\u589e\u5f3a\u7684\u7b56\u7565\u7f51\u7edc\u5efa\u6a21\u667a\u80fd\u8003\u5b98\uff0c\u5f15\u5165\u5bf9\u6297\u56e0\u5b50\u8c03\u8282\u6d4b\u8bd5\u5f3a\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eExamPPO\u80fd\u6709\u6548\u8c03\u8282\u5bf9\u6297\u884c\u4e3a\uff0c\u66b4\u9732AV\u51b3\u7b56\u5f31\u70b9\uff0c\u5e76\u9002\u5e94\u5f02\u6784\u73af\u5883\u3002", "conclusion": "ExamPPO\u4e3a\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u667a\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u53ef\u590d\u73b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.21859", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.21859", "abs": "https://arxiv.org/abs/2507.21859", "authors": ["Michael Kaiser", "Clemens Gro\u00df", "Lisa Marie Otto", "Steffen M\u00fcller"], "title": "Evaluating Interactions between Automated Vehicles and Cyclists using a coupled In-the-Loop Test Environment", "comment": null, "summary": "Testing and evaluating automated driving systems (ADS) in interactions with\nvulnerable road users (VRUs), such as cyclists, are essential for improving the\nsafety of VRUs, but often lack realism. This paper presents and validates a\ncoupled in-the-loop test environment that integrates a Cyclist-in-the Loop test\nbench with a Vehicle-in-the-Loop test bench via a virtual environment (VE)\ndeveloped in Unreal Engine 5. The setup enables closed-loop, bidirectional\ninteraction between a real human cyclist and a real automated vehicle under\nsafe and controllable conditions. The automated vehicle reacts to cyclist\ngestures via stimulated camera input, while the cyclist, riding a stationary\nbicycle, perceives and reacts to the vehicle in the VE in real time. Validation\nexperiments are conducted using a real automated shuttle bus with a\ntrack-and-follow function, performing three test maneuvers - straight-line\ndriving with stop, circular track driving, and double lane change - on a\nproving ground and in the coupled in-the-loop test environment. The performance\nis evaluated by comparing the resulting vehicle trajectories in both\nenvironments. Additionally, the introduced latencies of individual components\nin the test setup are measured. The results demonstrate the feasibility of the\napproach and highlight its strengths and limitations for realistic ADS\nevaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u8026\u5408\u7684\u95ed\u73af\u6d4b\u8bd5\u73af\u5883\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e0e\u9a91\u884c\u8005\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u901a\u8fc7\u865a\u62df\u73af\u5883\u5b9e\u73b0\u771f\u5b9e\u9a91\u884c\u8005\u548c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u53cc\u5411\u4e92\u52a8\u3002", "motivation": "\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e0e\u9a91\u884c\u8005\u4ea4\u4e92\u6d4b\u8bd5\u7684\u771f\u5b9e\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408Cyclist-in-the-Loop\u548cVehicle-in-the-Loop\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u901a\u8fc7Unreal Engine 5\u865a\u62df\u73af\u5883\u5b9e\u73b0\u53cc\u5411\u4ea4\u4e92\u3002", "result": "\u9a8c\u8bc1\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u53ef\u884c\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u771f\u5b9e\u81ea\u52a8\u9a7e\u9a76\u8bc4\u4f30\u4e2d\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u6d4b\u8bd5\u73af\u5883\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e0e\u9a91\u884c\u8005\u4ea4\u4e92\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b89\u5168\u4e14\u53ef\u63a7\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2507.21896", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.21896", "abs": "https://arxiv.org/abs/2507.21896", "authors": ["Dominic Guri", "George Kantor"], "title": "A Systematic Robot Design Optimization Methodology with Application to Redundant Dual-Arm Manipulators", "comment": "8 pages, 6 figures, 2 tables", "summary": "One major recurring challenge in deploying manipulation robots is determining\nthe optimal placement of manipulators to maximize performance. This challenge\nis exacerbated in complex, cluttered agricultural environments of high-value\ncrops, such as flowers, fruits, and vegetables, that could greatly benefit from\nrobotic systems tailored to their specific requirements. However, the design of\nsuch systems remains a challenging, intuition-driven process, limiting the\naffordability and adoption of robotics-based automation by domain experts like\nfarmers. To address this challenge, we propose a four-part design optimization\nmethodology for automating the development of task-specific robotic systems.\nThis framework includes (a) a robot design model, (b) task and environment\nrepresentations for simulation, (c) task-specific performance metrics, and (d)\noptimization algorithms for refining configurations. We demonstrate our\nframework by optimizing a dual-arm robotic system for pepper harvesting using\ntwo off-the-shelf redundant manipulators. To enhance performance, we introduce\nnovel task metrics that leverage self-motion manifolds to characterize\nmanipulator redundancy comprehensively. Our results show that our framework\nachieves simultaneous improvements in reachability success rates and\nimprovements in dexterity. Specifically, our approach improves reachability\nsuccess by at least 14\\% over baseline methods and achieves over 30\\%\nimprovement in dexterity based on our task-specific metric.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56db\u90e8\u5206\u8bbe\u8ba1\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5f00\u53d1\u4efb\u52a1\u4e13\u7528\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5e76\u5728\u8fa3\u6912\u91c7\u6458\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u519c\u4e1a\u73af\u5883\u4e2d\u673a\u5668\u4eba\u90e8\u7f72\u7684\u76f4\u89c9\u9a71\u52a8\u8bbe\u8ba1\u95ee\u9898\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u53ef\u8d1f\u62c5\u6027\u3002", "method": "\u5305\u62ec\u673a\u5668\u4eba\u8bbe\u8ba1\u6a21\u578b\u3001\u4efb\u52a1\u4e0e\u73af\u5883\u6a21\u62df\u8868\u793a\u3001\u4efb\u52a1\u6027\u80fd\u6307\u6807\u548c\u4f18\u5316\u7b97\u6cd5\uff0c\u5229\u7528\u81ea\u8fd0\u52a8\u6d41\u5f62\u91cf\u5316\u5197\u4f59\u3002", "result": "\u5728\u53ef\u8fbe\u6027\u6210\u529f\u7387\u4e0a\u81f3\u5c11\u63d0\u534714%\uff0c\u7075\u5de7\u6027\u63d0\u534730%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u4e13\u7528\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4e3a\u519c\u4e1a\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.21957", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.21957", "abs": "https://arxiv.org/abs/2507.21957", "authors": ["Dominic Guri", "George Kantor"], "title": "ODE Methods for Computing One-Dimensional Self-Motion Manifolds", "comment": null, "summary": "Redundant manipulators are well understood to offer infinite joint\nconfigurations for achieving a desired end-effector pose. The multiplicity of\ninverse kinematics (IK) solutions allows for the simultaneous solving of\nauxiliary tasks like avoiding joint limits or obstacles. However, the most\nwidely used IK solvers are numerical gradient-based iterative methods that\ninherently return a locally optimal solution. In this work, we explore the\ncomputation of self-motion manifolds (SMMs), which represent the set of all\njoint configurations that solve the inverse kinematics problem for redundant\nmanipulators. Thus, SMMs are global IK solutions for redundant manipulators. We\nfocus on task redundancies of dimensionality 1, introducing a novel ODE\nformulation for computing SMMs using standard explicit fixed-step ODE\nintegrators. We also address the challenge of ``inducing'' redundancy in\notherwise non-redundant manipulators assigned to tasks naturally described by\none degree of freedom less than the non-redundant manipulator. Furthermore,\nrecognizing that SMMs can consist of multiple disconnected components, we\npropose methods for searching for these separate SMM components. Our\nformulations and algorithms compute accurate SMM solutions without requiring\nadditional IK refinement, and we extend our methods to prismatic joint systems\n-- an area not covered in current SMM literature. This manuscript presents the\nderivation of these methods and several examples that show how the methods work\nand their limitations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u5197\u4f59\u673a\u68b0\u81c2\u81ea\u8fd0\u52a8\u6d41\u5f62\uff08SMMs\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7ODE\u516c\u5f0f\u548c\u56fa\u5b9a\u6b65\u957f\u79ef\u5206\u5668\u5b9e\u73b0\u5168\u5c40\u9006\u8fd0\u52a8\u5b66\uff08IK\uff09\u6c42\u89e3\uff0c\u5e76\u6269\u5c55\u5230\u68f1\u67f1\u5173\u8282\u7cfb\u7edf\u3002", "motivation": "\u5197\u4f59\u673a\u68b0\u81c2\u7684\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\u5b58\u5728\u591a\u4e2a\u5c40\u90e8\u6700\u4f18\u89e3\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u5168\u5c40\u89e3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8ba1\u7b97SMMs\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eODE\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u4e00\u7ef4\u4efb\u52a1\u5197\u4f59\u7684SMMs\uff0c\u5e76\u89e3\u51b3\u4e86\u975e\u5197\u4f59\u673a\u68b0\u81c2\u7684\u5197\u4f59\u8bf1\u5bfc\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u641c\u7d22SMMs\u591a\u8fde\u901a\u5206\u91cf\u7684\u65b9\u6cd5\u3002", "result": "\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u8ba1\u7b97SMMs\uff0c\u65e0\u9700\u989d\u5916\u7684IK\u7ec6\u5316\uff0c\u5e76\u6210\u529f\u6269\u5c55\u5230\u68f1\u67f1\u5173\u8282\u7cfb\u7edf\u3002", "conclusion": "\u672c\u6587\u7684\u65b9\u6cd5\u4e3a\u5197\u4f59\u673a\u68b0\u81c2\u7684\u5168\u5c40IK\u6c42\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u4f46\u5b58\u5728\u4e00\u5b9a\u7684\u5c40\u9650\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.21965", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.21965", "abs": "https://arxiv.org/abs/2507.21965", "authors": ["Yi Wang", "Peiyao Zhang", "Mojtaba Esfandiari", "Peter Gehlbach", "Iulian I. Iordachita"], "title": "A Deep Learning-Driven Autonomous System for Retinal Vein Cannulation: Validation Using a Chicken Embryo Model", "comment": null, "summary": "Retinal vein cannulation (RVC) is a minimally invasive microsurgical\nprocedure for treating retinal vein occlusion (RVO), a leading cause of vision\nimpairment. However, the small size and fragility of retinal veins, coupled\nwith the need for high-precision, tremor-free needle manipulation, create\nsignificant technical challenges. These limitations highlight the need for\nrobotic assistance to improve accuracy and stability. This study presents an\nautomated robotic system with a top-down microscope and B-scan optical\ncoherence tomography (OCT) imaging for precise depth sensing. Deep\nlearning-based models enable real-time needle navigation, contact detection,\nand vein puncture recognition, using a chicken embryo model as a surrogate for\nhuman retinal veins. The system autonomously detects needle position and\npuncture events with 85% accuracy. The experiments demonstrate notable\nreductions in navigation and puncture times compared to manual methods. Our\nresults demonstrate the potential of integrating advanced imaging and deep\nlearning to automate microsurgical tasks, providing a pathway for safer and\nmore reliable RVC procedures with enhanced precision and reproducibility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf\uff08OCT\uff09\u6210\u50cf\uff0c\u7528\u4e8e\u89c6\u7f51\u819c\u9759\u8109\u63d2\u7ba1\uff08RVC\uff09\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u624b\u672f\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u89c6\u7f51\u819c\u9759\u8109\u63d2\u7ba1\uff08RVC\uff09\u56e0\u9759\u8109\u5fae\u5c0f\u8106\u5f31\u4e14\u9700\u9ad8\u7cbe\u5ea6\u64cd\u4f5c\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u4e9f\u9700\u673a\u5668\u4eba\u8f85\u52a9\u63d0\u5347\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u5e26\u9876\u90e8\u663e\u5fae\u955c\u548cB-scan OCT\u6210\u50cf\u7684\u81ea\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b9e\u73b0\u5b9e\u65f6\u9488\u5934\u5bfc\u822a\u3001\u63a5\u89e6\u68c0\u6d4b\u548c\u9759\u8109\u7a7f\u523a\u8bc6\u522b\uff0c\u4f7f\u7528\u9e21\u80da\u80ce\u6a21\u578b\u6a21\u62df\u4eba\u7c7b\u89c6\u7f51\u819c\u9759\u8109\u3002", "result": "\u7cfb\u7edf\u81ea\u4e3b\u68c0\u6d4b\u9488\u5934\u4f4d\u7f6e\u548c\u7a7f\u523a\u4e8b\u4ef6\u7684\u51c6\u786e\u7387\u8fbe85%\uff0c\u5bfc\u822a\u548c\u7a7f\u523a\u65f6\u95f4\u8f83\u624b\u52a8\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u7ed3\u5408\u5148\u8fdb\u6210\u50cf\u548c\u6df1\u5ea6\u5b66\u4e60\u53ef\u81ea\u52a8\u5316\u663e\u5fae\u624b\u672f\u4efb\u52a1\uff0c\u4e3a\u66f4\u5b89\u5168\u3001\u53ef\u9760\u7684RVC\u624b\u672f\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.21981", "categories": ["cs.RO", "68T40", "I.2.9"], "pdf": "https://arxiv.org/pdf/2507.21981", "abs": "https://arxiv.org/abs/2507.21981", "authors": ["Yufei Jia", "Guangyu Wang", "Yuhang Dong", "Junzhe Wu", "Yupei Zeng", "Haonan Lin", "Zifan Wang", "Haizhou Ge", "Weibin Gu", "Kairui Ding", "Zike Yan", "Yunjie Cheng", "Yue Li", "Ziming Wang", "Chuxuan Li", "Wei Sui", "Lu Shi", "Guanzhong Tian", "Ruqi Huang", "Guyue Zhou"], "title": "DISCOVERSE: Efficient Robot Simulation in Complex High-Fidelity Environments", "comment": "8pages, IROS2025 (Camera Ready)", "summary": "We present the first unified, modular, open-source 3DGS-based simulation\nframework for Real2Sim2Real robot learning. It features a holistic Real2Sim\npipeline that synthesizes hyper-realistic geometry and appearance of complex\nreal-world scenarios, paving the way for analyzing and bridging the Sim2Real\ngap. Powered by Gaussian Splatting and MuJoCo, Discoverse enables massively\nparallel simulation of multiple sensor modalities and accurate physics, with\ninclusive supports for existing 3D assets, robot models, and ROS plugins,\nempowering large-scale robot learning and complex robotic benchmarks. Through\nextensive experiments on imitation learning, Discoverse demonstrates\nstate-of-the-art zero-shot Sim2Real transfer performance compared to existing\nsimulators. For code and demos: https://air-discoverse.github.io/.", "AI": {"tldr": "Discoverse\u662f\u4e00\u4e2a\u57fa\u4e8e3D\u9ad8\u65af\u6563\u5c04\uff083DGS\uff09\u7684\u7edf\u4e00\u3001\u6a21\u5757\u5316\u5f00\u6e90\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8eReal2Sim2Real\u673a\u5668\u4eba\u5b66\u4e60\uff0c\u652f\u6301\u591a\u4f20\u611f\u5668\u6a21\u6001\u548c\u7cbe\u786e\u7269\u7406\u6a21\u62df\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u573a\u666f\u7684\u51e0\u4f55\u548c\u5916\u89c2\u6a21\u62df\u95ee\u9898\uff0c\u7f29\u5c0fSim2Real\u5dee\u8ddd\uff0c\u63a8\u52a8\u5927\u89c4\u6a21\u673a\u5668\u4eba\u5b66\u4e60\u548c\u590d\u6742\u673a\u5668\u4eba\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u7ed3\u5408\u9ad8\u65af\u6563\u5c04\uff08Gaussian Splatting\uff09\u548cMuJoCo\uff0c\u63d0\u4f9b\u591a\u4f20\u611f\u5668\u5e76\u884c\u6a21\u62df\uff0c\u652f\u6301\u73b0\u67093D\u8d44\u4ea7\u3001\u673a\u5668\u4eba\u6a21\u578b\u548cROS\u63d2\u4ef6\u3002", "result": "\u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672cSim2Real\u8fc1\u79fb\u6027\u80fd\u3002", "conclusion": "Discoverse\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u903c\u771f\u7684\u4eff\u771f\u5e73\u53f0\uff0c\u663e\u8457\u63d0\u5347\u4e86Sim2Real\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2507.22042", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.22042", "abs": "https://arxiv.org/abs/2507.22042", "authors": ["Ruturaj Sambhus", "Kapi Ketan Mehta", "Ali MirMohammad Sadeghi", "Basit Muhammad Imran", "Jeeseop Kim", "Taizoon Chunawala", "Vittorio Pastore", "Sujith Vijayan", "Kaveh Akbari Hamed"], "title": "A Nonlinear MPC Framework for Loco-Manipulation of Quadrupedal Robots with Non-Negligible Manipulator Dynamics", "comment": null, "summary": "Model predictive control (MPC) combined with reduced-order template models\nhas emerged as a powerful tool for trajectory optimization in dynamic legged\nlocomotion. However, loco-manipulation tasks performed by legged robots\nintroduce additional complexity, necessitating computationally efficient MPC\nalgorithms capable of handling high-degree-of-freedom (DoF) models. This letter\npresents a computationally efficient nonlinear MPC (NMPC) framework tailored\nfor loco-manipulation tasks of quadrupedal robots equipped with robotic\nmanipulators whose dynamics are non-negligible relative to those of the\nquadruped. The proposed framework adopts a decomposition strategy that couples\nlocomotion template models -- such as the single rigid body (SRB) model -- with\na full-order dynamic model of the robotic manipulator for torque-level control.\nThis decomposition enables efficient real-time solution of the NMPC problem in\na receding horizon fashion at 60 Hz. The optimal state and input trajectories\ngenerated by the NMPC for locomotion are tracked by a low-level nonlinear\nwhole-body controller (WBC) running at 500 Hz, while the optimal torque\ncommands for the manipulator are directly applied. The layered control\narchitecture is validated through extensive numerical simulations and hardware\nexperiments on a 15-kg Unitree Go2 quadrupedal robot augmented with a 4.4-kg\n4-DoF Kinova arm. Given that the Kinova arm dynamics are non-negligible\nrelative to the Go2 base, the proposed NMPC framework demonstrates robust\nstability in performing diverse loco-manipulation tasks, effectively handling\nexternal disturbances, payload variations, and uneven terrain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u56db\u8db3\u673a\u5668\u4eba\u642d\u8f7d\u673a\u68b0\u81c2\u7684\u9ad8\u6548\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u7b56\u7565\u5b9e\u73b0\u5b9e\u65f6\u8ba1\u7b97\uff0c\u5e76\u5728\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u3002", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u5728\u6267\u884c\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u65f6\uff0c\u7531\u4e8e\u673a\u68b0\u81c2\u52a8\u529b\u5b66\u4e0d\u53ef\u5ffd\u7565\uff0c\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u63a7\u5236\u6846\u67b6\u3002", "method": "\u91c7\u7528\u5206\u89e3\u7b56\u7565\uff0c\u5c06\u5355\u521a\u4f53\uff08SRB\uff09\u6a21\u578b\u4e0e\u673a\u68b0\u81c2\u5168\u9636\u52a8\u529b\u5b66\u6a21\u578b\u7ed3\u5408\uff0c\u5b9e\u73b0\u626d\u77e9\u7ea7\u63a7\u5236\uff0cNMPC\u4ee560 Hz\u9891\u7387\u5b9e\u65f6\u6c42\u89e3\u3002", "result": "\u5728Unitree Go2\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0cNMPC\u6846\u67b6\u80fd\u591f\u7a33\u5b9a\u6267\u884c\u591a\u6837\u5316\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\uff0c\u6709\u6548\u5e94\u5bf9\u5e72\u6270\u3001\u8d1f\u8f7d\u53d8\u5316\u548c\u4e0d\u5e73\u5766\u5730\u5f62\u3002", "conclusion": "\u63d0\u51fa\u7684NMPC\u6846\u67b6\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u3002"}}
