{"id": "2506.21627", "categories": ["cs.RO", "cs.AI", "F.4.3; I.2.9"], "pdf": "https://arxiv.org/pdf/2506.21627", "abs": "https://arxiv.org/abs/2506.21627", "authors": ["Shiyi Wang", "Wenbo Li", "Yiteng Chen", "Qingyao Wu", "Huiping Zhuang"], "title": "FrankenBot: Brain-Morphic Modular Orchestration for Robotic Manipulation with Vision-Language Models", "comment": "15 pages, 4 figures, under review of NeurIPS", "summary": "Developing a general robot manipulation system capable of performing a wide\nrange of tasks in complex, dynamic, and unstructured real-world environments\nhas long been a challenging task. It is widely recognized that achieving\nhuman-like efficiency and robustness manipulation requires the robotic brain to\nintegrate a comprehensive set of functions, such as task planning, policy\ngeneration, anomaly monitoring and handling, and long-term memory, achieving\nhigh-efficiency operation across all functions. Vision-Language Models (VLMs),\npretrained on massive multimodal data, have acquired rich world knowledge,\nexhibiting exceptional scene understanding and multimodal reasoning\ncapabilities. However, existing methods typically focus on realizing only a\nsingle function or a subset of functions within the robotic brain, without\nintegrating them into a unified cognitive architecture. Inspired by a\ndivide-and-conquer strategy and the architecture of the human brain, we propose\nFrankenBot, a VLM-driven, brain-morphic robotic manipulation framework that\nachieves both comprehensive functionality and high operational efficiency. Our\nframework includes a suite of components, decoupling a part of key functions\nfrom frequent VLM calls, striking an optimal balance between functional\ncompleteness and system efficiency. Specifically, we map task planning, policy\ngeneration, memory management, and low-level interfacing to the cortex,\ncerebellum, temporal lobe-hippocampus complex, and brainstem, respectively, and\ndesign efficient coordination mechanisms for the modules. We conducted\ncomprehensive experiments in both simulation and real-world robotic\nenvironments, demonstrating that our method offers significant advantages in\nanomaly detection and handling, long-term memory, operational efficiency, and\nstability -- all without requiring any fine-tuning or retraining.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6FrankenBot\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u8111\u7ed3\u6784\u5b9e\u73b0\u591a\u529f\u80fd\u96c6\u6210\u4e0e\u9ad8\u6548\u64cd\u4f5c\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u6267\u884c\u591a\u79cd\u4efb\u52a1\u7684\u901a\u7528\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u9700\u8981\u6574\u5408\u4efb\u52a1\u89c4\u5212\u3001\u7b56\u7565\u751f\u6210\u7b49\u529f\u80fd\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u5355\u4e00\u529f\u80fd\u3002", "method": "\u91c7\u7528\u5206\u6cbb\u7b56\u7565\u548c\u4eba\u8111\u7ed3\u6784\u542f\u53d1\uff0c\u5c06\u4efb\u52a1\u89c4\u5212\u3001\u7b56\u7565\u751f\u6210\u7b49\u529f\u80fd\u6620\u5c04\u5230\u4e0d\u540c\u8111\u533a\uff0c\u8bbe\u8ba1\u9ad8\u6548\u534f\u8c03\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFrankenBot\u5728\u5f02\u5e38\u68c0\u6d4b\u3001\u957f\u671f\u8bb0\u5fc6\u548c\u64cd\u4f5c\u6548\u7387\u7b49\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e14\u65e0\u9700\u5fae\u8c03\u6216\u91cd\u65b0\u8bad\u7ec3\u3002", "conclusion": "FrankenBot\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u529f\u80fd\u5168\u9762\u6027\u4e0e\u64cd\u4f5c\u6548\u7387\u7684\u5e73\u8861\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.21628", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21628", "abs": "https://arxiv.org/abs/2506.21628", "authors": ["Magnus Dierking", "Christopher E. Mower", "Sarthak Das", "Huang Helong", "Jiacheng Qiu", "Cody Reading", "Wei Chen", "Huidong Liang", "Huang Guowei", "Jan Peters", "Quan Xingyue", "Jun Wang", "Haitham Bou-Ammar"], "title": "Ark: An Open-source Python-based Framework for Robot Learning", "comment": null, "summary": "Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics\nChallenges to the first humanoid-robot kickboxing tournament-yet commercial\nautonomy still lags behind progress in machine learning. A major bottleneck is\nsoftware: current robot stacks demand steep learning curves, low-level C/C++\nexpertise, fragmented tooling, and intricate hardware integration, in stark\ncontrast to the Python-centric, well-documented ecosystems that propelled\nmodern AI. We introduce ARK, an open-source, Python-first robotics framework\ndesigned to close that gap. ARK presents a Gym-style environment interface that\nallows users to collect data, preprocess it, and train policies using\nstate-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)\nwhile seamlessly toggling between high-fidelity simulation and physical robots.\nA lightweight client-server architecture provides networked\npublisher-subscriber communication, and optional C/C++ bindings ensure\nreal-time performance when needed. ARK ships with reusable modules for control,\nSLAM, motion planning, system identification, and visualization, along with\nnative ROS interoperability. Comprehensive documentation and case studies-from\nmanipulation to mobile navigation-demonstrate rapid prototyping, effortless\nhardware swapping, and end-to-end pipelines that rival the convenience of\nmainstream machine-learning workflows. By unifying robotics and AI practices\nunder a common Python umbrella, ARK lowers entry barriers and accelerates\nresearch and commercial deployment of autonomous robots.", "AI": {"tldr": "ARK\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u3001\u4ee5Python\u4e3a\u6838\u5fc3\u7684\u673a\u5668\u4eba\u6846\u67b6\uff0c\u65e8\u5728\u7b80\u5316\u673a\u5668\u4eba\u8f6f\u4ef6\u5f00\u53d1\uff0c\u964d\u4f4e\u5b66\u4e60\u95e8\u69db\uff0c\u5e76\u52a0\u901f\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u7814\u7a76\u548c\u5546\u4e1a\u90e8\u7f72\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u8f6f\u4ef6\u6808\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\uff0c\u5de5\u5177\u5206\u6563\uff0c\u786c\u4ef6\u96c6\u6210\u590d\u6742\uff0c\u4e0ePython\u4e3b\u5bfc\u7684\u73b0\u4ee3AI\u751f\u6001\u7cfb\u7edf\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\uff0cARK\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "ARK\u63d0\u4f9bGym\u98ce\u683c\u7684\u73af\u5883\u63a5\u53e3\uff0c\u652f\u6301\u6570\u636e\u6536\u96c6\u3001\u9884\u5904\u7406\u548c\u7b56\u7565\u8bad\u7ec3\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u67b6\u6784\u548cC/C++\u7ed1\u5b9a\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u7075\u6d3b\u6027\u3002", "result": "ARK\u5c55\u793a\u4e86\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u3001\u786c\u4ef6\u65e0\u7f1d\u5207\u6362\u548c\u7aef\u5230\u7aef\u6d41\u7a0b\u7684\u4fbf\u5229\u6027\uff0c\u53ef\u4e0e\u4e3b\u6d41\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u5ab2\u7f8e\u3002", "conclusion": "ARK\u901a\u8fc7\u7edf\u4e00Python\u751f\u6001\u4e0b\u7684\u673a\u5668\u4eba\u548cAI\u5b9e\u8df5\uff0c\u964d\u4f4e\u4e86\u5165\u95e8\u95e8\u69db\uff0c\u52a0\u901f\u4e86\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u7814\u7a76\u548c\u5546\u4e1a\u5e94\u7528\u3002"}}
{"id": "2506.21630", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21630", "abs": "https://arxiv.org/abs/2506.21630", "authors": ["Yixin Sun", "Li Li", "Wenke E", "Amir Atapour-Abarghouei", "Toby P. Breckon"], "title": "TOMD: A Trail-based Off-road Multimodal Dataset for Traversable Pathway Segmentation under Challenging Illumination Conditions", "comment": "8 pages, 9 figures, 2025 IJCNN", "summary": "Detecting traversable pathways in unstructured outdoor environments remains a\nsignificant challenge for autonomous robots, especially in critical\napplications such as wide-area search and rescue, as well as incident\nmanagement scenarios like forest fires. Existing datasets and models primarily\ntarget urban settings or wide, vehicle-traversable off-road tracks, leaving a\nsubstantial gap in addressing the complexity of narrow, trail-like off-road\nscenarios. To address this, we introduce the Trail-based Off-road Multimodal\nDataset (TOMD), a comprehensive dataset specifically designed for such\nenvironments. TOMD features high-fidelity multimodal sensor data -- including\n128-channel LiDAR, stereo imagery, GNSS, IMU, and illumination measurements --\ncollected through repeated traversals under diverse conditions. We also propose\na dynamic multiscale data fusion model for accurate traversable pathway\nprediction. The study analyzes the performance of early, cross, and mixed\nfusion strategies under varying illumination levels. Results demonstrate the\neffectiveness of our approach and the relevance of illumination in segmentation\nperformance. We publicly release TOMD at https://github.com/yyyxs1125/TMOD to\nsupport future research in trail-based off-road navigation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u72ed\u7a84\u8d8a\u91ce\u5c0f\u5f84\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6TOMD\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u52a8\u6001\u591a\u5c3a\u5ea6\u6570\u636e\u878d\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u51c6\u786e\u9884\u6d4b\u53ef\u901a\u884c\u8def\u5f84\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u57ce\u5e02\u6216\u5bbd\u9614\u7684\u8d8a\u91ce\u73af\u5883\uff0c\u65e0\u6cd5\u6ee1\u8db3\u72ed\u7a84\u5c0f\u5f84\u573a\u666f\u7684\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728\u641c\u7d22\u6551\u63f4\u548c\u68ee\u6797\u706b\u707e\u7b49\u5173\u952e\u5e94\u7528\u4e2d\u3002", "method": "\u5f15\u5165TOMD\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\uff0c\u5e76\u63d0\u51fa\u52a8\u6001\u591a\u5c3a\u5ea6\u6570\u636e\u878d\u5408\u6a21\u578b\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u878d\u5408\u7b56\u7565\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u4e14\u5149\u7167\u6761\u4ef6\u5bf9\u5206\u5272\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "TOMD\u6570\u636e\u96c6\u548c\u63d0\u51fa\u7684\u6a21\u578b\u4e3a\u8d8a\u91ce\u5c0f\u5f84\u5bfc\u822a\u7814\u7a76\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u6269\u5c55\u3002"}}
{"id": "2506.21631", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21631", "abs": "https://arxiv.org/abs/2506.21631", "authors": ["Tianliang Yao", "Bingrui Li", "Bo Lu", "Zhiqiang Pei", "Yixuan Yuan", "Peng Qi"], "title": "Real-Time 3D Guidewire Reconstruction from Intraoperative DSA Images for Robot-Assisted Endovascular Interventions", "comment": "This paper has been accepted by IEEE/RSJ IROS 2025", "summary": "Accurate three-dimensional (3D) reconstruction of guidewire shapes is crucial\nfor precise navigation in robot-assisted endovascular interventions.\nConventional 2D Digital Subtraction Angiography (DSA) is limited by the absence\nof depth information, leading to spatial ambiguities that hinder reliable\nguidewire shape sensing. This paper introduces a novel multimodal framework for\nreal-time 3D guidewire reconstruction, combining preoperative 3D Computed\nTomography Angiography (CTA) with intraoperative 2D DSA images. The method\nutilizes robust feature extraction to address noise and distortion in 2D DSA\ndata, followed by deformable image registration to align the 2D projections\nwith the 3D CTA model. Subsequently, the inverse projection algorithm\nreconstructs the 3D guidewire shape, providing real-time, accurate spatial\ninformation. This framework significantly enhances spatial awareness for\nrobotic-assisted endovascular procedures, effectively bridging the gap between\npreoperative planning and intraoperative execution. The system demonstrates\nnotable improvements in real-time processing speed, reconstruction accuracy,\nand computational efficiency. The proposed method achieves a projection error\nof 1.76$\\pm$0.08 pixels and a length deviation of 2.93$\\pm$0.15\\%, with a frame\nrate of 39.3$\\pm$1.5 frames per second (FPS). These advancements have the\npotential to optimize robotic performance and increase the precision of complex\nendovascular interventions, ultimately contributing to better clinical\noutcomes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u672f\u524d3D CTA\u548c\u672f\u4e2d2D DSA\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f63D\u5bfc\u4e1d\u91cd\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u8f85\u52a9\u8840\u7ba1\u5185\u624b\u672f\u7684\u7a7a\u95f4\u611f\u77e5\u548c\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf2D DSA\u7f3a\u4e4f\u6df1\u5ea6\u4fe1\u606f\uff0c\u5bfc\u81f4\u7a7a\u95f4\u6a21\u7cca\u6027\uff0c\u9650\u5236\u4e86\u5bfc\u4e1d\u5f62\u72b6\u7684\u53ef\u9760\u611f\u77e5\u3002", "method": "\u91c7\u7528\u9c81\u68d2\u7279\u5f81\u63d0\u53d6\u5904\u74062D DSA\u6570\u636e\u566a\u58f0\uff0c\u901a\u8fc7\u53ef\u53d8\u5f62\u56fe\u50cf\u914d\u51c6\u5bf9\u9f502D\u6295\u5f71\u4e0e3D CTA\u6a21\u578b\uff0c\u6700\u540e\u5229\u7528\u9006\u6295\u5f71\u7b97\u6cd5\u91cd\u5efa3D\u5bfc\u4e1d\u5f62\u72b6\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e861.76\u00b10.08\u50cf\u7d20\u7684\u6295\u5f71\u8bef\u5dee\u548c2.93\u00b10.15%\u7684\u957f\u5ea6\u504f\u5dee\uff0c\u5e27\u7387\u4e3a39.3\u00b11.5 FPS\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f18\u5316\u4e86\u673a\u5668\u4eba\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u590d\u6742\u8840\u7ba1\u5185\u624b\u672f\u7684\u7cbe\u5ea6\uff0c\u6709\u671b\u6539\u5584\u4e34\u5e8a\u6548\u679c\u3002"}}
{"id": "2506.21635", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21635", "abs": "https://arxiv.org/abs/2506.21635", "authors": ["Haiping Yang", "Huaxing Liu", "Wei Wu", "Zuohui Chen", "Ning Wu"], "title": "AeroLite-MDNet: Lightweight Multi-task Deviation Detection Network for UAV Landing", "comment": null, "summary": "Unmanned aerial vehicles (UAVs) are increasingly employed in diverse\napplications such as land surveying, material transport, and environmental\nmonitoring. Following missions like data collection or inspection, UAVs must\nland safely at docking stations for storage or recharging, which is an\nessential requirement for ensuring operational continuity. However, accurate\nlanding remains challenging due to factors like GPS signal interference. To\naddress this issue, we propose a deviation warning system for UAV landings,\npowered by a novel vision-based model called AeroLite-MDNet. This model\nintegrates a multiscale fusion module for robust cross-scale object detection\nand incorporates a segmentation branch for efficient orientation estimation. We\nintroduce a new evaluation metric, Average Warning Delay (AWD), to quantify the\nsystem's sensitivity to landing deviations. Furthermore, we contribute a new\ndataset, UAVLandData, which captures real-world landing deviation scenarios to\nsupport training and evaluation. Experimental results show that our system\nachieves an AWD of 0.7 seconds with a deviation detection accuracy of 98.6\\%,\ndemonstrating its effectiveness in enhancing UAV landing reliability. Code will\nbe available at https://github.com/ITTTTTI/Maskyolo.git", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u65e0\u4eba\u673a\u7740\u9646\u504f\u5dee\u9884\u8b66\u7cfb\u7edfAeroLite-MDNet\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u878d\u5408\u6a21\u5757\u548c\u5206\u5272\u5206\u652f\u63d0\u9ad8\u68c0\u6d4b\u548c\u65b9\u5411\u4f30\u8ba1\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u65b0\u8bc4\u4f30\u6307\u6807AWD\u548c\u65b0\u6570\u636e\u96c6UAVLandData\u3002\u5b9e\u9a8c\u663e\u793a\u7cfb\u7edfAWD\u4e3a0.7\u79d2\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe98.6%\u3002", "motivation": "\u65e0\u4eba\u673a\u7740\u9646\u65f6\u56e0GPS\u4fe1\u53f7\u5e72\u6270\u7b49\u95ee\u9898\u96be\u4ee5\u7cbe\u51c6\u7740\u9646\uff0c\u5f71\u54cd\u64cd\u4f5c\u8fde\u7eed\u6027\uff0c\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faAeroLite-MDNet\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u878d\u5408\u6a21\u5757\u548c\u5206\u5272\u5206\u652f\uff0c\u7528\u4e8e\u68c0\u6d4b\u7740\u9646\u504f\u5dee\u548c\u4f30\u8ba1\u65b9\u5411\uff1b\u5f15\u5165AWD\u6307\u6807\u548c\u65b0\u6570\u636e\u96c6UAVLandData\u3002", "result": "\u7cfb\u7edfAWD\u4e3a0.7\u79d2\uff0c\u504f\u5dee\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe98.6%\uff0c\u663e\u8457\u63d0\u5347\u7740\u9646\u53ef\u9760\u6027\u3002", "conclusion": "AeroLite-MDNet\u7cfb\u7edf\u80fd\u6709\u6548\u89e3\u51b3\u65e0\u4eba\u673a\u7740\u9646\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u9ad8\u7740\u9646\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2506.21689", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21689", "abs": "https://arxiv.org/abs/2506.21689", "authors": ["Jason Lim", "Florian Richter", "Zih-Yun Chiu", "Jaeyon Lee", "Ethan Quist", "Nathan Fisher", "Jonathan Chambers", "Steven Hong", "Michael C. Yip"], "title": "Optimal Motion Scaling for Delayed Telesurgery", "comment": "Accepted to IROS 2025", "summary": "Robotic teleoperation over long communication distances poses challenges due\nto delays in commands and feedback from network latency. One simple yet\neffective strategy to reduce errors and increase performance under delay is to\ndownscale the relative motion between the operating surgeon and the robot. The\nquestion remains as to what is the optimal scaling factor, and how this value\nchanges depending on the level of latency as well as operator tendencies. We\npresent user studies investigating the relationship between latency, scaling\nfactor, and performance. The results of our studies demonstrate a statistically\nsignificant difference in performance between users and across scaling factors\nfor certain levels of delay. These findings indicate that the optimal scaling\nfactor for a given level of delay is specific to each user, motivating the need\nfor personalized models for optimal performance. We present techniques to model\nthe user-specific mapping of latency level to scaling factor for optimal\nperformance, leading to an efficient and effective solution to optimizing\nperformance of robotic teleoperation and specifically telesurgery under large\ncommunication delay.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u957f\u8ddd\u79bb\u901a\u4fe1\u5ef6\u8fdf\u4e0b\uff0c\u901a\u8fc7\u8c03\u6574\u673a\u5668\u4eba\u8fd0\u52a8\u7f29\u653e\u56e0\u5b50\u4f18\u5316\u8fdc\u7a0b\u624b\u672f\u673a\u5668\u4eba\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u4e2a\u6027\u5316\u7f29\u653e\u56e0\u5b50\u5bf9\u6027\u80fd\u63d0\u5347\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u89e3\u51b3\u957f\u8ddd\u79bb\u901a\u4fe1\u5ef6\u8fdf\u5bfc\u81f4\u7684\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u63a2\u7d22\u7f29\u653e\u56e0\u5b50\u4e0e\u5ef6\u8fdf\u53ca\u64cd\u4f5c\u8005\u4e60\u60ef\u7684\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\u5206\u6790\u5ef6\u8fdf\u3001\u7f29\u653e\u56e0\u5b50\u4e0e\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u5e76\u5f00\u53d1\u4e2a\u6027\u5316\u6a21\u578b\u4ee5\u4f18\u5316\u7f29\u653e\u56e0\u5b50\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u5ef6\u8fdf\u6c34\u5e73\u4e0b\uff0c\u7f29\u653e\u56e0\u5b50\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e14\u6700\u4f18\u7f29\u653e\u56e0\u5b50\u56e0\u4eba\u800c\u5f02\u3002", "conclusion": "\u4e2a\u6027\u5316\u7f29\u653e\u56e0\u5b50\u6a21\u578b\u80fd\u6709\u6548\u4f18\u5316\u8fdc\u7a0b\u624b\u672f\u673a\u5668\u4eba\u5728\u9ad8\u5ef6\u8fdf\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.21732", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.21732", "abs": "https://arxiv.org/abs/2506.21732", "authors": ["Ameya Salvi", "Venkat Krovi"], "title": "Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation", "comment": null, "summary": "Vision-based lane keeping is a topic of significant interest in the robotics\nand autonomous ground vehicles communities in various on-road and off-road\napplications. The skid-steered vehicle architecture has served as a useful\nvehicle platform for human controlled operations. However, systematic modeling,\nespecially of the skid-slip wheel terrain interactions (primarily in off-road\nsettings) has created bottlenecks for automation deployment. End-to-end\nlearning based methods such as imitation learning and deep reinforcement\nlearning, have gained prominence as a viable deployment option to counter the\nlack of accurate analytical models. However, the systematic formulation and\nsubsequent verification/validation in dynamic operation regimes (particularly\nfor skid-steered vehicles) remains a work in progress. To this end, a novel\napproach for structured formulation for learning visual navigation is proposed\nand investigated in this work. Extensive software simulations, hardware\nevaluations and ablation studies now highlight the significantly improved\nperformance of the proposed approach against contemporary literature.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u89c6\u89c9\u5bfc\u822a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6ed1\u79fb\u8f6c\u5411\u8f66\u8f86\u5728\u52a8\u6001\u64cd\u4f5c\u4e2d\u7f3a\u4e4f\u51c6\u786e\u5206\u6790\u6a21\u578b\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u6a21\u4eff\u5b66\u4e60\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u63d0\u51fa\u7ed3\u6784\u5316\u5b66\u4e60\u89c6\u89c9\u5bfc\u822a\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u8f6f\u4ef6\u6a21\u62df\u3001\u786c\u4ef6\u8bc4\u4f30\u548c\u6d88\u878d\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u7ed3\u6784\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3a\u89c6\u89c9\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6587\u732e\u3002"}}
{"id": "2506.21853", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21853", "abs": "https://arxiv.org/abs/2506.21853", "authors": ["Dewei Wang", "Chenjia Ba", "Chenhui Li", "Jiyuan Shi", "Yan Ding", "Chi Zhang", "Bin Zhao"], "title": "Skill-Nav: Enhanced Navigation with Versatile Quadrupedal Locomotion via Waypoint Interface", "comment": "17pages, 6 figures", "summary": "Quadrupedal robots have demonstrated exceptional locomotion capabilities\nthrough Reinforcement Learning (RL), including extreme parkour maneuvers.\nHowever, integrating locomotion skills with navigation in quadrupedal robots\nhas not been fully investigated, which holds promise for enhancing\nlong-distance movement capabilities. In this paper, we propose Skill-Nav, a\nmethod that incorporates quadrupedal locomotion skills into a hierarchical\nnavigation framework using waypoints as an interface. Specifically, we train a\nwaypoint-guided locomotion policy using deep RL, enabling the robot to\nautonomously adjust its locomotion skills to reach targeted positions while\navoiding obstacles. Compared with direct velocity commands, waypoints offer a\nsimpler yet more flexible interface for high-level planning and low-level\ncontrol. Utilizing waypoints as the interface allows for the application of\nvarious general planning tools, such as large language models (LLMs) and path\nplanning algorithms, to guide our locomotion policy in traversing terrains with\ndiverse obstacles. Extensive experiments conducted in both simulated and\nreal-world scenarios demonstrate that Skill-Nav can effectively traverse\ncomplex terrains and complete challenging navigation tasks.", "AI": {"tldr": "Skill-Nav\u65b9\u6cd5\u901a\u8fc7\u5c06\u56db\u8db3\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u6280\u80fd\u4e0e\u5bfc\u822a\u7ed3\u5408\uff0c\u5229\u7528\u8def\u5f84\u70b9\u4f5c\u4e3a\u63a5\u53e3\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u5730\u5f62\u7684\u9ad8\u6548\u5bfc\u822a\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5c06\u56db\u8db3\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u6280\u80fd\u4e0e\u5bfc\u822a\u7ed3\u5408\uff0c\u4ee5\u63d0\u5347\u957f\u8ddd\u79bb\u79fb\u52a8\u80fd\u529b\u3002", "method": "\u63d0\u51faSkill-Nav\u65b9\u6cd5\uff0c\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8def\u5f84\u70b9\u5f15\u5bfc\u7684\u8fd0\u52a8\u7b56\u7565\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u81ea\u4e3b\u8c03\u6574\u6280\u80fd\u4ee5\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\u5e76\u907f\u5f00\u969c\u788d\u3002", "result": "\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u573a\u666f\u4e2d\uff0cSkill-Nav\u80fd\u6709\u6548\u7a7f\u8d8a\u590d\u6742\u5730\u5f62\u5e76\u5b8c\u6210\u6311\u6218\u6027\u5bfc\u822a\u4efb\u52a1\u3002", "conclusion": "Skill-Nav\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u7684\u5bfc\u822a\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21860", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21860", "abs": "https://arxiv.org/abs/2506.21860", "authors": ["Xiangyu Shi", "Yanyuan Qiao", "Lingqiao Liu", "Feras Dayoub"], "title": "Embodied Domain Adaptation for Object Detection", "comment": "Accepted by IROS 2025", "summary": "Mobile robots rely on object detectors for perception and object localization\nin indoor environments. However, standard closed-set methods struggle to handle\nthe diverse objects and dynamic conditions encountered in real homes and labs.\nOpen-vocabulary object detection (OVOD), driven by Vision Language Models\n(VLMs), extends beyond fixed labels but still struggles with domain shifts in\nindoor environments. We introduce a Source-Free Domain Adaptation (SFDA)\napproach that adapts a pre-trained model without accessing source data. We\nrefine pseudo labels via temporal clustering, employ multi-scale threshold\nfusion, and apply a Mean Teacher framework with contrastive learning. Our\nEmbodied Domain Adaptation for Object Detection (EDAOD) benchmark evaluates\nadaptation under sequential changes in lighting, layout, and object diversity.\nOur experiments show significant gains in zero-shot detection performance and\nflexible adaptation to dynamic indoor conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6e90\u6570\u636e\u7684\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff08SFDA\uff09\uff0c\u901a\u8fc7\u65f6\u95f4\u805a\u7c7b\u548c\u591a\u5c3a\u5ea6\u9608\u503c\u878d\u5408\u6539\u8fdb\u4f2a\u6807\u7b7e\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u7684Mean Teacher\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5ba4\u5185\u52a8\u6001\u73af\u5883\u4e0b\u7684\u96f6\u6837\u672c\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6807\u51c6\u95ed\u96c6\u65b9\u6cd5\u548c\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u68c0\u6d4b\uff08OVOD\uff09\u5728\u5ba4\u5185\u73af\u5883\u4e2d\u56e0\u9886\u57df\u504f\u79fb\u548c\u52a8\u6001\u6761\u4ef6\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6e90\u6570\u636e\u65e0\u5173\u7684\u9886\u57df\u81ea\u9002\u5e94\uff08SFDA\uff09\uff0c\u901a\u8fc7\u65f6\u95f4\u805a\u7c7b\u4f18\u5316\u4f2a\u6807\u7b7e\uff0c\u591a\u5c3a\u5ea6\u9608\u503c\u878d\u5408\uff0c\u4ee5\u53ca\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u7684Mean Teacher\u6846\u67b6\u3002", "result": "\u5728EDAOD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u96f6\u6837\u672c\u68c0\u6d4b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u5e76\u80fd\u7075\u6d3b\u9002\u5e94\u52a8\u6001\u5ba4\u5185\u6761\u4ef6\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u9886\u57df\u9002\u5e94\u95ee\u9898\uff0c\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u7684\u611f\u77e5\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21982", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.21982", "abs": "https://arxiv.org/abs/2506.21982", "authors": ["Akshay Jaitly", "Jack Cline", "Siavash Farzan"], "title": "A MILP-Based Solution to Multi-Agent Motion Planning and Collision Avoidance in Constrained Environments", "comment": "Accepted to 2025 IEEE International Conference on Automation Science\n  and Engineering (CASE 2025)", "summary": "We propose a mixed-integer linear program (MILP) for multi-agent motion\nplanning that embeds Polytopic Action-based Motion Planning (PAAMP) into a\nsequence-then-solve pipeline. Region sequences confine each agent to adjacent\nconvex polytopes, while a big-M hyperplane model enforces inter-agent\nseparation. Collision constraints are applied only to agents sharing or\nneighboring a region, which reduces binary variables exponentially compared\nwith naive formulations. An L1 path-length-plus-acceleration cost yields smooth\ntrajectories. We prove finite-time convergence and demonstrate on\nrepresentative multi-agent scenarios with obstacles that our formulation\nproduces collision-free trajectories an order of magnitude faster than an\nunstructured MILP baseline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u89c4\u5212\uff0c\u901a\u8fc7\u5d4c\u5165PAAMP\u548c\u5e8f\u5217-\u6c42\u89e3\u6d41\u7a0b\uff0c\u663e\u8457\u51cf\u5c11\u53d8\u91cf\u6570\u91cf\u5e76\u63d0\u9ad8\u6c42\u89e3\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u89c4\u5212\u4e2d\u4f20\u7edfMILP\u65b9\u6cd5\u53d8\u91cf\u8fc7\u591a\u3001\u6c42\u89e3\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408PAAMP\u548c\u5e8f\u5217-\u6c42\u89e3\u6d41\u7a0b\uff0c\u4f7f\u7528\u533a\u57df\u5e8f\u5217\u9650\u5236\u667a\u80fd\u4f53\u8fd0\u52a8\uff0c\u5e76\u901a\u8fc7big-M\u8d85\u5e73\u9762\u6a21\u578b\u786e\u4fdd\u667a\u80fd\u4f53\u5206\u79bb\uff0c\u51cf\u5c11\u78b0\u649e\u7ea6\u675f\u7684\u53d8\u91cf\u6570\u91cf\u3002", "result": "\u5728\u4ee3\u8868\u6027\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u975e\u7ed3\u6784\u5316MILP\u57fa\u7ebf\u5feb\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u4e14\u80fd\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u53d8\u91cf\u548c\u7ea6\u675f\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u89c4\u5212\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.22028", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22028", "abs": "https://arxiv.org/abs/2506.22028", "authors": ["Ossi Parikka", "Roel Pieters"], "title": "LMPVC and Policy Bank: Adaptive voice control for industrial robots with code generating LLMs and reusable Pythonic policies", "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN). For further information, videos and\n  code, see https://github.com/ozzyuni/LMPVC", "summary": "Modern industry is increasingly moving away from mass manufacturing, towards\nmore specialized and personalized products. As manufacturing tasks become more\ncomplex, full automation is not always an option, human involvement may be\nrequired. This has increased the need for advanced human robot collaboration\n(HRC), and with it, improved methods for interaction, such as voice control.\nRecent advances in natural language processing, driven by artificial\nintelligence (AI), have the potential to answer this demand. Large language\nmodels (LLMs) have rapidly developed very impressive general reasoning\ncapabilities, and many methods of applying this to robotics have been proposed,\nincluding through the use of code generation. This paper presents Language\nModel Program Voice Control (LMPVC), an LLM-based prototype voice control\narchitecture with integrated policy programming and teaching capabilities,\nbuilt for use with Robot Operating System 2 (ROS2) compatible robots. The\narchitecture builds on prior works using code generation for voice control by\nimplementing an additional programming and teaching system, the Policy Bank. We\nfind this system can compensate for the limitations of the underlying LLM, and\nallow LMPVC to adapt to different downstream tasks without a slow and costly\ntraining process. The architecture and additional results are released on\nGitHub (https://github.com/ozzyuni/LMPVC).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8bed\u97f3\u63a7\u5236\u67b6\u6784LMPVC\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edfROS2\uff0c\u901a\u8fc7\u96c6\u6210\u7b56\u7565\u7f16\u7a0b\u548c\u6559\u5b66\u529f\u80fd\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u5236\u9020\u4efb\u52a1\u4e2d\u7684\u4eba\u673a\u534f\u4f5c\u9700\u6c42\u3002", "motivation": "\u73b0\u4ee3\u5236\u9020\u4e1a\u8d8b\u5411\u4e2a\u6027\u5316\u548c\u590d\u6742\u5316\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u4eba\u673a\u534f\u4f5c\uff08HRC\uff09\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u8bed\u97f3\u63a7\u5236\u3002AI\u9a71\u52a8\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u4e3a\u6b64\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "LMPVC\u67b6\u6784\u57fa\u4e8eLLM\uff0c\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u5b9e\u73b0\u8bed\u97f3\u63a7\u5236\uff0c\u5e76\u5f15\u5165\u7b56\u7565\u94f6\u884c\uff08Policy Bank\uff09\u7cfb\u7edf\uff0c\u652f\u6301\u7f16\u7a0b\u548c\u6559\u5b66\u529f\u80fd\u3002", "result": "LMPVC\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\uff0c\u65e0\u9700\u8017\u65f6\u8d39\u529b\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5f25\u8865\u4e86\u5e95\u5c42LLM\u7684\u5c40\u9650\u6027\u3002", "conclusion": "LMPVC\u4e3a\u590d\u6742\u5236\u9020\u4efb\u52a1\u4e2d\u7684\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5df2\u5728GitHub\u4e0a\u5f00\u6e90\u3002"}}
{"id": "2506.22034", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22034", "abs": "https://arxiv.org/abs/2506.22034", "authors": ["Kejia Chen", "Celina Dettmering", "Florian Pachler", "Zhuo Liu", "Yue Zhang", "Tailai Cheng", "Jonas Dirr", "Zhenshan Bing", "Alois Knoll", "R\u00fcdiger Daub"], "title": "Multi-Robot Assembly of Deformable Linear Objects Using Multi-Modal Perception", "comment": null, "summary": "Industrial assembly of deformable linear objects (DLOs) such as cables offers\ngreat potential for many industries. However, DLOs pose several challenges for\nrobot-based automation due to the inherent complexity of deformation and,\nconsequentially, the difficulties in anticipating the behavior of DLOs in\ndynamic situations. Although existing studies have addressed isolated\nsubproblems like shape tracking, grasping, and shape control, there has been\nlimited exploration of integrated workflows that combine these individual\nprocesses. To address this gap, we propose an object-centric perception and\nplanning framework to achieve a comprehensive DLO assembly process throughout\nthe industrial value chain. The framework utilizes visual and tactile\ninformation to track the DLO's shape as well as contact state across different\nstages, which facilitates effective planning of robot actions. Our approach\nencompasses robot-based bin picking of DLOs from cluttered environments,\nfollowed by a coordinated handover to two additional robots that mount the DLOs\nonto designated fixtures. Real-world experiments employing a setup with\nmultiple robots demonstrate the effectiveness of the approach and its relevance\nto industrial scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u8c61\u611f\u77e5\u548c\u89c4\u5212\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u5de5\u4e1a\u4ef7\u503c\u94fe\u4e2d\u5168\u9762\u7684\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\uff08DLO\uff09\u88c5\u914d\u8fc7\u7a0b\u3002", "motivation": "\u5de5\u4e1a\u4e2dDLO\uff08\u5982\u7535\u7f06\uff09\u7684\u88c5\u914d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u7531\u4e8e\u5176\u53d8\u5f62\u590d\u6742\u6027\u548c\u52a8\u6001\u884c\u4e3a\u96be\u4ee5\u9884\u6d4b\uff0c\u673a\u5668\u4eba\u81ea\u52a8\u5316\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u4ec5\u89e3\u51b3\u90e8\u5206\u5b50\u95ee\u9898\uff0c\u7f3a\u4e4f\u96c6\u6210\u5316\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u5229\u7528\u89c6\u89c9\u548c\u89e6\u89c9\u4fe1\u606f\u8ddf\u8e2aDLO\u7684\u5f62\u72b6\u548c\u63a5\u89e6\u72b6\u6001\uff0c\u7ed3\u5408\u673a\u5668\u4eba\u52a8\u4f5c\u89c4\u5212\uff0c\u5b9e\u73b0\u4ece\u6742\u4e71\u73af\u5883\u4e2d\u5206\u62e3\u5230\u591a\u673a\u5668\u4eba\u534f\u540c\u88c5\u914d\u7684\u5168\u6d41\u7a0b\u3002", "result": "\u901a\u8fc7\u591a\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u53ca\u5176\u5728\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aDLO\u7684\u5de5\u4e1a\u88c5\u914d\u63d0\u4f9b\u4e86\u96c6\u6210\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2506.22087", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22087", "abs": "https://arxiv.org/abs/2506.22087", "authors": ["Armand Jordana", "Jianghan Zhang", "Joseph Amigo", "Ludovic Righetti"], "title": "An Introduction to Zero-Order Optimization Techniques for Robotics", "comment": null, "summary": "Zero-order optimization techniques are becoming increasingly popular in\nrobotics due to their ability to handle non-differentiable functions and escape\nlocal minima. These advantages make them particularly useful for trajectory\noptimization and policy optimization. In this work, we propose a mathematical\ntutorial on random search. It offers a simple and unifying perspective for\nunderstanding a wide range of algorithms commonly used in robotics. Leveraging\nthis viewpoint, we classify many trajectory optimization methods under a common\nframework and derive novel competitive RL algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5173\u4e8e\u968f\u673a\u641c\u7d22\u7684\u6570\u5b66\u6559\u7a0b\uff0c\u4e3a\u7406\u89e3\u673a\u5668\u4eba\u5b66\u4e2d\u5e38\u7528\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7edf\u4e00\u89c6\u89d2\uff0c\u5e76\u57fa\u4e8e\u6b64\u5206\u7c7b\u4e86\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u63a8\u5bfc\u51fa\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u96f6\u9636\u4f18\u5316\u6280\u672f\u5728\u673a\u5668\u4eba\u5b66\u4e2d\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\uff0c\u56e0\u5176\u80fd\u5904\u7406\u4e0d\u53ef\u5fae\u51fd\u6570\u548c\u9003\u79bb\u5c40\u90e8\u6781\u5c0f\u503c\uff0c\u9002\u7528\u4e8e\u8f68\u8ff9\u4f18\u5316\u548c\u7b56\u7565\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u968f\u673a\u641c\u7d22\u7684\u6570\u5b66\u6559\u7a0b\uff0c\u63d0\u4f9b\u7edf\u4e00\u89c6\u89d2\uff0c\u5206\u7c7b\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u57fa\u4e8e\u7edf\u4e00\u89c6\u89d2\uff0c\u63a8\u5bfc\u51fa\u65b0\u7684\u7ade\u4e89\u6027\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "conclusion": "\u968f\u673a\u641c\u7d22\u7684\u6570\u5b66\u6559\u7a0b\u4e3a\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7b80\u5355\u800c\u7edf\u4e00\u7684\u89c6\u89d2\uff0c\u5e76\u63a8\u52a8\u4e86\u65b0\u7b97\u6cd5\u7684\u5f00\u53d1\u3002"}}
{"id": "2506.22116", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22116", "abs": "https://arxiv.org/abs/2506.22116", "authors": ["Noora Sassali", "Roel Pieters"], "title": "Evaluating Pointing Gestures for Target Selection in Human-Robot Collaboration", "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN). Preprint", "summary": "Pointing gestures are a common interaction method used in Human-Robot\nCollaboration for various tasks, ranging from selecting targets to guiding\nindustrial processes. This study introduces a method for localizing pointed\ntargets within a planar workspace. The approach employs pose estimation, and a\nsimple geometric model based on shoulder-wrist extension to extract gesturing\ndata from an RGB-D stream. The study proposes a rigorous methodology and\ncomprehensive analysis for evaluating pointing gestures and target selection in\ntypical robotic tasks. In addition to evaluating tool accuracy, the tool is\nintegrated into a proof-of-concept robotic system, which includes object\ndetection, speech transcription, and speech synthesis to demonstrate the\nintegration of multiple modalities in a collaborative application. Finally, a\ndiscussion over tool limitations and performance is provided to understand its\nrole in multimodal robotic systems. All developments are available at:\nhttps://github.com/NMKsas/gesture_pointer.git.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u59ff\u6001\u4f30\u8ba1\u548c\u51e0\u4f55\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5e73\u9762\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u5b9a\u4f4d\u6307\u5411\u76ee\u6807\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u6307\u5411\u624b\u52bf\u662f\u4eba\u673a\u534f\u4f5c\u4e2d\u5e38\u89c1\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u4f46\u7f3a\u4e4f\u4e00\u79cd\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u5b9a\u4f4d\u548c\u8bc4\u4f30\u6307\u5411\u76ee\u6807\u7684\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u59ff\u6001\u4f30\u8ba1\u548c\u57fa\u4e8e\u80a9-\u8155\u4f38\u5c55\u7684\u51e0\u4f55\u6a21\u578b\uff0c\u4eceRGB-D\u6d41\u4e2d\u63d0\u53d6\u624b\u52bf\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u591a\u6a21\u6001\u6280\u672f\uff08\u5982\u7269\u4f53\u68c0\u6d4b\u548c\u8bed\u97f3\u5408\u6210\uff09\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e25\u8c28\u7684\u65b9\u6cd5\u8bba\uff0c\u5e76\u901a\u8fc7\u6982\u5ff5\u9a8c\u8bc1\u673a\u5668\u4eba\u7cfb\u7edf\u5c55\u793a\u4e86\u591a\u6a21\u6001\u96c6\u6210\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u5de5\u5177\u5728\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u5e94\u5bf9\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9650\u5236\u3002"}}
{"id": "2506.22170", "categories": ["cs.RO", "math.OC", "00A69, 93C85, 14H55", "I.2.9"], "pdf": "https://arxiv.org/pdf/2506.22170", "abs": "https://arxiv.org/abs/2506.22170", "authors": ["Yu Zhang", "Xiao-Song Yang"], "title": "RM-Dijkstra: A surface optimal path planning algorithm based on Riemannian metric", "comment": "7 pages", "summary": "The Dijkstra algorithm is a classic path planning method, which operates in a\ndiscrete graph space to determine the shortest path from a specified source\npoint to a target node or all other nodes based on non-negative edge weights.\nNumerous studies have focused on the Dijkstra algorithm due to its potential\napplication. However, its application in surface path planning for mobile\nrobots remains largely unexplored. In this letter, a surface optimal path\nplanning algorithm called RM-Dijkstra is proposed, which is based on Riemannian\nmetric model. By constructing a new Riemannian metric on the 2D projection\nplane, the surface optimal path planning problem is therefore transformed into\na geometric problem on the 2D plane with new Riemannian metric. Induced by the\nstandard Euclidean metric on surface, the constructed new metric reflects\nenvironmental information of the robot and ensures that the projection map is\nan isometric immersion. By conducting a series of simulation tests, the\nexperimental results demonstrate that the RM-Dijkstra algorithm not only\neffectively solves the optimal path planning problem on surfaces, but also\noutperforms traditional path planning algorithms in terms of path accuracy and\nsmoothness, particularly in complex scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ece\u66fc\u5ea6\u91cf\u7684\u8868\u9762\u6700\u4f18\u8def\u5f84\u89c4\u5212\u7b97\u6cd5RM-Dijkstra\uff0c\u901a\u8fc7\u5c06\u8868\u9762\u8def\u5f84\u89c4\u5212\u95ee\u9898\u8f6c\u5316\u4e3a\u4e8c\u7ef4\u5e73\u9762\u4e0a\u7684\u51e0\u4f55\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8def\u5f84\u7cbe\u5ea6\u548c\u5e73\u6ed1\u5ea6\u3002", "motivation": "Dijkstra\u7b97\u6cd5\u5728\u79bb\u6563\u56fe\u7a7a\u95f4\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u79fb\u52a8\u673a\u5668\u4eba\u8868\u9762\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u57fa\u4e8e\u9ece\u66fc\u5ea6\u91cf\u6a21\u578b\uff0c\u5728\u4e8c\u7ef4\u6295\u5f71\u5e73\u9762\u4e0a\u6784\u9020\u65b0\u7684\u9ece\u66fc\u5ea6\u91cf\uff0c\u5c06\u8868\u9762\u6700\u4f18\u8def\u5f84\u89c4\u5212\u95ee\u9898\u8f6c\u5316\u4e3a\u4e8c\u7ef4\u5e73\u9762\u4e0a\u7684\u51e0\u4f55\u95ee\u9898\u3002", "result": "RM-Dijkstra\u7b97\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u4f18\u4e8e\u4f20\u7edf\u7b97\u6cd5\uff0c\u8def\u5f84\u7cbe\u5ea6\u548c\u5e73\u6ed1\u5ea6\u663e\u8457\u63d0\u5347\u3002", "conclusion": "RM-Dijkstra\u7b97\u6cd5\u4e3a\u8868\u9762\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2506.22174", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22174", "abs": "https://arxiv.org/abs/2506.22174", "authors": ["Bavo Lesy", "Siemen Herremans", "Robin Kerstens", "Jan Steckel", "Walter Daems", "Siegfried Mercelis", "Ali Anwar"], "title": "ASVSim (AirSim for Surface Vehicles): A High-Fidelity Simulation Framework for Autonomous Surface Vehicle Research", "comment": "14 Pages, 11 Figures", "summary": "The transport industry has recently shown significant interest in unmanned\nsurface vehicles (USVs), specifically for port and inland waterway transport.\nThese systems can improve operational efficiency and safety, which is\nespecially relevant in the European Union, where initiatives such as the Green\nDeal are driving a shift towards increased use of inland waterways. At the same\ntime, a shortage of qualified personnel is accelerating the adoption of\nautonomous solutions. However, there is a notable lack of open-source,\nhigh-fidelity simulation frameworks and datasets for developing and evaluating\nsuch solutions. To address these challenges, we introduce AirSim For Surface\nVehicles (ASVSim), an open-source simulation framework specifically designed\nfor autonomous shipping research in inland and port environments. The framework\ncombines simulated vessel dynamics with marine sensor simulation capabilities,\nincluding radar and camera systems and supports the generation of synthetic\ndatasets for training computer vision models and reinforcement learning agents.\nBuilt upon Cosys-AirSim, ASVSim provides a comprehensive platform for\ndeveloping autonomous navigation algorithms and generating synthetic datasets.\nThe simulator supports research of both traditional control methods and deep\nlearning-based approaches. Through limited experiments, we demonstrate the\npotential of the simulator in these research areas. ASVSim is provided as an\nopen-source project under the MIT license, making autonomous navigation\nresearch accessible to a larger part of the ocean engineering community.", "AI": {"tldr": "ASVSim\u662f\u4e00\u4e2a\u5f00\u6e90\u4eff\u771f\u6846\u67b6\uff0c\u4e13\u4e3a\u5185\u6cb3\u548c\u6e2f\u53e3\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u822a\u8fd0\u7814\u7a76\u8bbe\u8ba1\uff0c\u7ed3\u5408\u4e86\u8239\u8236\u52a8\u529b\u5b66\u548c\u6d77\u6d0b\u4f20\u611f\u5668\u6a21\u62df\u529f\u80fd\u3002", "motivation": "\u8fd0\u8f93\u884c\u4e1a\u5bf9\u65e0\u4eba\u6c34\u9762\u8f66\u8f86\uff08USVs\uff09\u7684\u5174\u8da3\u589e\u52a0\uff0c\u4f46\u7f3a\u4e4f\u5f00\u6e90\u9ad8\u4fdd\u771f\u4eff\u771f\u6846\u67b6\u548c\u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u81ea\u4e3b\u89e3\u51b3\u65b9\u6848\u7684\u5f00\u53d1\u4e0e\u8bc4\u4f30\u3002", "method": "\u57fa\u4e8eCosys-AirSim\u6784\u5efa\uff0cASVSim\u63d0\u4f9b\u8239\u8236\u52a8\u529b\u5b66\u6a21\u62df\u3001\u6d77\u6d0b\u4f20\u611f\u5668\uff08\u5982\u96f7\u8fbe\u548c\u6444\u50cf\u5934\uff09\u4eff\u771f\uff0c\u5e76\u652f\u6301\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\u3002", "result": "\u901a\u8fc7\u6709\u9650\u5b9e\u9a8c\u5c55\u793a\u4e86ASVSim\u5728\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "ASVSim\u4f5c\u4e3a\u5f00\u6e90\u9879\u76ee\uff0c\u4e3a\u6d77\u6d0b\u5de5\u7a0b\u793e\u533a\u63d0\u4f9b\u4e86\u81ea\u4e3b\u5bfc\u822a\u7814\u7a76\u7684\u53ef\u8bbf\u95ee\u5e73\u53f0\u3002"}}
{"id": "2506.22176", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22176", "abs": "https://arxiv.org/abs/2506.22176", "authors": ["Holly Dinkel", "Raghavendra Navaratna", "Jingyi Xiang", "Brian Coltin", "Trey Smith", "Timothy Bretl"], "title": "KnotDLO: Toward Interpretable Knot Tying", "comment": "4 pages, 5 figures, presented at the Workshop on 3D Visual\n  Representations for Manipulation at the 2023 IEEE International Conference on\n  Robotics and Automation in Yokohama, Japan. Video presentation\n  [https://youtu.be/mg30uCUtpOk]. Poster\n  [https://hollydinkel.github.io/assets/pdf/ICRA20243DVRM_poster.pdf] 3DVRM\n  Workshop [https://3d-manipulation-workshop.github.io/]", "summary": "This work presents KnotDLO, a method for one-handed Deformable Linear Object\n(DLO) knot tying that is robust to occlusion, repeatable for varying rope\ninitial configurations, interpretable for generating motion policies, and\nrequires no human demonstrations or training. Grasp and target waypoints for\nfuture DLO states are planned from the current DLO shape. Grasp poses are\ncomputed from indexing the tracked piecewise linear curve representing the DLO\nstate based on the current curve shape and are piecewise continuous. KnotDLO\ncomputes intermediate waypoints from the geometry of the current DLO state and\nthe desired next state. The system decouples visual reasoning from control. In\n16 trials of knot tying, KnotDLO achieves a 50% success rate in tying an\noverhand knot from previously unseen configurations.", "AI": {"tldr": "KnotDLO\u662f\u4e00\u79cd\u65e0\u9700\u4eba\u7c7b\u6f14\u793a\u6216\u8bad\u7ec3\u7684\u5355\u624b\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\uff08DLO\uff09\u6253\u7ed3\u65b9\u6cd5\uff0c\u5177\u6709\u6297\u906e\u6321\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3DLO\u6253\u7ed3\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u5982\u906e\u6321\u3001\u521d\u59cb\u914d\u7f6e\u53d8\u5316\u4ee5\u53ca\u9700\u8981\u4eba\u7c7b\u6f14\u793a\u6216\u8bad\u7ec3\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f53\u524dDLO\u5f62\u72b6\u89c4\u5212\u6293\u53d6\u548c\u76ee\u6807\u8def\u5f84\u70b9\uff0c\u5229\u7528\u5206\u6bb5\u7ebf\u6027\u66f2\u7ebf\u7d22\u5f15\u8ba1\u7b97\u6293\u53d6\u59ff\u6001\uff0c\u5e76\u6839\u636e\u51e0\u4f55\u5f62\u72b6\u751f\u6210\u4e2d\u95f4\u8def\u5f84\u70b9\u3002", "result": "\u572816\u6b21\u6253\u7ed3\u8bd5\u9a8c\u4e2d\uff0cKnotDLO\u5728\u4ece\u672a\u89c1\u8fc7\u7684\u914d\u7f6e\u4e0b\u5b9e\u73b0\u4e8650%\u7684\u6210\u529f\u7387\u3002", "conclusion": "KnotDLO\u5c55\u793a\u4e86\u65e0\u9700\u4eba\u7c7b\u5e72\u9884\u7684DLO\u6253\u7ed3\u80fd\u529b\uff0c\u5e76\u5177\u6709\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.22364", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22364", "abs": "https://arxiv.org/abs/2506.22364", "authors": ["Joe Johnson", "Phanender Chalasani", "Arnav Shah", "Ram L. Ray", "Muthukumar Bagavathiannan"], "title": "Robotic Multimodal Data Acquisition for In-Field Deep Learning Estimation of Cover Crop Biomass", "comment": "Accepted in the Extended Abstract, The 22nd International Conference\n  on Ubiquitous Robots (UR 2025), Texas, USA", "summary": "Accurate weed management is essential for mitigating significant crop yield\nlosses, necessitating effective weed suppression strategies in agricultural\nsystems. Integrating cover crops (CC) offers multiple benefits, including soil\nerosion reduction, weed suppression, decreased nitrogen requirements, and\nenhanced carbon sequestration, all of which are closely tied to the aboveground\nbiomass (AGB) they produce. However, biomass production varies significantly\ndue to microsite variability, making accurate estimation and mapping essential\nfor identifying zones of poor weed suppression and optimizing targeted\nmanagement strategies. To address this challenge, developing a comprehensive CC\nmap, including its AGB distribution, will enable informed decision-making\nregarding weed control methods and optimal application rates. Manual visual\ninspection is impractical and labor-intensive, especially given the extensive\nfield size and the wide diversity and variation of weed species and sizes. In\nthis context, optical imagery and Light Detection and Ranging (LiDAR) data are\ntwo prominent sources with unique characteristics that enhance AGB estimation.\nThis study introduces a ground robot-mounted multimodal sensor system designed\nfor agricultural field mapping. The system integrates optical and LiDAR data,\nleveraging machine learning (ML) methods for data fusion to improve biomass\npredictions. The best ML-based model for dry AGB estimation achieved a\ncoefficient of determination value of 0.88, demonstrating robust performance in\ndiverse field conditions. This approach offers valuable insights for\nsite-specific management, enabling precise weed suppression strategies and\npromoting sustainable farming practices.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u4f20\u611f\u5668\u548c\u673a\u5668\u5b66\u4e60\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u7cbe\u786e\u4f30\u7b97\u8986\u76d6\u4f5c\u7269\u7684\u5730\u4e0a\u751f\u7269\u91cf\uff08AGB\uff09\uff0c\u4ee5\u4f18\u5316\u6742\u8349\u7ba1\u7406\u7b56\u7565\u3002", "motivation": "\u51c6\u786e\u7684\u6742\u8349\u7ba1\u7406\u5bf9\u51cf\u5c11\u4f5c\u7269\u4ea7\u91cf\u635f\u5931\u81f3\u5173\u91cd\u8981\uff0c\u800c\u8986\u76d6\u4f5c\u7269\u7684\u5730\u4e0a\u751f\u7269\u91cf\uff08AGB\uff09\u662f\u5f71\u54cd\u6742\u8349\u6291\u5236\u6548\u679c\u7684\u5173\u952e\u56e0\u7d20\u3002\u7531\u4e8eAGB\u7684\u7a7a\u95f4\u53d8\u5f02\u6027\u5927\uff0c\u9700\u8981\u7cbe\u786e\u4f30\u7b97\u548c\u6620\u5c04\u4ee5\u5b9e\u73b0\u9488\u5bf9\u6027\u7ba1\u7406\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u5730\u9762\u673a\u5668\u4eba\u642d\u8f7d\u7684\u591a\u6a21\u6001\u4f20\u611f\u5668\u7cfb\u7edf\uff0c\u7ed3\u5408\u5149\u5b66\u5f71\u50cf\u548cLiDAR\u6570\u636e\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u6570\u636e\u878d\u5408\uff0c\u4ee5\u63d0\u9ad8AGB\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "result": "\u6700\u4f73\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9\u5e72\u71e5AGB\u7684\u4f30\u7b97\u8fbe\u5230\u4e860.88\u7684\u51b3\u5b9a\u7cfb\u6570\uff0c\u8868\u660e\u5176\u5728\u591a\u6837\u5316\u7530\u95f4\u6761\u4ef6\u4e0b\u5177\u6709\u7a33\u5065\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7cbe\u51c6\u6742\u8349\u6291\u5236\u7b56\u7565\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u53ef\u6301\u7eed\u519c\u4e1a\u5b9e\u8df5\u3002"}}
