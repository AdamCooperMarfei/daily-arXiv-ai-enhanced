{"id": "2507.08100", "categories": ["cs.RO", "cond-mat.soft", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.08100", "abs": "https://arxiv.org/abs/2507.08100", "authors": ["Lucy Liu", "Justin Werfel", "Federico Toschi", "L. Mahadevan"], "title": "Noise-Enabled Goal Attainment in Crowded Collectives", "comment": null, "summary": "In crowded environments, individuals must navigate around other occupants to\nreach their destinations. Understanding and controlling traffic flows in these\nspaces is relevant to coordinating robot swarms and designing infrastructure\nfor dense populations. Here, we combine simulations, theory, and robotic\nexperiments to study how noisy motion can disrupt traffic jams and enable flow\nas agents travel to individual goals. Above a critical noise level, large jams\ndo not persist. From this observation, we analytically approximate the goal\nattainment rate as a function of the noise level, then solve for the optimal\nagent density and noise level that maximize the swarm's goal attainment rate.\nWe perform robotic experiments to corroborate our simulated and theoretical\nresults. Finally, we compare simple, local navigation approaches with a\nsophisticated but computationally costly central planner. A simple reactive\nscheme performs well up to moderate densities and is far more computationally\nefficient than a planner, suggesting lessons for real-world problems.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6a21\u62df\u3001\u7406\u8bba\u548c\u673a\u5668\u4eba\u5b9e\u9a8c\uff0c\u63a2\u8ba8\u566a\u58f0\u8fd0\u52a8\u5982\u4f55\u6253\u7834\u4ea4\u901a\u5835\u585e\u5e76\u4f18\u5316\u7fa4\u4f53\u76ee\u6807\u8fbe\u6210\u7387\uff0c\u53d1\u73b0\u7b80\u5355\u53cd\u5e94\u6027\u5bfc\u822a\u5728\u4e2d\u7b49\u5bc6\u5ea6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728\u62e5\u6324\u73af\u5883\u4e2d\uff0c\u4e2a\u4f53\u9700\u7ed5\u8fc7\u4ed6\u4eba\u5230\u8fbe\u76ee\u7684\u5730\uff0c\u7406\u89e3\u5e76\u63a7\u5236\u4ea4\u901a\u6d41\u5bf9\u673a\u5668\u4eba\u7fa4\u4f53\u534f\u8c03\u548c\u57fa\u7840\u8bbe\u65bd\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7ed3\u5408\u6a21\u62df\u3001\u7406\u8bba\u548c\u673a\u5668\u4eba\u5b9e\u9a8c\uff0c\u5206\u6790\u566a\u58f0\u6c34\u5e73\u5bf9\u4ea4\u901a\u6d41\u7684\u5f71\u54cd\uff0c\u5e76\u6c42\u89e3\u6700\u4f18\u566a\u58f0\u6c34\u5e73\u548c\u4ee3\u7406\u5bc6\u5ea6\u3002", "result": "\u9ad8\u4e8e\u4e34\u754c\u566a\u58f0\u6c34\u5e73\u65f6\uff0c\u5927\u5835\u585e\u4e0d\u4f1a\u6301\u7eed\uff1b\u7b80\u5355\u53cd\u5e94\u6027\u5bfc\u822a\u5728\u4e2d\u7b49\u5bc6\u5ea6\u4e0b\u8868\u73b0\u9ad8\u6548\u3002", "conclusion": "\u7b80\u5355\u53cd\u5e94\u6027\u5bfc\u822a\u5728\u4e2d\u7b49\u5bc6\u5ea6\u4e0b\u4f18\u4e8e\u590d\u6742\u4e2d\u592e\u89c4\u5212\u5668\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u542f\u793a\u3002"}}
{"id": "2507.08112", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08112", "abs": "https://arxiv.org/abs/2507.08112", "authors": ["Lamiaa H. Zain", "Hossam H. Ammar", "Raafat E. Shalaby"], "title": "Imitation Learning for Obstacle Avoidance Using End-to-End CNN-Based Sensor Fusion", "comment": null, "summary": "Obstacle avoidance is crucial for mobile robots' navigation in both known and\nunknown environments. This research designs, trains, and tests two custom\nConvolutional Neural Networks (CNNs), using color and depth images from a depth\ncamera as inputs. Both networks adopt sensor fusion to produce an output: the\nmobile robot's angular velocity, which serves as the robot's steering command.\nA newly obtained visual dataset for navigation was collected in diverse\nenvironments with varying lighting conditions and dynamic obstacles. During\ndata collection, a communication link was established over Wi-Fi between a\nremote server and the robot, using Robot Operating System (ROS) topics.\nVelocity commands were transmitted from the server to the robot, enabling\nsynchronized recording of visual data and the corresponding steering commands.\nVarious evaluation metrics, such as Mean Squared Error, Variance Score, and\nFeed-Forward time, provided a clear comparison between the two networks and\nclarified which one to use for the application.", "AI": {"tldr": "\u7814\u7a76\u8bbe\u8ba1\u5e76\u6d4b\u8bd5\u4e86\u4e24\u79cd\u81ea\u5b9a\u4e49\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\uff0c\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u5728\u5df2\u77e5\u548c\u672a\u77e5\u73af\u5883\u4e2d\u7684\u907f\u969c\u5bfc\u822a\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u907f\u969c\u5bfc\u822a\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u9700\u8981\u9ad8\u6548\u7684\u89c6\u89c9\u611f\u77e5\u548c\u51b3\u7b56\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u76f8\u673a\u7684\u5f69\u8272\u548c\u6df1\u5ea6\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u901a\u8fc7\u4f20\u611f\u5668\u878d\u5408\u751f\u6210\u673a\u5668\u4eba\u7684\u89d2\u901f\u5ea6\u4f5c\u4e3a\u8f6c\u5411\u547d\u4ee4\u3002", "result": "\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u6536\u96c6\u4e86\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u6bd4\u8f83\u4e86\u4e24\u79cd\u7f51\u7edc\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u660e\u786e\u4e86\u54ea\u79cd\u7f51\u7edc\u66f4\u9002\u5408\u5b9e\u9645\u5e94\u7528\uff0c\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u907f\u969c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.08224", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08224", "abs": "https://arxiv.org/abs/2507.08224", "authors": ["Chan Young Park", "Jillian Fisher", "Marius Memmel", "Dipika Khullar", "Andy Yun", "Abhishek Gupta", "Yejin Choi"], "title": "Making VLMs More Robot-Friendly: Self-Critical Distillation of Low-Level Procedural Reasoning", "comment": "Code Available: https://github.com/chan0park/SelfReVision", "summary": "Large language models (LLMs) have shown promise in robotic procedural\nplanning, yet their human-centric reasoning often omits the low-level, grounded\ndetails needed for robotic execution. Vision-language models (VLMs) offer a\npath toward more perceptually grounded plans, but current methods either rely\non expensive, large-scale models or are constrained to narrow simulation\nsettings. We introduce SelfReVision, a lightweight and scalable\nself-improvement framework for vision-language procedural planning.\nSelfReVision enables small VLMs to iteratively critique, revise, and verify\ntheir own plans-without external supervision or teacher models-drawing\ninspiration from chain-of-thought prompting and self-instruct paradigms.\nThrough this self-distillation loop, models generate higher-quality,\nexecution-ready plans that can be used both at inference and for continued\nfine-tuning. Using models varying from 3B to 72B, our results show that\nSelfReVision not only boosts performance over weak base VLMs but also\noutperforms models 100X the size, yielding improved control in downstream\nembodied tasks.", "AI": {"tldr": "SelfReVision\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u81ea\u6539\u8fdb\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u7a0b\u5e8f\u89c4\u5212\u4e2d\u7684\u8868\u73b0\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u673a\u5668\u4eba\u7a0b\u5e8f\u89c4\u5212\u4e2d\u7f3a\u4e4f\u4f4e\u5c42\u6b21\u7ec6\u8282\uff0c\u6216\u4f9d\u8d56\u6602\u8d35\u7684\u5927\u6a21\u578b\u3002SelfReVision\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u81ea\u6211\u6279\u8bc4\u3001\u4fee\u8ba2\u548c\u9a8c\u8bc1\u7684\u5faa\u73af\uff08\u53d7\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u548c\u81ea\u6211\u6307\u5bfc\u8303\u5f0f\u542f\u53d1\uff09\uff0c\u5c0f\u578bVLMs\u80fd\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u3001\u53ef\u6267\u884c\u7684\u8ba1\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSelfReVision\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u578bVLMs\u7684\u6027\u80fd\uff0c\u751a\u81f3\u4f18\u4e8e100\u500d\u5927\u5c0f\u7684\u6a21\u578b\uff0c\u5e76\u6539\u5584\u4e86\u4e0b\u6e38\u5177\u4f53\u4efb\u52a1\u7684\u63a7\u5236\u80fd\u529b\u3002", "conclusion": "SelfReVision\u4e3a\u8f7b\u91cf\u7ea7VLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u6539\u8fdb\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u7a0b\u5e8f\u89c4\u5212\u3002"}}
{"id": "2507.08262", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08262", "abs": "https://arxiv.org/abs/2507.08262", "authors": ["Wenbo Cui", "Chengyang Zhao", "Yuhui Chen", "Haoran Li", "Zhizheng Zhang", "Dongbin Zhao", "He Wang"], "title": "CL3R: 3D Reconstruction and Contrastive Learning for Enhanced Robotic Manipulation Representations", "comment": null, "summary": "Building a robust perception module is crucial for visuomotor policy\nlearning. While recent methods incorporate pre-trained 2D foundation models\ninto robotic perception modules to leverage their strong semantic\nunderstanding, they struggle to capture 3D spatial information and generalize\nacross diverse camera viewpoints. These limitations hinder the policy's\neffectiveness, especially in fine-grained robotic manipulation scenarios. To\naddress these challenges, we propose CL3R, a novel 3D pre-training framework\ndesigned to enhance robotic manipulation policies. Our method integrates both\nspatial awareness and semantic understanding by employing a point cloud Masked\nAutoencoder to learn rich 3D representations while leveraging pre-trained 2D\nfoundation models through contrastive learning for efficient semantic knowledge\ntransfer. Additionally, we propose a 3D visual representation pre-training\nframework for robotic tasks. By unifying coordinate systems across datasets and\nintroducing random fusion of multi-view point clouds, we mitigate camera view\nambiguity and improve generalization, enabling robust perception from novel\nviewpoints at test time. Extensive experiments in both simulation and the real\nworld demonstrate the superiority of our method, highlighting its effectiveness\nin visuomotor policy learning for robotic manipulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCL3R\u6846\u67b6\uff0c\u7ed3\u54083D\u7a7a\u95f4\u611f\u77e5\u4e0e\u8bed\u4e49\u7406\u89e3\uff0c\u901a\u8fc7\u70b9\u4e91\u63a9\u7801\u81ea\u7f16\u7801\u5668\u548c\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u76842D\u57fa\u7840\u6a21\u578b\uff0c\u4f46\u7f3a\u4e4f3D\u7a7a\u95f4\u4fe1\u606f\u4e14\u96be\u4ee5\u6cdb\u5316\u5230\u4e0d\u540c\u89c6\u89d2\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u7cbe\u7ec6\u64cd\u4f5c\u7684\u6548\u679c\u3002", "method": "CL3R\u4f7f\u7528\u70b9\u4e91\u63a9\u7801\u81ea\u7f16\u7801\u5668\u5b66\u4e603D\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u7ed3\u54082D\u57fa\u7840\u6a21\u578b\u7684\u8bed\u4e49\u77e5\u8bc6\uff1b\u7edf\u4e00\u5750\u6807\u7cfb\u5e76\u878d\u5408\u591a\u89c6\u89d2\u70b9\u4e91\u4ee5\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCL3R\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "CL3R\u901a\u8fc73D\u9884\u8bad\u7ec3\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89d2\u6cdb\u5316\u548c\u7a7a\u95f4\u611f\u77e5\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u611f\u77e5\u6a21\u5757\u3002"}}
{"id": "2507.08303", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08303", "abs": "https://arxiv.org/abs/2507.08303", "authors": ["Yang Zhang", "Zhanxiang Cao", "Buqing Nie", "Haoyang Li", "Yue Gao"], "title": "Learning Robust Motion Skills via Critical Adversarial Attacks for Humanoid Robots", "comment": "10 pages, 9 figures", "summary": "Humanoid robots show significant potential in daily tasks. However,\nreinforcement learning-based motion policies often suffer from robustness\ndegradation due to the sim-to-real dynamics gap, thereby affecting the agility\nof real robots. In this work, we propose a novel robust adversarial training\nparadigm designed to enhance the robustness of humanoid motion policies in real\nworlds. The paradigm introduces a learnable adversarial attack network that\nprecisely identifies vulnerabilities in motion policies and applies targeted\nperturbations, forcing the motion policy to enhance its robustness against\nperturbations through dynamic adversarial training. We conduct experiments on\nthe Unitree G1 humanoid robot for both perceptive locomotion and whole-body\ncontrol tasks. The results demonstrate that our proposed method significantly\nenhances the robot's motion robustness in real world environments, enabling\nsuccessful traversal of challenging terrains and highly agile whole-body\ntrajectory tracking.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6297\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5bf9\u6297\u653b\u51fb\u7f51\u7edc\u589e\u5f3a\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u7b56\u7565\u7684\u9c81\u68d2\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u8fd0\u52a8\u7b56\u7565\u56e0\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u52a8\u529b\u5b66\u5dee\u5f02\u5bfc\u81f4\u9c81\u68d2\u6027\u4e0b\u964d\uff0c\u5f71\u54cd\u673a\u5668\u4eba\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u654f\u6377\u6027\u3002", "method": "\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u5bf9\u6297\u653b\u51fb\u7f51\u7edc\uff0c\u7cbe\u51c6\u8bc6\u522b\u8fd0\u52a8\u7b56\u7565\u7684\u8106\u5f31\u70b9\u5e76\u65bd\u52a0\u9488\u5bf9\u6027\u6270\u52a8\uff0c\u901a\u8fc7\u52a8\u6001\u5bf9\u6297\u8bad\u7ec3\u63d0\u5347\u9c81\u68d2\u6027\u3002", "result": "\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u9c81\u68d2\u6027\uff0c\u6210\u529f\u5e94\u5bf9\u590d\u6742\u5730\u5f62\u548c\u9ad8\u654f\u6377\u6027\u5168\u8eab\u8f68\u8ff9\u8ddf\u8e2a\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u7684\u5bf9\u6297\u8bad\u7ec3\u8303\u5f0f\u6709\u6548\u589e\u5f3a\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u9c81\u68d2\u6027\u548c\u654f\u6377\u6027\u3002"}}
{"id": "2507.08349", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08349", "abs": "https://arxiv.org/abs/2507.08349", "authors": ["Junhui Wang", "Yan Qiao", "Chao Gao", "Naiqi Wu"], "title": "Joint Optimization-based Targetless Extrinsic Calibration for Multiple LiDARs and GNSS-Aided INS of Ground Vehicles", "comment": null, "summary": "Accurate extrinsic calibration between multiple LiDAR sensors and a\nGNSS-aided inertial navigation system (GINS) is essential for achieving\nreliable sensor fusion in intelligent mining environments. Such calibration\nenables vehicle-road collaboration by aligning perception data from\nvehicle-mounted sensors to a unified global reference frame. However, existing\nmethods often depend on artificial targets, overlapping fields of view, or\nprecise trajectory estimation, which are assumptions that may not hold in\npractice. Moreover, the planar motion of mining vehicles leads to observability\nissues that degrade calibration performance. This paper presents a targetless\nextrinsic calibration method that aligns multiple onboard LiDAR sensors to the\nGINS coordinate system without requiring overlapping sensor views or external\ntargets. The proposed approach introduces an observation model based on the\nknown installation height of the GINS unit to constrain unobservable\ncalibration parameters under planar motion. A joint optimization framework is\ndeveloped to refine both the extrinsic parameters and GINS trajectory by\nintegrating multiple constraints derived from geometric correspondences and\nmotion consistency. The proposed method is applicable to heterogeneous LiDAR\nconfigurations, including both mechanical and solid-state sensors. Extensive\nexperiments on simulated and real-world datasets demonstrate the accuracy,\nrobustness, and practical applicability of the approach under diverse sensor\nsetups.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u76ee\u6807\u6216\u91cd\u53e0\u89c6\u573a\u7684\u591aLiDAR\u4e0eGINS\u5916\u53c2\u6807\u5b9a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5df2\u77e5\u9ad8\u5ea6\u7ea6\u675f\u548c\u8054\u5408\u4f18\u5316\u6846\u67b6\u89e3\u51b3\u5e73\u9762\u8fd0\u52a8\u4e0b\u7684\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\u3002", "motivation": "\u667a\u80fd\u91c7\u77ff\u73af\u5883\u4e2d\uff0c\u591aLiDAR\u4e0eGINS\u7684\u7cbe\u786e\u5916\u53c2\u6807\u5b9a\u5bf9\u4f20\u611f\u5668\u878d\u5408\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u76ee\u6807\u6216\u91cd\u53e0\u89c6\u573a\uff0c\u5b9e\u9645\u4e2d\u96be\u4ee5\u6ee1\u8db3\u3002", "method": "\u57fa\u4e8eGINS\u5b89\u88c5\u9ad8\u5ea6\u7684\u89c2\u6d4b\u6a21\u578b\u7ea6\u675f\u4e0d\u53ef\u89c2\u6d4b\u53c2\u6570\uff0c\u7ed3\u5408\u51e0\u4f55\u5bf9\u5e94\u548c\u8fd0\u52a8\u4e00\u81f4\u6027\u7684\u8054\u5408\u4f18\u5316\u6846\u67b6\u3002", "result": "\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u4e0d\u540c\u4f20\u611f\u5668\u914d\u7f6e\u4e0b\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5f02\u6784LiDAR\u914d\u7f6e\uff0c\u89e3\u51b3\u4e86\u5e73\u9762\u8fd0\u52a8\u4e0b\u7684\u6807\u5b9a\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.08364", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08364", "abs": "https://arxiv.org/abs/2507.08364", "authors": ["Deteng Zhang", "Junjie Zhang", "Yan Sun", "Tao Li", "Hao Yin", "Hongzhao Xie", "Jie Yin"], "title": "Towards Robust Sensor-Fusion Ground SLAM: A Comprehensive Benchmark and A Resilient Framework", "comment": "This paper has already been accepted to IROS2025. 8 pages", "summary": "Considerable advancements have been achieved in SLAM methods tailored for\nstructured environments, yet their robustness under challenging corner cases\nremains a critical limitation. Although multi-sensor fusion approaches\nintegrating diverse sensors have shown promising performance improvements, the\nresearch community faces two key barriers: On one hand, the lack of\nstandardized and configurable benchmarks that systematically evaluate SLAM\nalgorithms under diverse degradation scenarios hinders comprehensive\nperformance assessment. While on the other hand, existing SLAM frameworks\nprimarily focus on fusing a limited set of sensor types, without effectively\naddressing adaptive sensor selection strategies for varying environmental\nconditions.\n  To bridge these gaps, we make three key contributions: First, we introduce\nM3DGR dataset: a sensor-rich benchmark with systematically induced degradation\npatterns including visual challenge, LiDAR degeneracy, wheel slippage and GNSS\ndenial. Second, we conduct a comprehensive evaluation of forty SLAM systems on\nM3DGR, providing critical insights into their robustness and limitations under\nchallenging real-world conditions. Third, we develop a resilient modular\nmulti-sensor fusion framework named Ground-Fusion++, which demonstrates robust\nperformance by coupling GNSS, RGB-D, LiDAR, IMU (Inertial Measurement Unit) and\nwheel odometry. Codes and datasets are publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86M3DGR\u6570\u636e\u96c6\u548cGround-Fusion++\u6846\u67b6\uff0c\u586b\u8865\u4e86SLAM\u5728\u590d\u6742\u73af\u5883\u4e0b\u8bc4\u4f30\u548c\u81ea\u9002\u5e94\u4f20\u611f\u5668\u878d\u5408\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709SLAM\u65b9\u6cd5\u5728\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u548c\u591a\u4f20\u611f\u5668\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\u3002", "method": "\u5f15\u5165M3DGR\u6570\u636e\u96c6\uff0c\u8bc4\u4f3040\u79cdSLAM\u7cfb\u7edf\uff0c\u5e76\u5f00\u53d1Ground-Fusion++\u6846\u67b6\uff0c\u878d\u5408\u591a\u79cd\u4f20\u611f\u5668\u3002", "result": "M3DGR\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u591a\u6837\u5316\u9000\u5316\u573a\u666f\uff0cGround-Fusion++\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u8868\u73b0\u9c81\u68d2\u3002", "conclusion": "M3DGR\u548cGround-Fusion++\u4e3aSLAM\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u548c\u65b9\u5411\uff0c\u63d0\u5347\u4e86\u591a\u4f20\u611f\u5668\u878d\u5408\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2507.08366", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08366", "abs": "https://arxiv.org/abs/2507.08366", "authors": ["Ghaith El-Dalahmeh", "Mohammad Reza Jabbarpour", "Bao Quoc Vo", "Ryszard Kowalczyk"], "title": "Intelligent Control of Spacecraft Reaction Wheel Attitude Using Deep Reinforcement Learning", "comment": null, "summary": "Reliable satellite attitude control is essential for the success of space\nmissions, particularly as satellites increasingly operate autonomously in\ndynamic and uncertain environments. Reaction wheels (RWs) play a pivotal role\nin attitude control, and maintaining control resilience during RW faults is\ncritical to preserving mission objectives and system stability. However,\ntraditional Proportional Derivative (PD) controllers and existing deep\nreinforcement learning (DRL) algorithms such as TD3, PPO, and A2C often fall\nshort in providing the real time adaptability and fault tolerance required for\nautonomous satellite operations. This study introduces a DRL-based control\nstrategy designed to improve satellite resilience and adaptability under fault\nconditions. Specifically, the proposed method integrates Twin Delayed Deep\nDeterministic Policy Gradient (TD3) with Hindsight Experience Replay (HER) and\nDimension Wise Clipping (DWC) referred to as TD3-HD to enhance learning in\nsparse reward environments and maintain satellite stability during RW failures.\nThe proposed approach is benchmarked against PD control and leading DRL\nalgorithms. Experimental results show that TD3-HD achieves significantly lower\nattitude error, improved angular velocity regulation, and enhanced stability\nunder fault conditions. These findings underscore the proposed method potential\nas a powerful, fault tolerant, onboard AI solution for autonomous satellite\nattitude control.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u63a7\u5236\u7b56\u7565TD3-HD\uff0c\u7528\u4e8e\u63d0\u5347\u536b\u661f\u5728\u53cd\u5e94\u8f6e\u6545\u969c\u6761\u4ef6\u4e0b\u7684\u9002\u5e94\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfPD\u63a7\u5236\u5668\u548c\u5176\u4ed6DRL\u7b97\u6cd5\u3002", "motivation": "\u536b\u661f\u5728\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u81ea\u4e3b\u8fd0\u884c\u65f6\uff0c\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u548c\u73b0\u6709DRL\u7b97\u6cd5\u5728\u5b9e\u65f6\u9002\u5e94\u6027\u548c\u5bb9\u9519\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u63a7\u5236\u7b56\u7565\u3002", "method": "\u7ed3\u5408Twin Delayed Deep Deterministic Policy Gradient\uff08TD3\uff09\u3001Hindsight Experience Replay\uff08HER\uff09\u548cDimension Wise Clipping\uff08DWC\uff09\uff0c\u63d0\u51faTD3-HD\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u6548\u679c\u548c\u6545\u969c\u6761\u4ef6\u4e0b\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cTD3-HD\u5728\u59ff\u6001\u8bef\u5dee\u3001\u89d2\u901f\u5ea6\u8c03\u8282\u548c\u6545\u969c\u6761\u4ef6\u4e0b\u7684\u7a33\u5b9a\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8ePD\u63a7\u5236\u5668\u548c\u5176\u4ed6DRL\u7b97\u6cd5\u3002", "conclusion": "TD3-HD\u662f\u4e00\u79cd\u5f3a\u5927\u4e14\u5bb9\u9519\u7684\u673a\u8f7dAI\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u81ea\u4e3b\u536b\u661f\u59ff\u6001\u63a7\u5236\u3002"}}
{"id": "2507.08420", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08420", "abs": "https://arxiv.org/abs/2507.08420", "authors": ["Haitian Wang", "Hezam Albaqami", "Xinyu Wang", "Muhammad Ibrahim", "Zainy M. Malakan", "Abdullah M. Algamdi", "Mohammed H. Alghamdi", "Ajmal Mian"], "title": "LiDAR, GNSS and IMU Sensor Alignment through Dynamic Time Warping to Construct 3D City Maps", "comment": "Preparing to submit to International Journal of Applied Earth\n  Observation and Geoinformation", "summary": "LiDAR-based 3D mapping suffers from cumulative drift causing global\nmisalignment, particularly in GNSS-constrained environments. To address this,\nwe propose a unified framework that fuses LiDAR, GNSS, and IMU data for\nhigh-resolution city-scale mapping. The method performs velocity-based temporal\nalignment using Dynamic Time Warping and refines GNSS and IMU signals via\nextended Kalman filtering. Local maps are built using Normal Distributions\nTransform-based registration and pose graph optimization with loop closure\ndetection, while global consistency is enforced using GNSS-constrained anchors\nfollowed by fine registration of overlapping segments. We also introduce a\nlarge-scale multimodal dataset captured in Perth, Western Australia to\nfacilitate future research in this direction. Our dataset comprises 144{,}000\nframes acquired with a 128-channel Ouster LiDAR, synchronized RTK-GNSS\ntrajectories, and MEMS-IMU measurements across 21 urban loops. To assess\ngeometric consistency, we evaluated our method using alignment metrics based on\nroad centerlines and intersections to capture both global and local accuracy.\nOur method reduces the average global alignment error from 3.32\\,m to 1.24\\,m,\nachieving a 61.4\\% improvement. The constructed high-fidelity map supports a\nwide range of applications, including smart city planning, geospatial data\nintegration, infrastructure monitoring, and GPS-free navigation. Our method,\nand dataset together establish a new benchmark for evaluating 3D city mapping\nin GNSS-constrained environments. The dataset and code will be released\npublicly.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408LiDAR\u3001GNSS\u548cIMU\u6570\u636e\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3GNSS\u53d7\u9650\u73af\u5883\u4e0b\u76843D\u5730\u56fe\u5168\u5c40\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "LiDAR-based 3D mapping\u5728GNSS\u53d7\u9650\u73af\u5883\u4e2d\u5b58\u5728\u7d2f\u79ef\u6f02\u79fb\u95ee\u9898\uff0c\u5bfc\u81f4\u5168\u5c40\u5bf9\u9f50\u8bef\u5dee\u3002", "method": "\u7ed3\u5408Dynamic Time Warping\u8fdb\u884c\u901f\u5ea6\u5bf9\u9f50\uff0c\u901a\u8fc7\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u4f18\u5316GNSS\u548cIMU\u6570\u636e\uff0c\u4f7f\u7528NDT\u914d\u51c6\u548c\u4f4d\u59ff\u56fe\u4f18\u5316\u6784\u5efa\u5c40\u90e8\u5730\u56fe\uff0c\u5e76\u901a\u8fc7GNSS\u7ea6\u675f\u951a\u70b9\u548c\u91cd\u53e0\u6bb5\u7cbe\u7ec6\u914d\u51c6\u4fdd\u8bc1\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5168\u5c40\u5bf9\u9f50\u8bef\u5dee\u4ece3.32\u7c73\u964d\u81f31.24\u7c73\uff0c\u63d0\u5347\u4e8661.4%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728GNSS\u53d7\u9650\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u4e863D\u57ce\u5e02\u5730\u56fe\u7684\u7cbe\u5ea6\uff0c\u5e76\u4e3a\u667a\u80fd\u57ce\u5e02\u89c4\u5212\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u4fdd\u771f\u5730\u56fe\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2507.08572", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08572", "abs": "https://arxiv.org/abs/2507.08572", "authors": ["Juraj Gavura", "Michal Vavrecka", "Igor Farkas", "Connor Gade"], "title": "Robotic Calibration Based on Haptic Feedback Improves Sim-to-Real Transfer", "comment": "ICANN 2025", "summary": "When inverse kinematics (IK) is adopted to control robotic arms in\nmanipulation tasks, there is often a discrepancy between the end effector (EE)\nposition of the robot model in the simulator and the physical EE in reality. In\nmost robotic scenarios with sim-to-real transfer, we have information about\njoint positions in both simulation and reality, but the EE position is only\navailable in simulation. We developed a novel method to overcome this\ndifficulty based on haptic feedback calibration, using a touchscreen in front\nof the robot that provides information on the EE position in the real\nenvironment. During the calibration procedure, the robot touches specific\npoints on the screen, and the information is stored. In the next stage, we\nbuild a transformation function from the data based on linear transformation\nand neural networks that is capable of outputting all missing variables from\nany partial input (simulated/real joint/EE position). Our results demonstrate\nthat a fully nonlinear neural network model performs best, significantly\nreducing positioning errors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89e6\u89c9\u53cd\u9988\u6821\u51c6\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e6\u6478\u5c4f\u83b7\u53d6\u771f\u5b9e\u73af\u5883\u4e2d\u672b\u7aef\u6267\u884c\u5668\u4f4d\u7f6e\u4fe1\u606f\uff0c\u663e\u8457\u51cf\u5c11\u5b9a\u4f4d\u8bef\u5dee\u3002", "motivation": "\u89e3\u51b3\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e2d\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u4f4d\u7f6e\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u4ec5\u6709\u4eff\u771f\u4e2d\u672b\u7aef\u6267\u884c\u5668\u4f4d\u7f6e\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u4f7f\u7528\u89e6\u6478\u5c4f\u6821\u51c6\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u4f4d\u7f6e\uff0c\u6784\u5efa\u57fa\u4e8e\u7ebf\u6027\u53d8\u6362\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u8f6c\u6362\u51fd\u6570\uff0c\u586b\u8865\u7f3a\u5931\u53d8\u91cf\u3002", "result": "\u975e\u7ebf\u6027\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5b9a\u4f4d\u8bef\u5dee\u3002", "conclusion": "\u901a\u8fc7\u89e6\u89c9\u53cd\u9988\u6821\u51c6\u548c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u8f6c\u79fb\u4e2d\u7684\u672b\u7aef\u6267\u884c\u5668\u4f4d\u7f6e\u4e0d\u4e00\u81f4\u95ee\u9898\u3002"}}
{"id": "2507.08656", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08656", "abs": "https://arxiv.org/abs/2507.08656", "authors": ["Aravind Elanjimattathil Vijayan", "Andrei Cramariuc", "Mattia Risiglione", "Christian Gehring", "Marco Hutter"], "title": "Multi-critic Learning for Whole-body End-effector Twist Tracking", "comment": null, "summary": "Learning whole-body control for locomotion and arm motions in a single policy\nhas challenges, as the two tasks have conflicting goals. For instance,\nefficient locomotion typically favors a horizontal base orientation, while\nend-effector tracking may benefit from base tilting to extend reachability.\nAdditionally, current Reinforcement Learning (RL) approaches using a pose-based\ntask specification lack the ability to directly control the end-effector\nvelocity, making smoothly executing trajectories very challenging. To address\nthese limitations, we propose an RL-based framework that allows for dynamic,\nvelocity-aware whole-body end-effector control. Our method introduces a\nmulti-critic actor architecture that decouples the reward signals for\nlocomotion and manipulation, simplifying reward tuning and allowing the policy\nto resolve task conflicts more effectively. Furthermore, we design a\ntwist-based end-effector task formulation that can track both discrete poses\nand motion trajectories. We validate our approach through a set of simulation\nand hardware experiments using a quadruped robot equipped with a robotic arm.\nThe resulting controller can simultaneously walk and move its end-effector and\nshows emergent whole-body behaviors, where the base assists the arm in\nextending the workspace, despite a lack of explicit formulations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u3001\u901f\u5ea6\u611f\u77e5\u7684\u5168\u8eab\u672b\u7aef\u6267\u884c\u5668\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u8fd0\u52a8\u4e0e\u624b\u81c2\u52a8\u4f5c\u7684\u51b2\u7a81\u76ee\u6807\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u540c\u65f6\u63a7\u5236\u673a\u5668\u4eba\u8fd0\u52a8\u548c\u672b\u7aef\u6267\u884c\u5668\u52a8\u4f5c\u65f6\u5b58\u5728\u51b2\u7a81\u76ee\u6807\uff08\u5982\u57fa\u5ea7\u503e\u659c\u4e0e\u6c34\u5e73\u8fd0\u52a8\uff09\uff0c\u4e14\u7f3a\u4e4f\u76f4\u63a5\u63a7\u5236\u672b\u7aef\u6267\u884c\u5668\u901f\u5ea6\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u591a\u8bc4\u8bba\u5bb6\u6f14\u5458\u67b6\u6784\uff0c\u89e3\u8026\u8fd0\u52a8\u548c\u64cd\u7eb5\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u626d\u8f6c\u7684\u672b\u7aef\u6267\u884c\u5668\u4efb\u52a1\u516c\u5f0f\u3002", "result": "\u5728\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u7684\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u8868\u660e\uff0c\u63a7\u5236\u5668\u80fd\u540c\u65f6\u884c\u8d70\u548c\u79fb\u52a8\u672b\u7aef\u6267\u884c\u5668\uff0c\u5e76\u8868\u73b0\u51fa\u5168\u8eab\u534f\u4f5c\u884c\u4e3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4efb\u52a1\u51b2\u7a81\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u3001\u901f\u5ea6\u611f\u77e5\u7684\u5168\u8eab\u63a7\u5236\u3002"}}
{"id": "2507.08726", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08726", "abs": "https://arxiv.org/abs/2507.08726", "authors": ["Yuekun Wu", "Yik Lung Pang", "Andrea Cavallaro", "Changjae Oh"], "title": "Learning human-to-robot handovers through 3D scene reconstruction", "comment": "8 pages, 6 figures, 2 table", "summary": "Learning robot manipulation policies from raw, real-world image data requires\na large number of robot-action trials in the physical environment. Although\ntraining using simulations offers a cost-effective alternative, the visual\ndomain gap between simulation and robot workspace remains a major limitation.\nGaussian Splatting visual reconstruction methods have recently provided new\ndirections for robot manipulation by generating realistic environments. In this\npaper, we propose the first method for learning supervised-based robot\nhandovers solely from RGB images without the need of real-robot training or\nreal-robot data collection. The proposed policy learner, Human-to-Robot\nHandover using Sparse-View Gaussian Splatting (H2RH-SGS), leverages sparse-view\nGaussian Splatting reconstruction of human-to-robot handover scenes to generate\nrobot demonstrations containing image-action pairs captured with a camera\nmounted on the robot gripper. As a result, the simulated camera pose changes in\nthe reconstructed scene can be directly translated into gripper pose changes.\nWe train a robot policy on demonstrations collected with 16 household objects\nand {\\em directly} deploy this policy in the real environment. Experiments in\nboth Gaussian Splatting reconstructed scene and real-world human-to-robot\nhandover experiments demonstrate that H2RH-SGS serves as a new and effective\nrepresentation for the human-to-robot handover task.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u89c6\u56fe\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u7684\u65b9\u6cd5\uff08H2RH-SGS\uff09\uff0c\u4ec5\u4eceRGB\u56fe\u50cf\u5b66\u4e60\u673a\u5668\u4eba\u4ea4\u63a5\u7b56\u7565\uff0c\u65e0\u9700\u771f\u5b9e\u673a\u5668\u4eba\u8bad\u7ec3\u6216\u6570\u636e\u6536\u96c6\u3002", "motivation": "\u89e3\u51b3\u4ece\u4eff\u771f\u5230\u771f\u5b9e\u73af\u5883\u7684\u89c6\u89c9\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u51cf\u5c11\u5bf9\u771f\u5b9e\u673a\u5668\u4eba\u8bd5\u9a8c\u7684\u4f9d\u8d56\u3002", "method": "\u5229\u7528\u7a00\u758f\u89c6\u56fe\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u4eba\u7c7b-\u673a\u5668\u4eba\u4ea4\u63a5\u573a\u666f\uff0c\u751f\u6210\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\uff0c\u8bad\u7ec3\u7b56\u7565\u5e76\u76f4\u63a5\u90e8\u7f72\u5230\u771f\u5b9e\u73af\u5883\u3002", "result": "\u572816\u79cd\u5bb6\u5ead\u7269\u54c1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cH2RH-SGS\u662f\u4e00\u79cd\u6709\u6548\u7684\u673a\u5668\u4eba\u4ea4\u63a5\u4efb\u52a1\u8868\u793a\u65b9\u6cd5\u3002", "conclusion": "H2RH-SGS\u4e3a\u673a\u5668\u4eba\u4ea4\u63a5\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
