<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 17]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [TRACE: Textual Reasoning for Affordance Coordinate Extraction](https://arxiv.org/abs/2511.01999)
*Sangyun Park,Jin Kim,Yuchen Cui,Matthew S. Brown*

Main category: cs.RO

TL;DR: TRACE方法通过将文本推理链集成到空间感知预测中，显著提升了视觉语言模型在机器人操作任务中的性能，在Where2Place基准上达到48.1%准确率。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型难以将高级指令转化为机器人操作所需的空间感知，现有视觉思维链方法计算成本高。

Method: 提出TRACE方法，通过自主管道创建包含指令和显式文本推理的大型数据集，并基于此微调视觉语言模型，使其在行动前外部化空间推理。

Result: 在Where2Place基准上达到48.1%准确率（相对提升9.6%），在更具挑战性的W2P(h)子集上达到55.0%准确率。消融研究显示性能与推理数据量直接相关。

Conclusion: 训练视觉语言模型生成文本推理链是增强基于VLM的机器人控制的精度、可靠性和可解释性的有效策略。

Abstract: Vision-Language Models (VLMs) struggle to translate high-level instructions
into the precise spatial affordances required for robotic manipulation. While
visual Chain-of-Thought (CoT) methods exist, they are often computationally
intensive. In this work, we introduce TRACE (Textual Reasoning for Affordance
Coordinate Extraction), a novel methodology that integrates a textual Chain of
Reasoning (CoR) into the affordance prediction process. We use this methodology
to create the TRACE dataset, a large-scale collection created via an autonomous
pipeline that pairs instructions with explicit textual rationales. By
fine-tuning a VLM on this data, our model learns to externalize its spatial
reasoning before acting. Our experiments show that our TRACE-tuned model
achieves state-of-the-art performance, reaching 48.1% accuracy on the primary
Where2Place (W2P) benchmark (a 9.6% relative improvement) and 55.0% on the more
challenging W2P(h) subset. Crucially, an ablation study demonstrates that
performance scales directly with the amount of reasoning data used, confirming
the CoR's effectiveness. Furthermore, analysis of the model's attention maps
reveals an interpretable reasoning process where focus shifts dynamically
across reasoning steps. This work shows that training VLMs to generate a
textual CoR is an effective and robust strategy for enhancing the precision,
reliability, and interpretability of VLM-based robot control. Our dataset and
code are available at https://github.com/jink-ucla/TRACE

</details>


### [2] [Stein-based Optimization of Sampling Distributions in Model Predictive Path Integral Control](https://arxiv.org/abs/2511.02015)
*Jace Aldrich,Odest Chadwicke Jenkins*

Main category: cs.RO

TL;DR: 提出了一种结合MPPI和SVGD的新方法SOPPI，通过SVGD优化MPPI的样本生成，动态更新噪声分布以改善轨迹优化性能


<details>
  <summary>Details</summary>
Motivation: 传统MPPI依赖高斯分布的随机采样，可能导致样本匮乏和次优结果，需要改进样本生成方法

Method: 在MPPI环境步骤之间引入SVGD更新，动态调整噪声分布，形成SOPPI算法

Result: 在Cart-Pole和二维双足行走任务中验证了方法的有效性，在多种超参数下性能优于标准MPPI，且在较少粒子数下仍可行

Conclusion: 该方法适用于更高自由度系统，并有潜力推动可微分模拟器的发展

Abstract: This paper presents a novel method for Model Predictive Path Integral (MPPI)
control that optimizes sample generation towards an optimal trajectory through
Stein Variational Gradient Descent (SVGD). MPPI is traditionally reliant on
randomly sampled trajectories, often by a Gaussian distribution. The result can
lead to sample deprivation, under-representing the space of possible
trajectories, and yield suboptimal results. Through introducing SVGD updates in
between MPPI environment steps, we present Stein-Optimized Path-Integral
Inference (SOPPI), an MPPI/SVGD algorithm that can dynamically update noise
distributions at runtime to shape a more optimal representation without an
excessive increase in computational requirements. We demonstrate the efficacy
of our method systems ranging from a Cart-Pole to a two-dimensional bipedal
walking task, indicating improved performance above standard MPPI across a
range of hyper-parameters and demonstrate feasibility at lower particle counts.
We discuss the applicability of this MPPI/SVGD method to higher
degree-of-freedom systems, as well as its potential to new developments in
state-of-the-art differentiable simulators.

</details>


### [3] [TurboMap: GPU-Accelerated Local Mapping for Visual SLAM](https://arxiv.org/abs/2511.02036)
*Parsa Hosseininejad,Kimia Khabiri,Shishir Gopinath,Soudabeh Mohammadhashemi,Karthik Dantu,Steven Y. Ko*

Main category: cs.RO

TL;DR: TurboMap是一个GPU加速和CPU优化的视觉SLAM局部建图模块，通过GPU和CPU优化解决局部建图性能瓶颈，在ORB-SLAM3基础上实现，在保持精度的同时显著提升速度。


<details>
  <summary>Details</summary>
Motivation: 识别视觉SLAM系统中局部建图过程的性能瓶颈，通过针对性的GPU和CPU优化来提升建图效率。

Method: 将地图点三角化和融合卸载到GPU，在CPU上加速冗余关键帧剔除，集成GPU加速求解器来加速局部束调整，基于ORB-SLAM3并使用CUDA进行GPU编程。

Result: 在EuRoC数据集上平均加速1.3倍，在TUM-VI数据集上平均加速1.6倍，在桌面和嵌入式平台上均保持原始系统精度。

Conclusion: TurboMap通过GPU和CPU协同优化有效解决了视觉SLAM局部建图的性能瓶颈，实现了显著的加速效果。

Abstract: This paper presents TurboMap, a GPU-accelerated and CPU-optimized local
mapping module for visual SLAM systems. We identify key performance bottlenecks
in the local mapping process for visual SLAM and address them through targeted
GPU and CPU optimizations. Specifically, we offload map point triangulation and
fusion to the GPU, accelerate redundant keyframe culling on the CPU, and
integrate a GPU-accelerated solver to speed up local bundle adjustment. Our
implementation is built on top of ORB-SLAM3 and leverages CUDA for GPU
programming. The experimental results show that TurboMap achieves an average
speedup of 1.3x in the EuRoC dataset and 1.6x in the TUM-VI dataset in the
local mapping module, on both desktop and embedded platforms, while maintaining
the accuracy of the original system.

</details>


### [4] [TACO: Trajectory-Aware Controller Optimization for Quadrotors](https://arxiv.org/abs/2511.02060)
*Hersh Sanghvi,Spencer Folk,Vijay Kumar,Camillo Jose Taylor*

Main category: cs.RO

TL;DR: TACO是一个实时优化四旋翼控制器参数的框架，通过预测模型和轻量级优化方案，根据参考轨迹和当前状态动态调整控制器增益，显著提升轨迹跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 传统四旋翼控制器使用固定参数，无法适应不同轨迹特性，牺牲了任务特定性能。需要一种能够在线自适应调整参数的方法来提升跟踪精度。

Method: 使用学习预测模型和轻量级优化方案实时优化控制器增益，支持轨迹自适应以改善动态可行性，并开发并行化模拟器进行大规模训练数据收集。

Result: 实验表明TACO在多种轨迹类型上优于传统静态参数调优，运行速度比黑盒优化基线快几个数量级，物理四旋翼部署可行，轨迹自适应显著降低跟踪误差。

Conclusion: TACO框架能够有效提升四旋翼轨迹跟踪性能，实现实时控制器参数优化，为动态环境下的高性能控制提供了实用解决方案。

Abstract: Controller performance in quadrotor trajectory tracking depends heavily on
parameter tuning, yet standard approaches often rely on fixed, manually tuned
parameters that sacrifice task-specific performance. We present
Trajectory-Aware Controller Optimization (TACO), a framework that adapts
controller parameters online based on the upcoming reference trajectory and
current quadrotor state. TACO employs a learned predictive model and a
lightweight optimization scheme to optimize controller gains in real time with
respect to a broad class of trajectories, and can also be used to adapt
trajectories to improve dynamic feasibility while respecting smoothness
constraints. To enable large-scale training, we also introduce a parallelized
quadrotor simulator supporting fast data collection on diverse trajectories.
Experiments on a variety of trajectory types show that TACO outperforms
conventional, static parameter tuning while operating orders of magnitude
faster than black-box optimization baselines, enabling practical real-time
deployment on a physical quadrotor. Furthermore, we show that adapting
trajectories using TACO significantly reduces the tracking error obtained by
the quadrotor.

</details>


### [5] [A Step Toward World Models: A Survey on Robotic Manipulation](https://arxiv.org/abs/2511.02097)
*Peng-Fei Zhang,Ying Cheng,Xiaofan Sun,Shijie Wang,Lei Zhu,Heng Tao Shen*

Main category: cs.RO

TL;DR: 该论文对机器人操作中的世界模型方法进行综述，分析其在感知、预测和控制中的作用，旨在为开发通用实用的机器人世界模型制定路线图。


<details>
  <summary>Details</summary>
Motivation: 自主代理需要在复杂动态环境中执行任务，这要求它们理解世界机制和动态，而不仅仅是反应控制或状态复制，因此需要开发能够编码环境状态、捕捉动态并支持预测、规划和推理的世界模型。

Method: 通过综述机器人操作中的方法，分析那些展现世界模型核心能力的方法，而不是局限于明确标记为世界模型的方法，考察它们在感知、预测和控制中的作用。

Result: 识别了世界模型的关键挑战和解决方案，提炼了真实世界模型应具备的核心组件、能力和功能。

Conclusion: 基于分析，旨在为开发通用实用的机器人世界模型制定路线图，推动自主代理在复杂环境中的能力发展。

Abstract: Autonomous agents are increasingly expected to operate in complex, dynamic,
and uncertain environments, performing tasks such as manipulation, navigation,
and decision-making. Achieving these capabilities requires agents to understand
the underlying mechanisms and dynamics of the world, moving beyond purely
reactive control or simple replication of observed states. This motivates the
development of world models as internal representations that encode
environmental states, capture dynamics, and enable prediction, planning, and
reasoning. Despite growing interest, the definition, scope, architectures, and
essential capabilities of world models remain ambiguous. In this survey, rather
than directly imposing a fixed definition and limiting our scope to methods
explicitly labeled as world models, we examine approaches that exhibit the core
capabilities of world models through a review of methods in robotic
manipulation. We analyze their roles across perception, prediction, and
control, identify key challenges and solutions, and distill the core
components, capabilities, and functions that a real world model should possess.
Building on this analysis, we aim to outline a roadmap for developing
generalizable and practical world models for robotics.

</details>


### [6] [Census-Based Population Autonomy For Distributed Robotic Teaming](https://arxiv.org/abs/2511.02147)
*Tyler M. Paine,Anastasia Bizyaeva,Michael R. Benjamin*

Main category: cs.RO

TL;DR: 本文提出了一种多机器人自主性的分层模型，结合了基于邻居输入加权计数的集体决策和基于多目标行为优化的个体决策，并在自主水面车辆上进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统在海洋环境中具有高效性和鲁棒性优势，但如何建模、分析和设计这些系统以实现协作的全部效益是一个挑战，因为多机器人自主性领域既包含集体行为也包含个体行为。

Method: 采用分层模型：集体决策使用非线性意见动态模型进行加权计数，个体决策使用区间规划进行多目标行为优化。还引入了一种分布式优化子群分配的新方法，机器人使用梯度下降算法最小化局部已知成本函数部分，同时受邻居意见状态影响以考虑未观测成本。

Result: 该模型可以简化为分布式优化和控制的基础算法，同时完整模型能够实现在实际场景中有用的新型集体行为。在三个不同类型的自主水面车辆实验中验证了模型的有效性：自适应采样场景、高价值单位保护场景和夺旗竞争游戏。

Conclusion: 提出的分层模型能够有效协调多机器人系统中的集体和个体决策，通过实验验证了其在真实场景中的实用性，为多机器人协作系统设计提供了新思路。

Abstract: Collaborating teams of robots show promise due in their ability to complete
missions more efficiently and with improved robustness, attributes that are
particularly useful for systems operating in marine environments. A key issue
is how to model, analyze, and design these multi-robot systems to realize the
full benefits of collaboration, a challenging task since the domain of
multi-robot autonomy encompasses both collective and individual behaviors. This
paper introduces a layered model of multi-robot autonomy that uses the
principle of census, or a weighted count of the inputs from neighbors, for
collective decision-making about teaming, coupled with multi-objective behavior
optimization for individual decision-making about actions. The census component
is expressed as a nonlinear opinion dynamics model and the multi-objective
behavior optimization is accomplished using interval programming. This model
can be reduced to recover foundational algorithms in distributed optimization
and control, while the full model enables new types of collective behaviors
that are useful in real-world scenarios. To illustrate these points, a new
method for distributed optimization of subgroup allocation is introduced where
robots use a gradient descent algorithm to minimize portions of the cost
functions that are locally known, while being influenced by the opinion states
from neighbors to account for the unobserved costs. With this method the group
can collectively use the information contained in the Hessian matrix of the
total global cost. The utility of this model is experimentally validated in
three categorically different experiments with fleets of autonomous surface
vehicles: an adaptive sampling scenario, a high value unit protection scenario,
and a competitive game of capture the flag.

</details>


### [7] [Text to Robotic Assembly of Multi Component Objects using 3D Generative AI and Vision Language Models](https://arxiv.org/abs/2511.02162)
*Alexander Htet Kyaw,Richa Gupta,Dhruv Shah,Anoop Sinha,Kory Mathewson,Stefanie Pender,Sachin Chitta,Yotto Koga,Faez Ahmed,Lawrence Sass,Randall Davis*

Main category: cs.RO

TL;DR: 提出了一种结合3D生成AI和视觉语言模型的管道，用于从自然语言实现多组件物体的机器人装配。


<details>
  <summary>Details</summary>
Motivation: 解决3D生成AI在创建多组件类型物体时面临的挑战，实现从文本提示到物理对象装配的完整流程。

Method: 利用视觉语言模型进行零样本多模态推理，将AI生成的网格分解为使用预定义结构和面板组件的多组件3D模型。

Result: 评估显示用户90.6%的时间偏好VLM生成的组件分配，显著优于基于规则(59.4%)和随机分配(2.5%)。

Conclusion: 该系统允许用户通过对话反馈细化组件分配，为使用生成AI和机器人制造物理对象提供了更大的人类控制和自主性。

Abstract: Advances in 3D generative AI have enabled the creation of physical objects
from text prompts, but challenges remain in creating objects involving multiple
component types. We present a pipeline that integrates 3D generative AI with
vision-language models (VLMs) to enable the robotic assembly of multi-component
objects from natural language. Our method leverages VLMs for zero-shot,
multi-modal reasoning about geometry and functionality to decompose
AI-generated meshes into multi-component 3D models using predefined structural
and panel components. We demonstrate that a VLM is capable of determining which
mesh regions need panel components in addition to structural components, based
on object functionality. Evaluation across test objects shows that users
preferred the VLM-generated assignments 90.6% of the time, compared to 59.4%
for rule-based and 2.5% for random assignment. Lastly, the system allows users
to refine component assignments through conversational feedback, enabling
greater human control and agency in making physical objects with generative AI
and robotics.

</details>


### [8] [Kinematic and Ergonomic Design of a Robotic Arm for Precision Laparoscopic Surgery](https://arxiv.org/abs/2511.02167)
*Tian Hao,Tong Lu,Che Chan*

Main category: cs.RO

TL;DR: 提出了一种用于腹腔镜手术的7自由度机器人手臂，通过运动学优化和人体工程学设计，显著提高了手术精度和操作舒适度。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助微创手术能够提高手术精度并减少外科医生疲劳，但需要优化的运动学和人体工程学设计来实现更好的性能。

Method: 设计了一个7自由度机器人手臂系统，包含远程运动中心(RCM)机制，并在通用机器人平台上实现，通过模拟手术任务评估性能。

Result: 实验结果显示，优化后的机器人设计将目标精度误差降低了50%以上，缩短了任务完成时间，并显著降低了操作者的肌肉劳损和不适感。

Conclusion: 运动学优化（如增加关节和震颤过滤）和以人为中心的人体工程学设计对于提高机器人辅助手术性能至关重要，这些见解可指导下一代手术机器人的开发。

Abstract: Robotic assistance in minimally invasive surgery can greatly enhance surgical
precision and reduce surgeon fatigue. This paper presents a focused
investigation on the kinematic and ergonomic design principles for a
laparoscopic surgical robotic arm aimed at high-precision tasks. We propose a
7-degree-of-freedom (7-DOF) robotic arm system that incorporates a remote
center of motion (RCM) at the instrument insertion point and ergonomic
considerations to improve surgeon interaction. The design is implemented on a
general-purpose robotic platform, and a series of simulated surgical tasks were
performed to evaluate targeting accuracy, task efficiency, and surgeon comfort
compared to conventional manual laparoscopy. Experimental results demonstrate
that the optimized robotic design achieves significantly improved targeting
accuracy (error reduced by over 50%) and shorter task completion times, while
substantially lowering operator muscle strain and discomfort. These findings
validate the importance of kinematic optimization (such as added articulations
and tremor filtering) and human-centered ergonomic design in enhancing the
performance of robot-assisted surgery. The insights from this work can guide
the development of next-generation surgical robots that improve surgical
outcomes and ergonomics for the operating team.

</details>


### [9] [A Quantitative Comparison of Centralised and Distributed Reinforcement Learning-Based Control for Soft Robotic Arms](https://arxiv.org/abs/2511.02192)
*Linxin Hou,Qirui Wu,Zhihang Qin,Neil Banerjee,Yongxin Guo,Cecilia Laschi*

Main category: cs.RO

TL;DR: 比较集中式和分布式多智能体强化学习在软体机器人臂控制中的性能，发现当控制段数n≤4时分布式策略无显著优势，n≤2时集中式更优，4<n≤12时分布式策略样本效率更高但训练时间更长。


<details>
  <summary>Details</summary>
Motivation: 研究集中式和分布式MARL架构在软体机器人控制中的性能差异，为软体机器人系统提供设计指导。

Method: 使用PyElastica和OpenAI Gym模拟软体机器人臂，在相同预算下训练全局PPO控制器和MAPPO，系统改变控制段数n，评估三种场景下的性能。

Result: n≤4时分布式策略无优势；n≤2时集中式更优；4<n≤12时分布式策略样本效率高、成功率强、鲁棒性好，但训练时间更长。

Conclusion: 集中式和分布式策略在软体机器人控制中存在权衡，为未来软体机器人从仿真到实物的转移提供了设计指导。

Abstract: This paper presents a quantitative comparison between centralised and
distributed multi-agent reinforcement learning (MARL) architectures for
controlling a soft robotic arm modelled as a Cosserat rod in simulation. Using
PyElastica and the OpenAI Gym interface, we train both a global Proximal Policy
Optimisation (PPO) controller and a Multi-Agent PPO (MAPPO) under identical
budgets. Both approaches are based on the arm having $n$ number of controlled
sections. The study systematically varies $n$ and evaluates the performance of
the arm to reach a fixed target in three scenarios: default baseline condition,
recovery from external disturbance, and adaptation to actuator failure.
Quantitative metrics used for the evaluation are mean action magnitude, mean
final distance, mean episode length, and success rate. The results show that
there are no significant benefits of the distributed policy when the number of
controlled sections $n\le4$. In very simple systems, when $n\le2$, the
centralised policy outperforms the distributed one. When $n$ increases to $4<
n\le 12$, the distributed policy shows a high sample efficiency. In these
systems, distributed policy promotes a stronger success rate, resilience, and
robustness under local observability and yields faster convergence given the
same sample size. However, centralised policies achieve much higher time
efficiency during training as it takes much less time to train the same size of
samples. These findings highlight the trade-offs between centralised and
distributed policy in reinforcement learning-based control for soft robotic
systems and provide actionable design guidance for future sim-to-real transfer
in soft rod-like manipulators.

</details>


### [10] [LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation](https://arxiv.org/abs/2511.02239)
*Youngjin Hong,Houjian Yu,Mingen Li,Changhyun Choi*

Main category: cs.RO

TL;DR: LACY是一个统一框架，在单一视觉语言模型中学习语言到动作(L2A)和动作到语言(A2L)的双向映射，通过主动增强策略实现自监督学习，在机器人操作任务中显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 当前基于语言指令到动作(L2A)的单向范式缺乏对行为的深层理解，限制了泛化能力和行为解释能力。需要动作到语言(A2L)的互补技能来发展更全面的基础认知。

Method: LACY框架联合训练三个协同任务：从语言生成参数化动作(L2A)、用语言解释观察到的动作(A2L)、验证两个语言描述之间的语义一致性(L2C)。通过主动增强策略针对低置信度案例自主生成和过滤训练数据。

Result: 在模拟和真实世界的拾取放置任务中，LACY平均提高任务成功率56.46%，并产生更鲁棒的语言-动作基础认知。

Conclusion: 双向语言-动作映射框架能够形成更丰富的内部表征，为机器人操作解锁新的自监督学习范式，显著提升任务性能和基础认知能力。

Abstract: Learning generalizable policies for robotic manipulation increasingly relies
on large-scale models that map language instructions to actions (L2A). However,
this one-way paradigm often produces policies that execute tasks without deeper
contextual understanding, limiting their ability to generalize or explain their
behavior. We argue that the complementary skill of mapping actions back to
language (A2L) is essential for developing more holistic grounding. An agent
capable of both acting and explaining its actions can form richer internal
representations and unlock new paradigms for self-supervised learning. We
introduce LACY (Language-Action Cycle), a unified framework that learns such
bidirectional mappings within a single vision-language model. LACY is jointly
trained on three synergistic tasks: generating parameterized actions from
language (L2A), explaining observed actions in language (A2L), and verifying
semantic consistency between two language descriptions (L2C). This enables a
self-improving cycle that autonomously generates and filters new training data
through an active augmentation strategy targeting low-confidence cases, thereby
improving the model without additional human labels. Experiments on
pick-and-place tasks in both simulation and the real world show that LACY
improves task success rates by 56.46% on average and yields more robust
language-action grounding for robotic manipulation. Project page:
https://vla2026.github.io/LACY/

</details>


### [11] [SuckTac: Camera-based Tactile Sucker for Unstructured Surface Perception and Interaction](https://arxiv.org/abs/2511.02294)
*Ruiyong Yuan,Jieji Ren,Zhanxuan Peng,Feifei Chen,Guoying Gu*

Main category: cs.RO

TL;DR: 提出了一种名为SuckTac的智能吸盘，将摄像头触觉传感器集成到优化结构中，提供高密度感知和鲁棒吸附能力。


<details>
  <summary>Details</summary>
Motivation: 现有吸盘缺乏高保真感知和触觉传感能力，无法识别目标表面的精细几何特征和交互状态，限制了在复杂环境中的鲁棒性能。

Method: 通过联合结构设计和优化，基于多材料集成铸造技术，将摄像头和光源嵌入吸盘内部，实现原位高密度感知；优化机械设计，包括改进轮廓、添加柔性唇缘和表面微结构。

Result: 在机器人布料操作和软体移动机器人检查等挑战性任务中表现出优越性能和广泛适用性。

Conclusion: SuckTac智能吸盘通过仿生设计和传感集成，显著提升了在非结构化环境中的吸附性能和感知能力。

Abstract: Suckers are significant for robots in picking, transferring, manipulation and
locomotion on diverse surfaces. However, most of the existing suckers lack
high-fidelity perceptual and tactile sensing, which impedes them from resolving
the fine-grained geometric features and interaction status of the target
surface. This limits their robust performance with irregular objects and in
complex, unstructured environments. Inspired by the adaptive structure and
high-performance sensory capabilities of cephalopod suckers, in this paper, we
propose a novel, intelligent sucker, named SuckTac, that integrates a
camera-based tactile sensor directly within its optimized structure to provide
high-density perception and robust suction. Specifically, through joint
structure design and optimization and based on a multi-material integrated
casting technique, a camera and light source are embedded into the sucker,
which enables in-situ, high-density perception of fine details like surface
shape, texture and roughness. To further enhance robustness and adaptability,
the sucker's mechanical design is also optimized by refining its profile,
adding a compliant lip, and incorporating surface microstructure. Extensive
experiments, including challenging tasks such as robotic cloth manipulation and
soft mobile robot inspection, demonstrate the superior performance and broad
applicability of the proposed system.

</details>


### [12] [ZJUNlict Extended Team Description Paper 2025](https://arxiv.org/abs/2511.02315)
*Zifei Wu,Lijie Wang,Zhe Yang,Shijie Yang,Liang Wang,Haoran Fu,Yinliang Cai,Rong Xiong*

Main category: cs.RO

TL;DR: ZJUNlict团队在过去一年中完成了硬件和软件两方面的改进。硬件方面，在v2023机器人中集成了IMU以提升姿态精度和角速度规划。软件方面，优化了策略和CUDA模块，显著提高了决策效率、球追踪预测和控球预测能力，以适应高节奏比赛动态。


<details>
  <summary>Details</summary>
Motivation: 为了适应高节奏的比赛动态，需要提升机器人的姿态控制精度和软件决策效率，以增强团队在激烈比赛中的竞争力。

Method: 硬件上集成IMU传感器提升姿态精度；软件上优化策略模块和CUDA模块，改进决策算法、球追踪预测和控球预测功能。

Result: 实现了机器人姿态控制精度的提升，以及软件决策效率、球追踪预测准确性和控球预测能力的显著改善。

Conclusion: 通过硬件IMU集成和软件模块优化，ZJUNlict团队成功提升了机器人在高节奏比赛中的整体性能和适应性。

Abstract: This paper presents the ZJUNlict team's work over the past year, covering
both hardware and software advancements. In the hardware domain, the
integration of an IMU into the v2023 robot was completed to enhance posture
accuracy and angular velocity planning. On the software side, key modules were
optimized, including the strategy and CUDA modules, with significant
improvements in decision making efficiency, ball pursuit prediction, and ball
possession prediction to adapt to high-tempo game dynamics.

</details>


### [13] [Whole-body motion planning and safety-critical control for aerial manipulation](https://arxiv.org/abs/2511.02342)
*Lin Yang,Jinwoo Lee,Domenico Campolo,H. Jin Kim,Jeonghyun Byun*

Main category: cs.RO

TL;DR: 提出基于超二次曲面的空中机械臂运动规划框架，结合Voronoi图和平衡流形生成避障轨迹，并通过安全关键控制器确保推力限制和碰撞避免。


<details>
  <summary>Details</summary>
Motivation: 解决空中机械臂在复杂环境中规划安全、动态可行轨迹的挑战，克服传统几何抽象（如包围盒或椭球体）的保守性问题。

Method: 使用超二次曲面加代理表示法建模机械臂和障碍物，开发最大间隙规划器融合Voronoi图和平衡流形，设计基于高阶控制屏障函数的安全关键控制器。

Result: 在模拟环境中优于采样规划器，产生更快、更安全、更平滑的轨迹，在几何保真度上超越基于椭球体的基线方法。物理实验验证了可行性和鲁棒性。

Conclusion: 该框架在仿真和硬件环境中均表现一致，为空中机械臂在复杂环境中的安全操作提供了有效解决方案。

Abstract: Aerial manipulation combines the maneuverability of multirotors with the
dexterity of robotic arms to perform complex tasks in cluttered spaces. Yet
planning safe, dynamically feasible trajectories remains difficult due to
whole-body collision avoidance and the conservativeness of common geometric
abstractions such as bounding boxes or ellipsoids. We present a whole-body
motion planning and safety-critical control framework for aerial manipulators
built on superquadrics (SQs). Using an SQ-plus-proxy representation, we model
both the vehicle and obstacles with differentiable, geometry-accurate surfaces.
Leveraging this representation, we introduce a maximum-clearance planner that
fuses Voronoi diagrams with an equilibrium-manifold formulation to generate
smooth, collision-aware trajectories. We further design a safety-critical
controller that jointly enforces thrust limits and collision avoidance via
high-order control barrier functions. In simulation, our approach outperforms
sampling-based planners in cluttered environments, producing faster, safer, and
smoother trajectories and exceeding ellipsoid-based baselines in geometric
fidelity. Actual experiments on a physical aerial-manipulation platform confirm
feasibility and robustness, demonstrating consistent performance across
simulation and hardware settings. The video can be found at
https://youtu.be/hQYKwrWf1Ak.

</details>


### [14] [Dexterous Robotic Piano Playing at Scale](https://arxiv.org/abs/2511.02504)
*Le Chen,Yi Zhao,Jan Schneider,Quankai Gao,Simon Guist,Cheng Qian,Juho Kannala,Bernhard Schölkopf,Joni Pajarinen,Dieter Büchler*

Main category: cs.RO

TL;DR: OmniPianist是首个通过可扩展、无需人类演示学习能够演奏近千首音乐作品的机器人系统，通过最优运输自动指法、大规模强化学习和流匹配变换器实现双手机器人钢琴演奏。


<details>
  <summary>Details</summary>
Motivation: 赋予机器人手类人灵巧性是机器人学的长期目标，双手机器人钢琴演奏是一个特别具有挑战性的任务：高维度、接触丰富且需要快速精确控制。

Method: 1. 基于最优运输的自动指法策略；2. 训练2000多个专门化智能体的大规模强化学习；3. 使用流匹配变换器进行大规模模仿学习。

Result: 构建了包含超过100万轨迹的RP1M++数据集，OmniPianist能够演奏广泛范围的音乐作品，实验验证了方法的有效性和可扩展性。

Conclusion: 该方法推进了大规模灵巧机器人钢琴演奏的发展，展示了无需人类演示的可扩展学习方法的潜力。

Abstract: Endowing robot hands with human-level dexterity has been a long-standing goal
in robotics. Bimanual robotic piano playing represents a particularly
challenging task: it is high-dimensional, contact-rich, and requires fast,
precise control. We present OmniPianist, the first agent capable of performing
nearly one thousand music pieces via scalable, human-demonstration-free
learning. Our approach is built on three core components. First, we introduce
an automatic fingering strategy based on Optimal Transport (OT), allowing the
agent to autonomously discover efficient piano-playing strategies from scratch
without demonstrations. Second, we conduct large-scale Reinforcement Learning
(RL) by training more than 2,000 agents, each specialized in distinct music
pieces, and aggregate their experience into a dataset named RP1M++, consisting
of over one million trajectories for robotic piano playing. Finally, we employ
a Flow Matching Transformer to leverage RP1M++ through large-scale imitation
learning, resulting in the OmniPianist agent capable of performing a wide range
of musical pieces. Extensive experiments and ablation studies highlight the
effectiveness and scalability of our approach, advancing dexterous robotic
piano playing at scale.

</details>


### [15] [Non-Contact Manipulation of Induced Magnetic Dipoles](https://arxiv.org/abs/2511.02761)
*Seth Stewart,Joseph Pawelski,Steve Ward,Andrew J. Petruska*

Main category: cs.RO

TL;DR: 本文展示了在实验室测试中对半浮力铝球进行闭环位置控制，探索了不同力反演方法的有效性，这是实现感应磁偶极子3自由度位置控制更广泛应用的关键第一步。


<details>
  <summary>Details</summary>
Motivation: 将磁操纵扩展到导电非磁性物体，为以前仅限于硬磁或软磁材料的广泛应用打开了大门，特别是在空间碎片回收方面具有特殊意义。

Method: 基于先前利用感应涡流产生的反向磁矩实现3D开环位置控制的工作，本研究实现了闭环位置控制，并探索了不同的力反演方法。

Result: 在实验室测试中成功实现了对半浮力铝球的闭环位置控制。

Conclusion: 闭环方法是实现感应磁偶极子3自由度位置控制更广泛应用的关键第一步。

Abstract: Extending the field of magnetic manipulation to conductive, non-magnetic
objects opens the door for a wide array of applications previously limited to
hard or soft magnetic materials. Of particular interest is the recycling of
space debris through the use of oscillating magnetic fields, which represent a
cache of raw materials in an environment particularly suited to the low forces
generated from inductive magnetic manipulation. Building upon previous work
that demonstrated 3D open-loop position control by leveraging the opposing
dipole moment created from induced eddy currents, this work demonstrates
closed-loop position control of a semi-buoyant aluminum sphere in lab tests,
and the efficacy of varying methods for force inversion is explored. The
closed-loop methods represent a critical first step towards wider applications
for 3-DOF position control of induced magnetic dipoles.

</details>


### [16] [XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations](https://arxiv.org/abs/2511.02776)
*Shichao Fan,Kun Wu,Zhengping Che,Xinhua Wang,Di Wu,Fei Liao,Ning Liu,Yixue Zhang,Zhen Zhao,Zhiyuan Xu,Meng Li,Qingjie Liu,Shanghang Zhang,Min Wan,Jian Tang*

Main category: cs.RO

TL;DR: XR-1是一个新颖的视觉-语言-动作模型框架，通过统一视觉-运动编码(UVMC)解决现有VLA模型在精确低级动作生成和跨领域数据融合方面的挑战，在多样化机器人平台上实现了卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型面临两个基本挑战：(i)从高维观测生成精确的低级动作，(ii)跨越异构数据源（包括不同机器人体现和人类演示）的领域差距。现有方法未能充分利用大规模异构数据集中的互补多模态知识。

Method: 提出XR-1框架，引入统一视觉-运动编码(UVMC)作为离散潜在表示，通过双分支VQ-VAE联合编码视觉动态和机器人运动。采用三阶段训练范式：自监督UVMC学习、UVMC引导的大规模跨体现预训练、任务特定后训练。

Result: 在6种不同机器人体现上进行了超过14,000次实验，涵盖120多种多样化操作任务。XR-1始终优于最先进的基线方法，并在新物体、背景变化、干扰物和光照变化方面表现出强大的泛化能力。

Conclusion: XR-1通过UVMC表示和三阶段训练，有效解决了VLA模型的关键挑战，为跨机器人、任务和环境的通用可扩展学习提供了有前景的解决方案。

Abstract: Recent progress in large-scale robotic datasets and vision-language models
(VLMs) has advanced research on vision-language-action (VLA) models. However,
existing VLA models still face two fundamental challenges: (i) producing
precise low-level actions from high-dimensional observations, (ii) bridging
domain gaps across heterogeneous data sources, including diverse robot
embodiments and human demonstrations. Existing methods often encode latent
variables from either visual dynamics or robotic actions to guide policy
learning, but they fail to fully exploit the complementary multi-modal
knowledge present in large-scale, heterogeneous datasets. In this work, we
present X Robotic Model 1 (XR-1), a novel framework for versatile and scalable
VLA learning across diverse robots, tasks, and environments. XR-1 introduces
the \emph{Unified Vision-Motion Codes (UVMC)}, a discrete latent representation
learned via a dual-branch VQ-VAE that jointly encodes visual dynamics and
robotic motion. UVMC addresses these challenges by (i) serving as an
intermediate representation between the observations and actions, and (ii)
aligning multimodal dynamic information from heterogeneous data sources to
capture complementary knowledge. To effectively exploit UVMC, we propose a
three-stage training paradigm: (i) self-supervised UVMC learning, (ii)
UVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and
(iii) task-specific post-training. We validate XR-1 through extensive
real-world experiments with more than 14,000 rollouts on six different robot
embodiments, spanning over 120 diverse manipulation tasks. XR-1 consistently
outperforms state-of-the-art baselines such as $\pi_{0.5}$, $\pi_0$, RDT,
UniVLA, and GR00T-N1.5 while demonstrating strong generalization to novel
objects, background variations, distractors, and illumination changes. Our
project is at https://xr-1-vla.github.io/.

</details>


### [17] [TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System](https://arxiv.org/abs/2511.02832)
*Yanjie Ze,Siheng Zhao,Weizhuo Wang,Angjoo Kanazawa,Rocky Duan,Pieter Abbeel,Guanya Shi,Jiajun Wu,C. Karen Liu*

Main category: cs.RO

TL;DR: TWIST2是一个便携式、无需动作捕捉的人形机器人遥操作和数据收集系统，通过VR设备获取全身运动数据，结合低成本机器人颈部实现全身控制，能够高效收集演示数据并训练分层视觉运动策略。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人领域缺乏有效的数据收集框架，现有系统要么使用解耦控制，要么依赖昂贵的动作捕捉设备，限制了可扩展性。

Method: 使用PICO4U VR设备实时获取全身人体运动数据，设计2自由度低成本机器人颈部（约250美元）实现自我中心视觉，实现整体人-人形机器人控制。

Result: 系统能够在15分钟内收集100次演示，成功率接近100%，并训练出能够自主控制完整人形机器人身体的分层视觉运动策略，成功完成全身灵巧操作和动态踢球任务。

Conclusion: TWIST2提供了一个完全可复现的开源系统，解决了人形机器人数据收集和控制的扩展性问题，为大规模人形机器人学习奠定了基础。

Abstract: Large-scale data has driven breakthroughs in robotics, from language models
to vision-language-action models in bimanual manipulation. However, humanoid
robotics lacks equally effective data collection frameworks. Existing humanoid
teleoperation systems either use decoupled control or depend on expensive
motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid
teleoperation and data collection system that preserves full whole-body control
while advancing scalability. Our system leverages PICO4U VR for obtaining
real-time whole-body human motions, with a custom 2-DoF robot neck (cost around
$250) for egocentric vision, enabling holistic human-to-humanoid control. We
demonstrate long-horizon dexterous and mobile humanoid skills and we can
collect 100 demonstrations in 15 minutes with an almost 100% success rate.
Building on this pipeline, we propose a hierarchical visuomotor policy
framework that autonomously controls the full humanoid body based on egocentric
vision. Our visuomotor policy successfully demonstrates whole-body dexterous
manipulation and dynamic kicking tasks. The entire system is fully reproducible
and open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also
open-sourced at https://twist-data.github.io .

</details>
