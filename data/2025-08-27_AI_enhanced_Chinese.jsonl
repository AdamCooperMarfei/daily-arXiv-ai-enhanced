{"id": "2508.18397", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18397", "abs": "https://arxiv.org/abs/2508.18397", "authors": ["Antonio Guillen-Perez"], "title": "Mining the Long Tail: A Comparative Study of Data-Centric Criticality Metrics for Robust Offline Reinforcement Learning in Autonomous Motion Planning", "comment": null, "summary": "Offline Reinforcement Learning (RL) presents a promising paradigm for\ntraining autonomous vehicle (AV) planning policies from large-scale, real-world\ndriving logs. However, the extreme data imbalance in these logs, where mundane\nscenarios vastly outnumber rare \"long-tail\" events, leads to brittle and unsafe\npolicies when using standard uniform data sampling. In this work, we address\nthis challenge through a systematic, large-scale comparative study of data\ncuration strategies designed to focus the learning process on information-rich\nsamples. We investigate six distinct criticality weighting schemes which are\ncategorized into three families: heuristic-based, uncertainty-based, and\nbehavior-based. These are evaluated at two temporal scales, the individual\ntimestep and the complete scenario. We train seven goal-conditioned\nConservative Q-Learning (CQL) agents with a state-of-the-art, attention-based\narchitecture and evaluate them in the high-fidelity Waymax simulator. Our\nresults demonstrate that all data curation methods significantly outperform the\nbaseline. Notably, data-driven curation using model uncertainty as a signal\nachieves the most significant safety improvements, reducing the collision rate\nby nearly three-fold (from 16.0% to 5.5%). Furthermore, we identify a clear\ntrade-off where timestep-level weighting excels at reactive safety while\nscenario-level weighting improves long-horizon planning. Our work provides a\ncomprehensive framework for data curation in Offline RL and underscores that\nintelligent, non-uniform sampling is a critical component for building safe and\nreliable autonomous agents.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u6027\u6bd4\u8f83\u7814\u7a76\u516d\u79cd\u6570\u636e\u7b5b\u9009\u7b56\u7565\uff0c\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4f7f\u81ea\u52a8\u9a7e\u9a76\u8ba1\u5212\u7b56\u7565\u66f4\u5b89\u5168\u53ef\u9760\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u89c4\u6a21\u5b9e\u9645\u9a7e\u9a76\u65e5\u5fd7\u4e2d\u9047\u5230\u6781\u7aef\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e73\u51e1\u573a\u666f\u8fc7\u591a\u800c\u7a00\u6709\u957f\u5c3e\u4e8b\u4ef6\u8fc7\u5c11\uff0c\u5bfc\u81f4\u7b56\u7565\u8106\u5f31\u4e0d\u5b89\u5168\u3002", "method": "\u7814\u7a76\u4e86\u516d\u79cd\u4e34\u754c\u6743\u91cd\u65b9\u6848\uff0c\u5206\u4e3a\u4e09\u5927\u7c7b\uff1a\u542b\u4e49\u57fa\u7840\u3001\u4e0d\u786e\u5b9a\u6027\u57fa\u7840\u548c\u884c\u4e3a\u57fa\u7840\u3002\u5728\u4e24\u4e2a\u65f6\u95f4\u5c3a\u5ea6\uff08\u5355\u4e2a\u65f6\u95f4\u6b65\u548c\u5b8c\u6574\u573a\u666f\uff09\u8fdb\u884c\u8bc4\u4f30\uff0c\u8bad\u7ec3\u4e03\u4e2a\u76ee\u6807\u6761\u4ef6CQL\u7ee7\u6ce8\u5668\u3002", "result": "\u6240\u6709\u6570\u636e\u7b5b\u9009\u65b9\u6cd5\u90fd\u663e\u8457\u8d85\u8fc7\u57fa\u51c6\u7ebf\uff0c\u5176\u4e2d\u4f7f\u7528\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u5b89\u5168\u6027\u6539\u5584\u6700\u4e3a\u663e\u8457\uff0c\u78b0\u649e\u7387\u4ece16.0%\u964d\u81f35.5%\uff0c\u51cf\u5c11\u4e86\u8fd1\u4e09\u500d\u3002\u65f6\u95f4\u6b65\u7ea7\u6743\u91cd\u5728\u53cd\u5e94\u5f0f\u5b89\u5168\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u573a\u666f\u7ea7\u6743\u91cd\u5728\u957f\u671f\u8ba1\u5212\u4e0a\u66f4\u597d\u3002", "conclusion": "\u667a\u80fd\u7684\u975e\u5747\u5300\u91c7\u6837\u662f\u6784\u5efa\u5b89\u5168\u53ef\u9760\u81ea\u4e3b\u4ee3\u7406\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u672c\u7814\u7a76\u4e3a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6570\u636e\u7b5b\u9009\u63d0\u4f9b\u4e86\u5168\u9762\u6846\u67b6\u3002"}}
{"id": "2508.18399", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18399", "abs": "https://arxiv.org/abs/2508.18399", "authors": ["Christian Friedrich", "Ralf Gulde", "Armin Lechler", "Alexander Verl"], "title": "Maintenance automation: methods for robotics manipulation planning and execution", "comment": "11 pages, 12 figures", "summary": "Automating complex tasks using robotic systems requires skills for planning,\ncontrol and execution. This paper proposes a complete robotic system for\nmaintenance automation, which can automate disassembly and assembly operations\nunder environmental uncertainties (e.g. deviations between prior plan\ninformation). The cognition of the robotic system is based on a planning\napproach (using CAD and RGBD data) and includes a method to interpret a\nsymbolic plan and transform it to a set of executable robot instructions. The\ncomplete system is experimentally evaluated using real-world applications. This\nwork shows the first step to transfer these theoretical results into a\npractical robotic solution.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u673a\u5668\u4eba\u7ef4\u62a4\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u4e0b\u81ea\u52a8\u5316\u6267\u884c\u62c6\u5378\u548c\u7ec4\u88c5\u64cd\u4f5c\u3002", "motivation": "\u81ea\u52a8\u5316\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\u9700\u8981\u89c4\u5212\u3001\u63a7\u5236\u548c\u6267\u884c\u6280\u80fd\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u73af\u5883\u4e0d\u786e\u5b9a\u6027\uff08\u5982\u5148\u9a8c\u8ba1\u5212\u4fe1\u606f\u504f\u5dee\uff09\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u57fa\u4e8eCAD\u548cRGBD\u6570\u636e\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u5305\u62ec\u7b26\u53f7\u5316\u8ba1\u5212\u89e3\u91ca\u548c\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u673a\u5668\u4eba\u6307\u4ee4\u7684\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u662f\u5c06\u7406\u8bba\u6210\u679c\u8f6c\u5316\u4e3a\u5b9e\u7528\u673a\u5668\u4eba\u89e3\u51b3\u65b9\u6848\u7684\u7b2c\u4e00\u6b65\uff0c\u5c55\u793a\u4e86\u5b8c\u6574\u7684\u673a\u5668\u4eba\u7ef4\u62a4\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2508.18400", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18400", "abs": "https://arxiv.org/abs/2508.18400", "authors": ["Christian Friedrich", "Akos Csiszar", "Armin Lechler", "Alexander Verl"], "title": "Efficient task and path planning for maintenance automation using a robot system", "comment": "10 pages, 10 figures", "summary": "The research and development of intelligent automation solutions is a\nground-breaking point for the factory of the future. A promising and\nchallenging mission is the use of autonomous robot systems to automate tasks in\nthe field of maintenance. For this purpose, the robot system must be able to\nplan autonomously the different manipulation tasks and the corresponding paths.\nBasic requirements are the development of algorithms with a low computational\ncomplexity and the possibility to deal with environmental uncertainties. In\nthis work, an approach is presented, which is especially suited to solve the\nproblem of maintenance automation. For this purpose, offline data from CAD is\ncombined with online data from an RGBD vision system via a probabilistic\nfilter, to compensate uncertainties from offline data. For planning the\ndifferent tasks, a method is explained, which use a symbolic description,\nfounded on a novel sampling-based method to compute the disassembly space. For\npath planning we use global state-of-the art algorithms with a method that\nallows the adaption of the exploration stepsize in order to reduce the planning\ntime. Every method is experimentally validated and discussed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5de5\u5382\u7ef4\u62a4\u81ea\u52a8\u5316\u7684\u667a\u80fd\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7ed3\u5408\u79bb\u7ebfCAD\u6570\u636e\u548c\u5728\u7ebfRGBD\u89c6\u89c9\u6570\u636e\uff0c\u901a\u8fc7\u6982\u7387\u6ee4\u6ce2\u8865\u507f\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u7528\u57fa\u4e8e\u91c7\u6837\u7684\u7b26\u53f7\u5316\u65b9\u6cd5\u8fdb\u884c\u4efb\u52a1\u89c4\u5212\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u6b65\u957f\u7684\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u6765\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u5de5\u5382\u7ef4\u62a4\u81ea\u52a8\u5316\u662f\u672a\u6765\u5de5\u5382\u7684\u5173\u952e\u7a81\u7834\u70b9\uff0c\u9700\u8981\u5f00\u53d1\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u4e14\u80fd\u5904\u7406\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u6765\u5b8c\u6210\u7ef4\u62a4\u4efb\u52a1\u3002", "method": "\u7ed3\u5408\u79bb\u7ebfCAD\u6570\u636e\u548c\u5728\u7ebfRGBD\u89c6\u89c9\u6570\u636e\u901a\u8fc7\u6982\u7387\u6ee4\u6ce2\u8865\u507f\u4e0d\u786e\u5b9a\u6027\uff1b\u4f7f\u7528\u57fa\u4e8e\u7b26\u53f7\u5316\u63cf\u8ff0\u7684\u91c7\u6837\u65b9\u6cd5\u8ba1\u7b97\u62c6\u5378\u7a7a\u95f4\u8fdb\u884c\u4efb\u52a1\u89c4\u5212\uff1b\u91c7\u7528\u81ea\u9002\u5e94\u63a2\u7d22\u6b65\u957f\u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u51cf\u5c11\u89c4\u5212\u65f6\u95f4\u3002", "result": "\u6240\u6709\u65b9\u6cd5\u90fd\u7ecf\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u548c\u8ba8\u8bba\uff0c\u8bc1\u660e\u4e86\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5de5\u5382\u7ef4\u62a4\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u878d\u5408\u591a\u6e90\u6570\u636e\u548c\u4f18\u5316\u89c4\u5212\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u4efb\u52a1\u6267\u884c\u80fd\u529b\u3002"}}
{"id": "2508.18443", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18443", "abs": "https://arxiv.org/abs/2508.18443", "authors": ["Ruohan Zhang", "Uksang Yoo", "Yichen Li", "Arpit Argawal", "Wenzhen Yuan"], "title": "PneuGelSight: Soft Robotic Vision-Based Proprioception and Tactile Sensing", "comment": "16 pages, 12 figures, International Journal of Robotics Research\n  (accepted), 2025", "summary": "Soft pneumatic robot manipulators are popular in industrial and\nhuman-interactive applications due to their compliance and flexibility.\nHowever, deploying them in real-world scenarios requires advanced sensing for\ntactile feedback and proprioception. Our work presents a novel vision-based\napproach for sensorizing soft robots. We demonstrate our approach on\nPneuGelSight, a pioneering pneumatic manipulator featuring high-resolution\nproprioception and tactile sensing via an embedded camera. To optimize the\nsensor's performance, we introduce a comprehensive pipeline that accurately\nsimulates its optical and dynamic properties, facilitating a zero-shot\nknowledge transition from simulation to real-world applications. PneuGelSight\nand our sim-to-real pipeline provide a novel, easily implementable, and robust\nsensing methodology for soft robots, paving the way for the development of more\nadvanced soft robots with enhanced sensory capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u4f20\u611f\u65b9\u6cd5PneuGelSight\uff0c\u901a\u8fc7\u5d4c\u5165\u5f0f\u6444\u50cf\u5934\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u672c\u4f53\u611f\u77e5\u548c\u89e6\u89c9\u611f\u77e5\uff0c\u5e76\u5f00\u53d1\u4e86\u4ece\u4eff\u771f\u5230\u771f\u5b9e\u73af\u5883\u7684\u96f6\u6837\u672c\u77e5\u8bc6\u8fc1\u79fb\u7ba1\u9053\u3002", "motivation": "\u8f6f\u4f53\u6c14\u52a8\u673a\u5668\u4eba\u5728\u5de5\u4e1a\u548c\u4eba\u9645\u4ea4\u4e92\u5e94\u7528\u4e2d\u5177\u6709\u987a\u5e94\u6027\u548c\u7075\u6d3b\u6027\u4f18\u52bf\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9700\u8981\u5148\u8fdb\u7684\u4f20\u611f\u6280\u672f\u6765\u63d0\u4f9b\u89e6\u89c9\u53cd\u9988\u548c\u672c\u4f53\u611f\u77e5\u3002", "method": "\u5f00\u53d1\u4e86PneuGelSight\u6c14\u52a8\u64cd\u4f5c\u5668\uff0c\u91c7\u7528\u5d4c\u5165\u5f0f\u6444\u50cf\u5934\u5b9e\u73b0\u4f20\u611f\u529f\u80fd\uff0c\u5e76\u5efa\u7acb\u4e86\u5b8c\u6574\u7684\u5149\u5b66\u548c\u52a8\u529b\u5b66\u7279\u6027\u4eff\u771f\u7ba1\u9053\uff0c\u652f\u6301\u4ece\u4eff\u771f\u5230\u771f\u5b9e\u73af\u5883\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u5206\u8fa8\u7387\u7684\u672c\u4f53\u611f\u77e5\u548c\u89e6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u4eff\u771f\u5230\u771f\u5b9e\u73af\u5883\u7684\u6709\u6548\u77e5\u8bc6\u8fc1\u79fb\u3002", "conclusion": "PneuGelSight\u548c\u4eff\u771f\u5230\u771f\u5b9e\u7ba1\u9053\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u3001\u6613\u4e8e\u5b9e\u73b0\u4e14\u9c81\u68d2\u7684\u4f20\u611f\u65b9\u6cd5\uff0c\u4e3a\u5f00\u53d1\u5177\u6709\u589e\u5f3a\u611f\u77e5\u80fd\u529b\u7684\u5148\u8fdb\u8f6f\u4f53\u673a\u5668\u4eba\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.18460", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18460", "abs": "https://arxiv.org/abs/2508.18460", "authors": ["Tianze Liu", "Md Abu Bakr Siddique", "Hongyu An"], "title": "Mimicking associative learning of rats via a neuromorphic robot in open field maze using spatial cell models", "comment": null, "summary": "Data-driven Artificial Intelligence (AI) approaches have exhibited remarkable\nprowess across various cognitive tasks using extensive training data. However,\nthe reliance on large datasets and neural networks presents challenges such as\nhighpower consumption and limited adaptability, particularly in\nSWaP-constrained applications like planetary exploration. To address these\nissues, we propose enhancing the autonomous capabilities of intelligent robots\nby emulating the associative learning observed in animals. Associative learning\nenables animals to adapt to their environment by memorizing concurrent events.\nBy replicating this mechanism, neuromorphic robots can navigate dynamic\nenvironments autonomously, learning from interactions to optimize performance.\nThis paper explores the emulation of associative learning in rodents using\nneuromorphic robots within open-field maze environments, leveraging insights\nfrom spatial cells such as place and grid cells. By integrating these models,\nwe aim to enable online associative learning for spatial tasks in real-time\nscenarios, bridging the gap between biological spatial cognition and robotics\nfor advancements in autonomous systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u6a21\u62df\u52a8\u7269\u8054\u60f3\u5b66\u4e60\u673a\u5236\u6765\u589e\u5f3a\u667a\u80fd\u673a\u5668\u4eba\u7684\u81ea\u4e3b\u80fd\u529b\uff0c\u4f7f\u7528\u795e\u7ecf\u5f62\u6001\u673a\u5668\u4eba\u5728\u5f00\u653e\u8ff7\u5bab\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u7a7a\u95f4\u4efb\u52a1\u7684\u5728\u7ebf\u8054\u60f3\u5b66\u4e60", "motivation": "\u4f20\u7edf\u6570\u636e\u9a71\u52a8\u7684AI\u65b9\u6cd5\u4f9d\u8d56\u5927\u6570\u636e\u96c6\u548c\u795e\u7ecf\u7f51\u7edc\uff0c\u5b58\u5728\u9ad8\u529f\u8017\u548c\u9002\u5e94\u6027\u6709\u9650\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728SWaP\u53d7\u9650\u7684\u5e94\u7528\u4e2d\u5982\u884c\u661f\u63a2\u7d22\u3002\u9700\u8981\u5bfb\u627e\u66f4\u9ad8\u6548\u7684\u81ea\u4e3b\u5b66\u4e60\u65b9\u5f0f", "method": "\u6a21\u62df\u556e\u9f7f\u7c7b\u52a8\u7269\u7684\u8054\u60f3\u5b66\u4e60\u673a\u5236\uff0c\u5229\u7528\u7a7a\u95f4\u7ec6\u80de\uff08\u4f4d\u7f6e\u7ec6\u80de\u548c\u7f51\u683c\u7ec6\u80de\uff09\u7684\u751f\u7269\u5b66\u89c1\u89e3\uff0c\u5728\u795e\u7ecf\u5f62\u6001\u673a\u5668\u4eba\u4e2d\u5b9e\u73b0\u5173\u8054\u8bb0\u5fc6\u548c\u5b9e\u65f6\u73af\u5883\u9002\u5e94", "result": "\u901a\u8fc7\u795e\u7ecf\u5f62\u6001\u673a\u5668\u4eba\u6210\u529f\u5728\u5f00\u653e\u8ff7\u5bab\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u8054\u60f3\u5b66\u4e60\u80fd\u529b\uff0c\u80fd\u591f\u81ea\u4e3b\u5bfc\u822a\u52a8\u6001\u73af\u5883\u5e76\u901a\u8fc7\u4ea4\u4e92\u5b66\u4e60\u4f18\u5316\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u751f\u7269\u7a7a\u95f4\u8ba4\u77e5\u4e0e\u673a\u5668\u4eba\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c"}}
{"id": "2508.18606", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18606", "abs": "https://arxiv.org/abs/2508.18606", "authors": ["Nicky Zimmerman", "Joel Loo", "Ayush Agrawal", "David Hsu"], "title": "SignLoc: Robust Localization using Navigation Signs and Public Maps", "comment": "Under submission for Robotics and Automation Letters (RA-L)", "summary": "Navigation signs and maps, such as floor plans and street maps, are widely\navailable and serve as ubiquitous aids for way-finding in human environments.\nYet, they are rarely used by robot systems. This paper presents SignLoc, a\nglobal localization method that leverages navigation signs to localize the\nrobot on publicly available maps -- specifically floor plans and OpenStreetMap\n(OSM) graphs -- without prior sensor-based mapping. SignLoc first extracts a\nnavigation graph from the input map. It then employs a probabilistic\nobservation model to match directional and locational cues from the detected\nsigns to the graph, enabling robust topo-semantic localization within a Monte\nCarlo framework. We evaluated SignLoc in diverse large-scale environments: part\nof a university campus, a shopping mall, and a hospital complex. Experimental\nresults show that SignLoc reliably localizes the robot after observing only one\nto two signs.", "AI": {"tldr": "SignLoc\u662f\u4e00\u79cd\u5229\u7528\u5bfc\u822a\u6807\u5fd7\u8fdb\u884c\u5168\u5c40\u5b9a\u4f4d\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u5148\u9a8c\u4f20\u611f\u5668\u5efa\u56fe\uff0c\u4ec5\u9700\u89c2\u5bdf1-2\u4e2a\u6807\u5fd7\u5373\u53ef\u5728\u516c\u5f00\u5730\u56fe\u4e0a\u5b9e\u73b0\u9c81\u68d2\u7684\u62d3\u6251\u8bed\u4e49\u5b9a\u4f4d", "motivation": "\u5bfc\u822a\u6807\u5fd7\u548c\u5730\u56fe\uff08\u5982\u5e73\u9762\u56fe\u548c\u8857\u9053\u5730\u56fe\uff09\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u5e7f\u6cdb\u5b58\u5728\uff0c\u4f46\u5f88\u5c11\u88ab\u673a\u5668\u4eba\u7cfb\u7edf\u4f7f\u7528\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u8fd9\u4e9b\u73b0\u6210\u7684\u5bfc\u822a\u6807\u5fd7\u6765\u5b9e\u73b0\u673a\u5668\u4eba\u7684\u5168\u5c40\u5b9a\u4f4d", "method": "\u9996\u5148\u4ece\u8f93\u5165\u5730\u56fe\u4e2d\u63d0\u53d6\u5bfc\u822a\u56fe\uff0c\u7136\u540e\u4f7f\u7528\u6982\u7387\u89c2\u6d4b\u6a21\u578b\u5c06\u68c0\u6d4b\u5230\u7684\u6807\u5fd7\u4e2d\u7684\u65b9\u5411\u548c\u4f4d\u7f6e\u7ebf\u7d22\u4e0e\u56fe\u8fdb\u884c\u5339\u914d\uff0c\u5728\u8499\u7279\u5361\u6d1b\u6846\u67b6\u5185\u5b9e\u73b0\u9c81\u68d2\u7684\u62d3\u6251\u8bed\u4e49\u5b9a\u4f4d", "result": "\u5728\u5927\u578b\u73af\u5883\uff08\u5927\u5b66\u6821\u56ed\u3001\u8d2d\u7269\u4e2d\u5fc3\u3001\u533b\u9662\u7efc\u5408\u4f53\uff09\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSignLoc\u5728\u4ec5\u89c2\u5bdf1-2\u4e2a\u6807\u5fd7\u540e\u5c31\u80fd\u53ef\u9760\u5730\u5b9a\u4f4d\u673a\u5668\u4eba", "conclusion": "SignLoc\u6210\u529f\u8bc1\u660e\u4e86\u5229\u7528\u73b0\u6210\u5bfc\u822a\u6807\u5fd7\u548c\u516c\u5f00\u5730\u56fe\u8fdb\u884c\u673a\u5668\u4eba\u5168\u5c40\u5b9a\u4f4d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u65e0\u9700\u4f20\u611f\u5668\u5efa\u56fe\u7684\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84"}}
{"id": "2508.18627", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18627", "abs": "https://arxiv.org/abs/2508.18627", "authors": ["Ziyuan Jiao", "Yida Niu", "Zeyu Zhang", "Yangyang Wu", "Yao Su", "Yixin Zhu", "Hangxin Liu", "Song-Chun Zhu"], "title": "Integration of Robot and Scene Kinematics for Sequential Mobile Manipulation Planning", "comment": "20 pages, 13 figures; accepted by Transactions on Robotics", "summary": "We present a Sequential Mobile Manipulation Planning (SMMP) framework that\ncan solve long-horizon multi-step mobile manipulation tasks with coordinated\nwhole-body motion, even when interacting with articulated objects. By\nabstracting environmental structures as kinematic models and integrating them\nwith the robot's kinematics, we construct an Augmented Configuration Apace\n(A-Space) that unifies the previously separate task constraints for navigation\nand manipulation, while accounting for the joint reachability of the robot\nbase, arm, and manipulated objects. This integration facilitates efficient\nplanning within a tri-level framework: a task planner generates symbolic action\nsequences to model the evolution of A-Space, an optimization-based motion\nplanner computes continuous trajectories within A-Space to achieve desired\nconfigurations for both the robot and scene elements, and an intermediate plan\nrefinement stage selects action goals that ensure long-horizon feasibility. Our\nsimulation studies first confirm that planning in A-Space achieves an 84.6\\%\nhigher task success rate compared to baseline methods. Validation on real\nrobotic systems demonstrates fluid mobile manipulation involving (i) seven\ntypes of rigid and articulated objects across 17 distinct contexts, and (ii)\nlong-horizon tasks of up to 14 sequential steps. Our results highlight the\nsignificance of modeling scene kinematics into planning entities, rather than\nencoding task-specific constraints, offering a scalable and generalizable\napproach to complex robotic manipulation.", "AI": {"tldr": "\u901a\u8fc7\u6784\u5efa\u589e\u5e7f\u914d\u7f6e\u7a7a\u95f4(A-Space)\u7edf\u4e00\u5bfc\u822a\u548c\u64cd\u4f5c\u7ea6\u675f\uff0c\u63d0\u51fa\u4e09\u5c42\u6b21\u6a21\u578b\u6765\u89e3\u51b3\u957f\u65f6\u57df\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\uff0c\u5728\u6a21\u62df\u4e2d\u4efb\u52a1\u6210\u529f\u7387\u63d0\u534784.6%\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u5177\u6709\u53ef\u6269\u5c55\u6027\u7684\u590d\u6742\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u957f\u65f6\u57df\u591a\u6b65\u9aa4\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5bfc\u822a\u548c\u64cd\u4f5c\u7ea6\u675f\u5206\u79bb\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u89c4\u5212\u65b9\u6cd5\u6765\u5904\u7406\u673a\u5668\u4eba\u57fa\u5ea7\u3001\u624b\u81c2\u548c\u64cd\u4f5c\u5bf9\u8c61\u7684\u534f\u8c03\u8fbe\u5230\u6027\u95ee\u9898\u3002", "method": "\u6784\u5efa\u589e\u5e7f\u914d\u7f6e\u7a7a\u95f4(A-Space)\uff0c\u5c06\u73af\u5883\u7ed3\u6784\u62bd\u8c61\u4e3a\u8fd0\u52a8\u5b66\u6a21\u578b\u5e76\u4e0e\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u96c6\u6210\uff0c\u91c7\u7528\u4efb\u52a1\u89c4\u5212\u5668\u3001\u4f18\u5316\u57fa\u4e8e\u8fd0\u52a8\u89c4\u5212\u5668\u548c\u4e2d\u95f4\u89c4\u5212\u7cbe\u70bc\u9636\u6bb5\u7684\u4e09\u5c42\u6846\u67b6\u8fdb\u884c\u89c4\u5212\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u663e\u793a\u4efb\u52a1\u6210\u529f\u7387\u6bd4\u57fa\u51c6\u65b9\u6cd5\u63d0\u9ad884.6%\uff0c\u771f\u5b9e\u673a\u5668\u4eba\u7cfb\u7edf\u572817\u4e2a\u4e0d\u540c\u573a\u666f\u4e2d\u6210\u529f\u5904\u74067\u79cd\u7c7b\u578b\u7684\u521a\u4f53\u548c\u5173\u8282\u5bf9\u8c61\uff0c\u80fd\u591f\u5b8c\u6210\u6700\u957f14\u4e2a\u6b65\u9aa4\u7684\u957f\u65f6\u57df\u4efb\u52a1\u3002", "conclusion": "\u5c06\u573a\u666f\u8fd0\u52a8\u5b66\u6a21\u578b\u96c6\u6210\u5230\u89c4\u5212\u5b9e\u4f53\u4e2d\uff0c\u800c\u975e\u7f16\u7801\u4efb\u52a1\u7279\u5b9a\u7ea6\u675f\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u548c\u901a\u7528\u7684\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u65b9\u6cd5\u3002"}}
{"id": "2508.18662", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18662", "abs": "https://arxiv.org/abs/2508.18662", "authors": ["Stefan Ramdhan", "Winnie Trandinh", "Istvan David", "Vera Pantelic", "Mark Lawford"], "title": "Engineering Automotive Digital Twins on Standardized Architectures: A Case Study", "comment": "7 pages, 6 figures. Submitted to EDTconf 2025", "summary": "Digital twin (DT) technology has become of interest in the automotive\nindustry. There is a growing need for smarter services that utilize the unique\ncapabilities of DTs, ranging from computer-aided remote control to cloud-based\nfleet coordination. Developing such services starts with the software\narchitecture. However, the scarcity of DT architectural guidelines poses a\nchallenge for engineering automotive DTs. Currently, the only DT architectural\nstandard is the one defined in ISO 23247. Though not developed for automotive\nsystems, it is one of the few feasible starting points for automotive DTs. In\nthis work, we investigate the suitability of the ISO 23247 reference\narchitecture for developing automotive DTs. Through the case study of\ndeveloping an Adaptive Cruise Control DT for a 1/10\\textsuperscript{th}-scale\nautonomous vehicle, we identify some strengths and limitations of the reference\narchitecture and begin distilling future directions for researchers,\npractitioners, and standard developers.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76ISO 23247\u6807\u51c6\u5728\u6c7d\u8f66\u6570\u5b57\u53cc\u80de\u67b6\u6784\u4e2d\u7684\u9002\u7528\u6027\uff0c\u901a\u8fc7\u9002\u5e94\u6027\u5de1\u822a\u63a7\u5236\u6848\u4f8b\u5206\u6790\u5176\u4f18\u52bf\u548c\u5c40\u9650\u6027", "motivation": "\u6c7d\u8f66\u884c\u4e1a\u5bf9\u6570\u5b57\u53cc\u80de\u6280\u672f\u7684\u9700\u6c42\u589e\u957f\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u7684\u67b6\u6784\u6307\u5357\uff0cISO 23247\u662f\u5c11\u6570\u53ef\u7528\u7684\u5f00\u59cb\u70b9", "method": "\u901a\u8fc7\u4e3a1/10\u7f29\u6bd4\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f68\u9053\u8f66\u5f00\u53d1\u9002\u5e94\u6027\u5de1\u822a\u63a7\u5236\u6570\u5b57\u53cc\u80de\u7684\u6848\u4f8b\u7814\u7a76", "result": "\u8bc6\u522b\u4e86ISO 23247\u53c2\u8003\u67b6\u6784\u5728\u6c7d\u8f66\u9886\u57df\u7684\u4e00\u4e9b\u4f18\u52bf\u548c\u5c40\u9650\u6027", "conclusion": "\u4e3a\u7814\u7a76\u4eba\u5458\u3001\u5b9e\u8df5\u8005\u548c\u6807\u51c6\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u7684\u542f\u793a"}}
{"id": "2508.18691", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18691", "abs": "https://arxiv.org/abs/2508.18691", "authors": ["Himanshu Gaurav Singh", "Pieter Abbeel", "Jitendra Malik", "Antonio Loquercio"], "title": "Deep Sensorimotor Control by Imitating Predictive Models of Human Motion", "comment": "Blog Post: https://hgaurav2k.github.io/trackr/", "summary": "As the embodiment gap between a robot and a human narrows, new opportunities\narise to leverage datasets of humans interacting with their surroundings for\nrobot learning. We propose a novel technique for training sensorimotor policies\nwith reinforcement learning by imitating predictive models of human motions.\nOur key insight is that the motion of keypoints on human-inspired robot\nend-effectors closely mirrors the motion of corresponding human body keypoints.\nThis enables us to use a model trained to predict future motion on human data\n\\emph{zero-shot} on robot data. We train sensorimotor policies to track the\npredictions of such a model, conditioned on a history of past robot states,\nwhile optimizing a relatively sparse task reward. This approach entirely\nbypasses gradient-based kinematic retargeting and adversarial losses, which\nlimit existing methods from fully leveraging the scale and diversity of modern\nhuman-scene interaction datasets. Empirically, we find that our approach can\nwork across robots and tasks, outperforming existing baselines by a large\nmargin. In addition, we find that tracking a human motion model can substitute\nfor carefully designed dense rewards and curricula in manipulation tasks. Code,\ndata and qualitative results available at\nhttps://jirl-upenn.github.io/track_reward/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\u6765\u8bad\u7ec3\u673a\u5668\u4eba\u4f20\u611f\u5668\u8fd0\u52a8\u7b56\u7565\u7684\u65b0\u65b9\u6cd5\uff0c\u65e0\u9700\u68af\u5ea6\u8fd0\u52a8\u91cd\u5b9a\u5411\u6216\u5bf9\u6297\u635f\u5931\uff0c\u53ef\u76f4\u63a5\u5229\u7528\u4eba\u7c7b\u4ea4\u4e92\u6570\u636e", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u4e4b\u95f4\u7684\u4f53\u73b0\u5dee\u8ddd\u7f29\u5c0f\uff0c\u53ef\u4ee5\u5229\u7528\u4eba\u7c7b\u4e0e\u73af\u5883\u4ea4\u4e92\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6765\u8fdb\u884c\u673a\u5668\u4eba\u5b66\u4e60\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u8fd0\u52a8\u91cd\u5b9a\u5411\u548c\u5bf9\u6297\u635f\u5931\u7684\u9650\u5236", "method": "\u4f7f\u7528\u5728\u4eba\u7c7b\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u9884\u6d4b\u6a21\u578b\u96f6\u6837\u672c\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u6570\u636e\uff0c\u8bad\u7ec3\u4f20\u611f\u5668\u8fd0\u52a8\u7b56\u7565\u6765\u8ddf\u8e2a\u6a21\u578b\u9884\u6d4b\uff0c\u540c\u65f6\u4f18\u5316\u7a00\u758f\u4efb\u52a1\u5956\u52b1", "result": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u673a\u5668\u4eba\u548c\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u80fd\u66ff\u4ee3\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5bc6\u96c6\u5956\u52b1\u548c\u8bfe\u7a0b\u5b66\u4e60", "conclusion": "\u901a\u8fc7\u8ddf\u8e2a\u4eba\u7c7b\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u5229\u7528\u4eba\u7c7b\u573a\u666f\u4ea4\u4e92\u6570\u636e\u96c6\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u65b0\u7684\u6709\u6548\u9014\u5f84"}}
{"id": "2508.18694", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18694", "abs": "https://arxiv.org/abs/2508.18694", "authors": ["Jaehwan Jeong", "Tuan-Anh Vu", "Mohammad Jony", "Shahab Ahmad", "Md. Mukhlesur Rahman", "Sangpil Kim", "M. Khalid Jawed"], "title": "AgriChrono: A Multi-modal Dataset Capturing Crop Growth and Lighting Variability with a Field Robot", "comment": null, "summary": "Existing datasets for precision agriculture have primarily been collected in\nstatic or controlled environments such as indoor labs or greenhouses, often\nwith limited sensor diversity and restricted temporal span. These conditions\nfail to reflect the dynamic nature of real farmland, including illumination\nchanges, crop growth variation, and natural disturbances. As a result, models\ntrained on such data often lack robustness and generalization when applied to\nreal-world field scenarios. In this paper, we present AgriChrono, a novel\nrobotic data collection platform and multi-modal dataset designed to capture\nthe dynamic conditions of real-world agricultural environments. Our platform\nintegrates multiple sensors and enables remote, time-synchronized acquisition\nof RGB, Depth, LiDAR, and IMU data, supporting efficient and repeatable\nlong-term data collection across varying illumination and crop growth stages.\nWe benchmark a range of state-of-the-art 3D reconstruction models on the\nAgriChrono dataset, highlighting the difficulty of reconstruction in real-world\nfield environments and demonstrating its value as a research asset for\nadvancing model generalization under dynamic conditions. The code and dataset\nare publicly available at: https://github.com/StructuresComp/agri-chrono", "AI": {"tldr": "AgriChrono\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\u5e73\u53f0\u548c\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u6355\u6349\u771f\u5b9e\u519c\u4e1a\u73af\u5883\u7684\u52a8\u6001\u6761\u4ef6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5728\u9759\u6001\u6216\u53d7\u63a7\u73af\u5883\u4e2d\u6536\u96c6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7cbe\u51c6\u519c\u4e1a\u6570\u636e\u96c6\u4e3b\u8981\u5728\u9759\u6001\u6216\u53d7\u63a7\u73af\u5883\u4e2d\u6536\u96c6\uff0c\u4f20\u611f\u5668\u591a\u6837\u6027\u6709\u9650\uff0c\u65f6\u95f4\u8de8\u5ea6\u53d7\u9650\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u519c\u7530\u7684\u52a8\u6001\u7279\u6027\uff08\u5982\u5149\u7167\u53d8\u5316\u3001\u4f5c\u7269\u751f\u957f\u53d8\u5316\u548c\u81ea\u7136\u5e72\u6270\uff09\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7f3a\u4e4f\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u96c6\u6210\u4e86\u591a\u79cd\u4f20\u611f\u5668\u7684\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\u5e73\u53f0\uff0c\u652f\u6301\u8fdc\u7a0b\u3001\u65f6\u95f4\u540c\u6b65\u91c7\u96c6RGB\u3001\u6df1\u5ea6\u3001LiDAR\u548cIMU\u6570\u636e\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u5149\u7167\u548c\u4f5c\u7269\u751f\u957f\u9636\u6bb5\u8fdb\u884c\u9ad8\u6548\u3001\u53ef\u91cd\u590d\u7684\u957f\u671f\u6570\u636e\u6536\u96c6\u3002", "result": "\u5728AgriChrono\u6570\u636e\u96c6\u4e0a\u5bf9\u591a\u79cd\u6700\u5148\u8fdb\u76843D\u91cd\u5efa\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5728\u771f\u5b9e\u519c\u7530\u73af\u5883\u4e2d\u8fdb\u884c\u91cd\u5efa\u7684\u56f0\u96be\uff0c\u5e76\u8bc1\u660e\u4e86\u8be5\u6570\u636e\u96c6\u4f5c\u4e3a\u7814\u7a76\u8d44\u4ea7\u5728\u63d0\u5347\u52a8\u6001\u6761\u4ef6\u4e0b\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u4ef7\u503c\u3002", "conclusion": "AgriChrono\u5e73\u53f0\u548c\u6570\u636e\u96c6\u4e3a\u7cbe\u51c6\u519c\u4e1a\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6570\u636e\u8d44\u6e90\uff0c\u80fd\u591f\u6709\u6548\u4fc3\u8fdb\u6a21\u578b\u5728\u771f\u5b9e\u52a8\u6001\u519c\u4e1a\u73af\u5883\u4e2d\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2508.18705", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18705", "abs": "https://arxiv.org/abs/2508.18705", "authors": ["Santosh Thoduka", "Sebastian Houben", "Juergen Gall", "Paul G. Pl\u00f6ger"], "title": "Enhancing Video-Based Robot Failure Detection Using Task Knowledge", "comment": "Accepted at ECMR 2025", "summary": "Robust robotic task execution hinges on the reliable detection of execution\nfailures in order to trigger safe operation modes, recovery strategies, or task\nreplanning. However, many failure detection methods struggle to provide\nmeaningful performance when applied to a variety of real-world scenarios. In\nthis paper, we propose a video-based failure detection approach that uses\nspatio-temporal knowledge in the form of the actions the robot performs and\ntask-relevant objects within the field of view. Both pieces of information are\navailable in most robotic scenarios and can thus be readily obtained. We\ndemonstrate the effectiveness of our approach on three datasets that we amend,\nin part, with additional annotations of the aforementioned task-relevant\nknowledge. In light of the results, we also propose a data augmentation method\nthat improves performance by applying variable frame rates to different parts\nof the video. We observe an improvement from 77.9 to 80.0 in F1 score on the\nARMBench dataset without additional computational expense and an additional\nincrease to 81.4 with test-time augmentation. The results emphasize the\nimportance of spatio-temporal information during failure detection and suggest\nfurther investigation of suitable heuristics in future implementations. Code\nand annotations are available.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u9891\u7684\u673a\u5668\u4eba\u6267\u884c\u5931\u8d25\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u52a8\u4f5c\u4fe1\u606f\u548c\u4efb\u52a1\u76f8\u5173\u7269\u4f53\u7684\u65f6\u7a7a\u77e5\u8bc6\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6280\u672f\u63d0\u5347\u68c0\u6d4b\u6027\u80fd", "motivation": "\u73b0\u6709\u5931\u8d25\u68c0\u6d4b\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6027\u80fd\u6709\u9650\uff0c\u9700\u8981\u53ef\u9760\u68c0\u6d4b\u6267\u884c\u5931\u8d25\u6765\u89e6\u53d1\u5b89\u5168\u64cd\u4f5c\u6a21\u5f0f\u3001\u6062\u590d\u7b56\u7565\u6216\u4efb\u52a1\u91cd\u89c4\u5212", "method": "\u4f7f\u7528\u673a\u5668\u4eba\u6267\u884c\u52a8\u4f5c\u548c\u89c6\u91ce\u4e2d\u4efb\u52a1\u76f8\u5173\u7269\u4f53\u7684\u65f6\u7a7a\u77e5\u8bc6\uff0c\u63d0\u51fa\u53ef\u53d8\u5e27\u7387\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u5bf9\u89c6\u9891\u4e0d\u540c\u90e8\u5206\u5e94\u7528\u4e0d\u540c\u5e27\u7387", "result": "\u5728ARMBench\u6570\u636e\u96c6\u4e0aF1\u5206\u6570\u4ece77.9\u63d0\u5347\u523080.0\uff0c\u4f7f\u7528\u6d4b\u8bd5\u65f6\u589e\u5f3a\u540e\u8fdb\u4e00\u6b65\u63d0\u5347\u523081.4\uff0c\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u5f00\u9500", "conclusion": "\u65f6\u7a7a\u4fe1\u606f\u5bf9\u5931\u8d25\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u5efa\u8bae\u672a\u6765\u7814\u7a76\u8fdb\u4e00\u6b65\u63a2\u7d22\u5408\u9002\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5"}}
{"id": "2508.18802", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18802", "abs": "https://arxiv.org/abs/2508.18802", "authors": ["Li Sun", "Jiefeng Wu", "Feng Chen", "Ruizhe Liu", "Yanchao Yang"], "title": "HyperTASR: Hypernetwork-Driven Task-Aware Scene Representations for Robust Manipulation", "comment": null, "summary": "Effective policy learning for robotic manipulation requires scene\nrepresentations that selectively capture task-relevant environmental features.\nCurrent approaches typically employ task-agnostic representation extraction,\nfailing to emulate the dynamic perceptual adaptation observed in human\ncognition. We present HyperTASR, a hypernetwork-driven framework that modulates\nscene representations based on both task objectives and the execution phase.\nOur architecture dynamically generates representation transformation parameters\nconditioned on task specifications and progression state, enabling\nrepresentations to evolve contextually throughout task execution. This approach\nmaintains architectural compatibility with existing policy learning frameworks\nwhile fundamentally reconfiguring how visual features are processed. Unlike\nmethods that simply concatenate or fuse task embeddings with task-agnostic\nrepresentations, HyperTASR establishes computational separation between\ntask-contextual and state-dependent processing paths, enhancing learning\nefficiency and representational quality. Comprehensive evaluations in both\nsimulation and real-world environments demonstrate substantial performance\nimprovements across different representation paradigms. Through ablation\nstudies and attention visualization, we confirm that our approach selectively\nprioritizes task-relevant scene information, closely mirroring human adaptive\nperception during manipulation tasks. The project website is at\n\\href{https://lisunphil.github.io/HyperTASR_projectpage/}{lisunphil.github.io/HyperTASR\\_projectpage}.", "AI": {"tldr": "HyperTASR\u662f\u4e00\u4e2a\u57fa\u4e8e\u8d85\u7f51\u7edc\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u76ee\u6807\u548c\u6267\u884c\u9636\u6bb5\u52a8\u6001\u8c03\u5236\u573a\u666f\u8868\u5f81\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u7b56\u7565\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u8868\u5f81\u63d0\u53d6\u65b9\u6cd5\u901a\u5e38\u662f\u4efb\u52a1\u65e0\u5173\u7684\uff0c\u65e0\u6cd5\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u4e2d\u7684\u52a8\u6001\u611f\u77e5\u9002\u5e94\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6839\u636e\u4efb\u52a1\u4e0a\u4e0b\u6587\u52a8\u6001\u8c03\u6574\u7684\u8868\u5f81\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u8d85\u7f51\u7edc\u67b6\u6784\uff0c\u6839\u636e\u4efb\u52a1\u89c4\u8303\u548c\u8fdb\u5c55\u72b6\u6001\u52a8\u6001\u751f\u6210\u8868\u5f81\u8f6c\u6362\u53c2\u6570\uff0c\u4f7f\u8868\u5f81\u80fd\u591f\u5728\u4efb\u52a1\u6267\u884c\u8fc7\u7a0b\u4e2d\u4e0a\u4e0b\u6587\u6f14\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u73b0\u6709\u7b56\u7565\u5b66\u4e60\u6846\u67b6\u7684\u67b6\u6784\u517c\u5bb9\u6027\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u8868\u5f81\u8303\u5f0f\u4e0b\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u548c\u6ce8\u610f\u529b\u53ef\u89c6\u5316\u8bc1\u5b9e\u4e86\u5176\u9009\u62e9\u6027\u5173\u6ce8\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u7684\u80fd\u529b\u3002", "conclusion": "HyperTASR\u901a\u8fc7\u5efa\u7acb\u4efb\u52a1\u4e0a\u4e0b\u6587\u548c\u72b6\u6001\u4f9d\u8d56\u5904\u7406\u8def\u5f84\u7684\u8ba1\u7b97\u5206\u79bb\uff0c\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u548c\u8868\u5f81\u8d28\u91cf\uff0c\u66f4\u597d\u5730\u6a21\u62df\u4e86\u4eba\u7c7b\u5728\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u81ea\u9002\u5e94\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2508.18817", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18817", "abs": "https://arxiv.org/abs/2508.18817", "authors": ["Colin Merk", "Ismail Geles", "Jiaxu Xing", "Angel Romero", "Giorgia Ramponi", "Davide Scaramuzza"], "title": "Learning Real-World Acrobatic Flight from Human Preferences", "comment": "8 pages, 7 figures", "summary": "Preference-based reinforcement learning (PbRL) enables agents to learn\ncontrol policies without requiring manually designed reward functions, making\nit well-suited for tasks where objectives are difficult to formalize or\ninherently subjective. Acrobatic flight poses a particularly challenging\nproblem due to its complex dynamics, rapid movements, and the importance of\nprecise execution. In this work, we explore the use of PbRL for agile drone\ncontrol, focusing on the execution of dynamic maneuvers such as powerloops.\nBuilding on Preference-based Proximal Policy Optimization (Preference PPO), we\npropose Reward Ensemble under Confidence (REC), an extension to the reward\nlearning objective that improves preference modeling and learning stability.\nOur method achieves 88.4% of the shaped reward performance, compared to 55.2%\nwith standard Preference PPO. We train policies in simulation and successfully\ntransfer them to real-world drones, demonstrating multiple acrobatic maneuvers\nwhere human preferences emphasize stylistic qualities of motion. Furthermore,\nwe demonstrate the applicability of our probabilistic reward model in a\nrepresentative MuJoCo environment for continuous control. Finally, we highlight\nthe limitations of manually designed rewards, observing only 60.7% agreement\nwith human preferences. These results underscore the effectiveness of PbRL in\ncapturing complex, human-centered objectives across both physical and simulated\ndomains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u504f\u597d\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08REC\uff09\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u7279\u6280\u98de\u884c\u52a8\u4f5c\u63a7\u5236\uff0c\u901a\u8fc7\u6539\u8fdb\u5956\u52b1\u5efa\u6a21\u548c\u5b66\u4e60\u7a33\u5b9a\u6027\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u90fd\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002", "motivation": "\u7279\u6280\u98de\u884c\u5177\u6709\u590d\u6742\u7684\u52a8\u529b\u5b66\u7279\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u8bbe\u8ba1\u5408\u9002\u7684\u5956\u52b1\u51fd\u6570\u3002\u57fa\u4e8e\u504f\u597d\u7684\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u4ece\u4eba\u7c7b\u4e3b\u89c2\u504f\u597d\u4e2d\u5b66\u4e60\uff0c\u66f4\u9002\u5408\u6355\u6349\u52a8\u4f5c\u7684\u98ce\u683c\u548c\u8d28\u91cf\u8981\u6c42\u3002", "method": "\u57fa\u4e8ePreference PPO\u63d0\u51faReward Ensemble under Confidence (REC)\u65b9\u6cd5\uff0c\u6539\u8fdb\u5956\u52b1\u5b66\u4e60\u76ee\u6807\uff0c\u63d0\u5347\u504f\u597d\u5efa\u6a21\u548c\u5b66\u4e60\u7a33\u5b9a\u6027\u3002\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3\u7b56\u7565\uff0c\u7136\u540e\u8fc1\u79fb\u5230\u771f\u5b9e\u65e0\u4eba\u673a\u4e0a\u3002", "result": "REC\u65b9\u6cd5\u8fbe\u523088.4%\u7684\u6210\u578b\u5956\u52b1\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u6807\u51c6Preference PPO\u768455.2%\u3002\u6210\u529f\u5b9e\u73b0\u591a\u79cd\u7279\u6280\u98de\u884c\u52a8\u4f5c\uff0c\u4eba\u5de5\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u4e00\u81f4\u6027\u4ec5\u4e3a60.7%\u3002", "conclusion": "\u57fa\u4e8e\u504f\u597d\u7684\u5f3a\u5316\u5b66\u4e60\u80fd\u6709\u6548\u6355\u6349\u590d\u6742\u7684\u4eba\u7c7b\u4e2d\u5fc3\u76ee\u6807\uff0cREC\u65b9\u6cd5\u5728\u7269\u7406\u548c\u6a21\u62df\u9886\u57df\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u96be\u4ee5\u5f62\u5f0f\u5316\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2508.18820", "categories": ["cs.RO", "cs.FL"], "pdf": "https://arxiv.org/pdf/2508.18820", "abs": "https://arxiv.org/abs/2508.18820", "authors": ["Christian Henkel", "Marco Lampacrescia", "Michaela Klauck", "Matteo Morelli"], "title": "AS2FM: Enabling Statistical Model Checking of ROS 2 Systems for Robust Autonomy", "comment": "Accepted at IROS2025", "summary": "Designing robotic systems to act autonomously in unforeseen environments is a\nchallenging task. This work presents a novel approach to use formal\nverification, specifically Statistical Model Checking (SMC), to verify system\nproperties of autonomous robots at design-time. We introduce an extension of\nthe SCXML format, designed to model system components including both Robot\nOperating System 2 (ROS 2) and Behavior Tree (BT) features. Further, we\ncontribute Autonomous Systems to Formal Models (AS2FM), a tool to translate the\nfull system model into JANI. The use of JANI, a standard format for\nquantitative model checking, enables verification of system properties with\noff-the-shelf SMC tools. We demonstrate the practical usability of AS2FM both\nin terms of applicability to real-world autonomous robotic control systems, and\nin terms of verification runtime scaling. We provide a case study, where we\nsuccessfully identify problems in a ROS 2-based robotic manipulation use case\nthat is verifiable in less than one second using consumer hardware.\nAdditionally, we compare to the state of the art and demonstrate that our\nmethod is more comprehensive in system feature support, and that the\nverification runtime scales linearly with the size of the model, instead of\nexponentially.", "AI": {"tldr": "\u63d0\u51faAS2FM\u5de5\u5177\uff0c\u5c06\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u6a21\u578b\u8f6c\u6362\u4e3aJANI\u683c\u5f0f\uff0c\u4f7f\u7528\u7edf\u8ba1\u6a21\u578b\u68c0\u9a8c(SMC)\u5728\u8bbe\u8ba1\u65f6\u9a8c\u8bc1\u7cfb\u7edf\u5c5e\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u652f\u6301\u66f4\u5168\u9762\u7684\u7cfb\u7edf\u7279\u6027\u4e14\u9a8c\u8bc1\u65f6\u95f4\u7ebf\u6027\u589e\u957f\u3002", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u8fd0\u884c\u65f6\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u8bbe\u8ba1\u65f6\u9a8c\u8bc1\u7cfb\u7edf\u5c5e\u6027\u4ee5\u786e\u4fdd\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u6269\u5c55SCXML\u683c\u5f0f\u5efa\u6a21ROS 2\u548c\u884c\u4e3a\u6811\u7ec4\u4ef6\uff0c\u5f00\u53d1AS2FM\u5de5\u5177\u5c06\u7cfb\u7edf\u6a21\u578b\u8f6c\u6362\u4e3a\u6807\u51c6JANI\u683c\u5f0f\uff0c\u5229\u7528\u73b0\u6210SMC\u5de5\u5177\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6210\u529f\u8bc6\u522bROS 2\u673a\u68b0\u81c2\u7528\u4f8b\u4e2d\u7684\u95ee\u9898\uff0c\u9a8c\u8bc1\u65f6\u95f4\u5c11\u4e8e1\u79d2\uff0c\u9a8c\u8bc1\u8fd0\u884c\u65f6\u95f4\u968f\u6a21\u578b\u5927\u5c0f\u7ebf\u6027\u589e\u957f\u800c\u975e\u6307\u6570\u589e\u957f\u3002", "conclusion": "AS2FM\u5de5\u5177\u5b9e\u7528\u6027\u5f3a\uff0c\u80fd\u591f\u6709\u6548\u9a8c\u8bc1\u771f\u5b9e\u81ea\u4e3b\u673a\u5668\u4eba\u63a7\u5236\u7cfb\u7edf\uff0c\u9a8c\u8bc1\u6548\u7387\u9ad8\u4e14\u53ef\u6269\u5c55\u6027\u597d\u3002"}}
{"id": "2508.18937", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18937", "abs": "https://arxiv.org/abs/2508.18937", "authors": ["Wang Jiayin", "Wei Yanran", "Jiang Lei", "Guo Xiaoyu", "Zheng Ayong", "Zhao Weidong", "Li Zhongkui"], "title": "VisionSafeEnhanced VPC: Cautious Predictive Control with Visibility Constraints under Uncertainty for Autonomous Robotic Surgery", "comment": "8 pages, 6 figures", "summary": "Autonomous control of the laparoscope in robot-assisted Minimally Invasive\nSurgery (MIS) has received considerable research interest due to its potential\nto improve surgical safety. Despite progress in pixel-level Image-Based Visual\nServoing (IBVS) control, the requirement of continuous visibility and the\nexistence of complex disturbances, such as parameterization error, measurement\nnoise, and uncertainties of payloads, could degrade the surgeon's visual\nexperience and compromise procedural safety. To address these limitations, this\npaper proposes VisionSafeEnhanced Visual Predictive Control (VPC), a robust and\nuncertainty-adaptive framework for autonomous laparoscope control that\nguarantees Field of View (FoV) safety under uncertainty. Firstly, Gaussian\nProcess Regression (GPR) is utilized to perform hybrid (deterministic +\nstochastic) quantification of operational uncertainties including residual\nmodel uncertainties, stochastic uncertainties, and external disturbances. Based\non uncertainty quantification, a novel safety aware trajectory optimization\nframework with probabilistic guarantees is proposed, where a\nuncertainty-adaptive safety Control Barrier Function (CBF) condition is given\nbased on uncertainty propagation, and chance constraints are simultaneously\nformulated based on probabilistic approximation. This uncertainty aware\nformulation enables adaptive control effort allocation, minimizing unnecessary\ncamera motion while maintaining robustness. The proposed method is validated\nthrough comparative simulations and experiments on a commercial surgical robot\nplatform (MicroPort MedBot Toumai) performing a sequential multi-target lymph\nnode dissection. Compared with baseline methods, the framework maintains\nnear-perfect target visibility (>99.9%), reduces tracking e", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86VisionSafeEnhanced VPC\u6846\u67b6\uff0c\u901a\u8fc7\u6838\u7b97\u5b50\u56de\u5f52\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5e76\u7ed3\u5408\u63a7\u5236\u969c\u788d\u51fd\u6570\u6765\u5b9e\u73b0\u81ea\u4e3b\u817f\u955c\u63a7\u5236\u7684\u7a33\u5065\u6027\u548c\u89c6\u91ce\u5b89\u5168\u4fdd\u969c\u3002", "motivation": "\u89e3\u51b3\u76ee\u6807\u53ef\u89c1\u6027\u4e0d\u8fde\u7eed\u3001\u53c2\u6570\u5316\u9519\u8bef\u3001\u6d4b\u91cf\u566a\u58f0\u7b49\u590d\u6742\u5e72\u6270\u5bf9\u624b\u672f\u89c6\u89c9\u4f53\u9a8c\u548c\u5b89\u5160\u7684\u5f71\u54cd\u3002", "method": "\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52(GPR)\u8fdb\u884c\u6df7\u5408\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u6982\u7387\u4fdd\u969c\u7684\u5b89\u5168\u611f\u77e5\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u5305\u62ec\u4e0d\u786e\u5b9a\u6027\u9002\u5e94\u6027\u5b89\u5168\u63a7\u5236\u969c\u788d\u51fd\u6570\u6761\u4ef6\u548c\u673a\u4f1a\u7ea6\u675f\u3002", "result": "\u5728\u5546\u7528\u624b\u672f\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8fdb\u884c\u7684\u6a21\u62df\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0c\u8be5\u6846\u67b6\u80fd\u7ef4\u6301\u8fd1\u4e8e\u5b8c\u7f8e\u7684\u76ee\u6807\u53ef\u89c1\u6027(>99.9%)\uff0c\u5e76\u51cf\u5c11\u8ddf\u8e2a\u9519\u8bef\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u4fdd\u969c\u624b\u672f\u89c6\u91ce\u5b89\u5168\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u63a7\u5236\u52b3\u52a8\u5206\u914d\uff0c\u5728\u4fdd\u6301\u7a33\u5065\u6027\u7684\u540c\u65f6\u6700\u5c0f\u5316\u6444\u50cf\u5934\u8fd0\u52a8\u3002"}}
{"id": "2508.18967", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18967", "abs": "https://arxiv.org/abs/2508.18967", "authors": ["Hichem Cheriet", "Khellat Kihel Badra", "Chouraqui Samira"], "title": "Enhanced UAV Path Planning Using the Tangent Intersection Guidance (TIG) Algorithm", "comment": "Accepted for publication in JAMRIS Journal", "summary": "Efficient and safe navigation of Unmanned Aerial Vehicles (UAVs) is critical\nfor various applications, including combat support, package delivery and Search\nand Rescue Operations. This paper introduces the Tangent Intersection Guidance\n(TIG) algorithm, an advanced approach for UAV path planning in both static and\ndynamic environments. The algorithm uses the elliptic tangent intersection\nmethod to generate feasible paths. It generates two sub-paths for each threat,\nselects the optimal route based on a heuristic rule, and iteratively refines\nthe path until the target is reached. Considering the UAV kinematic and dynamic\nconstraints, a modified smoothing technique based on quadratic B\\'ezier curves\nis adopted to generate a smooth and efficient route. Experimental results show\nthat the TIG algorithm can generate the shortest path in less time, starting\nfrom 0.01 seconds, with fewer turning angles compared to A*, PRM, RRT*, Tangent\nGraph, and Static APPATT algorithms in static environments. Furthermore, in\ncompletely unknown and partially known environments, TIG demonstrates efficient\nreal-time path planning capabilities for collision avoidance, outperforming APF\nand Dynamic APPATT algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u692d\u5706\u5207\u7ebf\u4ea4\u70b9\u7684\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u7b97\u6cd5TIG\uff0c\u80fd\u591f\u5728\u9759\u6001\u548c\u52a8\u6001\u73af\u5883\u4e2d\u5feb\u901f\u751f\u6210\u5e73\u6ed1\u7684\u6700\u77ed\u8def\u5f84\uff0c\u76f8\u6bd4\u73b0\u6709\u7b97\u6cd5\u5177\u6709\u66f4\u5feb\u7684\u8ba1\u7b97\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u4f5c\u6218\u652f\u6301\u3001\u5305\u88f9\u6295\u9012\u548c\u641c\u6551\u884c\u52a8\u7b49\u5e94\u7528\u4e2d\u9700\u8981\u9ad8\u6548\u5b89\u5168\u7684\u5bfc\u822a\uff0c\u73b0\u6709\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u5728\u5b9e\u65f6\u6027\u548c\u8def\u5f84\u8d28\u91cf\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u3002", "method": "\u91c7\u7528\u692d\u5706\u5207\u7ebf\u4ea4\u70b9\u65b9\u6cd5\u751f\u6210\u53ef\u884c\u8def\u5f84\uff0c\u4e3a\u6bcf\u4e2a\u5a01\u80c1\u751f\u6210\u4e24\u6761\u5b50\u8def\u5f84\uff0c\u57fa\u4e8e\u542f\u53d1\u5f0f\u89c4\u5219\u9009\u62e9\u6700\u4f18\u8def\u7ebf\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u76f4\u81f3\u5230\u8fbe\u76ee\u6807\u3002\u7ed3\u5408\u65e0\u4eba\u673a\u8fd0\u52a8\u5b66\u7ea6\u675f\uff0c\u4f7f\u7528\u57fa\u4e8e\u4e8c\u6b21\u8d1d\u585e\u5c14\u66f2\u7ebf\u7684\u6539\u8fdb\u5e73\u6ed1\u6280\u672f\u751f\u6210\u5e73\u6ed1\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTIG\u7b97\u6cd5\u5728\u9759\u6001\u73af\u5883\u4e2d\u76f8\u6bd4A*\u3001PRM\u3001RRT*\u7b49\u7b97\u6cd5\u80fd\u4ee5\u66f4\u77ed\u65f6\u95f4\uff08\u4ece0.01\u79d2\u5f00\u59cb\uff09\u751f\u6210\u6700\u77ed\u8def\u5f84\uff0c\u4e14\u8f6c\u5f2f\u89d2\u5ea6\u66f4\u5c11\u3002\u5728\u672a\u77e5\u548c\u90e8\u5206\u5df2\u77e5\u73af\u5883\u4e2d\uff0cTIG\u8868\u73b0\u51fa\u9ad8\u6548\u7684\u5b9e\u65f6\u907f\u969c\u80fd\u529b\uff0c\u4f18\u4e8eAPF\u548cDynamic APPATT\u7b97\u6cd5\u3002", "conclusion": "TIG\u7b97\u6cd5\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u5728\u9759\u6001\u548c\u52a8\u6001\u73af\u5883\u4e2d\u90fd\u80fd\u5feb\u901f\u751f\u6210\u5e73\u6ed1\u7684\u6700\u4f18\u8def\u5f84\uff0c\u5177\u6709\u5f88\u597d\u7684\u5b9e\u65f6\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2508.19002", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19002", "abs": "https://arxiv.org/abs/2508.19002", "authors": ["Shipeng Lyu", "Fangyuan Wang", "Weiwei Lin", "Luhao Zhu", "David Navarro-Alarcon", "Guodong Guo"], "title": "HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots", "comment": "8 pages, 8 figures,4 tables", "summary": "Achieving both behavioral similarity and appropriateness in human-like motion\ngeneration for humanoid robot remains an open challenge, further compounded by\nthe lack of cross-embodiment adaptability. To address this problem, we propose\nHuBE, a bi-level closed-loop framework that integrates robot state, goal poses,\nand contextual situations to generate human-like behaviors, ensuring both\nbehavioral similarity and appropriateness, and eliminating structural\nmismatches between motion generation and execution. To support this framework,\nwe construct HPose, a context-enriched dataset featuring fine-grained\nsituational annotations. Furthermore, we introduce a bone scaling-based data\naugmentation strategy that ensures millimeter-level compatibility across\nheterogeneous humanoid robots. Comprehensive evaluations on multiple commercial\nplatforms demonstrate that HuBE significantly improves motion similarity,\nbehavioral appropriateness, and computational efficiency over state-of-the-art\nbaselines, establishing a solid foundation for transferable and human-like\nbehavior execution across diverse humanoid robots.", "AI": {"tldr": "HuBE\u662f\u4e00\u4e2a\u53cc\u5c42\u95ed\u73af\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u673a\u5668\u4eba\u72b6\u6001\u3001\u76ee\u6807\u59ff\u6001\u548c\u60c5\u5883\u4e0a\u4e0b\u6587\u6765\u751f\u6210\u7c7b\u4eba\u884c\u4e3a\uff0c\u89e3\u51b3\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u4e2d\u884c\u4e3a\u76f8\u4f3c\u6027\u548c\u9002\u5f53\u6027\u7684\u6311\u6218\uff0c\u5e76\u5b9e\u73b0\u4e86\u8de8\u5f02\u6784\u673a\u5668\u4eba\u7684\u6beb\u7c73\u7ea7\u517c\u5bb9\u6027\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u4e2d\u540c\u65f6\u5b9e\u73b0\u884c\u4e3a\u76f8\u4f3c\u6027\u548c\u9002\u5f53\u6027\u7684\u5f00\u653e\u6311\u6218\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u8de8\u5177\u8eab\u9002\u5e94\u6027\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faHuBE\u53cc\u5c42\u95ed\u73af\u6846\u67b6\uff0c\u6574\u5408\u673a\u5668\u4eba\u72b6\u6001\u3001\u76ee\u6807\u59ff\u6001\u548c\u60c5\u5883\u4e0a\u4e0b\u6587\uff1b\u6784\u5efaHPose\u60c5\u5883\u4e30\u5bcc\u6570\u636e\u96c6\uff1b\u5f15\u5165\u57fa\u4e8e\u9aa8\u9abc\u7f29\u653e\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u786e\u4fdd\u8de8\u5f02\u6784\u673a\u5668\u4eba\u7684\u6beb\u7c73\u7ea7\u517c\u5bb9\u6027\u3002", "result": "\u5728\u591a\u4e2a\u5546\u4e1a\u5e73\u53f0\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cHuBE\u5728\u8fd0\u52a8\u76f8\u4f3c\u6027\u3001\u884c\u4e3a\u9002\u5f53\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HuBE\u4e3a\u8de8\u591a\u6837\u5316\u4eba\u5f62\u673a\u5668\u4eba\u7684\u53ef\u8fc1\u79fb\u548c\u7c7b\u4eba\u884c\u4e3a\u6267\u884c\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2508.19074", "categories": ["cs.RO", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.19074", "abs": "https://arxiv.org/abs/2508.19074", "authors": ["ZhenDong Chen", "ZhanShang Nie", "ShiXing Wan", "JunYi Li", "YongTian Cheng", "Shuai Zhao"], "title": "An LLM-powered Natural-to-Robotic Language Translation Framework with Correctness Guarantees", "comment": null, "summary": "The Large Language Models (LLM) are increasingly being deployed in robotics\nto generate robot control programs for specific user tasks, enabling embodied\nintelligence. Existing methods primarily focus on LLM training and prompt\ndesign that utilize LLMs to generate executable programs directly from user\ntasks in natural language. However, due to the inconsistency of the LLMs and\nthe high complexity of the tasks, such best-effort approaches often lead to\ntremendous programming errors in the generated code, which significantly\nundermines the effectiveness especially when the light-weight LLMs are applied.\nThis paper introduces a natural-robotic language translation framework that (i)\nprovides correctness verification for generated control programs and (ii)\nenhances the performance of LLMs in program generation via feedback-based\nfine-tuning for the programs. To achieve this, a Robot Skill Language (RSL) is\nproposed to abstract away from the intricate details of the control programs,\nbridging the natural language tasks with the underlying robot skills. Then, the\nRSL compiler and debugger are constructed to verify RSL programs generated by\nthe LLM and provide error feedback to the LLM for refining the outputs until\nbeing verified by the compiler. This provides correctness guarantees for the\nLLM-generated programs before being offloaded to the robots for execution,\nsignificantly enhancing the effectiveness of LLM-powered robotic applications.\nExperiments demonstrate NRTrans outperforms the existing method under a range\nof LLMs and tasks, and achieves a high success rate for light-weight LLMs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u7136-\u673a\u5668\u4eba\u8bed\u8a00\u7ffb\u8bd1\u6846\u67b6NRTrans\uff0c\u901a\u8fc7Robot Skill Language (RSL)\u548c\u7f16\u8bd1\u5668\u9a8c\u8bc1\u673a\u5236\uff0c\u89e3\u51b3\u4e86LLM\u751f\u6210\u673a\u5668\u4eba\u63a7\u5236\u7a0b\u5e8f\u65f6\u7684\u9519\u8bef\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7a0b\u5e8f\u751f\u6210\u7684\u6b63\u786e\u6027\u548c\u8f7b\u91cf\u7ea7LLM\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528LLM\u76f4\u63a5\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u53ef\u6267\u884c\u7a0b\u5e8f\uff0c\u4f46\u7531\u4e8eLLM\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u4efb\u52a1\u7684\u590d\u6742\u6027\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u4ee3\u7801\u51fa\u73b0\u5927\u91cf\u9519\u8bef\uff0c\u5c24\u5176\u662f\u5728\u4f7f\u7528\u8f7b\u91cf\u7ea7LLM\u65f6\u6548\u679c\u66f4\u5dee\u3002", "method": "\u63d0\u51faRobot Skill Language (RSL)\u6765\u62bd\u8c61\u63a7\u5236\u7a0b\u5e8f\u7684\u590d\u6742\u7ec6\u8282\uff0c\u6784\u5efaRSL\u7f16\u8bd1\u5668\u548c\u8c03\u8bd5\u5668\u9a8c\u8bc1LLM\u751f\u6210\u7684RSL\u7a0b\u5e8f\uff0c\u5e76\u901a\u8fc7\u9519\u8bef\u53cd\u9988\u7ec6\u8c03LLM\u8f93\u51fa\uff0c\u76f4\u5230\u7a0b\u5e8f\u901a\u8fc7\u7f16\u8bd1\u5668\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aNRTrans\u5728\u591a\u79cdLLM\u548c\u4efb\u52a1\u4e0b\u90fd\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e3a\u8f7b\u91cf\u7ea7LLM\u5b9e\u73b0\u4e86\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u751f\u6210\u7684\u673a\u5668\u4eba\u63a7\u5236\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u6b63\u786e\u6027\u4fdd\u8bc1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LLM\u9a71\u52a8\u673a\u5668\u4eba\u5e94\u7528\u7684\u6548\u679c\u3002"}}
{"id": "2508.19114", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.19114", "abs": "https://arxiv.org/abs/2508.19114", "authors": ["Alkesh K. Srivastava", "Jared Michael Levin", "Alexander Derrico", "Philip Dames"], "title": "DELIVER: A System for LLM-Guided Coordinated Multi-Robot Pickup and Delivery using Voronoi-Based Relay Planning", "comment": "Submission under review at the 2026 IEEE/SICE International Symposium\n  on System Integration (SII 2026)", "summary": "We present DELIVER (Directed Execution of Language-instructed Item Via\nEngineered Relay), a fully integrated framework for cooperative multi-robot\npickup and delivery driven by natural language commands. DELIVER unifies\nnatural language understanding, spatial decomposition, relay planning, and\nmotion execution to enable scalable, collision-free coordination in real-world\nsettings. Given a spoken or written instruction, a lightweight instance of\nLLaMA3 interprets the command to extract pickup and delivery locations. The\nenvironment is partitioned using a Voronoi tessellation to define\nrobot-specific operating regions. Robots then compute optimal relay points\nalong shared boundaries and coordinate handoffs. A finite-state machine governs\neach robot's behavior, enabling robust execution. We implement DELIVER on the\nMultiTRAIL simulation platform and validate it in both ROS2-based Gazebo\nsimulations and real-world hardware using TurtleBot3 robots. Empirical results\nshow that DELIVER maintains consistent mission cost across varying team sizes\nwhile reducing per-agent workload by up to 55% compared to a single-agent\nsystem. Moreover, the number of active relay agents remains low even as team\nsize increases, demonstrating the system's scalability and efficient agent\nutilization. These findings underscore DELIVER's modular and extensible\narchitecture for language-guided multi-robot coordination, advancing the\nfrontiers of cyber-physical system integration.", "AI": {"tldr": "DELIVER\u662f\u4e00\u4e2a\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u591a\u673a\u5668\u4eba\u534f\u4f5c\u62fe\u53d6\u914d\u9001\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u8a00\u7406\u89e3\u3001\u7a7a\u95f4\u5206\u89e3\u3001\u4e2d\u7ee7\u89c4\u5212\u548c\u8fd0\u52a8\u6267\u884c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u65e0\u78b0\u649e\u534f\u8c03\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u9a71\u52a8\u7684\u534f\u4f5c\u62fe\u53d6\u914d\u9001\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u96c6\u6210\u6846\u67b6\u6765\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u65e0\u78b0\u649e\u7684\u5b9e\u65f6\u534f\u8c03\u3002", "method": "\u4f7f\u7528LLaMA3\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u63d0\u53d6\u4f4d\u7f6e\u4fe1\u606f\uff0cVoronoi\u7a7a\u95f4\u5206\u89e3\u5b9a\u4e49\u673a\u5668\u4eba\u64cd\u4f5c\u533a\u57df\uff0c\u8ba1\u7b97\u6700\u4f18\u4e2d\u7ee7\u70b9\u534f\u8c03\u4ea4\u63a5\uff0c\u6709\u9650\u72b6\u6001\u673a\u63a7\u5236\u673a\u5668\u4eba\u884c\u4e3a\u3002", "result": "\u5728MultiTRAIL\u5e73\u53f0\u548c\u771f\u5b9eTurtleBot3\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u5355\u673a\u5668\u4eba\u7cfb\u7edf\u964d\u4f4e55%\u7684\u5355\u4f53\u5de5\u4f5c\u91cf\uff0c\u56e2\u961f\u89c4\u6a21\u589e\u5927\u65f6\u4e2d\u7ee7\u4ee3\u7406\u6570\u91cf\u4fdd\u6301\u4f4e\u4f4d\uff0c\u5c55\u73b0\u826f\u597d\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "DELIVER\u7684\u6a21\u5757\u5316\u53ef\u6269\u5c55\u67b6\u6784\u63a8\u52a8\u4e86\u8bed\u8a00\u5f15\u5bfc\u7684\u591a\u673a\u5668\u4eba\u534f\u8c03\u6280\u672f\u53d1\u5c55\uff0c\u4e3a\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u96c6\u6210\u63d0\u4f9b\u4e86\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19131", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19131", "abs": "https://arxiv.org/abs/2508.19131", "authors": ["Shreya Gummadi", "Mateus V. Gasparino", "Gianluca Capezzuto", "Marcelo Becker", "Girish Chowdhary"], "title": "ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown Environments", "comment": null, "summary": "The advancement of robotics and autonomous navigation systems hinges on the\nability to accurately predict terrain traversability. Traditional methods for\ngenerating datasets to train these prediction models often involve putting\nrobots into potentially hazardous environments, posing risks to equipment and\nsafety. To solve this problem, we present ZeST, a novel approach leveraging\nvisual reasoning capabilities of Large Language Models (LLMs) to create a\ntraversability map in real-time without exposing robots to danger. Our approach\nnot only performs zero-shot traversability and mitigates the risks associated\nwith real-world data collection but also accelerates the development of\nadvanced navigation systems, offering a cost-effective and scalable solution.\nTo support our findings, we present navigation results, in both controlled\nindoor and unstructured outdoor environments. As shown in the experiments, our\nmethod provides safer navigation when compared to other state-of-the-art\nmethods, constantly reaching the final goal.", "AI": {"tldr": "ZeST\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u5730\u5f62\u53ef\u901a\u884c\u6027\u9884\u6d4b\uff0c\u65e0\u9700\u8ba9\u673a\u5668\u4eba\u8fdb\u5165\u5371\u9669\u73af\u5883\u5373\u53ef\u751f\u6210\u5b9e\u65f6\u53ef\u901a\u884c\u6027\u5730\u56fe\u3002", "motivation": "\u4f20\u7edf\u7684\u5730\u5f62\u53ef\u901a\u884c\u6027\u9884\u6d4b\u65b9\u6cd5\u9700\u8981\u5c06\u673a\u5668\u4eba\u7f6e\u4e8e\u5371\u9669\u73af\u5883\u4e2d\u6536\u96c6\u6570\u636e\uff0c\u5b58\u5728\u8bbe\u5907\u635f\u574f\u548c\u5b89\u5168\u98ce\u9669\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5b89\u5168\u3001\u6210\u672c\u6548\u76ca\u66f4\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u5b66\u4e60\u65b9\u5f0f\u5b9e\u65f6\u751f\u6210\u5730\u5f62\u53ef\u901a\u884c\u6027\u5730\u56fe\uff0c\u907f\u514d\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u98ce\u9669\u6570\u636e\u6536\u96c6\u3002", "result": "\u5728\u53d7\u63a7\u5ba4\u5185\u548c\u975e\u7ed3\u6784\u5316\u5ba4\u5916\u73af\u5883\u4e2d\u8fdb\u884c\u5bfc\u822a\u6d4b\u8bd5\uff0c\u76f8\u6bd4\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u4f9b\u66f4\u5b89\u5168\u7684\u5bfc\u822a\uff0c\u80fd\u591f\u6301\u7eed\u5230\u8fbe\u6700\u7ec8\u76ee\u6807\u3002", "conclusion": "ZeST\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u5bfc\u822a\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u52a0\u901f\u4e86\u5148\u8fdb\u5bfc\u822a\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.19150", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19150", "abs": "https://arxiv.org/abs/2508.19150", "authors": ["Juan Carlos Sabor\u00edo", "Marc Vinci", "Oscar Lima", "Sebastian Stock", "Lennart Niecksch", "Martin G\u00fcnther", "Alexander Sung", "Joachim Hertzberg", "Martin Atzm\u00fcller"], "title": "Uncertainty-Resilient Active Intention Recognition for Robotic Assistants", "comment": "(To appear) In Proceedings of ECMR 2025", "summary": "Purposeful behavior in robotic assistants requires the integration of\nmultiple components and technological advances. Often, the problem is reduced\nto recognizing explicit prompts, which limits autonomy, or is oversimplified\nthrough assumptions such as near-perfect information. We argue that a critical\ngap remains unaddressed -- specifically, the challenge of reasoning about the\nuncertain outcomes and perception errors inherent to human intention\nrecognition. In response, we present a framework designed to be resilient to\nuncertainty and sensor noise, integrating real-time sensor data with a\ncombination of planners. Centered around an intention-recognition POMDP, our\napproach addresses cooperative planning and acting under uncertainty. Our\nintegrated framework has been successfully tested on a physical robot with\npromising results.", "AI": {"tldr": "\u57fa\u4e8ePOMDP\u7684\u610f\u56fe\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u89c4\u5212\u5668\u548c\u5b9e\u65f6\u4f20\u611f\u5668\u6570\u636e\uff0c\u89e3\u51b3\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u611f\u77e5\u9519\u8bef\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u81ea\u4e3b\u6027\u7684\u673a\u5668\u4eba\u52a9\u624b\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u52a9\u624b\u7cfb\u7edf\u591a\u4ec5\u9650\u4e8e\u8bc6\u522b\u663e\u5f0f\u63d0\u793a\uff0c\u6216\u4f5c\u51fa\u8fc7\u4e8e\u7b80\u5316\u7684\u5047\u8bbe\uff08\u5982\u8fd1\u5b8c\u7f8e\u4fe1\u606f\uff09\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u4eba\u7c7b\u610f\u56fe\u8bc6\u522b\u4e2d\u5e38\u89c1\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u611f\u77e5\u9519\u8bef\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u610f\u56fe\u8bc6\u522bPOMDP\u7684\u6846\u67b6\uff0c\u6574\u5408\u5b9e\u65f6\u4f20\u611f\u5668\u6570\u636e\u548c\u591a\u79cd\u89c4\u5212\u5668\uff0c\u4ee5\u5e94\u5bf9\u4e0d\u786e\u5b9a\u6027\u548c\u4f20\u611f\u5668\u566a\u58f0\u3002\u8be5\u65b9\u6cd5\u91cd\u70b9\u89e3\u51b3\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u7684\u534f\u4f5c\u89c4\u5212\u548c\u884c\u52a8\u95ee\u9898\u3002", "result": "\u8be5\u96c6\u6210\u6846\u67b6\u5df2\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u6210\u529f\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5e76\u53d6\u5f97\u4e86\u6709\u524d\u666f\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u548c\u6548\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u4eba\u673a\u534f\u4f5c\u4e2d\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u611f\u77e5\u9519\u8bef\u7684\u5173\u952e\u7a7a\u767d\uff0c\u901a\u8fc7POMDP\u6846\u67b6\u5b9e\u73b0\u4e86\u66f4\u81ea\u4e3b\u3001\u66f4\u5065\u58ee\u7684\u673a\u5668\u4eba\u52a9\u624b\u884c\u4e3a\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u673a\u5668\u4eba\u534f\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19153", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19153", "abs": "https://arxiv.org/abs/2508.19153", "authors": ["Allen Wang", "Gavin Tao"], "title": "QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning", "comment": "14pages, 9 figures, Journal paper", "summary": "We address vision-guided quadruped motion control with reinforcement learning\n(RL) and highlight the necessity of combining proprioception with vision for\nrobust control. We propose QuadKAN, a spline-parameterized cross-modal policy\ninstantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates\na spline encoder for proprioception and a spline fusion head for\nproprioception-vision inputs. This structured function class aligns the\nstate-to-action mapping with the piecewise-smooth nature of gait, improving\nsample efficiency, reducing action jitter and energy consumption, and providing\ninterpretable posture-action sensitivities. We adopt Multi-Modal Delay\nRandomization (MMDR) and perform end-to-end training with Proximal Policy\nOptimization (PPO). Evaluations across diverse terrains, including both even\nand uneven surfaces and scenarios with static or dynamic obstacles, demonstrate\nthat QuadKAN achieves consistently higher returns, greater distances, and fewer\ncollisions than state-of-the-art (SOTA) baselines. These results show that\nspline-parameterized policies offer a simple, effective, and interpretable\nalternative for robust vision-guided locomotion. A repository will be made\navailable upon acceptance.", "AI": {"tldr": "\u63d0\u51faQuadKAN\u6846\u67b6\uff0c\u4f7f\u7528\u6837\u6761\u53c2\u6570\u5316\u7684KAN\u7f51\u7edc\u7ed3\u5408\u672c\u4f53\u611f\u89c9\u548c\u89c6\u89c9\u8f93\u5165\uff0c\u5b9e\u73b0\u56db\u8db3\u673a\u5668\u4eba\u9c81\u68d2\u8fd0\u52a8\u63a7\u5236\uff0c\u5728\u591a\u79cd\u5730\u5f62\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u5f15\u5bfc\u56db\u8db3\u8fd0\u52a8\u63a7\u5236\u95ee\u9898\uff0c\u5f3a\u8c03\u7ed3\u5408\u672c\u4f53\u611f\u89c9\u548c\u89c6\u89c9\u5bf9\u4e8e\u9c81\u68d2\u63a7\u5236\u7684\u5fc5\u8981\u6027\u3002", "method": "\u63d0\u51faQuadKAN\u6846\u67b6\uff0c\u4f7f\u7528\u6837\u6761\u7f16\u7801\u5668\u5904\u7406\u672c\u4f53\u611f\u89c9\uff0c\u6837\u6761\u878d\u5408\u5934\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\uff0c\u91c7\u7528MMDR\u548cPPO\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u79cd\u5730\u5f62\u6d4b\u8bd5\u4e2d\uff0cQuadKAN\u83b7\u5f97\u66f4\u9ad8\u56de\u62a5\u3001\u66f4\u8fdc\u8ddd\u79bb\u548c\u66f4\u5c11\u78b0\u649e\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6837\u6761\u53c2\u6570\u5316\u7b56\u7565\u4e3a\u9c81\u68d2\u89c6\u89c9\u5f15\u5bfc\u8fd0\u52a8\u63d0\u4f9b\u4e86\u7b80\u5355\u3001\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2508.19164", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.19164", "abs": "https://arxiv.org/abs/2508.19164", "authors": ["Morokot Sakal", "George Nehma", "Camilo Riano-Rios", "Madhur Tiwari"], "title": "Real-time Testing of Satellite Attitude Control With a Reaction Wheel Hardware-In-the-Loop Platform", "comment": "15 pages, 10 figures, 2025 AAS/AIAA Astrodynamics Specialist\n  Conference", "summary": "We propose the Hardware-in-the-Loop (HIL) test of an adaptive satellite\nattitude control system with reaction wheel health estimation capabilities.\nPrevious simulations and Software-in-the-Loop testing have prompted further\nexperiments to explore the validity of the controller with real momentum\nexchange devices in the loop. This work is a step toward a comprehensive\ntesting framework for validation of spacecraft attitude control algorithms. The\nproposed HIL testbed includes brushless DC motors and drivers that communicate\nusing a CAN bus, an embedded computer that executes control and adaptation\nlaws, and a satellite simulator that produces simulated sensor data, estimated\nattitude states, and responds to actions of the external actuators. We propose\nmethods to artificially induce failures on the reaction wheels, and present\nrelated issues and lessons learned.", "AI": {"tldr": "\u786c\u4ef6\u5728\u73af\u6d4b\u8bd5\u901a\u8fc7\u5b9e\u9645\u7535\u673a\u548cCAN\u603b\u7ebf\u9a8c\u8bc1\u536b\u661f\u59ff\u6001\u63a7\u5236\u7cfb\u7edf\u7684\u53cd\u5e94\u8f6e\u5065\u5eb7\u4f30\u8ba1\u80fd\u529b", "motivation": "\u4e4b\u524d\u7684\u4eff\u771f\u548c\u8f6f\u4ef6\u5728\u73af\u6d4b\u8bd5\u63a8\u52a8\u4e86\u9700\u8981\u5728\u5faa\u73af\u4e2d\u5305\u542b\u5b9e\u9645\u52a8\u91cf\u4ea4\u6362\u8bbe\u5907\u6765\u9a8c\u8bc1\u63a7\u5236\u5668\u6709\u6548\u6027\u7684\u8fdb\u4e00\u6b65\u5b9e\u9a8c", "method": "\u6784\u5efaHIL\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5305\u542b\u65e0\u5237\u76f4\u6d41\u7535\u673a\u3001CAN\u603b\u7ebf\u901a\u4fe1\u3001\u5d4c\u5165\u5f0f\u8ba1\u7b97\u673a\u6267\u884c\u63a7\u5236\u7b97\u6cd5\uff0c\u4ee5\u53ca\u536b\u661f\u4eff\u771f\u5668\u751f\u6210\u4f20\u611f\u5668\u6570\u636e\u548c\u54cd\u5e94\u5916\u90e8\u6267\u884c\u5668\u52a8\u4f5c", "result": "\u63d0\u51fa\u4e86\u4eba\u5de5\u5f15\u53d1\u53cd\u5e94\u8f6e\u6545\u969c\u7684\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u76f8\u5173\u95ee\u9898\u548c\u7ecf\u9a8c\u6559\u8bad", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u662f\u5f80\u5b8c\u5584\u592a\u7a7a\u8f7d\u5177\u59ff\u6001\u63a7\u5236\u7b97\u6cd5\u9a8c\u8bc1\u6846\u67b6\u8fdb\u884c\u7684\u4e00\u6b65"}}
{"id": "2508.19168", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19168", "abs": "https://arxiv.org/abs/2508.19168", "authors": ["Liding Zhang", "Kejia Chen", "Kuanqi Cai", "Yu Zhang", "Yixuan Dang", "Yansong Wu", "Zhenshan Bing", "Fan Wu", "Sami Haddadin", "Alois Knoll"], "title": "Direction Informed Trees (DIT*): Optimal Path Planning via Direction Filter and Direction Cost Heuristic", "comment": "7 pages, 5 figures, 2025 IEEE International Conference on Robotics\n  and Automation (ICRA)", "summary": "Optimal path planning requires finding a series of feasible states from the\nstarting point to the goal to optimize objectives. Popular path planning\nalgorithms, such as Effort Informed Trees (EIT*), employ effort heuristics to\nguide the search. Effective heuristics are accurate and computationally\nefficient, but achieving both can be challenging due to their conflicting\nnature. This paper proposes Direction Informed Trees (DIT*), a sampling-based\nplanner that focuses on optimizing the search direction for each edge,\nresulting in goal bias during exploration. We define edges as generalized\nvectors and integrate similarity indexes to establish a directional filter that\nselects the nearest neighbors and estimates direction costs. The estimated\ndirection cost heuristics are utilized in edge evaluation. This strategy allows\nthe exploration to share directional information efficiently. DIT* convergence\nfaster than existing single-query, sampling-based planners on tested problems\nin R^4 to R^16 and has been demonstrated in real-world environments with\nvarious planning tasks. A video showcasing our experimental results is\navailable at: https://youtu.be/2SX6QT2NOek", "AI": {"tldr": "DIT*\u662f\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u641c\u7d22\u65b9\u5411\u548c\u5efa\u7acb\u65b9\u5411\u6ee4\u6ce2\u5668\uff0c\u6bd4\u73b0\u6709\u7b97\u6cd5\u5728R^4\u5230R^16\u7a7a\u95f4\u4e2d\u6536\u655b\u66f4\u5feb", "motivation": "\u73b0\u6709\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff08\u5982EIT*\uff09\u4f7f\u7528effort\u542f\u53d1\u5f0f\u6765\u6307\u5bfc\u641c\u7d22\uff0c\u4f46\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u5f80\u5f80\u96be\u4ee5\u517c\u987e\uff0c\u9700\u8981\u66f4\u597d\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5", "method": "\u5c06\u8fb9\u5b9a\u4e49\u4e3a\u5e7f\u4e49\u5411\u91cf\uff0c\u96c6\u6210\u76f8\u4f3c\u6027\u6307\u6570\u5efa\u7acb\u65b9\u5411\u6ee4\u6ce2\u5668\u6765\u9009\u62e9\u6700\u8fd1\u90bb\u548c\u4f30\u8ba1\u65b9\u5411\u6210\u672c\uff0c\u5728\u8fb9\u8bc4\u4f30\u4e2d\u4f7f\u7528\u4f30\u8ba1\u7684\u65b9\u5411\u6210\u672c\u542f\u53d1\u5f0f", "result": "DIT*\u5728R^4\u5230R^16\u7684\u6d4b\u8bd5\u95ee\u9898\u4e0a\u6bd4\u73b0\u6709\u7684\u5355\u67e5\u8be2\u57fa\u4e8e\u91c7\u6837\u89c4\u5212\u5668\u6536\u655b\u66f4\u5feb\uff0c\u5e76\u5728\u5404\u79cd\u89c4\u5212\u4efb\u52a1\u7684\u771f\u5b9e\u73af\u5883\u4e2d\u5f97\u5230\u9a8c\u8bc1", "conclusion": "\u901a\u8fc7\u4f18\u5316\u641c\u7d22\u65b9\u5411\u548c\u5171\u4eab\u65b9\u5411\u4fe1\u606f\uff0cDIT*\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u8def\u5f84\u89c4\u5212\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.19172", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19172", "abs": "https://arxiv.org/abs/2508.19172", "authors": ["Luca Grillotti", "Lisa Coiffard", "Oscar Pang", "Maxence Faldor", "Antoine Cully"], "title": "From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity", "comment": "Accepted at CoRL 2025", "summary": "Autonomous skill discovery aims to enable robots to acquire diverse behaviors\nwithout explicit supervision. Learning such behaviors directly on physical\nhardware remains challenging due to safety and data efficiency constraints.\nExisting methods, including Quality-Diversity Actor-Critic (QDAC), require\nmanually defined skill spaces and carefully tuned heuristics, limiting\nreal-world applicability. We propose Unsupervised Real-world Skill Acquisition\n(URSA), an extension of QDAC that enables robots to autonomously discover and\nmaster diverse, high-performing skills directly in the real world. We\ndemonstrate that URSA successfully discovers diverse locomotion skills on a\nUnitree A1 quadruped in both simulation and the real world. Our approach\nsupports both heuristic-driven skill discovery and fully unsupervised settings.\nWe also show that the learned skill repertoire can be reused for downstream\ntasks such as real-world damage adaptation, where URSA outperforms all\nbaselines in 5 out of 9 simulated and 3 out of 5 real-world damage scenarios.\nOur results establish a new framework for real-world robot learning that\nenables continuous skill discovery with limited human intervention,\nrepresenting a significant step toward more autonomous and adaptable robotic\nsystems. Demonstration videos are available at\nhttp://adaptive-intelligent-robotics.github.io/URSA .", "AI": {"tldr": "URSA\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u771f\u5b9e\u4e16\u754c\u6280\u80fd\u83b7\u53d6\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86QDAC\u7b97\u6cd5\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u81ea\u4e3b\u53d1\u73b0\u548c\u638c\u63e1\u591a\u6837\u5316\u9ad8\u6027\u80fd\u6280\u80fd\uff0c\u65e0\u9700\u4eba\u5de5\u5b9a\u4e49\u6280\u80fd\u7a7a\u95f4\u6216\u7cbe\u5fc3\u8c03\u6574\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982QDAC\u9700\u8981\u624b\u52a8\u5b9a\u4e49\u6280\u80fd\u7a7a\u95f4\u548c\u7cbe\u5fc3\u8c03\u6574\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u3002\u76f4\u63a5\u5728\u7269\u7406\u786c\u4ef6\u4e0a\u5b66\u4e60\u884c\u4e3a\u9762\u4e34\u5b89\u5168\u548c\u6570\u636e\u6548\u7387\u7684\u6311\u6218\u3002", "method": "URSA\u662fQDAC\u7684\u6269\u5c55\uff0c\u652f\u6301\u542f\u53d1\u5f0f\u9a71\u52a8\u7684\u6280\u80fd\u53d1\u73b0\u548c\u5b8c\u5168\u65e0\u76d1\u7763\u8bbe\u7f6e\uff0c\u80fd\u591f\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u81ea\u4e3b\u53d1\u73b0\u591a\u6837\u5316\u8fd0\u52a8\u6280\u80fd\u3002", "result": "\u5728Unitree A1\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u6210\u529f\u53d1\u73b0\u4e86\u591a\u6837\u5316\u8fd0\u52a8\u6280\u80fd\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4e2d\u90fd\u6709\u6548\u3002\u5728\u635f\u4f24\u9002\u5e94\u4efb\u52a1\u4e2d\uff0c\u57285/9\u4eff\u771f\u548c3/5\u771f\u5b9e\u4e16\u754c\u635f\u4f24\u573a\u666f\u4e2d\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "URSA\u4e3a\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5b66\u4e60\u5efa\u7acb\u4e86\u65b0\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u6709\u9650\u4eba\u5de5\u5e72\u9884\u4e0b\u7684\u6301\u7eed\u6280\u80fd\u53d1\u73b0\uff0c\u662f\u8fc8\u5411\u66f4\u81ea\u4e3b\u548c\u9002\u5e94\u6027\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2508.19186", "categories": ["cs.RO", "cs.AI", "cs.FL", "I.2.9; I.2; D.2.4"], "pdf": "https://arxiv.org/pdf/2508.19186", "abs": "https://arxiv.org/abs/2508.19186", "authors": ["Christopher Chandler", "Bernd Porr", "Giulia Lafratta", "Alice Miller"], "title": "Real-Time Model Checking for Closed-Loop Robot Reactive Planning", "comment": "30 pages excluding references, 18 figures, submitted to Formal\n  Aspects of Computing", "summary": "We present a new application of model checking which achieves real-time\nmulti-step planning and obstacle avoidance on a real autonomous robot. We have\ndeveloped a small, purpose-built model checking algorithm which generates plans\nin situ based on \"core\" knowledge and attention as found in biological agents.\nThis is achieved in real-time using no pre-computed data on a low-powered\ndevice. Our approach is based on chaining temporary control systems which are\nspawned to counteract disturbances in the local environment that disrupt an\nautonomous agent from its preferred action (or resting state). A novel\ndiscretization of 2D LiDAR data sensitive to bounded variations in the local\nenvironment is used. Multi-step planning using model checking by forward\ndepth-first search is applied to cul-de-sac and playground scenarios. Both\nempirical results and informal proofs of two fundamental properties of our\napproach demonstrate that model checking can be used to create efficient\nmulti-step plans for local obstacle avoidance, improving on the performance of\na reactive agent which can only plan one step. Our approach is an instructional\ncase study for the development of safe, reliable and explainable planning in\nthe context of autonomous vehicles.", "AI": {"tldr": "\u57fa\u4e8e\u6a21\u578b\u68c0\u67e5\u7684\u5b9e\u65f6\u591a\u6b65\u89c4\u5212\u6280\u672f\uff0c\u5728\u4f4e\u529f\u8017\u8bbe\u5907\u4e0a\u5b9e\u73b0\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u969c\u788d\u907f\u514d\u548c\u591a\u6b65\u89c4\u5212", "motivation": "\u4eff\u7167\u751f\u7269\u4ee3\u7406\u7684\"u6838\u5fc3\u77e5\u8bc6\"\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5f00\u53d1\u80fd\u591f\u5728\u5b9e\u65f6\u73af\u5883\u4e2d\u8fdb\u884c\u591a\u6b65\u89c4\u5212\u7684\u65b9\u6cd5\uff0c\u63d0\u5347\u53cd\u5e94\u5f0f\u673a\u5668\u4eba\u7684\u6027\u80fd", "method": "\u4f7f\u7528\u4e13\u95e8\u8bbe\u8ba1\u7684\u5c0f\u578b\u6a21\u578b\u68c0\u67e5\u7b97\u6cd5\uff0c\u901a\u8fc7\u94fe\u5f0f\u4e34\u65f6\u63a7\u5236\u7cfb\u7edf\u6765\u5bf9\u6297\u73af\u5883\u5e72\u6270\uff0c\u91c7\u7528\u5bf9\u5c40\u90e8\u73af\u5883\u6709\u754c\u53d8\u5316\u654f\u611f\u76842D LiDAR\u6570\u636e\u79bb\u6563\u5316\u65b9\u6cd5\uff0c\u8fdb\u884c\u524d\u5411\u6df1\u5ea6\u4f18\u5148\u641c\u7d22", "result": "\u5728cul-de-sac\u548c\u6e38\u620f\u573a\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u591a\u6b65\u89c4\u5212\uff0c\u8bc1\u660e\u6a21\u578b\u68c0\u67e5\u80fd\u591f\u4e3a\u5c40\u90e8\u969c\u788d\u907f\u514d\u521b\u5efa\u9ad8\u6548\u591a\u6b65\u8ba1\u5212\uff0c\u6027\u80fd\u8d85\u8fc7\u4ec5\u80fd\u89c4\u5212\u4e00\u6b65\u7684\u53cd\u5e94\u5f0f\u4ee3\u7406", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u4e3b\u8f66\u8f86\u9886\u57df\u5f00\u53d1\u5b89\u5168\u3001\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u89c4\u5212\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6559\u5b66\u6848\u4f8b\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u6a21\u578b\u68c0\u67e5\u5728\u5b9e\u65f6\u591a\u6b65\u89c4\u5212\u4e2d\u7684\u5e94\u7528\u4ef7\u503c"}}
{"id": "2508.19191", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19191", "abs": "https://arxiv.org/abs/2508.19191", "authors": ["Yue Wang", "Wenjie Deng", "Haotian Xue", "Di Cui", "Yiqi Chen", "Mingchuan Zhou", "Haochao Ying", "Jian Wu"], "title": "AutoRing: Imitation Learning--based Autonomous Intraocular Foreign Body Removal Manipulation with Eye Surgical Robot", "comment": null, "summary": "Intraocular foreign body removal demands millimeter-level precision in\nconfined intraocular spaces, yet existing robotic systems predominantly rely on\nmanual teleoperation with steep learning curves. To address the challenges of\nautonomous manipulation (particularly kinematic uncertainties from variable\nmotion scaling and variation of the Remote Center of Motion (RCM) point), we\npropose AutoRing, an imitation learning framework for autonomous intraocular\nforeign body ring manipulation. Our approach integrates dynamic RCM calibration\nto resolve coordinate-system inconsistencies caused by intraocular instrument\nvariation and introduces the RCM-ACT architecture, which combines\naction-chunking transformers with real-time kinematic realignment. Trained\nsolely on stereo visual data and instrument kinematics from expert\ndemonstrations in a biomimetic eye model, AutoRing successfully completes ring\ngrasping and positioning tasks without explicit depth sensing. Experimental\nvalidation demonstrates end-to-end autonomy under uncalibrated microscopy\nconditions. The results provide a viable framework for developing intelligent\neye-surgical systems capable of complex intraocular procedures.", "AI": {"tldr": "AutoRing\u662f\u4e00\u4e2a\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u81ea\u4e3b\u773c\u5185\u5f02\u7269\u73af\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001RCM\u6821\u51c6\u548cRCM-ACT\u67b6\u6784\u89e3\u51b3\u8fd0\u52a8\u7f29\u653e\u548cRCM\u70b9\u53d8\u5316\u5e26\u6765\u7684\u8fd0\u52a8\u5b66\u4e0d\u786e\u5b9a\u6027\uff0c\u4ec5\u4f7f\u7528\u7acb\u4f53\u89c6\u89c9\u6570\u636e\u548c\u4e13\u5bb6\u6f14\u793a\u5373\u53ef\u5b8c\u6210\u7cbe\u786e\u64cd\u4f5c\u3002", "motivation": "\u773c\u5185\u5f02\u7269\u79fb\u9664\u9700\u8981\u5728\u6709\u9650\u7a7a\u95f4\u5185\u5b9e\u73b0\u6beb\u7c73\u7ea7\u7cbe\u5ea6\uff0c\u73b0\u6709\u673a\u5668\u4eba\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u624b\u52a8\u9065\u64cd\u4f5c\uff0c\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\uff0c\u9700\u8981\u89e3\u51b3\u81ea\u4e3b\u64cd\u4f5c\u4e2d\u7684\u8fd0\u52a8\u5b66\u4e0d\u786e\u5b9a\u6027\u6311\u6218\u3002", "method": "\u63d0\u51faAutoRing\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u52a8\u6001RCM\u6821\u51c6\u89e3\u51b3\u5750\u6807\u7cfb\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5f15\u5165\u7ed3\u5408\u52a8\u4f5c\u5206\u5757\u53d8\u6362\u5668\u548c\u5b9e\u65f6\u8fd0\u52a8\u5b66\u91cd\u65b0\u5bf9\u9f50\u7684RCM-ACT\u67b6\u6784\uff0c\u4ec5\u4f7f\u7528\u7acb\u4f53\u89c6\u89c9\u6570\u636e\u548c\u4e13\u5bb6\u6f14\u793a\u7684\u4eea\u5668\u8fd0\u52a8\u5b66\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u672a\u6821\u51c6\u663e\u5fae\u955c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u7aef\u5230\u7aef\u81ea\u4e3b\u64cd\u4f5c\uff0c\u6210\u529f\u5b8c\u6210\u73af\u6293\u53d6\u548c\u5b9a\u4f4d\u4efb\u52a1\uff0c\u65e0\u9700\u663e\u5f0f\u6df1\u5ea6\u611f\u77e5\u3002", "conclusion": "\u4e3a\u5f00\u53d1\u80fd\u591f\u6267\u884c\u590d\u6742\u773c\u5185\u624b\u672f\u7684\u667a\u80fd\u773c\u79d1\u624b\u672f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u6846\u67b6\u3002"}}
{"id": "2508.19199", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19199", "abs": "https://arxiv.org/abs/2508.19199", "authors": ["Alex LaGrassa", "Zixuan Huang", "Dmitry Berenson", "Oliver Kroemer"], "title": "Planning-Query-Guided Model Generation for Model-Based Deformable Object Manipulation", "comment": "9 pages, 7 figures", "summary": "Efficient planning in high-dimensional spaces, such as those involving\ndeformable objects, requires computationally tractable yet sufficiently\nexpressive dynamics models. This paper introduces a method that automatically\ngenerates task-specific, spatially adaptive dynamics models by learning which\nregions of the object require high-resolution modeling to achieve good task\nperformance for a given planning query. Task performance depends on the complex\ninterplay between the dynamics model, world dynamics, control, and task\nrequirements. Our proposed diffusion-based model generator predicts per-region\nmodel resolutions based on start and goal pointclouds that define the planning\nquery. To efficiently collect the data for learning this mapping, a two-stage\nprocess optimizes resolution using predictive dynamics as a prior before\ndirectly optimizing using closed-loop performance. On a tree-manipulation task,\nour method doubles planning speed with only a small decrease in task\nperformance over using a full-resolution model. This approach informs a path\ntowards using previous planning and control data to generate computationally\nefficient yet sufficiently expressive dynamics models for new tasks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6e17\u900f\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6839\u636e\u4efb\u52a1\u9700\u6c42\u81ea\u52a8\u751f\u6210\u7a7a\u95f4\u9002\u914d\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u901a\u8fc7\u5b66\u4e60\u8bc6\u522b\u5bf9\u4efb\u52a1\u6027\u80fd\u5173\u952e\u7684\u533a\u57df\u6765\u63d0\u9ad8\u89c4\u5212\u6548\u7387\u3002", "motivation": "\u5728\u9ad8\u7ef4\u5ea6\u7a7a\u95f4\uff08\u5982\u53ef\u53d8\u5f62\u7269\u4f53\uff09\u4e2d\u8fdb\u884c\u9ad8\u6548\u89c4\u5212\u9700\u8981\u8ba1\u7b97\u53ef\u5904\u7406\u4f46\u5145\u5206\u8868\u8fbe\u7684\u52a8\u529b\u5b66\u6a21\u578b\u3002\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u56fa\u5b9a\u5206\u8fa8\u7387\u7684\u6a21\u578b\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d39\u7528\u9ad8\u6216\u6027\u80fd\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6e17\u900f\u6a21\u578b\u7684\u6a21\u578b\u751f\u6210\u5668\uff0c\u6839\u636e\u8d77\u59cb\u548c\u76ee\u6807\u70b9\u4e91\u9884\u6d4b\u5404\u533a\u57df\u7684\u6a21\u578b\u5206\u8fa8\u7387\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\uff1a\u5148\u4f7f\u7528\u9884\u6d4b\u52a8\u529b\u5b66\u4f5c\u4e3a\u5148\u9a8c\u4f18\u5316\u5206\u8fa8\u7387\uff0c\u7136\u540e\u76f4\u63a5\u4f18\u5316\u95ed\u73af\u6027\u80fd\u3002", "result": "\u5728\u6811\u6728\u64cd\u7eb5\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c06\u89c4\u5212\u901f\u5ea6\u63d0\u9ad8\u4e86\u4e00\u500d\uff0c\u800c\u4efb\u52a1\u6027\u80fd\u4ec5\u6709\u8f7b\u5fae\u4e0b\u964d\uff0c\u8f83\u4f7f\u7528\u5168\u5206\u8fa8\u7387\u6a21\u578b\u66f4\u9ad8\u6548\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u5229\u7528\u5386\u53f2\u89c4\u5212\u548c\u63a7\u5236\u6570\u636e\u751f\u6210\u8ba1\u7b97\u9ad8\u6548\u4e14\u5145\u5206\u8868\u8fbe\u7684\u52a8\u529b\u5b66\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\uff0c\u9002\u7528\u4e8e\u65b0\u4efb\u52a1\u7684\u9ad8\u6548\u89c4\u5212\u3002"}}
{"id": "2508.19236", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19236", "abs": "https://arxiv.org/abs/2508.19236", "authors": ["Hao Shi", "Bin Xie", "Yingfei Liu", "Lin Sun", "Fengrong Liu", "Tiancai Wang", "Erjin Zhou", "Haoqiang Fan", "Xiangyu Zhang", "Gao Huang"], "title": "MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation", "comment": "The project is available at https://shihao1895.github.io/MemoryVLA", "summary": "Temporal context is essential for robotic manipulation because such tasks are\ninherently non-Markovian, yet mainstream VLA models typically overlook it and\nstruggle with long-horizon, temporally dependent tasks. Cognitive science\nsuggests that humans rely on working memory to buffer short-lived\nrepresentations for immediate control, while the hippocampal system preserves\nverbatim episodic details and semantic gist of past experience for long-term\nmemory. Inspired by these mechanisms, we propose MemoryVLA, a\nCognition-Memory-Action framework for long-horizon robotic manipulation. A\npretrained VLM encodes the observation into perceptual and cognitive tokens\nthat form working memory, while a Perceptual-Cognitive Memory Bank stores\nlow-level details and high-level semantics consolidated from it. Working memory\nretrieves decision-relevant entries from the bank, adaptively fuses them with\ncurrent tokens, and updates the bank by merging redundancies. Using these\ntokens, a memory-conditioned diffusion action expert yields temporally aware\naction sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasks\nacross three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, it\nachieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperforming\nstate-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain on\nBridge. On 12 real-world tasks spanning general skills and long-horizon\ntemporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizon\ntasks showing a +26 improvement over state-of-the-art baseline. Project Page:\nhttps://shihao1895.github.io/MemoryVLA", "AI": {"tldr": "MemoryVLA\u662f\u4e00\u4e2a\u53d7\u4eba\u7c7b\u8bb0\u5fc6\u673a\u5236\u542f\u53d1\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u5de5\u4f5c\u8bb0\u5fc6\u548c\u957f\u671f\u8bb0\u5fc6\u7cfb\u7edf\u5904\u7406\u65f6\u5e8f\u4e0a\u4e0b\u6587\uff0c\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u4e3b\u6d41VLA\u6a21\u578b\u5ffd\u7565\u65f6\u5e8f\u4e0a\u4e0b\u6587\uff0c\u96be\u4ee5\u5904\u7406\u957f\u65f6\u7a0b\u3001\u65f6\u95f4\u4f9d\u8d56\u7684\u4efb\u52a1\u3002\u53d7\u8ba4\u77e5\u79d1\u5b66\u542f\u53d1\uff0c\u4eba\u7c7b\u4f9d\u8d56\u5de5\u4f5c\u8bb0\u5fc6\u548c\u957f\u671f\u8bb0\u5fc6\u7cfb\u7edf\u6765\u5904\u7406\u65f6\u5e8f\u4fe1\u606f", "method": "\u63d0\u51faCognition-Memory-Action\u6846\u67b6\uff1a\u9884\u8bad\u7ec3VLM\u7f16\u7801\u89c2\u6d4b\u4e3a\u611f\u77e5\u548c\u8ba4\u77e5token\u5f62\u6210\u5de5\u4f5c\u8bb0\u5fc6\uff0c\u611f\u77e5-\u8ba4\u77e5\u8bb0\u5fc6\u5e93\u5b58\u50a8\u4f4e\u5c42\u7ec6\u8282\u548c\u9ad8\u5c42\u8bed\u4e49\uff0c\u5de5\u4f5c\u8bb0\u5fc6\u4ece\u5e93\u4e2d\u68c0\u7d22\u76f8\u5173\u6761\u76ee\u5e76\u878d\u5408\uff0c\u8bb0\u5fc6\u6761\u4ef6\u6269\u6563\u52a8\u4f5c\u4e13\u5bb6\u751f\u6210\u65f6\u5e8f\u611f\u77e5\u52a8\u4f5c\u5e8f\u5217", "result": "\u5728150+\u4eff\u771f\u548c\u771f\u5b9e\u4efb\u52a1\u4e2d\u6d4b\u8bd5\uff1aSimplerEnv-Bridge 71.9%\u3001Fractal 72.7%\u3001LIBERO-5 96.5%\u6210\u529f\u7387\uff0c\u5747\u4f18\u4e8eCogACT\u548cpi-0\uff0cBridge\u4efb\u52a1\u63d0\u534714.6%\uff1b\u771f\u5b9e\u4e16\u754c12\u4e2a\u4efb\u52a184.0%\u6210\u529f\u7387\uff0c\u957f\u65f6\u7a0b\u4efb\u52a1\u63d0\u534726%", "conclusion": "MemoryVLA\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8bb0\u5fc6\u673a\u5236\u6709\u6548\u5904\u7406\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u65f6\u5e8f\u4e0a\u4e0b\u6587\u4f9d\u8d56\uff0c\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u65f6\u5e8f\u611f\u77e5\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
