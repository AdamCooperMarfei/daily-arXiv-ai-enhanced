<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 26]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [FICO: Finite-Horizon Closed-Loop Factorization for Unified Multi-Agent Path Finding](https://arxiv.org/abs/2511.13961)
*Jiarui Li,Alessandro Zanardi,Runyu Zhang,Gioele Zardini*

Main category: cs.RO

TL;DR: 提出了一个系统级的多智能体路径规划框架，集成了规划与执行，通过闭环控制设计处理不确定性，能够实时响应并扩展到数千个智能体。


<details>
  <summary>Details</summary>
Motivation: 现有MAPF方法将规划与执行分离，以临时方式处理问题变体，缺乏对不确定性的显式建模，需要系统级框架来统一处理。

Method: 引入MAPF系统作为核心形式模型，提出有限时域闭环分解算法，利用组合结构实现高效闭环操作，受滚动时域控制启发。

Result: 计算时间比开环基线减少两个数量级，在随机延迟和智能体到达情况下显著提高吞吐量，支持毫秒级响应和数千智能体规模。

Conclusion: 通过系统级建模、分解和闭环设计，为MAPF的分析和推进建立了原则性基础。

Abstract: Multi-Agent Path Finding is a fundamental problem in robotics and AI, yet most existing formulations treat planning and execution separately and address variants of the problem in an ad hoc manner. This paper presents a system-level framework for MAPF that integrates planning and execution, generalizes across variants, and explicitly models uncertainties. At its core is the MAPF system, a formal model that casts MAPF as a control design problem encompassing classical and uncertainty-aware formulations. To solve it, we introduce Finite-Horizon Closed-Loop Factorization (FICO), a factorization-based algorithm inspired by receding-horizon control that exploits compositional structure for efficient closed-loop operation. FICO enables real-time responses -- commencing execution within milliseconds -- while scaling to thousands of agents and adapting seamlessly to execution-time uncertainties. Extensive case studies demonstrate that it reduces computation time by up to two orders of magnitude compared with open-loop baselines, while delivering significantly higher throughput under stochastic delays and agent arrivals. These results establish a principled foundation for analyzing and advancing MAPF through system-level modeling, factorization, and closed-loop design.

</details>


### [2] [LIO-MARS: Non-uniform Continuous-time Trajectories for Real-time LiDAR-Inertial-Odometry](https://arxiv.org/abs/2511.13985)
*Jan Quenzel,Sven Behnke*

Main category: cs.RO

TL;DR: 提出了LIO-MARS激光雷达-惯性里程计系统，通过多分辨率面元图与高斯混合模型的对齐，结合连续时间B样条轨迹，实现了高精度的实时定位与建图。


<details>
  <summary>Details</summary>
Motivation: 自主机器人系统需要鲁棒的实时感知能力进行安全导航，特别是在搜救等任务中。IMU和LiDAR传感器具有互补性，需要有效融合两者的优势。

Method: 使用连续时间B样条轨迹，通过非均匀时间节点放置确保轨迹连续性；采用克罗内克和与积加速协方差和GMM计算；使用无迹变换去偏斜面元，分割扫描段进行运动补偿；结合相对位姿软约束和预积分IMU伪测量。

Result: 在多种手持、地面和空中车辆数据集上的广泛评估表明，LIO-MARS相对于最近的LIO系统具有最先进的性能质量。

Conclusion: LIO-MARS系统通过创新的多分辨率面元图对齐、连续时间轨迹表示和高效的协方差计算，实现了鲁棒且精确的激光雷达-惯性里程计。

Abstract: Autonomous robotic systems heavily rely on environment knowledge to safely navigate. For search & rescue, a flying robot requires robust real-time perception, enabled by complementary sensors. IMU data constrains acceleration and rotation, whereas LiDAR measures accurate distances around the robot. Building upon the LiDAR odometry MARS, our LiDAR-inertial odometry (LIO) jointly aligns multi-resolution surfel maps with a Gaussian mixture model (GMM) using a continuous-time B-spline trajectory. Our new scan window uses non-uniform temporal knot placement to ensure continuity over the whole trajectory without additional scan delay. Moreover, we accelerate essential covariance and GMM computations with Kronecker sums and products by a factor of 3.3. An unscented transform de-skews surfels, while a splitting into intra-scan segments facilitates motion compensation during spline optimization. Complementary soft constraints on relative poses and preintegrated IMU pseudo-measurements further improve robustness and accuracy. Extensive evaluation showcases the state-of-the-art quality of our LIO-MARS w.r.t. recent LIO systems on various handheld, ground and aerial vehicle-based datasets.

</details>


### [3] [Searching in Space and Time: Unified Memory-Action Loops for Open-World Object Retrieval](https://arxiv.org/abs/2511.14004)
*Taijing Chen,Sateesh Kumar,Junhong Xu,George Pavlakos,J oydeep Biswas,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: STAR是一个统一时空检索的框架，将记忆查询和具身行动结合在单一决策循环中，在动态开放世界中实现物体检索。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能解决部分问题：场景图捕捉空间关系但忽略时间基础，时序推理方法建模动态但不支持具身交互，动态场景图处理两者但仍是封闭世界。需要统一时空检索的解决方案。

Method: STAR框架利用非参数长期记忆和工作记忆支持高效回忆，使用视觉语言模型在每一步选择时序或空间行动，在单一决策循环中统一记忆查询和具身行动。

Result: 在STARBench基准测试和Tiago机器人实验中，STAR持续优于场景图和仅记忆的基线方法，证明了将时空搜索作为统一问题处理的好处。

Conclusion: 将时间搜索和空间搜索作为统一问题处理具有显著优势，STAR框架在动态开放世界物体检索任务中表现优异。

Abstract: Service robots must retrieve objects in dynamic, open-world settings where requests may reference attributes ("the red mug"), spatial context ("the mug on the table"), or past states ("the mug that was here yesterday"). Existing approaches capture only parts of this problem: scene graphs capture spatial relations but ignore temporal grounding, temporal reasoning methods model dynamics but do not support embodied interaction, and dynamic scene graphs handle both but remain closed-world with fixed vocabularies. We present STAR (SpatioTemporal Active Retrieval), a framework that unifies memory queries and embodied actions within a single decision loop. STAR leverages non-parametric long-term memory and a working memory to support efficient recall, and uses a vision-language model to select either temporal or spatial actions at each step. We introduce STARBench, a benchmark of spatiotemporal object search tasks across simulated and real environments. Experiments in STARBench and on a Tiago robot show that STAR consistently outperforms scene-graph and memory-only baselines, demonstrating the benefits of treating search in time and search in space as a unified problem.

</details>


### [4] [FACA: Fair and Agile Multi-Robot Collision Avoidance in Constrained Environments with Dynamic Priorities](https://arxiv.org/abs/2511.14024)
*Jaskirat Singh,Rohan Chandra*

Main category: cs.RO

TL;DR: FACA是一种公平敏捷的碰撞避免方法，机器人通过自然语言协调任务，使用新型人工势场算法在冲突时自动形成"环岛"效应，实验显示效率提升3.5倍以上。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统在救援、配送等关键应用中需要高速通过受限空间，这些空间通常拥挤着多个异构智能体，且角色和优先级会快速变化，需要机器人既安全又敏捷。

Method: 提出FACA方法，机器人通过自然语言相互协调，采用新型人工势场算法在冲突时自动产生"环岛"效应来平衡安全性与敏捷性。

Result: 实验表明FACA效率显著提升，完成任务速度比基线快3.5倍以上，时间减少超过70%，同时保持稳健的安全边界。

Conclusion: FACA方法通过自然语言协调和环岛式冲突解决机制，在多机器人系统中实现了安全与敏捷的良好平衡。

Abstract: Multi-robot systems are increasingly being used for critical applications such as rescuing injured people, delivering food and medicines, and monitoring key areas. These applications usually involve navigating at high speeds through constrained spaces such as small gaps. Navigating such constrained spaces becomes particularly challenging when the space is crowded with multiple heterogeneous agents all of which have urgent priorities. What makes the problem even harder is that during an active response situation, roles and priorities can quickly change on a dime without informing the other agents. In order to complete missions in such environments, robots must not only be safe, but also agile, able to dodge and change course at a moment's notice. In this paper, we propose FACA, a fair and agile collision avoidance approach where robots coordinate their tasks by talking to each other via natural language (just as people do). In FACA, robots balance safety with agility via a novel artificial potential field algorithm that creates an automatic ``roundabout'' effect whenever a conflict arises. Our experiments show that FACA achieves a improvement in efficiency, completing missions more than 3.5X faster than baselines with a time reduction of over 70% while maintaining robust safety margins.

</details>


### [5] [BIM-Discrepancy-Driven Active Sensing for Risk-Aware UAV-UGV Navigation](https://arxiv.org/abs/2511.14037)
*Hesam Mojtahedi,Reza Akhavian*

Main category: cs.RO

TL;DR: 提出基于BIM差异驱动的主动感知框架，用于无人机和无人地面车辆在动态建筑环境中的协同导航，通过风险触发重新扫描显著降低风险并提高导航安全性。


<details>
  <summary>Details</summary>
Motivation: 传统导航方法依赖静态BIM先验或有限的机载感知，无法适应动态建筑环境的变化，需要一种能够融合实时感知数据与BIM先验的主动感知框架。

Method: 框架持续融合空中和地面机器人的实时LiDAR数据与BIM先验，维护演化的2D占据地图，通过统一的走廊风险度量（包含占据不确定性、BIM地图差异和间隙）评估导航安全性，当风险超过阈值时无人机自主重新扫描受影响区域。

Result: 在PX4-Gazebo仿真中验证，风险触发重新扫描使平均走廊风险降低58%，地图熵降低43%，同时保持间隙裕度在0.4米以上；与前沿探索方法相比，在减半的任务时间内实现相似的不确定性降低。

Conclusion: 将BIM先验与风险自适应空中感知相结合，能够为建筑机器人实现可扩展的、不确定性感知的自主性。

Abstract: This paper presents a BIM-discrepancy-driven active sensing framework for cooperative navigation between unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) in dynamic construction environments. Traditional navigation approaches rely on static Building Information Modeling (BIM) priors or limited onboard perception. In contrast, our framework continuously fuses real-time LiDAR data from aerial and ground robots with BIM priors to maintain an evolving 2D occupancy map. We quantify navigation safety through a unified corridor-risk metric integrating occupancy uncertainty, BIM-map discrepancy, and clearance. When risk exceeds safety thresholds, the UAV autonomously re-scans affected regions to reduce uncertainty and enable safe replanning. Validation in PX4-Gazebo simulation with Robotec GPU LiDAR demonstrates that risk-triggered re-scanning reduces mean corridor risk by 58% and map entropy by 43% compared to static BIM navigation, while maintaining clearance margins above 0.4 m. Compared to frontier-based exploration, our approach achieves similar uncertainty reduction in half the mission time. These results demonstrate that integrating BIM priors with risk-adaptive aerial sensing enables scalable, uncertainty-aware autonomy for construction robotics.

</details>


### [6] [FlexiCup: Wireless Multimodal Suction Cup with Dual-Zone Vision-Tactile Sensing](https://arxiv.org/abs/2511.14139)
*Junhao Gong,Shoujie Li,Kit-Wa Sou,Changqing Guo,Hourong Huang,Tong Wu,Yifan Xie,Chenxin Liang,Chuqiao Lyu,Xiaojun Liang,Wenbo Ding*

Main category: cs.RO

TL;DR: FlexiCup是一种完全无线的多模态吸盘，集成了双区域视觉触觉传感，支持真空和伯努利两种吸附模式，通过模块化机械配置实现无线自主操作。


<details>
  <summary>Details</summary>
Motivation: 传统吸盘缺乏在非结构化环境中进行接触感知操作的能力，需要开发具有传感功能的智能吸盘系统。

Method: 采用双区域传感设计：中央区域通过照明控制在视觉和触觉模态间动态切换，外围区域提供连续空间感知。支持真空和伯努利两种吸附模式，具备板载计算和电源实现完全无线自主。

Result: 在结构化表面上的模块化感知驱动抓取中，真空模式平均成功率90.0%，伯努利模式86.7%。基于扩散的端到端学习在倾斜运输任务中成功率73.3%，橙子提取任务66.7%。双区域观测协调提供13%的接触感知操作改进。

Conclusion: FlexiCup展示了多模态传感吸盘在非结构化环境中接触感知操作的可行性，双区域传感设计显著提升了操作性能。

Abstract: Conventional suction cups lack sensing capabilities for contact-aware manipulation in unstructured environments. This paper presents FlexiCup, a fully wireless multimodal suction cup that integrates dual-zone vision-tactile sensing. The central zone dynamically switches between vision and tactile modalities via illumination control for contact detection, while the peripheral zone provides continuous spatial awareness for approach planning. FlexiCup supports both vacuum and Bernoulli suction modes through modular mechanical configurations, achieving complete wireless autonomy with onboard computation and power. We validate hardware versatility through dual control paradigms. Modular perception-driven grasping across structured surfaces with varying obstacle densities demonstrates comparable performance between vacuum (90.0% mean success) and Bernoulli (86.7% mean success) modes. Diffusion-based end-to-end learning achieves 73.3% success on inclined transport and 66.7% on orange extraction tasks. Ablation studies confirm that multi-head attention coordinating dual-zone observations provides 13% improvements for contact-aware manipulation. Hardware designs and firmware are available at https://anonymous.4open.science/api/repo/FlexiCup-DA7D/file/index.html?v=8f531b44.

</details>


### [7] [AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models](https://arxiv.org/abs/2511.14148)
*Yuhua Jiang,Shuang Cheng,Yan Ding,Feifei Gao,Biqing Qi*

Main category: cs.RO

TL;DR: 提出AsyncVLA框架，通过异步流匹配引入时间灵活性，使VLA模型能够在非均匀时间表中生成动作令牌，并具备动作上下文感知和自我纠正能力。


<details>
  <summary>Details</summary>
Motivation: 传统VLA模型使用同步流匹配，依赖刚性的均匀时间表，缺乏动作上下文感知和异步自我纠正，在长时程任务中不稳定，单个动作错误会级联导致失败。

Method: 提出异步流匹配VLA框架，引入时间灵活性和自我纠正机制，通过置信度评估器提取初始生成动作的置信度，选择性优化不准确的动作令牌，并提供SFM和AFM的统一训练过程。

Result: 在机器人操作基准测试中，AsyncVLA表现出数据效率和自我纠正能力，在通用具身评估中实现了最先进的结果。

Conclusion: AsyncVLA通过异步生成在AFM中实现了状态的最优表现，证明了异步流匹配在VLA模型中的有效性。

Abstract: Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at https://github.com/YuhuaJiang2002/AsyncVLA.

</details>


### [8] [RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action](https://arxiv.org/abs/2511.14161)
*Xiaoquan Sun,Ruijian Zhang,Kang Pang,Bingchen Miao,Yuxiang Tan,Zhen Yang,Ming Li,Jiayu Chen*

Main category: cs.RO

TL;DR: 提出了RoboTidy基准测试，用于评估语言引导的家庭整理任务，支持视觉-语言-动作和视觉-语言导航的训练与评估。


<details>
  <summary>Details</summary>
Motivation: 当前的家庭整理基准测试缺乏用户偏好建模、移动性支持，且泛化能力差，难以全面评估语言到动作的综合能力。

Method: 构建了500个逼真的3D高斯散射家庭场景，包含500个物体和容器，将整理任务表述为"动作(物体,容器)"列表，并提供6.4k个高质量操作演示轨迹和1.5k个导航轨迹。

Result: 创建了一个统一的家庭整理基准测试平台，支持小样本和大规模训练，并在现实世界中部署用于物体整理。

Conclusion: RoboTidy为语言引导机器人提供了可扩展的平台，填补了具身AI中的关键空白，实现了全面且现实的评估。

Abstract: Household tidying is an important application area, yet current benchmarks neither model user preferences nor support mobility, and they generalize poorly, making it hard to comprehensively assess integrated language-to-action capabilities. To address this, we propose RoboTidy, a unified benchmark for language-guided household tidying that supports Vision-Language-Action (VLA) and Vision-Language-Navigation (VLN) training and evaluation. RoboTidy provides 500 photorealistic 3D Gaussian Splatting (3DGS) household scenes (covering 500 objects and containers) with collisions, formulates tidying as an "Action (Object, Container)" list, and supplies 6.4k high-quality manipulation demonstration trajectories and 1.5k naviagtion trajectories to support both few-shot and large-scale training. We also deploy RoboTidy in the real world for object tidying, establishing an end-to-end benchmark for household tidying. RoboTidy offers a scalable platform and bridges a key gap in embodied AI by enabling holistic and realistic evaluation of language-guided robots.

</details>


### [9] [Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion](https://arxiv.org/abs/2511.14178)
*Zhuo Li,Junjia Liu,Zhipeng Dong,Tao Teng,Quentin Rouxel,Darwin Caldwell,Fei Chen*

Main category: cs.RO

TL;DR: VLA-Pilot是一种即插即用的推理时策略引导方法，无需额外微调或数据收集即可实现预训练VLA模型的零样本部署，显著提升下游机器人操作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 预训练的VLA模型在下游部署时性能显著下降，而微调方法依赖昂贵的演示数据收集和密集计算，在实际场景中不实用。

Method: 提出VLA-Pilot方法，这是一种推理时策略引导技术，无需额外微调或数据收集，可直接应用于预训练VLA模型。

Result: 在6个真实世界下游操作任务和2种不同机器人平台上进行评估，涵盖分布内和分布外场景，显著提升了现成预训练VLA策略的成功率。

Conclusion: VLA-Pilot能够实现预训练VLA模型的鲁棒零样本泛化，适用于多样化的任务和机器人平台。

Abstract: Vision-Language-Action (VLA) models have demonstrated significant potential in real-world robotic manipulation. However, pre-trained VLA policies still suffer from substantial performance degradation during downstream deployment. Although fine-tuning can mitigate this issue, its reliance on costly demonstration collection and intensive computation makes it impractical in real-world settings. In this work, we introduce VLA-Pilot, a plug-and-play inference-time policy steering method for zero-shot deployment of pre-trained VLA without any additional fine-tuning or data collection. We evaluate VLA-Pilot on six real-world downstream manipulation tasks across two distinct robotic embodiments, encompassing both in-distribution and out-of-distribution scenarios. Experimental results demonstrate that VLA-Pilot substantially boosts the success rates of off-the-shelf pre-trained VLA policies, enabling robust zero-shot generalization to diverse tasks and embodiments. Experimental videos and code are available at: https://rip4kobe.github.io/vla-pilot/.

</details>


### [10] [Dual-Variable Force Characterisation method for Human-Robot Interaction in Wearable Robotics](https://arxiv.org/abs/2511.14327)
*Felipe Ballen-Moreno,Pasquale Ferrentino,Milan Amighi,Bram Vanderborght,Tom Verstraten*

Main category: cs.RO

TL;DR: 提出了一种双变量表征方法，用于评估穿戴式机器人物理接口中软组织的非线性行为，解决了现有单变量方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 穿戴式机器人物理交互涉及复杂运动和非线性软组织行为，现有表征方法仅使用单一拟合变量，无法充分描述多自由度交互。

Method: 引入包含法向力和切向力的双变量表征方法，识别可靠材料参数并评估单变量拟合对力和扭矩响应的影响。

Result: 通过分析不同场景和材料模型的归一化均方误差，证明了双变量表征的重要性，为物理交互仿真提供了更准确的基础。

Conclusion: 双变量表征方法能够更准确地描述穿戴式机器人物理接口中的复杂交互，为改善安全性和舒适性提供了重要工具。

Abstract: Understanding the physical interaction with wearable robots is essential to ensure safety and comfort. However, this interaction is complex in two key aspects: (1) the motion involved, and (2) the non-linear behaviour of soft tissues. Multiple approaches have been undertaken to better understand this interaction and to improve the quantitative metrics of physical interfaces or cuffs. As these two topics are closely interrelated, finite modelling and soft tissue characterisation offer valuable insights into pressure distribution and shear stress induced by the cuff. Nevertheless, current characterisation methods typically rely on a single fitting variable along one degree of freedom, which limits their applicability, given that interactions with wearable robots often involve multiple degrees of freedom. To address this limitation, this work introduces a dual-variable characterisation method, involving normal and tangential forces, aimed at identifying reliable material parameters and evaluating the impact of single-variable fitting on force and torque responses. This method demonstrates the importance of incorporating two variables into the characterisation process by analysing the normalized mean square error (NMSE) across different scenarios and material models, providing a foundation for simulation at the closest possible level, with a focus on the cuff and the human limb involved in the physical interaction between the user and the wearable robot.

</details>


### [11] [MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning](https://arxiv.org/abs/2511.14330)
*Yizhen Yin,Yuhua Qi,Dapeng Feng,Hongbo Chen,Hongjun Ma,Jin Wu,Yi Jiang*

Main category: cs.RO

TL;DR: MA-SLAM是一种基于深度强化学习的主动SLAM系统，通过新颖的结构化地图表示和全局规划器，在大规模环境中显著减少了探索时间和距离。


<details>
  <summary>Details</summary>
Motivation: 当前主动SLAM方法在小规模受控环境中表现良好，但在大规模多样化环境中面临探索时间长、路径优化不足的挑战。

Method: 提出结构化地图表示，通过空间数据离散化、边界点和历史轨迹集成来有效封装已访问区域；采用深度强化学习决策模块和全局规划器优化探索路径。

Result: 在三个仿真环境和真实无人地面车辆上的实验表明，该方法相比最先进方法显著减少了探索时间和距离。

Conclusion: MA-SLAM系统通过结构化地图表示和全局规划策略，有效解决了大规模环境中的高效探索问题。

Abstract: Active Simultaneous Localization and Mapping (Active SLAM) involves the strategic planning and precise control of a robotic system's movement in order to construct a highly accurate and comprehensive representation of its surrounding environment, which has garnered significant attention within the research community. While the current methods demonstrate efficacy in small and controlled settings, they face challenges when applied to large-scale and diverse environments, marked by extended periods of exploration and suboptimal paths of discovery. In this paper, we propose MA-SLAM, a Map-Aware Active SLAM system based on Deep Reinforcement Learning (DRL), designed to address the challenge of efficient exploration in large-scale environments. In pursuit of this objective, we put forward a novel structured map representation. By discretizing the spatial data and integrating the boundary points and the historical trajectory, the structured map succinctly and effectively encapsulates the visited regions, thereby serving as input for the deep reinforcement learning based decision module. Instead of sequentially predicting the next action step within the decision module, we have implemented an advanced global planner to optimize the exploration path by leveraging long-range target points. We conducted experiments in three simulation environments and deployed in a real unmanned ground vehicle (UGV), the results demonstrate that our approach significantly reduces both the duration and distance of exploration compared with state-of-the-art methods.

</details>


### [12] [Simultaneous Localization and 3D-Semi Dense Mapping for Micro Drones Using Monocular Camera and Inertial Sensors](https://arxiv.org/abs/2511.14335)
*Jeryes Danial,Yosi Ben Asher,Itzik Klein*

Main category: cs.RO

TL;DR: 提出了一种结合稀疏关键点姿态估计和密集边缘重建的轻量级单目SLAM系统，通过深度学习深度预测和边缘检测，融合惯性数据解决尺度模糊问题，实现资源受限环境下的实时建图和导航。


<details>
  <summary>Details</summary>
Motivation: 当前单目SLAM算法存在稀疏方法缺乏详细几何信息、学习驱动方法计算量大、以及尺度模糊影响精度等问题，需要一种既保持几何细节又能在低功耗平台上实时运行的解决方案。

Method: 结合稀疏关键点姿态估计与密集边缘重建，使用深度学习进行深度预测和边缘检测，通过优化算法精化关键点和边缘的几何一致性，利用扩展卡尔曼滤波器融合惯性数据解决尺度模糊问题。

Result: 系统在DJI Tello无人机上实时运行，在室内走廊和TUM RGBD数据集上展示了鲁棒的自主导航和避障能力，为资源受限环境提供了有效的建图和导航解决方案。

Conclusion: 该方法提供了一种实用有效的实时建图和导航方案，特别适用于计算资源受限的环境，成功解决了单目SLAM的尺度模糊和计算效率问题。

Abstract: Monocular simultaneous localization and mapping (SLAM) algorithms estimate drone poses and build a 3D map using a single camera. Current algorithms include sparse methods that lack detailed geometry, while learning-driven approaches produce dense maps but are computationally intensive. Monocular SLAM also faces scale ambiguities, which affect its accuracy. To address these challenges, we propose an edge-aware lightweight monocular SLAM system combining sparse keypoint-based pose estimation with dense edge reconstruction. Our method employs deep learning-based depth prediction and edge detection, followed by optimization to refine keypoints and edges for geometric consistency, without relying on global loop closure or heavy neural computations. We fuse inertial data with vision by using an extended Kalman filter to resolve scale ambiguity and improve accuracy. The system operates in real time on low-power platforms, as demonstrated on a DJI Tello drone with a monocular camera and inertial sensors. In addition, we demonstrate robust autonomous navigation and obstacle avoidance in indoor corridors and on the TUM RGBD dataset. Our approach offers an effective, practical solution to real-time mapping and navigation in resource-constrained environments.

</details>


### [13] [Going Places: Place Recognition in Artificial and Natural Systems](https://arxiv.org/abs/2511.14341)
*Michael Milford,Tobias Fischer*

Main category: cs.RO

TL;DR: 这篇综述整合了机器人系统、动物研究和人类研究的发现，探讨不同系统如何编码和回忆位置，提出了统一的概念框架来考虑和开发位置识别机制。


<details>
  <summary>Details</summary>
Motivation: 位置识别对于生物导航和自主系统都至关重要，需要从多个领域整合见解以促进人工定位技术的发展。

Method: 通过综合机器人系统、动物研究和人类研究的发现，分析计算和表征策略，包括拓扑映射、线索整合和记忆管理等收敛解决方案。

Result: 揭示了动物系统的多模态导航和环境适应机制，人类研究的语义位置概念、文化影响和内省能力，以及人工系统的可扩展架构和数据驱动模型。

Conclusion: 提出了统一的概念框架来开发位置识别机制，识别了泛化性、鲁棒性和环境变化等关键挑战，旨在通过连接动物导航研究和人类空间认知研究的见解来促进人工定位技术的创新。

Abstract: Place recognition, the ability to identify previously visited locations, is critical for both biological navigation and autonomous systems. This review synthesizes findings from robotic systems, animal studies, and human research to explore how different systems encode and recall place. We examine the computational and representational strategies employed across artificial systems, animals, and humans, highlighting convergent solutions such as topological mapping, cue integration, and memory management. Animal systems reveal evolved mechanisms for multimodal navigation and environmental adaptation, while human studies provide unique insights into semantic place concepts, cultural influences, and introspective capabilities. Artificial systems showcase scalable architectures and data-driven models. We propose a unifying set of concepts by which to consider and develop place recognition mechanisms and identify key challenges such as generalization, robustness, and environmental variability. This review aims to foster innovations in artificial localization by connecting future developments in artificial place recognition systems to insights from both animal navigation research and human spatial cognition studies.

</details>


### [14] [Perception-aware Exploration for Consumer-grade UAVs](https://arxiv.org/abs/2511.14393)
*Svetlana Seliunina,Daniel Schleich,Sven Behnke*

Main category: cs.RO

TL;DR: 将多无人机自主探索扩展到消费级无人机（如DJI Mini 3 Pro），提出视点对选择、轨迹规划和半分布式通信方案，在硬件限制下实现安全探索和地图重建。


<details>
  <summary>Details</summary>
Motivation: 将先进的多无人机自主探索技术应用到消费级无人机平台，克服硬件限制，实现低成本、高效的自主探索能力。

Method: 提出完整管道：选择可估计深度的视点对，规划满足运动约束的轨迹，采用半分布式通信方案平衡工作负载。

Result: 通过仿真验证了不同数量无人机的性能，证明即使在消费级无人机硬件限制下，仍能安全探索环境并重建地图。

Conclusion: 该方法成功将多无人机自主探索技术适配到消费级平台，为低成本自主探索应用提供了可行解决方案。

Abstract: In our work, we extend the current state-of-the-art approach for autonomous multi-UAV exploration to consumer-level UAVs, such as the DJI Mini 3 Pro. We propose a pipeline that selects viewpoint pairs from which the depth can be estimated and plans the trajectory that satisfies motion constraints necessary for odometry estimation. For the multi-UAV exploration, we propose a semi-distributed communication scheme that distributes the workload in a balanced manner. We evaluate our model performance in simulation for different numbers of UAVs and prove its ability to safely explore the environment and reconstruct the map even with the hardware limitations of consumer-grade UAVs.

</details>


### [15] [Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning](https://arxiv.org/abs/2511.14396)
*Xiuxiu Qi,Yu Yang,Jiannong Cao,Luyao Bai,Chongshan Fan,Chengtai Cao,Hongpeng Wang*

Main category: cs.RO

TL;DR: 提出了CCoL框架，通过视觉-语言-动作的连续协同学习解决行为克隆中的复合误差问题，确保时间一致性执行和细粒度语义对齐


<details>
  <summary>Details</summary>
Motivation: 克服行为克隆中序列动作决策的复合误差，解决现有方法存在的物理不连续性和语义-物理错位问题

Method: 通过视觉、语言和本体感觉输入的连续协同学习生成平滑动作轨迹，使用双向交叉注意力将语言语义锚定到视觉运动表示

Result: 在三个模拟套件中平均相对提升8.0%，在双手插入任务中相对增益达19.2%，在7自由度机器人上验证了泛化能力

Conclusion: CCoL框架有效解决了语义-物理错位问题，实现了稳健平滑的动作执行，在模拟和真实环境中均表现出色

Abstract: Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.

</details>


### [16] [Self-Supervised Multisensory Pretraining for Contact-Rich Robot Reinforcement Learning](https://arxiv.org/abs/2511.14427)
*Rickmer Krohn,Vignesh Prasad,Gabriele Tiboni,Georgia Chalvatzaki*

Main category: cs.RO

TL;DR: 提出MSDP框架，通过掩码自编码训练多感官表征，结合非对称架构实现快速学习和鲁棒控制，在接触丰富的机器人操作任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在多感官环境中学习困难的问题，特别是在感官噪声和动态变化的情况下

Method: 基于掩码自编码训练transformer编码器重建多感官观测，下游策略学习采用非对称架构，评论家使用交叉注意力提取动态特征，执行器接收稳定池化表征

Result: 在多种接触丰富的机器人操作任务中展示加速学习和鲁棒性能，对传感器噪声和物体动态变化具有强鲁棒性，真实机器人上仅需6000次在线交互即可实现高成功率

Conclusion: MSDP为复杂多感官机器人控制提供了简单而强大的解决方案

Abstract: Effective contact-rich manipulation requires robots to synergistically leverage vision, force, and proprioception. However, Reinforcement Learning agents struggle to learn in such multisensory settings, especially amidst sensory noise and dynamic changes. We propose MultiSensory Dynamic Pretraining (MSDP), a novel framework for learning expressive multisensory representations tailored for task-oriented policy learning. MSDP is based on masked autoencoding and trains a transformer-based encoder by reconstructing multisensory observations from only a subset of sensor embeddings, leading to cross-modal prediction and sensor fusion. For downstream policy learning, we introduce a novel asymmetric architecture, where a cross-attention mechanism allows the critic to extract dynamic, task-specific features from the frozen embeddings, while the actor receives a stable pooled representation to guide its actions. Our method demonstrates accelerated learning and robust performance under diverse perturbations, including sensor noise, and changes in object dynamics. Evaluations in multiple challenging, contact-rich robot manipulation tasks in simulation and the real world showcase the effectiveness of MSDP. Our approach exhibits strong robustness to perturbations and achieves high success rates on the real robot with as few as 6,000 online interactions, offering a simple yet powerful solution for complex multisensory robotic control.

</details>


### [17] [Mutation Testing for Industrial Robotic Systems](https://arxiv.org/abs/2511.14432)
*Marcela Gonçalves dos Santos,Sylvain Hallé,Fábio Petrillo*

Main category: cs.RO

TL;DR: 本文提出了针对工业机器人系统的领域特定变异测试方法，通过定义捕获机器人动作和传感器语义的变异算子，提高了测试套件的有效性。


<details>
  <summary>Details</summary>
Motivation: 工业机器人系统故障可能导致严重事故和高昂停机时间，传统变异测试算子不适用于涉及消息命令和物理世界交互的机器人程序。

Method: 提出在高层读写操作级别生成有意义的变异体，包括运动、夹具动作和传感器噪声注入等机器人特定操作。

Result: 在拾取放置场景的实证研究表明，该方法产生更具信息量的变异体，相比传统算子减少了无效或等价情况的数量。

Conclusion: 变异测试有潜力提高测试套件质量，为更安全可靠的工业机器人系统做出贡献。

Abstract: Industrial robotic systems (IRS) are increasingly deployed in diverse environments, where failures can result in severe accidents and costly downtime. Ensuring the reliability of the software controlling these systems is therefore critical. Mutation testing, a technique widely used in software engineering, evaluates the effectiveness of test suites by introducing small faults, or mutants, into the code. However, traditional mutation operators are poorly suited to robotic programs, which involve message-based commands and interactions with the physical world. This paper explores the adaptation of mutation testing to IRS by defining domain-specific mutation operators that capture the semantics of robot actions and sensor readings. We propose a methodology for generating meaningful mutants at the level of high-level read and write operations, including movement, gripper actions, and sensor noise injection. An empirical study on a pick-and-place scenario demonstrates that our approach produces more informative mutants and reduces the number of invalid or equivalent cases compared to conventional operators. Results highlight the potential of mutation testing to enhance test suite quality and contribute to safer, more reliable industrial robotic systems.

</details>


### [18] [Achieving Safe Control Online through Integration of Harmonic Control Lyapunov-Barrier Functions with Unsafe Object-Centric Action Policies](https://arxiv.org/abs/2511.14434)
*Marlow Fawn,Matthias Scheutz*

Main category: cs.RO

TL;DR: 提出一种将谐波控制李雅普诺夫-屏障函数与任意机器人策略结合的方法，将不安全策略转化为具有形式化保证的安全策略


<details>
  <summary>Details</summary>
Motivation: 需要将强化学习等训练得到的机器人策略与安全约束结合，确保在保持任务驱动行为的同时满足安全要求

Method: 使用从信号时序逻辑规范导出的谐波控制李雅普诺夫-屏障函数生成安全证书，与给定策略结合产生安全命令

Result: 在静止机械臂的移动任务中成功实现避障，将不安全策略转化为安全策略

Conclusion: 该方法能够为复杂规范和动态任务设置提供形式化安全保证

Abstract: We propose a method for combining Harmonic Control Lyapunov-Barrier Functions (HCLBFs) derived from Signal Temporal Logic (STL) specifications with any given robot policy to turn an unsafe policy into a safe one with formal guarantees.  The two components are combined via HCLBF-derived safety certificates, thus producing commands that preserve both safety and task-driven behavior.  We demonstrate with a simple proof-of-concept implementation for an object-centric force-based policy trained through reinforcement learning for a movement task of a stationary robot arm that is able to avoid colliding with obstacles on a table top after combining the policy with the safety constraints.  The proposed method can be generalized to more complex specifications and dynamic task settings.

</details>


### [19] [Advancing Minimally Invasive Precision Surgery in Open Cavities with Robotic Flexible Endoscopy](https://arxiv.org/abs/2511.14458)
*Michelle Mattille,Alexandre Mesot,Miriam Weisskopf,Nicole Ochsenbein-Kölble,Ueli Moehrlen,Bradley J. Nelson,Quentin Boehler*

Main category: cs.RO

TL;DR: 开发了一个用于开放式腔体微创手术的机器人平台，结合磁驱动柔性内窥镜、远程操作和半自主导航能力，通过实时场景拼接增强手术视野，并在羊模型中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 柔性机器人在微创手术中具有优势，但在开放式腔体中的内窥镜干预面临挑战，包括缺乏解剖约束、设备灵活性导致的控制困难，以及内窥镜视野有限影响情境感知。

Method: 结合磁驱动柔性内窥镜与远程操作和半自主导航能力，执行目标激光消融；通过重建实时内窥镜场景拼接图提供扩展的连续视觉上下文。

Result: 在羊模型中验证了该系统能够解决开放式空间微创手术的关键限制，特别是在胎儿镜激光凝固这一复杂手术中展示了潜力。

Conclusion: 该机器人平台成功克服了开放式腔体微创手术的挑战，为仅由经验丰富外科医生执行的复杂手术提供了可行的解决方案。

Abstract: Flexible robots hold great promise for enhancing minimally invasive surgery (MIS) by providing superior dexterity, precise control, and safe tissue interaction. Yet, translating these advantages into endoscopic interventions within open cavities remains challenging. The lack of anatomical constraints and the inherent flexibility of such devices complicate their control, while the limited field of view of endoscopes restricts situational awareness. We present a robotic platform designed to overcome these challenges and demonstrate its potential in fetoscopic laser coagulation, a complex MIS procedure typically performed only by highly experienced surgeons. Our system combines a magnetically actuated flexible endoscope with teleoperated and semi-autonomous navigation capabilities for performing targeted laser ablations. To enhance surgical awareness, the platform reconstructs real-time mosaics of the endoscopic scene, providing an extended and continuous visual context. The ability of this system to address the key limitations of MIS in open spaces is validated in vivo in an ovine model.

</details>


### [20] [Aerial Assistance System for Automated Firefighting during Turntable Ladder Operations](https://arxiv.org/abs/2511.14504)
*Jan Quenzel,Valerij Sekin,Daniel Schleich,Alexander Miller,Merlin Stampa,Norbert Pahlke,Christof Röhrig,Sven Behnke*

Main category: cs.RO

TL;DR: 提出了一种使用无人机和电动消防监视器的自动化消防辅助系统，通过无人机自主检测和定位热源，并自动引导消防水枪瞄准火源。


<details>
  <summary>Details</summary>
Motivation: 工业设施火灾由于建筑规模大、视觉障碍多，导致消防员难以准确判断火源位置，增加了整体损害并延长了灭火时间。

Method: 使用安装在转台梯上的电动消防监视器，配合无人机在空中自主飞行检测热源。无人机在基于地理数据的安全飞行通道内飞行，检测并定位热源，然后自动规划并移动到两个三角测量位置进行持续火源定位，同时系统自动引导消防水枪瞄准火源。

Result: 初步测试中，该辅助系统成功定位了多个热源，并将水枪准确导向火源。

Conclusion: 该系统能够有效提高工业火灾中火源定位的准确性，并通过自动化操作提升灭火效率。

Abstract: Fires in industrial facilities pose special challenges to firefighters, e.g., due to the sheer size and scale of the buildings. The resulting visual obstructions impair firefighting accuracy, further compounded by inaccurate assessments of the fire's location. Such imprecision simultaneously increases the overall damage and prolongs the fire-brigades operation unnecessarily.
  We propose an automated assistance system for firefighting using a motorized fire monitor on a turntable ladder with aerial support from an unmanned aerial vehicle (UAV). The UAV flies autonomously within an obstacle-free flight funnel derived from geodata, detecting and localizing heat sources. An operator supervises the operation on a handheld controller and selects a fire target in reach. After the selection, the UAV automatically plans and traverses between two triangulation poses for continued fire localization. Simultaneously, our system steers the fire monitor to ensure the water jet reaches the detected heat source. In preliminary tests, our assistance system successfully localized multiple heat sources and directed a water jet towards the fires.

</details>


### [21] [Masked IRL: LLM-Guided Reward Disambiguation from Demonstrations and Language](https://arxiv.org/abs/2511.14565)
*Minyoung Hwang,Alexandra Forsey-Smerek,Nathaniel Dennler,Andreea Bobu*

Main category: cs.RO

TL;DR: Masked IRL结合语言指令和演示数据，通过LLM推断状态相关性掩码，在数据有限时提升奖励学习的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统基于演示的奖励学习在数据有限时容易过拟合到虚假相关性，而语言指令可以更直接地指定任务重点，但现有方法未能充分利用语言来消除歧义。

Method: 提出Masked IRL框架，使用LLM从语言指令推断状态相关性掩码，强制模型对不相关状态组件保持不变性；当指令模糊时，利用LLM在演示上下文中澄清指令。

Result: 在仿真和真实机器人实验中，Masked IRL比现有语言条件IRL方法性能提升达15%，同时数据使用量减少4.7倍，显示出更好的样本效率、泛化能力和对模糊语言的鲁棒性。

Conclusion: 语言指令和演示数据提供互补信息，Masked IRL通过LLM有效整合两者，显著提升了奖励学习的效率和可靠性。

Abstract: Robots can adapt to user preferences by learning reward functions from demonstrations, but with limited data, reward models often overfit to spurious correlations and fail to generalize. This happens because demonstrations show robots how to do a task but not what matters for that task, causing the model to focus on irrelevant state details. Natural language can more directly specify what the robot should focus on, and, in principle, disambiguate between many reward functions consistent with the demonstrations. However, existing language-conditioned reward learning methods typically treat instructions as simple conditioning signals, without fully exploiting their potential to resolve ambiguity. Moreover, real instructions are often ambiguous themselves, so naive conditioning is unreliable. Our key insight is that these two input types carry complementary information: demonstrations show how to act, while language specifies what is important. We propose Masked Inverse Reinforcement Learning (Masked IRL), a framework that uses large language models (LLMs) to combine the strengths of both input types. Masked IRL infers state-relevance masks from language instructions and enforces invariance to irrelevant state components. When instructions are ambiguous, it uses LLM reasoning to clarify them in the context of the demonstrations. In simulation and on a real robot, Masked IRL outperforms prior language-conditioned IRL methods by up to 15% while using up to 4.7 times less data, demonstrating improved sample-efficiency, generalization, and robustness to ambiguous language. Project page: https://MIT-CLEAR-Lab.github.io/Masked-IRL and Code: https://github.com/MIT-CLEAR-Lab/Masked-IRL

</details>


### [22] [Is Your VLM for Autonomous Driving Safety-Ready? A Comprehensive Benchmark for Evaluating External and In-Cabin Risks](https://arxiv.org/abs/2511.14592)
*Xianhui Meng,Yuchen Zhang,Zhijian Huang,Zheng Lu,Ziling Ji,Yaoyao Yin,Hongyuan Zhang,Guangfeng Jiang,Yandan Lin,Long Chen,Hangjun Ye,Li Zhang,Jun Liu,Xiaoshuai Hao*

Main category: cs.RO

TL;DR: 提出了DSBench，首个全面的驾驶安全基准测试，用于统一评估视觉语言模型在外部环境风险和车内驾驶行为安全方面的表现。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在自动驾驶中前景广阔，但其在安全关键场景下的适用性尚未充分探索，缺乏同时评估外部环境风险和车内驾驶行为安全的综合基准测试。

Method: 构建DSBench基准，包含外部环境风险和车内驾驶行为安全两大类别，细分为10个关键类别和28个子类别，涵盖广泛的安全关键场景。

Result: 对各种主流开源和闭源视觉语言模型的广泛评估显示，在复杂安全关键情况下性能显著下降，凸显了紧迫的安全问题。

Conclusion: 构建了包含98K个实例的大型数据集，微调现有视觉语言模型可显著提升其安全性能，为推进自动驾驶技术铺平道路。

Abstract: Vision-Language Models (VLMs) show great promise for autonomous driving, but their suitability for safety-critical scenarios is largely unexplored, raising safety concerns. This issue arises from the lack of comprehensive benchmarks that assess both external environmental risks and in-cabin driving behavior safety simultaneously. To bridge this critical gap, we introduce DSBench, the first comprehensive Driving Safety Benchmark designed to assess a VLM's awareness of various safety risks in a unified manner. DSBench encompasses two major categories: external environmental risks and in-cabin driving behavior safety, divided into 10 key categories and a total of 28 sub-categories. This comprehensive evaluation covers a wide range of scenarios, ensuring a thorough assessment of VLMs' performance in safety-critical contexts. Extensive evaluations across various mainstream open-source and closed-source VLMs reveal significant performance degradation under complex safety-critical situations, highlighting urgent safety concerns. To address this, we constructed a large dataset of 98K instances focused on in-cabin and external safety scenarios, showing that fine-tuning on this dataset significantly enhances the safety performance of existing VLMs and paves the way for advancing autonomous driving technology. The benchmark toolkit, code, and model checkpoints will be publicly accessible.

</details>


### [23] [Gallant: Voxel Grid-based Humanoid Locomotion and Local-navigation across 3D Constrained Terrains](https://arxiv.org/abs/2511.14625)
*Qingwei Ben,Botian Xu,Kailin Li,Feiyu Jia,Wentao Zhang,Jingping Wang,Jingbo Wang,Dahua Lin,Jiangmiao Pang*

Main category: cs.RO

TL;DR: Gallant是一个基于体素网格的人形机器人3D约束地形运动框架，使用体素化LiDAR数据作为轻量级感知表示，通过z分组2D CNN实现端到端优化，在复杂地形中实现接近100%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度图像或高程图的人形机器人感知模块只能提供局部平坦的环境视图，无法捕捉完整的3D结构，限制了在复杂地形中的运动能力。

Method: 使用体素化LiDAR数据作为结构化感知表示，采用z分组2D CNN将感知映射到控制策略，开发高保真LiDAR模拟器支持可扩展训练。

Result: Gallant的广泛感知覆盖使单一策略能够处理地面障碍物之外的情况，包括侧向障碍、顶部约束、多层结构和狭窄通道，在楼梯攀爬和平台跨越等挑战性场景中实现接近100%的成功率。

Conclusion: 基于体素网格的感知框架Gallant通过端到端优化显著提升了人形机器人在3D约束地形中的运动能力，超越了现有方法的局限性。

Abstract: Robust humanoid locomotion requires accurate and globally consistent perception of the surrounding 3D environment. However, existing perception modules, mainly based on depth images or elevation maps, offer only partial and locally flattened views of the environment, failing to capture the full 3D structure. This paper presents Gallant, a voxel-grid-based framework for humanoid locomotion and local navigation in 3D constrained terrains. It leverages voxelized LiDAR data as a lightweight and structured perceptual representation, and employs a z-grouped 2D CNN to map this representation to the control policy, enabling fully end-to-end optimization. A high-fidelity LiDAR simulation that dynamically generates realistic observations is developed to support scalable, LiDAR-based training and ensure sim-to-real consistency. Experimental results show that Gallant's broader perceptual coverage facilitates the use of a single policy that goes beyond the limitations of previous methods confined to ground-level obstacles, extending to lateral clutter, overhead constraints, multi-level structures, and narrow passages. Gallant also firstly achieves near 100% success rates in challenging scenarios such as stair climbing and stepping onto elevated platforms through improved end-to-end optimization.

</details>


### [24] [NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards](https://arxiv.org/abs/2511.14659)
*Chia-Yu Hung,Navonil Majumder,Haoyuan Deng,Liu Renhang,Yankang Ang,Amir Zadeh,Chuan Li,Dorien Herremans,Ziwei Wang,Soujanya Poria*

Main category: cs.RO

TL;DR: NORA-1.5通过添加基于流匹配的动作专家和奖励模型驱动的后训练，显著提升了VLA模型在具身任务中的性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在不同具身系统和真实环境中的可靠性和泛化能力仍然不足，需要更稳健的解决方案。

Method: 在预训练的NORA骨干网络上添加流匹配动作专家，并开发结合世界模型和地面真值偏差启发式的奖励模型，通过直接偏好优化进行后训练。

Result: NORA-1.5在模拟和真实世界基准测试中超越了NORA和其他最先进VLA模型，奖励驱动的后训练显著提升了任务成功率。

Conclusion: NORA-1.5和奖励引导的后训练为开发适合真实世界部署的可靠具身智能体提供了可行路径。

Abstract: Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.

</details>


### [25] [Robust Verification of Controllers under State Uncertainty via Hamilton-Jacobi Reachability Analysis](https://arxiv.org/abs/2511.14755)
*Albert Lin,Alessandro Pinto,Somil Bansal*

Main category: cs.RO

TL;DR: 提出了RoVer-CoRe框架，首个基于Hamilton-Jacobi可达性分析的方法，用于验证感知不确定性下的感知控制系统安全性和鲁棒控制器设计。


<details>
  <summary>Details</summary>
Motivation: 随着基于感知的控制器在自主系统中越来越普及，需要形式化验证其在感知不确定性下的安全性和性能。现有验证方法要么限制控制器类型，要么分析过于保守，而HJ可达性分析在感知系统中的应用尚未充分探索。

Method: 提出RoVer-CoRe框架，通过将系统控制器、观测函数和状态估计模块串联，构建等效闭环系统，使其与现有可达性框架兼容。框架包含形式化安全验证和鲁棒控制器设计方法。

Result: 在飞机滑行和基于神经网络的漫游车导航案例研究中验证了框架的有效性。

Conclusion: RoVer-CoRe是首个基于HJ可达性分析的感知系统验证框架，能够处理感知不确定性下的安全验证和鲁棒控制器设计问题。

Abstract: As perception-based controllers for autonomous systems become increasingly popular in the real world, it is important that we can formally verify their safety and performance despite perceptual uncertainty. Unfortunately, the verification of such systems remains challenging, largely due to the complexity of the controllers, which are often nonlinear, nonconvex, learning-based, and/or black-box. Prior works propose verification algorithms that are based on approximate reachability methods, but they often restrict the class of controllers and systems that can be handled or result in overly conservative analyses. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for general nonlinear systems that can compute optimal reachable sets under worst-case system uncertainties; however, its application to perception-based systems is currently underexplored. In this work, we propose RoVer-CoRe, a framework for the Robust Verification of Controllers via HJ Reachability. To the best of our knowledge, RoVer-CoRe is the first HJ reachability-based framework for the verification of perception-based systems under perceptual uncertainty. Our key insight is to concatenate the system controller, observation function, and the state estimation modules to obtain an equivalent closed-loop system that is readily compatible with existing reachability frameworks. Within RoVer-CoRe, we propose novel methods for formal safety verification and robust controller design. We demonstrate the efficacy of the framework in case studies involving aircraft taxiing and NN-based rover navigation. Code is available at the link in the footnote.

</details>


### [26] [HMC: Learning Heterogeneous Meta-Control for Contact-Rich Loco-Manipulation](https://arxiv.org/abs/2511.14756)
*Lai Wei,Xuanbin Peng,Ri-Zhao Qiu,Tianshu Huang,Xuxin Cheng,Xiaolong Wang*

Main category: cs.RO

TL;DR: 提出了一个异构元控制框架，用于机器人定位操作，通过自适应结合位置、阻抗和混合力-位置控制模式，在真实人形机器人上实现了50%以上的性能提升


<details>
  <summary>Details</summary>
Motivation: 现实世界机器人演示学习面临复杂交互动态的挑战，纯位置控制器在处理接触或可变负载时表现不佳

Method: 提出异构元控制框架，包含HMC-Controller用于在扭矩空间连续混合不同控制模式的动作，以及HMC-Policy使用专家混合路由从大规模位置数据和精细力感知演示中学习

Result: 在真实人形机器人上的实验显示，在具有挑战性的任务如柔性桌面擦拭和抽屉开启中，相比基线方法有超过50%的相对性能提升

Conclusion: 异构元控制框架有效提升了机器人定位操作的性能，证明了该方法的有效性

Abstract: Learning from real-world robot demonstrations holds promise for interacting with complex real-world environments. However, the complexity and variability of interaction dynamics often cause purely positional controllers to struggle with contacts or varying payloads. To address this, we propose a Heterogeneous Meta-Control (HMC) framework for Loco-Manipulation that adaptively stitches multiple control modalities: position, impedance, and hybrid force-position. We first introduce an interface, HMC-Controller, for blending actions from different control profiles continuously in the torque space. HMC-Controller facilitates both teleoperation and policy deployment. Then, to learn a robust force-aware policy, we propose HMC-Policy to unify different controllers into a heterogeneous architecture. We adopt a mixture-of-experts style routing to learn from large-scale position-only data and fine-grained force-aware demonstrations. Experiments on a real humanoid robot show over 50% relative improvement vs. baselines on challenging tasks such as compliant table wiping and drawer opening, demonstrating the efficacy of HMC.

</details>
