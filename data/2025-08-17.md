<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [WiFi-based Global Localization in Large-Scale Environments Leveraging Structural Priors from osmAG](https://arxiv.org/abs/2508.10144)
*Xu Ma,Jiajie Zhang,Fujing Xie,Sören Schwertfeger*

Main category: cs.RO

TL;DR: 提出了一种基于WiFi和OpenStreetMap Area Graph (osmAG)的室内定位框架，结合信号传播建模和osmAG的先验信息，显著提高了定位精度和空间效率。


<details>
  <summary>Details</summary>
Motivation: 解决GPS信号缺失的室内环境中自主机器人的全局定位问题，利用现有WiFi基础设施和osmAG实现高效、可扩展的定位方案。

Method: 离线阶段通过迭代优化算法定位WiFi接入点（AP），在线阶段利用增强的osmAG地图进行实时机器人定位。

Result: 离线阶段AP定位误差均值3.79米（比三边测量提升35.3%）；在线阶段指纹区误差均值3.12米（比KNN指纹提升8.77%），非指纹区误差均值3.83米（提升81.05%）。

Conclusion: 该框架在复杂多楼层环境中验证有效，提供了一种可扩展、经济高效的室内机器人定位解决方案，解决了机器人绑架问题。

Abstract: Global localization is essential for autonomous robotics, especially in
indoor environments where the GPS signal is denied. We propose a novel
WiFi-based localization framework that leverages ubiquitous wireless
infrastructure and the OpenStreetMap Area Graph (osmAG) for large-scale indoor
environments. Our approach integrates signal propagation modeling with osmAG's
geometric and topological priors. In the offline phase, an iterative
optimization algorithm localizes WiFi Access Points (APs) by modeling wall
attenuation, achieving a mean localization error of 3.79 m (35.3\% improvement
over trilateration). In the online phase, real-time robot localization uses the
augmented osmAG map, yielding a mean error of 3.12 m in fingerprinted areas
(8.77\% improvement over KNN fingerprinting) and 3.83 m in non-fingerprinted
areas (81.05\% improvement). Comparison with a fingerprint-based method shows
that our approach is much more space efficient and achieves superior
localization accuracy, especially for positions where no fingerprint data are
available. Validated across a complex 11,025 &m^2& multi-floor environment,
this framework offers a scalable, cost-effective solution for indoor robotic
localization, solving the kidnapped robot problem. The code and dataset are
available at https://github.com/XuMa369/osmag-wifi-localization.

</details>


### [2] [Systematic Constraint Formulation and Collision-Free Trajectory Planning Using Space-Time Graphs of Convex Sets](https://arxiv.org/abs/2508.10203)
*Matthew D. Osburn,Cameron K. Peterson,John L. Salmon*

Main category: cs.RO

TL;DR: 论文提出了一种在动态环境中生成最优无碰撞轨迹的方法，利用Graphs of Convex Sets (GCS) 和 Space-Time Graphs of Convex Sets (ST-GCS) 框架，无需初始猜测即可求解。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，空间和时间约束使得数值求解器的初始猜测难以生成，需要一种无需初始猜测的方法来生成最优轨迹。

Method: 采用GCS和ST-GCS框架，通过凸集图和时间-空间凸集图生成最优轨迹，并探索了通用GCS兼容约束的推导。

Result: 在静态环境中，ST-GCS生成的轨迹与标准GCS等效；在动态环境中，ST-GCS能够找到最小距离的无碰撞轨迹。

Conclusion: ST-GCS框架在动态环境中有效生成最优无碰撞轨迹，无需初始猜测，具有实际应用潜力。

Abstract: In this paper, we create optimal, collision-free, time-dependent trajectories
through cluttered dynamic environments. The many spatial and temporal
constraints make finding an initial guess for a numerical solver difficult.
Graphs of Convex Sets (GCS) and the recently developed Space-Time Graphs of
Convex Sets formulation (ST-GCS) enable us to generate optimal minimum distance
collision-free trajectories without providing an initial guess to the solver.
We also explore the derivation of general GCS-compatible constraints and
document an intuitive strategy for adapting general constraints to the
framework. We show that ST-GCS produces equivalent trajectories to the standard
GCS formulation when the environment is static. We then show ST-GCS operating
in dynamic environments to find minimum distance collision-free trajectories.

</details>


### [3] [Hybrid Data-Driven Predictive Control for Robust and Reactive Exoskeleton Locomotion Synthesis](https://arxiv.org/abs/2508.10269)
*Kejun Li,Jeeseop Kim,Maxime Brunet,Marine Pétriaux,Yisong Yue,Aaron D. Ames*

Main category: cs.RO

TL;DR: 提出了一种混合数据驱动预测控制（HDDPC）框架，用于外骨骼的稳健双足运动，通过同时规划脚部接触计划和连续轨迹，提高了动态环境中的适应性。


<details>
  <summary>Details</summary>
Motivation: 外骨骼在动态环境中需要实时响应变化以实现稳健的双足运动，传统方法难以同时处理接触计划和轨迹规划。

Method: 采用Hankel矩阵建模系统动力学，结合步间（S2S）过渡，将接触调度与轨迹规划集成到一个统一的框架中。

Result: 在Atalante外骨骼上验证了该方法的有效性，显示出更强的鲁棒性和适应性。

Conclusion: HDDPC框架为外骨骼的稳健和反应性行走提供了一种高效的统一解决方案。

Abstract: Robust bipedal locomotion in exoskeletons requires the ability to dynamically
react to changes in the environment in real time. This paper introduces the
hybrid data-driven predictive control (HDDPC) framework, an extension of the
data-enabled predictive control, that addresses these challenges by
simultaneously planning foot contact schedules and continuous domain
trajectories. The proposed framework utilizes a Hankel matrix-based
representation to model system dynamics, incorporating step-to-step (S2S)
transitions to enhance adaptability in dynamic environments. By integrating
contact scheduling with trajectory planning, the framework offers an efficient,
unified solution for locomotion motion synthesis that enables robust and
reactive walking through online replanning. We validate the approach on the
Atalante exoskeleton, demonstrating improved robustness and adaptability.

</details>


### [4] [ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver](https://arxiv.org/abs/2508.10333)
*Wenxuan Song,Ziyang Zhou,Han Zhao,Jiayi Chen,Pengxiang Ding,Haodong Yan,Yuxin Huang,Feilong Tang,Donglin Wang,Haoang Li*

Main category: cs.RO

TL;DR: ReconVLA是一种通过重构目标区域来引导视觉注意力的VLA模型，解决了当前VLA模型中视觉注意力分散的问题。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在视觉注意力分配上表现不佳，注意力分散导致无法准确聚焦目标区域。

Method: 提出ReconVLA，利用扩散变换器重构图像中的目标区域（注视区域），促使模型学习细粒度表示并准确分配视觉注意力。

Result: 实验表明，ReconVLA在模拟和真实环境中均表现出精确操作和泛化能力。

Conclusion: ReconVLA通过隐式接地方法有效提升了VLA模型的视觉注意力分配和任务执行能力。

Abstract: Recent advances in Vision-Language-Action (VLA) models have enabled robotic
agents to integrate multimodal understanding with action execution. However,
our empirical analysis reveals that current VLAs struggle to allocate visual
attention to target regions. Instead, visual attention is always dispersed. To
guide the visual attention grounding on the correct target, we propose
ReconVLA, a reconstructive VLA model with an implicit grounding paradigm.
Conditioned on the model's visual outputs, a diffusion transformer aims to
reconstruct the gaze region of the image, which corresponds to the target
manipulated objects. This process prompts the VLA model to learn fine-grained
representations and accurately allocate visual attention, thus effectively
leveraging task-specific visual information and conducting precise
manipulation. Moreover, we curate a large-scale pretraining dataset comprising
over 100k trajectories and 2 million data samples from open-source robotic
datasets, further boosting the model's generalization in visual reconstruction.
Extensive experiments in simulation and the real world demonstrate the
superiority of our implicit grounding method, showcasing its capabilities of
precise manipulation and generalization. Our project page is
https://zionchow.github.io/ReconVLA/.

</details>


### [5] [BEASST: Behavioral Entropic Gradient based Adaptive Source Seeking for Mobile Robots](https://arxiv.org/abs/2508.10363)
*Donipolo Ghimire,Aamodh Suresh,Carlos Nieto-Granda,Solmaz S. Kia*

Main category: cs.RO

TL;DR: BEASST框架通过行为熵和概率加权函数，实现移动机器人在复杂未知环境中高效平衡探索与利用，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在未知环境中源寻找的挑战，通过动态调整行为策略以适应信号可靠性和任务紧迫性。

Method: 基于行为熵和Prelec概率加权函数，定义目标函数，动态调整机器人行为从风险规避到风险寻求。

Result: 实验验证显示，BEASST在路径长度减少15%和源定位速度提高20%上优于现有方法。

Conclusion: BEASST框架在理论和实践中均表现出色，为复杂环境中的源寻找提供了高效解决方案。

Abstract: This paper presents BEASST (Behavioral Entropic Gradient-based Adaptive
Source Seeking for Mobile Robots), a novel framework for robotic source seeking
in complex, unknown environments. Our approach enables mobile robots to
efficiently balance exploration and exploitation by modeling normalized signal
strength as a surrogate probability of source location. Building on Behavioral
Entropy(BE) with Prelec's probability weighting function, we define an
objective function that adapts robot behavior from risk-averse to risk-seeking
based on signal reliability and mission urgency. The framework provides
theoretical convergence guarantees under unimodal signal assumptions and
practical stability under bounded disturbances. Experimental validation across
DARPA SubT and multi-room scenarios demonstrates that BEASST consistently
outperforms state-of-the-art methods, achieving 15% reduction in path length
and 20% faster source localization through intelligent uncertainty-driven
navigation that dynamically transitions between aggressive pursuit and cautious
exploration.

</details>


### [6] [Few-shot Vision-based Human Activity Recognition with MLLM-based Visual Reinforcement Learning](https://arxiv.org/abs/2508.10371)
*Wenqi Zheng,Yutaka Arakawa*

Main category: cs.RO

TL;DR: 论文提出了一种名为FAVOR的方法，将视觉强化学习应用于多模态大型语言模型（MLLM），以提升少样本人类活动识别（HAR）的性能。


<details>
  <summary>Details</summary>
Motivation: 在少样本数据场景下，强化学习能够通过反馈优化模型输出，但在多模态人类活动识别领域的应用尚未充分探索。

Method: 利用MLLM生成候选响应，结合奖励函数和GRPO算法优化模型，实现少样本适应。

Result: 在四个HAR数据集和五种设置下的实验证明了方法的优越性。

Conclusion: FAVOR方法通过视觉强化学习显著提升了模型的泛化能力和推理能力，适用于少样本HAR任务。

Abstract: Reinforcement learning in large reasoning models enables learning from
feedback on their outputs, making it particularly valuable in scenarios where
fine-tuning data is limited. However, its application in multi-modal human
activity recognition (HAR) domains remains largely underexplored. Our work
extends reinforcement learning to the human activity recognition domain with
multimodal large language models. By incorporating visual reinforcement
learning in the training process, the model's generalization ability on
few-shot recognition can be greatly improved. Additionally, visual
reinforcement learning can enhance the model's reasoning ability and enable
explainable analysis in the inference stage. We name our few-shot human
activity recognition method with visual reinforcement learning FAVOR.
Specifically, our approach first utilizes a multimodal large language model
(MLLM) to generate multiple candidate responses for the human activity image,
each containing reasoning traces and final answers. These responses are then
evaluated using reward functions, and the MLLM model is subsequently optimized
using the Group Relative Policy Optimization (GRPO) algorithm. In this way, the
MLLM model can be adapted to human activity recognition with only a few
samples. Extensive experiments on four human activity recognition datasets and
five different settings demonstrate the superiority of the proposed method.

</details>


### [7] [A Semantic-Aware Framework for Safe and Intent-Integrative Assistance in Upper-Limb Exoskeletons](https://arxiv.org/abs/2508.10378)
*Yu Chen,Shu Miao,Chunyu Wu,Jingsong Mu,Bo OuYang,Xiang Li*

Main category: cs.RO

TL;DR: 论文提出了一种语义感知框架，将大语言模型集成到任务规划中，使上肢外骨骼能根据任务语义调整辅助配置，并通过异常检测和实时重规划确保安全。


<details>
  <summary>Details</summary>
Motivation: 现有上肢外骨骼解决方案缺乏对任务语义的理解和与用户的协作规划能力，限制了其通用性。

Method: 框架结合大语言模型提取任务语义，自动配置辅助参数；使用扩散异常检测器监控交互状态并触发实时重规划；在线轨迹优化和阻抗控制确保安全。

Result: 实验表明，该方法能有效适应用户认知、语义变化任务，并对异常做出可靠响应。

Conclusion: 提出的语义感知框架提升了上肢外骨骼的适应性和安全性，为家庭护理场景提供了更通用的解决方案。

Abstract: Upper-limb exoskeletons are primarily designed to provide assistive support
by accurately interpreting and responding to human intentions. In home-care
scenarios, exoskeletons are expected to adapt their assistive configurations
based on the semantic information of the task, adjusting appropriately in
accordance with the nature of the object being manipulated. However, existing
solutions often lack the ability to understand task semantics or
collaboratively plan actions with the user, limiting their generalizability. To
address this challenge, this paper introduces a semantic-aware framework that
integrates large language models into the task planning framework, enabling the
delivery of safe and intent-integrative assistance. The proposed approach
begins with the exoskeleton operating in transparent mode to capture the
wearer's intent during object grasping. Once semantic information is extracted
from the task description, the system automatically configures appropriate
assistive parameters. In addition, a diffusion-based anomaly detector is used
to continuously monitor the state of human-robot interaction and trigger
real-time replanning in response to detected anomalies. During task execution,
online trajectory refinement and impedance control are used to ensure safety
and regulate human-robot interaction. Experimental results demonstrate that the
proposed method effectively aligns with the wearer's cognition, adapts to
semantically varying tasks, and responds reliably to anomalies.

</details>


### [8] [Super LiDAR Reflectance for Robotic Perception](https://arxiv.org/abs/2508.10398)
*Wei Gao,Jie Zhang,Mingle Zhao,Zhiyuan Zhang,Shu Kong,Maani Ghaffari,Dezhen Song,Cheng-Zhong Xu,Hui Kong*

Main category: cs.RO

TL;DR: 论文提出了一种新框架，利用非重复扫描LiDAR（NRS-LiDAR）从稀疏数据生成密集LiDAR反射图像，解决了低成本LiDAR数据稀疏的问题。


<details>
  <summary>Details</summary>
Motivation: 传统上，主动光学传感未被视为主流视觉模态，但技术进步使其潜力显现。低成本LiDAR数据稀疏限制了其应用，需要解决方案。

Method: 提出了一种创新框架，包括反射率校准和从静态到动态场景的转换，利用NRS-LiDAR特性生成密集反射图像。

Result: 开发了LiDAR反射图像密集化的数据集和专用网络，并展示了在闭环检测和交通车道检测等应用中的效果。

Conclusion: 该工作通过生成密集反射图像，扩展了低成本LiDAR的应用范围，推动了主动视觉的发展。

Abstract: Conventionally, human intuition often defines vision as a modality of passive
optical sensing, while active optical sensing is typically regarded as
measuring rather than the default modality of vision. However, the situation
now changes: sensor technologies and data-driven paradigms empower active
optical sensing to redefine the boundaries of vision, ushering in a new era of
active vision. Light Detection and Ranging (LiDAR) sensors capture reflectance
from object surfaces, which remains invariant under varying illumination
conditions, showcasing significant potential in robotic perception tasks such
as detection, recognition, segmentation, and Simultaneous Localization and
Mapping (SLAM). These applications often rely on dense sensing capabilities,
typically achieved by high-resolution, expensive LiDAR sensors. A key challenge
with low-cost LiDARs lies in the sparsity of scan data, which limits their
broader application. To address this limitation, this work introduces an
innovative framework for generating dense LiDAR reflectance images from sparse
data, leveraging the unique attributes of non-repeating scanning LiDAR
(NRS-LiDAR). We tackle critical challenges, including reflectance calibration
and the transition from static to dynamic scene domains, facilitating the
reconstruction of dense reflectance images in real-world settings. The key
contributions of this work include a comprehensive dataset for LiDAR
reflectance image densification, a densification network tailored for
NRS-LiDAR, and diverse applications such as loop closure and traffic lane
detection using the generated dense reflectance images.

</details>


### [9] [Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning](https://arxiv.org/abs/2508.10399)
*Wenlong Liang,Rui Zhou,Yang Ma,Bing Zhang,Songlin Li,Yijia Liao,Ping Kuang*

Main category: cs.RO

TL;DR: 本文综述了大模型赋能的具身AI，重点探讨了自主决策和具身学习，分析了分层与端到端决策范式，并介绍了大模型如何增强模仿学习和强化学习。


<details>
  <summary>Details</summary>
Motivation: 具身AI是实现通用人工智能（AGI）的重要途径，但现有系统在开放动态环境中仍难以达到人类水平。大模型的突破为具身AI提供了新的可能性。

Method: 通过分层和端到端决策范式分析大模型在高层规划、低层执行和反馈中的作用，并探讨其在模仿学习和强化学习中的应用。首次将世界模型纳入具身AI的综述。

Result: 大模型显著提升了具身AI的感知、交互、规划和学习能力，但仍存在挑战。

Conclusion: 尽管取得了进展，具身AI仍需进一步研究以解决现有挑战，推动AGI的实现。

Abstract: Embodied AI aims to develop intelligent systems with physical forms capable
of perceiving, decision-making, acting, and learning in real-world
environments, providing a promising way to Artificial General Intelligence
(AGI). Despite decades of explorations, it remains challenging for embodied
agents to achieve human-level intelligence for general-purpose tasks in open
dynamic environments. Recent breakthroughs in large models have revolutionized
embodied AI by enhancing perception, interaction, planning and learning. In
this article, we provide a comprehensive survey on large model empowered
embodied AI, focusing on autonomous decision-making and embodied learning. We
investigate both hierarchical and end-to-end decision-making paradigms,
detailing how large models enhance high-level planning, low-level execution,
and feedback for hierarchical decision-making, and how large models enhance
Vision-Language-Action (VLA) models for end-to-end decision making. For
embodied learning, we introduce mainstream learning methodologies, elaborating
on how large models enhance imitation learning and reinforcement learning
in-depth. For the first time, we integrate world models into the survey of
embodied AI, presenting their design methods and critical roles in enhancing
decision-making and learning. Though solid advances have been achieved,
challenges still exist, which are discussed at the end of this survey,
potentially as the further research directions.

</details>


### [10] [CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model](https://arxiv.org/abs/2508.10416)
*Zhuoyuan Yu,Yuxing Long,Zihan Yang,Chengyan Zeng,Hongwei Fan,Jiyao Zhang,Hao Dong*

Main category: cs.RO

TL;DR: 提出了一种名为Self-correction Flywheel的后训练范式，通过利用模型的错误轨迹生成自校正数据，逐步提升视觉-语言导航模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言导航模型在执行指令时容易偏离正确轨迹，且缺乏有效的错误校正能力。

Method: 利用模型的错误轨迹作为数据源，识别偏差并自动生成自校正数据，通过多次迭代训练提升模型性能。

Result: 在R2R-CE和RxR-CE基准测试中，CorrectNav模型分别达到65.1%和69.3%的成功率，优于之前最佳模型。

Conclusion: Self-correction Flywheel范式显著提升了模型的错误校正能力和导航性能，适用于动态障碍物避障和长指令跟随。

Abstract: Existing vision-and-language navigation models often deviate from the correct
trajectory when executing instructions. However, these models lack effective
error correction capability, hindering their recovery from errors. To address
this challenge, we propose Self-correction Flywheel, a novel post-training
paradigm. Instead of considering the model's error trajectories on the training
set as a drawback, our paradigm emphasizes their significance as a valuable
data source. We have developed a method to identify deviations in these error
trajectories and devised innovative techniques to automatically generate
self-correction data for perception and action. These self-correction data
serve as fuel to power the model's continued training. The brilliance of our
paradigm is revealed when we re-evaluate the model on the training set,
uncovering new error trajectories. At this time, the self-correction flywheel
begins to spin. Through multiple flywheel iterations, we progressively enhance
our monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE
and RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success
rates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2%
and 16.4%. Real robot tests in various indoor and outdoor environments
demonstrate \method's superior capability of error correction, dynamic obstacle
avoidance, and long instruction following.

</details>


### [11] [MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion](https://arxiv.org/abs/2508.10423)
*Qi Liu,Xiaopeng Zhang,Mingshan Tan,Shuaikang Ma,Jinliang Ding,Yanjie Li*

Main category: cs.RO

TL;DR: 提出了一种名为MASH的新方法，通过协作异构多智能体深度强化学习（MARL）优化单个人形机器人的运动能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常采用单智能体强化学习或多智能体强化学习用于多机器人系统任务，而本研究探索了将协作异构MARL应用于单个人形机器人运动优化的新范式。

Method: MASH方法将每个肢体（腿和手臂）视为独立智能体，探索机器人动作空间，同时共享全局批评器进行协作学习。

Result: 实验表明，MASH加速了训练收敛并提高了全身协作能力，优于传统的单智能体强化学习方法。

Conclusion: 该研究推动了MARL在单个人形机器人控制中的应用，为高效运动策略提供了新见解。

Abstract: This paper proposes a novel method to enhance locomotion for a single
humanoid robot through cooperative-heterogeneous multi-agent deep reinforcement
learning (MARL). While most existing methods typically employ single-agent
reinforcement learning algorithms for a single humanoid robot or MARL
algorithms for multi-robot system tasks, we propose a distinct paradigm:
applying cooperative-heterogeneous MARL to optimize locomotion for a single
humanoid robot. The proposed method, multi-agent reinforcement learning for
single humanoid locomotion (MASH), treats each limb (legs and arms) as an
independent agent that explores the robot's action space while sharing a global
critic for cooperative learning. Experiments demonstrate that MASH accelerates
training convergence and improves whole-body cooperation ability, outperforming
conventional single-agent reinforcement learning methods. This work advances
the integration of MARL into single-humanoid-robot control, offering new
insights into efficient locomotion strategies.

</details>


### [12] [Enabling Generic Robot Skill Implementation Using Object Oriented Programming](https://arxiv.org/abs/2508.10497)
*Abdullah Farrukh,Achim Wagner,Martin Ruskowski*

Main category: cs.RO

TL;DR: 提出了一种简化机器人系统接口的软件框架，旨在降低部署机器人系统的复杂性，特别针对中小企业和研究人员。


<details>
  <summary>Details</summary>
Motivation: 中小企业和研究人员在缺乏机器人专业知识的情况下，部署和维护机器人系统面临挑战，且容易依赖外部集成商导致供应商锁定。

Method: 使用Python实现一个原型框架，通过抽象层统一不同制造商和型号的机器人接口。

Result: 开发了一个原型框架，专注于简化现代机器人系统的接口，适用于分拣单元中的Yaskawa Motoman GP4。

Conclusion: 该框架为中小企业和研究人员提供了一种简化机器人系统部署的解决方案，减少了对外部专家的依赖。

Abstract: Developing robotic algorithms and integrating a robotic subsystem into a
larger system can be a difficult task. Particularly in small and medium-sized
enterprises (SMEs) where robotics expertise is lacking, implementing,
maintaining and developing robotic systems can be a challenge. As a result,
many companies rely on external expertise through system integrators, which, in
some cases, can lead to vendor lock-in and external dependency. In the academic
research on intelligent manufacturing systems, robots play a critical role in
the design of robust autonomous systems. Similar challenges are faced by
researchers who want to use robotic systems as a component in a larger smart
system, without having to deal with the complexity and vastness of the robot
interfaces in detail. In this paper, we propose a software framework that
reduces the effort required to deploy a working robotic system. The focus is
solely on providing a concept for simplifying the different interfaces of a
modern robot system and using an abstraction layer for different manufacturers
and models. The Python programming language is used to implement a prototype of
the concept. The target system is a bin-picking cell containing a Yaskawa
Motoman GP4.

</details>


### [13] [KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection](https://arxiv.org/abs/2508.10511)
*Andrea Rosasco,Federico Ceola,Giulia Pasquale,Lorenzo Natale*

Main category: cs.RO

TL;DR: KDPE提出了一种基于核密度估计的策略，用于过滤Diffusion Policy生成的有害轨迹，同时保持较低的计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决Diffusion Policy在机器人策略执行中可能偏离数据分布的问题，包括去噪过程的随机性和数据异常值的影响。

Method: 使用核密度估计（KDE）和流形感知核来建模动作的概率密度函数，过滤Diffusion Policy的输出。

Result: 在模拟单臂任务和真实机器人实验中，KDPE表现优于Diffusion Policy。

Conclusion: KDPE通过核密度估计有效提升了机器人策略的性能和稳定性。

Abstract: Learning robot policies that capture multimodality in the training data has
been a long-standing open challenge for behavior cloning. Recent approaches
tackle the problem by modeling the conditional action distribution with
generative models. One of these approaches is Diffusion Policy, which relies on
a diffusion model to denoise random points into robot action trajectories.
While achieving state-of-the-art performance, it has two main drawbacks that
may lead the robot out of the data distribution during policy execution. First,
the stochasticity of the denoising process can highly impact on the quality of
generated trajectory of actions. Second, being a supervised learning approach,
it can learn data outliers from the dataset used for training. Recent work
focuses on mitigating these limitations by combining Diffusion Policy either
with large-scale training or with classical behavior cloning algorithms.
Instead, we propose KDPE, a Kernel Density Estimation-based strategy that
filters out potentially harmful trajectories output of Diffusion Policy while
keeping a low test-time computational overhead. For Kernel Density Estimation,
we propose a manifold-aware kernel to model a probability density function for
actions composed of end-effector Cartesian position, orientation, and gripper
state. KDPE overall achieves better performance than Diffusion Policy on
simulated single-arm tasks and real robot experiments.
  Additional material and code are available on our project page
https://hsp-iit.github.io/KDPE/.

</details>


### [14] [MLM: Learning Multi-task Loco-Manipulation Whole-Body Control for Quadruped Robot with Arm](https://arxiv.org/abs/2508.10538)
*Xin Liu,Bida Ma,Chenkun Qi,Yan Ding,Zhaxizhuoma,Guorong Zhang,Pengan Chen,Kehui Liu,Zhongjie Jia,Chuyue Guan,Yule Mo,Jiaqi Liu,Feng Gao,Jiangwei Zhong,Bin Zhao,Xuelong Li*

Main category: cs.RO

TL;DR: 提出了一种名为MLM的强化学习框架，结合真实世界和仿真数据，使配备六自由度机械臂的四足机器人能够自主或通过远程操作完成全身运动与操作任务。


<details>
  <summary>Details</summary>
Motivation: 解决四足机器人在全身运动与操作中的多任务控制难题。

Method: 引入轨迹库和自适应课程采样机制，提出轨迹-速度预测策略网络，结合仿真数据和课程奖励。

Result: 仿真和真实世界实验验证了方法的有效性和零样本迁移能力。

Conclusion: MLM框架在多任务执行中表现良好，为四足机器人的全身运动与操作提供了有效解决方案。

Abstract: Whole-body loco-manipulation for quadruped robots with arm remains a
challenging problem, particularly in achieving multi-task control. To address
this, we propose MLM, a reinforcement learning framework driven by both
real-world and simulation data. It enables a six-DoF robotic arm--equipped
quadruped robot to perform whole-body loco-manipulation for multiple tasks
autonomously or under human teleoperation. To address the problem of balancing
multiple tasks during the learning of loco-manipulation, we introduce a
trajectory library with an adaptive, curriculum-based sampling mechanism. This
approach allows the policy to efficiently leverage real-world collected
trajectories for learning multi-task loco-manipulation. To address deployment
scenarios with only historical observations and to enhance the performance of
policy execution across tasks with different spatial ranges, we propose a
Trajectory-Velocity Prediction policy network. It predicts unobservable future
trajectories and velocities. By leveraging extensive simulation data and
curriculum-based rewards, our controller achieves whole-body behaviors in
simulation and zero-shot transfer to real-world deployment. Ablation studies in
simulation verify the necessity and effectiveness of our approach, while
real-world experiments on the Go2 robot with an Airbot robotic arm demonstrate
the policy's good performance in multi-task execution.

</details>


### [15] [Why Report Failed Interactions With Robots?! Towards Vignette-based Interaction Quality](https://arxiv.org/abs/2508.10603)
*Agnes Axelsson,Merle Reimann,Ronald Cumbal,Hannah Pelikan,Divesh Lala*

Main category: cs.RO

TL;DR: 论文提出使用民族志小故事（ethnographic vignettes）来突显人机交互（HRI）中的失败案例，弥补现有研究的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs提升了人机交互质量，但与人人交互相比仍存在不足，且失败案例因情境而异，难以泛化。

Method: 提出通过撰写民族志小故事的方法，基于个人经验记录HRI中的失败案例。

Result: 小故事能多学科视角展示失败、提升机器人能力透明度，并记录研究中遗漏的意外行为。

Conclusion: 建议将小故事作为现有交互评估方法的补充工具。

Abstract: Although the quality of human-robot interactions has improved with the advent
of LLMs, there are still various factors that cause systems to be sub-optimal
when compared to human-human interactions. The nature and criticality of
failures are often dependent on the context of the interaction and so cannot be
generalized across the wide range of scenarios and experiments which have been
implemented in HRI research. In this work we propose the use of a technique
overlooked in the field of HRI, ethnographic vignettes, to clearly highlight
these failures, particularly those that are rarely documented. We describe the
methodology behind the process of writing vignettes and create our own based on
our personal experiences with failures in HRI systems. We emphasize the
strength of vignettes as the ability to communicate failures from a
multi-disciplinary perspective, promote transparency about the capabilities of
robots, and document unexpected behaviours which would otherwise be omitted
from research reports. We encourage the use of vignettes to augment existing
interaction evaluation methods.

</details>


### [16] [Synthesis of Deep Neural Networks with Safe Robust Adaptive Control for Reliable Operation of Wheeled Mobile Robots](https://arxiv.org/abs/2508.10634)
*Mehdi Heydari Shahna,Jouni Mattila*

Main category: cs.RO

TL;DR: 论文提出了一种结合深度神经网络（DNN）和鲁棒自适应控制（RAC）的分层控制策略，用于重型轮式移动机器人（WMR），以确保高精度控制和安全性。


<details>
  <summary>Details</summary>
Motivation: 重型WMR在严格国际标准和易受干扰的环境下运行，传统黑盒方法难以满足需求，因此需要一种兼顾高精度和安全的控制策略。

Method: 设计了一个分层控制策略，包括DNN作为主控制策略和两级安全层（低层RAC和高层监控），以应对不同强度的干扰。

Result: 实验验证了该策略在6000公斤WMR上的有效性，确保了系统的稳定性和安全性。

Conclusion: DNN与RAC的结合为重型WMR提供了一种高效且安全的控制解决方案。

Abstract: Deep neural networks (DNNs) can enable precise control while maintaining low
computational costs by circumventing the need for dynamic modeling. However,
the deployment of such black-box approaches remains challenging for heavy-duty
wheeled mobile robots (WMRs), which are subject to strict international
standards and prone to faults and disturbances. We designed a hierarchical
control policy for heavy-duty WMRs, monitored by two safety layers with
differing levels of authority. To this end, a DNN policy was trained and
deployed as the primary control strategy, providing high-precision performance
under nominal operating conditions. When external disturbances arise and reach
a level of intensity such that the system performance falls below a predefined
threshold, a low-level safety layer intervenes by deactivating the primary
control policy and activating a model-free robust adaptive control (RAC)
policy. This transition enables the system to continue operating while ensuring
stability by effectively managing the inherent trade-off between system
robustness and responsiveness. Regardless of the control policy in use, a
high-level safety layer continuously monitors system performance during
operation. It initiates a shutdown only when disturbances become sufficiently
severe such that compensation is no longer viable and continued operation would
jeopardize the system or its environment. The proposed synthesis of DNN and RAC
policy guarantees uniform exponential stability of the entire WMR system while
adhering to safety standards to some extent. The effectiveness of the proposed
approach was further validated through real-time experiments using a 6,000 kg
WMR.

</details>


### [17] [An Open-Source User-Friendly Interface for Simulating Magnetic Soft Robots using Simulation Open Framework Architecture (SOFA)](https://arxiv.org/abs/2508.10686)
*Carla Wehner,Finn Schubert,Heiko Hellkamp,Julius Hahnewald,Kilian Scheafer,Muhammad Bilal Khan,Oliver Gutfleisch*

Main category: cs.RO

TL;DR: 本文介绍了一个基于SOFA的开源、用户友好的仿真工具，专门用于模拟磁性软体机器人，旨在填补理论建模与实际设计之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 现有仿真平台缺乏对磁性材料的专门支持，难以满足不同专业水平研究人员的需求。

Method: 使用SOFA框架开发了一个仿真界面，支持定义材料属性、施加磁场并实时观察变形，同时集成了直观控制和应力分析功能。

Result: 通过四个基准模型（梁、三指和四指夹持器、蝴蝶）验证了工具的功能性，其易用性适合初学者和高级研究人员。

Conclusion: 未来将通过实验验证和与行业标准有限元求解器的对比，进一步提高仿真精度，确保模拟的预测性和真实性。

Abstract: Soft robots, particularly magnetic soft robots, require specialized
simulation tools to accurately model their deformation under external magnetic
fields. However, existing platforms often lack dedicated support for magnetic
materials, making them difficult to use for researchers at different expertise
levels. This work introduces an open-source, user-friendly simulation interface
using the Simulation Open Framework Architecture (SOFA), specifically designed
to model magnetic soft robots. The tool enables users to define material
properties, apply magnetic fields, and observe resulting deformations in real
time. By integrating intuitive controls and stress analysis capabilities, it
aims to bridge the gap between theoretical modeling and practical design. Four
benchmark models - a beam, three- and four-finger grippers, and a butterfly -
demonstrate its functionality. The software's ease of use makes it accessible
to both beginners and advanced researchers. Future improvements will refine
accuracy through experimental validation and comparison with industry-standard
finite element solvers, ensuring realistic and predictive simulations of
magnetic soft robots.

</details>


### [18] [Biasing Frontier-Based Exploration with Saliency Areas](https://arxiv.org/abs/2508.10689)
*Matteo Luperto,Valerii Stakanov,Giacomo Boracchi,Nicola Basilico,Francesco Amigoni*

Main category: cs.RO

TL;DR: 论文提出了一种基于显著性区域的自主探索方法，通过神经网络识别关键区域，优化机器人的探索策略。


<details>
  <summary>Details</summary>
Motivation: 现有探索策略通常只关注最大化探索面积，而忽略了环境中某些区域的重要性。这些关键区域可能引导发现更大的未知区域。

Method: 使用神经网络生成的显著性地图识别关键区域（显著性区域），并将其用于优化常见的探索策略。

Result: 实验表明，利用显著性区域可以显著影响机器人在探索过程中的行为。

Conclusion: 通过引入显著性区域，可以更高效地完成环境探索任务。

Abstract: Autonomous exploration is a widely studied problem where a robot
incrementally builds a map of a previously unknown environment. The robot
selects the next locations to reach using an exploration strategy. To do so,
the robot has to balance between competing objectives, like exploring the
entirety of the environment, while being as fast as possible. Most exploration
strategies try to maximise the explored area to speed up exploration; however,
they do not consider that parts of the environment are more important than
others, as they lead to the discovery of large unknown areas. We propose a
method that identifies \emph{saliency areas} as those areas that are of high
interest for exploration, by using saliency maps obtained from a neural network
that, given the current map, implements a termination criterion to estimate
whether the environment can be considered fully-explored or not. We use
saliency areas to bias some widely used exploration strategies, showing, with
an extensive experimental campaign, that this knowledge can significantly
influence the behavior of the robot during exploration.

</details>


### [19] [Learning Task Execution Hierarchies for Redundant Robots](https://arxiv.org/abs/2508.10780)
*Alessandro Adami,Aris Synodinos,Matteo Iovino,Ruggero Carli,Pietro Falco*

Main category: cs.RO

TL;DR: 论文提出了一种自动学习任务堆叠（SoT）层次和参数的新框架，结合强化学习和遗传编程，无需人工干预即可发现任务优先级和控制策略。


<details>
  <summary>Details</summary>
Motivation: 现代机器人系统的高冗余性需要灵活可靠的管理方法，传统SoT依赖专家手动设计，限制了适应性和可访问性。

Method: 结合强化学习和遗传编程，通过基于精度、安全性和执行时间等指标的成本函数指导学习过程。

Result: 在移动-YuMi平台上验证，结果表明学习的SoT使机器人能动态适应环境变化，平衡竞争目标并保持任务执行的鲁棒性。

Conclusion: 该方法为复杂机器人冗余管理提供了一种通用且用户友好的解决方案，减少了专家设计的依赖。

Abstract: Modern robotic systems, such as mobile manipulators, humanoids, and aerial
robots with arms, often possess high redundancy, enabling them to perform
multiple tasks simultaneously. Managing this redundancy is key to achieving
reliable and flexible behavior. A widely used approach is the Stack of Tasks
(SoT), which organizes control objectives by priority within a unified
framework. However, traditional SoTs are manually designed by experts, limiting
their adaptability and accessibility. This paper introduces a novel framework
that automatically learns both the hierarchy and parameters of a SoT from
user-defined objectives. By combining Reinforcement Learning and Genetic
Programming, the system discovers task priorities and control strategies
without manual intervention. A cost function based on intuitive metrics such as
precision, safety, and execution time guides the learning process. We validate
our method through simulations and experiments on the mobile-YuMi platform, a
dual-arm mobile manipulator with high redundancy. Results show that the learned
SoTs enable the robot to dynamically adapt to changing environments and inputs,
balancing competing objectives while maintaining robust task execution. This
approach provides a general and user-friendly solution for redundancy
management in complex robots, advancing human-centered robot programming and
reducing the need for expert design.

</details>


### [20] [The SET Perceptual Factors Framework: Towards Assured Perception for Autonomous Systems](https://arxiv.org/abs/2508.10798)
*Troi Williams*

Main category: cs.RO

TL;DR: 论文提出了SET感知因素框架，用于系统分析环境因素对机器人感知的影响，以提高自主系统的安全性和可信度。


<details>
  <summary>Details</summary>
Motivation: 自主系统的部署引发了对安全性和可信度的担忧，尤其是感知可靠性问题。感知失败常由环境因素引起，可能导致事故，影响公众信任。

Method: 引入SET框架，包括SET状态树和SET因素树，用于分类和建模感知任务中的不确定性来源。开发感知因素模型以量化不确定性。

Result: SET框架提供了一种透明、标准化的方法，用于识别、建模和传达感知风险。

Conclusion: 该框架旨在提升自主系统的安全保证，增强公众理解和信任。

Abstract: Future autonomous systems promise significant societal benefits, yet their
deployment raises concerns about safety and trustworthiness. A key concern is
assuring the reliability of robot perception, as perception seeds safe
decision-making. Failures in perception are often due to complex yet common
environmental factors and can lead to accidents that erode public trust. To
address this concern, we introduce the SET (Self, Environment, and Target)
Perceptual Factors Framework. We designed the framework to systematically
analyze how factors such as weather, occlusion, or sensor limitations
negatively impact perception. To achieve this, the framework employs SET State
Trees to categorize where such factors originate and SET Factor Trees to model
how these sources and factors impact perceptual tasks like object detection or
pose estimation. Next, we develop Perceptual Factor Models using both trees to
quantify the uncertainty for a given task. Our framework aims to promote
rigorous safety assurances and cultivate greater public understanding and trust
in autonomous systems by offering a transparent and standardized method for
identifying, modeling, and communicating perceptual risks.

</details>


### [21] [A Multimodal Neural Network for Recognizing Subjective Self-Disclosure Towards Social Robots](https://arxiv.org/abs/2508.10828)
*Henry Powell,Guy Laban,Emily S. Cross*

Main category: cs.RO

TL;DR: 本文提出了一种基于情感识别文献的多模态注意力网络，用于建模人类与机器人互动中的主观自我披露行为，并通过新的损失函数显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着社交机器人在各种社交场景中的应用增加，需要其能够理解和响应人类的自我披露行为，但目前缺乏相关计算模型。

Method: 开发了一种多模态注意力网络，基于情感识别模型，使用自收集的视频语料库进行训练，并提出了尺度保持交叉熵损失函数。

Result: 最佳模型F1分数达到0.83，比基线模型提高了0.48。

Conclusion: 该研究为社交机器人识别人类自我披露行为提供了重要进展，对社交认知能力的发展至关重要。

Abstract: Subjective self-disclosure is an important feature of human social
interaction. While much has been done in the social and behavioural literature
to characterise the features and consequences of subjective self-disclosure,
little work has been done thus far to develop computational systems that are
able to accurately model it. Even less work has been done that attempts to
model specifically how human interactants self-disclose with robotic partners.
It is becoming more pressing as we require social robots to work in conjunction
with and establish relationships with humans in various social settings. In
this paper, our aim is to develop a custom multimodal attention network based
on models from the emotion recognition literature, training this model on a
large self-collected self-disclosure video corpus, and constructing a new loss
function, the scale preserving cross entropy loss, that improves upon both
classification and regression versions of this problem. Our results show that
the best performing model, trained with our novel loss function, achieves an F1
score of 0.83, an improvement of 0.48 from the best baseline model. This result
makes significant headway in the aim of allowing social robots to pick up on an
interaction partner's self-disclosures, an ability that will be essential in
social robots with social cognition.

</details>


### [22] [CVIRO: A Consistent and Tightly-Coupled Visual-Inertial-Ranging Odometry on Lie Groups](https://arxiv.org/abs/2508.10867)
*Yizhi Zhou,Ziwei Kang,Jiawei Xia,Xuan Wang*

Main category: cs.RO

TL;DR: 提出了一种基于李群的紧密耦合视觉-惯性-测距里程计系统（CVIRO），解决了UWB辅助VIO系统中的不一致性问题，提高了定位精度和一致性。


<details>
  <summary>Details</summary>
Motivation: UWB辅助VIO系统的不一致性主要由系统可观测性未正确保持和UWB锚点位置假设已知导致校准不确定性被忽略引起，影响了定位性能。

Method: 将UWB锚点状态纳入系统状态，显式考虑UWB校准不确定性，利用李群的不变误差特性确保可观测一致性。

Result: 通过分析和实验证明，CVIRO算法自然保持了系统的正确不可观测子空间，显著提高了定位精度和一致性。

Conclusion: CVIRO系统在定位精度和一致性上优于现有方法，为UWB辅助VIO系统提供了一种有效的解决方案。

Abstract: Ultra Wideband (UWB) is widely used to mitigate drift in visual-inertial
odometry (VIO) systems. Consistency is crucial for ensuring the estimation
accuracy of a UWBaided VIO system. An inconsistent estimator can degrade
localization performance, where the inconsistency primarily arises from two
main factors: (1) the estimator fails to preserve the correct system
observability, and (2) UWB anchor positions are assumed to be known, leading to
improper neglect of calibration uncertainty. In this paper, we propose a
consistent and tightly-coupled visual-inertial-ranging odometry (CVIRO) system
based on the Lie group. Our method incorporates the UWB anchor state into the
system state, explicitly accounting for UWB calibration uncertainty and
enabling the joint and consistent estimation of both robot and anchor states.
Furthermore, observability consistency is ensured by leveraging the invariant
error properties of the Lie group. We analytically prove that the CVIRO
algorithm naturally maintains the system's correct unobservable subspace,
thereby preserving estimation consistency. Extensive simulations and
experiments demonstrate that CVIRO achieves superior localization accuracy and
consistency compared to existing methods.

</details>


### [23] [TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning](https://arxiv.org/abs/2508.10872)
*Anantha Narayanan,Battu Bhanu Teja,Pruthwik Mishra*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的框架（A2C算法），用于优化卫星轨道参数以实现精确的地面覆盖，并在计算效率和收敛速度上优于PPO算法。


<details>
  <summary>Details</summary>
Motivation: 随着近地轨道（LEO）日益拥挤，卫星部署和运行面临更高的碰撞风险，需要更高效的轨道规划方法。

Method: 通过将问题建模为马尔可夫决策过程（MDP），在自定义的OpenAI Gymnasium环境中使用A2C算法调整轨道参数（如半长轴、偏心率等）。

Result: A2C算法在累积奖励（10.0 vs 9.263025）和收敛速度（2,000 vs 63,000时间步）上显著优于PPO算法。

Conclusion: 该方法证明了强化学习在LEO任务规划中的高效性和可扩展性，为实时任务规划提供了新思路。

Abstract: The increasing congestion of Low Earth Orbit (LEO) poses persistent
challenges to the efficient deployment and safe operation of Earth observation
satellites. Mission planners must now account not only for mission-specific
requirements but also for the increasing collision risk with active satellites
and space debris. This work presents a reinforcement learning framework using
the Advantage Actor-Critic (A2C) algorithm to optimize satellite orbital
parameters for precise terrestrial coverage within predefined surface radii. By
formulating the problem as a Markov Decision Process (MDP) within a custom
OpenAI Gymnasium environment, our method simulates orbital dynamics using
classical Keplerian elements. The agent progressively learns to adjust five of
the orbital parameters - semi-major axis, eccentricity, inclination, right
ascension of ascending node, and the argument of perigee-to achieve targeted
terrestrial coverage. Comparative evaluation against Proximal Policy
Optimization (PPO) demonstrates A2C's superior performance, achieving 5.8x
higher cumulative rewards (10.0 vs 9.263025) while converging in 31.5x fewer
timesteps (2,000 vs 63,000). The A2C agent consistently meets mission
objectives across diverse target coordinates while maintaining computational
efficiency suitable for real-time mission planning applications. Key
contributions include: (1) a TLE-based orbital simulation environment
incorporating physics constraints, (2) validation of actor-critic methods'
superiority over trust region approaches in continuous orbital control, and (3)
demonstration of rapid convergence enabling adaptive satellite deployment. This
approach establishes reinforcement learning as a computationally efficient
alternative for scalable and intelligent LEO mission planning.

</details>
