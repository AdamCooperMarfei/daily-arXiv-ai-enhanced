<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 34]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Learning to Move in Rhythm: Task-Conditioned Motion Policies with Orbital Stability Guarantees](https://arxiv.org/abs/2507.10602)
*Maximilian Stölzle,T. Konstantin Rusch,Zach J. Patterson,Rodrigo Pérez-Dattari,Francesco Stella,Josie Hughes,Cosimo Della Santina,Daniela Rus*

Main category: cs.RO

TL;DR: 提出了一种名为OSMPs的新框架，结合学习编码器和Hopf分岔，解决了动态运动基元在周期性行为和任务插值上的局限性，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 动态运动基元在复杂周期性行为和任务插值上表现不佳，限制了其实际应用范围。

Method: 引入OSMPs框架，结合学习编码器和Hopf分岔，确保轨道稳定性和横向收缩性，并通过任务条件化实现多目标运动表示。

Result: 在仿真和真实实验中，OSMPs在多种机器人平台上表现优于现有基线方法。

Conclusion: OSMPs提供了一种高效且稳定的方法，能够广泛适用于周期性运动任务，并具备零样本泛化能力。

Abstract: Learning from demonstration provides a sample-efficient approach to acquiring
complex behaviors, enabling robots to move robustly, compliantly, and with
fluidity. In this context, Dynamic Motion Primitives offer built - in stability
and robustness to disturbances but often struggle to capture complex periodic
behaviors. Moreover, they are limited in their ability to interpolate between
different tasks. These shortcomings substantially narrow their applicability,
excluding a wide class of practically meaningful tasks such as locomotion and
rhythmic tool use. In this work, we introduce Orbitally Stable Motion
Primitives (OSMPs) - a framework that combines a learned diffeomorphic encoder
with a supercritical Hopf bifurcation in latent space, enabling the accurate
acquisition of periodic motions from demonstrations while ensuring formal
guarantees of orbital stability and transverse contraction. Furthermore, by
conditioning the bijective encoder on the task, we enable a single learned
policy to represent multiple motion objectives, yielding consistent zero-shot
generalization to unseen motion objectives within the training distribution. We
validate the proposed approach through extensive simulation and real-world
experiments across a diverse range of robotic platforms - from collaborative
arms and soft manipulators to a bio-inspired rigid-soft turtle robot -
demonstrating its versatility and effectiveness in consistently outperforming
state-of-the-art baselines such as diffusion policies, among others.

</details>


### [2] [Vision Language Action Models in Robotic Manipulation: A Systematic Review](https://arxiv.org/abs/2507.10672)
*Muhayy Ud Din,Waseem Akram,Lyes Saad Saoud,Jan Rosell,Irfan Hussain*

Main category: cs.RO

TL;DR: 本文综述了视觉语言动作（VLA）模型在机器人领域的应用，分析了102个模型、26个数据集和12个仿真平台，提出了数据集评估新标准和二维分类框架，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 统一视觉感知、自然语言理解和机器人控制，推动通用机器人代理的发展。

Method: 通过分析VLA模型、数据集和仿真平台，提出分类框架和评估标准，总结现有挑战和未来方向。

Result: 揭示了当前数据集的不足，提出了模块化架构设计和多模态对齐策略等未来方向。

Conclusion: 本文为VLA模型的发展提供了技术参考和概念路线图，从数据生成到实际部署提供了全面指导。

Abstract: Vision Language Action (VLA) models represent a transformative shift in
robotics, with the aim of unifying visual perception, natural language
understanding, and embodied control within a single learning framework. This
review presents a comprehensive and forward-looking synthesis of the VLA
paradigm, with a particular emphasis on robotic manipulation and
instruction-driven autonomy. We comprehensively analyze 102 VLA models, 26
foundational datasets, and 12 simulation platforms that collectively shape the
development and evaluation of VLAs models. These models are categorized into
key architectural paradigms, each reflecting distinct strategies for
integrating vision, language, and control in robotic systems. Foundational
datasets are evaluated using a novel criterion based on task complexity,
variety of modalities, and dataset scale, allowing a comparative analysis of
their suitability for generalist policy learning. We introduce a
two-dimensional characterization framework that organizes these datasets based
on semantic richness and multimodal alignment, showing underexplored regions in
the current data landscape. Simulation environments are evaluated for their
effectiveness in generating large-scale data, as well as their ability to
facilitate transfer from simulation to real-world settings and the variety of
supported tasks. Using both academic and industrial contributions, we recognize
ongoing challenges and outline strategic directions such as scalable
pretraining protocols, modular architectural design, and robust multimodal
alignment strategies. This review serves as both a technical reference and a
conceptual roadmap for advancing embodiment and robotic control, providing
insights that span from dataset generation to real world deployment of
generalist robotic agents.

</details>


### [3] [Exteroception through Proprioception Sensing through Improved Contact Modeling for Soft Growing Robots](https://arxiv.org/abs/2507.10694)
*Francesco Fuentes,Serigne Diagne,Zachary Kingston,Laura H. Blumenschein*

Main category: cs.RO

TL;DR: 利用软体生长机器人作为环境探索和地图构建工具，通过碰撞行为建模和几何模拟器验证其在非结构化环境中的潜力。


<details>
  <summary>Details</summary>
Motivation: 软体机器人因其被动形变特性在非结构化环境中表现出色，但需要更好地理解碰撞与形变以利用其进行环境结构感知。

Method: 首先分析离散转向中的碰撞行为，建立几何模拟器模拟2D环境中的机器人轨迹，并通过蒙特卡洛采样验证模型有效性。

Result: 在均匀和非均匀环境中，该方法能快速逼近最优部署策略，展示了软体生长机器人在环境探索中的潜力。

Conclusion: 软体生长机器人可作为有效的环境探索和地图构建工具，其模型和模拟器验证了其在非结构化环境中的实用性。

Abstract: Passive deformation due to compliance is a commonly used benefit of soft
robots, providing opportunities to achieve robust actuation with few active
degrees of freedom. Soft growing robots in particular have shown promise in
navigation of unstructured environments due to their passive deformation. If
their collisions and subsequent deformations can be better understood, soft
robots could be used to understand the structure of the environment from direct
tactile measurements. In this work, we propose the use of soft growing robots
as mapping and exploration tools. We do this by first characterizing collision
behavior during discrete turns, then leveraging this model to develop a
geometry-based simulator that models robot trajectories in 2D environments.
Finally, we demonstrate the model and simulator validity by mapping unknown
environments using Monte Carlo sampling to estimate the optimal next deployment
given current knowledge. Over both uniform and non-uniform environments, this
selection method rapidly approaches ideal actions, showing the potential for
soft growing robots in unstructured environment exploration and mapping.

</details>


### [4] [RCG: Safety-Critical Scenario Generation for Robust Autonomous Driving via Real-World Crash Grounding](https://arxiv.org/abs/2507.10749)
*Benjamin Stoler,Juliet Yang,Jonathan Francis,Jean Oh*

Main category: cs.RO

TL;DR: 提出了一种名为RCG的场景生成框架，通过将碰撞语义融入对抗扰动流程，生成更真实且高风险的安全关键场景，提升自动驾驶系统的训练效果。


<details>
  <summary>Details</summary>
Motivation: 现实驾驶数据集中安全关键场景稀缺，难以有效训练和评估自动驾驶系统。

Method: 结合对比预训练和微调技术，构建安全感知的行为表示，并嵌入到现有场景生成流程中。

Result: 实验显示，使用生成场景训练的自动驾驶系统下游成功率平均提升9.2%，且生成的对抗行为更真实。

Conclusion: RCG框架能生成更有效和真实的测试场景，显著提升自动驾驶系统的鲁棒性。

Abstract: Safety-critical scenarios are essential for training and evaluating
autonomous driving (AD) systems, yet remain extremely rare in real-world
driving datasets. To address this, we propose Real-world Crash Grounding (RCG),
a scenario generation framework that integrates crash-informed semantics into
adversarial perturbation pipelines. We construct a safety-aware behavior
representation through contrastive pre-training on large-scale driving logs,
followed by fine-tuning on a small, crash-rich dataset with approximate
trajectory annotations extracted from video. This embedding captures semantic
structure aligned with real-world accident behaviors and supports selection of
adversary trajectories that are both high-risk and behaviorally realistic. We
incorporate the resulting selection mechanism into two prior scenario
generation pipelines, replacing their handcrafted scoring objectives with an
embedding-based criterion. Experimental results show that ego agents trained
against these generated scenarios achieve consistently higher downstream
success rates, with an average improvement of 9.2% across seven evaluation
settings. Qualitative and quantitative analyses further demonstrate that our
approach produces more plausible and nuanced adversary behaviors, enabling more
effective and realistic stress testing of AD systems. Code and tools will be
released publicly.

</details>


### [5] [rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active Instance-Level Object Understanding](https://arxiv.org/abs/2507.10776)
*Howard H. Qian,Yiting Chen,Gaotian Wang,Podshara Chanrungmaneekul,Kaiyu Hang*

Main category: cs.RO

TL;DR: 提出了一种实时交互感知框架rt-RISeg，通过机器人交互和设计的体帧不变特征（BFIF）连续分割未见物体，无需学习分割模型，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统未见物体实例分割（UOIS）方法依赖大规模数据集训练，容易过拟合静态视觉特征，泛化性能差。

Method: 基于视觉交互性原理，提出rt-RISeg框架，利用机器人交互和体帧不变特征（BFIF）实时分割物体。

Result: 平均分割准确率比现有UOIS方法高27.5%，且生成的分割掩码可作为视觉基础模型的提示进一步提升性能。

Conclusion: rt-RISeg通过交互感知实现了高效、自包含的未见物体分割，为机器人操作任务提供了新思路。

Abstract: Successful execution of dexterous robotic manipulation tasks in new
environments, such as grasping, depends on the ability to proficiently segment
unseen objects from the background and other objects. Previous works in unseen
object instance segmentation (UOIS) train models on large-scale datasets, which
often leads to overfitting on static visual features. This dependency results
in poor generalization performance when confronted with out-of-distribution
scenarios. To address this limitation, we rethink the task of UOIS based on the
principle that vision is inherently interactive and occurs over time. We
propose a novel real-time interactive perception framework, rt-RISeg, that
continuously segments unseen objects by robot interactions and analysis of a
designed body frame-invariant feature (BFIF). We demonstrate that the relative
rotational and linear velocities of randomly sampled body frames, resulting
from selected robot interactions, can be used to identify objects without any
learned segmentation model. This fully self-contained segmentation pipeline
generates and updates object segmentation masks throughout each robot
interaction without the need to wait for an action to finish. We showcase the
effectiveness of our proposed interactive perception method by achieving an
average object segmentation accuracy rate 27.5% greater than state-of-the-art
UOIS methods. Furthermore, although rt-RISeg is a standalone framework, we show
that the autonomously generated segmentation masks can be used as prompts to
vision foundation models for significantly improved performance.

</details>


### [6] [Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection](https://arxiv.org/abs/2507.10814)
*Huiyi Wang,Fahim Shahriar,Alireza Azimi,Gautham Vasan,Rupam Mahmood,Colin Bellinger*

Main category: cs.RO

TL;DR: 该研究提出了一种将预训练模型（如大型语言模型和物体检测器）与目标条件强化学习结合的方法，以提升机器人抓取任务的通用性和效率。


<details>
  <summary>Details</summary>
Motivation: 通用机器人操作（如抓取）在家庭和工作场景中至关重要，但传统方法学习物体交互成本高。预训练模型能高效处理文本提示和识别物体，为强化学习提供支持。

Method: 使用预训练物体检测模型，通过文本提示识别物体并生成掩码，用于目标条件强化学习。掩码提供物体无关的提示，提升特征共享和泛化能力。

Result: 在模拟抓取任务中，掩码目标条件方法在分布内外物体上均保持约90%的成功率，且收敛更快、回报更高。

Conclusion: 该方法通过结合预训练模型和目标条件强化学习，显著提升了机器人抓取任务的通用性和性能。

Abstract: General-purpose robotic manipulation, including reach and grasp, is essential
for deployment into households and workspaces involving diverse and evolving
tasks. Recent advances propose using large pre-trained models, such as Large
Language Models and object detectors, to boost robotic perception in
reinforcement learning. These models, trained on large datasets via
self-supervised learning, can process text prompts and identify diverse objects
in scenes, an invaluable skill in RL where learning object interaction is
resource-intensive. This study demonstrates how to integrate such models into
Goal-Conditioned Reinforcement Learning to enable general and versatile robotic
reach and grasp capabilities. We use a pre-trained object detection model to
enable the agent to identify the object from a text prompt and generate a mask
for goal conditioning. Mask-based goal conditioning provides object-agnostic
cues, improving feature sharing and generalization. The effectiveness of the
proposed framework is demonstrated in a simulated reach-and-grasp task, where
the mask-based goal conditioning consistently maintains a $\sim$90\% success
rate in grasping both in and out-of-distribution objects, while also ensuring
faster convergence to higher returns.

</details>


### [7] [Mixed Discrete and Continuous Planning using Shortest Walks in Graphs of Convex Sets](https://arxiv.org/abs/2507.10878)
*Savva Morozov,Tobia Marcucci,Bernhard Paus Graesdal,Alexandre Amice,Pablo A. Parrilo,Russ Tedrake*

Main category: cs.RO

TL;DR: 论文研究了凸集图（GCS）中的最短路径问题（SWP），提出了一种基于半定规划和增量搜索的近似求解方法，并在机器人学中展示了其广泛应用。


<details>
  <summary>Details</summary>
Motivation: 凸集图（GCS）为混合离散-连续规划问题提供了一种统一的建模语言，但现有方法通常需要专门解决方案，缺乏通用性和高效性。

Method: 通过半定规划合成成本函数的二次下界，并利用增量搜索算法近似求解最短路径。

Result: 实验验证了该方法在碰撞避免运动规划、技能链和混合系统最优控制中的高效性和性能优势。

Conclusion: GCS中的SWP为机器人学中的混合规划问题提供了通用且高效的解决方案。

Abstract: We study the Shortest-Walk Problem (SWP) in a Graph of Convex Sets (GCS). A
GCS is a graph where each vertex is paired with a convex program, and each edge
couples adjacent programs via additional costs and constraints. A walk in a GCS
is a sequence of vertices connected by edges, where vertices may be repeated.
The length of a walk is given by the cumulative optimal value of the
corresponding convex programs. To solve the SWP in GCS, we first synthesize a
piecewise-quadratic lower bound on the problem's cost-to-go function using
semidefinite programming. Then we use this lower bound to guide an
incremental-search algorithm that yields an approximate shortest walk. We show
that the SWP in GCS is a natural language for many mixed discrete-continuous
planning problems in robotics, unifying problems that typically require
specialized solutions while delivering high performance and computational
efficiency. We demonstrate this through experiments in collision-free motion
planning, skill chaining, and optimal control of hybrid systems.

</details>


### [8] [Object-Centric Mobile Manipulation through SAM2-Guided Perception and Imitation Learning](https://arxiv.org/abs/2507.10899)
*Wang Zhicheng,Satoshi Yagi,Satoshi Yamamori,Jun Morimoto*

Main category: cs.RO

TL;DR: 提出了一种基于SAM2的对象中心方法，用于移动操作任务，通过整合操作方向信息，提升任务在不同方向上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前移动操作框架通常将导航和操作解耦，导致导航不精确时性能下降，尤其是方向偏差问题。

Method: 采用SAM2基础模型，结合操作方向信息，实现任务在不同方向上的统一理解。

Result: 在自定义移动操作器上测试，相比Action Chunking Transformer，模型在多样化方向下表现更优。

Conclusion: 该方法显著提升了基于模仿学习的移动操作系统的泛化能力和鲁棒性。

Abstract: Imitation learning for mobile manipulation is a key challenge in the field of
robotic manipulation. However, current mobile manipulation frameworks typically
decouple navigation and manipulation, executing manipulation only after
reaching a certain location. This can lead to performance degradation when
navigation is imprecise, especially due to misalignment in approach angles. To
enable a mobile manipulator to perform the same task from diverse orientations,
an essential capability for building general-purpose robotic models, we propose
an object-centric method based on SAM2, a foundation model towards solving
promptable visual segmentation in images, which incorporates manipulation
orientation information into our model. Our approach enables consistent
understanding of the same task from different orientations. We deploy the model
on a custom-built mobile manipulator and evaluate it on a pick-and-place task
under varied orientation angles. Compared to Action Chunking Transformer, our
model maintains superior generalization when trained with demonstrations from
varied approach angles. This work significantly enhances the generalization and
robustness of imitation learning-based mobile manipulation systems.

</details>


### [9] [Fast Non-Episodic Adaptive Tuning of Robot Controllers with Online Policy Optimization](https://arxiv.org/abs/2507.10914)
*James A. Preiss,Fengze Xie,Yiheng Lin,Adam Wierman,Yisong Yue*

Main category: cs.RO

TL;DR: 论文提出了一种名为M-GAPS的单轨迹在线策略优化算法，用于动态调整机器人控制器参数，适应时变环境，并在硬件实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决机器人控制器参数在时变动态、策略类和优化目标下的在线调整问题，避免依赖预先已知信息或人工分段。

Method: 提出M-GAPS算法，结合四旋翼状态空间和策略类的重新参数化，优化了非线性几何四旋翼控制器的性能。

Result: 硬件实验表明，M-GAPS比基于模型和无模型的基线方法更快找到接近最优参数，尤其在不利分段长度下表现更优，并能快速适应未建模的风和负载扰动。

Conclusion: M-GAPS展示了在线策略优化在硬件上的实用性，比经典自适应控制更灵活，比无模型强化学习更稳定高效。

Abstract: We study online algorithms to tune the parameters of a robot controller in a
setting where the dynamics, policy class, and optimality objective are all
time-varying. The system follows a single trajectory without episodes or state
resets, and the time-varying information is not known in advance. Focusing on
nonlinear geometric quadrotor controllers as a test case, we propose a
practical implementation of a single-trajectory model-based online policy
optimization algorithm, M-GAPS,along with reparameterizations of the quadrotor
state space and policy class to improve the optimization landscape. In hardware
experiments,we compare to model-based and model-free baselines that impose
artificial episodes. We show that M-GAPS finds near-optimal parameters more
quickly, especially when the episode length is not favorable. We also show that
M-GAPS rapidly adapts to heavy unmodeled wind and payload disturbances, and
achieves similar strong improvement on a 1:6-scale Ackermann-steered car. Our
results demonstrate the hardware practicality of this emerging class of online
policy optimization that offers significantly more flexibility than classic
adaptive control, while being more stable and data-efficient than model-free
reinforcement learning.

</details>


### [10] [Unified Modeling and Structural Optimization of Multi-magnet Embedded Soft Continuum Robots for Enhanced Kinematic Performances](https://arxiv.org/abs/2507.10950)
*Zhiwei Wu,Jiahao Luo,Siyi Wei,Jinhui Zhang*

Main category: cs.RO

TL;DR: 提出了一种统一建模与优化框架，提升多磁体嵌入式软连续体机器人（MeSCRs）的运动学性能。通过可微系统公式化和结构优化，揭示了最优磁体配置的闭式解。


<details>
  <summary>Details</summary>
Motivation: 提升多磁体嵌入式软连续体机器人的运动学性能，解决其可控自由度与磁体配置的优化问题。

Method: 基于扩展伪刚体模型建立可微系统公式，结合微分几何开发结构优化框架，分析均衡性和配置空间几何。

Result: 最大可控自由度等于嵌入磁体数量的两倍；优化条件表明需调制配置空间度量谱以提升局部性能。

Conclusion: 提出的框架有效优化了MeSCRs的运动学性能，并通过仿真验证了其有效性。

Abstract: This paper presents a unified modeling and optimization framework to enhance
the kinematic performance of multi-magnet embedded soft continuum robots
(MeSCRs). To this end, we establish a differentiable system formulation based
on an extended pseudo-rigid-body model. This formulation enables analysis of
the equilibrium well-posedness and the geometry of the induced configuration
under magnetic actuation. In particular, we show that the maximum controllable
degrees of freedom of a MeSCR equal twice the number of embedded magnets. We
subsequently develop a structural optimization framework based on differential
geometry that links classical kinematic measures (e.g., manipulability and
dexterity) to the configuration of embedded magnets. The resulting optimization
condition reveals that improving local performance requires structurally
modulating the spectrum of the configuration space metric to counteract its
distortion. Closed-form solutions for optimal magnet configurations are derived
under representative conditions, and a gradient-based numerical method is
proposed for general design scenarios. Simulation studies validate the
effectiveness of the proposed framework.

</details>


### [11] [Whom to Respond To? A Transformer-Based Model for Multi-Party Social Robot Interaction](https://arxiv.org/abs/2507.10960)
*He Zhu,Ryo Miyoshi,Yuki Okafuji*

Main category: cs.RO

TL;DR: 提出了一种基于Transformer的多任务学习框架，用于改进社交机器人在多用户环境中的决策能力，并通过新损失函数和数据集验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 多用户环境中，社交机器人需要理解上下文并决定何时及对谁做出响应，而现有研究主要关注单用户交互。

Method: 采用Transformer框架，引入两种新损失函数：一种优化场景建模，另一种指导机器人响应选择。构建了包含真实复杂性的多用户HRI数据集。

Result: 模型在响应决策上表现优于现有启发式和单任务方法，达到最先进水平。

Conclusion: 该研究推动了社交机器人在自然、上下文感知的多方交互中的发展。

Abstract: Prior human-robot interaction (HRI) research has primarily focused on
single-user interactions, where robots do not need to consider the timing or
recipient of their responses. However, in multi-party interactions, such as at
malls and hospitals, social robots must understand the context and decide both
when and to whom they should respond. In this paper, we propose a
Transformer-based multi-task learning framework to improve the decision-making
process of social robots, particularly in multi-user environments. Considering
the characteristics of HRI, we propose two novel loss functions: one that
enforces constraints on active speakers to improve scene modeling, and another
that guides response selection towards utterances specifically directed at the
robot. Additionally, we construct a novel multi-party HRI dataset that captures
real-world complexities, such as gaze misalignment. Experimental results
demonstrate that our model achieves state-of-the-art performance in respond
decisions, outperforming existing heuristic-based and single-task approaches.
Our findings contribute to the development of socially intelligent social
robots capable of engaging in natural and context-aware multi-party
interactions.

</details>


### [12] [EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for Spatially Generalizable Contact-rich Tasks](https://arxiv.org/abs/2507.10961)
*Joohwan Seo,Arvind Kruthiventy,Soomi Lee,Megan Teng,Xiang Zhang,Seoyeon Choi,Jongeun Choi,Roberto Horowitz*

Main category: cs.RO

TL;DR: 论文提出了一种名为EquiContact的分层策略框架，用于学习视觉驱动的机器人策略，以实现接触密集型任务的空间泛化。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过少量演示训练，实现钉孔任务（PiH）策略的鲁棒空间泛化。

Method: 框架包括高层视觉规划器（Diff-EDF）和低层顺应性视觉运动策略（G-CompACT），利用局部观测和SE(3)-等变性设计。

Result: 实验显示，PiH任务的成功率接近完美，并能泛化到未见过的空间配置。

Conclusion: 提出的框架和原则（顺应性、局部策略和诱导等变性）有效实现了接触密集型任务的空间泛化。

Abstract: This paper presents a framework for learning vision-based robotic policies
for contact-rich manipulation tasks that generalize spatially across task
configurations. We focus on achieving robust spatial generalization of the
policy for the peg-in-hole (PiH) task trained from a small number of
demonstrations. We propose EquiContact, a hierarchical policy composed of a
high-level vision planner (Diffusion Equivariant Descriptor Field, Diff-EDF)
and a novel low-level compliant visuomotor policy (Geometric Compliant ACT,
G-CompACT). G-CompACT operates using only localized observations (geometrically
consistent error vectors (GCEV), force-torque readings, and wrist-mounted RGB
images) and produces actions defined in the end-effector frame. Through these
design choices, we show that the entire EquiContact pipeline is
SE(3)-equivariant, from perception to force control. We also outline three key
components for spatially generalizable contact-rich policies: compliance,
localized policies, and induced equivariance. Real-world experiments on PiH
tasks demonstrate a near-perfect success rate and robust generalization to
unseen spatial configurations, validating the proposed framework and
principles. The experimental videos can be found on the project website:
https://sites.google.com/berkeley.edu/equicontact

</details>


### [13] [SMART-Merge Planner: A Safe Merging and Real-Time Motion Planner for Autonomous Highway On-Ramp Merging](https://arxiv.org/abs/2507.10968)
*Toktam Mohammadnejad,Jovin D'sa,Behdad Chalaki,Hossein Nourkhiz Mahjoub,Ehsan Moradi-Pari*

Main category: cs.RO

TL;DR: SMART-Merge是一种基于格栅的运动规划器，旨在实现安全舒适的强制并道，通过优化成本项和速度启发式，实现高效并道。


<details>
  <summary>Details</summary>
Motivation: 高速公路并道是一项复杂的驾驶任务，涉及安全间隙识别、速度调整和交互，需在有限时间内完成并保证安全和舒适。

Method: 提出SMART-Merge规划器，通过调整成本项和引入速度启发式，优化并道过程。

Result: 在高保真CarMaker模拟中，SMART-Merge在数百种并道场景中成功率达100%，且并道时间最短。

Conclusion: SMART-Merge能高效处理复杂强制并道任务，为自动驾驶提供可靠解决方案。

Abstract: Merging onto a highway is a complex driving task that requires identifying a
safe gap, adjusting speed, often interactions to create a merging gap, and
completing the merge maneuver within a limited time window while maintaining
safety and driving comfort. In this paper, we introduce a Safe Merging and
Real-Time Merge (SMART-Merge) planner, a lattice-based motion planner designed
to facilitate safe and comfortable forced merging. By deliberately adapting
cost terms to the unique challenges of forced merging and introducing a desired
speed heuristic, SMART-Merge planner enables the ego vehicle to merge
successfully while minimizing the merge time. We verify the efficiency and
effectiveness of the proposed merge planner through high-fidelity CarMaker
simulations on hundreds of highway merge scenarios. Our proposed planner
achieves the success rate of 100% as well as completes the merge maneuver in
the shortest amount of time compared with the baselines, demonstrating our
planner's capability to handle complex forced merge tasks and provide a
reliable and robust solution for autonomous highway merge. The simulation
result videos are available at
https://sites.google.com/view/smart-merge-planner/home.

</details>


### [14] [Uncertainty Aware Mapping for Vision-Based Underwater Robots](https://arxiv.org/abs/2507.10991)
*Abhimanyu Bhowmik,Mohit Singh,Madhushree Sannigrahi,Martin Ludvigsen,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文探讨了如何在基于视觉的水下机器人中表示地图不一致性，并将深度估计置信度融入体素地图框架Voxblox中，改进了权重计算和更新机制。


<details>
  <summary>Details</summary>
Motivation: 传统传感器和预规划路径在受限空间内无法适用，视觉传感器因噪声和环境变化导致不确定性，需改进地图表示方法。

Method: 使用RAFT-Stereo模型估计场景深度和置信度，并集成到Voxblox框架中，改进权重计算和更新机制。

Result: 在受限水池和Trondheim峡湾码头进行实验，验证了不确定性可视化的变化。

Conclusion: 提出的方法能有效表示地图不一致性，并提升水下机器人在受限空间中的感知能力。

Abstract: Vision-based underwater robots can be useful in inspecting and exploring
confined spaces where traditional sensors and preplanned paths cannot be
followed. Sensor noise and situational change can cause significant uncertainty
in environmental representation. Thus, this paper explores how to represent
mapping inconsistency in vision-based sensing and incorporate depth estimation
confidence into the mapping framework. The scene depth and the confidence are
estimated using the RAFT-Stereo model and are integrated into a voxel-based
mapping framework, Voxblox. Improvements in the existing Voxblox weight
calculation and update mechanism are also proposed. Finally, a qualitative
analysis of the proposed method is performed in a confined pool and in a pier
in the Trondheim fjord. Experiments using an underwater robot demonstrated the
change in uncertainty in the visualization.

</details>


### [15] [ILCL: Inverse Logic-Constraint Learning from Temporally Constrained Demonstrations](https://arxiv.org/abs/2507.11000)
*Minwoo Cho,Jaehwi Jang,Daehyung Park*

Main category: cs.RO

TL;DR: 提出了一种名为ILCL的新方法，通过遗传算法和逻辑约束强化学习解决时间约束学习问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决从演示中学习时间约束以重现逻辑约束行为的挑战性问题。

Method: 结合遗传算法的时间逻辑挖掘（GA-TL-Mining）和逻辑约束强化学习（Logic-CRL），通过零和博弈框架学习TLTL约束。

Result: 在四个时间约束任务上优于现有方法，并成功应用于真实世界的任务。

Conclusion: ILCL方法在学习和迁移时间约束方面表现出色，具有实际应用潜力。

Abstract: We aim to solve the problem of temporal-constraint learning from
demonstrations to reproduce demonstration-like logic-constrained behaviors.
Learning logic constraints is challenging due to the combinatorially large
space of possible specifications and the ill-posed nature of non-Markovian
constraints. To figure it out, we introduce a novel temporal-constraint
learning method, which we call inverse logic-constraint learning (ILCL). Our
method frames ICL as a two-player zero-sum game between 1) a genetic
algorithm-based temporal-logic mining (GA-TL-Mining) and 2) logic-constrained
reinforcement learning (Logic-CRL). GA-TL-Mining efficiently constructs syntax
trees for parameterized truncated linear temporal logic (TLTL) without
predefined templates. Subsequently, Logic-CRL finds a policy that maximizes
task rewards under the constructed TLTL constraints via a novel constraint
redistribution scheme. Our evaluations show ILCL outperforms state-of-the-art
baselines in learning and transferring TL constraints on four temporally
constrained tasks. We also demonstrate successful transfer to real-world
peg-in-shallow-hole tasks.

</details>


### [16] [Learning to Tune Like an Expert: Interpretable and Scene-Aware Navigation via MLLM Reasoning and CVAE-Based Adaptation](https://arxiv.org/abs/2507.11001)
*Yanbo Wang,Zipeng Fang,Lei Zhao,Weidong Chen*

Main category: cs.RO

TL;DR: LE-Nav是一种基于多模态大语言模型和条件变分自编码器的导航框架，通过自适应调整规划器超参数，实现零样本场景理解和专家级调优，显著提升导航性能和社会接受度。


<details>
  <summary>Details</summary>
Motivation: 传统导航系统在动态和非结构化环境中表现不佳，强化学习方法因泛化能力差和模拟多样性不足而难以实际应用。

Method: 结合多模态大语言模型推理和条件变分自编码器，利用单样本示例和思维链提示策略实现场景感知和超参数自适应调整。

Result: 实验表明LE-Nav在多样规划器和场景中实现人类级调优，实际导航试验和用户研究显示其在成功率、效率、安全性和舒适度上优于现有方法。

Conclusion: LE-Nav通过智能超参数调优和场景理解，显著提升了导航系统的性能和用户接受度。

Abstract: Service robots are increasingly deployed in diverse and dynamic environments,
where both physical layouts and social contexts change over time and across
locations. In these unstructured settings, conventional navigation systems that
rely on fixed parameters often fail to generalize across scenarios, resulting
in degraded performance and reduced social acceptance. Although recent
approaches have leveraged reinforcement learning to enhance traditional
planners, these methods often fail in real-world deployments due to poor
generalization and limited simulation diversity, which hampers effective
sim-to-real transfer. To tackle these issues, we present LE-Nav, an
interpretable and scene-aware navigation framework that leverages multi-modal
large language model reasoning and conditional variational autoencoders to
adaptively tune planner hyperparameters. To achieve zero-shot scene
understanding, we utilize one-shot exemplars and chain-of-thought prompting
strategies. Additionally, a conditional variational autoencoder captures the
mapping between natural language instructions and navigation hyperparameters,
enabling expert-level tuning. Experiments show that LE-Nav can generate
hyperparameters achieving human-level tuning across diverse planners and
scenarios. Real-world navigation trials and a user study on a smart wheelchair
platform demonstrate that it outperforms state-of-the-art methods on
quantitative metrics such as success rate, efficiency, safety, and comfort,
while receiving higher subjective scores for perceived safety and social
acceptance. Code is available at https://github.com/Cavendish518/LE-Nav.

</details>


### [17] [Enhancing Autonomous Manipulator Control with Human-in-loop for Uncertain Assembly Environments](https://arxiv.org/abs/2507.11006)
*Ashutosh Mishra,Shreya Santra,Hazal Gozbasi,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 研究提出了一种结合自主机器人与人类控制的先进方法，用于提升月球任务中机器人操作的可靠性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决在不确定和挑战性环境中（如月球任务）机器人操作的可靠性问题，通过结合人类决策与自主功能。

Method: 集成实时反馈的机器人操作器、动态误差检测、自适应控制，以及数字孪生模拟和人类干预。

Result: 系统在模拟月球条件下验证了其性能，能够应对极端光照、多变地形和传感器限制。

Conclusion: 该方法显著提升了机器人操作的可靠性和效率，适用于空间任务。

Abstract: This study presents an advanced approach to enhance robotic manipulation in
uncertain and challenging environments, with a focus on autonomous operations
augmented by human-in-the-loop (HITL) control for lunar missions. By
integrating human decision-making with autonomous robotic functions, the
research improves task reliability and efficiency for space applications. The
key task addressed is the autonomous deployment of flexible solar panels using
an extendable ladder-like structure and a robotic manipulator with real-time
feedback for precision. The manipulator relays position and force-torque data,
enabling dynamic error detection and adaptive control during deployment. To
mitigate the effects of sinkage, variable payload, and low-lighting conditions,
efficient motion planning strategies are employed, supplemented by human
control that allows operators to intervene in ambiguous scenarios. Digital twin
simulation enhances system robustness by enabling continuous feedback,
iterative task refinement, and seamless integration with the deployment
pipeline. The system has been tested to validate its performance in simulated
lunar conditions and ensure reliability in extreme lighting, variable terrain,
changing payloads, and sensor limitations.

</details>


### [18] [TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update](https://arxiv.org/abs/2507.11069)
*Jeongyun Kim,Seunghoon Jeong,Giseop Kim,Myung-Hwan Jeon,Eunji Jun,Ayoung Kim*

Main category: cs.RO

TL;DR: TRAN-D是一种基于2D高斯泼溅的透明物体深度重建方法，通过分离透明物体与背景并优化高斯分布，显著提升了稀疏视图和动态环境下的3D几何重建效果。


<details>
  <summary>Details</summary>
Motivation: 透明物体的3D几何重建因反射和折射等物理特性而具有挑战性，尤其在稀疏视图和动态环境中。

Method: TRAN-D通过分离透明物体与背景，采用对象感知损失和高斯优化，结合物理模拟快速优化重建。

Result: 在合成和真实场景中，TRAN-D比现有方法平均绝对误差降低39%，单图像更新时精度达48.46%。

Conclusion: TRAN-D在透明物体重建中表现出色，显著优于现有方法，适用于动态和稀疏视图场景。

Abstract: Understanding the 3D geometry of transparent objects from RGB images is
challenging due to their inherent physical properties, such as reflection and
refraction. To address these difficulties, especially in scenarios with sparse
views and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian
Splatting-based depth reconstruction method for transparent objects. Our key
insight lies in separating transparent objects from the background, enabling
focused optimization of Gaussians corresponding to the object. We mitigate
artifacts with an object-aware loss that places Gaussians in obscured regions,
ensuring coverage of invisible surfaces while reducing overfitting.
Furthermore, we incorporate a physics-based simulation that refines the
reconstruction in just a few seconds, effectively handling object removal and
chain-reaction movement of remaining objects without the need for rescanning.
TRAN-D is evaluated on both synthetic and real-world sequences, and it
consistently demonstrated robust improvements over existing GS-based
state-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean
absolute error by over 39% for the synthetic TRansPose sequences. Furthermore,
despite being updated using only one image, TRAN-D reaches a {\delta} < 2.5 cm
accuracy of 48.46%, over 1.5 times that of baselines, which uses six images.
Code and more results are available at https://jeongyun0609.github.io/TRAN-D/.

</details>


### [19] [Closed Form Time Derivatives of the Equations of Motion of Rigid Body Systems](https://arxiv.org/abs/2507.11076)
*Andreas Mueller,Shivesh Kumar*

Main category: cs.RO

TL;DR: 本文提出了一种封闭形式的二阶时间导数方程，用于描述刚体系统的运动方程（EOM），为机器人控制提供了更直观的结构洞察。


<details>
  <summary>Details</summary>
Motivation: 在机器人控制中，尤其是包含弹性组件的多体系统，不仅需要平滑的轨迹，还需要控制力/力矩的时间导数。现有递归算法缺乏直观性，因此需要一种更直接的表达方式。

Method: 采用李群（Lie group）描述刚体系统，推导出紧凑且易于参数化的二阶时间导数EOM封闭形式。

Result: 提出的方法提供了EOM时间导数的直接结构表达，避免了递归算法的复杂性。

Conclusion: 该方法为机器人控制中的EOM导数计算提供了更直观和高效的替代方案。

Abstract: Derivatives of equations of motion(EOM) describing the dynamics of rigid body
systems are becoming increasingly relevant for the robotics community and find
many applications in design and control of robotic systems. Controlling robots,
and multibody systems comprising elastic components in particular, not only
requires smooth trajectories but also the time derivatives of the control
forces/torques, hence of the EOM. This paper presents the time derivatives of
the EOM in closed form up to second-order as an alternative formulation to the
existing recursive algorithms for this purpose, which provides a direct insight
into the structure of the derivatives. The Lie group formulation for rigid body
systems is used giving rise to very compact and easily parameterized equations.

</details>


### [20] [Force-Based Viscosity and Elasticity Measurements for Material Biomechanical Characterisation with a Collaborative Robotic Arm](https://arxiv.org/abs/2507.11133)
*Luca Beber,Edoardo Lamon,Giacomo Moretti,Matteo Saveriano,Luca Fambri,Luigi Palopoli,Daniele Fontanelli*

Main category: cs.RO

TL;DR: 论文评估了机器人系统在估计材料粘弹性参数方面的准确性，并初步验证了其在生物样本中的适用性。


<details>
  <summary>Details</summary>
Motivation: 诊断活动（如超声扫描和触诊）成本低但易出错，机器人解决方案可减少结果的主观性并缩短等待时间。

Method: 通过机器人系统测量不同材料的粘弹性参数，并与高精度仪器测得的真实值对比。

Result: 实验结果显示机器人系统的准确性接近真实值，支持其在临床应用的潜力。

Conclusion: 机器人系统在估计组织生物力学特性方面具有高准确性，有望用于临床诊断。

Abstract: Diagnostic activities, such as ultrasound scans and palpation, are relatively
low-cost. They play a crucial role in the early detection of health problems
and in assessing their progression. However, they are also error-prone
activities, which require highly skilled medical staff. The use of robotic
solutions can be key to decreasing the inherent subjectivity of the results and
reducing the waiting list. For a robot to perform palpation or ultrasound
scans, it must effectively manage physical interactions with the human body,
which greatly benefits from precise estimation of the patient's tissue
biomechanical properties. This paper assesses the accuracy and precision of a
robotic system in estimating the viscoelastic parameters of various materials,
including some tests on ex vivo tissues as a preliminary proof-of-concept
demonstration of the method's applicability to biological samples. The
measurements are compared against a ground truth derived from silicone
specimens with different viscoelastic properties, characterised using a
high-precision instrument. Experimental results show that the robotic system's
accuracy closely matches the ground truth, increasing confidence in the
potential use of robots for such clinical applications.

</details>


### [21] [A Robust Controller based on Gaussian Processes for Robotic Manipulators with Unknown Uncertainty](https://arxiv.org/abs/2507.11170)
*Giulio Giacomuzzo,Mohamed Abdelwahab,Marco Calì,Alberto Dalla Libera,Ruggero Carli*

Main category: cs.RO

TL;DR: 提出了一种基于学习的鲁棒反馈线性化策略，用于拉格朗日系统的精确轨迹跟踪，结合高斯过程回归估计模型失配并通过鲁棒化控制器保证跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 针对拉格朗日系统的轨迹跟踪问题，传统方法需要已知模型失配的边界，而本文提出了一种无需先验边界的方法。

Method: 采用高斯过程回归（GPR）估计模型失配，并将其与传统反馈线性化结合，通过鲁棒化控制器补偿剩余不确定性。

Result: 理论证明高概率下能保证渐近跟踪性能，并在2自由度平面机器人上进行了数值验证。

Conclusion: 该方法在无需模型失配边界的情况下，实现了拉格朗日系统的精确轨迹跟踪。

Abstract: In this paper, we propose a novel learning-based robust feedback
linearization strategy to ensure precise trajectory tracking for an important
family of Lagrangian systems. We assume a nominal knowledge of the dynamics is
given but no a-priori bounds on the model mismatch are available. In our
approach, the key ingredient is the adoption of a regression framework based on
Gaussian Processes (GPR) to estimate the model mismatch. This estimate is added
to the outer loop of a classical feedback linearization scheme based on the
nominal knowledge available. Then, to compensate for the residual uncertainty,
we robustify the controller including an additional term whose size is designed
based on the variance provided by the GPR framework. We proved that, with high
probability, the proposed scheme is able to guarantee asymptotic tracking of a
desired trajectory. We tested numerically our strategy on a 2 degrees of
freedom planar robot.

</details>


### [22] [MPC-based Coarse-to-Fine Motion Planning for Robotic Object Transportation in Cluttered Environments](https://arxiv.org/abs/2507.11211)
*Chen Cai,Ernesto Dickel Saraiva,Ya-jun Pan,Steven Liu*

Main category: cs.RO

TL;DR: 提出了一种新颖的从粗到细的运动规划框架，用于机器人在杂乱、未建模环境中的操作。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在复杂、未知环境中运动规划的挑战，尤其是在部分和不确定观测下生成可行轨迹的需求。

Method: 结合双摄像头感知系统和基于B样条的模型预测控制（MPC）方案，逐步优化环境模型和运动规划。

Result: 实验验证了该框架在不确定性和杂乱环境中的鲁棒性和适应性。

Conclusion: 该框架能够有效支持动态重规划和闭环运动学，适用于复杂环境中的机器人操作。

Abstract: This letter presents a novel coarse-to-fine motion planning framework for
robotic manipulation in cluttered, unmodeled environments. The system
integrates a dual-camera perception setup with a B-spline-based model
predictive control (MPC) scheme. Initially, the planner generates feasible
global trajectories from partial and uncertain observations. As new visual data
are incrementally fused, both the environment model and motion planning are
progressively refined. A vision-based cost function promotes target-driven
exploration, while a refined kernel-perceptron collision detector enables
efficient constraint updates for real-time planning. The framework accommodates
closed-chain kinematics and supports dynamic replanning. Experiments on a
multi-arm platform validate its robustness and adaptability under uncertainties
and clutter.

</details>


### [23] [Comparison of Localization Algorithms between Reduced-Scale and Real-Sized Vehicles Using Visual and Inertial Sensors](https://arxiv.org/abs/2507.11241)
*Tobias Kern,Leon Tolksdorf,Christian Birkner*

Main category: cs.RO

TL;DR: 研究探讨了物理缩比车辆对视觉和视觉-惯性自定位算法精度的影响，发现OpenVINS在缩比和实际尺寸车辆中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 加速高级自动驾驶功能的开发，通过缩比车辆测试自定位算法的可行性。

Method: 选择ROS2兼容的视觉和视觉-惯性算法（OpenVINS、VINS-Fusion、RTAB-Map），在缩比车辆上采集数据并与实际尺寸车辆数据对比。

Result: OpenVINS定位误差最低，缩比车辆与实际车辆在平移运动估计上差异较小，旋转运动估计无显著差异。

Conclusion: 缩比车辆可作为自定位算法的测试平台，OpenVINS表现最优。

Abstract: Physically reduced-scale vehicles are emerging to accelerate the development
of advanced automated driving functions. In this paper, we investigate the
effects of scaling on self-localization accuracy with visual and
visual-inertial algorithms using cameras and an inertial measurement unit
(IMU). For this purpose, ROS2-compatible visual and visual-inertial algorithms
are selected, and datasets are chosen as a baseline for real-sized vehicles. A
test drive is conducted to record data of reduced-scale vehicles. We compare
the selected localization algorithms, OpenVINS, VINS-Fusion, and RTAB-Map, in
terms of their pose accuracy against the ground-truth and against data from
real-sized vehicles. When comparing the implementation of the selected
localization algorithms to real-sized vehicles, OpenVINS has the lowest average
localization error. Although all selected localization algorithms have
overlapping error ranges, OpenVINS also performs best when applied to a
reduced-scale vehicle. When reduced-scale vehicles were compared to real-sized
vehicles, minor differences were found in translational vehicle motion
estimation accuracy. However, no significant differences were found when
comparing the estimation accuracy of rotational vehicle motion, allowing RSVRs
to be used as testing platforms for self-localization algorithms.

</details>


### [24] [Development of an Autonomous Mobile Robotic System for Efficient and Precise Disinfection](https://arxiv.org/abs/2507.11270)
*Ting-Wei Ou,Jia-Hao Jiang,Guan-Lin Huang,Kuu-Young Young*

Main category: cs.RO

TL;DR: 提出了一种针对病毒热点区域的移动机器人紫外线消毒系统，优化消毒剂量，显著减少消毒时间。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情凸显了医院自动化消毒的紧迫性，现有研究忽视人类活动对病毒分布的影响。

Method: 开发移动机器人系统，优先消毒高风险区域，优化紫外线剂量。

Result: 在两例医院场景中，消毒时间分别减少30.7%和31.9%，效果相同。

Conclusion: 该系统提高了消毒效率，减少了低风险区域的不必要暴露。

Abstract: The COVID-19 pandemic has severely affected public health, healthcare
systems, and daily life, especially amid resource shortages and limited
workers. This crisis has underscored the urgent need for automation in hospital
environments, particularly disinfection, which is crucial to controlling virus
transmission and improving the safety of healthcare personnel and patients.
Ultraviolet (UV) light disinfection, known for its high efficiency, has been
widely adopted in hospital settings. However, most existing research focuses on
maximizing UV coverage while paying little attention to the impact of human
activity on virus distribution. To address this issue, we propose a mobile
robotic system for UV disinfection focusing on the virus hotspot. The system
prioritizes disinfection in high-risk areas and employs an approach for
optimized UV dosage to ensure that all surfaces receive an adequate level of UV
exposure while significantly reducing disinfection time. It not only improves
disinfection efficiency but also minimizes unnecessary exposure in low-risk
areas. In two representative hospital scenarios, our method achieves the same
disinfection effectiveness while reducing disinfection time by 30.7% and 31.9%,
respectively. The video of the experiment is available at:
https://youtu.be/wHcWzOcoMPM.

</details>


### [25] [Ocean Diviner: A Diffusion-Augmented Reinforcement Learning for AUV Robust Control in the Underwater Tasks](https://arxiv.org/abs/2507.11283)
*Weiyi Liu,Jingzehua Xu,Guanwen Xie,Yi Li*

Main category: cs.RO

TL;DR: 本文提出了一种扩散增强的强化学习方法，用于自主水下车辆（AUV）的鲁棒控制，解决了水下轨迹规划和动态环境适应的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 水下环境复杂多变，传统控制方法在动态条件下表现不佳，需要一种更鲁棒和灵活的解决方案。

Method: 结合扩散模型和强化学习，通过扩散U-Net架构生成多步轨迹，并利用混合学习架构优化策略。

Result: 仿真实验表明，该方法在复杂海洋条件下优于传统控制方法，具有更高的适应性和可靠性。

Conclusion: 扩散增强的强化学习方法为AUV控制提供了鲁棒且高效的解决方案。

Abstract: This paper presents a diffusion-augmented reinforcement learning (RL)
approach for robust autonomous underwater vehicle (AUV) control, addressing key
challenges in underwater trajectory planning and dynamic environment
adaptation. The proposed method integrates three core innovations: (1) A
diffusion-based trajectory generation framework that produces physically
feasible multi-step trajectories, enhanced by a high-dimensional state encoding
mechanism combining current observations with historical states and actions
through a novel diffusion U-Net architecture, significantly improving
long-horizon planning. (2) A sample-efficient hybrid learning architecture that
synergizes diffusion-guided exploration with RL policy optimization, where the
diffusion model generates diverse candidate actions and the RL critic selects
optimal actions, achieving higher exploration efficiency and policy stability
in dynamic underwater environments. Extensive simulation experiments validating
the method's superior robustness and flexibility, outperforms conventional
control methods in challenging marine conditions, offering enhanced
adaptability and reliability for AUV operations in the underwater tasks.

</details>


### [26] [Diffusion-Based Imaginative Coordination for Bimanual Manipulation](https://arxiv.org/abs/2507.11296)
*Huilin Xu,Jian Ding,Jiakun Xu,Ruixiang Wang,Jun Chen,Jinjie Mai,Yanwei Fu,Bernard Ghanem,Feng Xu,Mohamed Elhoseiny*

Main category: cs.RO

TL;DR: 提出了一种基于扩散的统一框架，用于联合优化视频和动作预测，显著提高了双手机器人操作的成功率。


<details>
  <summary>Details</summary>
Motivation: 双手机器人操作在工业自动化和家庭服务中至关重要，但由于高维动作空间和复杂的协调需求，存在显著挑战。视频预测的潜力在此领域尚未充分探索。

Method: 提出了一种多帧潜在预测策略，在压缩潜在空间中编码未来状态，并引入单向注意力机制，视频预测依赖于动作，而动作预测独立于视频预测。

Result: 在两个模拟基准和真实世界实验中，成功率显著提升，ALOHA提高24.9%，RoboTwin提高11.1%，真实世界实验提高32.5%。

Conclusion: 该方法通过联合优化视频和动作预测，显著提升了双手机器人操作的性能，且模型和代码已开源。

Abstract: Bimanual manipulation is crucial in robotics, enabling complex tasks in
industrial automation and household services. However, it poses significant
challenges due to the high-dimensional action space and intricate coordination
requirements. While video prediction has been recently studied for
representation learning and control, leveraging its ability to capture rich
dynamic and behavioral information, its potential for enhancing bimanual
coordination remains underexplored. To bridge this gap, we propose a unified
diffusion-based framework for the joint optimization of video and action
prediction. Specifically, we propose a multi-frame latent prediction strategy
that encodes future states in a compressed latent space, preserving
task-relevant features. Furthermore, we introduce a unidirectional attention
mechanism where video prediction is conditioned on the action, while action
prediction remains independent of video prediction. This design allows us to
omit video prediction during inference, significantly enhancing efficiency.
Experiments on two simulated benchmarks and a real-world setting demonstrate a
significant improvement in the success rate over the strong baseline ACT using
our method, achieving a \textbf{24.9\%} increase on ALOHA, an \textbf{11.1\%}
increase on RoboTwin, and a \textbf{32.5\%} increase in real-world experiments.
Our models and code are publicly available at
https://github.com/return-sleep/Diffusion_based_imaginative_Coordination.

</details>


### [27] [All Eyes, no IMU: Learning Flight Attitude from Vision Alone](https://arxiv.org/abs/2507.11302)
*Jesse J. Hagenaars,Stein Stroobants,Sander M. Bohte,Guido C. H. E. De Croon*

Main category: cs.RO

TL;DR: 本文提出了一种仅依赖视觉的飞行控制方法，利用事件相机和神经网络替代传统惯性传感器，实现了无人机的稳定飞行。


<details>
  <summary>Details</summary>
Motivation: 许多飞行生物依赖视觉而非惯性传感器进行姿态控制，而现有飞行机器人通常依赖加速度计和陀螺仪。本文旨在探索仅依赖视觉的飞行控制方法。

Method: 使用向下事件相机和递归卷积神经网络，通过监督学习训练，从事件流中估计姿态和旋转速率。

Result: 实验表明，该方法能成功替代传统惯性测量单元，实现稳定飞行，并在不同环境中验证了泛化能力。

Conclusion: 视觉飞行控制是未来小型自主飞行机器人的可行方案，尤其在昆虫尺度机器人中具有潜力。

Abstract: Vision is an essential part of attitude control for many flying animals, some
of which have no dedicated sense of gravity. Flying robots, on the other hand,
typically depend heavily on accelerometers and gyroscopes for attitude
stabilization. In this work, we present the first vision-only approach to
flight control for use in generic environments. We show that a quadrotor drone
equipped with a downward-facing event camera can estimate its attitude and
rotation rate from just the event stream, enabling flight control without
inertial sensors. Our approach uses a small recurrent convolutional neural
network trained through supervised learning. Real-world flight tests
demonstrate that our combination of event camera and low-latency neural network
is capable of replacing the inertial measurement unit in a traditional flight
control loop. Furthermore, we investigate the network's generalization across
different environments, and the impact of memory and different fields of view.
While networks with memory and access to horizon-like visual cues achieve best
performance, variants with a narrower field of view achieve better relative
generalization. Our work showcases vision-only flight control as a promising
candidate for enabling autonomous, insect-scale flying robots.

</details>


### [28] [Acting and Planning with Hierarchical Operational Models on a Mobile Robot: A Study with RAE+UPOM](https://arxiv.org/abs/2507.11345)
*Oscar Lima,Marc Vinci,Sunandita Patra,Sebastian Stock,Joachim Hertzberg,Martin Atzmueller,Malik Ghallab,Dana Nau,Paolo Traverso*

Main category: cs.RO

TL;DR: 论文提出了一种集成执行器-规划器系统（RAE+UPOM），通过共享层次化操作模型，实现了在机器人任务执行中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决符号规划器模型与实际机器人控制结构之间的不一致性问题。

Method: 结合Reactive Acting Engine (RAE) 和 anytime UCT-like Monte Carlo planner (UPOM)，共享层次化操作模型。

Result: 在真实世界的物体收集任务中展示了鲁棒的任务执行能力，能够应对动作失败和传感器噪声。

Conclusion: RAE+UPOM系统为机器人任务执行提供了一种有效的集成规划和执行方法。

Abstract: Robotic task execution faces challenges due to the inconsistency between
symbolic planner models and the rich control structures actually running on the
robot. In this paper, we present the first physical deployment of an integrated
actor-planner system that shares hierarchical operational models for both
acting and planning, interleaving the Reactive Acting Engine (RAE) with an
anytime UCT-like Monte Carlo planner (UPOM). We implement RAE+UPOM on a mobile
manipulator in a real-world deployment for an object collection task. Our
experiments demonstrate robust task execution under action failures and sensor
noise, and provide empirical insights into the interleaved acting-and-planning
decision making process.

</details>


### [29] [From Production Logistics to Smart Manufacturing: The Vision for a New RoboCup Industrial League](https://arxiv.org/abs/2507.11402)
*Supun Dissanayaka,Alexander Ferrein,Till Hofmann,Kosuke Nakajima,Mario Sanz-Lopez,Jesus Savage,Daniel Swoboda,Matteo Tschesche,Wataru Uemura,Tarik Viehmann,Shohei Yasuda*

Main category: cs.RO

TL;DR: RoboCup Logistics League转型为RoboCup Smart Manufacturing League，以更全面地反映现代工厂的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有RoboCup Logistics League未能涵盖智能制造的最新发展，削弱了其相关性。

Method: 设计新的多赛道竞赛，涵盖工业机器人挑战（如装配、人机协作和人形机器人），同时保留生产物流重点。

Result: 新竞赛预计更具吸引力，并能聚焦当前和未来的工业机器人挑战。

Conclusion: 新竞赛将提升相关性并吸引更多参与者。

Abstract: The RoboCup Logistics League is a RoboCup competition in a smart factory
scenario that has focused on task planning, job scheduling, and multi-agent
coordination. The focus on production logistics allowed teams to develop highly
competitive strategies, but also meant that some recent developments in the
context of smart manufacturing are not reflected in the competition, weakening
its relevance over the years. In this paper, we describe the vision for the
RoboCup Smart Manufacturing League, a new competition designed as a larger
smart manufacturing scenario, reflecting all the major aspects of a modern
factory. It will consist of several tracks that are initially independent but
gradually combined into one smart manufacturing scenario. The new tracks will
cover industrial robotics challenges such as assembly, human-robot
collaboration, and humanoid robotics, but also retain a focus on production
logistics. We expect the reenvisioned competition to be more attractive to
newcomers and well-tried teams, while also shifting the focus to current and
future challenges of industrial robotics.

</details>


### [30] [Multi-IMU Sensor Fusion for Legged Robots](https://arxiv.org/abs/2507.11447)
*Shuo Yang,John Z. Zhang,Ibrahima Sory Sow,Zachary Manchester*

Main category: cs.RO

TL;DR: 本文提出了一种用于足式机器人的状态估计方法，通过低成本、紧凑且轻量化的传感器实现低漂移的位姿和速度估计。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂运动条件下标准本体感知里程计的主要误差源问题。

Method: 利用多个惯性测量单元和关节编码器数据，通过扩展卡尔曼滤波融合，再结合相机数据在因子图滑动窗口估计器中形成视觉-惯性-腿部里程计方法。

Result: 在多种挑战性运动任务中，算法表现出最小的位置偏差。

Conclusion: 该方法在剧烈地面冲击、足部滑动和突然身体旋转等场景下仍能保持稳定性能，提供了开源实现和数据集。

Abstract: This paper presents a state-estimation solution for legged robots that uses a
set of low-cost, compact, and lightweight sensors to achieve low-drift pose and
velocity estimation under challenging locomotion conditions. The key idea is to
leverage multiple inertial measurement units on different links of the robot to
correct a major error source in standard proprioceptive odometry. We fuse the
inertial sensor information and joint encoder measurements in an extended
Kalman filter, then combine the velocity estimate from this filter with camera
data in a factor-graph-based sliding-window estimator to form a
visual-inertial-leg odometry method. We validate our state estimator through
comprehensive theoretical analysis and hardware experiments performed using
real-world robot data collected during a variety of challenging locomotion
tasks. Our algorithm consistently achieves minimal position deviation, even in
scenarios involving substantial ground impact, foot slippage, and sudden body
rotations. A C++ implementation, along with a large-scale dataset, is available
at https://github.com/ShuoYangRobotics/Cerberus2.0.

</details>


### [31] [Human-Robot collaboration in surgery: Advances and challenges towards autonomous surgical assistants](https://arxiv.org/abs/2507.11460)
*Jacinto Colan,Ana Davila,Yutaro Yamada,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: 本文系统回顾了自主手术机器人助手（ASARs）的发展与挑战，重点关注其在复杂手术中为外科医生提供主动支持的场景。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索机器人如何更有效地辅助外科医生，提升手术的可靠性和安全性。

Method: 遵循PRISMA指南，对IEEE Xplore、Scopus和Web of Science数据库中的32项研究进行了详细分析。

Result: 研究发现ASARs在手术中的应用主要集中在内窥镜引导和自主工具操作上，但仍面临多项挑战。

Conclusion: 结论指出需进一步研究以解决人机协作中的关键问题，推动手术机器人的广泛采用。

Abstract: Human-robot collaboration in surgery represents a significant area of
research, driven by the increasing capability of autonomous robotic systems to
assist surgeons in complex procedures. This systematic review examines the
advancements and persistent challenges in the development of autonomous
surgical robotic assistants (ASARs), focusing specifically on scenarios where
robots provide meaningful and active support to human surgeons. Adhering to the
PRISMA guidelines, a comprehensive literature search was conducted across the
IEEE Xplore, Scopus, and Web of Science databases, resulting in the selection
of 32 studies for detailed analysis. Two primary collaborative setups were
identified: teleoperation-based assistance and direct hands-on interaction. The
findings reveal a growing research emphasis on ASARs, with predominant
applications currently in endoscope guidance, alongside emerging progress in
autonomous tool manipulation. Several key challenges hinder wider adoption,
including the alignment of robotic actions with human surgeon preferences, the
necessity for procedural awareness within autonomous systems, the establishment
of seamless human-robot information exchange, and the complexities of skill
acquisition in shared workspaces. This review synthesizes current trends,
identifies critical limitations, and outlines future research directions
essential to improve the reliability, safety, and effectiveness of human-robot
collaboration in surgical environments.

</details>


### [32] [LF: Online Multi-Robot Path Planning Meets Optimal Trajectory Control](https://arxiv.org/abs/2507.11464)
*Ajay Shankar,Keisuke Okumura,Amanda Prorok*

Main category: cs.RO

TL;DR: 提出了一种多机器人控制框架，结合集中式路径规划与分散式轨迹控制，实现高效、可扩展的点对点导航。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人在全环境信息下的点对点导航问题，同时避免碰撞和死锁，适应动态工作空间。

Method: 采用分层框架：1) 集中式离散路径规划（基于MAPF）；2) 分散式连续轨迹控制。结合LaCAM和Freyja实现。

Result: 框架支持15个真实多旋翼机器人在动态环境中的高效导航，适应异步目标更新和人机交互。

Conclusion: 该框架为多机器人导航提供了鲁棒且可扩展的解决方案，适用于动态和复杂环境。

Abstract: We propose a multi-robot control paradigm to solve point-to-point navigation
tasks for a team of holonomic robots with access to the full environment
information. The framework invokes two processes asynchronously at high
frequency: (i) a centralized, discrete, and full-horizon planner for computing
collision- and deadlock-free paths rapidly, leveraging recent advances in
multi-agent pathfinding (MAPF), and (ii) dynamics-aware, robot-wise optimal
trajectory controllers that ensure all robots independently follow their
assigned paths reliably. This hierarchical shift in planning representation
from (i) discrete and coupled to (ii) continuous and decoupled domains enables
the framework to maintain long-term scalable motion synthesis. As an
instantiation of this idea, we present LF, which combines a fast
state-of-the-art MAPF solver (LaCAM), and a robust feedback control stack
(Freyja) for executing agile robot maneuvers. LF provides a robust and
versatile mechanism for lifelong multi-robot navigation even under asynchronous
and partial goal updates, and adapts to dynamic workspaces simply by quick
replanning. We present various multirotor and ground robot demonstrations,
including the deployment of 15 real multirotors with random, consecutive target
updates while a person walks through the operational workspace.

</details>


### [33] [Robot Drummer: Learning Rhythmic Skills for Humanoid Drumming](https://arxiv.org/abs/2507.11498)
*Asad Ali Shahid,Francesco Braghin,Loris Roveda*

Main category: cs.RO

TL;DR: 论文介绍了一个名为Robot Drummer的人形机器人系统，能够通过强化学习实现高精度和表现力的鼓乐演奏。


<details>
  <summary>Details</summary>
Motivation: 探索人形机器人在音乐表演等表达性领域的潜力，尤其是鼓乐演奏中的快速反应和多肢协调挑战。

Method: 将鼓乐演奏转化为时序接触链，并将乐曲分解为固定长度片段，通过强化学习并行训练单一策略。

Result: 在三十多首摇滚、金属和爵士乐曲中，Robot Drummer表现出高F1分数，并展现出类似人类的鼓乐策略。

Conclusion: 研究表明强化学习能够帮助人形机器人进入创意音乐表演领域，展现出类似人类的演奏行为。

Abstract: Humanoid robots have seen remarkable advances in dexterity, balance, and
locomotion, yet their role in expressive domains, such as music performance,
remains largely unexplored. Musical tasks, like drumming, present unique
challenges, including split-second timing, rapid contacts, and multi-limb
coordination over pieces lasting minutes. In this paper, we introduce Robot
Drummer, a humanoid system capable of expressive, high-precision drumming
across a diverse repertoire of songs. We formulate humanoid drumming as
sequential fulfillment of timed-contacts and transform drum scores in to a
Rhythmic Contact Chain. To handle the long-horizon nature of musical
performance, we decompose each piece into fixed-length segments and train a
single policy across all segments in parallel using reinforcement learning.
Through extensive experiments on over thirty popular rock, metal, and jazz
tracks, our results demonstrate that Robot Drummer consistently achieves high
F1 scores. The learned behaviors exhibit emergent human-like drumming
strategies, such as cross-arm strikes, and adaptive sticks assignments,
demonstrating the potential of reinforcement learning to bring humanoid robots
into the domain of creative musical performance. Project page:
\href{https://robot-drummer.github.io}{robot-drummer.github.io}

</details>


### [34] [LLM-based ambiguity detection in natural language instructions for collaborative surgical robots](https://arxiv.org/abs/2507.11525)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: 提出了一种基于大语言模型（LLMs）的框架，用于检测手术场景中的自然语言指令歧义，提高人机协作的安全性。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言指令在安全关键领域（如手术）中的歧义问题，以减少人机交互中的风险。

Method: 使用多种提示技术配置的LLM评估器集合，结合链式思维评估器和共形预测，检测指令中的歧义。

Result: 在区分手术指令歧义方面，Llama 3.2 11B和Gemma 3 12B的分类准确率超过60%。

Conclusion: 该方法通过提前识别歧义指令，提高了手术中人机协作的安全性和可靠性。

Abstract: Ambiguity in natural language instructions poses significant risks in
safety-critical human-robot interaction, particularly in domains such as
surgery. To address this, we propose a framework that uses Large Language
Models (LLMs) for ambiguity detection specifically designed for collaborative
surgical scenarios. Our method employs an ensemble of LLM evaluators, each
configured with distinct prompting techniques to identify linguistic,
contextual, procedural, and critical ambiguities. A chain-of-thought evaluator
is included to systematically analyze instruction structure for potential
issues. Individual evaluator assessments are synthesized through conformal
prediction, which yields non-conformity scores based on comparison to a labeled
calibration dataset. Evaluating Llama 3.2 11B and Gemma 3 12B, we observed
classification accuracy exceeding 60% in differentiating ambiguous from
unambiguous surgical instructions. Our approach improves the safety and
reliability of human-robot collaboration in surgery by offering a mechanism to
identify potentially ambiguous instructions before robot action.

</details>
