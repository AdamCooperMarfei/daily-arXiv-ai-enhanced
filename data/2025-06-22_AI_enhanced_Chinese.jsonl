{"id": "2506.14855", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14855", "abs": "https://arxiv.org/abs/2506.14855", "authors": ["Tommaso Belvedere", "Michael Ziegltrum", "Giulio Turrisi", "Valerio Modugno"], "title": "Feedback-MPPI: Fast Sampling-Based MPC via Rollout Differentiation -- Adios low-level controllers", "comment": null, "summary": "Model Predictive Path Integral control is a powerful sampling-based approach\nsuitable for complex robotic tasks due to its flexibility in handling nonlinear\ndynamics and non-convex costs. However, its applicability in real-time,\nhighfrequency robotic control scenarios is limited by computational demands.\nThis paper introduces Feedback-MPPI (F-MPPI), a novel framework that augments\nstandard MPPI by computing local linear feedback gains derived from sensitivity\nanalysis inspired by Riccati-based feedback used in gradient-based MPC. These\ngains allow for rapid closed-loop corrections around the current state without\nrequiring full re-optimization at each timestep. We demonstrate the\neffectiveness of F-MPPI through simulations and real-world experiments on two\nrobotic platforms: a quadrupedal robot performing dynamic locomotion on uneven\nterrain and a quadrotor executing aggressive maneuvers with onboard\ncomputation. Results illustrate that incorporating local feedback significantly\nimproves control performance and stability, enabling robust, high-frequency\noperation suitable for complex robotic systems.", "AI": {"tldr": "F-MPPI\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u5c40\u90e8\u7ebf\u6027\u53cd\u9988\u589e\u76ca\uff0c\u63d0\u5347\u4e86MPPI\u63a7\u5236\u5728\u5b9e\u65f6\u9ad8\u9891\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4e0e\u7a33\u5b9a\u6027\u3002", "motivation": "MPPI\u63a7\u5236\u5728\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u8ba1\u7b97\u9700\u6c42\u9650\u5236\u4e86\u5176\u5728\u5b9e\u65f6\u9ad8\u9891\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "F-MPPI\u901a\u8fc7\u7075\u654f\u5ea6\u5206\u6790\u8ba1\u7b97\u5c40\u90e8\u7ebf\u6027\u53cd\u9988\u589e\u76ca\uff0c\u5b9e\u73b0\u5feb\u901f\u95ed\u73af\u6821\u6b63\uff0c\u65e0\u9700\u6bcf\u6b65\u91cd\u65b0\u4f18\u5316\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u8868\u660e\uff0cF-MPPI\u663e\u8457\u63d0\u5347\u4e86\u63a7\u5236\u6027\u80fd\u548c\u7a33\u5b9a\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "conclusion": "F-MPPI\u901a\u8fc7\u5c40\u90e8\u53cd\u9988\u589e\u76ca\uff0c\u6210\u529f\u89e3\u51b3\u4e86MPPI\u5728\u5b9e\u65f6\u9ad8\u9891\u63a7\u5236\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2506.14857", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14857", "abs": "https://arxiv.org/abs/2506.14857", "authors": ["Suman Raj", "Swapnil Padhi", "Ruchi Bhoot", "Prince Modi", "Yogesh Simmhan"], "title": "Towards Perception-based Collision Avoidance for UAVs when Guiding the Visually Impaired", "comment": "16 pages, 7 figures; Accepted as Late-Breaking Results at the\n  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n  2023", "summary": "Autonomous navigation by drones using onboard sensors combined with machine\nlearning and computer vision algorithms is impacting a number of domains,\nincluding agriculture, logistics, and disaster management. In this paper, we\nexamine the use of drones for assisting visually impaired people (VIPs) in\nnavigating through outdoor urban environments. Specifically, we present a\nperception-based path planning system for local planning around the\nneighborhood of the VIP, integrated with a global planner based on GPS and maps\nfor coarse planning. We represent the problem using a geometric formulation and\npropose a multi DNN based framework for obstacle avoidance of the UAV as well\nas the VIP. Our evaluations conducted on a drone human system in a university\ncampus environment verifies the feasibility of our algorithms in three\nscenarios; when the VIP walks on a footpath, near parked vehicles, and in a\ncrowded street.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u611f\u77e5\u7684\u8def\u5f84\u89c4\u5212\u7cfb\u7edf\uff0c\u7ed3\u5408\u65e0\u4eba\u673a\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u5e2e\u52a9\u89c6\u969c\u4eba\u58eb\u5728\u6237\u5916\u57ce\u5e02\u73af\u5883\u4e2d\u5bfc\u822a\u3002", "motivation": "\u901a\u8fc7\u65e0\u4eba\u673a\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u89e3\u51b3\u89c6\u969c\u4eba\u58eb\u5728\u6237\u5916\u5bfc\u822a\u4e2d\u7684\u969c\u788d\u7269\u907f\u8ba9\u95ee\u9898\u3002", "method": "\u91c7\u7528\u51e0\u4f55\u95ee\u9898\u5efa\u6a21\u548c\u591aDNN\u6846\u67b6\uff0c\u7ed3\u5408\u5c40\u90e8\u611f\u77e5\u89c4\u5212\u548c\u5168\u5c40GPS\u5730\u56fe\u89c4\u5212\u3002", "result": "\u5728\u4e09\u79cd\u573a\u666f\uff08\u4eba\u884c\u9053\u3001\u505c\u8f66\u533a\u3001\u62e5\u6324\u8857\u9053\uff09\u4e2d\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u7cfb\u7edf\u5728\u89c6\u969c\u4eba\u58eb\u5bfc\u822a\u4e2d\u8868\u73b0\u51fa\u5b9e\u7528\u6027\uff0c\u4e3a\u672a\u6765\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.14865", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.14865", "abs": "https://arxiv.org/abs/2506.14865", "authors": ["Xuemin Chi", "Hakan Girgin", "Tobias L\u00f6w", "Yangyang Xie", "Teng Xue", "Jihao Huang", "Cheng Hu", "Zhitao Liu", "Sylvain Calinon"], "title": "Efficient and Real-Time Motion Planning for Robotics Using Projection-Based Optimization", "comment": "submitted to IROS 2025", "summary": "Generating motions for robots interacting with objects of various shapes is a\ncomplex challenge, further complicated by the robot geometry and multiple\ndesired behaviors. While current robot programming tools (such as inverse\nkinematics, collision avoidance, and manipulation planning) often treat these\nproblems as constrained optimization, many existing solvers focus on specific\nproblem domains or do not exploit geometric constraints effectively. We propose\nan efficient first-order method, Augmented Lagrangian Spectral Projected\nGradient Descent (ALSPG), which leverages geometric projections via Euclidean\nprojections, Minkowski sums, and basis functions. We show that by using\ngeometric constraints rather than full constraints and gradients, ALSPG\nsignificantly improves real-time performance. Compared to second-order methods\nlike iLQR, ALSPG remains competitive in the unconstrained case. We validate our\nmethod through toy examples and extensive simulations, and demonstrate its\neffectiveness on a 7-axis Franka robot, a 6-axis P-Rob robot and a 1:10 scale\ncar in real-world experiments. Source codes, experimental data and videos are\navailable on the project webpage: https://sites.google.com/view/alspg-oc", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4e00\u9636\u65b9\u6cd5ALSPG\uff0c\u5229\u7528\u51e0\u4f55\u6295\u5f71\u4f18\u5316\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u9762\u4e34\u590d\u6742\u51e0\u4f55\u548c\u591a\u884c\u4e3a\u9700\u6c42\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u51e0\u4f55\u7ea6\u675f\u3002", "method": "\u91c7\u7528ALSPG\u65b9\u6cd5\uff0c\u7ed3\u5408\u6b27\u51e0\u91cc\u5f97\u6295\u5f71\u3001Minkowski\u548c\u57fa\u51fd\u6570\u3002", "result": "ALSPG\u5728\u5b9e\u65f6\u6027\u80fd\u4e0a\u4f18\u4e8e\u4e8c\u9636\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u3002", "conclusion": "ALSPG\u901a\u8fc7\u51e0\u4f55\u7ea6\u675f\u4f18\u5316\uff0c\u4e3a\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.14968", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14968", "abs": "https://arxiv.org/abs/2506.14968", "authors": ["Rajat Kumar Jenamani", "Tom Silver", "Ben Dodson", "Shiqin Tong", "Anthony Song", "Yuting Yang", "Ziang Liu", "Benjamin Howe", "Aimee Whitneck", "Tapomayukh Bhattacharjee"], "title": "FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild Personalization", "comment": "RSS 2025 - Outstanding Paper Award & Outstanding Systems Paper Award\n  Finalist", "summary": "Physical caregiving robots hold promise for improving the quality of life of\nmillions worldwide who require assistance with feeding. However, in-home meal\nassistance remains challenging due to the diversity of activities (e.g.,\neating, drinking, mouth wiping), contexts (e.g., socializing, watching TV),\nfood items, and user preferences that arise during deployment. In this work, we\npropose FEAST, a flexible mealtime-assistance system that can be personalized\nin-the-wild to meet the unique needs of individual care recipients. Developed\nin collaboration with two community researchers and informed by a formative\nstudy with a diverse group of care recipients, our system is guided by three\nkey tenets for in-the-wild personalization: adaptability, transparency, and\nsafety. FEAST embodies these principles through: (i) modular hardware that\nenables switching between assisted feeding, drinking, and mouth-wiping, (ii)\ndiverse interaction methods, including a web interface, head gestures, and\nphysical buttons, to accommodate diverse functional abilities and preferences,\nand (iii) parameterized behavior trees that can be safely and transparently\nadapted using a large language model. We evaluate our system based on the\npersonalization requirements identified in our formative study, demonstrating\nthat FEAST offers a wide range of transparent and safe adaptations and\noutperforms a state-of-the-art baseline limited to fixed customizations. To\ndemonstrate real-world applicability, we conduct an in-home user study with two\ncare recipients (who are community researchers), feeding them three meals each\nacross three diverse scenarios. We further assess FEAST's ecological validity\nby evaluating with an Occupational Therapist previously unfamiliar with the\nsystem. In all cases, users successfully personalize FEAST to meet their\nindividual needs and preferences. Website: https://emprise.cs.cornell.edu/feast", "AI": {"tldr": "FEAST\u662f\u4e00\u4e2a\u7075\u6d3b\u7684\u5bb6\u5ead\u7528\u9910\u8f85\u52a9\u7cfb\u7edf\uff0c\u652f\u6301\u4e2a\u6027\u5316\u5b9a\u5236\uff0c\u9002\u5e94\u4e0d\u540c\u7528\u6237\u9700\u6c42\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u786c\u4ef6\u548c\u591a\u6837\u5316\u4ea4\u4e92\u65b9\u5f0f\u5b9e\u73b0\u5b89\u5168\u900f\u660e\u7684\u4e2a\u6027\u5316\u670d\u52a1\u3002", "motivation": "\u89e3\u51b3\u5bb6\u5ead\u7528\u9910\u8f85\u52a9\u673a\u5668\u4eba\u9762\u4e34\u7684\u591a\u6837\u6027\u6311\u6218\uff0c\u5982\u6d3b\u52a8\u3001\u573a\u666f\u3001\u98df\u7269\u548c\u7528\u6237\u504f\u597d\u7684\u5dee\u5f02\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u786c\u4ef6\u3001\u591a\u6837\u5316\u4ea4\u4e92\u65b9\u5f0f\u548c\u53c2\u6570\u5316\u884c\u4e3a\u6811\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u5b89\u5168\u900f\u660e\u7684\u4e2a\u6027\u5316\u5b9a\u5236\u3002", "result": "FEAST\u5728\u900f\u660e\u5ea6\u548c\u5b89\u5168\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u548c\u804c\u4e1a\u6cbb\u7597\u5e08\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u9002\u7528\u6027\u3002", "conclusion": "FEAST\u80fd\u591f\u6709\u6548\u6ee1\u8db3\u4e0d\u540c\u7528\u6237\u7684\u4e2a\u6027\u5316\u9700\u6c42\uff0c\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u751f\u6001\u6548\u5ea6\u3002"}}
{"id": "2506.14975", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.14975", "abs": "https://arxiv.org/abs/2506.14975", "authors": ["Jeffrey Mao", "Raghuram Cauligi Srinivas", "Steven Nogar", "Giuseppe Loianno"], "title": "Time-Optimized Safe Navigation in Unstructured Environments through Learning Based Depth Completion", "comment": null, "summary": "Quadrotors hold significant promise for several applications such as\nagriculture, search and rescue, and infrastructure inspection. Achieving\nautonomous operation requires systems to navigate safely through complex and\nunfamiliar environments. This level of autonomy is particularly challenging due\nto the complexity of such environments and the need for real-time decision\nmaking especially for platforms constrained by size, weight, and power (SWaP),\nwhich limits flight time and precludes the use of bulky sensors like Light\nDetection and Ranging (LiDAR) for mapping. Furthermore, computing globally\noptimal, collision-free paths and translating them into time-optimized, safe\ntrajectories in real time adds significant computational complexity. To address\nthese challenges, we present a fully onboard, real-time navigation system that\nrelies solely on lightweight onboard sensors. Our system constructs a dense 3D\nmap of the environment using a novel visual depth estimation approach that\nfuses stereo and monocular learning-based depth, yielding longer-range, denser,\nand less noisy depth maps than conventional stereo methods. Building on this\nmap, we introduce a novel planning and trajectory generation framework capable\nof rapidly computing time-optimal global trajectories. As the map is\nincrementally updated with new depth information, our system continuously\nrefines the trajectory to maintain safety and optimality. Both our planner and\ntrajectory generator outperforms state-of-the-art methods in terms of\ncomputational efficiency and guarantee obstacle-free trajectories. We validate\nour system through robust autonomous flight experiments in diverse indoor and\noutdoor environments, demonstrating its effectiveness for safe navigation in\npreviously unknown settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u4f20\u611f\u5668\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5b9e\u65f6\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u878d\u5408\u7acb\u4f53\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6784\u5efa\u5bc6\u96c63D\u5730\u56fe\uff0c\u5e76\u5b9e\u73b0\u9ad8\u6548\u5168\u5c40\u8def\u5f84\u89c4\u5212\u548c\u8f68\u8ff9\u4f18\u5316\u3002", "motivation": "\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u590d\u6742\u672a\u77e5\u73af\u5883\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u5bfc\u822a\u9762\u4e34\u4f20\u611f\u5668\u9650\u5236\u548c\u5b9e\u65f6\u8ba1\u7b97\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u7acb\u4f53\u548c\u5355\u76ee\u5b66\u4e60\u6df1\u5ea6\u4f30\u8ba1\u6784\u5efa3D\u5730\u56fe\uff0c\u5f00\u53d1\u5feb\u901f\u5168\u5c40\u8f68\u8ff9\u89c4\u5212\u548c\u4f18\u5316\u6846\u67b6\u3002", "result": "\u7cfb\u7edf\u5728\u8ba1\u7b97\u6548\u7387\u548c\u907f\u969c\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5ba4\u5185\u5916\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u8f7b\u91cf\u7ea7\u65e0\u4eba\u673a\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.15009", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15009", "abs": "https://arxiv.org/abs/2506.15009", "authors": ["Jinjie Li", "Jiaxuan Li", "Kotaro Kaneko", "Liming Shu", "Moju Zhao"], "title": "Six-DoF Hand-Based Teleoperation for Omnidirectional Aerial Robots", "comment": "7 pages, 9 figures. This work has been accepted to IROS 2025. The\n  video will be released soon", "summary": "Omnidirectional aerial robots offer full 6-DoF independent control over\nposition and orientation, making them popular for aerial manipulation. Although\nadvancements in robotic autonomy, operating by human remains essential in\ncomplex aerial environments. Existing teleoperation approaches for multirotors\nfail to fully leverage the additional DoFs provided by omnidirectional\nrotation. Additionally, the dexterity of human fingers should be exploited for\nmore engaged interaction. In this work, we propose an aerial teleoperation\nsystem that brings the omnidirectionality of human hands into the unbounded\naerial workspace. Our system includes two motion-tracking marker sets -- one on\nthe shoulder and one on the hand -- along with a data glove to capture hand\ngestures. Using these inputs, we design four interaction modes for different\ntasks, including Spherical Mode and Cartesian Mode for long-range moving as\nwell as Operation Mode and Locking Mode for precise manipulation, where the\nhand gestures are utilized for seamless mode switching. We evaluate our system\non a valve-turning task in real world, demonstrating how each mode contributes\nto effective aerial manipulation. This interaction framework bridges human\ndexterity with aerial robotics, paving the way for enhanced teleoperated aerial\nmanipulation in unstructured environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4eba\u4f53\u624b\u90e8\u5168\u65b9\u4f4d\u8fd0\u52a8\u63a7\u5236\u7a7a\u4e2d\u673a\u5668\u4eba\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u80a9\u90e8\u548c\u624b\u90e8\u8fd0\u52a8\u8ffd\u8e2a\u53ca\u6570\u636e\u624b\u5957\u6355\u6349\u624b\u52bf\uff0c\u8bbe\u8ba1\u4e86\u56db\u79cd\u4ea4\u4e92\u6a21\u5f0f\uff0c\u5e76\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u5168\u65b9\u4f4d\u65cb\u8f6c\u7684\u81ea\u7531\u5ea6\uff0c\u4e14\u672a\u5145\u5206\u7ed3\u5408\u4eba\u7c7b\u624b\u6307\u7684\u7075\u6d3b\u6027\u3002", "method": "\u7cfb\u7edf\u5305\u62ec\u80a9\u90e8\u548c\u624b\u90e8\u8fd0\u52a8\u8ffd\u8e2a\u6807\u8bb0\u53ca\u6570\u636e\u624b\u5957\uff0c\u8bbe\u8ba1\u4e86\u56db\u79cd\u4ea4\u4e92\u6a21\u5f0f\uff08\u7403\u5f62\u3001\u7b1b\u5361\u5c14\u3001\u64cd\u4f5c\u548c\u9501\u5b9a\u6a21\u5f0f\uff09\u3002", "result": "\u5728\u771f\u5b9e\u9600\u95e8\u8f6c\u52a8\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5404\u6a21\u5f0f\u5bf9\u7a7a\u4e2d\u64cd\u4f5c\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06\u4eba\u7c7b\u7075\u6d3b\u6027\u4e0e\u7a7a\u4e2d\u673a\u5668\u4eba\u7ed3\u5408\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u8fdc\u7a0b\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.15012", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15012", "abs": "https://arxiv.org/abs/2506.15012", "authors": ["Alexandra Forsey-Smerek", "Julie Shah", "Andreea Bobu"], "title": "Context Matters: Learning Generalizable Rewards via Calibrated Features", "comment": "30 pages, 21 figures", "summary": "A key challenge in reward learning from human input is that desired agent\nbehavior often changes based on context. Traditional methods typically treat\neach new context as a separate task with its own reward function. For example,\nif a previously ignored stove becomes too hot to be around, the robot must\nlearn a new reward from scratch, even though the underlying preference for\nprioritizing safety over efficiency remains unchanged. We observe that context\ninfluences not the underlying preference itself, but rather the\n$\\textit{saliency}$--or importance--of reward features. For instance, stove\nheat affects the importance of the robot's proximity, yet the human's safety\npreference stays the same. Existing multi-task and meta IRL methods learn\ncontext-dependent representations $\\textit{implicitly}$--without distinguishing\nbetween preferences and feature importance--resulting in substantial data\nrequirements. Instead, we propose $\\textit{explicitly}$ modeling\ncontext-invariant preferences separately from context-dependent feature\nsaliency, creating modular reward representations that adapt to new contexts.\nTo achieve this, we introduce $\\textit{calibrated features}$--representations\nthat capture contextual effects on feature saliency--and present specialized\npaired comparison queries that isolate saliency from preference for efficient\nlearning. Experiments with simulated users show our method significantly\nimproves sample efficiency, requiring 10x fewer preference queries than\nbaselines to achieve equivalent reward accuracy, with up to 15% better\nperformance in low-data regimes (5-10 queries). An in-person user study (N=12)\ndemonstrates that participants can effectively teach their unique personal\ncontextual preferences using our method, enabling more adaptable and\npersonalized reward learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u663e\u5f0f\u5efa\u6a21\u4e0a\u4e0b\u6587\u4e0d\u53d8\u504f\u597d\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u7279\u5f81\u663e\u8457\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6821\u51c6\u7279\u5f81\u548c\u4e13\u7528\u67e5\u8be2\u63d0\u9ad8\u5956\u52b1\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u6bcf\u4e2a\u65b0\u4e0a\u4e0b\u6587\u89c6\u4e3a\u72ec\u7acb\u4efb\u52a1\uff0c\u5bfc\u81f4\u6570\u636e\u9700\u6c42\u5927\u4e14\u6548\u7387\u4f4e\u3002\u672c\u6587\u89c2\u5bdf\u5230\u4e0a\u4e0b\u6587\u5f71\u54cd\u7684\u662f\u7279\u5f81\u663e\u8457\u6027\u800c\u975e\u504f\u597d\u672c\u8eab\uff0c\u56e0\u6b64\u63d0\u51fa\u663e\u5f0f\u5206\u79bb\u5efa\u6a21\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u5f15\u5165\u6821\u51c6\u7279\u5f81\u8868\u793a\u4e0a\u4e0b\u6587\u5bf9\u7279\u5f81\u663e\u8457\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u4f7f\u7528\u4e13\u7528\u914d\u5bf9\u6bd4\u8f83\u67e5\u8be2\u5206\u79bb\u663e\u8457\u6027\u548c\u504f\u597d\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6837\u672c\u6548\u7387\u663e\u8457\u63d0\u9ad8\uff0c\u4ec5\u9700\u57fa\u7ebf1/10\u7684\u67e5\u8be2\u91cf\u5373\u53ef\u8fbe\u5230\u76f8\u540c\u5956\u52b1\u7cbe\u5ea6\uff0c\u4f4e\u6570\u636e\u91cf\u4e0b\u6027\u80fd\u63d0\u534715%\u3002\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u663e\u5f0f\u5206\u79bb\u5efa\u6a21\u504f\u597d\u548c\u7279\u5f81\u663e\u8457\u6027\u80fd\u663e\u8457\u63d0\u5347\u5956\u52b1\u5b66\u4e60\u7684\u9002\u5e94\u6027\u548c\u4e2a\u6027\u5316\u80fd\u529b\u3002"}}
{"id": "2506.15032", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15032", "abs": "https://arxiv.org/abs/2506.15032", "authors": ["Winston Smith", "Andrew Boateng", "Taha Shaheen", "Yu Zhang"], "title": "Assigning Multi-Robot Tasks to Multitasking Robots", "comment": null, "summary": "One simplifying assumption in existing and well-performing task allocation\nmethods is that the robots are single-tasking: each robot operates on a single\ntask at any given time. While this assumption is harmless to make in some\nsituations, it can be inefficient or even infeasible in others. In this paper,\nwe consider assigning multi-robot tasks to multitasking robots. The key\ncontribution is a novel task allocation framework that incorporates the\nconsideration of physical constraints introduced by multitasking. This is in\ncontrast to the existing work where such constraints are largely ignored. After\nformulating the problem, we propose a compilation to weighted MAX-SAT, which\nallows us to leverage existing solvers for a solution. A more efficient greedy\nheuristic is then introduced. For evaluation, we first compare our methods with\na modern baseline that is efficient for single-tasking robots to validate the\nbenefits of multitasking in synthetic domains. Then, using a site-clearing\nscenario in simulation, we further illustrate the complex task interaction\nconsidered by the multitasking robots in our approach to demonstrate its\nperformance. Finally, we demonstrate a physical experiment to show how\nmultitasking enabled by our approach can benefit task efficiency in a realistic\nsetting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4efb\u52a1\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u6846\u67b6\uff0c\u8003\u8651\u4e86\u7269\u7406\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7\u52a0\u6743MAX-SAT\u7f16\u8bd1\u548c\u8d2a\u5fc3\u542f\u53d1\u5f0f\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u6548\u6c42\u89e3\u3002", "motivation": "\u73b0\u6709\u4efb\u52a1\u5206\u914d\u65b9\u6cd5\u5047\u8bbe\u673a\u5668\u4eba\u5355\u4efb\u52a1\u8fd0\u884c\uff0c\u6548\u7387\u4f4e\u6216\u4e0d\u73b0\u5b9e\uff0c\u9700\u89e3\u51b3\u591a\u4efb\u52a1\u5206\u914d\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u65b0\u6846\u67b6\uff0c\u8003\u8651\u7269\u7406\u7ea6\u675f\uff0c\u7f16\u8bd1\u4e3a\u52a0\u6743MAX-SAT\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u8d2a\u5fc3\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "\u5728\u5408\u6210\u57df\u548c\u6a21\u62df\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u591a\u4efb\u52a1\u5206\u914d\u7684\u4f18\u52bf\uff0c\u5e76\u901a\u8fc7\u7269\u7406\u5b9e\u9a8c\u5c55\u793a\u4e86\u5b9e\u9645\u6548\u7387\u63d0\u5347\u3002", "conclusion": "\u591a\u4efb\u52a1\u5206\u914d\u6846\u67b6\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6548\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2506.15085", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.15085", "abs": "https://arxiv.org/abs/2506.15085", "authors": ["Paige Tutt\u00f6s\u00ed", "Shivam Mehta", "Zachary Syvenky", "Bermet Burkanova", "Gustav Eje Henter", "Angelica Lim"], "title": "EmojiVoice: Towards long-term controllable expressivity in robot speech", "comment": "Accepted to RO-MAN 2025, Demo at HRI 2025 :\n  https://dl.acm.org/doi/10.5555/3721488.3721774", "summary": "Humans vary their expressivity when speaking for extended periods to maintain\nengagement with their listener. Although social robots tend to be deployed with\n``expressive'' joyful voices, they lack this long-term variation found in human\nspeech. Foundation model text-to-speech systems are beginning to mimic the\nexpressivity in human speech, but they are difficult to deploy offline on\nrobots. We present EmojiVoice, a free, customizable text-to-speech (TTS)\ntoolkit that allows social roboticists to build temporally variable, expressive\nspeech on social robots. We introduce emoji-prompting to allow fine-grained\ncontrol of expressivity on a phase level and use the lightweight Matcha-TTS\nbackbone to generate speech in real-time. We explore three case studies: (1) a\nscripted conversation with a robot assistant, (2) a storytelling robot, and (3)\nan autonomous speech-to-speech interactive agent. We found that using varied\nemoji prompting improved the perception and expressivity of speech over a long\nperiod in a storytelling task, but expressive voice was not preferred in the\nassistant use case.", "AI": {"tldr": "EmojiVoice\u662f\u4e00\u4e2a\u514d\u8d39\u3001\u53ef\u5b9a\u5236\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u5de5\u5177\u5305\uff0c\u65e8\u5728\u4e3a\u793e\u4ea4\u673a\u5668\u4eba\u63d0\u4f9b\u957f\u671f\u53d8\u5316\u7684\u8868\u8fbe\u6027\u8bed\u97f3\u3002", "motivation": "\u793e\u4ea4\u673a\u5668\u4eba\u901a\u5e38\u4f7f\u7528\u5355\u8c03\u7684\u2018\u5feb\u4e50\u2019\u8bed\u97f3\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u8bed\u97f3\u4e2d\u7684\u957f\u671f\u53d8\u5316\u3002\u73b0\u6709\u7684\u57fa\u7840\u6a21\u578bTTS\u7cfb\u7edf\u96be\u4ee5\u79bb\u7ebf\u90e8\u7f72\u3002", "method": "\u4f7f\u7528emoji\u63d0\u793a\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u57fa\u4e8e\u8f7b\u91cf\u7ea7Matcha-TTS\u5b9e\u65f6\u751f\u6210\u8bed\u97f3\u3002\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u6548\u679c\u3002", "result": "\u5728\u8bb2\u6545\u4e8b\u4efb\u52a1\u4e2d\uff0c\u53d8\u5316\u7684emoji\u63d0\u793a\u63d0\u5347\u4e86\u8bed\u97f3\u7684\u611f\u77e5\u548c\u8868\u8fbe\u6027\uff0c\u4f46\u5728\u52a9\u624b\u573a\u666f\u4e2d\u8868\u8fbe\u6027\u8bed\u97f3\u4e0d\u53d7\u6b22\u8fce\u3002", "conclusion": "EmojiVoice\u4e3a\u793e\u4ea4\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8868\u8fbe\u6027\u8bed\u97f3\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u6839\u636e\u573a\u666f\u8c03\u6574\u4f7f\u7528\u3002"}}
{"id": "2506.15087", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15087", "abs": "https://arxiv.org/abs/2506.15087", "authors": ["Yuankai Lin", "Xiaofan Lu", "Jiahui Chen", "Hua Yang"], "title": "3D Vision-tactile Reconstruction from Infrared and Visible Images for Robotic Fine-grained Tactile Perception", "comment": null, "summary": "To achieve human-like haptic perception in anthropomorphic grippers, the\ncompliant sensing surfaces of vision tactile sensor (VTS) must evolve from\nconventional planar configurations to biomimetically curved topographies with\ncontinuous surface gradients. However, planar VTSs have challenges when\nextended to curved surfaces, including insufficient lighting of surfaces,\nblurring in reconstruction, and complex spatial boundary conditions for surface\nstructures. With an end goal of constructing a human-like fingertip, our\nresearch (i) develops GelSplitter3D by expanding imaging channels with a prism\nand a near-infrared (NIR) camera, (ii) proposes a photometric stereo neural\nnetwork with a CAD-based normal ground truth generation method to calibrate\ntactile geometry, and (iii) devises a normal integration method with boundary\nconstraints of depth prior information to correcting the cumulative error of\nsurface integrals. We demonstrate better tactile sensing performance, a 40$\\%$\nimprovement in normal estimation accuracy, and the benefits of sensor shapes in\ngrasping and manipulation tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\uff08VTS\uff09\uff0c\u901a\u8fc7GelSplitter3D\u3001\u5149\u6d4b\u7acb\u4f53\u795e\u7ecf\u7f51\u7edc\u548c\u8fb9\u754c\u7ea6\u675f\u6cd5\uff0c\u63d0\u5347\u4e86\u66f2\u9762\u89e6\u89c9\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5e73\u9762VTS\u5728\u66f2\u9762\u5e94\u7528\u4e2d\u5b58\u5728\u5149\u7167\u4e0d\u8db3\u3001\u91cd\u5efa\u6a21\u7cca\u548c\u590d\u6742\u8fb9\u754c\u6761\u4ef6\u7b49\u95ee\u9898\uff0c\u9700\u6539\u8fdb\u4ee5\u5b9e\u73b0\u7c7b\u4eba\u89e6\u89c9\u611f\u77e5\u3002", "method": "1. \u5f00\u53d1GelSplitter3D\uff0c\u6269\u5c55\u6210\u50cf\u901a\u9053\uff1b2. \u63d0\u51fa\u5149\u6d4b\u7acb\u4f53\u795e\u7ecf\u7f51\u7edc\u548cCAD\u6cd5\u6821\u51c6\u89e6\u89c9\u51e0\u4f55\uff1b3. \u8bbe\u8ba1\u8fb9\u754c\u7ea6\u675f\u6cd5\u6821\u6b63\u79ef\u5206\u8bef\u5dee\u3002", "result": "\u89e6\u89c9\u611f\u77e5\u6027\u80fd\u63d0\u5347\uff0c\u6cd5\u7ebf\u4f30\u8ba1\u7cbe\u5ea6\u63d0\u9ad840%\uff0c\u4f20\u611f\u5668\u5f62\u72b6\u5728\u6293\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u66f2\u9762VTS\u7684\u6311\u6218\uff0c\u4e3a\u7c7b\u4eba\u6307\u5c16\u89e6\u89c9\u611f\u77e5\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.15096", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15096", "abs": "https://arxiv.org/abs/2506.15096", "authors": ["Zihe Ji", "Huangxuan Lin", "Yue Gao"], "title": "DyNaVLM: Zero-Shot Vision-Language Navigation System with Dynamic Viewpoints and Self-Refining Graph Memory", "comment": null, "summary": "We present DyNaVLM, an end-to-end vision-language navigation framework using\nVision-Language Models (VLM). In contrast to prior methods constrained by fixed\nangular or distance intervals, our system empowers agents to freely select\nnavigation targets via visual-language reasoning. At its core lies a\nself-refining graph memory that 1) stores object locations as executable\ntopological relations, 2) enables cross-robot memory sharing through\ndistributed graph updates, and 3) enhances VLM's decision-making via retrieval\naugmentation. Operating without task-specific training or fine-tuning, DyNaVLM\ndemonstrates high performance on GOAT and ObjectNav benchmarks. Real-world\ntests further validate its robustness and generalization. The system's three\ninnovations: dynamic action space formulation, collaborative graph memory, and\ntraining-free deployment, establish a new paradigm for scalable embodied robot,\nbridging the gap between discrete VLN tasks and continuous real-world\nnavigation.", "AI": {"tldr": "DyNaVLM\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5b9e\u73b0\u81ea\u7531\u9009\u62e9\u5bfc\u822a\u76ee\u6807\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u6216\u5fae\u8c03\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u56fa\u5b9a\u89d2\u5ea6\u6216\u8ddd\u79bb\u95f4\u9694\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5bfc\u822a\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u91c7\u7528\u81ea\u4f18\u5316\u56fe\u8bb0\u5fc6\u5b58\u50a8\u5bf9\u8c61\u4f4d\u7f6e\uff0c\u652f\u6301\u5206\u5e03\u5f0f\u56fe\u66f4\u65b0\u548c\u68c0\u7d22\u589e\u5f3a\uff0c\u52a8\u6001\u751f\u6210\u52a8\u4f5c\u7a7a\u95f4\u3002", "result": "\u5728GOAT\u548cObjectNav\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u9645\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DyNaVLM\u7684\u521b\u65b0\u5305\u62ec\u52a8\u6001\u52a8\u4f5c\u7a7a\u95f4\u3001\u534f\u4f5c\u56fe\u8bb0\u5fc6\u548c\u65e0\u8bad\u7ec3\u90e8\u7f72\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u673a\u5668\u4eba\u5bfc\u822a\u8bbe\u5b9a\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2506.15107", "categories": ["cs.RO", "cs.HC", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.15107", "abs": "https://arxiv.org/abs/2506.15107", "authors": ["Paige Tutt\u00f6s\u00ed"], "title": "I Know You're Listening: Adaptive Voice for HRI", "comment": "PhD Thesis Simon Fraser University https://summit.sfu.ca/item/39353\n  Read the Room: Adapting a Robot's Voice to Ambient and Social Contexts IROS\n  23 Mmm whatcha say? Uncovering distal and proximal context effects in first\n  and second-language word perception using psychophysical reverse correlation\n  INTERSPEECH 24 Emojivoice: Towards long-term controllable expressivity in\n  robot speech RO-MAN 25", "summary": "While the use of social robots for language teaching has been explored, there\nremains limited work on a task-specific synthesized voices for language\nteaching robots. Given that language is a verbal task, this gap may have severe\nconsequences for the effectiveness of robots for language teaching tasks. We\naddress this lack of L2 teaching robot voices through three contributions: 1.\nWe address the need for a lightweight and expressive robot voice. Using a\nfine-tuned version of Matcha-TTS, we use emoji prompting to create an\nexpressive voice that shows a range of expressivity over time. The voice can\nrun in real time with limited compute resources. Through case studies, we found\nthis voice more expressive, socially appropriate, and suitable for long periods\nof expressive speech, such as storytelling. 2. We explore how to adapt a\nrobot's voice to physical and social ambient environments to deploy our voices\nin various locations. We found that increasing pitch and pitch rate in noisy\nand high-energy environments makes the robot's voice appear more appropriate\nand makes it seem more aware of its current environment. 3. We create an\nEnglish TTS system with improved clarity for L2 listeners using known\nlinguistic properties of vowels that are difficult for these listeners. We used\na data-driven, perception-based approach to understand how L2 speakers use\nduration cues to interpret challenging words with minimal tense (long) and lax\n(short) vowels in English. We found that the duration of vowels strongly\ninfluences the perception for L2 listeners and created an \"L2 clarity mode\" for\nMatcha-TTS that applies a lengthening to tense vowels while leaving lax vowels\nunchanged. Our clarity mode was found to be more respectful, intelligible, and\nencouraging than base Matcha-TTS while reducing transcription errors in these\nchallenging tense/lax minimal pairs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u8bed\u8a00\u6559\u5b66\u673a\u5668\u4eba\u7f3a\u4e4f\u4efb\u52a1\u7279\u5b9a\u5408\u6210\u8bed\u97f3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u8868\u8fbe\u529b\u5f3a\u7684\u8bed\u97f3\u7cfb\u7edf\uff0c\u5e76\u63a2\u7d22\u4e86\u73af\u5883\u9002\u5e94\u6027\u53ca\u9488\u5bf9L2\u5b66\u4e60\u8005\u7684\u6e05\u6670\u5ea6\u6539\u8fdb\u3002", "motivation": "\u8bed\u8a00\u6559\u5b66\u673a\u5668\u4eba\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u4efb\u52a1\u7684\u5408\u6210\u8bed\u97f3\uff0c\u53ef\u80fd\u5f71\u54cd\u5176\u6559\u5b66\u6548\u679c\u3002", "method": "1. \u4f7f\u7528\u5fae\u8c03\u7248Matcha-TTS\uff0c\u901a\u8fc7\u8868\u60c5\u7b26\u53f7\u63d0\u793a\u751f\u6210\u5b9e\u65f6\u8fd0\u884c\u7684\u8868\u8fbe\u529b\u5f3a\u8bed\u97f3\uff1b2. \u8c03\u6574\u8bed\u97f3\u4ee5\u9002\u5e94\u7269\u7406\u548c\u793e\u4ea4\u73af\u5883\uff1b3. \u5f00\u53d1\u9488\u5bf9L2\u5b66\u4e60\u8005\u7684\u6e05\u6670\u8bed\u97f3\u6a21\u5f0f\uff0c\u5ef6\u957f\u7d27\u5f20\u5143\u97f3\u3002", "result": "\u8bed\u97f3\u66f4\u5177\u8868\u8fbe\u529b\u3001\u793e\u4ea4\u9002\u5e94\u6027\uff0c\u4e14\u5bf9L2\u5b66\u4e60\u8005\u66f4\u6e05\u6670\u3001\u5c0a\u91cd\u548c\u9f13\u52b1\uff0c\u51cf\u5c11\u8f6c\u5f55\u9519\u8bef\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bed\u97f3\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6559\u5b66\u673a\u5668\u4eba\u7684\u8868\u8fbe\u529b\u548c\u6559\u5b66\u6548\u679c\u3002"}}
{"id": "2506.15126", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15126", "abs": "https://arxiv.org/abs/2506.15126", "authors": ["Bingbing Zhang", "Huan Yin", "Shuo Liu", "Fumin Zhang", "Wen Xu"], "title": "VIMS: A Visual-Inertial-Magnetic-Sonar SLAM System in Underwater Environments", "comment": "This work has been accepted for publication at the IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)", "summary": "In this study, we present a novel simultaneous localization and mapping\n(SLAM) system, VIMS, designed for underwater navigation. Conventional\nvisual-inertial state estimators encounter significant practical challenges in\nperceptually degraded underwater environments, particularly in scale estimation\nand loop closing. To address these issues, we first propose leveraging a\nlow-cost single-beam sonar to improve scale estimation. Then, VIMS integrates a\nhigh-sampling-rate magnetometer for place recognition by utilizing magnetic\nsignatures generated by an economical magnetic field coil. Building on this, a\nhierarchical scheme is developed for visual-magnetic place recognition,\nenabling robust loop closure. Furthermore, VIMS achieves a balance between\nlocal feature tracking and descriptor-based loop closing, avoiding additional\ncomputational burden on the front end. Experimental results highlight the\nefficacy of the proposed VIMS, demonstrating significant improvements in both\nthe robustness and accuracy of state estimation within underwater environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVIMS\u7684\u65b0\u578b\u6c34\u4e0bSLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u4f4e\u6210\u672c\u5355\u6ce2\u675f\u58f0\u7eb3\u548c\u9ad8\u91c7\u6837\u7387\u78c1\u529b\u8ba1\uff0c\u89e3\u51b3\u4e86\u6c34\u4e0b\u73af\u5883\u4e2d\u5c3a\u5ea6\u4f30\u8ba1\u548c\u95ed\u73af\u68c0\u6d4b\u7684\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9-\u60ef\u6027\u72b6\u6001\u4f30\u8ba1\u5668\u5728\u6c34\u4e0b\u73af\u5883\u4e2d\u9762\u4e34\u5c3a\u5ea6\u4f30\u8ba1\u548c\u95ed\u73af\u68c0\u6d4b\u7684\u56f0\u96be\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u548c\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u5355\u6ce2\u675f\u58f0\u7eb3\u6539\u8fdb\u5c3a\u5ea6\u4f30\u8ba1\uff0c\u7ed3\u5408\u78c1\u529b\u8ba1\u8fdb\u884c\u5730\u70b9\u8bc6\u522b\uff0c\u5e76\u5f00\u53d1\u4e86\u89c6\u89c9-\u78c1\u529b\u5206\u5c42\u65b9\u6848\u4ee5\u5b9e\u73b0\u9c81\u68d2\u95ed\u73af\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVIMS\u5728\u6c34\u4e0b\u73af\u5883\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u72b6\u6001\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "VIMS\u901a\u8fc7\u521b\u65b0\u7684\u4f20\u611f\u5668\u878d\u5408\u548c\u5206\u5c42\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6c34\u4e0bSLAM\u7684\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2506.15132", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15132", "abs": "https://arxiv.org/abs/2506.15132", "authors": ["Yushi Wang", "Penghui Chen", "Xinyu Han", "Feng Wu", "Mingguo Zhao"], "title": "Booster Gym: An End-to-End Reinforcement Learning Framework for Humanoid Robot Locomotion", "comment": null, "summary": "Recent advancements in reinforcement learning (RL) have led to significant\nprogress in humanoid robot locomotion, simplifying the design and training of\nmotion policies in simulation. However, the numerous implementation details\nmake transferring these policies to real-world robots a challenging task. To\naddress this, we have developed a comprehensive code framework that covers the\nentire process from training to deployment, incorporating common RL training\nmethods, domain randomization, reward function design, and solutions for\nhandling parallel structures. This library is made available as a community\nresource, with detailed descriptions of its design and experimental results. We\nvalidate the framework on the Booster T1 robot, demonstrating that the trained\npolicies seamlessly transfer to the physical platform, enabling capabilities\nsuch as omnidirectional walking, disturbance resistance, and terrain\nadaptability. We hope this work provides a convenient tool for the robotics\ncommunity, accelerating the development of humanoid robots. The code can be\nfound in https://github.com/BoosterRobotics/booster_gym.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u9762\u7684\u4ee3\u7801\u6846\u67b6\uff0c\u7528\u4e8e\u7b80\u5316\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u7b56\u7565\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u5728\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5c06\u4eff\u771f\u8bad\u7ec3\u7684\u7b56\u7565\u8fc1\u79fb\u5230\u73b0\u5b9e\u673a\u5668\u4eba\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6db5\u76d6\u8bad\u7ec3\u5230\u90e8\u7f72\u5168\u8fc7\u7a0b\u7684\u4ee3\u7801\u6846\u67b6\uff0c\u5305\u62ec\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3001\u57df\u968f\u673a\u5316\u3001\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u53ca\u5e76\u884c\u7ed3\u6784\u5904\u7406\u3002", "result": "\u5728Booster T1\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u5168\u65b9\u4f4d\u884c\u8d70\u3001\u6297\u5e72\u6270\u548c\u5730\u5f62\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u673a\u5668\u4eba\u793e\u533a\u63d0\u4f9b\u4e86\u4fbf\u6377\u5de5\u5177\uff0c\u6709\u671b\u52a0\u901f\u4eba\u5f62\u673a\u5668\u4eba\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.15146", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15146", "abs": "https://arxiv.org/abs/2506.15146", "authors": ["Masaki Murooka", "Takahiro Hoshi", "Kensuke Fukumitsu", "Shimpei Masuda", "Marwan Hamze", "Tomoya Sasaki", "Mitsuharu Morisawa", "Eiichi Yoshida"], "title": "TACT: Humanoid Whole-body Contact Manipulation through Deep Imitation Learning with Tactile Modality", "comment": null, "summary": "Manipulation with whole-body contact by humanoid robots offers distinct\nadvantages, including enhanced stability and reduced load. On the other hand,\nwe need to address challenges such as the increased computational cost of\nmotion generation and the difficulty of measuring broad-area contact. We\ntherefore have developed a humanoid control system that allows a humanoid robot\nequipped with tactile sensors on its upper body to learn a policy for\nwhole-body manipulation through imitation learning based on human teleoperation\ndata. This policy, named tactile-modality extended ACT (TACT), has a feature to\ntake multiple sensor modalities as input, including joint position, vision, and\ntactile measurements. Furthermore, by integrating this policy with retargeting\nand locomotion control based on a biped model, we demonstrate that the\nlife-size humanoid robot RHP7 Kaleido is capable of achieving whole-body\ncontact manipulation while maintaining balance and walking. Through detailed\nexperimental verification, we show that inputting both vision and tactile\nmodalities into the policy contributes to improving the robustness of\nmanipulation involving broad and delicate contact.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTACT\u7684\u4eff\u4eba\u673a\u5668\u4eba\u63a7\u5236\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u4eba\u7c7b\u8fdc\u7a0b\u64cd\u4f5c\u6570\u636e\uff0c\u5b9e\u73b0\u5168\u8eab\u63a5\u89e6\u64cd\u4f5c\uff0c\u5e76\u7ed3\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u8f93\u5165\u63d0\u9ad8\u64cd\u4f5c\u9c81\u68d2\u6027\u3002", "motivation": "\u4eff\u4eba\u673a\u5668\u4eba\u901a\u8fc7\u5168\u8eab\u63a5\u89e6\u64cd\u4f5c\u53ef\u4ee5\u63d0\u9ad8\u7a33\u5b9a\u6027\u548c\u51cf\u5c11\u8d1f\u8f7d\uff0c\u4f46\u9762\u4e34\u8fd0\u52a8\u751f\u6210\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u5e7f\u57df\u63a5\u89e6\u6d4b\u91cf\u56f0\u96be\u7b49\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86TACT\u7b56\u7565\uff0c\u57fa\u4e8e\u4eba\u7c7b\u8fdc\u7a0b\u64cd\u4f5c\u6570\u636e\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u673a\u5668\u4eba\uff0c\u6574\u5408\u89c6\u89c9\u3001\u5173\u8282\u4f4d\u7f6e\u548c\u89e6\u89c9\u8f93\u5165\uff0c\u5e76\u7ed3\u5408\u53cc\u8db3\u6a21\u578b\u5b9e\u73b0\u5e73\u8861\u4e0e\u884c\u8d70\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cTACT\u7b56\u7565\u4f7f\u4eff\u4eba\u673a\u5668\u4ebaRHP7 Kaleido\u80fd\u591f\u5728\u4fdd\u6301\u5e73\u8861\u548c\u884c\u8d70\u7684\u540c\u65f6\u5b8c\u6210\u5168\u8eab\u63a5\u89e6\u64cd\u4f5c\uff0c\u89c6\u89c9\u548c\u89e6\u89c9\u8f93\u5165\u63d0\u9ad8\u4e86\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "TACT\u7b56\u7565\u6210\u529f\u89e3\u51b3\u4e86\u5168\u8eab\u63a5\u89e6\u64cd\u4f5c\u7684\u6311\u6218\uff0c\u4e3a\u4eff\u4eba\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.15150", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.15150", "abs": "https://arxiv.org/abs/2506.15150", "authors": ["Yuanlong Ji", "Xingbang Yang", "Ruoqi Zhao", "Qihan Ye", "Quan Zheng", "Yubo Fan"], "title": "Human Locomotion Implicit Modeling Based Real-Time Gait Phase Estimation", "comment": null, "summary": "Gait phase estimation based on inertial measurement unit (IMU) signals\nfacilitates precise adaptation of exoskeletons to individual gait variations.\nHowever, challenges remain in achieving high accuracy and robustness,\nparticularly during periods of terrain changes. To address this, we develop a\ngait phase estimation neural network based on implicit modeling of human\nlocomotion, which combines temporal convolution for feature extraction with\ntransformer layers for multi-channel information fusion. A channel-wise masked\nreconstruction pre-training strategy is proposed, which first treats gait phase\nstate vectors and IMU signals as joint observations of human locomotion, thus\nenhancing model generalization. Experimental results demonstrate that the\nproposed method outperforms existing baseline approaches, achieving a gait\nphase RMSE of $2.729 \\pm 1.071%$ and phase rate MAE of $0.037 \\pm 0.016%$ under\nstable terrain conditions with a look-back window of 2 seconds, and a phase\nRMSE of $3.215 \\pm 1.303%$ and rate MAE of $0.050 \\pm 0.023%$ under terrain\ntransitions. Hardware validation on a hip exoskeleton further confirms that the\nalgorithm can reliably identify gait cycles and key events, adapting to various\ncontinuous motion scenarios. This research paves the way for more intelligent\nand adaptive exoskeleton systems, enabling safer and more efficient human-robot\ninteraction across diverse real-world environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u5efa\u6a21\u7684\u6b65\u6001\u76f8\u4f4d\u4f30\u8ba1\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u65f6\u95f4\u5377\u79ef\u548cTransformer\u5c42\uff0c\u901a\u8fc7\u901a\u9053\u63a9\u7801\u91cd\u5efa\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u7a33\u5b9a\u548c\u5730\u5f62\u53d8\u5316\u6761\u4ef6\u4e0b\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6b65\u6001\u76f8\u4f4d\u4f30\u8ba1\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u5730\u5f62\u53d8\u5316\u65f6\u7684\u8868\u73b0\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408\u65f6\u95f4\u5377\u79ef\u548cTransformer\u5c42\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u91c7\u7528\u901a\u9053\u63a9\u7801\u91cd\u5efa\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5c06\u6b65\u6001\u76f8\u4f4d\u72b6\u6001\u5411\u91cf\u4e0eIMU\u4fe1\u53f7\u8054\u5408\u5efa\u6a21\u3002", "result": "\u5728\u7a33\u5b9a\u5730\u5f62\u4e0b\u6b65\u6001\u76f8\u4f4dRMSE\u4e3a2.729\u00b11.071%\uff0c\u76f8\u4f4d\u7387MAE\u4e3a0.037\u00b10.016%\uff1b\u5730\u5f62\u53d8\u5316\u65f6RMSE\u4e3a3.215\u00b11.303%\uff0cMAE\u4e3a0.050\u00b10.023%\u3002\u786c\u4ef6\u9a8c\u8bc1\u8868\u660e\u7b97\u6cd5\u80fd\u53ef\u9760\u8bc6\u522b\u6b65\u6001\u5468\u671f\u548c\u5173\u952e\u4e8b\u4ef6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u66f4\u667a\u80fd\u548c\u81ea\u9002\u5e94\u7684\u5916\u9aa8\u9abc\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u5347\u4e86\u4eba\u673a\u4ea4\u4e92\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2506.15157", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15157", "abs": "https://arxiv.org/abs/2506.15157", "authors": ["Hanbit Oh", "Andrea M. Salcedo-V\u00e1zquez", "Ixchel G. Ramirez-Alpizar", "Yukiyasu Domae"], "title": "Robust Instant Policy: Leveraging Student's t-Regression Model for Robust In-context Imitation Learning of Robot Manipulation", "comment": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS) 2025 accepted", "summary": "Imitation learning (IL) aims to enable robots to perform tasks autonomously\nby observing a few human demonstrations. Recently, a variant of IL, called\nIn-Context IL, utilized off-the-shelf large language models (LLMs) as instant\npolicies that understand the context from a few given demonstrations to perform\na new task, rather than explicitly updating network models with large-scale\ndemonstrations. However, its reliability in the robotics domain is undermined\nby hallucination issues such as LLM-based instant policy, which occasionally\ngenerates poor trajectories that deviate from the given demonstrations. To\nalleviate this problem, we propose a new robust in-context imitation learning\nalgorithm called the robust instant policy (RIP), which utilizes a Student's\nt-regression model to be robust against the hallucinated trajectories of\ninstant policies to allow reliable trajectory generation. Specifically, RIP\ngenerates several candidate robot trajectories to complete a given task from an\nLLM and aggregates them using the Student's t-distribution, which is beneficial\nfor ignoring outliers (i.e., hallucinations); thereby, a robust trajectory\nagainst hallucinations is generated. Our experiments, conducted in both\nsimulated and real-world environments, show that RIP significantly outperforms\nstate-of-the-art IL methods, with at least $26\\%$ improvement in task success\nrates, particularly in low-data scenarios for everyday tasks. Video results\navailable at https://sites.google.com/view/robustinstantpolicy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u4e0a\u4e0b\u6587\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\uff08RIP\uff09\uff0c\u901a\u8fc7\u4f7f\u7528Student's t\u56de\u5f52\u6a21\u578b\u6765\u51cf\u5c11LLM\u751f\u6210\u7684\u5e7b\u89c9\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u5373\u65f6\u7b56\u7565\u5728\u673a\u5668\u4eba\u9886\u57df\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u5bfc\u81f4\u8f68\u8ff9\u504f\u79bb\u6f14\u793a\uff0c\u5f71\u54cd\u53ef\u9760\u6027\u3002", "method": "RIP\u5229\u7528Student's t\u5206\u5e03\u805a\u5408LLM\u751f\u6210\u7684\u5019\u9009\u8f68\u8ff9\uff0c\u5ffd\u7565\u5f02\u5e38\u503c\uff08\u5e7b\u89c9\uff09\uff0c\u751f\u6210\u9c81\u68d2\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRIP\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u4efb\u52a1\u6210\u529f\u7387\u81f3\u5c11\u63d0\u9ad826%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RIP\u901a\u8fc7\u9c81\u68d2\u805a\u5408\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u4eff\u5b66\u4e60\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2506.15175", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15175", "abs": "https://arxiv.org/abs/2506.15175", "authors": ["Hanjun Kim", "Minwoo Jung", "Wooseong Yang", "Ayoung Kim"], "title": "SHeRLoc: Synchronized Heterogeneous Radar Place Recognition for Cross-Modal Localization", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Despite the growing adoption of radar in robotics, the majority of research\nhas been confined to homogeneous sensor types, overlooking the integration and\ncross-modality challenges inherent in heterogeneous radar technologies. This\nleads to significant difficulties in generalizing across diverse radar data\ntypes, with modality-aware approaches that could leverage the complementary\nstrengths of heterogeneous radar remaining unexplored. To bridge these gaps, we\npropose SHeRLoc, the first deep network tailored for heterogeneous radar, which\nutilizes RCS polar matching to align multimodal radar data. Our hierarchical\noptimal transport-based feature aggregation method generates rotationally\nrobust multi-scale descriptors. By employing FFT-similarity-based data mining\nand adaptive margin-based triplet loss, SHeRLoc enables FOV-aware metric\nlearning. SHeRLoc achieves an order of magnitude improvement in heterogeneous\nradar place recognition, increasing recall@1 from below 0.1 to 0.9 on a public\ndataset and outperforming state of-the-art methods. Also applicable to LiDAR,\nSHeRLoc paves the way for cross-modal place recognition and heterogeneous\nsensor SLAM. The source code will be available upon acceptance.", "AI": {"tldr": "SHeRLoc\u662f\u4e00\u79cd\u4e13\u4e3a\u5f02\u6784\u96f7\u8fbe\u8bbe\u8ba1\u7684\u6df1\u5ea6\u7f51\u7edc\uff0c\u901a\u8fc7RCS\u6781\u5750\u6807\u5339\u914d\u548c\u591a\u5c3a\u5ea6\u63cf\u8ff0\u7b26\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u6784\u96f7\u8fbe\u5730\u70b9\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5c40\u9650\u4e8e\u540c\u8d28\u96f7\u8fbe\u4f20\u611f\u5668\uff0c\u5ffd\u89c6\u4e86\u5f02\u6784\u96f7\u8fbe\u6280\u672f\u7684\u96c6\u6210\u548c\u8de8\u6a21\u6001\u6311\u6218\uff0c\u5bfc\u81f4\u96be\u4ee5\u6cdb\u5316\u5230\u591a\u6837\u96f7\u8fbe\u6570\u636e\u7c7b\u578b\u3002", "method": "\u63d0\u51faSHeRLoc\uff0c\u5229\u7528RCS\u6781\u5750\u6807\u5339\u914d\u5bf9\u9f50\u591a\u6a21\u6001\u96f7\u8fbe\u6570\u636e\uff0c\u91c7\u7528\u5206\u5c42\u6700\u4f18\u4f20\u8f93\u7279\u5f81\u805a\u5408\u548cFFT\u76f8\u4f3c\u6027\u6570\u636e\u6316\u6398\uff0c\u5b9e\u73b0FOV\u611f\u77e5\u5ea6\u91cf\u5b66\u4e60\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cSHeRLoc\u5c06\u5f02\u6784\u96f7\u8fbe\u5730\u70b9\u8bc6\u522b\u7684recall@1\u4ece\u4f4e\u4e8e0.1\u63d0\u5347\u81f30.9\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SHeRLoc\u4e0d\u4ec5\u9002\u7528\u4e8e\u96f7\u8fbe\uff0c\u8fd8\u4e3a\u8de8\u6a21\u6001\u5730\u70b9\u8bc6\u522b\u548c\u5f02\u6784\u4f20\u611f\u5668SLAM\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.15249", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15249", "abs": "https://arxiv.org/abs/2506.15249", "authors": ["Lucas Schulze", "Jan Peters", "Oleg Arenz"], "title": "Context-Aware Deep Lagrangian Networks for Model Predictive Control", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025", "summary": "Controlling a robot based on physics-informed dynamic models, such as deep\nLagrangian networks (DeLaN), can improve the generalizability and\ninterpretability of the resulting behavior. However, in complex environments,\nthe number of objects to potentially interact with is vast, and their physical\nproperties are often uncertain. This complexity makes it infeasible to employ a\nsingle global model. Therefore, we need to resort to online system\nidentification of context-aware models that capture only the currently relevant\naspects of the environment. While physical principles such as the conservation\nof energy may not hold across varying contexts, ensuring physical plausibility\nfor any individual context-aware model can still be highly desirable,\nparticularly when using it for receding horizon control methods such as Model\nPredictive Control (MPC). Hence, in this work, we extend DeLaN to make it\ncontext-aware, combine it with a recurrent network for online system\nidentification, and integrate it with a MPC for adaptive, physics-informed\ncontrol. We also combine DeLaN with a residual dynamics model to leverage the\nfact that a nominal model of the robot is typically available. We evaluate our\nmethod on a 7-DOF robot arm for trajectory tracking under varying loads. Our\nmethod reduces the end-effector tracking error by 39%, compared to a 21%\nimprovement achieved by a baseline that uses an extended Kalman filter.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6df1\u5ea6\u62c9\u683c\u6717\u65e5\u7f51\u7edc\uff08DeLaN\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u5728\u7ebf\u7cfb\u7edf\u8bc6\u522b\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\uff0c\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e2d\u673a\u5668\u4eba\u7684\u81ea\u9002\u5e94\u63a7\u5236\u3002", "motivation": "\u5728\u590d\u6742\u73af\u5883\u4e2d\uff0c\u5168\u5c40\u6a21\u578b\u96be\u4ee5\u5e94\u5bf9\u5927\u91cf\u4e0d\u786e\u5b9a\u7684\u7269\u4f53\u548c\u7269\u7406\u5c5e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5c40\u90e8\u6a21\u578b\u6765\u63d0\u5347\u63a7\u5236\u6548\u679c\u3002", "method": "\u6269\u5c55DeLaN\u4f7f\u5176\u5177\u5907\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\uff0c\u7ed3\u5408\u5faa\u73af\u7f51\u7edc\u8fdb\u884c\u5728\u7ebf\u7cfb\u7edf\u8bc6\u522b\uff0c\u5e76\u4e0eMPC\u96c6\u6210\uff1b\u540c\u65f6\u5f15\u5165\u6b8b\u5dee\u52a8\u529b\u5b66\u6a21\u578b\u4ee5\u5229\u7528\u5df2\u6709\u7684\u673a\u5668\u4eba\u540d\u4e49\u6a21\u578b\u3002", "result": "\u57287\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u7684\u8f68\u8ff9\u8ddf\u8e2a\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c06\u672b\u7aef\u6267\u884c\u5668\u8ddf\u8e2a\u8bef\u5dee\u964d\u4f4e\u4e8639%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u768421%\u6539\u8fdb\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u611f\u77e5\u7684DeLaN\u7ed3\u5408MPC\u548c\u6b8b\u5dee\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u63a7\u5236\u6027\u80fd\u3002"}}
{"id": "2506.15343", "categories": ["cs.RO", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.15343", "abs": "https://arxiv.org/abs/2506.15343", "authors": ["V\u00edctor Mayoral-Vilches"], "title": "Offensive Robot Cybersecurity", "comment": "Doctoral thesis", "summary": "Offensive Robot Cybersecurity introduces a groundbreaking approach by\nadvocating for offensive security methods empowered by means of automation. It\nemphasizes the necessity of understanding attackers' tactics and identifying\nvulnerabilities in advance to develop effective defenses, thereby improving\nrobots' security posture. This thesis leverages a decade of robotics\nexperience, employing Machine Learning and Game Theory to streamline the\nvulnerability identification and exploitation process. Intrinsically, the\nthesis uncovers a profound connection between robotic architecture and\ncybersecurity, highlighting that the design and creation aspect of robotics\ndeeply intertwines with its protection against attacks. This duality -- whereby\nthe architecture that shapes robot behavior and capabilities also necessitates\na defense mechanism through offensive and defensive cybersecurity strategies --\ncreates a unique equilibrium. Approaching cybersecurity with a dual perspective\nof defense and attack, rooted in an understanding of systems architecture, has\nbeen pivotal. Through comprehensive analysis, including ethical considerations,\nthe development of security tools, and executing cyber attacks on robot\nsoftware, hardware, and industry deployments, this thesis proposes a novel\narchitecture for cybersecurity cognitive engines. These engines, powered by\nadvanced game theory and machine learning, pave the way for autonomous\noffensive cybersecurity strategies for robots, marking a significant shift\ntowards self-defending robotic systems. This research not only underscores the\nimportance of offensive measures in enhancing robot cybersecurity but also sets\nthe stage for future advancements where robots are not just resilient to cyber\nthreats but are equipped to autonomously safeguard themselves.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u81ea\u52a8\u5316\u5b9e\u73b0\u8fdb\u653b\u6027\u5b89\u5168\u7684\u65b9\u6cd5\uff0c\u5f3a\u8c03\u901a\u8fc7\u7406\u89e3\u653b\u51fb\u8005\u6218\u672f\u548c\u63d0\u524d\u8bc6\u522b\u6f0f\u6d1e\u6765\u63d0\u5347\u673a\u5668\u4eba\u5b89\u5168\u6027\u3002\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u535a\u5f08\u8bba\uff0c\u8bba\u6587\u63ed\u793a\u4e86\u673a\u5668\u4eba\u67b6\u6784\u4e0e\u7f51\u7edc\u5b89\u5168\u7684\u6df1\u5c42\u8054\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7f51\u7edc\u5b89\u5168\u8ba4\u77e5\u5f15\u64ce\u67b6\u6784\u3002", "motivation": "\u673a\u5668\u4eba\u5b89\u5168\u9700\u8981\u4ece\u653b\u51fb\u8005\u89d2\u5ea6\u51fa\u53d1\uff0c\u63d0\u524d\u8bc6\u522b\u6f0f\u6d1e\u5e76\u5f00\u53d1\u9632\u5fa1\u63aa\u65bd\uff0c\u4ee5\u63d0\u5347\u6574\u4f53\u5b89\u5168\u6027\u3002", "method": "\u5229\u7528\u673a\u5668\u5b66\u4e60\u548c\u535a\u5f08\u8bba\u4f18\u5316\u6f0f\u6d1e\u8bc6\u522b\u4e0e\u5229\u7528\u8fc7\u7a0b\uff0c\u7ed3\u5408\u673a\u5668\u4eba\u67b6\u6784\u8bbe\u8ba1\uff0c\u5f00\u53d1\u8fdb\u653b\u6027\u548c\u9632\u5fa1\u6027\u7f51\u7edc\u5b89\u5168\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u535a\u5f08\u8bba\u548c\u673a\u5668\u5b66\u4e60\u7684\u7f51\u7edc\u5b89\u5168\u8ba4\u77e5\u5f15\u64ce\u67b6\u6784\uff0c\u652f\u6301\u81ea\u4e3b\u8fdb\u653b\u6027\u5b89\u5168\u7b56\u7565\uff0c\u63a8\u52a8\u81ea\u9632\u5fa1\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "conclusion": "\u7814\u7a76\u4e0d\u4ec5\u5f3a\u8c03\u4e86\u8fdb\u653b\u6027\u63aa\u65bd\u5bf9\u673a\u5668\u4eba\u5b89\u5168\u7684\u91cd\u8981\u6027\uff0c\u8fd8\u4e3a\u672a\u6765\u673a\u5668\u4eba\u81ea\u4e3b\u9632\u5fa1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.15376", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.15376", "abs": "https://arxiv.org/abs/2506.15376", "authors": ["Ahmed Ibrahim", "Francisco F. C. Rego", "\u00c9ric Busvelle"], "title": "Comparison of Innovative Strategies for the Coverage Problem: Path Planning, Search Optimization, and Applications in Underwater Robotics", "comment": null, "summary": "In many applications, including underwater robotics, the coverage problem\nrequires an autonomous vehicle to systematically explore a defined area while\nminimizing redundancy and avoiding obstacles. This paper investigates coverage\npath planning strategies to enhance the efficiency of underwater gliders,\nparticularly in maximizing the probability of detecting a radioactive source\nwhile ensuring safe navigation.\n  We evaluate three path-planning approaches: the Traveling Salesman Problem\n(TSP), Minimum Spanning Tree (MST), and Optimal Control Problem (OCP).\nSimulations were conducted in MATLAB, comparing processing time, uncovered\nareas, path length, and traversal time. Results indicate that OCP is preferable\nwhen traversal time is constrained, although it incurs significantly higher\ncomputational costs. Conversely, MST-based approaches provide faster but less\noptimal solutions. These findings offer insights into selecting appropriate\nalgorithms based on mission priorities, balancing efficiency and computational\nfeasibility.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6c34\u4e0b\u6ed1\u7fd4\u673a\u7684\u8986\u76d6\u8def\u5f84\u89c4\u5212\u7b56\u7565\uff0c\u6bd4\u8f83\u4e86TSP\u3001MST\u548cOCP\u4e09\u79cd\u65b9\u6cd5\uff0c\u53d1\u73b0OCP\u5728\u65f6\u95f4\u53d7\u9650\u65f6\u66f4\u4f18\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff1bMST\u5219\u66f4\u5feb\u4f46\u6548\u679c\u7a0d\u5dee\u3002", "motivation": "\u63d0\u5347\u6c34\u4e0b\u6ed1\u7fd4\u673a\u5728\u653e\u5c04\u6027\u6e90\u63a2\u6d4b\u4efb\u52a1\u4e2d\u7684\u8986\u76d6\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "method": "\u8bc4\u4f30\u4e86TSP\u3001MST\u548cOCP\u4e09\u79cd\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7MATLAB\u4eff\u771f\u6bd4\u8f83\u5904\u7406\u65f6\u95f4\u3001\u672a\u8986\u76d6\u533a\u57df\u3001\u8def\u5f84\u957f\u5ea6\u548c\u904d\u5386\u65f6\u95f4\u3002", "result": "OCP\u5728\u65f6\u95f4\u53d7\u9650\u65f6\u8868\u73b0\u6700\u4f73\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0cMST\u66f4\u5feb\u4f46\u6548\u679c\u8f83\u5dee\u3002", "conclusion": "\u6839\u636e\u4efb\u52a1\u4f18\u5148\u7ea7\u9009\u62e9\u7b97\u6cd5\uff0c\u5e73\u8861\u6548\u7387\u548c\u8ba1\u7b97\u53ef\u884c\u6027\u3002"}}
{"id": "2506.15380", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15380", "abs": "https://arxiv.org/abs/2506.15380", "authors": ["Taegeun Yang", "Jiwoo Hwang", "Jeil Jeong", "Minsung Yoon", "Sung-Eui Yoon"], "title": "Efficient Navigation Among Movable Obstacles using a Mobile Manipulator via Hierarchical Policy Learning", "comment": "8 pages, 6 figures, Accepted to IROS 2025. Supplementary Video:\n  https://youtu.be/sZ8_z7sYVP0", "summary": "We propose a hierarchical reinforcement learning (HRL) framework for\nefficient Navigation Among Movable Obstacles (NAMO) using a mobile manipulator.\nOur approach combines interaction-based obstacle property estimation with\nstructured pushing strategies, facilitating the dynamic manipulation of\nunforeseen obstacles while adhering to a pre-planned global path. The\nhigh-level policy generates pushing commands that consider environmental\nconstraints and path-tracking objectives, while the low-level policy precisely\nand stably executes these commands through coordinated whole-body movements.\nComprehensive simulation-based experiments demonstrate improvements in\nperforming NAMO tasks, including higher success rates, shortened traversed path\nlength, and reduced goal-reaching times, compared to baselines. Additionally,\nablation studies assess the efficacy of each component, while a qualitative\nanalysis further validates the accuracy and reliability of the real-time\nobstacle property estimation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u79fb\u52a8\u673a\u68b0\u81c2\u5728\u52a8\u6001\u969c\u788d\u7269\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u5bfc\u822a\uff0c\u7ed3\u5408\u969c\u788d\u7269\u5c5e\u6027\u4f30\u8ba1\u4e0e\u7ed3\u6784\u5316\u63a8\u52a8\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3\u5728\u52a8\u6001\u969c\u788d\u7269\u73af\u5883\u4e2d\u5bfc\u822a\u65f6\uff0c\u5982\u4f55\u9ad8\u6548\u4f30\u8ba1\u969c\u788d\u7269\u5c5e\u6027\u5e76\u52a8\u6001\u8c03\u6574\u8def\u5f84\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u9ad8\u5c42\u7b56\u7565\u751f\u6210\u63a8\u52a8\u547d\u4ee4\uff0c\u4f4e\u5c42\u7b56\u7565\u6267\u884c\u534f\u8c03\u5168\u8eab\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u6210\u529f\u7387\u548c\u8def\u5f84\u6548\u7387\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5b9e\u65f6\u969c\u788d\u7269\u5c5e\u6027\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u52a8\u6001\u969c\u788d\u7269\u5bfc\u822a\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2506.15402", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15402", "abs": "https://arxiv.org/abs/2506.15402", "authors": ["Miaoxin Pan", "Jinnan Li", "Yaowen Zhang", "Yi Yang", "Yufeng Yue"], "title": "MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System", "comment": null, "summary": "Object-level SLAM offers structured and semantically meaningful environment\nrepresentations, making it more interpretable and suitable for high-level\nrobotic tasks. However, most existing approaches rely on RGB-D sensors or\nmonocular views, which suffer from narrow fields of view, occlusion\nsensitivity, and limited depth perception-especially in large-scale or outdoor\nenvironments. These limitations often restrict the system to observing only\npartial views of objects from limited perspectives, leading to inaccurate\nobject modeling and unreliable data association. In this work, we propose\nMCOO-SLAM, a novel Multi-Camera Omnidirectional Object SLAM system that fully\nleverages surround-view camera configurations to achieve robust, consistent,\nand semantically enriched mapping in complex outdoor scenarios. Our approach\nintegrates point features and object-level landmarks enhanced with\nopen-vocabulary semantics. A semantic-geometric-temporal fusion strategy is\nintroduced for robust object association across multiple views, leading to\nimproved consistency and accurate object modeling, and an omnidirectional loop\nclosure module is designed to enable viewpoint-invariant place recognition\nusing scene-level descriptors. Furthermore, the constructed map is abstracted\ninto a hierarchical 3D scene graph to support downstream reasoning tasks.\nExtensive experiments in real-world demonstrate that MCOO-SLAM achieves\naccurate localization and scalable object-level mapping with improved\nrobustness to occlusion, pose variation, and environmental complexity.", "AI": {"tldr": "MCOO-SLAM\u662f\u4e00\u79cd\u591a\u76f8\u673a\u5168\u5411\u7269\u4f53SLAM\u7cfb\u7edf\uff0c\u5229\u7528\u73af\u7ed5\u89c6\u89d2\u76f8\u673a\u914d\u7f6e\u5b9e\u73b0\u590d\u6742\u6237\u5916\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u3001\u4e00\u81f4\u4e14\u8bed\u4e49\u4e30\u5bcc\u7684\u5efa\u56fe\u3002", "motivation": "\u73b0\u6709SLAM\u65b9\u6cd5\u4f9d\u8d56RGB-D\u6216\u5355\u76ee\u76f8\u673a\uff0c\u89c6\u91ce\u7a84\u3001\u6613\u53d7\u906e\u6321\u4e14\u6df1\u5ea6\u611f\u77e5\u6709\u9650\uff0c\u5bfc\u81f4\u7269\u4f53\u5efa\u6a21\u4e0d\u51c6\u786e\u548c\u6570\u636e\u5173\u8054\u4e0d\u53ef\u9760\u3002", "method": "\u6574\u5408\u70b9\u7279\u5f81\u548c\u7269\u4f53\u7ea7\u5730\u6807\uff0c\u5f15\u5165\u8bed\u4e49-\u51e0\u4f55-\u65f6\u95f4\u878d\u5408\u7b56\u7565\uff0c\u8bbe\u8ba1\u5168\u5411\u95ed\u73af\u6a21\u5757\uff0c\u5e76\u6784\u5efa\u5c42\u6b21\u53163D\u573a\u666f\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMCOO-SLAM\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u51c6\u786e\u5b9a\u4f4d\u548c\u53ef\u6269\u5c55\u7684\u7269\u4f53\u7ea7\u5efa\u56fe\uff0c\u5bf9\u906e\u6321\u3001\u59ff\u6001\u53d8\u5316\u548c\u73af\u5883\u590d\u6742\u6027\u5177\u6709\u66f4\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "MCOO-SLAM\u901a\u8fc7\u591a\u76f8\u673a\u5168\u5411\u89c6\u89d2\u548c\u8bed\u4e49\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u7684SLAM\u6027\u80fd\u3002"}}
{"id": "2506.15450", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15450", "abs": "https://arxiv.org/abs/2506.15450", "authors": ["Kun Liu", "Junhao Xiao", "Hao Lin", "Yue Cao", "Hui Peng", "Kaihong Huang", "Huimin Lu"], "title": "SurfAAV: Design and Implementation of a Novel Multimodal Surfing Aquatic-Aerial Vehicle", "comment": null, "summary": "Despite significant advancements in the research of aquatic-aerial robots,\nexisting configurations struggle to efficiently perform underwater, surface,\nand aerial movement simultaneously. In this paper, we propose a novel\nmultimodal surfing aquatic-aerial vehicle, SurfAAV, which efficiently\nintegrates underwater navigation, surface gliding, and aerial flying\ncapabilities. Thanks to the design of the novel differential thrust vectoring\nhydrofoil, SurfAAV can achieve efficient surface gliding and underwater\nnavigation without the need for a buoyancy adjustment system. This design\nprovides flexible operational capabilities for both surface and underwater\ntasks, enabling the robot to quickly carry out underwater monitoring\nactivities. Additionally, when it is necessary to reach another water body,\nSurfAAV can switch to aerial mode through a gliding takeoff, flying to the\ntarget water area to perform corresponding tasks. The main contribution of this\nletter lies in proposing a new solution for underwater, surface, and aerial\nmovement, designing a novel hybrid prototype concept, developing the required\ncontrol laws, and validating the robot's ability to successfully perform\nsurface gliding and gliding takeoff. SurfAAV achieves a maximum surface gliding\nspeed of 7.96 m/s and a maximum underwater speed of 3.1 m/s. The prototype's\nsurface gliding maneuverability and underwater cruising maneuverability both\nexceed those of existing aquatic-aerial vehicles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u6c34\u4e0a-\u7a7a\u4e2d\u673a\u5668\u4ebaSurfAAV\uff0c\u80fd\u9ad8\u6548\u6574\u5408\u6c34\u4e0b\u3001\u6c34\u9762\u548c\u7a7a\u4e2d\u8fd0\u52a8\u80fd\u529b\uff0c\u65e0\u9700\u6d6e\u529b\u8c03\u8282\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u6c34\u4e0a-\u7a7a\u4e2d\u673a\u5668\u4eba\u96be\u4ee5\u540c\u65f6\u9ad8\u6548\u5b8c\u6210\u6c34\u4e0b\u3001\u6c34\u9762\u548c\u7a7a\u4e2d\u8fd0\u52a8\u3002", "method": "\u8bbe\u8ba1\u4e86\u65b0\u578b\u5dee\u52a8\u63a8\u529b\u77e2\u91cf\u6c34\u7ffc\uff0c\u5b9e\u73b0\u9ad8\u6548\u6c34\u9762\u6ed1\u884c\u548c\u6c34\u4e0b\u5bfc\u822a\uff0c\u5e76\u5f00\u53d1\u4e86\u63a7\u5236\u5f8b\u3002", "result": "SurfAAV\u6c34\u9762\u6ed1\u884c\u901f\u5ea6\u8fbe7.96 m/s\uff0c\u6c34\u4e0b\u901f\u5ea6\u8fbe3.1 m/s\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u673a\u5668\u4eba\u3002", "conclusion": "SurfAAV\u4e3a\u6c34\u4e0b\u3001\u6c34\u9762\u548c\u7a7a\u4e2d\u8fd0\u52a8\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2506.15518", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15518", "abs": "https://arxiv.org/abs/2506.15518", "authors": ["Giulio Delama", "Igor Borowski", "Roland Jung", "Stephan Weiss"], "title": "Real-Time Initialization of Unknown Anchors for UWB-aided Navigation", "comment": null, "summary": "This paper presents a framework for the real-time initialization of unknown\nUltra-Wideband (UWB) anchors in UWB-aided navigation systems. The method is\ndesigned for localization solutions where UWB modules act as supplementary\nsensors. Our approach enables the automatic detection and calibration of\npreviously unknown anchors during operation, removing the need for manual\nsetup. By combining an online Positional Dilution of Precision (PDOP)\nestimation, a lightweight outlier detection method, and an adaptive robust\nkernel for non-linear optimization, our approach significantly improves\nrobustness and suitability for real-world applications compared to\nstate-of-the-art. In particular, we show that our metric which triggers an\ninitialization decision is more conservative than current ones commonly based\non initial linear or non-linear initialization guesses. This allows for better\ninitialization geometry and subsequently lower initialization errors. We\ndemonstrate the proposed approach on two different mobile robots: an autonomous\nforklift and a quadcopter equipped with a UWB-aided Visual-Inertial Odometry\n(VIO) framework. The results highlight the effectiveness of the proposed method\nwith robust initialization and low positioning error. We open-source our code\nin a C++ library including a ROS wrapper.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u521d\u59cb\u5316\u672a\u77e5\u8d85\u5bbd\u5e26\uff08UWB\uff09\u951a\u70b9\u7684\u6846\u67b6\uff0c\u7528\u4e8eUWB\u8f85\u52a9\u5bfc\u822a\u7cfb\u7edf\uff0c\u65e0\u9700\u624b\u52a8\u8bbe\u7f6e\u3002", "motivation": "\u89e3\u51b3UWB\u6a21\u5757\u4f5c\u4e3a\u8f85\u52a9\u4f20\u611f\u5668\u65f6\uff0c\u672a\u77e5\u951a\u70b9\u7684\u81ea\u52a8\u68c0\u6d4b\u548c\u6821\u51c6\u95ee\u9898\uff0c\u63d0\u5347\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "method": "\u7ed3\u5408\u5728\u7ebfPDOP\u4f30\u8ba1\u3001\u8f7b\u91cf\u7ea7\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u548c\u81ea\u9002\u5e94\u9c81\u68d2\u6838\u8fdb\u884c\u975e\u7ebf\u6027\u4f18\u5316\u3002", "result": "\u5728\u81ea\u4e3b\u53c9\u8f66\u548c\u56db\u65cb\u7ffc\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u521d\u59cb\u5316\u548c\u4f4e\u5b9a\u4f4d\u8bef\u5dee\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.15539", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15539", "abs": "https://arxiv.org/abs/2506.15539", "authors": ["Haoran Chen", "Weiliang Deng", "Biyu Ye", "Yifan Xiong", "Ximin Lyu"], "title": "Aerial Grasping via Maximizing Delta-Arm Workspace Utilization", "comment": "8 pages, 7 figures", "summary": "The workspace limits the operational capabilities and range of motion for the\nsystems with robotic arms. Maximizing workspace utilization has the potential\nto provide more optimal solutions for aerial manipulation tasks, increasing the\nsystem's flexibility and operational efficiency. In this paper, we introduce a\nnovel planning framework for aerial grasping that maximizes workspace\nutilization. We formulate an optimization problem to optimize the aerial\nmanipulator's trajectory, incorporating task constraints to achieve efficient\nmanipulation. To address the challenge of incorporating the delta arm's\nnon-convex workspace into optimization constraints, we leverage a Multilayer\nPerceptron (MLP) to map position points to feasibility\nprobabilities.Furthermore, we employ Reversible Residual Networks (RevNet) to\napproximate the complex forward kinematics of the delta arm, utilizing\nefficient model gradients to eliminate workspace constraints. We validate our\nmethods in simulations and real-world experiments to demonstrate their\neffectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u7a7a\u4e2d\u6293\u53d6\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u8f68\u8ff9\u548c\u5229\u7528MLP\u4e0eRevNet\u6280\u672f\uff0c\u6700\u5927\u5316\u673a\u68b0\u81c2\u5de5\u4f5c\u7a7a\u95f4\u5229\u7528\u7387\u3002", "motivation": "\u673a\u68b0\u81c2\u5de5\u4f5c\u7a7a\u95f4\u9650\u5236\u4e86\u64cd\u4f5c\u80fd\u529b\u548c\u8fd0\u52a8\u8303\u56f4\uff0c\u6700\u5927\u5316\u5229\u7528\u53ef\u63d0\u5347\u7a7a\u4e2d\u6293\u53d6\u4efb\u52a1\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002", "method": "\u6784\u5efa\u4f18\u5316\u95ee\u9898\u4f18\u5316\u8f68\u8ff9\uff0c\u4f7f\u7528MLP\u6620\u5c04\u4f4d\u7f6e\u53ef\u884c\u6027\uff0cRevNet\u8fd1\u4f3c\u6b63\u5411\u8fd0\u52a8\u5b66\u4ee5\u6d88\u9664\u7ea6\u675f\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u4e2d\u6293\u53d6\u7684\u5de5\u4f5c\u7a7a\u95f4\u5229\u7528\u7387\u548c\u64cd\u4f5c\u6548\u7387\u3002"}}
{"id": "2506.15607", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15607", "abs": "https://arxiv.org/abs/2506.15607", "authors": ["Shailesh", "Alok Raj", "Nayan Kumar", "Priya Shukla", "Andrew Melnik", "Micheal Beetz", "Gora Chand Nandi"], "title": "GRIM: Task-Oriented Grasping with Conditioning on Generative Examples", "comment": null, "summary": "Task-Oriented Grasping (TOG) presents a significant challenge, requiring a\nnuanced understanding of task semantics, object affordances, and the functional\nconstraints dictating how an object should be grasped for a specific task. To\naddress these challenges, we introduce GRIM (Grasp Re-alignment via Iterative\nMatching), a novel training-free framework for task-oriented grasping.\nInitially, a coarse alignment strategy is developed using a combination of\ngeometric cues and principal component analysis (PCA)-reduced DINO features for\nsimilarity scoring. Subsequently, the full grasp pose associated with the\nretrieved memory instance is transferred to the aligned scene object and\nfurther refined against a set of task-agnostic, geometrically stable grasps\ngenerated for the scene object, prioritizing task compatibility. In contrast to\nexisting learning-based methods, GRIM demonstrates strong generalization\ncapabilities, achieving robust performance with only a small number of\nconditioning examples.", "AI": {"tldr": "GRIM\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u4efb\u52a1\u5bfc\u5411\u6293\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u7ebf\u7d22\u548cPCA\u964d\u7ef4\u7684DINO\u7279\u5f81\u8fdb\u884c\u7c97\u5bf9\u9f50\uff0c\u518d\u7ed3\u5408\u4efb\u52a1\u65e0\u5173\u7684\u51e0\u4f55\u7a33\u5b9a\u6293\u53d6\u8fdb\u884c\u7ec6\u5316\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4efb\u52a1\u5bfc\u5411\u6293\u53d6\uff08TOG\uff09\u9700\u8981\u7406\u89e3\u4efb\u52a1\u8bed\u4e49\u3001\u5bf9\u8c61\u529f\u80fd\u53ca\u6293\u53d6\u7ea6\u675f\uff0c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0cGRIM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "GRIM\u91c7\u7528\u7c97\u5bf9\u9f50\u7b56\u7565\uff08\u51e0\u4f55\u7ebf\u7d22+PCA\u964d\u7ef4\u7684DINO\u7279\u5f81\uff09\uff0c\u518d\u901a\u8fc7\u4efb\u52a1\u65e0\u5173\u7684\u51e0\u4f55\u7a33\u5b9a\u6293\u53d6\u7ec6\u5316\u6293\u53d6\u59ff\u52bf\u3002", "result": "GRIM\u4ec5\u9700\u5c11\u91cf\u793a\u4f8b\u5373\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "GRIM\u4e3a\u4efb\u52a1\u5bfc\u5411\u6293\u53d6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.15666", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15666", "abs": "https://arxiv.org/abs/2506.15666", "authors": ["Haoyu Xiong", "Xiaomeng Xu", "Jimmy Wu", "Yifan Hou", "Jeannette Bohg", "Shuran Song"], "title": "Vision in Action: Learning Active Perception from Human Demonstrations", "comment": null, "summary": "We present Vision in Action (ViA), an active perception system for bimanual\nrobot manipulation. ViA learns task-relevant active perceptual strategies\n(e.g., searching, tracking, and focusing) directly from human demonstrations.\nOn the hardware side, ViA employs a simple yet effective 6-DoF robotic neck to\nenable flexible, human-like head movements. To capture human active perception\nstrategies, we design a VR-based teleoperation interface that creates a shared\nobservation space between the robot and the human operator. To mitigate VR\nmotion sickness caused by latency in the robot's physical movements, the\ninterface uses an intermediate 3D scene representation, enabling real-time view\nrendering on the operator side while asynchronously updating the scene with the\nrobot's latest observations. Together, these design elements enable the\nlearning of robust visuomotor policies for three complex, multi-stage bimanual\nmanipulation tasks involving visual occlusions, significantly outperforming\nbaseline systems.", "AI": {"tldr": "ViA\u662f\u4e00\u4e2a\u7528\u4e8e\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6d3b\u52a8\u611f\u77e5\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b66\u4e60\u4eba\u7c7b\u6f14\u793a\u7684\u4efb\u52a1\u76f8\u5173\u611f\u77e5\u7b56\u7565\uff0c\u7ed3\u54086\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u9888\u90e8\u548cVR\u9065\u64cd\u4f5c\u754c\u9762\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u7cfb\u7edf\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5b66\u4e60\u4eba\u7c7b\u6d3b\u52a8\u611f\u77e5\u7b56\u7565\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u4ee5\u63d0\u5347\u590d\u6742\u53cc\u624b\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u75286\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u9888\u90e8\u548cVR\u9065\u64cd\u4f5c\u754c\u9762\uff0c\u901a\u8fc7\u4e2d\u95f43D\u573a\u666f\u8868\u793a\u51cf\u5c11VR\u8fd0\u52a8\u75c5\uff0c\u5b66\u4e60\u4eba\u7c7b\u6f14\u793a\u7684\u611f\u77e5\u7b56\u7565\u3002", "result": "\u5728\u6d89\u53ca\u89c6\u89c9\u906e\u6321\u7684\u590d\u6742\u591a\u9636\u6bb5\u53cc\u624b\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cViA\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u7cfb\u7edf\u3002", "conclusion": "ViA\u901a\u8fc7\u7ed3\u5408\u786c\u4ef6\u548c\u8f6f\u4ef6\u8bbe\u8ba1\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6d3b\u52a8\u611f\u77e5\u7b56\u7565\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.15680", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15680", "abs": "https://arxiv.org/abs/2506.15680", "authors": ["Kaifeng Zhang", "Baoyu Li", "Kris Hauser", "Yunzhu Li"], "title": "Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos", "comment": "Project page: https://kywind.github.io/pgnd", "summary": "Modeling the dynamics of deformable objects is challenging due to their\ndiverse physical properties and the difficulty of estimating states from\nlimited visual information. We address these challenges with a neural dynamics\nframework that combines object particles and spatial grids in a hybrid\nrepresentation. Our particle-grid model captures global shape and motion\ninformation while predicting dense particle movements, enabling the modeling of\nobjects with varied shapes and materials. Particles represent object shapes,\nwhile the spatial grid discretizes the 3D space to ensure spatial continuity\nand enhance learning efficiency. Coupled with Gaussian Splattings for visual\nrendering, our framework achieves a fully learning-based digital twin of\ndeformable objects and generates 3D action-conditioned videos. Through\nexperiments, we demonstrate that our model learns the dynamics of diverse\nobjects -- such as ropes, cloths, stuffed animals, and paper bags -- from\nsparse-view RGB-D recordings of robot-object interactions, while also\ngeneralizing at the category level to unseen instances. Our approach\noutperforms state-of-the-art learning-based and physics-based simulators,\nparticularly in scenarios with limited camera views. Furthermore, we showcase\nthe utility of our learned models in model-based planning, enabling\ngoal-conditioned object manipulation across a range of tasks. The project page\nis available at https://kywind.github.io/pgnd .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7c92\u5b50\u4e0e\u7a7a\u95f4\u7f51\u683c\u7684\u795e\u7ecf\u52a8\u529b\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u52a8\u6001\u5efa\u6a21\u56e0\u7269\u7406\u7279\u6027\u591a\u6837\u4e14\u89c6\u89c9\u4fe1\u606f\u6709\u9650\u800c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u7c92\u5b50-\u7f51\u683c\u6df7\u5408\u8868\u793a\uff0c\u7ed3\u5408\u9ad8\u65af\u6e32\u67d3\u6280\u672f\uff0c\u5b9e\u73b0\u5168\u5b66\u4e60\u7684\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u3002", "result": "\u6a21\u578b\u80fd\u591f\u4ece\u7a00\u758f\u89c6\u89d2\u7684RGB-D\u6570\u636e\u4e2d\u5b66\u4e60\u591a\u79cd\u7269\u4f53\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u5e76\u5728\u7c7b\u522b\u7ea7\u522b\u6cdb\u5316\u5230\u672a\u89c1\u5b9e\u4f8b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u89c6\u89d2\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u6a21\u578b\u89c4\u5212\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
