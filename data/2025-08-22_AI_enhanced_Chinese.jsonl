{"id": "2508.14994", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.14994", "abs": "https://arxiv.org/abs/2508.14994", "authors": ["Murilo Vinicius da Silva", "Matheus Hipolito Carvalho", "Juliano Negri", "Thiago Segreto", "Gustavo J. G. Lahr", "Ricardo V. Godoy", "Marcelo Becker"], "title": "A Vision-Based Shared-Control Teleoperation Scheme for Controlling the Robotic Arm of a Four-Legged Robot", "comment": null, "summary": "In hazardous and remote environments, robotic systems perform critical tasks\ndemanding improved safety and efficiency. Among these, quadruped robots with\nmanipulator arms offer mobility and versatility for complex operations.\nHowever, teleoperating quadruped robots is challenging due to the lack of\nintegrated obstacle detection and intuitive control methods for the robotic\narm, increasing collision risks in confined or dynamically changing workspaces.\nTeleoperation via joysticks or pads can be non-intuitive and demands a high\nlevel of expertise due to its complexity, culminating in a high cognitive load\non the operator. To address this challenge, a teleoperation approach that\ndirectly maps human arm movements to the robotic manipulator offers a simpler\nand more accessible solution. This work proposes an intuitive remote control by\nleveraging a vision-based pose estimation pipeline that utilizes an external\ncamera with a machine learning-based model to detect the operator's wrist\nposition. The system maps these wrist movements into robotic arm commands to\ncontrol the robot's arm in real-time. A trajectory planner ensures safe\nteleoperation by detecting and preventing collisions with both obstacles and\nthe robotic arm itself. The system was validated on the real robot,\ndemonstrating robust performance in real-time control. This teleoperation\napproach provides a cost-effective solution for industrial applications where\nsafety, precision, and ease of use are paramount, ensuring reliable and\nintuitive robotic control in high-risk environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u59ff\u6001\u4f30\u8ba1\u7684\u76f4\u89c2\u56db\u8db3\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u65b9\u6cd5\uff0c\u901a\u8fc7\u5916\u90e8\u6444\u50cf\u5934\u68c0\u6d4b\u64cd\u4f5c\u8005\u624b\u8155\u4f4d\u7f6e\uff0c\u5b9e\u65f6\u6620\u5c04\u5230\u673a\u68b0\u81c2\u63a7\u5236\uff0c\u7ed3\u5408\u8f68\u8ff9\u89c4\u5212\u786e\u4fdd\u5b89\u5168\u64cd\u4f5c", "motivation": "\u5728\u5371\u9669\u548c\u8fdc\u7a0b\u73af\u5883\u4e2d\uff0c\u56db\u8db3\u673a\u5668\u4eba\u673a\u68b0\u81c2\u9700\u8981\u66f4\u5b89\u5168\u548c\u9ad8\u6548\u7684\u9065\u64cd\u4f5c\u3002\u4f20\u7edf\u6447\u6746\u63a7\u5236\u4e0d\u76f4\u89c2\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u8ba4\u77e5\u8d1f\u8377\u9ad8\uff0c\u7f3a\u4e4f\u96c6\u6210\u7684\u969c\u788d\u7269\u68c0\u6d4b\u529f\u80fd\uff0c\u5728\u53d7\u9650\u6216\u52a8\u6001\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u78b0\u649e\u98ce\u9669\u5927", "method": "\u5229\u7528\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5916\u90e8\u6444\u50cf\u5934\u89c6\u89c9\u59ff\u6001\u4f30\u8ba1\u7ba1\u9053\u68c0\u6d4b\u64cd\u4f5c\u8005\u624b\u8155\u4f4d\u7f6e\uff0c\u5c06\u8fd9\u4e9b\u624b\u8155\u8fd0\u52a8\u5b9e\u65f6\u6620\u5c04\u4e3a\u673a\u68b0\u81c2\u547d\u4ee4\uff0c\u7ed3\u5408\u8f68\u8ff9\u89c4\u5212\u5668\u68c0\u6d4b\u548c\u9632\u6b62\u4e0e\u969c\u788d\u7269\u53ca\u673a\u68b0\u81c2\u81ea\u8eab\u7684\u78b0\u649e", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u5b9e\u65f6\u63a7\u5236\u7684\u7a33\u5065\u6027\u80fd\uff0c\u80fd\u591f\u63d0\u4f9b\u53ef\u9760\u548c\u76f4\u89c2\u7684\u673a\u5668\u4eba\u63a7\u5236", "conclusion": "\u8fd9\u79cd\u9065\u64cd\u4f5c\u65b9\u6cd5\u4e3a\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5b89\u5168\u6027\u3001\u7cbe\u786e\u6027\u548c\u6613\u7528\u6027\u81f3\u5173\u91cd\u8981\u7684\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u786e\u4fdd\u53ef\u9760\u7684\u76f4\u89c2\u673a\u5668\u4eba\u63a7\u5236"}}
{"id": "2508.15002", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.15002", "abs": "https://arxiv.org/abs/2508.15002", "authors": ["Ren\u00e9 Zurbr\u00fcgg", "Andrei Cramariuc", "Marco Hutter"], "title": "GraspQP: Differentiable Optimization of Force Closure for Diverse and Robust Dexterous Grasping", "comment": null, "summary": "Dexterous robotic hands enable versatile interactions due to the flexibility\nand adaptability of multi-fingered designs, allowing for a wide range of\ntask-specific grasp configurations in diverse environments. However, to fully\nexploit the capabilities of dexterous hands, access to diverse and high-quality\ngrasp data is essential -- whether for developing grasp prediction models from\npoint clouds, training manipulation policies, or supporting high-level task\nplanning with broader action options. Existing approaches for dataset\ngeneration typically rely on sampling-based algorithms or simplified\nforce-closure analysis, which tend to converge to power grasps and often\nexhibit limited diversity. In this work, we propose a method to synthesize\nlarge-scale, diverse, and physically feasible grasps that extend beyond simple\npower grasps to include refined manipulations, such as pinches and tri-finger\nprecision grasps. We introduce a rigorous, differentiable energy formulation of\nforce closure, implicitly defined through a Quadratic Program (QP).\nAdditionally, we present an adjusted optimization method (MALA*) that improves\nperformance by dynamically rejecting gradient steps based on the distribution\nof energy values across all samples. We extensively evaluate our approach and\ndemonstrate significant improvements in both grasp diversity and the stability\nof final grasp predictions. Finally, we provide a new, large-scale grasp\ndataset for 5,700 objects from DexGraspNet, comprising five different grippers\nand three distinct grasp types.\n  Dataset and Code:https://graspqp.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u5fae\u5206\u80fd\u91cf\u516c\u5f0f\u548c\u4f18\u5316\u65b9\u6cd5\u7684\u5927\u89c4\u6a21\u7075\u5de7\u6293\u53d6\u6570\u636e\u96c6\u751f\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6293\u53d6\u591a\u6837\u6027\u548c\u7a33\u5b9a\u6027", "motivation": "\u73b0\u6709\u6293\u53d6\u6570\u636e\u96c6\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u91c7\u6837\u7b97\u6cd5\u6216\u7b80\u5316\u529b\u95ed\u5408\u5206\u6790\uff0c\u5f80\u5f80\u6536\u655b\u5230\u5f3a\u529b\u6293\u53d6\u4e14\u591a\u6837\u6027\u6709\u9650\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u7075\u5de7\u624b\u7684\u7075\u6d3b\u6027", "method": "\u5f15\u5165\u57fa\u4e8e\u4e8c\u6b21\u89c4\u5212(QP)\u7684\u4e25\u683c\u53ef\u5fae\u5206\u529b\u95ed\u5408\u80fd\u91cf\u516c\u5f0f\uff0c\u5e76\u63d0\u51fa\u8c03\u6574\u4f18\u5316\u65b9\u6cd5(MALA*)\u52a8\u6001\u62d2\u7edd\u57fa\u4e8e\u80fd\u91cf\u503c\u5206\u5e03\u7684\u68af\u5ea6\u6b65\u9aa4", "result": "\u65b9\u6cd5\u5728\u6293\u53d6\u591a\u6837\u6027\u548c\u6700\u7ec8\u6293\u53d6\u9884\u6d4b\u7a33\u5b9a\u6027\u65b9\u9762\u663e\u8457\u6539\u8fdb\uff0c\u63d0\u4f9b\u4e86\u5305\u542b5,700\u4e2a\u7269\u4f53\u30015\u79cd\u5939\u722a\u548c3\u79cd\u6293\u53d6\u7c7b\u578b\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6DexGraspNet", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5408\u6210\u8d85\u8d8a\u7b80\u5355\u5f3a\u529b\u6293\u53d6\u7684\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u4e14\u7269\u7406\u53ef\u884c\u7684\u6293\u53d6\u914d\u7f6e\uff0c\u5305\u62ec\u7cbe\u7ec6\u64cd\u4f5c\u5982\u634f\u53d6\u548c\u4e09\u6307\u7cbe\u786e\u6293\u53d6\uff0c\u4e3a\u7075\u5de7\u624b\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90"}}
{"id": "2508.15021", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.15021", "abs": "https://arxiv.org/abs/2508.15021", "authors": ["Mark Van der Merwe", "Devesh Jha"], "title": "In-Context Iterative Policy Improvement for Dynamic Manipulation", "comment": "14 pages. Accepted at CoRL 2025", "summary": "Attention-based architectures trained on internet-scale language data have\ndemonstrated state of the art reasoning ability for various language-based\ntasks, such as logic problems and textual reasoning. Additionally, these Large\nLanguage Models (LLMs) have exhibited the ability to perform few-shot\nprediction via in-context learning, in which input-output examples provided in\nthe prompt are generalized to new inputs. This ability furthermore extends\nbeyond standard language tasks, enabling few-shot learning for general\npatterns. In this work, we consider the application of in-context learning with\npre-trained language models for dynamic manipulation. Dynamic manipulation\nintroduces several crucial challenges, including increased dimensionality,\ncomplex dynamics, and partial observability. To address this, we take an\niterative approach, and formulate our in-context learning problem to predict\nadjustments to a parametric policy based on previous interactions. We show\nacross several tasks in simulation and on a physical robot that utilizing\nin-context learning outperforms alternative methods in the low data regime.\nVideo summary of this work and experiments can be found\nhttps://youtu.be/2inxpdrq74U?si=dAdDYsUEr25nZvRn.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u4f7f\u7528\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u52a8\u6001\u64cd\u4f5c\u4efb\u52a1\uff0c\u901a\u8fc7\u9884\u6d4b\u53c2\u6570\u5316\u7b56\u7565\u7684\u8c03\u6574\u6765\u5e94\u5bf9\u9ad8\u7ef4\u5ea6\u3001\u590d\u6742\u52a8\u529b\u5b66\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7b49\u6311\u6218\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u63a8\u7406\u80fd\u529b\u548c\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\uff0c\u4f46\u5982\u4f55\u5c06\u8fd9\u4e9b\u80fd\u529b\u6269\u5c55\u5230\u52a8\u6001\u64cd\u4f5c\u9886\u57df\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002\u52a8\u6001\u64cd\u4f5c\u5177\u6709\u9ad8\u7ef4\u5ea6\u3001\u590d\u6742\u52a8\u529b\u5b66\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7b49\u7279\u70b9\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u65b9\u6cd5\uff0c\u5c06\u4e0a\u4e0b\u6587\u5b66\u4e60\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u57fa\u4e8e\u5148\u524d\u4ea4\u4e92\u9884\u6d4b\u53c2\u6570\u5316\u7b56\u7565\u7684\u8c03\u6574\u3002\u901a\u8fc7\u8f93\u5165-\u8f93\u51fa\u793a\u4f8b\u6765\u6cdb\u5316\u5230\u65b0\u7684\u8f93\u5165\uff0c\u5b9e\u73b0\u5c11\u6837\u672c\u5b66\u4e60\u3002", "result": "\u5728\u4eff\u771f\u548c\u7269\u7406\u673a\u5668\u4eba\u4e0a\u7684\u591a\u4e2a\u4efb\u52a1\u4e2d\uff0c\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u5728\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u4f18\u4e8e\u5176\u4ed6\u66ff\u4ee3\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u6210\u529f\u5e94\u7528\u4e8e\u52a8\u6001\u64cd\u4f5c\u4efb\u52a1\uff0c\u4e3a\u89e3\u51b3\u590d\u6742\u673a\u5668\u4eba\u63a7\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.15038", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.MA", "I.2.9"], "pdf": "https://arxiv.org/pdf/2508.15038", "abs": "https://arxiv.org/abs/2508.15038", "authors": ["Makram Chahine", "William Yang", "Alaa Maalouf", "Justin Siriska", "Ninad Jadhav", "Daniel Vogt", "Stephanie Gil", "Robert Wood", "Daniela Rus"], "title": "Decentralized Vision-Based Autonomous Aerial Wildlife Monitoring", "comment": null, "summary": "Wildlife field operations demand efficient parallel deployment methods to\nidentify and interact with specific individuals, enabling simultaneous\ncollective behavioral analysis, and health and safety interventions. Previous\nrobotics solutions approach the problem from the herd perspective, or are\nmanually operated and limited in scale. We propose a decentralized vision-based\nmulti-quadrotor system for wildlife monitoring that is scalable, low-bandwidth,\nand sensor-minimal (single onboard RGB camera). Our approach enables robust\nidentification and tracking of large species in their natural habitat. We\ndevelop novel vision-based coordination and tracking algorithms designed for\ndynamic, unstructured environments without reliance on centralized\ncommunication or control. We validate our system through real-world\nexperiments, demonstrating reliable deployment in diverse field conditions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u7684\u5206\u6563\u5f0f\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u7528\u4e8e\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\uff0c\u5177\u6709\u53ef\u6269\u5c55\u3001\u4f4e\u5e26\u5bbd\u548c\u6700\u5c0f\u4f20\u611f\u5668\u9700\u6c42\u7684\u7279\u70b9", "motivation": "\u91ce\u751f\u52a8\u7269\u91ce\u5916\u4f5c\u4e1a\u9700\u8981\u9ad8\u6548\u7684\u5e76\u884c\u90e8\u7f72\u65b9\u6cd5\u6765\u8bc6\u522b\u548c\u4e0e\u7279\u5b9a\u4e2a\u4f53\u4e92\u52a8\uff0c\u4ee5\u652f\u6301\u540c\u65f6\u8fdb\u884c\u7684\u96c6\u4f53\u884c\u4e3a\u5206\u6790\u4ee5\u53ca\u5065\u5eb7\u548c\u5b89\u5168\u5e72\u9884", "method": "\u5f00\u53d1\u65b0\u9896\u7684\u57fa\u4e8e\u89c6\u89c9\u7684\u534f\u8c03\u548c\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u8bbe\u8ba1\u7528\u4e8e\u52a8\u6001\u975e\u7ed3\u6784\u5316\u73af\u5883\uff0c\u4e0d\u4f9d\u8d56\u96c6\u4e2d\u5f0f\u901a\u4fe1\u6216\u63a7\u5236\uff0c\u4ec5\u4f7f\u7528\u5355\u4e2a\u677f\u8f7dRGB\u6444\u50cf\u5934", "result": "\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u91ce\u5916\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u90e8\u7f72\u80fd\u529b", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u5728\u5927\u7269\u79cd\u7684\u81ea\u7136\u6816\u606f\u5730\u4e2d\u5b9e\u73b0\u7a33\u5065\u7684\u8bc6\u522b\u548c\u8ddf\u8e2a\uff0c\u89e3\u51b3\u4e86\u5148\u524d\u673a\u5668\u4eba\u89e3\u51b3\u65b9\u6848\u7684\u5c40\u9650\u6027"}}
{"id": "2508.15160", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.15160", "abs": "https://arxiv.org/abs/2508.15160", "authors": ["Hesam Azadjou", "Suraj Chakravarthi Raja", "Ali Marjaninejad", "Francisco J. Valero-Cuevas"], "title": "Hardware Implementation of a Zero-Prior-Knowledge Approach to Lifelong Learning in Kinematic Control of Tendon-Driven Quadrupeds", "comment": null, "summary": "Like mammals, robots must rapidly learn to control their bodies and interact\nwith their environment despite incomplete knowledge of their body structure and\nsurroundings. They must also adapt to continuous changes in both. This work\npresents a bio-inspired learning algorithm, General-to-Particular (G2P),\napplied to a tendon-driven quadruped robotic system developed and fabricated\nin-house. Our quadruped robot undergoes an initial five-minute phase of\ngeneralized motor babbling, followed by 15 refinement trials (each lasting 20\nseconds) to achieve specific cyclical movements. This process mirrors the\nexploration-exploitation paradigm observed in mammals. With each refinement,\nthe robot progressively improves upon its initial \"good enough\" solution. Our\nresults serve as a proof-of-concept, demonstrating the hardware-in-the-loop\nsystem's ability to learn the control of a tendon-driven quadruped with\nredundancies in just a few minutes to achieve functional and adaptive cyclical\nnon-convex movements. By advancing autonomous control in robotic locomotion,\nour approach paves the way for robots capable of dynamically adjusting to new\nenvironments, ensuring sustained adaptability and performance.", "AI": {"tldr": "\u4e00\u79cd\u53d7\u751f\u7269\u542f\u53d1\u7684G2P\u7b97\u6cd5\uff0c\u57285\u5206\u949f\u5168\u5c40\u8fd0\u52a8\u63a2\u7d22\u540e\u7ecf\u8fc715\u6b21\u7cbe\u70bc\u8bd5\u9a8c\uff0c\u4f7f\u80ce\u7ef3\u9a71\u52a8\u56db\u8db3\u673a\u5668\u4eba\u5feb\u901f\u5b66\u4f1a\u5faa\u73af\u8fd0\u52a8\u63a7\u5236", "motivation": "\u6a21\u4eff\u54fa\u4e73\u52a8\u7269\u7684\u5b66\u4e60\u673a\u5236\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u5728\u4e0d\u5b8c\u6574\u77e5\u8bc6\u4e0b\u5feb\u901f\u5b66\u4f1a\u8eab\u4f53\u63a7\u5236\u548c\u73af\u5883\u9002\u5e94\u7684\u6311\u6218", "method": "\u91c7\u7528\u4ece\u4e00\u822c\u5230\u7279\u5b9a(G2P)\u7684\u5b66\u4e60\u7b97\u6cd5\uff0c\u5305\u62ec5\u5206\u949f\u5168\u5c40\u8fd0\u52a8\u63a2\u7d22\u548c15\u6b21\u6bcf\u6b2120\u79d2\u7684\u7cbe\u70bc\u8bd5\u9a8c", "result": "\u7cfb\u7edf\u5728\u51e0\u5206\u949f\u5185\u5b66\u4f1a\u4e86\u80ce\u7ef3\u9a71\u52a8\u56db\u8db3\u673a\u5668\u4eba\u7684\u529f\u80fd\u6027\u5faa\u73af\u8fd0\u52a8\u63a7\u5236\uff0c\u5e76\u5177\u6709\u9002\u5e94\u80fd\u529b", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u4e3b\u884c\u8d70\u63a7\u5236\u63a8\u8fdb\u4e86\u65b0\u8def\u5f84\uff0c\u4f7f\u5f97\u673a\u5668\u4eba\u80fd\u591f\u52a8\u6001\u9002\u5e94\u65b0\u73af\u5883\u4fdd\u6301\u6027\u80fd"}}
{"id": "2508.15201", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15201", "abs": "https://arxiv.org/abs/2508.15201", "authors": ["Haoran Li", "Yuhui Chen", "Wenbo Cui", "Weiheng Liu", "Kai Liu", "Mingcai Zhou", "Zhengtao Zhang", "Dongbin Zhao"], "title": "Survey of Vision-Language-Action Models for Embodied Manipulation", "comment": "in Chinese language", "summary": "Embodied intelligence systems, which enhance agent capabilities through\ncontinuous environment interactions, have garnered significant attention from\nboth academia and industry. Vision-Language-Action models, inspired by\nadvancements in large foundation models, serve as universal robotic control\nframeworks that substantially improve agent-environment interaction\ncapabilities in embodied intelligence systems. This expansion has broadened\napplication scenarios for embodied AI robots. This survey comprehensively\nreviews VLA models for embodied manipulation. Firstly, it chronicles the\ndevelopmental trajectory of VLA architectures. Subsequently, we conduct a\ndetailed analysis of current research across 5 critical dimensions: VLA model\nstructures, training datasets, pre-training methods, post-training methods, and\nmodel evaluation. Finally, we synthesize key challenges in VLA development and\nreal-world deployment, while outlining promising future research directions.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u56de\u987e\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u6a21\u578b\u5728\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u53d1\u5c55\uff0c\u5206\u6790\u4e86VLA\u67b6\u6784\u6f14\u8fdb\u3001\u6a21\u578b\u7ed3\u6784\u3001\u8bad\u7ec3\u6570\u636e\u96c6\u3001\u9884\u8bad\u7ec3\u65b9\u6cd5\u3001\u540e\u8bad\u7ec3\u65b9\u6cd5\u548c\u6a21\u578b\u8bc4\u4f30\u7b495\u4e2a\u5173\u952e\u7ef4\u5ea6\uff0c\u5e76\u603b\u7ed3\u4e86\u53d1\u5c55\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u6a21\u578b\u6280\u672f\u7684\u53d1\u5c55\uff0c\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4f5c\u4e3a\u901a\u7528\u673a\u5668\u4eba\u63a7\u5236\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\u4e2d\u4ee3\u7406\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u80fd\u529b\uff0c\u6269\u5c55\u4e86\u5177\u8eabAI\u673a\u5668\u4eba\u7684\u5e94\u7528\u573a\u666f\uff0c\u56e0\u6b64\u9700\u8981\u5bf9\u8fd9\u4e00\u9886\u57df\u8fdb\u884c\u7cfb\u7edf\u6027\u7efc\u8ff0\u3002", "method": "\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u9996\u5148\u68b3\u7406VLA\u67b6\u6784\u7684\u53d1\u5c55\u8f68\u8ff9\uff0c\u7136\u540e\u4ece5\u4e2a\u7ef4\u5ea6\u8be6\u7ec6\u5206\u6790\u73b0\u6709\u7814\u7a76\uff1a\u6a21\u578b\u7ed3\u6784\u3001\u8bad\u7ec3\u6570\u636e\u96c6\u3001\u9884\u8bad\u7ec3\u65b9\u6cd5\u3001\u540e\u8bad\u7ec3\u65b9\u6cd5\u548c\u6a21\u578b\u8bc4\u4f30\u3002", "result": "\u7cfb\u7edf\u603b\u7ed3\u4e86VLA\u6a21\u578b\u5728\u5177\u8eab\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u6280\u672f\u53d1\u5c55\u7684\u5173\u952e\u7ef4\u5ea6\u548c\u7814\u7a76\u8fdb\u5c55\u3002", "conclusion": "\u8bba\u6587\u7efc\u5408\u5206\u6790\u4e86VLA\u53d1\u5c55\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u548c\u5b9e\u9645\u90e8\u7f72\u95ee\u9898\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2508.15300", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.15300", "abs": "https://arxiv.org/abs/2508.15300", "authors": ["William McDonald", "Cedric Le Gentil", "Jennifer Wakulicz", "Teresa Vidal-Calleja"], "title": "Mag-Match: Magnetic Vector Field Features for Map Matching and Registration", "comment": "To be published in IROS: IEEE/RSJ International Conference on\n  Intelligent Robots and Systems, 2025", "summary": "Map matching and registration are essential tasks in robotics for\nlocalisation and integration of multi-session or multi-robot data. Traditional\nmethods rely on cameras or LiDARs to capture visual or geometric information\nbut struggle in challenging conditions like smoke or dust. Magnetometers, on\nthe other hand, detect magnetic fields, revealing features invisible to other\nsensors and remaining robust in such environments. In this paper, we introduce\nMag-Match, a novel method for extracting and describing features in 3D magnetic\nvector field maps to register different maps of the same area. Our feature\ndescriptor, based on higher-order derivatives of magnetic field maps, is\ninvariant to global orientation, eliminating the need for gravity-aligned\nmapping. To obtain these higher-order derivatives map-wide given point-wise\nmagnetometer data, we leverage a physics-informed Gaussian Process to perform\nefficient and recursive probabilistic inference of both the magnetic field and\nits derivatives. We evaluate Mag-Match in simulated and real-world experiments\nagainst a SIFT-based approach, demonstrating accurate map-to-map, robot-to-map,\nand robot-to-robot transformations - even without initial gravitational\nalignment.", "AI": {"tldr": "Mag-Match\u662f\u4e00\u79cd\u57fa\u4e8e\u78c1\u573a\u76843D\u5730\u56fe\u5339\u914d\u65b9\u6cd5\uff0c\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\u63d0\u53d6\u78c1\u573a\u9ad8\u9636\u5bfc\u6570\u7279\u5f81\uff0c\u65e0\u9700\u91cd\u529b\u5bf9\u9f50\u5373\u53ef\u5b9e\u73b0\u5730\u56fe\u6ce8\u518c\u548c\u673a\u5668\u4eba\u5b9a\u4f4d\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u76f8\u673a\u6216LiDAR\u7684\u5730\u56fe\u5339\u914d\u65b9\u6cd5\u5728\u70df\u96fe\u3001\u7070\u5c18\u7b49\u6076\u52a3\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u78c1\u529b\u8ba1\u80fd\u591f\u68c0\u6d4b\u5176\u4ed6\u4f20\u611f\u5668\u65e0\u6cd5\u611f\u77e5\u7684\u78c1\u573a\u7279\u5f81\uff0c\u5e76\u5728\u8fd9\u4e9b\u73af\u5883\u4e2d\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u78c1\u573a\u9ad8\u9636\u5bfc\u6570\u7684\u7279\u5f81\u63cf\u8ff0\u5b50\uff0c\u5229\u7528\u7269\u7406\u4fe1\u606f\u9ad8\u65af\u8fc7\u7a0b\u8fdb\u884c\u6982\u7387\u63a8\u7406\uff0c\u63d0\u53d6\u78c1\u573a\u53ca\u5176\u5bfc\u6570\u7279\u5f81\uff0c\u5b9e\u73b0\u65b9\u5411\u4e0d\u53d8\u7684\u5730\u56fe\u5339\u914d\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\uff0cMag-Match\u76f8\u6bd4\u57fa\u4e8eSIFT\u7684\u65b9\u6cd5\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u51c6\u786e\u5b9e\u73b0\u5730\u56fe\u5230\u5730\u56fe\u3001\u673a\u5668\u4eba\u5230\u5730\u56fe\u4ee5\u53ca\u673a\u5668\u4eba\u5230\u673a\u5668\u4eba\u7684\u53d8\u6362\u3002", "conclusion": "Mag-Match\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u6076\u52a3\u73af\u5883\u4e0b\u9c81\u68d2\u7684\u5730\u56fe\u5339\u914d\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u521d\u59cb\u91cd\u529b\u5bf9\u9f50\uff0c\u4e3a\u591a\u4f1a\u8bdd\u6216\u591a\u673a\u5668\u4eba\u6570\u636e\u96c6\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2508.15354", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.15354", "abs": "https://arxiv.org/abs/2508.15354", "authors": ["Chaoran Xiong", "Yulong Huang", "Fangwen Yu", "Changhao Chen", "Yue Wang", "Songpengchen Xia", "Ling Pei"], "title": "Sensing, Social, and Motion Intelligence in Embodied Navigation: A Comprehensive Survey", "comment": null, "summary": "Embodied navigation (EN) advances traditional navigation by enabling robots\nto perform complex egocentric tasks through sensing, social, and motion\nintelligence. In contrast to classic methodologies that rely on explicit\nlocalization and pre-defined maps, EN leverages egocentric perception and\nhuman-like interaction strategies. This survey introduces a comprehensive EN\nformulation structured into five stages: Transition, Observation, Fusion,\nReward-policy construction, and Action (TOFRA). The TOFRA framework serves to\nsynthesize the current state of the art, provide a critical review of relevant\nplatforms and evaluation metrics, and identify critical open research\nchallenges. A list of studies is available at\nhttps://github.com/Franky-X/Awesome-Embodied-Navigation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aTOFRA\u7684\u4e94\u9636\u6bb5\u6846\u67b6\u6765\u7cfb\u7edf\u5316\u5177\u8eab\u5bfc\u822a\u7814\u7a76\uff0c\u5305\u62ec\u8f6c\u6362\u3001\u89c2\u5bdf\u3001\u878d\u5408\u3001\u5956\u52b1\u7b56\u7565\u6784\u5efa\u548c\u884c\u52a8\u9636\u6bb5\uff0c\u5e76\u5bf9\u5f53\u524d\u6280\u672f\u5e73\u53f0\u3001\u8bc4\u4f30\u6307\u6807\u548c\u5f00\u653e\u6311\u6218\u8fdb\u884c\u4e86\u7efc\u8ff0\u3002", "motivation": "\u4f20\u7edf\u5bfc\u822a\u65b9\u6cd5\u4f9d\u8d56\u663e\u5f0f\u5b9a\u4f4d\u548c\u9884\u5b9a\u4e49\u5730\u56fe\uff0c\u800c\u5177\u8eab\u5bfc\u822a\u901a\u8fc7\u81ea\u6211\u4e2d\u5fc3\u611f\u77e5\u548c\u7c7b\u4eba\u4ea4\u4e92\u7b56\u7565\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u6267\u884c\u66f4\u590d\u6742\u7684\u81ea\u6211\u4e2d\u5fc3\u4efb\u52a1\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u6846\u67b6\u6765\u6574\u5408\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u3002", "method": "\u63d0\u51fa\u4e86TOFRA\u4e94\u9636\u6bb5\u6846\u67b6\uff08Transition\u8f6c\u6362\u3001Observation\u89c2\u5bdf\u3001Fusion\u878d\u5408\u3001Reward-policy\u5956\u52b1\u7b56\u7565\u6784\u5efa\u3001Action\u884c\u52a8\uff09\u6765\u7ed3\u6784\u5316\u5177\u8eab\u5bfc\u822a\u7814\u7a76\uff0c\u5e76\u5bf9\u76f8\u5173\u5e73\u53f0\u3001\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u6279\u5224\u6027\u7efc\u8ff0\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u5177\u8eab\u5bfc\u822a\u7814\u7a76\u6846\u67b6\uff0c\u6574\u7406\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u6280\u672f\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u5f00\u653e\u7814\u7a76\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5173\u7814\u7a76\u5217\u8868\u3002", "conclusion": "TOFRA\u6846\u67b6\u4e3a\u5177\u8eab\u5bfc\u822a\u9886\u57df\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\uff0c\u4f46\u4ecd\u5b58\u5728\u8bb8\u591a\u5f85\u89e3\u51b3\u7684\u7814\u7a76\u6311\u6218\u3002"}}
{"id": "2508.15427", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15427", "abs": "https://arxiv.org/abs/2508.15427", "authors": ["Huy Hoang Nguyen", "Johannes Huemer", "Markus Murschitz", "Tobias Glueck", "Minh Nhat Vu", "Andreas Kugi"], "title": "Lang2Lift: A Framework for Language-Guided Pallet Detection and Pose Estimation Integrated in Autonomous Outdoor Forklift Operation", "comment": "8 pages, 7 figures", "summary": "The logistics and construction industries face persistent challenges in\nautomating pallet handling, especially in outdoor environments with variable\npayloads, inconsistencies in pallet quality and dimensions, and unstructured\nsurroundings. In this paper, we tackle automation of a critical step in pallet\ntransport: the pallet pick-up operation. Our work is motivated by labor\nshortages, safety concerns, and inefficiencies in manually locating and\nretrieving pallets under such conditions. We present Lang2Lift, a framework\nthat leverages foundation models for natural language-guided pallet detection\nand 6D pose estimation, enabling operators to specify targets through intuitive\ncommands such as \"pick up the steel beam pallet near the crane.\" The perception\npipeline integrates Florence-2 and SAM-2 for language-grounded segmentation\nwith FoundationPose for robust pose estimation in cluttered, multi-pallet\noutdoor scenes under variable lighting. The resulting poses feed into a motion\nplanning module for fully autonomous forklift operation. We validate Lang2Lift\non the ADAPT autonomous forklift platform, achieving 0.76 mIoU pallet\nsegmentation accuracy on a real-world test dataset. Timing and error analysis\ndemonstrate the system's robustness and confirm its feasibility for deployment\nin operational logistics and construction environments. Video demonstrations\nare available at https://eric-nguyen1402.github.io/lang2lift.github.io/", "AI": {"tldr": "Lang2Lift\u662f\u4e00\u4e2a\u5229\u7528\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u5f15\u5bfc\u7684\u6258\u76d8\u68c0\u6d4b\u548c6D\u59ff\u6001\u4f30\u8ba1\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u5b9e\u73b0\u5168\u81ea\u52a8\u53c9\u8f66\u64cd\u4f5c", "motivation": "\u89e3\u51b3\u7269\u6d41\u548c\u5efa\u7b51\u884c\u4e1a\u5728\u81ea\u52a8\u5316\u6258\u76d8\u642c\u8fd0\u65b9\u9762\u7684\u6311\u6218\uff0c\u5305\u62ec\u52b3\u52a8\u529b\u77ed\u7f3a\u3001\u5b89\u5168\u9690\u60a3\u4ee5\u53ca\u5728\u53d8\u5316\u8d1f\u8f7d\u3001\u6258\u76d8\u8d28\u91cf\u4e0d\u4e00\u81f4\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e0b\u624b\u52a8\u5b9a\u4f4d\u548c\u68c0\u7d22\u6258\u76d8\u7684\u6548\u7387\u4f4e\u4e0b\u95ee\u9898", "method": "\u96c6\u6210Florence-2\u548cSAM-2\u8fdb\u884c\u8bed\u8a00\u57fa\u7840\u5206\u5272\uff0c\u7ed3\u5408FoundationPose\u5728\u6742\u4e71\u591a\u6258\u76d8\u6237\u5916\u573a\u666f\u4e2d\u8fdb\u884c\u9c81\u68d2\u59ff\u6001\u4f30\u8ba1\uff0c\u6700\u7ec8\u901a\u8fc7\u8fd0\u52a8\u89c4\u5212\u6a21\u5757\u5b9e\u73b0\u5168\u81ea\u52a8\u53c9\u8f66\u64cd\u4f5c", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.76 mIoU\u7684\u6258\u76d8\u5206\u5272\u7cbe\u5ea6\uff0c\u65f6\u5e8f\u548c\u8bef\u5dee\u5206\u6790\u8bc1\u660e\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u5728\u7269\u6d41\u548c\u5efa\u7b51\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u53ef\u884c\u6027", "conclusion": "Lang2Lift\u6846\u67b6\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u5148\u8fdb\u611f\u77e5\u6280\u672f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6237\u5916\u73af\u5883\u4e0b\u6258\u76d8\u81ea\u52a8\u5316\u642c\u8fd0\u7684\u5173\u952e\u6280\u672f\u96be\u9898\uff0c\u4e3a\u7269\u6d41\u548c\u5efa\u7b51\u884c\u4e1a\u7684\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.15501", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15501", "abs": "https://arxiv.org/abs/2508.15501", "authors": ["Deyu Zhang", "Xicheng Zhang", "Jiahao Li", "Tingting Long", "Xunhua Dai", "Yongjian Fu", "Jinrui Zhang", "Ju Ren", "Yaoxue Zhang"], "title": "LLM-Driven Self-Refinement for Embodied Drone Task Planning", "comment": "14pages", "summary": "We introduce SRDrone, a novel system designed for self-refinement task\nplanning in industrial-grade embodied drones. SRDrone incorporates two key\ntechnical contributions: First, it employs a continuous state evaluation\nmethodology to robustly and accurately determine task outcomes and provide\nexplanatory feedback. This approach supersedes conventional reliance on\nsingle-frame final-state assessment for continuous, dynamic drone operations.\nSecond, SRDrone implements a hierarchical Behavior Tree (BT) modification\nmodel. This model integrates multi-level BT plan analysis with a constrained\nstrategy space to enable structured reflective learning from experience.\nExperimental results demonstrate that SRDrone achieves a 44.87% improvement in\nSuccess Rate (SR) over baseline methods. Furthermore, real-world deployment\nutilizing an experience base optimized through iterative self-refinement\nattains a 96.25% SR. By embedding adaptive task refinement capabilities within\nan industrial-grade BT planning framework, SRDrone effectively integrates the\ngeneral reasoning intelligence of Large Language Models (LLMs) with the\nstringent physical execution constraints inherent to embodied drones. Code is\navailable at https://github.com/ZXiiiC/SRDrone.", "AI": {"tldr": "SRDrone\u662f\u4e00\u4e2a\u7528\u4e8e\u5de5\u4e1a\u7ea7\u65e0\u4eba\u673a\u81ea\u6211\u7cbe\u5316\u4efb\u52a1\u89c4\u5212\u7684\u65b0\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fde\u7eed\u72b6\u6001\u8bc4\u4f30\u548c\u884c\u4e3a\u6811\u5206\u5c42\u4fee\u6539\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5355\u5e27\u6700\u7ec8\u72b6\u6001\u8bc4\u4f30\uff0c\u65e0\u6cd5\u6ee1\u8db3\u8fde\u7eed\u52a8\u6001\u65e0\u4eba\u673a\u64cd\u4f5c\u7684\u9700\u6c42\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u548c\u51c6\u786e\u7684\u4efb\u52a1\u7ed3\u679c\u8bc4\u4f30\u65b9\u6cd5", "method": "\u91c7\u7528\u8fde\u7eed\u72b6\u6001\u8bc4\u4f30\u65b9\u6cd5\u786e\u5b9a\u4efb\u52a1\u7ed3\u679c\u5e76\u63d0\u4f9b\u89e3\u91ca\u6027\u53cd\u9988\uff1b\u5b9e\u73b0\u5206\u5c42\u884c\u4e3a\u6811\u4fee\u6539\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u7ea7\u884c\u4e3a\u6811\u5206\u6790\u548c\u7ea6\u675f\u7b56\u7565\u7a7a\u95f4\u8fdb\u884c\u7ed3\u6784\u5316\u53cd\u601d\u5b66\u4e60", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aSRDrone\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6210\u529f\u7387\u63d0\u534744.87%\uff1b\u901a\u8fc7\u8fed\u4ee3\u81ea\u6211\u7cbe\u5316\u4f18\u5316\u7684\u7ecf\u9a8c\u5e93\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u8fbe\u523096.25%\u7684\u6210\u529f\u7387", "conclusion": "SRDrone\u6210\u529f\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u63a8\u7406\u667a\u80fd\u4e0e\u65e0\u4eba\u673a\u4e25\u683c\u7269\u7406\u6267\u884c\u7ea6\u675f\u76f8\u7ed3\u5408\uff0c\u5728\u5de5\u4e1a\u7ea7\u884c\u4e3a\u6811\u89c4\u5212\u6846\u67b6\u4e2d\u5d4c\u5165\u4e86\u81ea\u9002\u5e94\u4efb\u52a1\u7cbe\u5316\u80fd\u529b"}}
{"id": "2508.15663", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15663", "abs": "https://arxiv.org/abs/2508.15663", "authors": ["Nikita Kachaev", "Andrei Spiridonov", "Andrey Gorodetsky", "Kirill Muravyev", "Nikita Oskolkov", "Aditya Narendra", "Vlad Shakhuro", "Dmitry Makarov", "Aleksandr I. Panov", "Polina Fedotova", "Alexey K. Kovalev"], "title": "Mind and Motion Aligned: A Joint Evaluation IsaacSim Benchmark for Task Planning and Low-Level Policies in Mobile Manipulation", "comment": null, "summary": "Benchmarks are crucial for evaluating progress in robotics and embodied AI.\nHowever, a significant gap exists between benchmarks designed for high-level\nlanguage instruction following, which often assume perfect low-level execution,\nand those for low-level robot control, which rely on simple, one-step commands.\nThis disconnect prevents a comprehensive evaluation of integrated systems where\nboth task planning and physical execution are critical. To address this, we\npropose Kitchen-R, a novel benchmark that unifies the evaluation of task\nplanning and low-level control within a simulated kitchen environment. Built as\na digital twin using the Isaac Sim simulator and featuring more than 500\ncomplex language instructions, Kitchen-R supports a mobile manipulator robot.\nWe provide baseline methods for our benchmark, including a task-planning\nstrategy based on a vision-language model and a low-level control policy based\non diffusion policy. We also provide a trajectory collection system. Our\nbenchmark offers a flexible framework for three evaluation modes: independent\nassessment of the planning module, independent assessment of the control\npolicy, and, crucially, an integrated evaluation of the whole system. Kitchen-R\nbridges a key gap in embodied AI research, enabling more holistic and realistic\nbenchmarking of language-guided robotic agents.", "AI": {"tldr": "Kitchen-R\u662f\u4e00\u4e2a\u65b0\u7684\u673a\u5668\u4eba\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728\u6a21\u62df\u53a8\u623f\u73af\u5883\u4e2d\u7edf\u4e00\u8bc4\u4f30\u4efb\u52a1\u89c4\u5212\u4e0e\u4f4e\u7ea7\u63a7\u5236\uff0c\u586b\u8865\u4e86\u9ad8\u7ea7\u8bed\u8a00\u6307\u4ee4\u8ddf\u968f\u4e0e\u4f4e\u7ea7\u673a\u5668\u4eba\u63a7\u5236\u4e4b\u95f4\u7684\u8bc4\u4f30\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff1a\u9ad8\u7ea7\u8bed\u8a00\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6\u5047\u8bbe\u5b8c\u7f8e\u4f4e\u7ea7\u6267\u884c\uff0c\u800c\u4f4e\u7ea7\u63a7\u5236\u57fa\u51c6\u4f9d\u8d56\u7b80\u5355\u5355\u6b65\u547d\u4ee4\uff0c\u7f3a\u4e4f\u5bf9\u4efb\u52a1\u89c4\u5212\u548c\u7269\u7406\u6267\u884c\u96c6\u6210\u7cfb\u7edf\u7684\u7efc\u5408\u8bc4\u4f30\u3002", "method": "\u57fa\u4e8eIsaac Sim\u6a21\u62df\u5668\u6784\u5efa\u6570\u5b57\u5b6a\u751f\u53a8\u623f\u73af\u5883\uff0c\u5305\u542b500\u591a\u4e2a\u590d\u6742\u8bed\u8a00\u6307\u4ee4\uff0c\u652f\u6301\u79fb\u52a8\u673a\u68b0\u81c2\u673a\u5668\u4eba\u3002\u63d0\u4f9b\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4efb\u52a1\u89c4\u5212\u7b56\u7565\u548c\u57fa\u4e8e\u6269\u6563\u7b56\u7565\u7684\u4f4e\u7ea7\u63a7\u5236\u7b56\u7565\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "Kitchen-R\u63d0\u4f9b\u4e86\u4e09\u79cd\u8bc4\u4f30\u6a21\u5f0f\u7684\u7075\u6d3b\u6846\u67b6\uff1a\u89c4\u5212\u6a21\u5757\u72ec\u7acb\u8bc4\u4f30\u3001\u63a7\u5236\u7b56\u7565\u72ec\u7acb\u8bc4\u4f30\u4ee5\u53ca\u5173\u952e\u7684\u7cfb\u7edf\u96c6\u6210\u8bc4\u4f30\u3002", "conclusion": "Kitchen-R\u586b\u8865\u4e86\u5177\u8eabAI\u7814\u7a76\u7684\u5173\u952e\u7a7a\u767d\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8bed\u8a00\u5f15\u5bfc\u673a\u5668\u4eba\u4ee3\u7406\u66f4\u5168\u9762\u548c\u73b0\u5b9e\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2508.15669", "categories": ["cs.RO", "cs.LG", "68T40", "I.2.9"], "pdf": "https://arxiv.org/pdf/2508.15669", "abs": "https://arxiv.org/abs/2508.15669", "authors": ["Annie S. Chen", "Philemon Brakel", "Antonia Bronars", "Annie Xie", "Sandy Huang", "Oliver Groth", "Maria Bauza", "Markus Wulfmeier", "Nicolas Heess", "Dushyant Rao"], "title": "Exploiting Policy Idling for Dexterous Manipulation", "comment": "A similar version to this paper was accepted at IROS 2025", "summary": "Learning-based methods for dexterous manipulation have made notable progress\nin recent years. However, learned policies often still lack reliability and\nexhibit limited robustness to important factors of variation. One failure\npattern that can be observed across many settings is that policies idle, i.e.\nthey cease to move beyond a small region of states when they reach certain\nstates. This policy idling is often a reflection of the training data. For\ninstance, it can occur when the data contains small actions in areas where the\nrobot needs to perform high-precision motions, e.g., when preparing to grasp an\nobject or object insertion. Prior works have tried to mitigate this phenomenon\ne.g. by filtering the training data or modifying the control frequency.\nHowever, these approaches can negatively impact policy performance in other\nways. As an alternative, we investigate how to leverage the detectability of\nidling behavior to inform exploration and policy improvement. Our approach,\nPause-Induced Perturbations (PIP), applies perturbations at detected idling\nstates, thus helping it to escape problematic basins of attraction. On a range\nof challenging simulated dual-arm tasks, we find that this simple approach can\nalready noticeably improve test-time performance, with no additional\nsupervision or training. Furthermore, since the robot tends to idle at critical\npoints in a movement, we also find that learning from the resulting episodes\nleads to better iterative policy improvement compared to prior approaches. Our\nperturbation strategy also leads to a 15-35% improvement in absolute success\nrate on a real-world insertion task that requires complex multi-finger\nmanipulation.", "AI": {"tldr": "\u63d0\u51faPause-Induced Perturbations (PIP)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u68c0\u6d4b\u5230\u7684\u7a7a\u95f2\u72b6\u6001\u65bd\u52a0\u6270\u52a8\u6765\u5e2e\u52a9\u7b56\u7565\u9003\u79bb\u95ee\u9898\u5438\u5f15\u57df\uff0c\u63d0\u9ad8\u7075\u5de7\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\u548c\u6210\u529f\u7387", "motivation": "\u5b66\u4e60\u578b\u7075\u5de7\u64cd\u4f5c\u7b56\u7565\u7ecf\u5e38\u5728\u7279\u5b9a\u72b6\u6001\u51fa\u73b0\u7a7a\u95f2\u73b0\u8c61\uff08\u505c\u6b62\u79fb\u52a8\uff09\uff0c\u8fd9\u53cd\u6620\u4e86\u8bad\u7ec3\u6570\u636e\u7684\u5c40\u9650\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u6570\u636e\u8fc7\u6ee4\u6216\u63a7\u5236\u9891\u7387\u8c03\u6574\u4f1a\u5bf9\u7b56\u7565\u6027\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd", "method": "PIP\u65b9\u6cd5\u68c0\u6d4b\u7b56\u7565\u7a7a\u95f2\u72b6\u6001\uff0c\u5e76\u5728\u8fd9\u4e9b\u72b6\u6001\u65bd\u52a0\u6270\u52a8\uff0c\u5e2e\u52a9\u7b56\u7565\u9003\u79bb\u95ee\u9898\u5438\u5f15\u57df\uff0c\u65e0\u9700\u989d\u5916\u76d1\u7763\u6216\u8bad\u7ec3", "result": "\u5728\u6a21\u62df\u53cc\u81c2\u4efb\u52a1\u4e2d\u663e\u8457\u6539\u5584\u6d4b\u8bd5\u6027\u80fd\uff0c\u771f\u5b9e\u4e16\u754c\u63d2\u5165\u4efb\u52a1\u4e2d\u7edd\u5bf9\u6210\u529f\u7387\u63d0\u534715-35%\uff0c\u4e14\u5728\u5173\u952e\u8fd0\u52a8\u70b9\u7a7a\u95f2\u7684\u7279\u70b9\u4f7f\u5f97\u4ece\u6270\u52a8\u4ea7\u751f\u7684episodes\u5b66\u4e60\u80fd\u5e26\u6765\u66f4\u597d\u7684\u8fed\u4ee3\u7b56\u7565\u6539\u8fdb", "conclusion": "PIP\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6270\u52a8\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u7075\u5de7\u64cd\u4f5c\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u9ad8\u7cbe\u5ea6\u52a8\u4f5c\u7684\u5173\u952e\u72b6\u6001"}}
{"id": "2508.15732", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.15732", "abs": "https://arxiv.org/abs/2508.15732", "authors": ["Gargi Das", "Daegyun Choi", "Donghoon Kim"], "title": "Understanding and Utilizing Dynamic Coupling in Free-Floating Space Manipulators for On-Orbit Servicing", "comment": "17 pages, 7 figures, 2025 AAS/AIAA Astrodynamics Specialist\n  Conference", "summary": "This study proposes a dynamic coupling-informed trajectory optimization\nalgorithm for free-floating space manipulator systems (SMSs). Dynamic coupling\nbetween the base and the manipulator arms plays a critical role in influencing\nthe system's behavior. While prior research has predominantly focused on\nminimizing this coupling, often overlooking its potential advantages, this work\ninvestigates how dynamic coupling can instead be leveraged to improve\ntrajectory planning. Singular value decomposition (SVD) of the dynamic coupling\nmatrix is employed to identify the dominant components governing coupling\nbehavior. A quantitative metric is then formulated to characterize the strength\nand directionality of the coupling and is incorporated into a trajectory\noptimization framework. To assess the feasibility of the optimized trajectory,\na sliding mode control-based tracking controller is designed to generate the\nrequired joint torque inputs. Simulation results demonstrate that explicitly\naccounting for dynamic coupling in trajectory planning enables more informed\nand potentially more efficient operation, offering new directions for the\ncontrol of free-floating SMSs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u8026\u5408\u4fe1\u606f\u7684\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5\uff0c\u5229\u7528\u7a7a\u95f4\u64cd\u7eb5\u5668\u7cfb\u7edf\u7684\u52a8\u6001\u8026\u5408\u6765\u6539\u5584\u8f68\u8ff9\u89c4\u5212\uff0c\u800c\u975e\u50cf\u4ee5\u5f80\u7814\u7a76\u90a3\u6837\u5c1d\u8bd5\u6700\u5c0f\u5316\u8026\u5408\u6548\u5e94\u3002", "motivation": "\u4ee5\u5f80\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6700\u5c0f\u5316\u7a7a\u95f4\u64cd\u7eb5\u5668\u7cfb\u7edf\u7684\u52a8\u6001\u8026\u5408\uff0c\u800c\u5ffd\u89c6\u4e86\u8026\u5408\u7684\u6f5c\u5728\u4f18\u52bf\u3002\u672c\u6587\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u5982\u4f55\u5229\u7528\u52a8\u6001\u8026\u5408\u6765\u6539\u5584\u8f68\u8ff9\u89c4\u5212\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u52a8\u6001\u8026\u5408\u77e9\u9635\u7684\u5947\u5f02\u503c\u5206\u89e3(SVD)\u6765\u8bc6\u522b\u7ecf\u8425\u8026\u5408\u884c\u4e3a\u7684\u4e3b\u5bfc\u7ec4\u4ef6\uff0c\u5f62\u6210\u5b9a\u91cf\u6307\u6807\u6765\u5b9a\u6027\u5316\u8026\u5408\u5f3a\u5ea6\u548c\u65b9\u5411\u6027\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\u4e2d\u3002\u8bbe\u8ba1\u6ed1\u6a21\u63a7\u5236\u8ddf\u8e2a\u63a7\u5236\u5668\u6765\u8bc4\u4f30\u4f18\u5316\u8f68\u8ff9\u7684\u53ef\u884c\u6027\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0c\u5728\u8f68\u8ff9\u89c4\u5212\u4e2d\u660e\u786e\u8003\u8651\u52a8\u6001\u8026\u5408\u80fd\u591f\u5b9e\u73b0\u66f4\u77e5\u8bc6\u5316\u548c\u66f4\u9ad8\u6548\u7684\u64cd\u4f5c\uff0c\u4e3a\u81ea\u7531\u6d6e\u52a8\u7a7a\u95f4\u64cd\u7eb5\u5668\u7cfb\u7edf\u7684\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u52a8\u6001\u8026\u5408\u800c\u975e\u5c1d\u8bd5\u6700\u5c0f\u5316\u5b83\uff0c\u53ef\u4ee5\u5728\u8f68\u8ff9\u89c4\u5212\u4e2d\u5b9e\u73b0\u66f4\u4f18\u79f0\u7684\u6027\u80fd\uff0c\u8fd9\u4e3a\u7a7a\u95f4\u64cd\u7eb5\u5668\u7cfb\u7edf\u7684\u63a7\u5236\u7b56\u7565\u5f00\u542f\u4e86\u65b0\u7684\u7814\u7a76\u601d\u8def\u3002"}}
{"id": "2508.15755", "categories": ["cs.RO", "cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15755", "abs": "https://arxiv.org/abs/2508.15755", "authors": ["Jie Xu", "Eric Heiden", "Iretiayo Akinola", "Dieter Fox", "Miles Macklin", "Yashraj Narang"], "title": "Neural Robot Dynamics", "comment": null, "summary": "Accurate and efficient simulation of modern robots remains challenging due to\ntheir high degrees of freedom and intricate mechanisms. Neural simulators have\nemerged as a promising alternative to traditional analytical simulators,\ncapable of efficiently predicting complex dynamics and adapting to real-world\ndata; however, existing neural simulators typically require\napplication-specific training and fail to generalize to novel tasks and/or\nenvironments, primarily due to inadequate representations of the global state.\nIn this work, we address the problem of learning generalizable neural\nsimulators for robots that are structured as articulated rigid bodies. We\npropose NeRD (Neural Robot Dynamics), learned robot-specific dynamics models\nfor predicting future states for articulated rigid bodies under contact\nconstraints. NeRD uniquely replaces the low-level dynamics and contact solvers\nin an analytical simulator and employs a robot-centric and spatially-invariant\nsimulation state representation. We integrate the learned NeRD models as an\ninterchangeable backend solver within a state-of-the-art robotics simulator. We\nconduct extensive experiments to show that the NeRD simulators are stable and\naccurate over a thousand simulation steps; generalize across tasks and\nenvironment configurations; enable policy learning exclusively in a neural\nengine; and, unlike most classical simulators, can be fine-tuned from\nreal-world data to bridge the gap between simulation and reality.", "AI": {"tldr": "NeRD\u662f\u4e00\u4e2a\u7528\u4e8e\u5173\u8282\u5f0f\u521a\u6027\u673a\u5668\u4eba\u52a8\u529b\u5b66\u5efa\u6a21\u7684\u795e\u7ecf\u6a21\u62df\u5668\uff0c\u80fd\u591f\u66ff\u4ee3\u4f20\u7edf\u5206\u6790\u6a21\u62df\u5668\u7684\u5e95\u5c42\u6c42\u89e3\u5668\uff0c\u5177\u6709\u7a7a\u95f4\u4e0d\u53d8\u6027\u548c\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\uff0c\u652f\u6301\u4ece\u771f\u5b9e\u6570\u636e\u5fae\u8c03\u4ee5\u51cf\u5c11\u4eff\u771f\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u6a21\u62df\u5668\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u5e94\u7528\u8bad\u7ec3\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u65b0\u4efb\u52a1\u548c\u73af\u5883\uff0c\u4e3b\u8981\u56e0\u4e3a\u5168\u5c40\u72b6\u6001\u8868\u793a\u4e0d\u8db3\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6cdb\u5316\u4e14\u53ef\u9002\u5e94\u771f\u5b9e\u6570\u636e\u7684\u673a\u5668\u4eba\u52a8\u529b\u5b66\u6a21\u578b\u3002", "method": "\u63d0\u51faNeRD\uff08\u795e\u7ecf\u673a\u5668\u4eba\u52a8\u529b\u5b66\uff09\u6a21\u578b\uff0c\u66ff\u4ee3\u5206\u6790\u6a21\u62df\u5668\u4e2d\u7684\u5e95\u5c42\u52a8\u529b\u5b66\u548c\u63a5\u89e6\u6c42\u89e3\u5668\uff0c\u91c7\u7528\u673a\u5668\u4eba\u4e2d\u5fc3\u4e14\u7a7a\u95f4\u4e0d\u53d8\u7684\u4eff\u771f\u72b6\u6001\u8868\u793a\uff0c\u5e76\u96c6\u6210\u5230\u73b0\u6709\u673a\u5668\u4eba\u6a21\u62df\u5668\u4e2d\u4f5c\u4e3a\u53ef\u4e92\u6362\u540e\u7aef\u6c42\u89e3\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660eNeRD\u6a21\u62df\u5668\u5728\u6570\u5343\u4e2a\u4eff\u771f\u6b65\u9aa4\u4e2d\u4fdd\u6301\u7a33\u5b9a\u548c\u51c6\u786e\uff1b\u80fd\u591f\u8de8\u4efb\u52a1\u548c\u73af\u5883\u914d\u7f6e\u6cdb\u5316\uff1b\u652f\u6301\u5728\u795e\u7ecf\u5f15\u64ce\u4e2d\u5b8c\u5168\u8fdb\u884c\u7b56\u7565\u5b66\u4e60\uff1b\u53ef\u4ee5\u4ece\u771f\u5b9e\u4e16\u754c\u6570\u636e\u5fae\u8c03\u4ee5\u7f29\u5c0f\u4eff\u771f\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u3002", "conclusion": "NeRD\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u53ef\u6cdb\u5316\u7684\u795e\u7ecf\u6a21\u62df\u5668\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5173\u8282\u5f0f\u521a\u6027\u673a\u5668\u4eba\u7684\u590d\u6742\u52a8\u529b\u5b66\uff0c\u5e76\u6210\u529f\u6865\u63a5\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
