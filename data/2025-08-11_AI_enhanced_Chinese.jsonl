{"id": "2508.05773", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.05773", "abs": "https://arxiv.org/abs/2508.05773", "authors": ["Keyvan Majd", "Hardik Parwana", "Bardh Hoxha", "Steven Hong", "Hideki Okamoto", "Georgios Fainekos"], "title": "GPU-Accelerated Barrier-Rate Guided MPPI Control for Tractor-Trailer Systems", "comment": "Accepted to IEEE ITSC 2025", "summary": "Articulated vehicles such as tractor-trailers, yard trucks, and similar\nplatforms must often reverse and maneuver in cluttered spaces where pedestrians\nare present. We present how Barrier-Rate guided Model Predictive Path Integral\n(BR-MPPI) control can solve navigation in such challenging environments.\nBR-MPPI embeds Control Barrier Function (CBF) constraints directly into the\npath-integral update. By steering the importance-sampling distribution toward\ncollision-free, dynamically feasible trajectories, BR-MPPI enhances the\nexploration strength of MPPI and improves robustness of resulting trajectories.\nThe method is evaluated in the high-fidelity CarMaker simulator on a 12 [m]\ntractor-trailer tasked with reverse and forward parking in a parking lot.\nBR-MPPI computes control inputs in above 100 [Hz] on a single GPU (for\nscenarios with eight obstacles) and maintains better parking clearance than a\nstandard MPPI baseline and an MPPI with collision cost baseline.", "AI": {"tldr": "BR-MPPI\u63a7\u5236\u65b9\u6cd5\u901a\u8fc7\u5d4c\u5165CBF\u7ea6\u675f\uff0c\u63d0\u5347\u4e86MPPI\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u5b9e\u65f6\u6027\u548c\u907f\u969c\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u94f0\u63a5\u5f0f\u8f66\u8f86\u5728\u62e5\u6324\u73af\u5883\u4e2d\u5012\u8f66\u548c\u673a\u52a8\u65f6\u7684\u5bfc\u822a\u95ee\u9898\uff0c\u786e\u4fdd\u5b89\u5168\u6027\u548c\u52a8\u6001\u53ef\u884c\u6027\u3002", "method": "\u5728\u8def\u5f84\u79ef\u5206\u66f4\u65b0\u4e2d\u5d4c\u5165\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBF\uff09\u7ea6\u675f\uff0c\u5f15\u5bfc\u91c7\u6837\u5206\u5e03\u671d\u5411\u65e0\u78b0\u649e\u4e14\u52a8\u6001\u53ef\u884c\u7684\u8f68\u8ff9\u3002", "result": "\u5728\u9ad8\u4fdd\u771fCarMaker\u6a21\u62df\u5668\u4e2d\uff0cBR-MPPI\u572812\u7c73\u62d6\u6302\u8f66\u5012\u8f66\u548c\u524d\u8fdb\u505c\u8f66\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6MPPI\u548c\u5e26\u78b0\u649e\u6210\u672c\u7684MPPI\u57fa\u7ebf\u3002", "conclusion": "BR-MPPI\u5728\u5b9e\u65f6\u6027\u548c\u907f\u969c\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e2d\u7684\u8f66\u8f86\u5bfc\u822a\u3002"}}
{"id": "2508.05838", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY", "68T07, 68T40, 90C40, 93E35", "I.2.6; I.2.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2508.05838", "abs": "https://arxiv.org/abs/2508.05838", "authors": ["Ahmad Farooq", "Kamran Iqbal"], "title": "Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction", "comment": "Published in the Proceedings of the 2025 3rd International Conference\n  on Robotics, Control and Vision Engineering (RCVE'25). 6 pages, 3 figures, 1\n  table", "summary": "This paper presents a novel approach that integrates vision foundation models\nwith reinforcement learning to enhance object interaction capabilities in\nsimulated environments. By combining the Segment Anything Model (SAM) and\nYOLOv5 with a Proximal Policy Optimization (PPO) agent operating in the\nAI2-THOR simulation environment, we enable the agent to perceive and interact\nwith objects more effectively. Our comprehensive experiments, conducted across\nfour diverse indoor kitchen settings, demonstrate significant improvements in\nobject interaction success rates and navigation efficiency compared to a\nbaseline agent without advanced perception. The results show a 68% increase in\naverage cumulative reward, a 52.5% improvement in object interaction success\nrate, and a 33% increase in navigation efficiency. These findings highlight the\npotential of integrating foundation models with reinforcement learning for\ncomplex robotic tasks, paving the way for more sophisticated and capable\nautonomous agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408Segment Anything Model\uff08SAM\uff09\u548cYOLOv5\u4ee5\u53caPPO\u7b97\u6cd5\uff0c\u5728AI2-THOR\u6a21\u62df\u73af\u5883\u4e2d\u63d0\u5347\u4e86\u7269\u4f53\u4ea4\u4e92\u80fd\u529b\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u7269\u4f53\u4ea4\u4e92\u6210\u529f\u7387\u548c\u5bfc\u822a\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u63d0\u5347\u6a21\u62df\u73af\u5883\u4e2d\u667a\u80fd\u4f53\u7684\u7269\u4f53\u4ea4\u4e92\u80fd\u529b\uff0c\u63a2\u7d22\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\u6f5c\u529b\u3002", "method": "\u7ed3\u5408SAM\u548cYOLOv5\u4f5c\u4e3a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u4f7f\u7528PPO\u7b97\u6cd5\u5728AI2-THOR\u73af\u5883\u4e2d\u8bad\u7ec3\u667a\u80fd\u4f53\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5e73\u5747\u7d2f\u79ef\u5956\u52b1\u63d0\u534768%\uff0c\u7269\u4f53\u4ea4\u4e92\u6210\u529f\u7387\u63d0\u9ad852.5%\uff0c\u5bfc\u822a\u6548\u7387\u63d0\u534733%\u3002", "conclusion": "\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\u5728\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u66f4\u5148\u8fdb\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.05936", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05936", "abs": "https://arxiv.org/abs/2508.05936", "authors": ["Haohui Pan", "Takuya Kiyokawa", "Tomoki Ishikura", "Shingo Hamada", "Genichiro Matsuda", "Kensuke Harada"], "title": "Modular Vacuum-Based Fixturing System for Adaptive Disassembly Workspace Integration", "comment": "8 pages, 9 figures", "summary": "The disassembly of small household appliances poses significant challenges\ndue to their complex and curved geometries, which render traditional rigid\nfixtures inadequate. In this paper, we propose a modular vacuum-based fixturing\nsystem that leverages commercially available balloon-type soft grippers to\nconform to arbitrarily shaped surfaces and provide stable support during\nscrew-removal tasks. To enable a reliable deployment of the system, we develop\na stability-aware planning framework that samples the bottom surface of the\ntarget object, filters candidate contact points based on geometric continuity,\nand evaluates support configurations using convex hull-based static stability\ncriteria. We compare the quality of object placement under different numbers\nand configurations of balloon hands. In addition, real-world experiments were\nconducted to compare the success rates of traditional rigid fixtures with our\nproposed system. The results demonstrate that our method consistently achieves\nhigher success rates and superior placement stability during screw removal\ntasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u5757\u5316\u771f\u7a7a\u5939\u5177\u7684\u7cfb\u7edf\uff0c\u5229\u7528\u8f6f\u6293\u624b\u9002\u5e94\u590d\u6742\u51e0\u4f55\u5f62\u72b6\uff0c\u63d0\u9ad8\u87ba\u4e1d\u62c6\u5378\u4efb\u52a1\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u521a\u6027\u5939\u5177\u65e0\u6cd5\u9002\u5e94\u5c0f\u578b\u5bb6\u7535\u7684\u590d\u6742\u66f2\u9762\u51e0\u4f55\u5f62\u72b6\uff0c\u5bfc\u81f4\u62c6\u5378\u56f0\u96be\u3002", "method": "\u5f00\u53d1\u4e86\u7a33\u5b9a\u6027\u611f\u77e5\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u91c7\u6837\u3001\u7b5b\u9009\u5019\u9009\u63a5\u89e6\u70b9\uff0c\u5e76\u4f7f\u7528\u51f8\u5305\u9759\u6001\u7a33\u5b9a\u6027\u6807\u51c6\u8bc4\u4f30\u914d\u7f6e\u3002", "result": "\u4e0e\u4f20\u7edf\u521a\u6027\u5939\u5177\u76f8\u6bd4\uff0c\u65b0\u7cfb\u7edf\u5728\u87ba\u4e1d\u62c6\u5378\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u548c\u7a33\u5b9a\u6027\u66f4\u9ad8\u3002", "conclusion": "\u6a21\u5757\u5316\u771f\u7a7a\u5939\u5177\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u7269\u4f53\u7684\u62c6\u5378\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.05937", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05937", "abs": "https://arxiv.org/abs/2508.05937", "authors": ["Gen Sako", "Takuya Kiyokawa", "Kensuke Harada", "Tomoki Ishikura", "Naoya Miyaji", "Genichiro Matsuda"], "title": "Affordance-Guided Dual-Armed Disassembly Teleoperation for Mating Parts", "comment": "6 pages, 9 figures", "summary": "Robotic non-destructive disassembly of mating parts remains challenging due\nto the need for flexible manipulation and the limited visibility of internal\nstructures. This study presents an affordance-guided teleoperation system that\nenables intuitive human demonstrations for dual-arm fix-and-disassemble tasks\nfor mating parts. The system visualizes feasible grasp poses and disassembly\ndirections in a virtual environment, both derived from the object's geometry,\nto address occlusions and structural complexity. To prevent excessive position\ntracking under load when following the affordance, we integrate a hybrid\ncontroller that combines position and impedance control into the teleoperated\ndisassembly arm. Real-world experiments validate the effectiveness of the\nproposed system, showing improved task success rates and reduced object pose\ndeviation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u611f\u77e5\u5f15\u5bfc\u7684\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u4eba\u975e\u7834\u574f\u6027\u62c6\u5378\u914d\u5bf9\u96f6\u4ef6\u7684\u6311\u6218\uff0c\u901a\u8fc7\u865a\u62df\u73af\u5883\u53ef\u89c6\u5316\u53ef\u884c\u6293\u53d6\u4f4d\u59ff\u548c\u62c6\u5378\u65b9\u5411\uff0c\u7ed3\u5408\u6df7\u5408\u63a7\u5236\u5668\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u673a\u5668\u4eba\u975e\u7834\u574f\u6027\u62c6\u5378\u914d\u5bf9\u96f6\u4ef6\u56e0\u64cd\u4f5c\u7075\u6d3b\u6027\u548c\u5185\u90e8\u7ed3\u6784\u53ef\u89c1\u6027\u53d7\u9650\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u4e00\u79cd\u76f4\u89c2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u611f\u77e5\u5f15\u5bfc\u7684\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u7ed3\u5408\u865a\u62df\u73af\u5883\u53ef\u89c6\u5316\u53ef\u884c\u6293\u53d6\u4f4d\u59ff\u548c\u62c6\u5378\u65b9\u5411\uff0c\u5e76\u96c6\u6210\u6df7\u5408\u63a7\u5236\u5668\uff08\u4f4d\u7f6e\u4e0e\u963b\u6297\u63a7\u5236\uff09\u4ee5\u4f18\u5316\u8ddf\u8e2a\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u4e14\u7269\u4f53\u4f4d\u59ff\u504f\u5dee\u51cf\u5c11\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u590d\u6742\u62c6\u5378\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u89c2\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05941", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05941", "abs": "https://arxiv.org/abs/2508.05941", "authors": ["Zhanyi Sun", "Shuran Song"], "title": "Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution", "comment": null, "summary": "Visuomotor policies trained via behavior cloning are vulnerable to covariate\nshift, where small deviations from expert trajectories can compound into\nfailure. Common strategies to mitigate this issue involve expanding the\ntraining distribution through human-in-the-loop corrections or synthetic data\naugmentation. However, these approaches are often labor-intensive, rely on\nstrong task assumptions, or compromise the quality of imitation. We introduce\nLatent Policy Barrier, a framework for robust visuomotor policy learning.\nInspired by Control Barrier Functions, LPB treats the latent embeddings of\nexpert demonstrations as an implicit barrier separating safe, in-distribution\nstates from unsafe, out-of-distribution (OOD) ones. Our approach decouples the\nrole of precise expert imitation and OOD recovery into two separate modules: a\nbase diffusion policy solely on expert data, and a dynamics model trained on\nboth expert and suboptimal policy rollout data. At inference time, the dynamics\nmodel predicts future latent states and optimizes them to stay within the\nexpert distribution. Both simulated and real-world experiments show that LPB\nimproves both policy robustness and data efficiency, enabling reliable\nmanipulation from limited expert data and without additional human correction\nor annotation.", "AI": {"tldr": "LPB\u6846\u67b6\u901a\u8fc7\u5c06\u4e13\u5bb6\u6f14\u793a\u7684\u6f5c\u5728\u5d4c\u5165\u4f5c\u4e3a\u5b89\u5168\u4e0e\u4e0d\u5b89\u5168\u72b6\u6001\u7684\u9690\u5f0f\u5c4f\u969c\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u884c\u4e3a\u514b\u9686\u4e2d\u56e0\u534f\u53d8\u91cf\u504f\u79fb\u5bfc\u81f4\u7684\u7b56\u7565\u8106\u5f31\u6027\u95ee\u9898\uff0c\u907f\u514d\u4f9d\u8d56\u4eba\u5de5\u4fee\u6b63\u6216\u6570\u636e\u589e\u5f3a\u3002", "method": "\u7ed3\u5408\u57fa\u7840\u6269\u6563\u7b56\u7565\u548c\u52a8\u6001\u6a21\u578b\uff0c\u524d\u8005\u4e13\u6ce8\u4e8e\u4e13\u5bb6\u6570\u636e\u6a21\u4eff\uff0c\u540e\u8005\u5229\u7528\u4e13\u5bb6\u548c\u6b21\u4f18\u7b56\u7565\u6570\u636e\u9884\u6d4b\u672a\u6765\u72b6\u6001\u5e76\u4f18\u5316\u5176\u4fdd\u6301\u5728\u4e13\u5bb6\u5206\u5e03\u5185\u3002", "result": "\u6a21\u62df\u548c\u771f\u5b9e\u5b9e\u9a8c\u8868\u660e\uff0cLPB\u5728\u6709\u9650\u4e13\u5bb6\u6570\u636e\u4e0b\u63d0\u5347\u4e86\u7b56\u7565\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\uff0c\u65e0\u9700\u989d\u5916\u4eba\u5de5\u5e72\u9884\u3002", "conclusion": "LPB\u4e3a\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u6709\u9650\u4e13\u5bb6\u6570\u636e\u573a\u666f\u3002"}}
{"id": "2508.05946", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.05946", "abs": "https://arxiv.org/abs/2508.05946", "authors": ["Nello Balossino", "Rossana Damiano", "Cristina Gena", "Alberto Lillo", "Anna Maria Marras", "Claudio Mattutino", "Antonio Pizzo", "Alessia Prin", "Fabiana Vernero"], "title": "Social and Telepresence Robots for Accessibility and Inclusion in Small Museums", "comment": null, "summary": "There are still many museums that present accessibility barriers,\nparticularly regarding perceptual, cultural, and cognitive aspects. This is\nespecially evident in low-density population areas. The aim of the ROBSO-PM\nproject is to improve the accessibility of small museums through the use of\nsocial robots and social telepresence robots, focusing on three museums as case\nstudies: the Museum of the Holy Shroud in Turin, a small but globally known\ninstitution, and two lesser known mountain museums: the Museum of the Champlas\ndu Col Carnival and the Pragelato Museum of Alpine Peoples' Costumes and\nTraditions. The project explores two main applications for robots: as guides\nsupporting inclusive visits for foreign or disabled visitors, and as\ntelepresence tools allowing people with limited mobility to access museums\nremotely. From a research perspective, key topics include storytelling, robot\npersonality, empathy, personalization, and, in the case of telepresence,\ncollaboration between the robot and the person, with clearly defined roles and\nautonomy.", "AI": {"tldr": "ROBSO-PM\u9879\u76ee\u901a\u8fc7\u793e\u4ea4\u673a\u5668\u4eba\u548c\u8fdc\u7a0b\u793e\u4ea4\u673a\u5668\u4eba\u63d0\u5347\u5c0f\u578b\u535a\u7269\u9986\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u611f\u77e5\u3001\u6587\u5316\u548c\u8ba4\u77e5\u969c\u788d\u3002", "motivation": "\u5c0f\u578b\u535a\u7269\u9986\u5728\u4f4e\u5bc6\u5ea6\u4eba\u53e3\u5730\u533a\u5b58\u5728\u53ef\u8bbf\u95ee\u6027\u969c\u788d\uff0c\u9879\u76ee\u65e8\u5728\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5728\u4e09\u4e2a\u535a\u7269\u9986\u6848\u4f8b\u4e2d\uff0c\u673a\u5668\u4eba\u88ab\u7528\u4f5c\u5bfc\u89c8\u5de5\u5177\u548c\u8fdc\u7a0b\u8bbf\u95ee\u5de5\u5177\uff0c\u7814\u7a76\u5185\u5bb9\u5305\u62ec\u53d9\u4e8b\u3001\u673a\u5668\u4eba\u4e2a\u6027\u5316\u548c\u534f\u4f5c\u3002", "result": "\u9879\u76ee\u63a2\u7d22\u4e86\u673a\u5668\u4eba\u5728\u5bfc\u89c8\u548c\u8fdc\u7a0b\u8bbf\u95ee\u4e2d\u7684\u5e94\u7528\uff0c\u5f3a\u8c03\u4e2a\u6027\u5316\u548c\u534f\u4f5c\u3002", "conclusion": "\u793e\u4ea4\u673a\u5668\u4eba\u6709\u671b\u63d0\u5347\u535a\u7269\u9986\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u5c24\u5176\u5728\u5c0f\u578b\u548c\u504f\u8fdc\u5730\u533a\u535a\u7269\u9986\u3002"}}
{"id": "2508.05972", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05972", "abs": "https://arxiv.org/abs/2508.05972", "authors": ["Shaoting Liu", "Zhou Liu"], "title": "Dynamical Trajectory Planning of Disturbance Consciousness for Air-Land Bimodal Unmanned Aerial Vehicles", "comment": null, "summary": "Air-land bimodal vehicles provide a promising solution for navigating complex\nenvironments by combining the flexibility of aerial locomotion with the energy\nefficiency of ground mobility. To enhance the robustness of trajectory planning\nunder environmental disturbances, this paper presents a disturbance-aware\nplanning framework that incorporates real-time disturbance estimation into both\npath searching and trajectory optimization. A key component of the framework is\na disturbance-adaptive safety boundary adjustment mechanism, which dynamically\nmodifies the vehicle's feasible dynamic boundaries based on estimated\ndisturbances to ensure trajectory feasibility. Leveraging the dynamics model of\nthe bimodal vehicle, the proposed approach achieves adaptive and reliable\nmotion planning across different terrains and operating conditions. A series of\nreal-world experiments and benchmark comparisons on a custom-built platform\nvalidate the effectiveness and robustness of the method, demonstrating\nimprovements in tracking accuracy, task efficiency, and energy performance\nunder both ground and aerial disturbances.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6270\u52a8\u611f\u77e5\u7684\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u7a7a\u9646\u53cc\u6a21\u8f66\u8f86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u7ed3\u5408\u7a7a\u4e2d\u548c\u5730\u9762\u79fb\u52a8\u7684\u4f18\u52bf\uff0c\u63d0\u9ad8\u8f66\u8f86\u5728\u73af\u5883\u6270\u52a8\u4e0b\u7684\u8f68\u8ff9\u89c4\u5212\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6270\u52a8\u611f\u77e5\u7684\u89c4\u5212\u6846\u67b6\uff0c\u5305\u62ec\u5b9e\u65f6\u6270\u52a8\u4f30\u8ba1\u548c\u52a8\u6001\u5b89\u5168\u8fb9\u754c\u8c03\u6574\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86\u8ddf\u8e2a\u7cbe\u5ea6\u3001\u4efb\u52a1\u6548\u7387\u548c\u80fd\u6e90\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u548c\u53ef\u9760\u7684\u8fd0\u52a8\u89c4\u5212\u3002"}}
{"id": "2508.06053", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06053", "abs": "https://arxiv.org/abs/2508.06053", "authors": ["Kaixuan Wu", "Yuanzhuo Xu", "Zejun Zhang", "Weiping Zhu", "Steve Drew", "Xiaoguang Niu"], "title": "ReNiL: Relative Neural Inertial Locator with Any-Scale Bayesian Inference", "comment": null, "summary": "Pedestrian inertial localization is key for mobile and IoT services because\nit provides infrastructure-free positioning. Yet most learning-based methods\ndepend on fixed sliding-window integration, struggle to adapt to diverse motion\nscales and cadences, and yield inconsistent uncertainty, limiting real-world\nuse. We present ReNiL, a Bayesian deep-learning framework for accurate,\nefficient, and uncertainty-aware pedestrian localization. ReNiL introduces\nInertial Positioning Demand Points (IPDPs) to estimate motion at contextually\nmeaningful waypoints instead of dense tracking, and supports inference on IMU\nsequences at any scale so cadence can match application needs. It couples a\nmotion-aware orientation filter with an Any-Scale Laplace Estimator (ASLE), a\ndual-task network that blends patch-based self-supervision with Bayesian\nregression. By modeling displacements with a Laplace distribution, ReNiL\nprovides homogeneous Euclidean uncertainty that integrates cleanly with other\nsensors. A Bayesian inference chain links successive IPDPs into consistent\ntrajectories. On RoNIN-ds and a new WUDataset covering indoor and outdoor\nmotion from 28 participants, ReNiL achieves state-of-the-art displacement\naccuracy and uncertainty consistency, outperforming TLIO, CTIN, iMoT, and RoNIN\nvariants while reducing computation. Application studies further show\nrobustness and practicality for mobile and IoT localization, making ReNiL a\nscalable, uncertainty-aware foundation for next-generation positioning.", "AI": {"tldr": "ReNiL\u662f\u4e00\u4e2a\u8d1d\u53f6\u65af\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u884c\u4eba\u60ef\u6027\u5b9a\u4f4d\uff0c\u901a\u8fc7IPDPs\u548cASLE\u6280\u672f\u6539\u8fdb\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u6ed1\u52a8\u7a97\u53e3\u96c6\u6210\uff0c\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684\u8fd0\u52a8\u5c3a\u5ea6\u548c\u6b65\u9891\uff0c\u4e14\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e0d\u4e00\u81f4\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "ReNiL\u5f15\u5165IPDPs\u4f30\u8ba1\u8fd0\u52a8\uff0c\u7ed3\u5408ASLE\u7f51\u7edc\uff08\u878d\u5408\u81ea\u76d1\u7763\u548c\u8d1d\u53f6\u65af\u56de\u5f52\uff09\uff0c\u5e76\u901a\u8fc7\u62c9\u666e\u62c9\u65af\u5206\u5e03\u5efa\u6a21\u4f4d\u79fb\uff0c\u63d0\u4f9b\u4e00\u81f4\u7684\u6b27\u51e0\u91cc\u5f97\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728RoNIN-ds\u548cWUDataset\u4e0a\uff0cReNiL\u5728\u4f4d\u79fb\u7cbe\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982TLIO\u3001CTIN\u7b49\uff09\uff0c\u540c\u65f6\u8ba1\u7b97\u91cf\u66f4\u4f4e\u3002", "conclusion": "ReNiL\u4e3a\u79fb\u52a8\u548c\u7269\u8054\u7f51\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u5176\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.06095", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06095", "abs": "https://arxiv.org/abs/2508.06095", "authors": ["Mitchell Abrams", "Thies Oelerich", "Christian Hartl-Nesic", "Andreas Kugi", "Matthias Scheutz"], "title": "Incremental Language Understanding for Online Motion Planning of Robot Manipulators", "comment": "8 pages, 9 figures, accepted at IROS 2025", "summary": "Human-robot interaction requires robots to process language incrementally,\nadapting their actions in real-time based on evolving speech input. Existing\napproaches to language-guided robot motion planning typically assume fully\nspecified instructions, resulting in inefficient stop-and-replan behavior when\ncorrections or clarifications occur. In this paper, we introduce a novel\nreasoning-based incremental parser which integrates an online motion planning\nalgorithm within the cognitive architecture. Our approach enables continuous\nadaptation to dynamic linguistic input, allowing robots to update motion plans\nwithout restarting execution. The incremental parser maintains multiple\ncandidate parses, leveraging reasoning mechanisms to resolve ambiguities and\nrevise interpretations when needed. By combining symbolic reasoning with online\nmotion planning, our system achieves greater flexibility in handling speech\ncorrections and dynamically changing constraints. We evaluate our framework in\nreal-world human-robot interaction scenarios, demonstrating online adaptions of\ngoal poses, constraints, or task objectives. Our results highlight the\nadvantages of integrating incremental language understanding with real-time\nmotion planning for natural and fluid human-robot collaboration. The\nexperiments are demonstrated in the accompanying video at\nwww.acin.tuwien.ac.at/42d5.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a8\u7406\u7684\u589e\u91cf\u89e3\u6790\u5668\uff0c\u7ed3\u5408\u5728\u7ebf\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u5b9e\u65f6\u9002\u5e94\u52a8\u6001\u8bed\u8a00\u8f93\u5165\uff0c\u65e0\u9700\u91cd\u542f\u6267\u884c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u6307\u4ee4\u5b8c\u5168\u660e\u786e\uff0c\u5bfc\u81f4\u4fee\u6b63\u6216\u6f84\u6e05\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u505c\u6b62\u5e76\u91cd\u65b0\u89c4\u5212\u3002", "method": "\u5f15\u5165\u589e\u91cf\u89e3\u6790\u5668\uff0c\u7ef4\u62a4\u591a\u4e2a\u5019\u9009\u89e3\u6790\uff0c\u7ed3\u5408\u7b26\u53f7\u63a8\u7406\u4e0e\u5728\u7ebf\u8fd0\u52a8\u89c4\u5212\u3002", "result": "\u5728\u771f\u5b9e\u4eba\u673a\u4ea4\u4e92\u573a\u666f\u4e2d\u9a8c\u8bc1\uff0c\u80fd\u5728\u7ebf\u8c03\u6574\u76ee\u6807\u4f4d\u59ff\u3001\u7ea6\u675f\u6216\u4efb\u52a1\u76ee\u6807\u3002", "conclusion": "\u589e\u91cf\u8bed\u8a00\u7406\u89e3\u4e0e\u5b9e\u65f6\u8fd0\u52a8\u89c4\u5212\u7ed3\u5408\uff0c\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u7684\u81ea\u7136\u6027\u548c\u6d41\u7545\u6027\u3002"}}
{"id": "2508.06096", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06096", "abs": "https://arxiv.org/abs/2508.06096", "authors": ["Eric Jing", "Abdeslam Boularias"], "title": "Bounding Distributional Shifts in World Modeling through Novelty Detection", "comment": "7 pages, 6 figures", "summary": "Recent work on visual world models shows significant promise in latent state\ndynamics obtained from pre-trained image backbones. However, most of the\ncurrent approaches are sensitive to training quality, requiring near-complete\ncoverage of the action and state space during training to prevent divergence\nduring inference. To make a model-based planning algorithm more robust to the\nquality of the learned world model, we propose in this work to use a\nvariational autoencoder as a novelty detector to ensure that proposed action\ntrajectories during planning do not cause the learned model to deviate from the\ntraining data distribution. To evaluate the effectiveness of this approach, a\nseries of experiments in challenging simulated robot environments was carried\nout, with the proposed method incorporated into a model-predictive control\npolicy loop extending the DINO-WM architecture. The results clearly show that\nthe proposed method improves over state-of-the-art solutions in terms of data\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u65b0\u9896\u6027\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u89c6\u89c9\u4e16\u754c\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u4e16\u754c\u6a21\u578b\u5bf9\u8bad\u7ec3\u8d28\u91cf\u654f\u611f\uff0c\u9700\u8981\u8fd1\u4e4e\u5b8c\u5168\u8986\u76d6\u52a8\u4f5c\u548c\u72b6\u6001\u7a7a\u95f4\u4ee5\u907f\u514d\u63a8\u7406\u65f6\u53d1\u6563\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4f5c\u4e3a\u65b0\u9896\u6027\u68c0\u6d4b\u5668\uff0c\u786e\u4fdd\u89c4\u5212\u4e2d\u7684\u52a8\u4f5c\u8f68\u8ff9\u4e0d\u504f\u79bb\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u3002", "result": "\u5728\u6a21\u62df\u673a\u5668\u4eba\u73af\u5883\u4e2d\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\u3002"}}
{"id": "2508.06181", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06181", "abs": "https://arxiv.org/abs/2508.06181", "authors": ["Jan W\u0119grzynowski", "Piotr Kicki", "Grzegorz Czechmanowski", "Maciej Krupka", "Krzysztof Walas"], "title": "Beyond Constant Parameters: Hyper Prediction Models and HyperMPC", "comment": null, "summary": "Model Predictive Control (MPC) is among the most widely adopted and reliable\nmethods for robot control, relying critically on an accurate dynamics model.\nHowever, existing dynamics models used in the gradient-based MPC are limited by\ncomputational complexity and state representation. To address this limitation,\nwe propose the Hyper Prediction Model (HyperPM) - a novel approach in which we\nproject the unmodeled dynamics onto a time-dependent dynamics model. This\ntime-dependency is captured through time-varying model parameters, whose\nevolution over the MPC prediction horizon is learned using a neural network.\nSuch formulation preserves the computational efficiency and robustness of the\nbase model while equipping it with the capacity to anticipate previously\nunmodeled phenomena. We evaluated the proposed approach on several challenging\nsystems, including real-world F1TENTH autonomous racing, and demonstrated that\nit significantly reduces long-horizon prediction errors. Moreover, when\nintegrated within the MPC framework (HyperMPC), our method consistently\noutperforms existing state-of-the-art techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u4f9d\u8d56\u52a8\u6001\u6a21\u578b\u7684Hyper Prediction Model\uff08HyperPM\uff09\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u65f6\u95f4\u53d8\u5316\u53c2\u6570\uff0c\u663e\u8457\u51cf\u5c11\u957f\u65f6\u9884\u6d4b\u8bef\u5dee\uff0c\u5e76\u5728MPC\u6846\u67b6\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u68af\u5ea6\u7684MPC\u52a8\u6001\u6a21\u578b\u53d7\u9650\u4e8e\u8ba1\u7b97\u590d\u6742\u6027\u548c\u72b6\u6001\u8868\u793a\uff0c\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u672a\u5efa\u6a21\u52a8\u6001\u3002", "method": "\u63d0\u51faHyperPM\uff0c\u5c06\u672a\u5efa\u6a21\u52a8\u6001\u6295\u5f71\u5230\u65f6\u95f4\u4f9d\u8d56\u7684\u52a8\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u65f6\u95f4\u53d8\u5316\u53c2\u6570\u3002", "result": "\u5728F1TENTH\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u7b49\u6311\u6218\u6027\u7cfb\u7edf\u4e2d\uff0c\u663e\u8457\u51cf\u5c11\u957f\u65f6\u9884\u6d4b\u8bef\u5dee\uff0c\u5e76\u5728MPC\u6846\u67b6\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "HyperPM\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u63d0\u5347\u4e86\u52a8\u6001\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06206", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06206", "abs": "https://arxiv.org/abs/2508.06206", "authors": ["Hanqing Wang", "Shaoyang Wang", "Yiming Zhong", "Zemin Yang", "Jiamin Wang", "Zhiqing Cui", "Jiahao Yuan", "Yifan Han", "Mingyu Liu", "Yuexin Ma"], "title": "Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model", "comment": null, "summary": "Affordance grounding focuses on predicting the specific regions of objects\nthat are associated with the actions to be performed by robots. It plays a\nvital role in the fields of human-robot interaction, human-object interaction,\nembodied manipulation, and embodied perception. Existing models often neglect\nthe affordance shared among different objects because they lack the\nChain-of-Thought(CoT) reasoning abilities, limiting their out-of-domain (OOD)\ngeneralization and explicit reasoning capabilities. To address these\nchallenges, we propose Affordance-R1, the first unified affordance grounding\nframework that integrates cognitive CoT guided Group Relative Policy\nOptimization (GRPO) within a reinforcement learning paradigm. Specifically, we\ndesigned a sophisticated affordance function, which contains format,\nperception, and cognition rewards to effectively guide optimization directions.\nFurthermore, we constructed a high-quality affordance-centric reasoning\ndataset, ReasonAff, to support training. Trained exclusively via reinforcement\nlearning with GRPO and without explicit reasoning data, Affordance-R1 achieves\nrobust zero-shot generalization and exhibits emergent test-time reasoning\ncapabilities. Comprehensive experiments demonstrate that our model outperforms\nwell-established methods and exhibits open-world generalization. To the best of\nour knowledge, Affordance-R1 is the first to integrate GRPO-based RL with\nreasoning into affordance reasoning. The code of our method and our dataset is\nreleased on https://github.com/hq-King/Affordance-R1.", "AI": {"tldr": "Affordance-R1 \u662f\u4e00\u4e2a\u7edf\u4e00\u7684 affordance grounding \u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u8ba4\u77e5 Chain-of-Thought \u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u8de8\u57df\u6cdb\u5316\u548c\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u7f3a\u4e4f\u8de8\u5bf9\u8c61 affordance \u5171\u4eab\u548c\u63a8\u7406\u80fd\u529b\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u548c\u63a8\u7406\u8868\u73b0\u3002", "method": "\u63d0\u51fa Affordance-R1\uff0c\u6574\u5408\u8ba4\u77e5 CoT \u548c GRPO \u5f3a\u5316\u5b66\u4e60\uff0c\u8bbe\u8ba1\u5305\u542b\u683c\u5f0f\u3001\u611f\u77e5\u548c\u8ba4\u77e5\u5956\u52b1\u7684 affordance \u51fd\u6570\uff0c\u5e76\u4f7f\u7528 ReasonAff \u6570\u636e\u96c6\u652f\u6301\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u5728\u96f6\u6837\u672c\u6cdb\u5316\u548c\u63a8\u7406\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u5f00\u653e\u4e16\u754c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Affordance-R1 \u9996\u6b21\u5c06 GRPO \u5f3a\u5316\u5b66\u4e60\u4e0e\u63a8\u7406\u7ed3\u5408\uff0c\u4e3a affordance reasoning \u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.06207", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06207", "abs": "https://arxiv.org/abs/2508.06207", "authors": ["Andrea Dal Prete", "Seyram Ofori", "Chan Yon Sin", "Ashwin Narayan", "Francesco Braghin", "Marta Gandolla", "Haoyong Yu"], "title": "Computer Vision-based Adaptive Control for Back Exoskeleton Performance Optimization", "comment": null, "summary": "Back exoskeletons can reduce musculoskeletal strain, but their effectiveness\ndepends on support modulation and adaptive control. This study addresses two\nchallenges: defining optimal support strategies and developing adaptive control\nbased on payload estimation. We introduce an optimization space based on muscle\nactivity reduction, perceived discomfort, and user preference, constructing\nfunctions to identify optimal strategies. Experiments with 12 subjects revealed\noptimal operating regions, highlighting the need for dynamic modulation. Based\non these insights, we developed a vision-based adaptive control pipeline that\nestimates payloads in real-time by enhancing exoskeleton contextual\nunderstanding, minimising latency and enabling support adaptation within the\ndefined optimisation space. Validation with 12 more subjects showed over 80%\naccuracy and improvements across all metrics. Compared to static control,\nadaptive modulation reduced peak back muscle activation by up to 23% while\npreserving user preference and minimising discomfort. These findings validate\nthe proposed framework and highlight the potential of intelligent,\ncontext-aware control in industrial exoskeletons.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u808c\u8089\u6d3b\u52a8\u51cf\u5c11\u3001\u4e0d\u9002\u611f\u548c\u7528\u6237\u504f\u597d\u7684\u4f18\u5316\u7a7a\u95f4\uff0c\u5f00\u53d1\u4e86\u5b9e\u65f6\u8d1f\u8f7d\u4f30\u8ba1\u7684\u81ea\u9002\u5e94\u63a7\u5236\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u80cc\u90e8\u808c\u8089\u6fc0\u6d3b\u5cf0\u503c\u3002", "motivation": "\u89e3\u51b3\u80cc\u90e8\u5916\u9aa8\u9abc\u5728\u652f\u6301\u8c03\u5236\u548c\u81ea\u9002\u5e94\u63a7\u5236\u65b9\u9762\u7684\u6311\u6218\uff0c\u4ee5\u4f18\u5316\u5176\u6548\u679c\u3002", "method": "\u6784\u5efa\u4f18\u5316\u7a7a\u95f4\uff0c\u5f00\u53d1\u57fa\u4e8e\u89c6\u89c9\u7684\u81ea\u9002\u5e94\u63a7\u5236\u7ba1\u9053\uff0c\u5b9e\u65f6\u4f30\u8ba1\u8d1f\u8f7d\u5e76\u52a8\u6001\u8c03\u6574\u652f\u6301\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u81ea\u9002\u5e94\u8c03\u5236\u5c06\u80cc\u90e8\u808c\u8089\u6fc0\u6d3b\u5cf0\u503c\u964d\u4f4e23%\uff0c\u51c6\u786e\u7387\u8d8580%\uff0c\u4e14\u7528\u6237\u504f\u597d\u548c\u4e0d\u9002\u611f\u5f97\u5230\u6539\u5584\u3002", "conclusion": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u667a\u80fd\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u63a7\u5236\u5728\u5de5\u4e1a\u5916\u9aa8\u9abc\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.06229", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06229", "abs": "https://arxiv.org/abs/2508.06229", "authors": ["Zihao Xu", "Ce Hao", "Chunzheng Wang", "Kuankuan Sima", "Fan Shi", "Jin Song Dong"], "title": "REBot: Reflexive Evasion Robot for Instantaneous Dynamic Obstacle Avoidance", "comment": null, "summary": "Dynamic obstacle avoidance (DOA) is critical for quadrupedal robots operating\nin environments with moving obstacles or humans. Existing approaches typically\nrely on navigation-based trajectory replanning, which assumes sufficient\nreaction time and leading to fails when obstacles approach rapidly. In such\nscenarios, quadrupedal robots require reflexive evasion capabilities to perform\ninstantaneous, low-latency maneuvers. This paper introduces Reflexive Evasion\nRobot (REBot), a control framework that enables quadrupedal robots to achieve\nreal-time reflexive obstacle avoidance. REBot integrates an avoidance policy\nand a recovery policy within a finite-state machine. With carefully designed\nlearning curricula and by incorporating regularization and adaptive rewards,\nREBot achieves robust evasion and rapid stabilization in instantaneous DOA\ntasks. We validate REBot through extensive simulations and real-world\nexperiments, demonstrating notable improvements in avoidance success rates,\nenergy efficiency, and robustness to fast-moving obstacles. Videos and appendix\nare available on https://rebot-2025.github.io/.", "AI": {"tldr": "REBot\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u907f\u969c\u548c\u6062\u590d\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u56db\u8db3\u673a\u5668\u4eba\u7684\u5b9e\u65f6\u53cd\u5c04\u6027\u907f\u969c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u907f\u969c\u6210\u529f\u7387\u3001\u80fd\u6548\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5bfc\u822a\u8f68\u8ff9\u91cd\u89c4\u5212\uff0c\u53cd\u5e94\u65f6\u95f4\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5e94\u5bf9\u5feb\u901f\u63a5\u8fd1\u7684\u969c\u788d\u7269\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4f4e\u5ef6\u8fdf\u7684\u53cd\u5c04\u6027\u907f\u969c\u80fd\u529b\u3002", "method": "REBot\u7ed3\u5408\u907f\u969c\u7b56\u7565\u548c\u6062\u590d\u7b56\u7565\uff0c\u901a\u8fc7\u6709\u9650\u72b6\u6001\u673a\u3001\u5b66\u4e60\u8bfe\u7a0b\u3001\u6b63\u5219\u5316\u548c\u81ea\u9002\u5e94\u5956\u52b1\u5b9e\u73b0\u5b9e\u65f6\u907f\u969c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cREBot\u5728\u907f\u969c\u6210\u529f\u7387\u3001\u80fd\u6548\u548c\u5feb\u901f\u79fb\u52a8\u969c\u788d\u7269\u7684\u9c81\u68d2\u6027\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "REBot\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u53cd\u5c04\u6027\u907f\u969c\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u73af\u5883\u3002"}}
{"id": "2508.06266", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06266", "abs": "https://arxiv.org/abs/2508.06266", "authors": ["Zezeng Li", "Rui Yang", "Ruochen Chen", "ZhongXuan Luo", "Liming Chen"], "title": "ADPro: a Test-time Adaptive Diffusion Policy for Robot Manipulation via Manifold and Initial Noise Constraints", "comment": null, "summary": "Diffusion policies have recently emerged as a powerful class of visuomotor\ncontrollers for robot manipulation, offering stable training and expressive\nmulti-modal action modeling. However, existing approaches typically treat\naction generation as an unconstrained denoising process, ignoring valuable a\npriori knowledge about geometry and control structure. In this work, we propose\nthe Adaptive Diffusion Policy (ADP), a test-time adaptation method that\nintroduces two key inductive biases into the diffusion. First, we embed a\ngeometric manifold constraint that aligns denoising updates with task-relevant\nsubspaces, leveraging the fact that the relative pose between the end-effector\nand target scene provides a natural gradient direction, and guiding denoising\nalong the geodesic path of the manipulation manifold. Then, to reduce\nunnecessary exploration and accelerate convergence, we propose an analytically\nguided initialization: rather than sampling from an uninformative prior, we\ncompute a rough registration between the gripper and target scenes to propose a\nstructured initial noisy action. ADP is compatible with pre-trained diffusion\npolicies and requires no retraining, enabling test-time adaptation that tailors\nthe policy to specific tasks, thereby enhancing generalization across novel\ntasks and environments. Experiments on RLBench, CALVIN, and real-world dataset\nshow that ADPro, an implementation of ADP, improves success rates,\ngeneralization, and sampling efficiency, achieving up to 25% faster execution\nand 9% points over strong diffusion baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u6269\u6563\u7b56\u7565\uff08ADP\uff09\uff0c\u901a\u8fc7\u51e0\u4f55\u7ea6\u675f\u548c\u521d\u59cb\u5316\u4f18\u5316\u63d0\u5347\u6269\u6563\u7b56\u7565\u7684\u6027\u80fd\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u65b0\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u7b56\u7565\u5728\u52a8\u4f5c\u751f\u6210\u4e2d\u5ffd\u7565\u4e86\u51e0\u4f55\u548c\u63a7\u5236\u7ed3\u6784\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "ADP\u5f15\u5165\u51e0\u4f55\u6d41\u5f62\u7ea6\u675f\u548c\u89e3\u6790\u5f15\u5bfc\u521d\u59cb\u5316\uff0c\u4f18\u5316\u6269\u6563\u8fc7\u7a0b\u7684\u52a8\u4f5c\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u663e\u793aADP\u5728RLBench\u3001CALVIN\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e2d\u63d0\u5347\u4e86\u6210\u529f\u7387\u3001\u6cdb\u5316\u80fd\u529b\u548c\u91c7\u6837\u6548\u7387\uff0c\u6267\u884c\u901f\u5ea6\u63d0\u534725%\uff0c\u6210\u529f\u7387\u63d0\u9ad89%\u3002", "conclusion": "ADP\u901a\u8fc7\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u7b56\u7565\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.06276", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06276", "abs": "https://arxiv.org/abs/2508.06276", "authors": ["Juan Heredia", "Christian Schlette", "Mikkel Baun Kj\u00e6rgaard"], "title": "EcBot: Data-Driven Energy Consumption Open-Source MATLAB Library for Manipulators", "comment": null, "summary": "Existing literature proposes models for estimating the electrical power of\nmanipulators, yet two primary limitations prevail. First, most models are\npredominantly tested using traditional industrial robots. Second, these models\noften lack accuracy. To address these issues, we introduce an open source\nMatlab-based library designed to automatically generate \\ac{ec} models for\nmanipulators. The necessary inputs for the library are Denavit-Hartenberg\nparameters, link masses, and centers of mass. Additionally, our model is\ndata-driven and requires real operational data, including joint positions,\nvelocities, accelerations, electrical power, and corresponding timestamps. We\nvalidated our methodology by testing on four lightweight robots sourced from\nthree distinct manufacturers: Universal Robots, Franka Emika, and Kinova. The\nmodel underwent testing, and the results demonstrated an RMSE ranging from 1.42\nW to 2.80 W for the training dataset and from 1.45 W to 5.25 W for the testing\ndataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMatlab\u7684\u5f00\u6e90\u5e93\uff0c\u7528\u4e8e\u751f\u6210\u673a\u68b0\u81c2\u7684\u80fd\u8017\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u6d4b\u8bd5\u8303\u56f4\u548c\u51c6\u786e\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u673a\u68b0\u81c2\u80fd\u8017\u6a21\u578b\u591a\u9488\u5bf9\u4f20\u7edf\u5de5\u4e1a\u673a\u5668\u4eba\u4e14\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u4f7f\u7528Denavit-Hartenberg\u53c2\u6570\u3001\u8d28\u91cf\u3001\u8d28\u5fc3\u7b49\u8f93\u5165\uff0c\u7ed3\u5408\u5b9e\u65f6\u64cd\u4f5c\u6570\u636e\uff0c\u5f00\u53d1\u6570\u636e\u9a71\u52a8\u6a21\u578b\u3002", "result": "\u5728\u56db\u79cd\u8f7b\u91cf\u7ea7\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u8bad\u7ec3\u96c6RMSE\u4e3a1.42-2.80W\uff0c\u6d4b\u8bd5\u96c6\u4e3a1.45-5.25W\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u673a\u68b0\u81c2\u80fd\u8017\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2508.06278", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06278", "abs": "https://arxiv.org/abs/2508.06278", "authors": ["Petr Novak", "Stefan Biffl", "Marek Obitko", "Petr Kadera"], "title": "Mitigating Undesired Conditions in Flexible Production with Product-Process-Resource Asset Knowledge Graphs", "comment": "3 pages, 1 figure", "summary": "Contemporary industrial cyber-physical production systems (CPPS) composed of\nrobotic workcells face significant challenges in the analysis of undesired\nconditions due to the flexibility of Industry 4.0 that disrupts traditional\nquality assurance mechanisms. This paper presents a novel industry-oriented\nsemantic model called Product-Process-Resource Asset Knowledge Graph (PPR-AKG),\nwhich is designed to analyze and mitigate undesired conditions in flexible\nCPPS. Built on top of the well-proven Product-Process-Resource (PPR) model\noriginating from ISA-95 and VDI-3682, a comprehensive OWL ontology addresses\nshortcomings of conventional model-driven engineering for CPPS, particularly\ninadequate undesired condition and error handling representation. The\nintegration of semantic technologies with large language models (LLMs) provides\nintuitive interfaces for factory operators, production planners, and engineers\nto interact with the entire model using natural language. Evaluation with the\nuse case addressing electric vehicle battery remanufacturing demonstrates that\nthe PPR-AKG approach efficiently supports resource allocation based on\nexplicitly represented capabilities as well as identification and mitigation of\nundesired conditions in production. The key contributions include (1) a\nholistic PPR-AKG model capturing multi-dimensional production knowledge, and\n(2) the useful combination of the PPR-AKG with LLM-based chatbots for human\ninteraction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPPR-AKG\u7684\u8bed\u4e49\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u6790\u548c\u7f13\u89e3\u7075\u6d3b\u5de5\u4e1aCPPS\u4e2d\u7684\u4e0d\u826f\u6761\u4ef6\uff0c\u7ed3\u5408\u4e86\u8bed\u4e49\u6280\u672f\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5e76\u901a\u8fc7\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\u518d\u5236\u9020\u7684\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5de5\u4e1aCPPS\u7684\u7075\u6d3b\u6027\u5bfc\u81f4\u4f20\u7edf\u8d28\u91cf\u4fdd\u8bc1\u673a\u5236\u5931\u6548\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u5206\u6790\u548c\u7f13\u89e3\u4e0d\u826f\u6761\u4ef6\u3002", "method": "\u57fa\u4e8ePPR\u6a21\u578b\u6784\u5efa\u4e86PPR-AKG\u8bed\u4e49\u6a21\u578b\uff0c\u7ed3\u5408OWL\u672c\u4f53\u548cLLMs\u6280\u672f\uff0c\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u63a5\u53e3\u3002", "result": "PPR-AKG\u5728\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\u518d\u5236\u9020\u6848\u4f8b\u4e2d\u6709\u6548\u652f\u6301\u8d44\u6e90\u5206\u914d\u548c\u4e0d\u826f\u6761\u4ef6\u7684\u8bc6\u522b\u4e0e\u7f13\u89e3\u3002", "conclusion": "PPR-AKG\u6a21\u578b\u548cLLM\u7ed3\u5408\u4e3a\u5de5\u4e1aCPPS\u63d0\u4f9b\u4e86\u591a\u7ef4\u77e5\u8bc6\u8868\u793a\u548c\u76f4\u89c2\u7684\u4eba\u673a\u4ea4\u4e92\u65b9\u5f0f\u3002"}}
{"id": "2508.06283", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06283", "abs": "https://arxiv.org/abs/2508.06283", "authors": ["Saad Ejaz", "Marco Giberna", "Muhammad Shaheer", "Jose Andres Millan-Romera", "Ali Tourani", "Paul Kremer", "Holger Voos", "Jose Luis Sanchez-Lopez"], "title": "Situationally-aware Path Planning Exploiting 3D Scene Graphs", "comment": null, "summary": "3D Scene Graphs integrate both metric and semantic information, yet their\nstructure remains underutilized for improving path planning efficiency and\ninterpretability. In this work, we present S-Path, a situationally-aware path\nplanner that leverages the metric-semantic structure of indoor 3D Scene Graphs\nto significantly enhance planning efficiency. S-Path follows a two-stage\nprocess: it first performs a search over a semantic graph derived from the\nscene graph to yield a human-understandable high-level path. This also\nidentifies relevant regions for planning, which later allows the decomposition\nof the problem into smaller, independent subproblems that can be solved in\nparallel. We also introduce a replanning mechanism that, in the event of an\ninfeasible path, reuses information from previously solved subproblems to\nupdate semantic heuristics and prioritize reuse to further improve the\nefficiency of future planning attempts. Extensive experiments on both\nreal-world and simulated environments show that S-Path achieves average\nreductions of 5.7x in planning time while maintaining comparable path\noptimality to classical sampling-based planners and surpassing them in complex\nscenarios, making it an efficient and interpretable path planner for\nenvironments represented by indoor 3D Scene Graphs.", "AI": {"tldr": "S-Path\u5229\u75283D\u573a\u666f\u56fe\u7684\u8bed\u4e49\u7ed3\u6784\u63d0\u5347\u8def\u5f84\u89c4\u5212\u6548\u7387\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u89c4\u5212\u548c\u5b50\u95ee\u9898\u5206\u89e3\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u8def\u5f84\u89c4\u5212\u3002", "motivation": "\u73b0\u67093D\u573a\u666f\u56fe\u7684\u7ed3\u6784\u672a\u88ab\u5145\u5206\u5229\u7528\u4ee5\u63d0\u5347\u8def\u5f84\u89c4\u5212\u7684\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "S-Path\u91c7\u7528\u4e24\u9636\u6bb5\u89c4\u5212\uff1a\u5148\u5728\u8bed\u4e49\u56fe\u4e0a\u641c\u7d22\u751f\u6210\u9ad8\u5c42\u8def\u5f84\uff0c\u518d\u5206\u89e3\u4e3a\u5e76\u884c\u89e3\u51b3\u7684\u5b50\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u91cd\u89c4\u5212\u673a\u5236\u4f18\u5316\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u663e\u793aS-Path\u5e73\u5747\u51cf\u5c115.7\u500d\u89c4\u5212\u65f6\u95f4\uff0c\u8def\u5f84\u6700\u4f18\u6027\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u590d\u6742\u573a\u666f\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "S-Path\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5ba4\u51853D\u573a\u666f\u56fe\u73af\u5883\u3002"}}
{"id": "2508.06291", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06291", "abs": "https://arxiv.org/abs/2508.06291", "authors": ["Christian Rauch", "Bj\u00f6rn Ellensohn", "Linus Nwankwo", "Vedant Dave", "Elmar Rueckert"], "title": "Real-Time 3D Vision-Language Embedding Mapping", "comment": null, "summary": "A metric-accurate semantic 3D representation is essential for many robotic\ntasks. This work proposes a simple, yet powerful, way to integrate the 2D\nembeddings of a Vision-Language Model in a metric-accurate 3D representation at\nreal-time. We combine a local embedding masking strategy, for a more distinct\nembedding distribution, with a confidence-weighted 3D integration for more\nreliable 3D embeddings. The resulting metric-accurate embedding representation\nis task-agnostic and can represent semantic concepts on a global multi-room, as\nwell as on a local object-level. This enables a variety of interactive robotic\napplications that require the localisation of objects-of-interest via natural\nlanguage. We evaluate our approach on a variety of real-world sequences and\ndemonstrate that these strategies achieve a more accurate object-of-interest\nlocalisation while improving the runtime performance in order to meet our\nreal-time constraints. We further demonstrate the versatility of our approach\nin a variety of interactive handheld, mobile robotics and manipulation tasks,\nrequiring only raw image data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c062D\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u5230\u5b9e\u65f6\u3001\u9ad8\u7cbe\u5ea63D\u8868\u793a\u4e2d\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u5c40\u90e8\u5d4c\u5165\u63a9\u853d\u548c\u7f6e\u4fe1\u5ea6\u52a0\u67433D\u96c6\u6210\uff0c\u5b9e\u73b0\u4efb\u52a1\u65e0\u5173\u7684\u8bed\u4e493D\u8868\u793a\u3002", "motivation": "\u4e3a\u673a\u5668\u4eba\u4efb\u52a1\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u7684\u8bed\u4e493D\u8868\u793a\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u3002", "method": "\u91c7\u7528\u5c40\u90e8\u5d4c\u5165\u63a9\u853d\u7b56\u7565\u548c\u7f6e\u4fe1\u5ea6\u52a0\u67433D\u96c6\u6210\u65b9\u6cd5\u3002", "result": "\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63d0\u9ad8\u4e86\u76ee\u6807\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u4ea4\u4e92\u5f0f\u673a\u5668\u4eba\u4efb\u52a1\uff0c\u4ec5\u9700\u539f\u59cb\u56fe\u50cf\u6570\u636e\u3002"}}
{"id": "2508.06295", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06295", "abs": "https://arxiv.org/abs/2508.06295", "authors": ["Juan Heredia", "Emil Stubbe Kolvig-Raun", "Sune Lundo Sorensen", "Mikkel Baun Kjaergaard"], "title": "Evaluating Robot Program Performance with Power Consumption Driven Metrics in Lightweight Industrial Robots", "comment": null, "summary": "The code performance of industrial robots is typically analyzed through CPU\nmetrics, which overlook the physical impact of code on robot behavior. This\nstudy introduces a novel framework for assessing robot program performance from\nan embodiment perspective by analyzing the robot's electrical power profile.\nOur approach diverges from conventional CPU based evaluations and instead\nleverages a suite of normalized metrics, namely, the energy utilization\ncoefficient, the energy conversion metric, and the reliability coefficient, to\ncapture how efficiently and reliably energy is used during task execution.\nComplementing these metrics, the established robot wear metric provides further\ninsight into long term reliability. Our approach is demonstrated through an\nexperimental case study in machine tending, comparing four programs with\ndiverse strategies using a UR5e robot. The proposed metrics directly compare\nand categorize different robot programs, regardless of the specific task, by\nlinking code performance to its physical manifestation through power\nconsumption patterns. Our results reveal the strengths and weaknesses of each\nstrategy, offering actionable insights for optimizing robot programming\npractices. Enhancing energy efficiency and reliability through this embodiment\ncentric approach not only improves individual robot performance but also\nsupports broader industrial objectives such as sustainable manufacturing and\ncost reduction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7535\u529b\u80fd\u8017\u7684\u673a\u5668\u4eba\u7a0b\u5e8f\u6027\u80fd\u8bc4\u4f30\u6846\u67b6\uff0c\u66ff\u4ee3\u4f20\u7edfCPU\u6307\u6807\uff0c\u901a\u8fc7\u80fd\u6548\u548c\u53ef\u9760\u6027\u7cfb\u6570\u4f18\u5316\u7f16\u7a0b\u5b9e\u8df5\u3002", "motivation": "\u4f20\u7edfCPU\u6307\u6807\u5ffd\u89c6\u4e86\u4ee3\u7801\u5bf9\u673a\u5668\u4eba\u7269\u7406\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u9700\u4ece\u4f53\u73b0\u89d2\u5ea6\u8bc4\u4f30\u7a0b\u5e8f\u6027\u80fd\u3002", "method": "\u5f15\u5165\u5f52\u4e00\u5316\u6307\u6807\uff08\u5982\u80fd\u6548\u5229\u7528\u7cfb\u6570\u3001\u80fd\u91cf\u8f6c\u6362\u6307\u6807\u548c\u53ef\u9760\u6027\u7cfb\u6570\uff09\uff0c\u7ed3\u5408\u673a\u5668\u4eba\u78e8\u635f\u6307\u6807\uff0c\u5206\u6790\u7535\u529b\u80fd\u8017\u6a21\u5f0f\u3002", "result": "\u901a\u8fc7UR5e\u673a\u5668\u4eba\u5b9e\u9a8c\u6bd4\u8f83\u56db\u79cd\u7a0b\u5e8f\u7b56\u7565\uff0c\u63ed\u793a\u4e86\u5404\u7b56\u7565\u7684\u4f18\u7f3a\u70b9\uff0c\u4e3a\u4f18\u5316\u7f16\u7a0b\u63d0\u4f9b\u4f9d\u636e\u3002", "conclusion": "\u57fa\u4e8e\u80fd\u8017\u7684\u8bc4\u4f30\u65b9\u6cd5\u63d0\u5347\u4e86\u80fd\u6548\u548c\u53ef\u9760\u6027\uff0c\u652f\u6301\u53ef\u6301\u7eed\u5236\u9020\u548c\u6210\u672c\u964d\u4f4e\u7684\u5de5\u4e1a\u76ee\u6807\u3002"}}
{"id": "2508.06313", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06313", "abs": "https://arxiv.org/abs/2508.06313", "authors": ["Amir Hossein Barjini", "Mohammad Bahari", "Mahdi Hejrati", "Jouni Mattila"], "title": "Surrogate-Enhanced Modeling and Adaptive Modular Control of All-Electric Heavy-Duty Robotic Manipulators", "comment": "This is submitted to IEEE T-ASE", "summary": "This paper presents a unified system-level modeling and control framework for\nan all-electric heavy-duty robotic manipulator (HDRM) driven by\nelectromechanical linear actuators (EMLAs). A surrogate-enhanced actuator\nmodel, combining integrated electromechanical dynamics with a neural network\ntrained on a dedicated testbed, is integrated into an extended virtual\ndecomposition control (VDC) architecture augmented by a natural adaptation law.\nThe derived analytical HDRM model supports a hierarchical control structure\nthat seamlessly maps high-level force and velocity objectives to real-time\nactuator commands, accompanied by a Lyapunov-based stability proof. In\nmulti-domain simulations of both cubic and a custom planar triangular\ntrajectory, the proposed adaptive modular controller achieves sub-centimeter\nCartesian tracking accuracy. Experimental validation of the same 1-DoF platform\nunder realistic load emulation confirms the efficacy of the proposed control\nstrategy. These findings demonstrate that a surrogate-enhanced EMLA model\nembedded in the VDC approach can enable modular, real-time control of an\nall-electric HDRM, supporting its deployment in next-generation mobile working\nmachines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5168\u7535\u52a8\u91cd\u578b\u673a\u5668\u4eba\u64cd\u7eb5\u5668\u7684\u7edf\u4e00\u7cfb\u7edf\u7ea7\u5efa\u6a21\u4e0e\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u7684\u865a\u62df\u5206\u89e3\u63a7\u5236\u67b6\u6784\u548c\u81ea\u7136\u9002\u5e94\u5f8b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8ddf\u8e2a\u3002", "motivation": "\u4e3a\u5168\u7535\u52a8\u91cd\u578b\u673a\u5668\u4eba\u64cd\u7eb5\u5668\u5f00\u53d1\u4e00\u79cd\u6a21\u5757\u5316\u3001\u5b9e\u65f6\u63a7\u5236\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u5176\u5728\u4e0b\u4e00\u4ee3\u79fb\u52a8\u5de5\u4f5c\u673a\u5668\u4e2d\u7684\u90e8\u7f72\u3002", "method": "\u7ed3\u5408\u7535\u673a\u68b0\u7ebf\u6027\u9a71\u52a8\u5668\u7684\u4ee3\u7406\u589e\u5f3a\u6a21\u578b\u4e0e\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\uff0c\u96c6\u6210\u5230\u589e\u5f3a\u7684\u865a\u62df\u5206\u89e3\u63a7\u5236\u67b6\u6784\u4e2d\uff0c\u5e76\u901a\u8fc7\u674e\u96c5\u666e\u8bfa\u592b\u7a33\u5b9a\u6027\u8bc1\u660e\u652f\u6301\u5206\u5c42\u63a7\u5236\u7ed3\u6784\u3002", "result": "\u5728\u591a\u57df\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e2d\uff0c\u63a7\u5236\u5668\u5b9e\u73b0\u4e86\u4e9a\u5398\u7c73\u7ea7\u7684\u7b1b\u5361\u5c14\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "conclusion": "\u4ee3\u7406\u589e\u5f3a\u7684\u7535\u673a\u68b0\u7ebf\u6027\u9a71\u52a8\u5668\u6a21\u578b\u5d4c\u5165\u865a\u62df\u5206\u89e3\u63a7\u5236\u65b9\u6cd5\uff0c\u53ef\u5b9e\u73b0\u5168\u7535\u52a8\u91cd\u578b\u673a\u5668\u4eba\u64cd\u7eb5\u5668\u7684\u6a21\u5757\u5316\u5b9e\u65f6\u63a7\u5236\u3002"}}
{"id": "2508.06319", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06319", "abs": "https://arxiv.org/abs/2508.06319", "authors": ["Sagar Parekh", "Heramb Nemlekar", "Dylan P. Losey"], "title": "Towards Balanced Behavior Cloning from Imbalanced Datasets", "comment": null, "summary": "Robots should be able to learn complex behaviors from human demonstrations.\nIn practice, these human-provided datasets are inevitably imbalanced: i.e., the\nhuman demonstrates some subtasks more frequently than others. State-of-the-art\nmethods default to treating each element of the human's dataset as equally\nimportant. So if -- for instance -- the majority of the human's data focuses on\nreaching a goal, and only a few state-action pairs move to avoid an obstacle,\nthe learning algorithm will place greater emphasis on goal reaching. More\ngenerally, misalignment between the relative amounts of data and the importance\nof that data causes fundamental problems for imitation learning approaches. In\nthis paper we analyze and develop learning methods that automatically account\nfor mixed datasets. We formally prove that imbalanced data leads to imbalanced\npolicies when each state-action pair is weighted equally; these policies\nemulate the most represented behaviors, and not the human's complex, multi-task\ndemonstrations. We next explore algorithms that rebalance offline datasets\n(i.e., reweight the importance of different state-action pairs) without human\noversight. Reweighting the dataset can enhance the overall policy performance.\nHowever, there is no free lunch: each method for autonomously rebalancing\nbrings its own pros and cons. We formulate these advantages and disadvantages,\nhelping other researchers identify when each type of approach is most\nappropriate. We conclude by introducing a novel meta-gradient rebalancing\nalgorithm that addresses the primary limitations behind existing approaches.\nOur experiments show that dataset rebalancing leads to better downstream\nlearning, improving the performance of general imitation learning algorithms\nwithout requiring additional data collection. See our project website:\nhttps://collab.me.vt.edu/data_curation/.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u6a21\u4eff\u5b66\u4e60\u4e2d\u6570\u636e\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u81ea\u52a8\u91cd\u65b0\u5e73\u8861\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u5143\u68af\u5ea6\u7b97\u6cd5\u4ee5\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u4eba\u7c7b\u6f14\u793a\u7684\u6570\u636e\u96c6\u901a\u5e38\u4e0d\u5e73\u8861\uff0c\u5bfc\u81f4\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\u8fc7\u5ea6\u5173\u6ce8\u9ad8\u9891\u5b50\u4efb\u52a1\uff0c\u5ffd\u89c6\u4f4e\u9891\u4f46\u91cd\u8981\u7684\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u7b97\u6cd5\u91cd\u65b0\u5e73\u8861\u79bb\u7ebf\u6570\u636e\u96c6\uff0c\u8c03\u6574\u4e0d\u540c\u72b6\u6001-\u52a8\u4f5c\u5bf9\u7684\u6743\u91cd\uff0c\u5e76\u5f15\u5165\u5143\u68af\u5ea6\u91cd\u65b0\u5e73\u8861\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6570\u636e\u96c6\u91cd\u65b0\u5e73\u8861\u80fd\u63d0\u5347\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6536\u96c6\u3002", "conclusion": "\u91cd\u65b0\u5e73\u8861\u6570\u636e\u96c6\u662f\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u4e2d\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u65b0\u7b97\u6cd5\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.06330", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06330", "abs": "https://arxiv.org/abs/2508.06330", "authors": ["Baorun Li", "Chengrui Zhu", "Siyi Du", "Bingran Chen", "Jie Ren", "Wenfei Wang", "Yong Liu", "Jiajun Lv"], "title": "L2Calib: $SE(3)$-Manifold Reinforcement Learning for Robust Extrinsic Calibration with Degenerate Motion Resilience", "comment": "IROS2025", "summary": "Extrinsic calibration is essential for multi-sensor fusion, existing methods\nrely on structured targets or fully-excited data, limiting real-world\napplicability. Online calibration further suffers from weak excitation, leading\nto unreliable estimates. To address these limitations, we propose a\nreinforcement learning (RL)-based extrinsic calibration framework that\nformulates extrinsic calibration as a decision-making problem, directly\noptimizes $SE(3)$ extrinsics to enhance odometry accuracy. Our approach\nleverages a probabilistic Bingham distribution to model 3D rotations, ensuring\nstable optimization while inherently retaining quaternion symmetry. A\ntrajectory alignment reward mechanism enables robust calibration without\nstructured targets by quantitatively evaluating estimated tightly-coupled\ntrajectory against a reference trajectory. Additionally, an automated data\nselection module filters uninformative samples, significantly improving\nefficiency and scalability for large-scale datasets. Extensive experiments on\nUAVs, UGVs, and handheld platforms demonstrate that our method outperforms\ntraditional optimization-based approaches, achieving high-precision calibration\neven under weak excitation conditions. Our framework simplifies deployment on\ndiverse robotic platforms by eliminating the need for high-quality initial\nextrinsics and enabling calibration from routine operating data. The code is\navailable at https://github.com/APRIL-ZJU/learn-to-calibrate.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5916\u53c2\u6807\u5b9a\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u4f18\u5316SE(3)\u5916\u53c2\u63d0\u5347\u91cc\u7a0b\u8ba1\u7cbe\u5ea6\uff0c\u65e0\u9700\u7ed3\u6784\u5316\u76ee\u6807\u6216\u9ad8\u8d28\u91cf\u521d\u59cb\u5916\u53c2\u3002", "motivation": "\u73b0\u6709\u5916\u53c2\u6807\u5b9a\u65b9\u6cd5\u4f9d\u8d56\u7ed3\u6784\u5316\u76ee\u6807\u6216\u5b8c\u5168\u6fc0\u52b1\u6570\u636e\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff1b\u5728\u7ebf\u6807\u5b9a\u5728\u5f31\u6fc0\u52b1\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5c06\u5916\u53c2\u6807\u5b9a\u5efa\u6a21\u4e3a\u51b3\u7b56\u95ee\u9898\uff0c\u4f7f\u7528Bingham\u5206\u5e03\u5efa\u6a213D\u65cb\u8f6c\uff0c\u7ed3\u5408\u8f68\u8ff9\u5bf9\u9f50\u5956\u52b1\u673a\u5236\u548c\u81ea\u52a8\u6570\u636e\u9009\u62e9\u6a21\u5757\u3002", "result": "\u5728\u65e0\u4eba\u673a\u3001\u65e0\u4eba\u8f66\u548c\u624b\u6301\u5e73\u53f0\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5f31\u6fc0\u52b1\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u7b80\u5316\u4e86\u591a\u673a\u5668\u4eba\u5e73\u53f0\u7684\u5916\u53c2\u6807\u5b9a\u90e8\u7f72\uff0c\u652f\u6301\u4ece\u5e38\u89c4\u64cd\u4f5c\u6570\u636e\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6807\u5b9a\u3002"}}
{"id": "2508.06404", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.06404", "abs": "https://arxiv.org/abs/2508.06404", "authors": ["Abdullah Zareh Andaryan", "Michael G. H. Bell", "Mohsen Ramezani", "Glenn Geers"], "title": "V*: An Efficient Motion Planning Algorithm for Autonomous Vehicles", "comment": null, "summary": "Autonomous vehicle navigation in structured environments requires planners\ncapable of generating time-optimal, collision-free trajectories that satisfy\ndynamic and kinematic constraints. We introduce V*, a graph-based motion\nplanner that represents speed and direction as explicit state variables within\na discretised space-time-velocity lattice. Unlike traditional methods that\ndecouple spatial search from dynamic feasibility or rely on post-hoc smoothing,\nV* integrates both motion dimensions directly into graph construction through\ndynamic graph generation during search expansion. To manage the complexity of\nhigh-dimensional search, we employ a hexagonal discretisation strategy and\nprovide formal mathematical proofs establishing optimal waypoint spacing and\nminimal node redundancy under constrained heading transitions for\nvelocity-aware motion planning. We develop a mathematical formulation for\ntransient steering dynamics in the kinematic bicycle model, modelling steering\nangle convergence with exponential behaviour, and deriving the relationship for\nconvergence rate parameters. This theoretical foundation, combined with\ngeometric pruning strategies that eliminate expansions leading to infeasible\nsteering configurations, enables V* to evaluate dynamically admissible\nmanoeuvres, ensuring each trajectory is physically realisable without further\nrefinement. We further demonstrate V*'s performance in simulation studies with\ncluttered and dynamic environments involving moving obstacles, showing its\nability to avoid conflicts, yield proactively, and generate safe, efficient\ntrajectories with temporal reasoning capabilities for waiting behaviours and\ndynamic coordination.", "AI": {"tldr": "V*\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5728\u79bb\u6563\u7684\u65f6\u7a7a\u901f\u5ea6\u7f51\u683c\u4e2d\u663e\u5f0f\u8868\u793a\u901f\u5ea6\u548c\u65b9\u5411\uff0c\u76f4\u63a5\u96c6\u6210\u52a8\u6001\u53ef\u884c\u6027\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u89e3\u8026\u6216\u540e\u5904\u7406\u5e73\u6ed1\u3002", "motivation": "\u5728\u7ed3\u6784\u5316\u73af\u5883\u4e2d\uff0c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9700\u8981\u751f\u6210\u65f6\u95f4\u6700\u4f18\u3001\u65e0\u78b0\u649e\u4e14\u6ee1\u8db3\u52a8\u6001\u548c\u8fd0\u52a8\u5b66\u7ea6\u675f\u7684\u8f68\u8ff9\u3002", "method": "V*\u91c7\u7528\u516d\u8fb9\u5f62\u79bb\u6563\u5316\u7b56\u7565\uff0c\u52a8\u6001\u751f\u6210\u56fe\uff0c\u7ed3\u5408\u51e0\u4f55\u526a\u679d\u548c\u6570\u5b66\u5efa\u6a21\uff08\u5982\u8f6c\u5411\u52a8\u529b\u5b66\u7684\u6307\u6570\u6536\u655b\uff09\uff0c\u786e\u4fdd\u52a8\u6001\u53ef\u884c\u6027\u3002", "result": "\u4eff\u771f\u7814\u7a76\u8868\u660e\uff0cV*\u80fd\u591f\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u751f\u6210\u5b89\u5168\u9ad8\u6548\u7684\u8f68\u8ff9\uff0c\u5177\u5907\u4e3b\u52a8\u907f\u969c\u548c\u52a8\u6001\u534f\u8c03\u80fd\u529b\u3002", "conclusion": "V*\u901a\u8fc7\u76f4\u63a5\u96c6\u6210\u52a8\u6001\u7ea6\u675f\u548c\u9ad8\u7ef4\u641c\u7d22\u4f18\u5316\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u540e\u5904\u7406\u7684\u52a8\u6001\u53ef\u884c\u8fd0\u52a8\u89c4\u5212\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06426", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06426", "abs": "https://arxiv.org/abs/2508.06426", "authors": ["Youguang Xing", "Xu Luo", "Junlin Xie", "Lianli Gao", "Hengtao Shen", "Jingkuan Song"], "title": "Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation", "comment": "CoRL 2025", "summary": "Generalist robot policies trained on large-scale datasets such as Open\nX-Embodiment (OXE) demonstrate strong performance across a wide range of tasks.\nHowever, they often struggle to generalize beyond the distribution of their\ntraining data. In this paper, we investigate the underlying cause of this\nlimited generalization capability. We identify shortcut learning -- the\nreliance on task-irrelevant features -- as a key impediment to generalization.\nThrough comprehensive theoretical and empirical analysis, we uncover two\nprimary contributors to shortcut learning: (1) limited diversity within\nindividual sub-datasets, and (2) significant distributional disparities across\nsub-datasets, leading to dataset fragmentation. These issues arise from the\ninherent structure of large-scale datasets like OXE, which are typically\ncomposed of multiple sub-datasets collected independently across varied\nenvironments and embodiments. Our findings provide critical insights into\ndataset collection strategies that can reduce shortcut learning and enhance the\ngeneralization ability of generalist robot policies. Moreover, in scenarios\nwhere acquiring new large-scale data is impractical, we demonstrate that\ncarefully selected robotic data augmentation strategies can effectively reduce\nshortcut learning in existing offline datasets, thereby improving\ngeneralization capabilities of generalist robot policies, e.g., $\\pi_0$, in\nboth simulation and real-world environments. More information at\nhttps://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u5728\u5927\u578b\u6570\u636e\u96c6\uff08\u5982OXE\uff09\u4e0a\u8bad\u7ec3\u65f6\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u7684\u539f\u56e0\uff0c\u53d1\u73b0\u4efb\u52a1\u65e0\u5173\u7279\u5f81\u7684\u4f9d\u8d56\uff08\u6377\u5f84\u5b66\u4e60\uff09\u662f\u4e3b\u8981\u969c\u788d\uff0c\u5e76\u63d0\u51fa\u6570\u636e\u96c6\u6536\u96c6\u548c\u6570\u636e\u589e\u5f3a\u7b56\u7565\u4ee5\u6539\u5584\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u5728\u5927\u578b\u6570\u636e\u96c6\u8bad\u7ec3\u540e\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u6839\u672c\u539f\u56e0\uff0c\u4ee5\u63d0\u5347\u5176\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\uff0c\u8bc6\u522b\u6377\u5f84\u5b66\u4e60\u7684\u4e24\u4e2a\u4e3b\u8981\u6765\u6e90\uff1a\u5b50\u6570\u636e\u96c6\u5185\u90e8\u591a\u6837\u6027\u4e0d\u8db3\u548c\u5b50\u6570\u636e\u96c6\u95f4\u7684\u5206\u5e03\u5dee\u5f02\u3002\u63d0\u51fa\u6570\u636e\u96c6\u6536\u96c6\u7b56\u7565\u548c\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u4ee5\u51cf\u5c11\u6377\u5f84\u5b66\u4e60\u3002", "result": "\u53d1\u73b0\u6377\u5f84\u5b66\u4e60\u662f\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u7b56\u7565\uff0c\u5305\u62ec\u4f18\u5316\u6570\u636e\u96c6\u6536\u96c6\u548c\u91c7\u7528\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u6570\u636e\u96c6\u7ed3\u6784\u548c\u6570\u636e\u589e\u5f3a\uff0c\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u6377\u5f84\u5b66\u4e60\uff0c\u63d0\u5347\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
