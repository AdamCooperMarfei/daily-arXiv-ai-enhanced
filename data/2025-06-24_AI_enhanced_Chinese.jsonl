{"id": "2506.17328", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17328", "abs": "https://arxiv.org/abs/2506.17328", "authors": ["Yufan Liu", "Yi Wu", "Gweneth Ge", "Haoliang Cheng", "Rui Liu"], "title": "Reflective VLM Planning for Dual-Arm Desktop Cleaning: Bridging Open-Vocabulary Perception and Precise Manipulation", "comment": null, "summary": "Desktop cleaning demands open-vocabulary recognition and precise manipulation\nfor heterogeneous debris. We propose a hierarchical framework integrating\nreflective Vision-Language Model (VLM) planning with dual-arm execution via\nstructured scene representation. Grounded-SAM2 facilitates open-vocabulary\ndetection, while a memory-augmented VLM generates, critiques, and revises\nmanipulation sequences. These sequences are converted into parametric\ntrajectories for five primitives executed by coordinated Franka arms. Evaluated\nin simulated scenarios, our system achieving 87.2% task completion, a 28.8%\nimprovement over static VLM and 36.2% over single-arm baselines. Structured\nmemory integration proves crucial for robust, generalizable manipulation while\nmaintaining real-time control performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u89c4\u5212\u548c\u53cc\u81c2\u6267\u884c\u7684\u5c42\u6b21\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u684c\u9762\u6e05\u7406\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u5b8c\u6210\u7387\u3002", "motivation": "\u89e3\u51b3\u684c\u9762\u6e05\u7406\u4e2d\u5f00\u653e\u8bcd\u6c47\u8bc6\u522b\u548c\u7cbe\u786e\u64cd\u4f5c\u5f02\u8d28\u788e\u7247\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528Grounded-SAM2\u8fdb\u884c\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\uff0c\u5e76\u901a\u8fc7\u8bb0\u5fc6\u589e\u5f3a\u7684VLM\u751f\u6210\u3001\u8bc4\u4f30\u548c\u4fee\u8ba2\u64cd\u4f5c\u5e8f\u5217\uff0c\u8f6c\u6362\u4e3a\u53c2\u6570\u5316\u8f68\u8ff9\u7531\u53ccFranka\u81c2\u6267\u884c\u3002", "result": "\u5728\u6a21\u62df\u573a\u666f\u4e2d\uff0c\u7cfb\u7edf\u4efb\u52a1\u5b8c\u6210\u7387\u8fbe\u523087.2%\uff0c\u6bd4\u9759\u6001VLM\u63d0\u9ad828.8%\uff0c\u6bd4\u5355\u81c2\u57fa\u7ebf\u63d0\u9ad836.2%\u3002", "conclusion": "\u7ed3\u6784\u5316\u8bb0\u5fc6\u96c6\u6210\u5bf9\u9c81\u68d2\u3001\u53ef\u6cdb\u5316\u7684\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u63a7\u5236\u6027\u80fd\u3002"}}
{"id": "2506.17378", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17378", "abs": "https://arxiv.org/abs/2506.17378", "authors": ["Abhishek Phadke", "Shakib Mahmud Dipto", "Pratip Rana"], "title": "A workflow for generating synthetic LiDAR datasets in simulation environments", "comment": null, "summary": "This paper presents a simulation workflow for generating synthetic LiDAR\ndatasets to support autonomous vehicle perception, robotics research, and\nsensor security analysis. Leveraging the CoppeliaSim simulation environment and\nits Python API, we integrate time-of-flight LiDAR, image sensors, and two\ndimensional scanners onto a simulated vehicle platform operating within an\nurban scenario. The workflow automates data capture, storage, and annotation\nacross multiple formats (PCD, PLY, CSV), producing synchronized multimodal\ndatasets with ground truth pose information. We validate the pipeline by\ngenerating large-scale point clouds and corresponding RGB and depth imagery.\nThe study examines potential security vulnerabilities in LiDAR data, such as\nadversarial point injection and spoofing attacks, and demonstrates how\nsynthetic datasets can facilitate the evaluation of defense strategies.\nFinally, limitations related to environmental realism, sensor noise modeling,\nand computational scalability are discussed, and future research directions,\nsuch as incorporating weather effects, real-world terrain models, and advanced\nscanner configurations, are proposed. The workflow provides a versatile,\nreproducible framework for generating high-fidelity synthetic LiDAR datasets to\nadvance perception research and strengthen sensor security in autonomous\nsystems. Documentation and examples accompany this framework; samples of\nanimated cloud returns and image sensor data can be found at this Link.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5408\u6210LiDAR\u6570\u636e\u7684\u4eff\u771f\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u3001\u673a\u5668\u4eba\u7814\u7a76\u548c\u4f20\u611f\u5668\u5b89\u5168\u5206\u6790\u3002", "motivation": "\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u548c\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u9ad8\u8d28\u91cf\u3001\u591a\u6a21\u6001\u7684\u5408\u6210\u6570\u636e\uff0c\u540c\u65f6\u5206\u6790LiDAR\u6570\u636e\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u5229\u7528CoppeliaSim\u4eff\u771f\u73af\u5883\u548cPython API\uff0c\u96c6\u6210LiDAR\u3001\u56fe\u50cf\u4f20\u611f\u5668\u548c\u4e8c\u7ef4\u626b\u63cf\u4eea\uff0c\u81ea\u52a8\u5316\u6570\u636e\u6355\u83b7\u3001\u5b58\u50a8\u548c\u6807\u6ce8\u3002", "result": "\u751f\u6210\u4e86\u5927\u89c4\u6a21\u70b9\u4e91\u53ca\u540c\u6b65\u7684RGB\u548c\u6df1\u5ea6\u56fe\u50cf\uff0c\u9a8c\u8bc1\u4e86\u5de5\u4f5c\u6d41\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5408\u6210\u6570\u636e\u5728\u5b89\u5168\u6f0f\u6d1e\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6d41\u4e3a\u751f\u6210\u9ad8\u4fdd\u771f\u5408\u6210LiDAR\u6570\u636e\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u53ef\u590d\u73b0\u7684\u6846\u67b6\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u5929\u6c14\u6548\u679c\u548c\u771f\u5b9e\u5730\u5f62\u6a21\u578b\u3002"}}
{"id": "2506.17458", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17458", "abs": "https://arxiv.org/abs/2506.17458", "authors": ["Abhay Negi", "Omey M. Manyar", "Satyandra K. Gupta"], "title": "Kinematic Model Optimization via Differentiable Contact Manifold for In-Space Manipulation", "comment": "Accepted and presented in RSS 2025 Space Robotics Workshop\n  (https://albee.github.io/space-robotics-rss/). 3 pages with 1 figure", "summary": "Robotic manipulation in space is essential for emerging applications such as\ndebris removal and in-space servicing, assembly, and manufacturing (ISAM). A\nkey requirement for these tasks is the ability to perform precise, contact-rich\nmanipulation under significant uncertainty. In particular, thermal-induced\ndeformation of manipulator links and temperature-dependent encoder bias\nintroduce kinematic parameter errors that significantly degrade end-effector\naccuracy. Traditional calibration techniques rely on external sensors or\ndedicated calibration procedures, which can be infeasible or risky in dynamic,\nspace-based operational scenarios.\n  This paper proposes a novel method for kinematic parameter estimation that\nonly requires encoder measurements and binary contact detection. The approach\nfocuses on estimating link thermal deformation strain and joint encoder biases\nby leveraging information of the contact manifold - the set of relative SE(3)\nposes at which contact between the manipulator and environment occurs. We\npresent two core contributions: (1) a differentiable, learning-based model of\nthe contact manifold, and (2) an optimization-based algorithm for estimating\nkinematic parameters from encoder measurements at contact instances. By\nenabling parameter estimation using only encoder measurements and contact\ndetection, this method provides a robust, interpretable, and data-efficient\nsolution for safe and accurate manipulation in the challenging conditions of\nspace.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u9700\u7f16\u7801\u5668\u6d4b\u91cf\u548c\u63a5\u89e6\u68c0\u6d4b\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u53c2\u6570\u4f30\u8ba1\u65b0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u592a\u7a7a\u73af\u5883\u4e2d\u7684\u7cbe\u786e\u64cd\u4f5c\u3002", "motivation": "\u592a\u7a7a\u673a\u5668\u4eba\u64cd\u4f5c\uff08\u5982\u788e\u7247\u6e05\u9664\u548c\u5236\u9020\uff09\u9700\u8981\u9ad8\u7cbe\u5ea6\u63a5\u89e6\u64cd\u4f5c\uff0c\u4f46\u70ed\u53d8\u5f62\u548c\u7f16\u7801\u5668\u504f\u5dee\u5bfc\u81f4\u8fd0\u52a8\u5b66\u53c2\u6570\u8bef\u5dee\uff0c\u4f20\u7edf\u6821\u51c6\u65b9\u6cd5\u5728\u592a\u7a7a\u73af\u5883\u4e2d\u4e0d\u53ef\u884c\u3002", "method": "\u5229\u7528\u63a5\u89e6\u6d41\u5f62\u4fe1\u606f\uff0c\u63d0\u51fa\u57fa\u4e8e\u5b66\u4e60\u7684\u53ef\u5fae\u5206\u63a5\u89e6\u6d41\u5f62\u6a21\u578b\u548c\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u7f16\u7801\u5668\u6d4b\u91cf\u548c\u63a5\u89e6\u5b9e\u4f8b\u4f30\u8ba1\u53c2\u6570\u3002", "result": "\u8be5\u65b9\u6cd5\u4ec5\u9700\u7f16\u7801\u5668\u6570\u636e\u548c\u63a5\u89e6\u68c0\u6d4b\u5373\u53ef\u4f30\u8ba1\u53c2\u6570\uff0c\u4e3a\u592a\u7a7a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u3001\u7cbe\u786e\u64cd\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u592a\u7a7a\u73af\u5883\u4e2d\u5177\u6709\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6570\u636e\u6548\u7387\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u7684\u64cd\u4f5c\u573a\u666f\u3002"}}
{"id": "2506.17462", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17462", "abs": "https://arxiv.org/abs/2506.17462", "authors": ["Bernard Lange", "Anil Yildiz", "Mansur Arief", "Shehryar Khattak", "Mykel Kochenderfer", "Georgios Georgakis"], "title": "General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting", "comment": null, "summary": "Developing general-purpose navigation policies for unknown environments\nremains a core challenge in robotics. Most existing systems rely on\ntask-specific neural networks and fixed data flows, limiting generalizability.\nLarge Vision-Language Models (LVLMs) offer a promising alternative by embedding\nhuman-like knowledge suitable for reasoning and planning. Yet, prior LVLM-robot\nintegrations typically depend on pre-mapped spaces, hard-coded representations,\nand myopic exploration. We introduce the Agentic Robotic Navigation\nArchitecture (ARNA), a general-purpose navigation framework that equips an\nLVLM-based agent with a library of perception, reasoning, and navigation tools\navailable within modern robotic stacks. At runtime, the agent autonomously\ndefines and executes task-specific workflows that iteratively query the robotic\nmodules, reason over multimodal inputs, and select appropriate navigation\nactions. This approach enables robust navigation and reasoning in previously\nunmapped environments, providing a new perspective on robotic stack design.\nEvaluated in Habitat Lab on the HM-EQA benchmark, ARNA achieves\nstate-of-the-art performance, demonstrating effective exploration, navigation,\nand embodied question answering without relying on handcrafted plans, fixed\ninput representations, or pre-existing maps.", "AI": {"tldr": "ARNA\u662f\u4e00\u4e2a\u57fa\u4e8eLVLM\u7684\u901a\u7528\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u4e3b\u5b9a\u4e49\u4efb\u52a1\u5de5\u4f5c\u6d41\uff0c\u5b9e\u73b0\u672a\u77e5\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u5bfc\u822a\u548c\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u5bfc\u822a\u7cfb\u7edf\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u795e\u7ecf\u7f51\u7edc\u548c\u56fa\u5b9a\u6570\u636e\u6d41\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002LVLM\u63d0\u4f9b\u4eba\u7c7b\u77e5\u8bc6\u5d4c\u5165\uff0c\u4f46\u73b0\u6709\u96c6\u6210\u4f9d\u8d56\u9884\u6620\u5c04\u7a7a\u95f4\u548c\u786c\u7f16\u7801\u8868\u793a\u3002", "method": "ARNA\u7ed3\u5408\u611f\u77e5\u3001\u63a8\u7406\u548c\u5bfc\u822a\u5de5\u5177\u5e93\uff0c\u8fd0\u884c\u65f6\u81ea\u4e3b\u5b9a\u4e49\u4efb\u52a1\u5de5\u4f5c\u6d41\uff0c\u8fed\u4ee3\u67e5\u8be2\u673a\u5668\u4eba\u6a21\u5757\u5e76\u9009\u62e9\u5bfc\u822a\u52a8\u4f5c\u3002", "result": "\u5728HM-EQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cARNA\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u65e0\u9700\u624b\u5de5\u8ba1\u5212\u6216\u9884\u5efa\u5730\u56fe\u3002", "conclusion": "ARNA\u4e3a\u673a\u5668\u4eba\u6808\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u89c6\u89d2\uff0c\u5c55\u793a\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u63a2\u7d22\u548c\u5bfc\u822a\u80fd\u529b\u3002"}}
{"id": "2506.17473", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17473", "abs": "https://arxiv.org/abs/2506.17473", "authors": ["Shuyuan Wang", "Philip D. Loewen", "Michael Forbes", "Bhushan Gopaluni", "Wei Pan"], "title": "DiLQR: Differentiable Iterative Linear Quadratic Regulator via Implicit Differentiation", "comment": "Accepted at ICML 2025. Official conference page:\n  https://icml.cc/virtual/2025/poster/44176. OpenReview page:\n  https://openreview.net/forum?id=m2EfTrbv4o", "summary": "While differentiable control has emerged as a powerful paradigm combining\nmodel-free flexibility with model-based efficiency, the iterative Linear\nQuadratic Regulator (iLQR) remains underexplored as a differentiable component.\nThe scalability of differentiating through extended iterations and horizons\nposes significant challenges, hindering iLQR from being an effective\ndifferentiable controller. This paper introduces DiLQR, a framework that\nfacilitates differentiation through iLQR, allowing it to serve as a trainable\nand differentiable module, either as or within a neural network. A novel aspect\nof this framework is the analytical solution that it provides for the gradient\nof an iLQR controller through implicit differentiation, which ensures a\nconstant backward cost regardless of iteration, while producing an accurate\ngradient. We evaluate our framework on imitation tasks on famous control\nbenchmarks. Our analytical method demonstrates superior computational\nperformance, achieving up to 128x speedup and a minimum of 21x speedup compared\nto automatic differentiation. Our method also demonstrates superior learning\nperformance ($10^6$x) compared to traditional neural network policies and\nbetter model loss with differentiable controllers that lack exact analytical\ngradients. Furthermore, we integrate our module into a larger network with\nvisual inputs to demonstrate the capacity of our method for high-dimensional,\nfully end-to-end tasks. Codes can be found on the project homepage\nhttps://sites.google.com/view/dilqr/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDiLQR\u6846\u67b6\uff0c\u4f7fiLQR\u6210\u4e3a\u53ef\u5fae\u5206\u7684\u63a7\u5236\u5668\u6a21\u5757\uff0c\u901a\u8fc7\u9690\u5f0f\u5fae\u5206\u63d0\u4f9b\u68af\u5ea6\u89e3\u6790\u89e3\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u548c\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "iLQR\u4f5c\u4e3a\u53ef\u5fae\u5206\u7ec4\u4ef6\u7684\u6f5c\u529b\u672a\u88ab\u5145\u5206\u6316\u6398\uff0c\u6269\u5c55\u6027\u548c\u8fed\u4ee3\u68af\u5ea6\u8ba1\u7b97\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u5f15\u5165DiLQR\u6846\u67b6\uff0c\u901a\u8fc7\u9690\u5f0f\u5fae\u5206\u63d0\u4f9biLQR\u63a7\u5236\u5668\u7684\u68af\u5ea6\u89e3\u6790\u89e3\uff0c\u786e\u4fdd\u6052\u5b9a\u53cd\u5411\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u6a21\u4eff\u4efb\u52a1\u4e2d\uff0cDiLQR\u8ba1\u7b97\u901f\u5ea6\u63d0\u534721x-128x\uff0c\u5b66\u4e60\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\u548c\u7f3a\u4e4f\u7cbe\u786e\u68af\u5ea6\u7684\u63a7\u5236\u5668\u3002", "conclusion": "DiLQR\u6210\u529f\u5c06iLQR\u8f6c\u5316\u4e3a\u9ad8\u6548\u53ef\u5fae\u5206\u6a21\u5757\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u7aef\u5230\u7aef\u4efb\u52a1\u3002"}}
{"id": "2506.17486", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17486", "abs": "https://arxiv.org/abs/2506.17486", "authors": ["Zachary Ravichandran", "Ignacio Hounie", "Fernando Cladera", "Alejandro Ribeiro", "George J. Pappas", "Vijay Kumar"], "title": "Distilling On-device Language Models for Robot Planning with Minimal Human Intervention", "comment": null, "summary": "Large language models (LLMs) provide robots with powerful contextual\nreasoning abilities and a natural human interface. Yet, current LLM-enabled\nrobots typically depend on cloud-hosted models, limiting their usability in\nenvironments with unreliable communication infrastructure, such as outdoor or\nindustrial settings. We present PRISM, a framework for distilling small\nlanguage model (SLM)-enabled robot planners that run on-device with minimal\nhuman supervision. Starting from an existing LLM-enabled planner, PRISM\nautomatically synthesizes diverse tasks and environments, elicits plans from\nthe LLM, and uses this synthetic dataset to distill a compact SLM as a drop-in\nreplacement of the source model. We apply PRISM to three LLM-enabled planners\nfor mapping and exploration, manipulation, and household assistance, and we\ndemonstrate that PRISM improves the performance of Llama-3.2-3B from 10-20% of\nGPT-4o's performance to over 93% - using only synthetic data. We further\ndemonstrate that the distilled planners generalize across heterogeneous robotic\nplatforms (ground and aerial) and diverse environments (indoor and outdoor). We\nrelease all software, trained models, and datasets at\nhttps://zacravichandran.github.io/PRISM.", "AI": {"tldr": "PRISM\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u5408\u6210\u4efb\u52a1\u548c\u73af\u5883\uff0c\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u63d0\u53d6\u8ba1\u5212\uff0c\u5e76\u84b8\u998f\u51fa\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\uff0c\u4ee5\u5728\u8bbe\u5907\u4e0a\u8fd0\u884c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u4f9d\u8d56\u4e91\u7aefLLM\u7684\u673a\u5668\u4eba\u5728\u901a\u4fe1\u4e0d\u53ef\u9760\u7684\u73af\u5883\u4e2d\uff08\u5982\u6237\u5916\u6216\u5de5\u4e1a\u73af\u5883\uff09\u5b9e\u7528\u6027\u53d7\u9650\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5728\u672c\u5730\u8fd0\u884c\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "PRISM\u4ece\u73b0\u6709LLM\u89c4\u5212\u5668\u4e2d\u81ea\u52a8\u5408\u6210\u4efb\u52a1\u548c\u73af\u5883\uff0c\u63d0\u53d6\u8ba1\u5212\uff0c\u5e76\u5229\u7528\u5408\u6210\u6570\u636e\u84b8\u998f\u51fa\u7d27\u51d1\u7684SLM\u4f5c\u4e3a\u66ff\u4ee3\u3002", "result": "PRISM\u5c06Llama-3.2-3B\u7684\u6027\u80fd\u4eceGPT-4o\u768410-20%\u63d0\u5347\u81f393%\u4ee5\u4e0a\uff0c\u4e14\u84b8\u998f\u540e\u7684\u89c4\u5212\u5668\u80fd\u5728\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u548c\u73af\u5883\u4e2d\u6cdb\u5316\u3002", "conclusion": "PRISM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u672c\u5730\u5316\u7684\u673a\u5668\u4eba\u89c4\u5212\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u5177\u5907\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2506.17488", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.17488", "abs": "https://arxiv.org/abs/2506.17488", "authors": ["Pei-An Hsieh", "Kong Yao Chee", "M. Ani Hsieh"], "title": "Online Adaptation for Flying Quadrotors in Tight Formations", "comment": "10 pages, 4 figures", "summary": "The task of flying in tight formations is challenging for teams of quadrotors\nbecause the complex aerodynamic wake interactions can destabilize individual\nteam members as well as the team. Furthermore, these aerodynamic effects are\nhighly nonlinear and fast-paced, making them difficult to model and predict. To\novercome these challenges, we present L1 KNODE-DW MPC, an adaptive, mixed\nexpert learning based control framework that allows individual quadrotors to\naccurately track trajectories while adapting to time-varying aerodynamic\ninteractions during formation flights. We evaluate L1 KNODE-DW MPC in two\ndifferent three-quadrotor formations and show that it outperforms several MPC\nbaselines. Our results show that the proposed framework is capable of enabling\nthe three-quadrotor team to remain vertically aligned in close proximity\nthroughout the flight. These findings show that the L1 adaptive module\ncompensates for unmodeled disturbances most effectively when paired with an\naccurate dynamics model. A video showcasing our framework and the physical\nexperiments is available here: https://youtu.be/9QX1Q5Ut9Rs", "AI": {"tldr": "L1 KNODE-DW MPC\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u5b66\u4e60\u548c\u6df7\u5408\u4e13\u5bb6\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u591a\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u7d27\u5bc6\u7f16\u961f\u98de\u884c\u4e2d\u7684\u6c14\u52a8\u5e72\u6270\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u8ddf\u8e2a\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u7d27\u5bc6\u7f16\u961f\u98de\u884c\u4e2d\u590d\u6742\u7684\u6c14\u52a8\u5e72\u6270\u4f1a\u5bfc\u81f4\u65e0\u4eba\u673a\u4e0d\u7a33\u5b9a\uff0c\u4e14\u8fd9\u4e9b\u5e72\u6270\u96be\u4ee5\u5efa\u6a21\u548c\u9884\u6d4b\u3002", "method": "\u63d0\u51faL1 KNODE-DW MPC\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u5b66\u4e60\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u4ee5\u5e94\u5bf9\u65f6\u53d8\u6c14\u52a8\u5e72\u6270\u3002", "result": "\u5728\u4e09\u65e0\u4eba\u673a\u7f16\u961f\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u6846\u67b6\u4f18\u4e8e\u591a\u4e2aMPC\u57fa\u7ebf\uff0c\u80fd\u4fdd\u6301\u5782\u76f4\u5bf9\u9f50\u548c\u8fd1\u8ddd\u79bb\u98de\u884c\u3002", "conclusion": "L1\u81ea\u9002\u5e94\u6a21\u5757\u4e0e\u7cbe\u786e\u52a8\u529b\u5b66\u6a21\u578b\u7ed3\u5408\uff0c\u80fd\u6709\u6548\u8865\u507f\u672a\u5efa\u6a21\u5e72\u6270\uff0c\u63d0\u5347\u7f16\u961f\u98de\u884c\u6027\u80fd\u3002"}}
{"id": "2506.17516", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17516", "abs": "https://arxiv.org/abs/2506.17516", "authors": ["Zhou Chen", "Sanjoy Kundu", "Harsimran S. Baweja", "Sathyanarayanan N. Aakur"], "title": "EASE: Embodied Active Event Perception via Self-Supervised Energy Minimization", "comment": "Accepted to IEEE Robotics and Automation Letters, 2025", "summary": "Active event perception, the ability to dynamically detect, track, and\nsummarize events in real time, is essential for embodied intelligence in tasks\nsuch as human-AI collaboration, assistive robotics, and autonomous navigation.\nHowever, existing approaches often depend on predefined action spaces,\nannotated datasets, and extrinsic rewards, limiting their adaptability and\nscalability in dynamic, real-world scenarios. Inspired by cognitive theories of\nevent perception and predictive coding, we propose EASE, a self-supervised\nframework that unifies spatiotemporal representation learning and embodied\ncontrol through free energy minimization. EASE leverages prediction errors and\nentropy as intrinsic signals to segment events, summarize observations, and\nactively track salient actors, operating without explicit annotations or\nexternal rewards. By coupling a generative perception model with an\naction-driven control policy, EASE dynamically aligns predictions with\nobservations, enabling emergent behaviors such as implicit memory, target\ncontinuity, and adaptability to novel environments. Extensive evaluations in\nsimulation and real-world settings demonstrate EASE's ability to achieve\nprivacy-preserving and scalable event perception, providing a robust foundation\nfor embodied systems in unscripted, dynamic tasks.", "AI": {"tldr": "EASE\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7531\u80fd\u6700\u5c0f\u5316\u7edf\u4e00\u65f6\u7a7a\u8868\u793a\u5b66\u4e60\u548c\u5177\u8eab\u63a7\u5236\uff0c\u65e0\u9700\u6807\u6ce8\u6216\u5916\u90e8\u5956\u52b1\uff0c\u5b9e\u73b0\u52a8\u6001\u4e8b\u4ef6\u611f\u77e5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u52a8\u4f5c\u7a7a\u95f4\u548c\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u4e86\u5728\u52a8\u6001\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u7ed3\u5408\u751f\u6210\u611f\u77e5\u6a21\u578b\u548c\u52a8\u4f5c\u9a71\u52a8\u63a7\u5236\u7b56\u7565\uff0c\u5229\u7528\u9884\u6d4b\u8bef\u5dee\u548c\u71b5\u4f5c\u4e3a\u5185\u5728\u4fe1\u53f7\uff0c\u52a8\u6001\u5bf9\u9f50\u9884\u6d4b\u4e0e\u89c2\u5bdf\u3002", "result": "\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86EASE\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u53ef\u6269\u5c55\u4e8b\u4ef6\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "EASE\u4e3a\u65e0\u811a\u672c\u52a8\u6001\u4efb\u52a1\u4e2d\u7684\u5177\u8eab\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9c81\u68d2\u57fa\u7840\u3002"}}
{"id": "2506.17601", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17601", "abs": "https://arxiv.org/abs/2506.17601", "authors": ["Rohan Thakker", "Adarsh Patnaik", "Vince Kurtz", "Jonas Frey", "Jonathan Becktor", "Sangwoo Moon", "Rob Royce", "Marcel Kaufmann", "Georgios Georgakis", "Pascal Roth", "Joel Burdick", "Marco Hutter", "Shehryar Khattak"], "title": "Risk-Guided Diffusion: Toward Deploying Robot Foundation Models in Space, Where Failure Is Not An Option", "comment": null, "summary": "Safe, reliable navigation in extreme, unfamiliar terrain is required for\nfuture robotic space exploration missions. Recent generative-AI methods learn\nsemantically aware navigation policies from large, cross-embodiment datasets,\nbut offer limited safety guarantees. Inspired by human cognitive science, we\npropose a risk-guided diffusion framework that fuses a fast, learned \"System-1\"\nwith a slow, physics-based \"System-2\", sharing computation at both training and\ninference to couple adaptability with formal safety. Hardware experiments\nconducted at the NASA JPL's Mars-analog facility, Mars Yard, show that our\napproach reduces failure rates by up to $4\\times$ while matching the\ngoal-reaching performance of learning-based robotic models by leveraging\ninference-time compute without any additional training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5feb\u901f\u5b66\u4e60\u7cfb\u7edf\u4e0e\u6162\u901f\u7269\u7406\u7cfb\u7edf\u7684\u98ce\u9669\u5f15\u5bfc\u6269\u6563\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4e\u5931\u8d25\u7387\u5e76\u4fdd\u6301\u76ee\u6807\u8fbe\u6210\u6027\u80fd\u3002", "motivation": "\u4e3a\u672a\u6765\u592a\u7a7a\u63a2\u7d22\u4efb\u52a1\u63d0\u4f9b\u5b89\u5168\u53ef\u9760\u7684\u5bfc\u822a\uff0c\u89e3\u51b3\u73b0\u6709\u751f\u6210AI\u65b9\u6cd5\u5b89\u5168\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u878d\u5408\u5feb\u901f\u5b66\u4e60\u7684\u201c\u7cfb\u7edf1\u201d\u4e0e\u6162\u901f\u7269\u7406\u7684\u201c\u7cfb\u7edf2\u201d\uff0c\u901a\u8fc7\u98ce\u9669\u5f15\u5bfc\u6269\u6563\u6846\u67b6\u5171\u4eab\u8ba1\u7b97\u3002", "result": "\u5728NASA JPL\u7684Mars Yard\u5b9e\u9a8c\u4e2d\uff0c\u5931\u8d25\u7387\u964d\u4f4e4\u500d\uff0c\u76ee\u6807\u8fbe\u6210\u6027\u80fd\u4e0e\u5b66\u4e60\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u63a8\u7406\u8ba1\u7b97\u5b9e\u73b0\u4e86\u5b89\u5168\u6027\u4e0e\u9002\u5e94\u6027\u7684\u5e73\u8861\u3002"}}
{"id": "2506.17624", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17624", "abs": "https://arxiv.org/abs/2506.17624", "authors": ["Koki Nakagawa", "Yoshiyuki Ohmura", "Yasuo Kuniyoshi"], "title": "Imitation Learning for Active Neck Motion Enabling Robot Manipulation beyond the Field of View", "comment": "6 pages", "summary": "Most prior research in deep imitation learning has predominantly utilized\nfixed cameras for image input, which constrains task performance to the\npredefined field of view. However, enabling a robot to actively maneuver its\nneck can significantly expand the scope of imitation learning to encompass a\nwider variety of tasks and expressive actions such as neck gestures. To\nfacilitate imitation learning in robots capable of neck movement while\nsimultaneously performing object manipulation, we propose a teaching system\nthat systematically collects datasets incorporating neck movements while\nminimizing discomfort caused by dynamic viewpoints during teleoperation. In\naddition, we present a novel network model for learning manipulation tasks\nincluding active neck motion. Experimental results showed that our model can\nachieve a high success rate of around 90\\%, regardless of the distraction from\nthe viewpoint variations by active neck motion. Moreover, the proposed model\nproved particularly effective in challenging scenarios, such as when objects\nwere situated at the periphery or beyond the standard field of view, where\ntraditional models struggled. The proposed approach contributes to the\nefficiency of dataset collection and extends the applicability of imitation\nlearning to more complex and dynamic scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9888\u90e8\u8fd0\u52a8\u7684\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u89c6\u89d2\u6570\u636e\u96c6\u548c\u65b0\u7f51\u7edc\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u590d\u6742\u573a\u666f\u9002\u5e94\u6027\u3002", "motivation": "\u56fa\u5b9a\u89c6\u89d2\u9650\u5236\u4e86\u6a21\u4eff\u5b66\u4e60\u7684\u4efb\u52a1\u8303\u56f4\uff0c\u800c\u4e3b\u52a8\u9888\u90e8\u8fd0\u52a8\u53ef\u4ee5\u6269\u5c55\u4efb\u52a1\u591a\u6837\u6027\u548c\u8868\u73b0\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b\u9888\u90e8\u8fd0\u52a8\u7684\u6570\u636e\u96c6\u6536\u96c6\u7cfb\u7edf\u548c\u65b0\u578b\u7f51\u7edc\u6a21\u578b\uff0c\u7528\u4e8e\u5b66\u4e60\u5305\u542b\u9888\u90e8\u8fd0\u52a8\u7684\u64cd\u4f5c\u4efb\u52a1\u3002", "result": "\u6a21\u578b\u5728\u52a8\u6001\u89c6\u89d2\u5e72\u6270\u4e0b\u4ecd\u80fd\u4fdd\u6301\u7ea690%\u7684\u6210\u529f\u7387\uff0c\u4e14\u5728\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u5904\u7406\u7684\u8fb9\u7f18\u6216\u89c6\u91ce\u5916\u7269\u4f53\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6570\u636e\u96c6\u6536\u96c6\u6548\u7387\uff0c\u5e76\u5c06\u6a21\u4eff\u5b66\u4e60\u6269\u5c55\u5230\u66f4\u590d\u6742\u548c\u52a8\u6001\u7684\u573a\u666f\u3002"}}
{"id": "2506.17639", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17639", "abs": "https://arxiv.org/abs/2506.17639", "authors": ["Yuxuan Chen", "Xiao Li"], "title": "RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action models (VLA) have demonstrated remarkable capabilities\nand promising potential in solving complex robotic manipulation tasks. However,\ntheir substantial parameter sizes and high inference latency pose significant\nchallenges for real-world deployment, particularly on resource-constrained\nrobotic platforms. To address this issue, we begin by conducting an extensive\nempirical study to explore the effectiveness of model compression techniques\nwhen applied to VLAs. Building on the insights gained from these preliminary\nexperiments, we propose RLRC, a three-stage recovery method for compressed\nVLAs, including structured pruning, performance recovery based on SFT and RL,\nand further quantization. RLRC achieves up to an 8x reduction in memory usage\nand a 2.3x improvement in inference throughput, while maintaining or even\nsurpassing the original VLA's task success rate. Extensive experiments show\nthat RLRC consistently outperforms existing compression baselines,\ndemonstrating strong potential for on-device deployment of VLAs. Project\nwebsite: https://rlrc-vla.github.io", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRLRC\u7684\u4e09\u9636\u6bb5\u6062\u590d\u65b9\u6cd5\uff0c\u7528\u4e8e\u538b\u7f29\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff08VLA\uff09\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5185\u5b58\u4f7f\u7528\u5e76\u63d0\u9ad8\u4e86\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "VLA\u6a21\u578b\u5728\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5927\u53c2\u6570\u89c4\u6a21\u548c\u9ad8\u63a8\u7406\u5ef6\u8fdf\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "method": "RLRC\u65b9\u6cd5\u5305\u62ec\u7ed3\u6784\u5316\u526a\u679d\u3001\u57fa\u4e8eSFT\u548cRL\u7684\u6027\u80fd\u6062\u590d\u4ee5\u53ca\u8fdb\u4e00\u6b65\u91cf\u5316\u4e09\u4e2a\u9636\u6bb5\u3002", "result": "RLRC\u5b9e\u73b0\u4e86\u5185\u5b58\u4f7f\u7528\u51cf\u5c118\u500d\uff0c\u63a8\u7406\u541e\u5410\u91cf\u63d0\u9ad82.3\u500d\uff0c\u540c\u65f6\u4efb\u52a1\u6210\u529f\u7387\u4fdd\u6301\u6216\u8d85\u8fc7\u539f\u59cbVLA\u3002", "conclusion": "RLRC\u5728\u538b\u7f29VLA\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5728\u8bbe\u5907\u7aef\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.17775", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.17775", "abs": "https://arxiv.org/abs/2506.17775", "authors": ["Sebastian Sansoni", "Javier Gimenez", "Gast\u00f3n Castro", "Santiago Tosetti", "Flavio Craparo"], "title": "Optimizing Exploration with a New Uncertainty Framework for Active SLAM Systems", "comment": null, "summary": "Accurate reconstruction of the environment is a central goal of Simultaneous\nLocalization and Mapping (SLAM) systems. However, the agent's trajectory can\nsignificantly affect estimation accuracy. This paper presents a new method to\nmodel map uncertainty in Active SLAM systems using an Uncertainty Map (UM). The\nUM uses probability distributions to capture where the map is uncertain,\nallowing Uncertainty Frontiers (UF) to be defined as key\nexploration-exploitation objectives and potential stopping criteria. In\naddition, the method introduces the Signed Relative Entropy (SiREn), based on\nthe Kullback-Leibler divergence, to measure both coverage and uncertainty\ntogether. This helps balance exploration and exploitation through an\neasy-to-understand parameter. Unlike methods that depend on particular SLAM\nsetups, the proposed approach is compatible with different types of sensors,\nsuch as cameras, LiDARs, and multi-sensor fusion. It also addresses common\nproblems in exploration planning and stopping conditions. Furthermore,\nintegrating this map modeling approach with a UF-based planning system enables\nthe agent to autonomously explore open spaces, a behavior not previously\nobserved in the Active SLAM literature. Code and implementation details are\navailable as a ROS node, and all generated data are openly available for public\nuse, facilitating broader adoption and validation of the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u5730\u56fe\uff08UM\uff09\u7684\u4e3b\u52a8SLAM\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6982\u7387\u5206\u5e03\u5efa\u6a21\u5730\u56fe\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5f15\u5165Signed Relative Entropy\uff08SiREn\uff09\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "motivation": "\u89e3\u51b3SLAM\u7cfb\u7edf\u4e2d\u73af\u5883\u91cd\u5efa\u7cbe\u5ea6\u53d7\u8f68\u8ff9\u5f71\u54cd\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u901a\u7528\u4e14\u517c\u5bb9\u591a\u4f20\u611f\u5668\u7684\u5730\u56fe\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528UM\u6355\u6349\u5730\u56fe\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9a\u4e49Uncertainty Frontiers\uff08UF\uff09\u4f5c\u4e3a\u76ee\u6807\uff0c\u5e76\u5f15\u5165SiREn\u8861\u91cf\u8986\u76d6\u4e0e\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u4f20\u611f\u5668\uff0c\u89e3\u51b3\u4e86\u63a2\u7d22\u89c4\u5212\u548c\u505c\u6b62\u6761\u4ef6\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u81ea\u4e3b\u63a2\u7d22\u5f00\u653e\u7a7a\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u7528\u6027\u5f3a\uff0c\u4ee3\u7801\u548c\u6570\u636e\u516c\u5f00\uff0c\u4fbf\u4e8e\u63a8\u5e7f\u548c\u9a8c\u8bc1\u3002"}}
{"id": "2506.17811", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.17811", "abs": "https://arxiv.org/abs/2506.17811", "authors": ["Jacky Kwok", "Christopher Agia", "Rohan Sinha", "Matt Foutter", "Shulu Li", "Ion Stoica", "Azalia Mirhoseini", "Marco Pavone"], "title": "RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities\nin visuomotor control, yet ensuring their robustness in unstructured real-world\nenvironments remains a persistent challenge. In this paper, we investigate\ntest-time scaling through the lens of sampling and verification as means to\nenhance the robustness and generalization of VLAs. We first demonstrate that\nthe relationship between action error and the number of generated samples\nfollows an exponentiated power law across a range of VLAs, indicating the\nexistence of inference-time scaling laws. Building on these insights, we\nintroduce RoboMonkey, a test-time scaling framework for VLAs. At deployment,\nRoboMonkey samples a small set of actions from a VLA, applies Gaussian\nperturbation and majority voting to construct an action proposal distribution,\nand then uses a Vision Language Model (VLM)-based verifier to select the\noptimal action. We propose a synthetic data generation pipeline for training\nsuch VLM-based action verifiers, and demonstrate that scaling the synthetic\ndataset consistently improves verification and downstream accuracy. Through\nextensive simulated and hardware experiments, we show that pairing existing\nVLAs with RoboMonkey yields significant performance gains, achieving a 25%\nabsolute improvement on out-of-distribution tasks and 8% on in-distribution\ntasks. Additionally, when adapting to new robot setups, we show that\nfine-tuning both VLAs and action verifiers yields a 7% performance increase\ncompared to fine-tuning VLAs alone.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRoboMonkey\u6846\u67b6\uff0c\u901a\u8fc7\u91c7\u6837\u3001\u6270\u52a8\u548c\u9a8c\u8bc1\u589e\u5f3aVision-Language-Action\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1VLA\u6a21\u578b\u5728\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u975e\u7ed3\u6784\u5316\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u4ecd\u9762\u4e34\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6d4b\u8bd5\u65f6\u7f29\u653e\u63d0\u5347\u5176\u6027\u80fd\u3002", "method": "\u63d0\u51faRoboMonkey\u6846\u67b6\uff0c\u5305\u62ec\u91c7\u6837\u3001\u9ad8\u65af\u6270\u52a8\u3001\u591a\u6570\u6295\u7968\u6784\u5efa\u52a8\u4f5c\u5206\u5e03\uff0c\u4ee5\u53ca\u57fa\u4e8eVLM\u7684\u9a8c\u8bc1\u5668\u9009\u62e9\u6700\u4f18\u52a8\u4f5c\u3002\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u9a8c\u8bc1\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRoboMonkey\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0cOOD\u4efb\u52a1\u63d0\u534725%\uff0cID\u4efb\u52a1\u63d0\u53478%\u3002\u65b0\u673a\u5668\u4eba\u8bbe\u7f6e\u4e0b\uff0c\u8054\u5408\u5fae\u8c03VLA\u548c\u9a8c\u8bc1\u5668\u6027\u80fd\u63d0\u53477%\u3002", "conclusion": "RoboMonkey\u901a\u8fc7\u6d4b\u8bd5\u65f6\u7f29\u653e\u548c\u9a8c\u8bc1\u673a\u5236\u6709\u6548\u589e\u5f3aVLA\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.17823", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17823", "abs": "https://arxiv.org/abs/2506.17823", "authors": ["Kevin Chang", "Rakesh Vivekanandan", "Noah Pragin", "Sean Bullock", "Geoffrey Hollinger"], "title": "Learning to Dock: A Simulation-based Study on Closing the Sim2Real Gap in Autonomous Underwater Docking", "comment": "Advancing Quantitative and Qualitative Simulators for Marine\n  Applications Workshop Paper at International Conference on Robotics and\n  Automation 2025", "summary": "Autonomous Underwater Vehicle (AUV) docking in dynamic and uncertain\nenvironments is a critical challenge for underwater robotics. Reinforcement\nlearning is a promising method for developing robust controllers, but the\ndisparity between training simulations and the real world, or the sim2real gap,\noften leads to a significant deterioration in performance. In this work, we\nperform a simulation study on reducing the sim2real gap in autonomous docking\nthrough training various controllers and then evaluating them under realistic\ndisturbances. In particular, we focus on the real-world challenge of docking\nunder different payloads that are potentially outside the original training\ndistribution. We explore existing methods for improving robustness including\nrandomization techniques and history-conditioned controllers. Our findings\nprovide insights into mitigating the sim2real gap when training docking\ncontrollers. Furthermore, our work indicates areas of future research that may\nbe beneficial to the marine robotics community.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u51cf\u5c11AUV\u5728\u52a8\u6001\u73af\u5883\u4e2d\u81ea\u4e3b\u5bf9\u63a5\u7684\u6a21\u62df\u4e0e\u73b0\u5b9e\u5dee\u8ddd\uff0c\u63a2\u7d22\u968f\u673a\u5316\u6280\u672f\u548c\u5386\u53f2\u6761\u4ef6\u63a7\u5236\u5668\u7b49\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3AUV\u5728\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u81ea\u4e3b\u5bf9\u63a5\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u6a21\u62df\u4e0e\u73b0\u5b9e\u6027\u80fd\u5dee\u8ddd\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u591a\u79cd\u63a7\u5236\u5668\u5e76\u5728\u771f\u5b9e\u6270\u52a8\u4e0b\u8bc4\u4f30\uff0c\u63a2\u7d22\u968f\u673a\u5316\u6280\u672f\u548c\u5386\u53f2\u6761\u4ef6\u63a7\u5236\u5668\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u53ef\u51cf\u5c11\u6a21\u62df\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u7684\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6d77\u6d0b\u673a\u5668\u4eba\u793e\u533a\u63d0\u4f9b\u4e86\u51cf\u5c11\u6a21\u62df\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u7684\u89c1\u89e3\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.17831", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17831", "abs": "https://arxiv.org/abs/2506.17831", "authors": ["Mina Kian", "Mingyu Zong", "Katrin Fischer", "Anna-Maria Velentza", "Abhyuday Singh", "Kaleen Shrestha", "Pau Sang", "Shriya Upadhyay", "Wallace Browning", "Misha Arif Faruki", "S\u00e9bastien M. R. Arnold", "Bhaskar Krishnamachari", "Maja Matari\u0107"], "title": "Engagement and Disclosures in LLM-Powered Cognitive Behavioral Therapy Exercises: A Factorial Design Comparing the Influence of a Robot vs. Chatbot Over Time", "comment": null, "summary": "Many researchers are working to address the worldwide mental health crisis by\ndeveloping therapeutic technologies that increase the accessibility of care,\nincluding leveraging large language model (LLM) capabilities in chatbots and\nsocially assistive robots (SARs) used for therapeutic applications. Yet, the\neffects of these technologies over time remain unexplored. In this study, we\nuse a factorial design to assess the impact of embodiment and time spent\nengaging in therapeutic exercises on participant disclosures. We assessed\ntranscripts gathered from a two-week study in which 26 university student\nparticipants completed daily interactive Cognitive Behavioral Therapy (CBT)\nexercises in their residences using either an LLM-powered SAR or a disembodied\nchatbot. We evaluated the levels of active engagement and high intimacy of\ntheir disclosures (opinions, judgments, and emotions) during each session and\nover time. Our findings show significant interactions between time and\nembodiment for both outcome measures: participant engagement and intimacy\nincreased over time in the physical robot condition, while both measures\ndecreased in the chatbot condition.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5177\u8eab\u5316\uff08SAR\uff09\u4e0e\u975e\u5177\u8eab\u5316\uff08\u804a\u5929\u673a\u5668\u4eba\uff09\u5bf9\u5fc3\u7406\u6cbb\u7597\u53c2\u4e0e\u5ea6\u548c\u4eb2\u5bc6\u611f\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5177\u8eab\u5316\u968f\u65f6\u95f4\u63d0\u5347\u6548\u679c\uff0c\u800c\u975e\u5177\u8eab\u5316\u5219\u76f8\u53cd\u3002", "motivation": "\u89e3\u51b3\u5168\u7403\u5fc3\u7406\u5065\u5eb7\u5371\u673a\uff0c\u63a2\u7d22\u5177\u8eab\u5316\u6280\u672f\uff08\u5982SAR\uff09\u4e0e\u804a\u5929\u673a\u5668\u4eba\u5728\u5fc3\u7406\u6cbb\u7597\u4e2d\u7684\u957f\u671f\u6548\u679c\u5dee\u5f02\u3002", "method": "\u91c7\u7528\u56e0\u5b50\u8bbe\u8ba1\uff0c26\u540d\u5927\u5b66\u751f\u5728\u4e24\u5468\u5185\u6bcf\u5929\u4f7f\u7528SAR\u6216\u804a\u5929\u673a\u5668\u4eba\u5b8c\u6210CBT\u7ec3\u4e60\uff0c\u8bc4\u4f30\u53c2\u4e0e\u5ea6\u548c\u4eb2\u5bc6\u611f\u7684\u53d8\u5316\u3002", "result": "\u5177\u8eab\u5316\uff08SAR\uff09\u968f\u65f6\u95f4\u663e\u8457\u63d0\u5347\u53c2\u4e0e\u5ea6\u548c\u4eb2\u5bc6\u611f\uff0c\u800c\u804a\u5929\u673a\u5668\u4eba\u5219\u6548\u679c\u4e0b\u964d\u3002", "conclusion": "\u5177\u8eab\u5316\u8bbe\u8ba1\u5bf9\u5fc3\u7406\u6cbb\u7597\u7684\u957f\u671f\u6548\u679c\u66f4\u4f18\uff0c\u652f\u6301SAR\u5728\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.17832", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.17832", "abs": "https://arxiv.org/abs/2506.17832", "authors": ["Pratik Kunapuli", "Jake Welde", "Dinesh Jayaraman", "Vijay Kumar"], "title": "Leveling the Playing Field: Carefully Comparing Classical and Learned Controllers for Quadrotor Trajectory Tracking", "comment": "Accepted for publication to RSS 2025. 10 pages, 5 figures. Project\n  website: https://pratikkunapuli.github.io/rl-vs-gc/", "summary": "Learning-based control approaches like reinforcement learning (RL) have\nrecently produced a slew of impressive results for tasks like quadrotor\ntrajectory tracking and drone racing. Naturally, it is common to demonstrate\nthe advantages of these new controllers against established methods like\nanalytical controllers. We observe, however, that reliably comparing the\nperformance of such very different classes of controllers is more complicated\nthan might appear at first sight. As a case study, we take up the problem of\nagile tracking of an end-effector for a quadrotor with a fixed arm. We develop\na set of best practices for synthesizing the best-in-class RL and geometric\ncontrollers (GC) for benchmarking. In the process, we resolve widespread\nRL-favoring biases in prior studies that provide asymmetric access to: (1) the\ntask definition, in the form of an objective function, (2) representative\ndatasets, for parameter optimization, and (3) feedforward information,\ndescribing the desired future trajectory. The resulting findings are the\nfollowing: our improvements to the experimental protocol for comparing learned\nand classical controllers are critical, and each of the above asymmetries can\nyield misleading conclusions. Prior works have claimed that RL outperforms GC,\nbut we find the gaps between the two controller classes are much smaller than\npreviously published when accounting for symmetric comparisons. Geometric\ncontrol achieves lower steady-state error than RL, while RL has better\ntransient performance, resulting in GC performing better in relatively slow or\nless agile tasks, but RL performing better when greater agility is required.\nFinally, we open-source implementations of geometric and RL controllers for\nthese aerial vehicles, implementing best practices for future development.\nWebsite and code is available at https://pratikkunapuli.github.io/rl-vs-gc/", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5b66\u4e60\u7684\u63a7\u5236\u5668\uff08\u5982\u5f3a\u5316\u5b66\u4e60\uff09\u4e0e\u4f20\u7edf\u51e0\u4f55\u63a7\u5236\u5668\u5728\u56db\u65cb\u7ffc\u8f68\u8ff9\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u6bd4\u8f83\uff0c\u63d0\u51fa\u4e86\u5bf9\u79f0\u6bd4\u8f83\u7684\u6700\u4f73\u5b9e\u8df5\uff0c\u5e76\u53d1\u73b0\u4e24\u8005\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5404\u6709\u4f18\u52a3\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u6bd4\u8f83\u5b66\u4e60\u63a7\u5236\u5668\u4e0e\u4f20\u7edf\u63a7\u5236\u5668\u65f6\u5b58\u5728\u4e0d\u5bf9\u79f0\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bef\u5bfc\u6027\u7ed3\u8bba\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u516c\u5e73\u7684\u6bd4\u8f83\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff08\u56db\u65cb\u7ffc\u56fa\u5b9a\u81c2\u7684\u654f\u6377\u8ddf\u8e2a\u4efb\u52a1\uff09\uff0c\u5f00\u53d1\u4e86\u4e00\u5957\u6700\u4f73\u5b9e\u8df5\uff0c\u7528\u4e8e\u5408\u6210\u548c\u6bd4\u8f83\u5f3a\u5316\u5b66\u4e60\u4e0e\u51e0\u4f55\u63a7\u5236\u5668\u3002", "result": "\u5bf9\u79f0\u6bd4\u8f83\u540e\uff0c\u51e0\u4f55\u63a7\u5236\u5668\u5728\u7a33\u6001\u8bef\u5dee\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u5728\u77ac\u6001\u6027\u80fd\u4e0a\u66f4\u4f18\u3002", "conclusion": "\u6539\u8fdb\u7684\u5b9e\u9a8c\u534f\u8bae\u5bf9\u516c\u5e73\u6bd4\u8f83\u81f3\u5173\u91cd\u8981\uff0c\u672a\u6765\u7814\u7a76\u5e94\u907f\u514d\u4e0d\u5bf9\u79f0\u6027\uff0c\u5e76\u5f00\u6e90\u4e86\u63a7\u5236\u5668\u5b9e\u73b0\u3002"}}
{"id": "2506.17842", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17842", "abs": "https://arxiv.org/abs/2506.17842", "authors": ["Al-Harith Farhad", "Khalil Abuibaid", "Christiane Plociennik", "Achim Wagner", "Martin Ruskowski"], "title": "Generative Grasp Detection and Estimation with Concept Learning-based Safety Criteria", "comment": "RAAD 2025: 34th International Conference on Robotics in\n  Alpe-Adria-Danube Region", "summary": "Neural networks are often regarded as universal equations that can estimate\nany function. This flexibility, however, comes with the drawback of high\ncomplexity, rendering these networks into black box models, which is especially\nrelevant in safety-centric applications. To that end, we propose a pipeline for\na collaborative robot (Cobot) grasping algorithm that detects relevant tools\nand generates the optimal grasp. To increase the transparency and reliability\nof this approach, we integrate an explainable AI method that provides an\nexplanation for the underlying prediction of a model by extracting the learned\nfeatures and correlating them to corresponding classes from the input. These\nconcepts are then used as additional criteria to ensure the safe handling of\nwork tools. In this paper, we show the consistency of this approach and the\ncriterion for improving the handover position. This approach was tested in an\nindustrial environment, where a camera system was set up to enable a robot to\npick up certain tools and objects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u534f\u4f5c\u673a\u5668\u4eba\u6293\u53d6\u7b97\u6cd5\u7684\u900f\u660e\u5316\u6d41\u7a0b\uff0c\u7ed3\u5408\u53ef\u89e3\u91caAI\u65b9\u6cd5\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u9ed1\u76d2\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u590d\u6742\u6027\u548c\u4e0d\u900f\u660e\u6027\u95ee\u9898\u3002", "method": "\u96c6\u6210\u53ef\u89e3\u91caAI\u65b9\u6cd5\uff0c\u63d0\u53d6\u5b66\u4e60\u7279\u5f81\u5e76\u4e0e\u8f93\u5165\u7c7b\u522b\u5173\u8054\uff0c\u4f5c\u4e3a\u989d\u5916\u5b89\u5168\u6807\u51c6\u3002", "result": "\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u6293\u53d6\u4f4d\u7f6e\u7684\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u534f\u4f5c\u673a\u5668\u4eba\u6293\u53d6\u7b97\u6cd5\u7684\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u73af\u5883\u3002"}}
{"id": "2506.17868", "categories": ["cs.RO", "cs.LG", "math.DG"], "pdf": "https://arxiv.org/pdf/2506.17868", "abs": "https://arxiv.org/abs/2506.17868", "authors": ["Andrea Testa", "S\u00f8ren Hauberg", "Tamim Asfour", "Leonel Rozo"], "title": "Geometric Contact Flows: Contactomorphisms for Dynamics and Control", "comment": "Accepted at ICML 2025", "summary": "Accurately modeling and predicting complex dynamical systems, particularly\nthose involving force exchange and dissipation, is crucial for applications\nranging from fluid dynamics to robotics, but presents significant challenges\ndue to the intricate interplay of geometric constraints and energy transfer.\nThis paper introduces Geometric Contact Flows (GFC), a novel framework\nleveraging Riemannian and Contact geometry as inductive biases to learn such\nsystems. GCF constructs a latent contact Hamiltonian model encoding desirable\nproperties like stability or energy conservation. An ensemble of\ncontactomorphisms then adapts this model to the target dynamics while\npreserving these properties. This ensemble allows for uncertainty-aware\ngeodesics that attract the system's behavior toward the data support, enabling\nrobust generalization and adaptation to unseen scenarios. Experiments on\nlearning dynamics for physical systems and for controlling robots on\ninteraction tasks demonstrate the effectiveness of our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGeometric Contact Flows\uff08GFC\uff09\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u9ece\u66fc\u51e0\u4f55\u548c\u63a5\u89e6\u51e0\u4f55\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\uff0c\u5b66\u4e60\u590d\u6742\u52a8\u529b\u7cfb\u7edf\uff0c\u5e76\u5728\u7269\u7406\u7cfb\u7edf\u548c\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u51c6\u786e\u5efa\u6a21\u548c\u9884\u6d4b\u6d89\u53ca\u529b\u4ea4\u6362\u548c\u8017\u6563\u7684\u590d\u6742\u52a8\u529b\u7cfb\u7edf\u5bf9\u6d41\u4f53\u52a8\u529b\u5b66\u548c\u673a\u5668\u4eba\u5b66\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u51e0\u4f55\u7ea6\u675f\u548c\u80fd\u91cf\u4f20\u9012\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\uff0c\u5b58\u5728\u663e\u8457\u6311\u6218\u3002", "method": "GFC\u6784\u5efa\u4e86\u4e00\u4e2a\u6f5c\u5728\u63a5\u89e6\u54c8\u5bc6\u987f\u6a21\u578b\uff0c\u7f16\u7801\u7a33\u5b9a\u6027\u6216\u80fd\u91cf\u5b88\u6052\u7b49\u7406\u60f3\u7279\u6027\uff0c\u5e76\u901a\u8fc7\u63a5\u89e6\u540c\u80da\u7684\u96c6\u5408\u5c06\u8be5\u6a21\u578b\u9002\u5e94\u4e8e\u76ee\u6807\u52a8\u529b\u5b66\uff0c\u540c\u65f6\u4fdd\u7559\u8fd9\u4e9b\u7279\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGFC\u5728\u5b66\u4e60\u7269\u7406\u7cfb\u7edf\u52a8\u529b\u5b66\u548c\u63a7\u5236\u673a\u5668\u4eba\u4ea4\u4e92\u4efb\u52a1\u65b9\u9762\u5177\u6709\u6709\u6548\u6027\u3002", "conclusion": "GFC\u901a\u8fc7\u51e0\u4f55\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u52a8\u529b\u7cfb\u7edf\u7684\u9c81\u68d2\u5efa\u6a21\u548c\u9884\u6d4b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.17902", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17902", "abs": "https://arxiv.org/abs/2506.17902", "authors": ["Peiyu Luo", "Shilong Yao", "Yuhan Chen", "Max Q. -H. Meng"], "title": "Embedded Flexible Circumferential Sensing for Real-Time Intraoperative Environmental Perception in Continuum Robots", "comment": null, "summary": "Continuum robots have been widely adopted in robot-assisted minimally\ninvasive surgery (RMIS) because of their compact size and high flexibility.\nHowever, their proprioceptive capabilities remain limited, particularly in\nnarrow lumens, where lack of environmental awareness can lead to unintended\ntissue contact and surgical risks. To address this challenge, this work\nproposes a flexible annular sensor structure integrated around the vertebral\ndisks of continuum robots. The proposed design enables real-time environmental\nmapping by estimating the distance between the robotic disks and the\nsurrounding tissue, thereby facilitating safer operation through advanced\ncontrol strategies. The experiment has proven that its accuracy in obstacle\ndetection can reach 0.19 mm. Fabricated using flexible printed circuit (FPC)\ntechnology, the sensor demonstrates a modular and cost-effective design with\ncompact dimensions and low noise interference. Its adaptable parameters allow\ncompatibility with various continuum robot architectures, offering a promising\nsolution for enhancing intraoperative perception and control in surgical\nrobotics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u5728\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u690e\u95f4\u76d8\u5468\u56f4\u7684\u67d4\u6027\u73af\u5f62\u4f20\u611f\u5668\u7ed3\u6784\uff0c\u7528\u4e8e\u5b9e\u65f6\u73af\u5883\u6620\u5c04\uff0c\u63d0\u9ad8\u624b\u672f\u5b89\u5168\u6027\u3002", "motivation": "\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u5728\u72ed\u7a84\u8154\u9053\u4e2d\u7f3a\u4e4f\u73af\u5883\u611f\u77e5\u80fd\u529b\uff0c\u53ef\u80fd\u5bfc\u81f4\u610f\u5916\u7ec4\u7ec7\u63a5\u89e6\u548c\u624b\u672f\u98ce\u9669\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u67d4\u6027\u5370\u5237\u7535\u8def\u6280\u672f\u7684\u73af\u5f62\u4f20\u611f\u5668\uff0c\u7528\u4e8e\u5b9e\u65f6\u4f30\u8ba1\u673a\u5668\u4eba\u4e0e\u5468\u56f4\u7ec4\u7ec7\u7684\u8ddd\u79bb\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4f20\u611f\u5668\u5728\u969c\u788d\u7269\u68c0\u6d4b\u4e2d\u7684\u7cbe\u5ea6\u53ef\u8fbe0.19\u6beb\u7c73\uff0c\u5177\u6709\u6a21\u5757\u5316\u3001\u4f4e\u6210\u672c\u548c\u5c0f\u5c3a\u5bf8\u7684\u7279\u70b9\u3002", "conclusion": "\u8be5\u4f20\u611f\u5668\u8bbe\u8ba1\u4e3a\u589e\u5f3a\u624b\u672f\u673a\u5668\u4eba\u672f\u4e2d\u611f\u77e5\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.17960", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17960", "abs": "https://arxiv.org/abs/2506.17960", "authors": ["Jiaming Wang", "Diwen Liu", "Jizhuo Chen", "Jiaxuan Da", "Nuowen Qian", "Tram Minh Man", "Harold Soh"], "title": "GeNIE: A Generalizable Navigation System for In-the-Wild Environments", "comment": "8 pages, 5 figures. Jiaming Wang, Diwen Liu, and Jizhuo Chen\n  contributed equally", "summary": "Reliable navigation in unstructured, real-world environments remains a\nsignificant challenge for embodied agents, especially when operating across\ndiverse terrains, weather conditions, and sensor configurations. In this paper,\nwe introduce GeNIE (Generalizable Navigation System for In-the-Wild\nEnvironments), a robust navigation framework designed for global deployment.\nGeNIE integrates a generalizable traversability prediction model built on SAM2\nwith a novel path fusion strategy that enhances planning stability in noisy and\nambiguous settings. We deployed GeNIE in the Earth Rover Challenge (ERC) at\nICRA 2025, where it was evaluated across six countries spanning three\ncontinents. GeNIE took first place and achieved 79% of the maximum possible\nscore, outperforming the second-best team by 17%, and completed the entire\ncompetition without a single human intervention. These results set a new\nbenchmark for robust, generalizable outdoor robot navigation. We will release\nthe codebase, pretrained model weights, and newly curated datasets to support\nfuture research in real-world navigation.", "AI": {"tldr": "GeNIE\u662f\u4e00\u4e2a\u901a\u7528\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u53ef\u6cdb\u5316\u7684\u53ef\u901a\u884c\u6027\u9884\u6d4b\u6a21\u578b\u548c\u8def\u5f84\u878d\u5408\u7b56\u7565\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u5bfc\u822a\uff0c\u5e76\u5728ICRA 2025\u6bd4\u8d5b\u4e2d\u53d6\u5f97\u4f18\u5f02\u6210\u7ee9\u3002", "motivation": "\u89e3\u51b3\u975e\u7ed3\u6784\u5316\u771f\u5b9e\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u591a\u6837\u5316\u5730\u5f62\u3001\u5929\u6c14\u548c\u4f20\u611f\u5668\u914d\u7f6e\u4e0b\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u6574\u5408\u57fa\u4e8eSAM2\u7684\u53ef\u6cdb\u5316\u53ef\u901a\u884c\u6027\u9884\u6d4b\u6a21\u578b\u548c\u65b0\u578b\u8def\u5f84\u878d\u5408\u7b56\u7565\uff0c\u63d0\u5347\u89c4\u5212\u7a33\u5b9a\u6027\u3002", "result": "\u5728ICRA 2025\u6bd4\u8d5b\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u6210\u7ee9\u8d85\u51fa\u7b2c\u4e8c\u540d17%\uff0c\u5168\u7a0b\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002", "conclusion": "GeNIE\u4e3a\u6237\u5916\u673a\u5668\u4eba\u5bfc\u822a\u8bbe\u7acb\u4e86\u65b0\u6807\u6746\uff0c\u5e76\u5c06\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2506.17994", "categories": ["cs.RO", "cs.LG", "I.2.9; I.2.6; I.6.4"], "pdf": "https://arxiv.org/pdf/2506.17994", "abs": "https://arxiv.org/abs/2506.17994", "authors": ["Minh Trinh", "Andreas Ren\u00e9 Geist", "Josefine Monnet", "Stefan Vilceanu", "Sebastian Trimpe", "Christian Brecher"], "title": "Newtonian and Lagrangian Neural Networks: A Comparison Towards Efficient Inverse Dynamics Identification", "comment": "Paper accepted for publication in 14th IFAC Symposium on Robotics", "summary": "Accurate inverse dynamics models are essential tools for controlling\nindustrial robots. Recent research combines neural network regression with\ninverse dynamics formulations of the Newton-Euler and the Euler-Lagrange\nequations of motion, resulting in so-called Newtonian neural networks and\nLagrangian neural networks, respectively. These physics-informed models seek to\nidentify unknowns in the analytical equations from data. Despite their\npotential, current literature lacks guidance on choosing between Lagrangian and\nNewtonian networks. In this study, we show that when motor torques are\nestimated instead of directly measuring joint torques, Lagrangian networks\nprove less effective compared to Newtonian networks as they do not explicitly\nmodel dissipative torques. The performance of these models is compared to\nneural network regression on data of a MABI MAX 100 industrial robot.", "AI": {"tldr": "\u6bd4\u8f83\u725b\u987f\u795e\u7ecf\u7f51\u7edc\u548c\u62c9\u683c\u6717\u65e5\u795e\u7ecf\u7f51\u7edc\u5728\u5de5\u4e1a\u673a\u5668\u4eba\u9006\u52a8\u529b\u5b66\u5efa\u6a21\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u725b\u987f\u795e\u7ecf\u7f51\u7edc\u5728\u4f30\u8ba1\u7535\u673a\u626d\u77e9\u65f6\u66f4\u6709\u6548\u3002", "motivation": "\u5de5\u4e1a\u673a\u5668\u4eba\u63a7\u5236\u9700\u8981\u7cbe\u786e\u7684\u9006\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5173\u4e8e\u9009\u62e9\u62c9\u683c\u6717\u65e5\u6216\u725b\u987f\u795e\u7ecf\u7f51\u7edc\u7684\u6307\u5bfc\u3002", "method": "\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u56de\u5f52\u4e0e\u725b\u987f-\u6b27\u62c9\u548c\u6b27\u62c9-\u62c9\u683c\u6717\u65e5\u8fd0\u52a8\u65b9\u7a0b\u7684\u9006\u52a8\u529b\u5b66\u516c\u5f0f\uff0c\u6bd4\u8f83\u4e24\u79cd\u6a21\u578b\u5728MABI MAX 100\u5de5\u4e1a\u673a\u5668\u4eba\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u62c9\u683c\u6717\u65e5\u795e\u7ecf\u7f51\u7edc\u5728\u4f30\u8ba1\u7535\u673a\u626d\u77e9\u65f6\u6548\u679c\u8f83\u5dee\uff0c\u56e0\u5176\u672a\u663e\u5f0f\u5efa\u6a21\u8017\u6563\u626d\u77e9\u3002", "conclusion": "\u725b\u987f\u795e\u7ecf\u7f51\u7edc\u5728\u7535\u673a\u626d\u77e9\u4f30\u8ba1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u673a\u5668\u4eba\u63a7\u5236\u3002"}}
{"id": "2506.18016", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18016", "abs": "https://arxiv.org/abs/2506.18016", "authors": ["Yongxin Shao", "Binrui Wang", "Aihong Tan"], "title": "ADA-DPM: A Neural Descriptors-based Adaptive Noise Point Filtering Strategy for SLAM", "comment": null, "summary": "LiDAR SLAM has demonstrated significant application value in various fields,\nincluding mobile robot navigation and high-precision map construction. However,\nexisting methods often need to make a trade-off between positioning accuracy\nand system robustness when faced with dynamic object interference, point cloud\nnoise, and unstructured environments. To address this challenge, we propose an\nadaptive noise filtering SLAM strategy-ADA-DPM, achieving excellent preference\nin both aspects. We design the Dynamic Segmentation Head to predict the\ncategory of feature points belonging to dynamic points, to eliminate dynamic\nfeature points; design the Global Importance Scoring Head to adaptively select\nfeature points with higher contribution and features while suppressing noise\ninterference; and construct the Cross Layer Intra-Graph Convolution Module\n(GLI-GCN) to fuse multi-scale neighborhood structures, thereby enhancing the\ndiscriminative ability of overlapping features. Finally, to further validate\nthe effectiveness of our method, we tested it on several publicly available\ndatasets and achieved outstanding results.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u566a\u58f0\u8fc7\u6ee4SLAM\u7b56\u7565ADA-DPM\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u5272\u5934\u548c\u5168\u5c40\u91cd\u8981\u6027\u8bc4\u5206\u5934\u4f18\u5316\u7279\u5f81\u70b9\u9009\u62e9\uff0c\u7ed3\u5408GLI-GCN\u6a21\u5757\u63d0\u5347\u7279\u5f81\u5224\u522b\u80fd\u529b\uff0c\u5728\u52a8\u6001\u5e72\u6270\u548c\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709LiDAR SLAM\u65b9\u6cd5\u5728\u52a8\u6001\u7269\u4f53\u5e72\u6270\u3001\u70b9\u4e91\u566a\u58f0\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u9700\u6743\u8861\u5b9a\u4f4d\u7cbe\u5ea6\u4e0e\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "method": "\u8bbe\u8ba1\u52a8\u6001\u5206\u5272\u5934\u9884\u6d4b\u52a8\u6001\u7279\u5f81\u70b9\u7c7b\u522b\u5e76\u6d88\u9664\uff1b\u5168\u5c40\u91cd\u8981\u6027\u8bc4\u5206\u5934\u81ea\u9002\u5e94\u9009\u62e9\u9ad8\u8d21\u732e\u7279\u5f81\u70b9\uff1b\u6784\u5efaGLI-GCN\u6a21\u5757\u878d\u5408\u591a\u5c3a\u5ea6\u90bb\u57df\u7ed3\u6784\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u53d6\u5f97\u4e86\u51fa\u8272\u7684\u7ed3\u679c\u3002", "conclusion": "ADA-DPM\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u7cfb\u7edf\u9c81\u68d2\u6027\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2506.18040", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18040", "abs": "https://arxiv.org/abs/2506.18040", "authors": ["Chenghua Lu", "Kailuan Tang", "Xueming Hui", "Haoran Li", "Saekwang Nam", "Nathan F. Lepora"], "title": "StereoTacTip: Vision-based Tactile Sensing with Biomimetic Skin-Marker Arrangements", "comment": "11 pages, 13 figures", "summary": "Vision-Based Tactile Sensors (VBTSs) stand out for their superior performance\ndue to their high-information content output. Recently, marker-based VBTSs have\nbeen shown to give accurate geometry reconstruction when using stereo cameras.\n\\uhl{However, many marker-based VBTSs use complex biomimetic skin-marker\narrangements, which presents issues for the geometric reconstruction of the\nskin surface from the markers}. Here we investigate how the marker-based skin\nmorphology affects stereo vision-based tactile sensing, using a novel VBTS\ncalled the StereoTacTip. To achieve accurate geometry reconstruction, we\nintroduce: (i) stereo marker matching and tracking using a novel\nDelaunay-Triangulation-Ring-Coding algorithm; (ii) a refractive depth\ncorrection model that corrects the depth distortion caused by refraction in the\ninternal media; (iii) a skin surface correction model from the marker\npositions, relying on an inverse calculation of normals to the skin surface;\nand (iv)~methods for geometry reconstruction over multiple contacts. To\ndemonstrate these findings, we reconstruct topographic terrains on a large 3D\nmap. Even though contributions (i) and (ii) were developed for biomimetic\nmarkers, they should improve the performance of all marker-based VBTSs.\nOverall, this work illustrates that a thorough understanding and evaluation of\nthe morphologically-complex skin and marker-based tactile sensor principles are\ncrucial for obtaining accurate geometric information.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u6807\u8bb0\u7684\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\uff08VBTS\uff09\u4e2d\u76ae\u80a4\u5f62\u6001\u5bf9\u7acb\u4f53\u89c6\u89c9\u89e6\u89c9\u611f\u77e5\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStereoTacTip\u7684\u65b0\u578bVBTS\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u7b97\u6cd5\u548c\u6a21\u578b\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u51e0\u4f55\u91cd\u5efa\u3002", "motivation": "\u8bb8\u591a\u57fa\u4e8e\u6807\u8bb0\u7684VBTS\u4f7f\u7528\u590d\u6742\u7684\u4eff\u751f\u76ae\u80a4\u6807\u8bb0\u6392\u5217\uff0c\u5bfc\u81f4\u4ece\u6807\u8bb0\u91cd\u5efa\u76ae\u80a4\u8868\u9762\u51e0\u4f55\u5f62\u72b6\u65f6\u51fa\u73b0\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u6807\u8bb0\u76ae\u80a4\u5f62\u6001\u5982\u4f55\u5f71\u54cd\u7acb\u4f53\u89c6\u89c9\u89e6\u89c9\u611f\u77e5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bVBTS\uff08StereoTacTip\uff09\uff0c\u5e76\u5f15\u5165\u4ee5\u4e0b\u65b9\u6cd5\uff1a(i) \u4f7f\u7528Delaunay\u4e09\u89d2\u73af\u7f16\u7801\u7b97\u6cd5\u8fdb\u884c\u7acb\u4f53\u6807\u8bb0\u5339\u914d\u4e0e\u8ddf\u8e2a\uff1b(ii) \u63d0\u51fa\u6298\u5c04\u6df1\u5ea6\u6821\u6b63\u6a21\u578b\u4ee5\u4fee\u6b63\u5185\u90e8\u4ecb\u8d28\u6298\u5c04\u5f15\u8d77\u7684\u6df1\u5ea6\u5931\u771f\uff1b(iii) \u57fa\u4e8e\u6807\u8bb0\u4f4d\u7f6e\u7684\u76ae\u80a4\u8868\u9762\u6821\u6b63\u6a21\u578b\uff1b(iv) \u591a\u63a5\u89e6\u70b9\u7684\u51e0\u4f55\u91cd\u5efa\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5728\u5927\u89c4\u6a213D\u5730\u56fe\u4e0a\u91cd\u5efa\u5730\u5f62\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u8d21\u732e(i)\u548c(ii)\u867d\u7136\u9488\u5bf9\u4eff\u751f\u6807\u8bb0\u8bbe\u8ba1\uff0c\u4f46\u5e94\u80fd\u63d0\u5347\u6240\u6709\u57fa\u4e8e\u6807\u8bb0\u7684VBTS\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6df1\u5165\u7406\u89e3\u548c\u8bc4\u4f30\u5f62\u6001\u590d\u6742\u7684\u76ae\u80a4\u53ca\u57fa\u4e8e\u6807\u8bb0\u7684\u89e6\u89c9\u4f20\u611f\u5668\u539f\u7406\uff0c\u5bf9\u4e8e\u83b7\u53d6\u7cbe\u786e\u51e0\u4f55\u4fe1\u606f\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2506.18088", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.18088", "abs": "https://arxiv.org/abs/2506.18088", "authors": ["Tianxing Chen", "Zanxin Chen", "Baijun Chen", "Zijian Cai", "Yibin Liu", "Qiwei Liang", "Zixuan Li", "Xianliang Lin", "Yiheng Ge", "Zhenyu Gu", "Weiliang Deng", "Yubin Guo", "Tian Nian", "Xuanbing Xie", "Qiangyu Chen", "Kailun Su", "Tianling Xu", "Guodong Liu", "Mengkang Hu", "Huan-ang Gao", "Kaixuan Wang", "Zhixuan Liang", "Yusen Qin", "Xiaokang Yang", "Ping Luo", "Yao Mu"], "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation", "comment": "Project Page: https://robotwin-platform.github.io/", "summary": "Simulation-based data synthesis has emerged as a powerful paradigm for\nenhancing real-world robotic manipulation. However, existing synthetic datasets\nremain insufficient for robust bimanual manipulation due to two challenges: (1)\nthe lack of an efficient, scalable data generation method for novel tasks, and\n(2) oversimplified simulation environments that fail to capture real-world\ncomplexity. We present RoboTwin 2.0, a scalable simulation framework that\nenables automated, large-scale generation of diverse and realistic data, along\nwith unified evaluation protocols for dual-arm manipulation. We first construct\nRoboTwin-OD, a large-scale object library comprising 731 instances across 147\ncategories, each annotated with semantic and manipulation-relevant labels.\nBuilding on this foundation, we develop an expert data synthesis pipeline that\ncombines multimodal large language models (MLLMs) with simulation-in-the-loop\nrefinement to generate task-level execution code automatically. To improve\nsim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization\nalong five axes: clutter, lighting, background, tabletop height and language\ninstructions, thereby enhancing data diversity and policy robustness. We\ninstantiate this framework across 50 dual-arm tasks spanning five robot\nembodiments, and pre-collect over 100,000 domain-randomized expert\ntrajectories. Empirical results show a 10.9% gain in code generation success\nand improved generalization to novel real-world scenarios. A VLA model\nfine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)\non unseen scene real-world tasks, while zero-shot models trained solely on our\nsynthetic data achieve a 228% relative gain, highlighting strong generalization\nwithout real-world supervision. We release the data generator, benchmark,\ndataset, and code to support scalable research in robust bimanual manipulation.", "AI": {"tldr": "RoboTwin 2.0\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u591a\u6837\u4e14\u771f\u5b9e\u7684\u53cc\u673a\u68b0\u81c2\u64cd\u4f5c\u6570\u636e\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4eff\u771f\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u6210\u529f\u7387\u548c\u73b0\u5b9e\u573a\u666f\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5408\u6210\u6570\u636e\u96c6\u5728\u53cc\u673a\u68b0\u81c2\u64cd\u4f5c\u4e2d\u6548\u7387\u4f4e\u548c\u4eff\u771f\u73af\u5883\u8fc7\u4e8e\u7b80\u5316\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u5927\u89c4\u6a21\u7269\u4f53\u5e93\uff0c\u7ed3\u5408MLLMs\u548c\u4eff\u771f\u4f18\u5316\u751f\u6210\u4efb\u52a1\u7ea7\u4ee3\u7801\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u5316\u57df\u968f\u673a\u5316\u63d0\u5347\u6570\u636e\u591a\u6837\u6027\u3002", "result": "\u4ee3\u7801\u751f\u6210\u6210\u529f\u7387\u63d0\u534710.9%\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u73b0\u5b9e\u4efb\u52a1\u4e2d\u6cdb\u5316\u80fd\u529b\u663e\u8457\u589e\u5f3a\uff08\u6700\u9ad8367%\u76f8\u5bf9\u63d0\u5347\uff09\u3002", "conclusion": "RoboTwin 2.0\u4e3a\u53cc\u673a\u68b0\u81c2\u64cd\u4f5c\u7684\u7a33\u5065\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6570\u636e\u751f\u6210\u548c\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2506.18123", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18123", "abs": "https://arxiv.org/abs/2506.18123", "authors": ["Pranav Atreya", "Karl Pertsch", "Tony Lee", "Moo Jin Kim", "Arhan Jain", "Artur Kuramshin", "Clemens Eppner", "Cyrus Neary", "Edward Hu", "Fabio Ramos", "Jonathan Tremblay", "Kanav Arora", "Kirsty Ellis", "Luca Macesanu", "Matthew Leonard", "Meedeum Cho", "Ozgur Aslan", "Shivin Dass", "Jie Wang", "Xingfang Yuan", "Xuning Yang", "Abhishek Gupta", "Dinesh Jayaraman", "Glen Berseth", "Kostas Daniilidis", "Roberto Martin-Martin", "Youngwoon Lee", "Percy Liang", "Chelsea Finn", "Sergey Levine"], "title": "RoboArena: Distributed Real-World Evaluation of Generalist Robot Policies", "comment": "Website: https://robo-arena.github.io/", "summary": "Comprehensive, unbiased, and comparable evaluation of modern generalist\npolicies is uniquely challenging: existing approaches for robot benchmarking\ntypically rely on heavy standardization, either by specifying fixed evaluation\ntasks and environments, or by hosting centralized ''robot challenges'', and do\nnot readily scale to evaluating generalist policies across a broad range of\ntasks and environments. In this work, we propose RoboArena, a new approach for\nscalable evaluation of generalist robot policies in the real world. Instead of\nstandardizing evaluations around fixed tasks, environments, or locations, we\npropose to crowd-source evaluations across a distributed network of evaluators.\nImportantly, evaluators can freely choose the tasks and environments they\nevaluate on, enabling easy scaling of diversity, but they are required to\nperform double-blind evaluations over pairs of policies. Then, by aggregating\npreference feedback from pairwise comparisons across diverse tasks and\nenvironments, we can derive a ranking of policies. We instantiate our approach\nacross a network of evaluators at seven academic institutions using the DROID\nrobot platform. Through more than 600 pairwise real-robot evaluation episodes\nacross seven generalist policies, we demonstrate that our crowd-sourced\napproach can more accurately rank the performance of existing generalist\npolicies than conventional, centralized evaluation approaches, while being more\nscalable, resilient, and trustworthy. We open our evaluation network to the\ncommunity and hope that it can enable more accessible comparisons of generalist\nrobot policies.", "AI": {"tldr": "\u63d0\u51faRoboArena\uff0c\u4e00\u79cd\u901a\u8fc7\u4f17\u5305\u8bc4\u4f30\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u7684\u65b0\u65b9\u6cd5\uff0c\u53d6\u4ee3\u4f20\u7edf\u56fa\u5b9a\u4efb\u52a1\u8bc4\u4f30\uff0c\u63d0\u9ad8\u591a\u6837\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u4efb\u52a1\u6216\u96c6\u4e2d\u5f0f\u6311\u6218\uff0c\u96be\u4ee5\u6269\u5c55\u548c\u8bc4\u4f30\u901a\u7528\u7b56\u7565\u7684\u591a\u6837\u6027\u3002", "method": "\u901a\u8fc7\u5206\u5e03\u5f0f\u7f51\u7edc\u4f17\u5305\u8bc4\u4f30\uff0c\u8bc4\u4f30\u8005\u81ea\u7531\u9009\u62e9\u4efb\u52a1\u548c\u73af\u5883\uff0c\u8fdb\u884c\u53cc\u76f2\u7b56\u7565\u5bf9\u6bd4\uff0c\u6c47\u603b\u504f\u597d\u53cd\u9988\u751f\u6210\u7b56\u7565\u6392\u540d\u3002", "result": "\u57287\u4e2a\u5b66\u672f\u673a\u6784\u7684600\u591a\u6b21\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u51c6\u786e\u3001\u53ef\u6269\u5c55\u3001\u53ef\u9760\u3002", "conclusion": "RoboArena\u4e3a\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u53ef\u4fe1\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5f00\u653e\u7f51\u7edc\u4ee5\u4fc3\u8fdb\u793e\u533a\u5408\u4f5c\u3002"}}
{"id": "2506.18160", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18160", "abs": "https://arxiv.org/abs/2506.18160", "authors": ["Rutvik Patel", "Alec Kanyuck", "Zachary McNulty", "Zeren Yu", "Lisa Carlson", "Vann Heng", "Brice Johnson", "Satyandra K. Gupta"], "title": "Automated Plan Refinement for Improving Efficiency of Robotic Layup of Composite Sheets", "comment": null, "summary": "The automation of composite sheet layup is essential to meet the increasing\ndemand for composite materials in various industries. However, draping plans\nfor the robotic layup of composite sheets are not robust. A plan that works\nwell under a certain condition does not work well in a different condition.\nChanges in operating conditions due to either changes in material properties or\nworking environment may lead a draping plan to exhibit suboptimal performance.\nIn this paper, we present a comprehensive framework aimed at refining plans\nbased on the observed execution performance. Our framework prioritizes the\nminimization of uncompacted regions while simultaneously improving time\nefficiency. To achieve this, we integrate human expertise with data-driven\ndecision-making to refine expert-crafted plans for diverse production\nenvironments. We conduct experiments to validate the effectiveness of our\napproach, revealing significant reductions in the number of corrective paths\nrequired compared to initial expert-crafted plans. Through a combination of\nempirical data analysis, action-effectiveness modeling, and search-based\nrefinement, our system achieves superior time efficiency in robotic layup.\nExperimental results demonstrate the efficacy of our approach in optimizing the\nlayup process, thereby advancing the state-of-the-art in composite\nmanufacturing automation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u548c\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u590d\u5408\u6750\u6599\u94fa\u5c42\u7684\u673a\u5668\u4eba\u6267\u884c\u8ba1\u5212\uff0c\u51cf\u5c11\u672a\u538b\u5b9e\u533a\u57df\u5e76\u63d0\u9ad8\u65f6\u95f4\u6548\u7387\u3002", "motivation": "\u590d\u5408\u6750\u6599\u94fa\u5c42\u81ea\u52a8\u5316\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u73b0\u6709\u94fa\u5c42\u8ba1\u5212\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u9700\u8981\u4f18\u5316\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684\u751f\u4ea7\u73af\u5883\u3002", "method": "\u6574\u5408\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u4e0e\u6570\u636e\u9a71\u52a8\u51b3\u7b56\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3001\u52a8\u4f5c\u6709\u6548\u6027\u5efa\u6a21\u548c\u57fa\u4e8e\u641c\u7d22\u7684\u4f18\u5316\u6765\u6539\u8fdb\u94fa\u5c42\u8ba1\u5212\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u7ea0\u6b63\u8def\u5f84\u6570\u91cf\uff0c\u63d0\u9ad8\u4e86\u65f6\u95f4\u6548\u7387\uff0c\u4f18\u4e8e\u4e13\u5bb6\u521d\u59cb\u8bbe\u8ba1\u7684\u8ba1\u5212\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u4f18\u5316\u4e86\u673a\u5668\u4eba\u94fa\u5c42\u8fc7\u7a0b\uff0c\u63a8\u52a8\u4e86\u590d\u5408\u6750\u6599\u5236\u9020\u81ea\u52a8\u5316\u7684\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2506.18178", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18178", "abs": "https://arxiv.org/abs/2506.18178", "authors": ["Min Deng", "Bo Fu", "Lingyao Li", "Xi Wang"], "title": "Integrating LLMs and Digital Twins for Adaptive Multi-Robot Task Allocation in Construction", "comment": null, "summary": "Multi-robot systems are emerging as a promising solution to the growing\ndemand for productivity, safety, and adaptability across industrial sectors.\nHowever, effectively coordinating multiple robots in dynamic and uncertain\nenvironments, such as construction sites, remains a challenge, particularly due\nto unpredictable factors like material delays, unexpected site conditions, and\nweather-induced disruptions. To address these challenges, this study proposes\nan adaptive task allocation framework that strategically leverages the\nsynergistic potential of Digital Twins, Integer Programming (IP), and Large\nLanguage Models (LLMs). The multi-robot task allocation problem is formally\ndefined and solved using an IP model that accounts for task dependencies, robot\nheterogeneity, scheduling constraints, and re-planning requirements. A\nmechanism for narrative-driven schedule adaptation is introduced, in which\nunstructured natural language inputs are interpreted by an LLM, and\noptimization constraints are autonomously updated, enabling human-in-the-loop\nflexibility without manual coding. A digital twin-based system has been\ndeveloped to enable real-time synchronization between physical operations and\ntheir digital representations. This closed-loop feedback framework ensures that\nthe system remains dynamic and responsive to ongoing changes on site. A case\nstudy demonstrates both the computational efficiency of the optimization\nalgorithm and the reasoning performance of several LLMs, with top-performing\nmodels achieving over 97% accuracy in constraint and parameter extraction. The\nresults confirm the practicality, adaptability, and cross-domain applicability\nof the proposed methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u3001\u6574\u6570\u89c4\u5212\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u9002\u5e94\u4efb\u52a1\u5206\u914d\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u534f\u8c03\u95ee\u9898\u3002", "motivation": "\u5de5\u4e1a\u9886\u57df\u5bf9\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u5728\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u73af\u5883\uff08\u5982\u5efa\u7b51\u5de5\u5730\uff09\u4e2d\u534f\u8c03\u673a\u5668\u4eba\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u6574\u6570\u89c4\u5212\u6a21\u578b\u89e3\u51b3\u4efb\u52a1\u5206\u914d\u95ee\u9898\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89e3\u6790\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u5e76\u52a8\u6001\u66f4\u65b0\u7ea6\u675f\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u5b9e\u73b0\u5b9e\u65f6\u540c\u6b65\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\u4f18\u5316\u7b97\u6cd5\u9ad8\u6548\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ea6\u675f\u548c\u53c2\u6570\u63d0\u53d6\u4e2d\u51c6\u786e\u7387\u8d85\u8fc797%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u5b9e\u7528\u6027\u3001\u9002\u5e94\u6027\u548c\u8de8\u9886\u57df\u9002\u7528\u6027\u3002"}}
{"id": "2506.18212", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18212", "abs": "https://arxiv.org/abs/2506.18212", "authors": ["Pedro Miguel Uriguen Eljuri", "Hironobu Shibata", "Maeyama Katsuyoshi", "Yuanyuan Jia", "Tadahiro Taniguchi"], "title": "Haptic-ACT -- Pseudo Oocyte Manipulation by a Robot Using Multimodal Information and Action Chunking with Transformers", "comment": "Accepted at IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS2025) Project website\n  https://upedrou.github.io/haptic-act_IROS2025", "summary": "In this paper we introduce Haptic-ACT, an advanced robotic system for pseudo\noocyte manipulation, integrating multimodal information and Action Chunking\nwith Transformers (ACT). Traditional automation methods for oocyte transfer\nrely heavily on visual perception, often requiring human supervision due to\nbiological variability and environmental disturbances. Haptic-ACT enhances ACT\nby incorporating haptic feedback, enabling real-time grasp failure detection\nand adaptive correction. Additionally, we introduce a 3D-printed TPU soft\ngripper to facilitate delicate manipulations. Experimental results demonstrate\nthat Haptic-ACT improves the task success rate, robustness, and adaptability\ncompared to conventional ACT, particularly in dynamic environments. These\nfindings highlight the potential of multimodal learning in robotics for\nbiomedical automation.", "AI": {"tldr": "Haptic-ACT\u662f\u4e00\u79cd\u7ed3\u5408\u89e6\u89c9\u53cd\u9988\u548cACT\u7684\u5148\u8fdb\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u4f2a\u5375\u6bcd\u7ec6\u80de\u64cd\u4f5c\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3001\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edf\u5375\u6bcd\u7ec6\u80de\u8f6c\u79fb\u81ea\u52a8\u5316\u65b9\u6cd5\u4f9d\u8d56\u89c6\u89c9\u611f\u77e5\uff0c\u5e38\u9700\u4eba\u5de5\u76d1\u7763\uff0cHaptic-ACT\u901a\u8fc7\u89e6\u89c9\u53cd\u9988\u589e\u5f3aACT\uff0c\u89e3\u51b3\u751f\u7269\u53d8\u5f02\u548c\u73af\u5883\u5e72\u6270\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u89e6\u89c9\u53cd\u9988\u548cACT\uff0c\u5f15\u51653D\u6253\u5370TPU\u8f6f\u5939\u6301\u5668\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6293\u53d6\u5931\u8d25\u68c0\u6d4b\u548c\u81ea\u9002\u5e94\u6821\u6b63\u3002", "result": "\u5b9e\u9a8c\u8868\u660eHaptic-ACT\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4f18\u4e8e\u4f20\u7edfACT\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3001\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u591a\u6a21\u6001\u5b66\u4e60\u5728\u751f\u7269\u533b\u5b66\u81ea\u52a8\u5316\u673a\u5668\u4eba\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2506.18256", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18256", "abs": "https://arxiv.org/abs/2506.18256", "authors": ["Shuo Jiang", "Boce Hu", "Linfeng Zhao", "Lawson L. S. Wong"], "title": "Robot Tactile Gesture Recognition Based on Full-body Modular E-skin", "comment": null, "summary": "With the development of robot electronic skin technology, various tactile\nsensors, enhanced by AI, are unlocking a new dimension of perception for\nrobots. In this work, we explore how robots equipped with electronic skin can\nrecognize tactile gestures and interpret them as human commands. We developed a\nmodular robot E-skin, composed of multiple irregularly shaped skin patches,\nwhich can be assembled to cover the robot's body while capturing real-time\npressure and pose data from thousands of sensing points. To process this\ninformation, we propose an equivariant graph neural network-based recognizer\nthat efficiently and accurately classifies diverse tactile gestures, including\npoke, grab, stroke, and double-pat. By mapping the recognized gestures to\npredefined robot actions, we enable intuitive human-robot interaction purely\nthrough tactile input.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u4eba\u7535\u5b50\u76ae\u80a4\u6280\u672f\u5982\u4f55\u901a\u8fc7AI\u589e\u5f3a\u7684\u89e6\u89c9\u4f20\u611f\u5668\u8bc6\u522b\u89e6\u89c9\u624b\u52bf\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u4eba\u7c7b\u6307\u4ee4\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u7535\u5b50\u76ae\u80a4\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5982\u4f55\u5229\u7528\u89e6\u89c9\u4f20\u611f\u5668\u5b9e\u73b0\u66f4\u76f4\u89c2\u7684\u4eba\u673a\u4ea4\u4e92\u6210\u4e3a\u7814\u7a76\u91cd\u70b9\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u673a\u5668\u4eba\u7535\u5b50\u76ae\u80a4\uff0c\u7531\u591a\u4e2a\u4e0d\u89c4\u5219\u5f62\u72b6\u7684\u76ae\u80a4\u8865\u4e01\u7ec4\u6210\uff0c\u80fd\u591f\u5b9e\u65f6\u6355\u6349\u538b\u529b\u548c\u59ff\u6001\u6570\u636e\u3002\u91c7\u7528\u7b49\u53d8\u56fe\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\u9ad8\u6548\u51c6\u786e\u5730\u8bc6\u522b\u89e6\u89c9\u624b\u52bf\u3002", "result": "\u7cfb\u7edf\u6210\u529f\u5206\u7c7b\u4e86\u591a\u79cd\u89e6\u89c9\u624b\u52bf\uff08\u5982\u6233\u3001\u6293\u3001\u629a\u6478\u3001\u53cc\u51fb\uff09\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u4e3a\u9884\u5b9a\u4e49\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "conclusion": "\u901a\u8fc7\u89e6\u89c9\u8f93\u5165\u5b9e\u73b0\u4e86\u76f4\u89c2\u7684\u4eba\u673a\u4ea4\u4e92\uff0c\u5c55\u793a\u4e86\u7535\u5b50\u76ae\u80a4\u6280\u672f\u5728\u673a\u5668\u4eba\u611f\u77e5\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.18264", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18264", "abs": "https://arxiv.org/abs/2506.18264", "authors": ["Jagadeswara PKV Pothuri", "Aditya Bhatt", "Prajit KrisshnaKumar", "Manaswin Oddiraju", "Souma Chowdhury"], "title": "Learning Approach to Efficient Vision-based Active Tracking of a Flying Target by an Unmanned Aerial Vehicle", "comment": "AIAA Aviation 2025", "summary": "Autonomous tracking of flying aerial objects has important civilian and\ndefense applications, ranging from search and rescue to counter-unmanned aerial\nsystems (counter-UAS). Ground based tracking requires setting up\ninfrastructure, could be range limited, and may not be feasible in remote\nareas, crowded cities or in dense vegetation areas. Vision based active\ntracking of aerial objects from another airborne vehicle, e.g., a chaser\nunmanned aerial vehicle (UAV), promises to fill this important gap, along with\nserving aerial coordination use cases. Vision-based active tracking by a UAV\nentails solving two coupled problems: 1) compute-efficient and accurate\n(target) object detection and target state estimation; and 2) maneuver\ndecisions to ensure that the target remains in the field of view in the future\ntime-steps and favorably positioned for continued detection. As a solution to\nthe first problem, this paper presents a novel integration of standard deep\nlearning based architectures with Kernelized Correlation Filter (KCF) to\nachieve compute-efficient object detection without compromising accuracy,\nunlike standalone learning or filtering approaches. The proposed perception\nframework is validated using a lab-scale setup. For the second problem, to\nobviate the linearity assumptions and background variations limiting\neffectiveness of the traditional controllers, we present the use of\nreinforcement learning to train a neuro-controller for fast computation of\nvelocity maneuvers. New state space, action space and reward formulations are\ndeveloped for this purpose, and training is performed in simulation using\nAirSim. The trained model is also tested in AirSim with respect to complex\ntarget maneuvers, and is found to outperform a baseline PID control in terms of\ntracking up-time and average distance maintained (from the target) during\ntracking.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0eKCF\u7684\u9ad8\u6548\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u795e\u7ecf\u63a7\u5236\u5668\u89e3\u51b3\u65e0\u4eba\u673a\u89c6\u89c9\u8ddf\u8e2a\u95ee\u9898\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u4f20\u7edfPID\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u5730\u9762\u57fa\u7840\u8bbe\u65bd\u8ddf\u8e2a\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u4e0d\u8db3\uff0c\u5b9e\u73b0\u65e0\u4eba\u673a\u5bf9\u7a7a\u4e2d\u76ee\u6807\u7684\u89c6\u89c9\u4e3b\u52a8\u8ddf\u8e2a\u3002", "method": "1) \u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0eKCF\u5b9e\u73b0\u9ad8\u6548\u76ee\u6807\u68c0\u6d4b\uff1b2) \u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u795e\u7ecf\u63a7\u5236\u5668\u8fdb\u884c\u673a\u52a8\u51b3\u7b56\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u611f\u77e5\u6846\u67b6\u9ad8\u6548\u4e14\u51c6\u786e\uff0c\u795e\u7ecf\u63a7\u5236\u5668\u5728\u590d\u6742\u76ee\u6807\u673a\u52a8\u4e0b\u4f18\u4e8ePID\u63a7\u5236\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65e0\u4eba\u673a\u89c6\u89c9\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u548c\u52a8\u6001\u76ee\u6807\u3002"}}
{"id": "2506.18294", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18294", "abs": "https://arxiv.org/abs/2506.18294", "authors": ["Zhongyuan Li", "Honggang Gou", "Ping Li", "Jiaotong Guo", "Mao Ye"], "title": "Improvement on LiDAR-Camera Calibration Using Square Targets", "comment": null, "summary": "Precise sensor calibration is critical for autonomous vehicles as a\nprerequisite for perception algorithms to function properly. Rotation error of\none degree can translate to position error of meters in target object detection\nat large distance, leading to improper reaction of the system or even safety\nrelated issues. Many methods for multi-sensor calibration have been proposed.\nHowever, there are very few work that comprehensively consider the challenges\nof the calibration procedure when applied to factory manufacturing pipeline or\nafter-sales service scenarios. In this work, we introduce a fully automatic\nLiDAR-camera extrinsic calibration algorithm based on targets that is fast,\neasy to deploy and robust to sensor noises such as missing data. The core of\nthe method include: (1) an automatic multi-stage LiDAR board detection pipeline\nusing only geometry information with no specific material requirement; (2) a\nfast coarse extrinsic parameter search mechanism that is robust to initial\nextrinsic errors; (3) a direct optimization algorithm that is robust to sensor\nnoises. We validate the effectiveness of our methods through experiments on\ndata captured in real world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76ee\u6807\u7684\u5feb\u901f\u3001\u6613\u90e8\u7f72\u4e14\u5bf9\u4f20\u611f\u5668\u566a\u58f0\u9c81\u68d2\u7684\u5168\u81ea\u52a8LiDAR-\u76f8\u673a\u5916\u53c2\u6807\u5b9a\u7b97\u6cd5\u3002", "motivation": "\u7cbe\u786e\u7684\u4f20\u611f\u5668\u6807\u5b9a\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u5de5\u5382\u5236\u9020\u6216\u552e\u540e\u670d\u52a1\u573a\u666f\u7684\u6311\u6218\u3002", "method": "\u5305\u62ec\u81ea\u52a8\u591a\u9636\u6bb5LiDAR\u677f\u68c0\u6d4b\u3001\u5feb\u901f\u7c97\u5916\u53c2\u641c\u7d22\u673a\u5236\u548c\u76f4\u63a5\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u5728\u771f\u5b9e\u573a\u666f\u6570\u636e\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5feb\u901f\u3001\u9c81\u68d2\u4e14\u6613\u4e8e\u90e8\u7f72\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.18343", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18343", "abs": "https://arxiv.org/abs/2506.18343", "authors": ["Kawser Ahmed", "Mir Shahriar Fardin", "Md Arif Faysal Nayem", "Fahim Hafiz", "Swakkhar Shatabda"], "title": "TritonZ: A Remotely Operated Underwater Rover with Manipulator Arm for Exploration and Rescue Operations", "comment": "6 pages, 5 figures", "summary": "The increasing demand for underwater exploration and rescue operations\nenforces the development of advanced wireless or semi-wireless underwater\nvessels equipped with manipulator arms. This paper presents the implementation\nof a semi-wireless underwater vehicle, \"TritonZ\" equipped with a manipulator\narm, tailored for effective underwater exploration and rescue operations. The\nvehicle's compact design enables deployment in different submarine\nsurroundings, addressing the need for wireless systems capable of navigating\nchallenging underwater terrains. The manipulator arm can interact with the\nenvironment, allowing the robot to perform sophisticated tasks during\nexploration and rescue missions in emergency situations. TritonZ is equipped\nwith various sensors such as Pi-Camera, Humidity, and Temperature sensors to\nsend real-time environmental data. Our underwater vehicle controlled using a\ncustomized remote controller can navigate efficiently in the water where\nPi-Camera enables live streaming of the surroundings. Motion control and video\ncapture are performed simultaneously using this camera. The manipulator arm is\ndesigned to perform various tasks, similar to grasping, manipulating, and\ncollecting underwater objects. Experimental results shows the efficacy of the\nproposed remotely operated vehicle in performing a variety of underwater\nexploration and rescue tasks. Additionally, the results show that TritonZ can\nmaintain an average of 13.5cm/s with a minimal delay of 2-3 seconds.\nFurthermore, the vehicle can sustain waves underwater by maintaining its\nposition as well as average velocity. The full project details and source code\ncan be accessed at this link: https://github.com/kawser-ahmed-byte/TritonZ", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u534a\u65e0\u7ebf\u6c34\u4e0b\u8f66\u8f86'TritonZ'\uff0c\u914d\u5907\u673a\u68b0\u81c2\u548c\u591a\u79cd\u4f20\u611f\u5668\uff0c\u7528\u4e8e\u6c34\u4e0b\u63a2\u7d22\u548c\u6551\u63f4\u4efb\u52a1\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u9ad8\u6548\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u6c34\u4e0b\u63a2\u7d22\u548c\u6551\u63f4\u4efb\u52a1\u7684\u9700\u6c42\u589e\u52a0\uff0c\u63a8\u52a8\u4e86\u534a\u65e0\u7ebf\u6c34\u4e0b\u8f66\u8f86\u7684\u53d1\u5c55\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u6c34\u4e0b\u73af\u5883\u7684\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u7d27\u51d1\u7684\u534a\u65e0\u7ebf\u6c34\u4e0b\u8f66\u8f86\uff0c\u914d\u5907\u673a\u68b0\u81c2\u548c\u4f20\u611f\u5668\uff08\u5982Pi-Camera\u3001\u6e29\u6e7f\u5ea6\u4f20\u611f\u5668\uff09\uff0c\u901a\u8fc7\u5b9a\u5236\u9065\u63a7\u5668\u63a7\u5236\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6570\u636e\u4f20\u8f93\u548c\u4efb\u52a1\u6267\u884c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cTritonZ\u80fd\u4ee5\u5e73\u574713.5cm/s\u7684\u901f\u5ea6\u8fd0\u884c\uff0c\u5ef6\u8fdf\u4ec52-3\u79d2\uff0c\u5e76\u80fd\u7a33\u5b9a\u5e94\u5bf9\u6c34\u4e0b\u6ce2\u6d6a\u3002", "conclusion": "TritonZ\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u6c34\u4e0b\u63a2\u7d22\u548c\u6551\u63f4\u5de5\u5177\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\uff0c\u9879\u76ee\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.18355", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18355", "abs": "https://arxiv.org/abs/2506.18355", "authors": ["Qi Jing Chen", "Shilin Shan", "Quang-Cuong Pham"], "title": "Robotic Manipulation of a Rotating Chain with Bottom End Fixed", "comment": "6 pages, 5 figures", "summary": "This paper studies the problem of using a robot arm to manipulate a uniformly\nrotating chain with its bottom end fixed. Existing studies have investigated\nideal rotational shapes for practical applications, yet they do not discuss how\nthese shapes can be consistently achieved through manipulation planning. Our\nwork presents a manipulation strategy for stable and consistent shape\ntransitions. We find that the configuration space of such a chain is\nhomeomorphic to a three-dimensional cube. Using this property, we suggest a\nstrategy to manipulate the chain into different configurations, specifically\nfrom one rotation mode to another, while taking stability and feasibility into\nconsideration. We demonstrate the effectiveness of our strategy in physical\nexperiments by successfully transitioning from rest to the first two rotation\nmodes. The concepts explored in our work has critical applications in ensuring\nsafety and efficiency of drill string and yarn spinning operations.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u673a\u5668\u4eba\u624b\u81c2\u5982\u4f55\u64cd\u7eb5\u5e95\u90e8\u56fa\u5b9a\u7684\u5747\u5300\u65cb\u8f6c\u94fe\u6761\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7a33\u5b9a\u4e14\u4e00\u81f4\u7684\u5f62\u72b6\u8f6c\u6362\u7b56\u7565\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u63a2\u8ba8\u4e86\u7406\u60f3\u65cb\u8f6c\u5f62\u72b6\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u4f46\u672a\u6d89\u53ca\u5982\u4f55\u901a\u8fc7\u64cd\u7eb5\u89c4\u5212\u5b9e\u73b0\u8fd9\u4e9b\u5f62\u72b6\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5229\u7528\u94fe\u6761\u6784\u578b\u7a7a\u95f4\u4e0e\u4e09\u7ef4\u7acb\u65b9\u4f53\u540c\u80da\u7684\u6027\u8d28\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u7a33\u5b9a\u6027\u548c\u53ef\u884c\u6027\u7684\u64cd\u7eb5\u7b56\u7565\u3002", "result": "\u5728\u7269\u7406\u5b9e\u9a8c\u4e2d\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u9759\u6b62\u72b6\u6001\u5230\u524d\u4e24\u79cd\u65cb\u8f6c\u6a21\u5f0f\u7684\u8f6c\u6362\u3002", "conclusion": "\u8be5\u7b56\u7565\u5728\u94bb\u67f1\u548c\u7eb1\u7ebf\u7eba\u4e1d\u64cd\u4f5c\u4e2d\u5177\u6709\u91cd\u8981\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.18365", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.18365", "abs": "https://arxiv.org/abs/2506.18365", "authors": ["Imene Tarakli", "Samuele Vinanzi", "Richard Moore", "Alessandro Di Nuovo"], "title": "Robots and Children that Learn Together : Improving Knowledge Retention by Teaching Peer-Like Interactive Robots", "comment": null, "summary": "Despite growing interest in Learning-by-Teaching (LbT), few studies have\nexplored how this paradigm can be implemented with autonomous, peer-like social\nrobots in real classrooms. Most prior work has relied on scripted or\nWizard-of-Oz behaviors, limiting our understanding of how real-time,\ninteractive learning can be supported by artificial agents. This study\naddresses this gap by introducing Interactive Reinforcement Learning (RL) as a\ncognitive model for teachable social robots. We conducted two between-subject\nexperiments with 58 primary school children, who either taught a robot or\npracticed independently on a tablet while learning French vocabulary\n(memorization) and grammatical rules (inference). The robot, powered by\nInteractive RL, learned from the child's evaluative feedback. Children in the\nLbT condition achieved significantly higher retention gains compared to those\nin the self-practice condition, especially on the grammar task. Learners with\nlower prior knowledge benefited most from teaching the robot. Behavioural\nmetrics revealed that children adapted their teaching strategies over time and\nengaged more deeply during inference tasks. This work makes two contributions:\n(1) it introduces Interactive RL as a pedagogically effective and scalable\nmodel for peer-robot learning, and (2) it demonstrates, for the first time, the\nfeasibility of deploying multiple autonomous robots simultaneously in real\nclassrooms. These findings extend theoretical understanding of LbT by showing\nthat social robots can function not only as passive tutees but as adaptive\npartners that enhance meta-cognitive engagement and long-term learning\noutcomes.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u5728\u771f\u5b9e\u6559\u5ba4\u4e2d\u4f7f\u7528\u81ea\u4e3b\u793e\u4ea4\u673a\u5668\u4eba\u5b9e\u73b0\u201c\u6559\u4e2d\u5b66\u201d\uff08LbT\uff09\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6a21\u578b\uff0c\u53d1\u73b0\u5b66\u751f\u901a\u8fc7\u6559\u673a\u5668\u4eba\u663e\u8457\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u679c\uff0c\u5c24\u5176\u662f\u8bed\u6cd5\u4efb\u52a1\u548c\u4f4e\u57fa\u7840\u5b66\u751f\u3002", "motivation": "\u5c3d\u7ba1\u5bf9\u201c\u6559\u4e2d\u5b66\u201d\u6a21\u5f0f\u5174\u8da3\u589e\u957f\uff0c\u4f46\u5c11\u6709\u7814\u7a76\u5728\u771f\u5b9e\u6559\u5ba4\u4e2d\u63a2\u7d22\u81ea\u4e3b\u793e\u4ea4\u673a\u5668\u4eba\u7684\u5e94\u7528\uff0c\u73b0\u6709\u7814\u7a76\u591a\u4f9d\u8d56\u811a\u672c\u6216Wizard-of-Oz\u65b9\u6cd5\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u4ea4\u4e92\u5b66\u4e60\u7684\u7406\u89e3\u3002", "method": "\u91c7\u7528\u4ea4\u4e92\u5f0f\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4f5c\u4e3a\u53ef\u6559\u793e\u4ea4\u673a\u5668\u4eba\u7684\u8ba4\u77e5\u6a21\u578b\uff0c\u8fdb\u884c\u4e24\u9879\u5b9e\u9a8c\uff0c58\u540d\u5c0f\u5b66\u751f\u5206\u522b\u6559\u673a\u5668\u4eba\u6216\u72ec\u7acb\u7ec3\u4e60\u6cd5\u8bed\u8bcd\u6c47\u548c\u8bed\u6cd5\u3002", "result": "LbT\u7ec4\u5b66\u751f\u5728\u8bed\u6cd5\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u81ea\u4e3b\u7ec3\u4e60\u7ec4\uff0c\u4f4e\u57fa\u7840\u5b66\u751f\u53d7\u76ca\u6700\u5927\uff1b\u884c\u4e3a\u6570\u636e\u663e\u793a\u5b66\u751f\u8c03\u6574\u6559\u5b66\u7b56\u7565\u5e76\u66f4\u6df1\u5165\u53c2\u4e0e\u63a8\u7406\u4efb\u52a1\u3002", "conclusion": "\u4ea4\u4e92\u5f0fRL\u662f\u6559\u80b2\u673a\u5668\u4eba\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u6a21\u578b\uff0c\u9996\u6b21\u8bc1\u660e\u5728\u771f\u5b9e\u6559\u5ba4\u4e2d\u540c\u65f6\u90e8\u7f72\u591a\u4e2a\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u53ef\u884c\u6027\uff0c\u673a\u5668\u4eba\u53ef\u4f5c\u4e3a\u9002\u5e94\u6027\u4f19\u4f34\u63d0\u5347\u5143\u8ba4\u77e5\u53c2\u4e0e\u548c\u957f\u671f\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2506.18410", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18410", "abs": "https://arxiv.org/abs/2506.18410", "authors": ["Zhe Zhang", "Peijia Xie", "Zhirui Sun", "Bingyi Xia", "Bi-Ke Zhu", "Jiankun Wang"], "title": "Integrating Maneuverable Planning and Adaptive Control for Robot Cart-Pushing under Disturbances", "comment": "11 pages, 11 figures", "summary": "Precise and flexible cart-pushing is a challenging task for mobile robots.\nThe motion constraints during cart-pushing and the robot's redundancy lead to\ncomplex motion planning problems, while variable payloads and disturbances\npresent complicated dynamics. In this work, we propose a novel planning and\ncontrol framework for flexible whole-body coordination and robust adaptive\ncontrol. Our motion planning method employs a local coordinate representation\nand a novel kinematic model to solve a nonlinear optimization problem, thereby\nenhancing motion maneuverability by generating feasible and flexible push\nposes. Furthermore, we present a disturbance rejection control method to resist\ndisturbances and reduce control errors for the complex control problem without\nrequiring an accurate dynamic model. We validate our method through extensive\nexperiments in simulation and real-world settings, demonstrating its\nsuperiority over existing approaches. To the best of our knowledge, this is the\nfirst work to systematically evaluate the flexibility and robustness of\ncart-pushing methods in experiments. The video supplement is available at\nhttps://sites.google.com/view/mpac-pushing/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u7cbe\u786e\u7075\u6d3b\u63a8\u8f66\u7684\u89c4\u5212\u548c\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u5c40\u90e8\u5750\u6807\u8868\u793a\u548c\u65b0\u578b\u8fd0\u52a8\u5b66\u6a21\u578b\u4f18\u5316\u8fd0\u52a8\u89c4\u5212\uff0c\u5e76\u91c7\u7528\u6297\u5e72\u6270\u63a7\u5236\u65b9\u6cd5\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u63a8\u8f66\u4efb\u52a1\u4e2d\u7684\u8fd0\u52a8\u7ea6\u675f\u3001\u673a\u5668\u4eba\u5197\u4f59\u4ee5\u53ca\u52a8\u6001\u8d1f\u8f7d\u548c\u5e72\u6270\u5bfc\u81f4\u590d\u6742\u7684\u8fd0\u52a8\u89c4\u5212\u548c\u52a8\u529b\u5b66\u95ee\u9898\uff0c\u9700\u8981\u7075\u6d3b\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5c40\u90e8\u5750\u6807\u8868\u793a\u548c\u65b0\u578b\u8fd0\u52a8\u5b66\u6a21\u578b\u8fdb\u884c\u975e\u7ebf\u6027\u4f18\u5316\u89c4\u5212\uff0c\u7ed3\u5408\u6297\u5e72\u6270\u63a7\u5236\u65b9\u6cd5\u5904\u7406\u590d\u6742\u63a7\u5236\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u63a8\u8f66\u65b9\u6cd5\u7684\u7075\u6d3b\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u63a8\u8f66\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.18443", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18443", "abs": "https://arxiv.org/abs/2506.18443", "authors": ["Yang Lyu", "Zhenghao Zou", "Yanfeng Li", "Chunhui Zhao", "Quan Pan"], "title": "Radar and Event Camera Fusion for Agile Robot Ego-Motion Estimation", "comment": null, "summary": "Achieving reliable ego motion estimation for agile robots, e.g., aerobatic\naircraft, remains challenging because most robot sensors fail to respond timely\nand clearly to highly dynamic robot motions, often resulting in measurement\nblurring, distortion, and delays. In this paper, we propose an IMU-free and\nfeature-association-free framework to achieve aggressive ego-motion velocity\nestimation of a robot platform in highly dynamic scenarios by combining two\ntypes of exteroceptive sensors, an event camera and a millimeter wave radar,\nFirst, we used instantaneous raw events and Doppler measurements to derive\nrotational and translational velocities directly. Without a sophisticated\nassociation process between measurement frames, the proposed method is more\nrobust in texture-less and structureless environments and is more\ncomputationally efficient for edge computing devices. Then, in the back-end, we\npropose a continuous-time state-space model to fuse the hybrid time-based and\nevent-based measurements to estimate the ego-motion velocity in a fixed-lagged\nsmoother fashion. In the end, we validate our velometer framework extensively\nin self-collected experiment datasets. The results indicate that our IMU-free\nand association-free ego motion estimation framework can achieve reliable and\nefficient velocity output in challenging environments. The source code,\nillustrative video and dataset are available at\nhttps://github.com/ZzhYgwh/TwistEstimator.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u548c\u6beb\u7c73\u6ce2\u96f7\u8fbe\u7684\u65e0IMU\u548c\u65e0\u7279\u5f81\u5173\u8054\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u901f\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u9ad8\u52a8\u6001\u573a\u666f\u3002", "motivation": "\u9ad8\u52a8\u6001\u673a\u5668\u4eba\u8fd0\u52a8\u65f6\uff0c\u4f20\u7edf\u4f20\u611f\u5668\u56e0\u6d4b\u91cf\u6a21\u7cca\u3001\u5931\u771f\u548c\u5ef6\u8fdf\u96be\u4ee5\u5b9e\u73b0\u53ef\u9760\u7684\u8fd0\u52a8\u4f30\u8ba1\u3002", "method": "\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u548c\u6beb\u7c73\u6ce2\u96f7\u8fbe\uff0c\u76f4\u63a5\u5229\u7528\u539f\u59cb\u4e8b\u4ef6\u548c\u591a\u666e\u52d2\u6d4b\u91cf\u63a8\u5bfc\u901f\u5ea6\uff0c\u540e\u7aef\u91c7\u7528\u8fde\u7eed\u65f6\u95f4\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u8fdb\u884c\u878d\u5408\u3002", "result": "\u5728\u81ea\u91c7\u96c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6846\u67b6\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u901f\u5ea6\u8f93\u51fa\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u52a8\u6001\u573a\u666f\u4e0b\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0IMU\u548c\u65e0\u7279\u5f81\u5173\u8054\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.18448", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18448", "abs": "https://arxiv.org/abs/2506.18448", "authors": ["Quang Nguyen", "Tri Le", "Huy Nguyen", "Thieu Vo", "Tung D. Ta", "Baoru Huang", "Minh N. Vu", "Anh Nguyen"], "title": "GraspMAS: Zero-Shot Language-driven Grasp Detection with Multi-Agent System", "comment": "8 pages, accepted to IROS 2025", "summary": "Language-driven grasp detection has the potential to revolutionize\nhuman-robot interaction by allowing robots to understand and execute grasping\ntasks based on natural language commands. However, existing approaches face two\nkey challenges. First, they often struggle to interpret complex text\ninstructions or operate ineffectively in densely cluttered environments.\nSecond, most methods require a training or finetuning step to adapt to new\ndomains, limiting their generation in real-world applications. In this paper,\nwe introduce GraspMAS, a new multi-agent system framework for language-driven\ngrasp detection. GraspMAS is designed to reason through ambiguities and improve\ndecision-making in real-world scenarios. Our framework consists of three\nspecialized agents: Planner, responsible for strategizing complex queries;\nCoder, which generates and executes source code; and Observer, which evaluates\nthe outcomes and provides feedback. Intensive experiments on two large-scale\ndatasets demonstrate that our GraspMAS significantly outperforms existing\nbaselines. Additionally, robot experiments conducted in both simulation and\nreal-world settings further validate the effectiveness of our approach.", "AI": {"tldr": "GraspMAS\u662f\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\uff0c\u7528\u4e8e\u8bed\u8a00\u9a71\u52a8\u7684\u6293\u53d6\u68c0\u6d4b\uff0c\u901a\u8fc7\u4e09\u4e2a\u667a\u80fd\u4f53\uff08Planner\u3001Coder\u3001Observer\uff09\u89e3\u51b3\u590d\u6742\u6307\u4ee4\u89e3\u91ca\u548c\u5bc6\u96c6\u73af\u5883\u64cd\u4f5c\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u9a71\u52a8\u6293\u53d6\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u6307\u4ee4\u548c\u5bc6\u96c6\u73af\u5883\uff0c\u4e14\u9700\u989d\u5916\u8bad\u7ec3\u9002\u5e94\u65b0\u9886\u57df\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faGraspMAS\u6846\u67b6\uff0c\u5305\u542bPlanner\uff08\u7b56\u7565\u89c4\u5212\uff09\u3001Coder\uff08\u4ee3\u7801\u751f\u6210\u4e0e\u6267\u884c\uff09\u3001Observer\uff08\u7ed3\u679c\u8bc4\u4f30\u4e0e\u53cd\u9988\uff09\u4e09\u4e2a\u667a\u80fd\u4f53\u3002", "result": "\u5728\u4e24\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "GraspMAS\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u89e3\u51b3\u4e86\u8bed\u8a00\u9a71\u52a8\u6293\u53d6\u68c0\u6d4b\u7684\u5173\u952e\u6311\u6218\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.18454", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18454", "abs": "https://arxiv.org/abs/2506.18454", "authors": ["Alejandro Romero", "Gianluca Baldassarre", "Richard J. Duro", "Vieri Giuliano Santucci"], "title": "A Motivational Architecture for Open-Ended Learning Challenges in Robots", "comment": "Accepted to RLDM 2025", "summary": "Developing agents capable of autonomously interacting with complex and\ndynamic environments, where task structures may change over time and prior\nknowledge cannot be relied upon, is a key prerequisite for deploying artificial\nsystems in real-world settings. The open-ended learning framework identifies\nthe core challenges for creating such agents, including the ability to\nautonomously generate new goals, acquire the necessary skills (or curricula of\nskills) to achieve them, and adapt to non-stationary environments. While many\nexisting works tackles various aspects of these challenges in isolation, few\npropose integrated solutions that address them simultaneously. In this paper,\nwe introduce H-GRAIL, a hierarchical architecture that, through the use of\ndifferent typologies of intrinsic motivations and interconnected learning\nmechanisms, autonomously discovers new goals, learns the required skills for\ntheir achievement, generates skill sequences for tackling interdependent tasks,\nand adapts to non-stationary environments. We tested H-GRAIL in a real robotic\nscenario, demonstrating how the proposed solutions effectively address the\nvarious challenges of open-ended learning.", "AI": {"tldr": "H-GRAIL\u662f\u4e00\u79cd\u5206\u5c42\u67b6\u6784\uff0c\u901a\u8fc7\u5185\u5728\u52a8\u673a\u548c\u4e92\u8054\u5b66\u4e60\u673a\u5236\uff0c\u81ea\u4e3b\u53d1\u73b0\u76ee\u6807\u3001\u5b66\u4e60\u6280\u80fd\u5e76\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002", "motivation": "\u5f00\u53d1\u80fd\u5728\u52a8\u6001\u73af\u5883\u4e2d\u81ea\u4e3b\u4ea4\u4e92\u7684\u667a\u80fd\u4f53\uff0c\u89e3\u51b3\u5f00\u653e\u5b66\u4e60\u4e2d\u7684\u76ee\u6807\u751f\u6210\u3001\u6280\u80fd\u83b7\u53d6\u548c\u73af\u5883\u9002\u5e94\u95ee\u9898\u3002", "method": "\u63d0\u51faH-GRAIL\u5206\u5c42\u67b6\u6784\uff0c\u7ed3\u5408\u591a\u79cd\u5185\u5728\u52a8\u673a\u548c\u4e92\u8054\u5b66\u4e60\u673a\u5236\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86H-GRAIL\u7684\u6709\u6548\u6027\u3002", "conclusion": "H-GRAIL\u4e3a\u5f00\u653e\u5b66\u4e60\u4e2d\u7684\u591a\u6311\u6218\u63d0\u4f9b\u4e86\u96c6\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.18466", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.18466", "abs": "https://arxiv.org/abs/2506.18466", "authors": ["Matti Kr\u00fcger", "Daniel Tanneberg", "Chao Wang", "Stephan Hasler", "Michael Gienger"], "title": "Mirror Eyes: Explainable Human-Robot Interaction at a Glance", "comment": "Accepted to the 34th IEEE International Conference on Robot and Human\n  Interactive Communication (RO-MAN)", "summary": "The gaze of a person tends to reflect their interest. This work explores what\nhappens when this statement is taken literally and applied to robots. Here we\npresent a robot system that employs a moving robot head with a screen-based eye\nmodel that can direct the robot's gaze to points in physical space and present\na reflection-like mirror image of the attended region on top of each eye. We\nconducted a user study with 33 participants, who were asked to instruct the\nrobot to perform pick-and-place tasks, monitor the robot's task execution, and\ninterrupt it in case of erroneous actions. Despite a deliberate lack of\ninstructions about the role of the eyes and a very brief system exposure,\nparticipants felt more aware about the robot's information processing, detected\nerroneous actions earlier, and rated the user experience higher when eye-based\nmirroring was enabled compared to non-reflective eyes. These results suggest a\nbeneficial and intuitive utilization of the introduced method in cooperative\nhuman-robot interaction.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u673a\u5668\u4eba\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u51dd\u89c6\u884c\u4e3a\uff08\u5728\u773c\u775b\u4e0a\u663e\u793a\u6ce8\u89c6\u533a\u57df\u7684\u955c\u50cf\uff09\u5982\u4f55\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u4f53\u9a8c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u80fd\u589e\u5f3a\u7528\u6237\u5bf9\u673a\u5668\u4eba\u4fe1\u606f\u5904\u7406\u7684\u611f\u77e5\uff0c\u5e76\u63d0\u9ad8\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u9519\u8bef\u68c0\u6d4b\u901f\u5ea6\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u4eba\u6a21\u62df\u4eba\u7c7b\u51dd\u89c6\u884c\u4e3a\u662f\u5426\u80fd\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u7684\u76f4\u89c2\u6027\u548c\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5e26\u5c4f\u5e55\u773c\u775b\u6a21\u578b\u7684\u673a\u5668\u4eba\u5934\u90e8\u7cfb\u7edf\uff0c\u53ef\u663e\u793a\u6ce8\u89c6\u533a\u57df\u7684\u955c\u50cf\u3002\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff0833\u4eba\uff09\u8bc4\u4f30\u5176\u6548\u679c\u3002", "result": "\u7528\u6237\u5728\u4f7f\u7528\u5e26\u955c\u50cf\u773c\u775b\u7684\u673a\u5668\u4eba\u65f6\uff0c\u5bf9\u673a\u5668\u4eba\u4fe1\u606f\u5904\u7406\u7684\u611f\u77e5\u66f4\u5f3a\uff0c\u9519\u8bef\u68c0\u6d4b\u66f4\u5feb\uff0c\u7528\u6237\u4f53\u9a8c\u8bc4\u5206\u66f4\u9ad8\u3002", "conclusion": "\u6a21\u62df\u4eba\u7c7b\u51dd\u89c6\u884c\u4e3a\u7684\u673a\u5668\u4eba\u773c\u775b\u8bbe\u8ba1\u5728\u4eba\u673a\u534f\u4f5c\u4e2d\u5177\u6709\u76f4\u89c2\u4e14\u6709\u76ca\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.18526", "categories": ["cs.RO", "math.DS"], "pdf": "https://arxiv.org/pdf/2506.18526", "abs": "https://arxiv.org/abs/2506.18526", "authors": ["Dhruv Sorathiya", "Sarthak Sahoo", "Vivek Natarajan"], "title": "Design, fabrication and control of a cable-driven parallel robot", "comment": "4 pages, 8 fugures", "summary": "In cable driven parallel robots (CDPRs), the payload is suspended using a\nnetwork of cables whose length can be controlled to maneuver the payload within\nthe workspace. Compared to rigid link robots, CDPRs provide better\nmaneuverability due to the flexibility of the cables and consume lesser power\ndue to the high strength-to-weight ratio of the cables. However, amongst other\nthings, the flexibility of the cables and the fact that they can only pull (and\nnot push) render the dynamics of CDPRs complex. Hence advanced modelling\nparadigms and control algorithms must be developed to fully utilize the\npotential of CDPRs. Furthermore, given the complex dynamics of CDPRs, the\nmodels and control algorithms proposed for them must be validated on\nexperimental setups to ascertain their efficacy in practice. We have recently\ndeveloped an elaborate experimental setup for a CDPR with three cables and\nvalidated elementary open-loop motion planning algorithms on it. In this paper,\nwe describe several aspects of the design and fabrication of our setup,\nincluding component selection and assembly, and present our experimental\nresults. Our setup can reproduce complex phenomenon such as the transverse\nvibration of the cables seen in large CDPRs and will in the future be used to\nmodel and control such phenomenon and also to validate more sophisticated\nmotion planning algorithms.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u7535\u7f06\u9a71\u52a8\u5e76\u8054\u673a\u5668\u4eba\uff08CDPRs\uff09\u7684\u5b9e\u9a8c\u88c5\u7f6e\u8bbe\u8ba1\u4e0e\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5176\u590d\u6742\u52a8\u529b\u5b66\u73b0\u8c61\uff08\u5982\u7535\u7f06\u632f\u52a8\uff09\u7684\u590d\u73b0\u80fd\u529b\uff0c\u5e76\u8ba1\u5212\u7528\u4e8e\u672a\u6765\u9ad8\u7ea7\u63a7\u5236\u7b97\u6cd5\u7684\u9a8c\u8bc1\u3002", "motivation": "CDPRs\u56e0\u5176\u9ad8\u7075\u6d3b\u6027\u548c\u4f4e\u529f\u8017\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5176\u590d\u6742\u52a8\u529b\u5b66\u7279\u6027\u9700\u8981\u5148\u8fdb\u7684\u5efa\u6a21\u548c\u63a7\u5236\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "method": "\u8bbe\u8ba1\u5e76\u642d\u5efa\u4e86\u4e00\u4e2a\u4e09\u7535\u7f06CDPR\u5b9e\u9a8c\u88c5\u7f6e\uff0c\u9a8c\u8bc1\u4e86\u57fa\u672c\u7684\u5f00\u73af\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u88c5\u7f6e\u6210\u529f\u590d\u73b0\u4e86\u5927\u578bCDPR\u4e2d\u7684\u7535\u7f06\u6a2a\u5411\u632f\u52a8\u7b49\u590d\u6742\u73b0\u8c61\u3002", "conclusion": "\u8be5\u5b9e\u9a8c\u88c5\u7f6e\u4e3a\u672a\u6765\u590d\u6742\u52a8\u529b\u5b66\u5efa\u6a21\u3001\u63a7\u5236\u7b97\u6cd5\u9a8c\u8bc1\u53ca\u9ad8\u7ea7\u8fd0\u52a8\u89c4\u5212\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.18580", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18580", "abs": "https://arxiv.org/abs/2506.18580", "authors": ["Jan Michalczyk", "Stephan Weiss", "Jan Steinbrener"], "title": "Learning Point Correspondences In Radar 3D Point Clouds For Radar-Inertial Odometry", "comment": null, "summary": "Using 3D point clouds in odometry estimation in robotics often requires\nfinding a set of correspondences between points in subsequent scans. While\nthere are established methods for point clouds of sufficient quality,\nstate-of-the-art still struggles when this quality drops. Thus, this paper\npresents a novel learning-based framework for predicting robust point\ncorrespondences between pairs of noisy, sparse and unstructured 3D point clouds\nfrom a light-weight, low-power, inexpensive, consumer-grade System-on-Chip\n(SoC) Frequency Modulated Continuous Wave (FMCW) radar sensor. Our network is\nbased on the transformer architecture which allows leveraging the attention\nmechanism to discover pairs of points in consecutive scans with the greatest\nmutual affinity. The proposed network is trained in a self-supervised way using\nset-based multi-label classification cross-entropy loss, where the ground-truth\nset of matches is found by solving the Linear Sum Assignment (LSA) optimization\nproblem, which avoids tedious hand annotation of the training data.\nAdditionally, posing the loss calculation as multi-label classification permits\nsupervising on point correspondences directly instead of on odometry error,\nwhich is not feasible for sparse and noisy data from the SoC radar we use. We\nevaluate our method with an open-source state-of-the-art Radar-Inertial\nOdometry (RIO) framework in real-world Unmanned Aerial Vehicle (UAV) flights\nand with the widely used public Coloradar dataset. Evaluation shows that the\nproposed method improves the position estimation accuracy by over 14 % and 19 %\non average, respectively. The open source code and datasets can be found here:\nhttps://github.com/aau-cns/radar_transformer.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4f4e\u8d28\u91cf3D\u70b9\u4e91\u4e2d\u9884\u6d4b\u9c81\u68d2\u7684\u70b9\u5bf9\u5e94\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f7\u8fbe\u60ef\u6027\u91cc\u7a0b\u8ba1\u7684\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u70b9\u4e91\u8d28\u91cf\u4e0b\u964d\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6210\u672c\u96f7\u8fbe\u4f20\u611f\u5668\u4ea7\u751f\u7684\u7a00\u758f\u3001\u566a\u58f0\u6570\u636e\u4e2d\u3002", "method": "\u4f7f\u7528Transformer\u67b6\u6784\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u901a\u8fc7\u591a\u6807\u7b7e\u5206\u7c7b\u4ea4\u53c9\u71b5\u635f\u5931\u8bad\u7ec3\u7f51\u7edc\uff0c\u907f\u514d\u624b\u52a8\u6807\u6ce8\u3002", "result": "\u5728\u771f\u5b9e\u65e0\u4eba\u673a\u98de\u884c\u548c\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u4f4d\u7f6e\u4f30\u8ba1\u7cbe\u5ea6\u5206\u522b\u63d0\u9ad8\u4e8614%\u548c19%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u8d28\u91cf\u70b9\u4e91\u4e2d\u7684\u5bf9\u5e94\u5173\u7cfb\u95ee\u9898\uff0c\u4e3a\u4f4e\u6210\u672c\u96f7\u8fbe\u4f20\u611f\u5668\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.18583", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18583", "abs": "https://arxiv.org/abs/2506.18583", "authors": ["Nikhil Khedekar", "Kostas Alexis"], "title": "PG-LIO: Photometric-Geometric fusion for Robust LiDAR-Inertial Odometry", "comment": "8 pages, 6 figures", "summary": "LiDAR-Inertial Odometry (LIO) is widely used for accurate state estimation\nand mapping which is an essential requirement for autonomous robots.\nConventional LIO methods typically rely on formulating constraints from the\ngeometric structure sampled by the LiDAR. Hence, in the lack of geometric\nstructure, these tend to become ill-conditioned (degenerate) and fail.\nRobustness of LIO to such conditions is a necessity for its broader deployment.\nTo address this, we propose PG-LIO, a real-time LIO method that fuses\nphotometric and geometric information sampled by the LiDAR along with inertial\nconstraints from an Inertial Measurement Unit (IMU). This multi-modal\ninformation is integrated into a factor graph optimized over a sliding window\nfor real-time operation. We evaluate PG-LIO on multiple datasets that include\nboth geometrically well-conditioned as well as self-similar scenarios. Our\nmethod achieves accuracy on par with state-of-the-art LIO in geometrically\nwell-structured settings while significantly improving accuracy in degenerate\ncases including against methods that also fuse intensity. Notably, we\ndemonstrate only 1 m drift over a 1 km manually piloted aerial trajectory\nthrough a geometrically self-similar tunnel at an average speed of 7.5m/s (max\nspeed 10.8 m/s). For the benefit of the community, we shall also release our\nsource code https://github.com/ntnu-arl/mimosa", "AI": {"tldr": "PG-LIO\u662f\u4e00\u79cd\u878d\u5408LiDAR\u7684\u5149\u5ea6\u548c\u51e0\u4f55\u4fe1\u606f\u4ee5\u53caIMU\u60ef\u6027\u7ea6\u675f\u7684\u5b9e\u65f6LIO\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u51e0\u4f55\u7ed3\u6784\u7f3a\u5931\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edfLIO\u65b9\u6cd5\u5728\u51e0\u4f55\u7ed3\u6784\u7f3a\u5931\u65f6\u5bb9\u6613\u5931\u6548\uff0c\u9650\u5236\u4e86\u5176\u5728\u66f4\u5e7f\u6cdb\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "PG-LIO\u901a\u8fc7\u5c06\u591a\u6a21\u6001\u4fe1\u606f\uff08\u5149\u5ea6\u3001\u51e0\u4f55\u548cIMU\uff09\u96c6\u6210\u5230\u6ed1\u52a8\u7a97\u53e3\u56e0\u5b50\u56fe\u4e2d\u8fdb\u884c\u5b9e\u65f6\u4f18\u5316\u3002", "result": "\u5728\u51e0\u4f55\u7ed3\u6784\u826f\u597d\u548c\u81ea\u76f8\u4f3c\u573a\u666f\u4e2d\uff0cPG-LIO\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u9000\u5316\u60c5\u51b5\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PG-LIO\u5728\u51e0\u4f55\u7ed3\u6784\u7f3a\u5931\u60c5\u51b5\u4e0b\u5177\u6709\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u5e94\u7528\u3002"}}
{"id": "2506.18689", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18689", "abs": "https://arxiv.org/abs/2506.18689", "authors": ["Alessandro Saviolo", "Giuseppe Loianno"], "title": "NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed Target Tracking in Unstructured GPS-Denied Environments", "comment": null, "summary": "Autonomous aerial target tracking in unstructured and GPS-denied environments\nremains a fundamental challenge in robotics. Many existing methods rely on\nmotion capture systems, pre-mapped scenes, or feature-based localization to\nensure safety and control, limiting their deployment in real-world conditions.\nWe introduce NOVA, a fully onboard, object-centric framework that enables\nrobust target tracking and collision-aware navigation using only a stereo\ncamera and an IMU. Rather than constructing a global map or relying on absolute\nlocalization, NOVA formulates perception, estimation, and control entirely in\nthe target's reference frame. A tightly integrated stack combines a lightweight\nobject detector with stereo depth completion, followed by histogram-based\nfiltering to infer robust target distances under occlusion and noise. These\nmeasurements feed a visual-inertial state estimator that recovers the full\n6-DoF pose of the robot relative to the target. A nonlinear model predictive\ncontroller (NMPC) plans dynamically feasible trajectories in the target frame.\nTo ensure safety, high-order control barrier functions are constructed online\nfrom a compact set of high-risk collision points extracted from depth, enabling\nreal-time obstacle avoidance without maps or dense representations. We validate\nNOVA across challenging real-world scenarios, including urban mazes, forest\ntrails, and repeated transitions through buildings with intermittent GPS loss\nand severe lighting changes that disrupt feature-based localization. Each\nexperiment is repeated multiple times under similar conditions to assess\nresilience, showing consistent and reliable performance. NOVA achieves agile\ntarget following at speeds exceeding 50 km/h. These results show that\nhigh-speed vision-based tracking is possible in the wild using only onboard\nsensing, with no reliance on external localization or environment assumptions.", "AI": {"tldr": "NOVA\u662f\u4e00\u4e2a\u57fa\u4e8e\u7acb\u4f53\u76f8\u673a\u548cIMU\u7684\u81ea\u4e3b\u7a7a\u4e2d\u76ee\u6807\u8ddf\u8e2a\u6846\u67b6\uff0c\u65e0\u9700\u5168\u5c40\u5730\u56fe\u6216\u7edd\u5bf9\u5b9a\u4f4d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u901f\u3001\u9c81\u68d2\u7684\u76ee\u6807\u8ddf\u8e2a\u548c\u907f\u969c\u3002", "motivation": "\u89e3\u51b3\u5728\u65e0\u7ed3\u6784\u548cGPS\u7f3a\u5931\u73af\u5883\u4e2d\u81ea\u4e3b\u7a7a\u4e2d\u76ee\u6807\u8ddf\u8e2a\u7684\u6311\u6218\uff0c\u907f\u514d\u4f9d\u8d56\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u6216\u9884\u5efa\u5730\u56fe\u3002", "method": "\u7ed3\u5408\u8f7b\u91cf\u7ea7\u76ee\u6807\u68c0\u6d4b\u3001\u7acb\u4f53\u6df1\u5ea6\u8865\u5168\u3001\u76f4\u65b9\u56fe\u6ee4\u6ce2\u3001\u89c6\u89c9\u60ef\u6027\u72b6\u6001\u4f30\u8ba1\u548c\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\uff0c\u5b9e\u73b0\u76ee\u6807\u53c2\u8003\u6846\u67b6\u4e0b\u7684\u611f\u77e5\u3001\u4f30\u8ba1\u548c\u63a7\u5236\u3002", "result": "\u5728\u590d\u6742\u573a\u666f\uff08\u5982\u57ce\u5e02\u8ff7\u5bab\u3001\u68ee\u6797\u5c0f\u5f84\uff09\u4e2d\u5b9e\u73b0\u8d85\u8fc750 km/h\u7684\u9ad8\u901f\u76ee\u6807\u8ddf\u8e2a\uff0c\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "NOVA\u8bc1\u660e\u4e86\u4ec5\u4f9d\u9760\u673a\u8f7d\u4f20\u611f\u5668\u5373\u53ef\u5b9e\u73b0\u9ad8\u901f\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\uff0c\u65e0\u9700\u5916\u90e8\u5b9a\u4f4d\u6216\u73af\u5883\u5047\u8bbe\u3002"}}
{"id": "2506.18697", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18697", "abs": "https://arxiv.org/abs/2506.18697", "authors": ["Marios-Nektarios Stamatopoulos", "Shridhar Velhal", "Avijit Banerjee", "George Nikolakopoulos"], "title": "Safety-Aware Optimal Scheduling for Autonomous Masonry Construction using Collaborative Heterogeneous Aerial Robots", "comment": "This paper has been accepted for publication at the 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)", "summary": "This paper presents a novel high-level task planning and optimal coordination\nframework for autonomous masonry construction, using a team of heterogeneous\naerial robotic workers, consisting of agents with separate skills for brick\nplacement and mortar application. This introduces new challenges in scheduling\nand coordination, particularly due to the mortar curing deadline required for\nstructural bonding and ensuring the safety constraints among UAVs operating in\nparallel. To address this, an automated pipeline generates the wall\nconstruction plan based on the available bricks while identifying static\nstructural dependencies and potential conflicts for safe operation. The\nproposed framework optimizes UAV task allocation and execution timing by\nincorporating dynamically coupled precedence deadline constraints that account\nfor the curing process and static structural dependency constraints, while\nenforcing spatio-temporal constraints to prevent collisions and ensure safety.\nThe primary objective of the scheduler is to minimize the overall construction\nmakespan while minimizing logistics, traveling time between tasks, and the\ncuring time to maintain both adhesion quality and safe workspace separation.\nThe effectiveness of the proposed method in achieving coordinated and\ntime-efficient aerial masonry construction is extensively validated through\nGazebo simulated missions. The results demonstrate the framework's capability\nto streamline UAV operations, ensuring both structural integrity and safety\nduring the construction process.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u4e3b\u780c\u4f53\u65bd\u5de5\u7684\u9ad8\u5c42\u4efb\u52a1\u89c4\u5212\u548c\u4f18\u5316\u534f\u8c03\u6846\u67b6\uff0c\u5229\u7528\u5f02\u6784\u7a7a\u4e2d\u673a\u5668\u4eba\u56e2\u961f\u89e3\u51b3\u7816\u5757\u653e\u7f6e\u548c\u7802\u6d46\u5e94\u7528\u7684\u8c03\u5ea6\u4e0e\u534f\u8c03\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u7802\u6d46\u56fa\u5316\u671f\u9650\u548c\u65e0\u4eba\u673a\u5e76\u884c\u64cd\u4f5c\u7684\u5b89\u5168\u7ea6\u675f\u5e26\u6765\u7684\u8c03\u5ea6\u4e0e\u534f\u8c03\u65b0\u6311\u6218\u3002", "method": "\u81ea\u52a8\u5316\u7ba1\u9053\u751f\u6210\u5899\u4f53\u65bd\u5de5\u8ba1\u5212\uff0c\u4f18\u5316\u65e0\u4eba\u673a\u4efb\u52a1\u5206\u914d\u548c\u6267\u884c\u65f6\u95f4\uff0c\u7ed3\u5408\u52a8\u6001\u8026\u5408\u7684\u4f18\u5148\u671f\u9650\u7ea6\u675f\u548c\u9759\u6001\u7ed3\u6784\u4f9d\u8d56\u7ea6\u675f\u3002", "result": "\u901a\u8fc7Gazebo\u6a21\u62df\u9a8c\u8bc1\uff0c\u6846\u67b6\u80fd\u591f\u4f18\u5316\u65e0\u4eba\u673a\u64cd\u4f5c\uff0c\u786e\u4fdd\u65bd\u5de5\u8fc7\u7a0b\u4e2d\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5b9e\u73b0\u4e86\u534f\u8c03\u4e14\u9ad8\u6548\u7684\u7a7a\u4e2d\u780c\u4f53\u65bd\u5de5\uff0c\u6ee1\u8db3\u4e86\u7ed3\u6784\u7c98\u5408\u548c\u5b89\u5168\u64cd\u4f5c\u7684\u9700\u6c42\u3002"}}
{"id": "2506.18725", "categories": ["cs.RO", "cs.CG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18725", "abs": "https://arxiv.org/abs/2506.18725", "authors": ["Anirban Ghosh", "Ian Dahlin", "Ayan Dutta"], "title": "TDACloud: Point Cloud Recognition Using Topological Data Analysis", "comment": null, "summary": "Point cloud-based object/place recognition remains a problem of interest in\napplications such as autonomous driving, scene reconstruction, and\nlocalization. Extracting meaningful local descriptors from a query point cloud\nthat can be matched with the descriptors of the collected point clouds is a\nchallenging problem. Furthermore, when the query point cloud is noisy or has\nbeen transformed (e.g., rotated), it adds to the complexity. To this end, we\npropose a novel methodology, named TDACloud, using Topological Data Analysis\n(TDA) for local descriptor extraction from a point cloud, which does not need\nresource-intensive GPU-based machine learning training. More specifically, we\nused the ATOL vectorization method to generate vectors for point clouds. Unlike\nvoxelization, our proposed technique can take raw point clouds as inputs and\noutputs a fixed-size TDA-descriptor vector. To test the quality of the proposed\nTDACloud technique, we have implemented it on multiple real-world (e.g., Oxford\nRobotCar, KITTI-360) and realistic (e.g., ShapeNet) point cloud datasets for\nobject and place recognition. We have also tested TDACloud on noisy and\ntransformed test cases where the query point cloud has been scaled, translated,\nor rotated. Our results demonstrate high recognition accuracies in noisy\nconditions and large-scale real-world place recognition while outperforming the\nbaselines by up to approximately 14%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTDACloud\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u62d3\u6251\u6570\u636e\u5206\u6790\uff08TDA\uff09\u4ece\u70b9\u4e91\u4e2d\u63d0\u53d6\u5c40\u90e8\u63cf\u8ff0\u7b26\uff0c\u65e0\u9700GPU\u5bc6\u96c6\u578b\u8bad\u7ec3\uff0c\u5e76\u5728\u566a\u58f0\u548c\u53d8\u6362\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u70b9\u4e91\u8bc6\u522b\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u566a\u58f0\u548c\u53d8\u6362\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u4f9d\u8d56\u8d44\u6e90\u5bc6\u96c6\u578b\u8bad\u7ec3\u3002", "method": "\u4f7f\u7528ATOL\u5411\u91cf\u5316\u65b9\u6cd5\u751f\u6210\u70b9\u4e91\u7684\u56fa\u5b9a\u5927\u5c0fTDA\u63cf\u8ff0\u7b26\u5411\u91cf\uff0c\u76f4\u63a5\u5904\u7406\u539f\u59cb\u70b9\u4e91\u3002", "result": "\u5728\u771f\u5b9e\u548c\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cTDACloud\u5728\u566a\u58f0\u548c\u53d8\u6362\u6761\u4ef6\u4e0b\u8bc6\u522b\u51c6\u786e\u7387\u9ad8\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u7ea614%\u3002", "conclusion": "TDACloud\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u70b9\u4e91\u8bc6\u522b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2506.18779", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18779", "abs": "https://arxiv.org/abs/2506.18779", "authors": ["Bao Thach", "Siyeon Kim", "Britton Jordan", "Mohanraj Shanthi", "Tanner Watts", "Shing-Hei Ho", "James M. Ferguson", "Tucker Hermans", "Alan Kuntz"], "title": "DefFusionNet: Learning Multimodal Goal Shapes for Deformable Object Manipulation via a Diffusion-based Probabilistic Model", "comment": null, "summary": "Deformable object manipulation is critical to many real-world robotic\napplications, ranging from surgical robotics and soft material handling in\nmanufacturing to household tasks like laundry folding. At the core of this\nimportant robotic field is shape servoing, a task focused on controlling\ndeformable objects into desired shapes. The shape servoing formulation requires\nthe specification of a goal shape. However, most prior works in shape servoing\nrely on impractical goal shape acquisition methods, such as laborious\ndomain-knowledge engineering or manual manipulation. DefGoalNet previously\nposed the current state-of-the-art solution to this problem, which learns\ndeformable object goal shapes directly from a small number of human\ndemonstrations. However, it significantly struggles in multi-modal settings,\nwhere multiple distinct goal shapes can all lead to successful task completion.\nAs a deterministic model, DefGoalNet collapses these possibilities into a\nsingle averaged solution, often resulting in an unusable goal. In this paper,\nwe address this problem by developing DefFusionNet, a novel neural network that\nleverages the diffusion probabilistic model to learn a distribution over all\nvalid goal shapes rather than predicting a single deterministic outcome. This\nenables the generation of diverse goal shapes and avoids the averaging\nartifacts. We demonstrate our method's effectiveness on robotic tasks inspired\nby both manufacturing and surgical applications, both in simulation and on a\nphysical robot. Our work is the first generative model capable of producing a\ndiverse, multi-modal set of deformable object goals for real-world robotic\napplications.", "AI": {"tldr": "DefFusionNet\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6982\u7387\u6a21\u578b\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u751f\u6210\u591a\u6837\u5316\u7684\u53ef\u53d8\u5f62\u7269\u4f53\u76ee\u6807\u5f62\u72b6\uff0c\u89e3\u51b3\u4e86DefGoalNet\u5728\u591a\u6a21\u6001\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982DefGoalNet\uff09\u5728\u591a\u6a21\u6001\u573a\u666f\u4e2d\u65e0\u6cd5\u751f\u6210\u591a\u6837\u5316\u7684\u76ee\u6807\u5f62\u72b6\uff0c\u5bfc\u81f4\u5b9e\u7528\u6027\u53d7\u9650\u3002", "method": "\u5229\u7528\u6269\u6563\u6982\u7387\u6a21\u578b\u5b66\u4e60\u6240\u6709\u6709\u6548\u76ee\u6807\u5f62\u72b6\u7684\u5206\u5e03\uff0c\u800c\u975e\u5355\u4e00\u786e\u5b9a\u6027\u7ed3\u679c\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u7684\u76ee\u6807\u5f62\u72b6\u3002", "conclusion": "DefFusionNet\u662f\u9996\u4e2a\u80fd\u591f\u4e3a\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u751f\u6210\u591a\u6837\u5316\u3001\u591a\u6a21\u6001\u76ee\u6807\u5f62\u72b6\u7684\u751f\u6210\u6a21\u578b\u3002"}}
{"id": "2506.18812", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18812", "abs": "https://arxiv.org/abs/2506.18812", "authors": ["Aristotelis Papatheodorou", "Pranav Vaidhyanathan", "Natalia Ares", "Ioannis Havoutis"], "title": "Learning Physical Systems: Symplectification via Gauge Fixing in Dirac Structures", "comment": "Presented at Equivariant Systems: Theory and Applications in State\n  Estimation, Artificial Intelligence and Control, Robotics: Science and\n  Systems (RSS) 2025 Workshop, 6 Pages, 3 Figures", "summary": "Physics-informed deep learning has achieved remarkable progress by embedding\ngeometric priors, such as Hamiltonian symmetries and variational principles,\ninto neural networks, enabling structure-preserving models that extrapolate\nwith high accuracy. However, in systems with dissipation and holonomic\nconstraints, ubiquitous in legged locomotion and multibody robotics, the\ncanonical symplectic form becomes degenerate, undermining the very invariants\nthat guarantee stability and long-term prediction. In this work, we tackle this\nfoundational limitation by introducing Presymplectification Networks (PSNs),\nthe first framework to learn the symplectification lift via Dirac structures,\nrestoring a non-degenerate symplectic geometry by embedding constrained systems\ninto a higher-dimensional manifold. Our architecture combines a recurrent\nencoder with a flow-matching objective to learn the augmented phase-space\ndynamics end-to-end. We then attach a lightweight Symplectic Network (SympNet)\nto forecast constrained trajectories while preserving energy, momentum, and\nconstraint satisfaction. We demonstrate our method on the dynamics of the\nANYmal quadruped robot, a challenging contact-rich, multibody system. To the\nbest of our knowledge, this is the first framework that effectively bridges the\ngap between constrained, dissipative mechanical systems and symplectic\nlearning, unlocking a whole new class of geometric machine learning models,\ngrounded in first principles yet adaptable from data.", "AI": {"tldr": "\u63d0\u51fa\u4e86Presymplectification Networks (PSNs)\uff0c\u901a\u8fc7Dirac\u7ed3\u6784\u5b66\u4e60\u8f9b\u63d0\u5347\uff0c\u89e3\u51b3\u542b\u8017\u6563\u548c\u7ea6\u675f\u7cfb\u7edf\u4e2d\u8f9b\u5f62\u5f0f\u9000\u5316\u7684\u95ee\u9898\uff0c\u7ed3\u5408SympNet\u5b9e\u73b0\u7ea6\u675f\u8f68\u8ff9\u9884\u6d4b\u3002", "motivation": "\u5728\u542b\u8017\u6563\u548c\u7ea6\u675f\u7684\u7cfb\u7edf\u4e2d\uff0c\u4f20\u7edf\u8f9b\u5f62\u5f0f\u9000\u5316\uff0c\u5bfc\u81f4\u7a33\u5b9a\u6027\u4e0e\u957f\u671f\u9884\u6d4b\u80fd\u529b\u4e0b\u964d\u3002", "method": "\u5f15\u5165PSNs\u6846\u67b6\uff0c\u901a\u8fc7Dirac\u7ed3\u6784\u5b66\u4e60\u8f9b\u63d0\u5347\uff0c\u7ed3\u5408SympNet\u9884\u6d4b\u7ea6\u675f\u8f68\u8ff9\u3002", "result": "\u5728ANYmal\u56db\u8db3\u673a\u5668\u4eba\u52a8\u529b\u5b66\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "PSNs\u586b\u8865\u4e86\u7ea6\u675f\u8017\u6563\u7cfb\u7edf\u4e0e\u8f9b\u5b66\u4e60\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u4e3a\u51e0\u4f55\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.18825", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18825", "abs": "https://arxiv.org/abs/2506.18825", "authors": ["Yizhou Chen", "Hang Xu", "Dongjie Yu", "Zeqing Zhang", "Yi Ren", "Jia Pan"], "title": "SViP: Sequencing Bimanual Visuomotor Policies with Object-Centric Motion Primitives", "comment": "Project website: https://sites.google.com/view/svip-bimanual", "summary": "Imitation learning (IL), particularly when leveraging high-dimensional visual\ninputs for policy training, has proven intuitive and effective in complex\nbimanual manipulation tasks. Nonetheless, the generalization capability of\nvisuomotor policies remains limited, especially when small demonstration\ndatasets are available. Accumulated errors in visuomotor policies significantly\nhinder their ability to complete long-horizon tasks. To address these\nlimitations, we propose SViP, a framework that seamlessly integrates visuomotor\npolicies into task and motion planning (TAMP). SViP partitions human\ndemonstrations into bimanual and unimanual operations using a semantic scene\ngraph monitor. Continuous decision variables from the key scene graph are\nemployed to train a switching condition generator. This generator produces\nparameterized scripted primitives that ensure reliable performance even when\nencountering out-of-the-distribution observations. Using only 20 real-world\ndemonstrations, we show that SViP enables visuomotor policies to generalize\nacross out-of-distribution initial conditions without requiring object pose\nestimators. For previously unseen tasks, SViP automatically discovers effective\nsolutions to achieve the goal, leveraging constraint modeling in TAMP\nformulism. In real-world experiments, SViP outperforms state-of-the-art\ngenerative IL methods, indicating wider applicability for more complex tasks.\nProject website: https://sites.google.com/view/svip-bimanual", "AI": {"tldr": "SViP\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u4e0e\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\uff08TAMP\uff09\uff0c\u63d0\u5347\u4e86\u5c0f\u6837\u672c\u6a21\u4eff\u5b66\u4e60\u5728\u590d\u6742\u53cc\u624b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5728\u5c0f\u6837\u672c\u6570\u636e\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u53ca\u957f\u65f6\u4efb\u52a1\u4e2d\u8bef\u5dee\u7d2f\u79ef\u7684\u95ee\u9898\u3002", "method": "SViP\u5229\u7528\u8bed\u4e49\u573a\u666f\u56fe\u5206\u5272\u6f14\u793a\uff0c\u8bad\u7ec3\u5207\u6362\u6761\u4ef6\u751f\u6210\u5668\uff0c\u7ed3\u5408TAMP\u751f\u6210\u53c2\u6570\u5316\u811a\u672c\u539f\u8bed\u3002", "result": "\u4ec5\u970020\u4e2a\u771f\u5b9e\u6f14\u793a\uff0cSViP\u5728\u672a\u89c1\u521d\u59cb\u6761\u4ef6\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u751f\u6210\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "SViP\u4e3a\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u53cc\u624b\u64cd\u4f5c\u9886\u57df\u3002"}}
{"id": "2506.18844", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18844", "abs": "https://arxiv.org/abs/2506.18844", "authors": ["Olivier Gamache", "Jean-Michel Fortin", "Mat\u011bj Boxan", "Fran\u00e7ois Pomerleau", "Philippe Gigu\u00e8re"], "title": "Reproducible Evaluation of Camera Auto-Exposure Methods in the Field: Platform, Benchmark and Lessons Learned", "comment": "19 pages, 11 figures, pre-print version of the accepted paper for\n  IEEE Transactions on Field Robotics (T-FR)", "summary": "Standard datasets often present limitations, particularly due to the fixed\nnature of input data sensors, which makes it difficult to compare methods that\nactively adjust sensor parameters to suit environmental conditions. This is the\ncase with Automatic-Exposure (AE) methods, which rely on environmental factors\nto influence the image acquisition process. As a result, AE methods have\ntraditionally been benchmarked in an online manner, rendering experiments\nnon-reproducible. Building on our prior work, we propose a methodology that\nutilizes an emulator capable of generating images at any exposure time. This\napproach leverages BorealHDR, a unique multi-exposure stereo dataset, along\nwith its new extension, in which data was acquired along a repeated trajectory\nat different times of the day to assess the impact of changing illumination. In\ntotal, BorealHDR covers 13.4 km over 59 trajectories in challenging lighting\nconditions. The dataset also includes lidar-inertial-odometry-based maps with\npose estimation for each image frame, as well as Global Navigation Satellite\nSystem (GNSS) data for comparison. We demonstrate that by using images acquired\nat various exposure times, we can emulate realistic images with a\nRoot-Mean-Square Error (RMSE) below 1.78% compared to ground truth images.\nUsing this offline approach, we benchmarked eight AE methods, concluding that\nthe classical AE method remains the field's best performer. To further support\nreproducibility, we provide in-depth details on the development of our backpack\nacquisition platform, including hardware, electrical components, and\nperformance specifications. Additionally, we share valuable lessons learned\nfrom deploying the backpack over more than 25 km across various environments.\nOur code and dataset are available online at this link:\nhttps://github.com/norlab-ulaval/TFR24 BorealHDR", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6a21\u62df\u5668\u751f\u6210\u4e0d\u540c\u66dd\u5149\u65f6\u95f4\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8eBorealHDR\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u516b\u79cd\u81ea\u52a8\u66dd\u5149\u65b9\u6cd5\uff0c\u53d1\u73b0\u4f20\u7edf\u65b9\u6cd5\u6027\u80fd\u6700\u4f73\u3002", "motivation": "\u6807\u51c6\u6570\u636e\u96c6\u56e0\u8f93\u5165\u4f20\u611f\u5668\u56fa\u5b9a\uff0c\u96be\u4ee5\u6bd4\u8f83\u52a8\u6001\u8c03\u6574\u4f20\u611f\u5668\u53c2\u6570\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u81ea\u52a8\u66dd\u5149\uff08AE\uff09\u65b9\u6cd5\u3002\u73b0\u6709\u5728\u7ebf\u8bc4\u4f30\u65b9\u5f0f\u4e0d\u53ef\u590d\u73b0\u3002", "method": "\u5229\u7528BorealHDR\u591a\u66dd\u5149\u7acb\u4f53\u6570\u636e\u96c6\u53ca\u5176\u6269\u5c55\uff0c\u901a\u8fc7\u6a21\u62df\u5668\u751f\u6210\u4e0d\u540c\u66dd\u5149\u65f6\u95f4\u7684\u56fe\u50cf\uff0c\u79bb\u7ebf\u8bc4\u4f30AE\u65b9\u6cd5\u3002", "result": "\u6a21\u62df\u56fe\u50cf\u4e0e\u771f\u5b9e\u56fe\u50cf\u7684RMSE\u4f4e\u4e8e1.78%\uff0c\u4f20\u7edfAE\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u79bb\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5b9e\u9a8c\u590d\u73b0\u6027\uff0c\u4f20\u7edfAE\u65b9\u6cd5\u4ecd\u6700\u4f18\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u516c\u5f00\uff0c\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.18885", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18885", "abs": "https://arxiv.org/abs/2506.18885", "authors": ["Annika Thomas", "Aneesa Sonawalla", "Alex Rose", "Jonathan P. How"], "title": "GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM", "comment": null, "summary": "3D Gaussian splatting has emerged as an expressive scene representation for\nRGB-D visual SLAM, but its application to large-scale, multi-agent outdoor\nenvironments remains unexplored. Multi-agent Gaussian SLAM is a promising\napproach to rapid exploration and reconstruction of environments, offering\nscalable environment representations, but existing approaches are limited to\nsmall-scale, indoor environments. To that end, we propose Gaussian\nReconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative\nGaussian splatting SLAM method that integrates i) an implicit tracking module\nbased on local optimization over submaps and ii) an approach to inter- and\nintra-robot loop closure integrated into a pose-graph optimization framework.\nExperiments show that GRAND-SLAM provides state-of-the-art tracking performance\nand 28% higher PSNR than existing methods on the Replica indoor dataset, as\nwell as 91% lower multi-agent tracking error and improved rendering over\nexisting multi-agent methods on the large-scale, outdoor Kimera-Multi dataset.", "AI": {"tldr": "GRAND-SLAM\u662f\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u76843D\u9ad8\u65af\u6cfc\u6e85SLAM\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6237\u5916\u73af\u5883\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85SLAM\u65b9\u6cd5\u5c40\u9650\u4e8e\u5c0f\u89c4\u6a21\u5ba4\u5185\u73af\u5883\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u6237\u5916\u573a\u666f\u7684\u9700\u6c42\u3002", "method": "GRAND-SLAM\u7ed3\u5408\u4e86\u57fa\u4e8e\u5b50\u56fe\u7684\u9690\u5f0f\u8ddf\u8e2a\u6a21\u5757\u548c\u591a\u673a\u5668\u4eba\u95ed\u73af\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u96c6\u6210\u5230\u4f4d\u59ff\u56fe\u4f18\u5316\u6846\u67b6\u4e2d\u3002", "result": "\u5728Replica\u5ba4\u5185\u6570\u636e\u96c6\u4e0a\uff0cGRAND-SLAM\u7684\u8ddf\u8e2a\u6027\u80fd\u6700\u4f18\uff0cPSNR\u63d0\u534728%\uff1b\u5728Kimera-Multi\u6237\u5916\u6570\u636e\u96c6\u4e0a\uff0c\u591a\u667a\u80fd\u4f53\u8ddf\u8e2a\u8bef\u5dee\u964d\u4f4e91%\uff0c\u6e32\u67d3\u6548\u679c\u66f4\u4f18\u3002", "conclusion": "GRAND-SLAM\u4e3a\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u6237\u5916\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684SLAM\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.18897", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18897", "abs": "https://arxiv.org/abs/2506.18897", "authors": ["Xiaowei Chi", "Kuangzhi Ge", "Jiaming Liu", "Siyuan Zhou", "Peidong Jia", "Zichen He", "Yuzhen Liu", "Tingguang Li", "Lei Han", "Sirui Han", "Shanghang Zhang", "Yike Guo"], "title": "MinD: Unified Visual Imagination and Control via Hierarchical World Models", "comment": null, "summary": "Video generation models (VGMs) offer a promising pathway for unified world\nmodeling in robotics by integrating simulation, prediction, and manipulation.\nHowever, their practical application remains limited due to (1) slowgeneration\nspeed, which limits real-time interaction, and (2) poor consistency between\nimagined videos and executable actions. To address these challenges, we propose\nManipulate in Dream (MinD), a hierarchical diffusion-based world model\nframework that employs a dual-system design for vision-language manipulation.\nMinD executes VGM at low frequencies to extract video prediction features,\nwhile leveraging a high-frequency diffusion policy for real-time interaction.\nThis architecture enables low-latency, closed-loop control in manipulation with\ncoherent visual guidance. To better coordinate the two systems, we introduce a\nvideo-action diffusion matching module (DiffMatcher), with a novel co-training\nstrategy that uses separate schedulers for each diffusion model. Specifically,\nwe introduce a diffusion-forcing mechanism to DiffMatcher that aligns their\nintermediate representations during training, helping the fast action model\nbetter understand video-based predictions. Beyond manipulation, MinD also\nfunctions as a world simulator, reliably predicting task success or failure in\nlatent space before execution. Trustworthy analysis further shows that VGMs can\npreemptively evaluate task feasibility and mitigate risks. Extensive\nexperiments across multiple benchmarks demonstrate that MinD achieves\nstate-of-the-art manipulation (63%+) in RL-Bench, advancing the frontier of\nunified world modeling in robotics.", "AI": {"tldr": "MinD\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u7cfb\u7edf\u8bbe\u8ba1\u548cDiffMatcher\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u673a\u5668\u4eba\u4e2d\u7684\u5b9e\u65f6\u6027\u548c\u4e00\u81f4\u6027\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u95ed\u73af\u63a7\u5236\u548c\u4e16\u754c\u6a21\u62df\u529f\u80fd\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u6a21\u578b\uff08VGMs\uff09\u5728\u673a\u5668\u4eba\u7edf\u4e00\u4e16\u754c\u5efa\u6a21\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u53d7\u9650\u4e8e\u751f\u6210\u901f\u5ea6\u6162\u548c\u89c6\u9891\u4e0e\u52a8\u4f5c\u4e00\u81f4\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "MinD\u91c7\u7528\u5206\u5c42\u6269\u6563\u6846\u67b6\uff0c\u7ed3\u5408\u4f4e\u9891VGMs\u548c\u9ad8\u9891\u6269\u6563\u7b56\u7565\uff0c\u5f15\u5165DiffMatcher\u6a21\u5757\u53ca\u534f\u540c\u8bad\u7ec3\u7b56\u7565\uff0c\u4f18\u5316\u89c6\u9891\u4e0e\u52a8\u4f5c\u7684\u534f\u8c03\u3002", "result": "MinD\u5728RL-Bench\u4e2d\u5b9e\u73b0\u4e8663%+\u7684\u5148\u8fdb\u64cd\u4f5c\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u4efb\u52a1\u53ef\u884c\u6027\u9884\u8bc4\u4f30\u548c\u98ce\u9669\u7f13\u89e3\u80fd\u529b\u3002", "conclusion": "MinD\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u63d0\u5347\u4e86VGMs\u7684\u5b9e\u7528\u6027\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u4eba\u7edf\u4e00\u4e16\u754c\u5efa\u6a21\u7684\u53d1\u5c55\u3002"}}
