<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 22]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Gimballed Rotor Mechanism for Omnidirectional Quadrotors](https://arxiv.org/abs/2511.15909)
*J. Cristobal,A. Z. Zain Aldeen,M. Izadi,R. Faieghi*

Main category: cs.RO

TL;DR: 提出了一种采用万向节转子机制的模块化全向四旋翼无人机设计，通过独立倾斜每个转子实现六自由度全驱动控制，无需对四旋翼中心结构进行重大改动。


<details>
  <summary>Details</summary>
Motivation: 传统四旋翼无人机是欠驱动的，无法独立控制所有六个自由度。现有全向四旋翼设计通常需要重大结构修改，本设计旨在提供轻量级且易于集成的解决方案。

Method: 在转子平台内集成伺服电机，使每个转子能够独立倾斜，同时开发了PX4自动驾驶仪中的新控制分配方案。

Result: 成功进行了飞行测试，验证了所提出方法的有效性。

Conclusion: 万向节转子机制为构建全向四旋翼无人机提供了一种模块化且高效的解决方案，实现了全驱动控制而无需重大结构修改。

Abstract: This paper presents the design of a gimballed rotor mechanism as a modular and efficient solution for constructing omnidirectional quadrotors. Unlike conventional quadrotors, which are underactuated, this class of quadrotors achieves full actuation, enabling independent motion in all six degrees of freedom. While existing omnidirectional quadrotor designs often require significant structural modifications, the proposed gimballed rotor system maintains a lightweight and easy-to-integrate design by incorporating servo motors within the rotor platforms, allowing independent tilting of each rotor without major alterations to the central structure of a quadrotor. To accommodate this unconventional design, we develop a new control allocation scheme in PX4 Autopilot and present successful flight tests, validating the effectiveness of the proposed approach.

</details>


### [2] [I've Changed My Mind: Robots Adapting to Changing Human Goals during Collaboration](https://arxiv.org/abs/2511.15914)
*Debasmita Ghose,Oz Gitelson,Ryan Jin,Grace Abawe,Marynel Vazquez,Brian Scassellati*

Main category: cs.RO

TL;DR: 提出了一种检测人类目标变化的方法，通过追踪多个候选动作序列并验证其合理性，在检测到目标变化后重新评估过去动作并构建规划树来协助人类。


<details>
  <summary>Details</summary>
Motivation: 在真实协作场景中，人类经常在任务中途改变目标，而现有方法通常假设固定目标，难以适应这种动态变化。

Method: 追踪多个候选动作序列并验证其合理性，检测到目标变化后重新评估相关过去动作，构建Receding Horizon Planning树来选择协助动作并鼓励区分性动作来揭示更新后的目标。

Result: 在包含30种独特食谱的协作烹饪环境中评估，优于三种基线算法，能快速收敛到正确目标，减少任务完成时间，提高协作效率。

Conclusion: 该方法能有效检测和适应人类目标变化，显著提升人机协作性能。

Abstract: For effective human-robot collaboration, a robot must align its actions with human goals, even as they change mid-task. Prior approaches often assume fixed goals, reducing goal prediction to a one-time inference. However, in real-world scenarios, humans frequently shift goals, making it challenging for robots to adapt without explicit communication. We propose a method for detecting goal changes by tracking multiple candidate action sequences and verifying their plausibility against a policy bank. Upon detecting a change, the robot refines its belief in relevant past actions and constructs Receding Horizon Planning (RHP) trees to actively select actions that assist the human while encouraging Differentiating Actions to reveal their updated goal. We evaluate our approach in a collaborative cooking environment with up to 30 unique recipes and compare it to three comparable human goal prediction algorithms. Our method outperforms all baselines, quickly converging to the correct goal after a switch, reducing task completion time, and improving collaboration efficiency.

</details>


### [3] [The Role of Consequential and Functional Sound in Human-Robot Interaction: Toward Audio Augmented Reality Interfaces](https://arxiv.org/abs/2511.15956)
*Aliyah Smith,Monroe Kennedy*

Main category: cs.RO

TL;DR: 研究探索了机器人的声音对人类感知和行为的影响，包括操作噪音和设计听觉提示，特别关注空间声音在定位和交接任务中的作用。


<details>
  <summary>Details</summary>
Motivation: 随着机器人越来越多地融入日常环境，理解它们如何与人类沟通变得至关重要。声音提供了一个强大的交互渠道，包括操作噪音和有意设计的听觉提示。

Method: 研究了Kinova Gen3机械臂的结果性声音和功能性声音对人类感知的影响，包括通过定位和交接任务探索空间声音的新方法。

Result: Kinova Gen3的结果性声音没有负面影响感知，侧向空间定位准确但正面定位下降，空间声音能同时传达任务相关信息并促进温暖感、减少不适。

Conclusion: 功能性和变革性听觉设计有潜力增强人机协作，并为未来基于声音的交互策略提供信息。

Abstract: As robots become increasingly integrated into everyday environments, understanding how they communicate with humans is critical. Sound offers a powerful channel for interaction, encompassing both operational noises and intentionally designed auditory cues. In this study, we examined the effects of consequential and functional sounds on human perception and behavior, including a novel exploration of spatial sound through localization and handover tasks. Results show that consequential sounds of the Kinova Gen3 manipulator did not negatively affect perceptions, spatial localization is highly accurate for lateral cues but declines for frontal cues, and spatial sounds can simultaneously convey task-relevant information while promoting warmth and reducing discomfort. These findings highlight the potential of functional and transformative auditory design to enhance human-robot collaboration and inform future sound-based interaction strategies.

</details>


### [4] [PushingBots: Collaborative Pushing via Neural Accelerated Combinatorial Hybrid Optimization](https://arxiv.org/abs/2511.15995)
*Zili Tang,Ying Zhang,Meng Guo*

Main category: cs.RO

TL;DR: 该论文提出了一种多机器人协作推动任意形状物体到目标位置的方法，解决了在复杂环境中处理杂乱和可移动障碍物的问题。方法基于组合混合优化，包含任务分解分配、混合搜索和混合控制三个主要组件。


<details>
  <summary>Details</summary>
Motivation: 许多机器人没有配备机械手，许多物体不适合抓取操作（如大箱子和圆柱体）。在这些情况下，推动是一种简单有效的非抓取技能。现有工作通常假设预定义的推动模式和固定形状物体，本文解决了在复杂环境中多机器人协作推动任意物体到各自目的地的通用问题。

Method: 方法基于组合混合优化，包含三个主要组件：(I) 推动子任务的分解、排序和滚动分配给机器人子组；(II) 关键帧引导的混合搜索优化每个子任务的参数化推动模式序列；(III) 混合控制执行这些模式并在它们之间转换。还采用了基于扩散的加速器来预测关键帧和推动模式。

Result: 该框架在温和假设下是完整的。在不同机器人数量和一般形状物体下的效率和有效性在仿真和硬件实验中得到了广泛验证，并推广到异构机器人、平面装配和6D推动。

Conclusion: 提出的方法能够有效解决多机器人系统在复杂环境中协作推动任意形状物体的任务，克服了任务协调不确定性、接触模式切换和欠驱动等挑战，具有较好的通用性和实用性。

Abstract: Many robots are not equipped with a manipulator and many objects are not suitable for prehensile manipulation (such as large boxes and cylinders). In these cases, pushing is a simple yet effective non-prehensile skill for robots to interact with and further change the environment. Existing work often assumes a set of predefined pushing modes and fixed-shape objects. This work tackles the general problem of controlling a robotic fleet to push collaboratively numerous arbitrary objects to respective destinations, within complex environments of cluttered and movable obstacles. It incorporates several characteristic challenges for multi-robot systems such as online task coordination under large uncertainties of cost and duration, and for contact-rich tasks such as hybrid switching among different contact modes, and under-actuation due to constrained contact forces. The proposed method is based on combinatorial hybrid optimization over dynamic task assignments and hybrid execution via sequences of pushing modes and associated forces. It consists of three main components: (I) the decomposition, ordering and rolling assignment of pushing subtasks to robot subgroups; (II) the keyframe guided hybrid search to optimize the sequence of parameterized pushing modes for each subtask; (III) the hybrid control to execute these modes and transit among them. Last but not least, a diffusion-based accelerator is adopted to predict the keyframes and pushing modes that should be prioritized during hybrid search; and further improve planning efficiency. The framework is complete under mild assumptions. Its efficiency and effectiveness under different numbers of robots and general-shaped objects are validated extensively in simulations and hardware experiments, as well as generalizations to heterogeneous robots, planar assembly and 6D pushing.

</details>


### [5] [Semantic Glitch: Agency and Artistry in an Autonomous Pixel Cloud](https://arxiv.org/abs/2511.16048)
*Qing Zhang,Jing Huang,Mingyang Xu,Jun Rekimoto*

Main category: cs.RO

TL;DR: 本文提出了一种"低保真"的软体飞行机器人艺术装置"语义故障"，它采用基于多模态大语言模型的语义导航方法，摒弃传统传感器，通过自然语言提示赋予机器人生物启发式个性，创造具有叙事思维的机器人伴侣。


<details>
  <summary>Details</summary>
Motivation: 探索在主流机器人追求精确性能之外，通过"低保真"方法创造具有创意潜力的机器人，强调角色个性而非效率的成功标准。

Method: 开发了一个新颖的自主管道，不使用LiDAR和SLAM等传统传感器，仅依赖多模态大语言模型的语义理解进行导航，通过自然语言提示为机器人编写生物启发式个性。

Result: 通过13分钟自主飞行日志和后续研究统计验证了框架的鲁棒性，能够创建量化上不同的个性，并观察到从地标导航到"计划到执行"差距等涌现行为。

Conclusion: 展示了一个低保真框架，用于创造不完美的机器人伴侣，其成功标准在于角色个性而非效率，证明了缺乏精确本体感知可以产生不可预测但合理的行为。

Abstract: While mainstream robotics pursues metric precision and flawless performance, this paper explores the creative potential of a deliberately "lo-fi" approach. We present the "Semantic Glitch," a soft flying robotic art installation whose physical form, a 3D pixel style cloud, is a "physical glitch" derived from digital archaeology. We detail a novel autonomous pipeline that rejects conventional sensors like LiDAR and SLAM, relying solely on the qualitative, semantic understanding of a Multimodal Large Language Model to navigate. By authoring a bio-inspired personality for the robot through a natural language prompt, we create a "narrative mind" that complements the "weak," historically, loaded body. Our analysis begins with a 13-minute autonomous flight log, and a follow-up study statistically validates the framework's robustness for authoring quantifiably distinct personas. The combined analysis reveals emergent behaviors, from landmark-based navigation to a compelling "plan to execution" gap, and a character whose unpredictable, plausible behavior stems from a lack of precise proprioception. This demonstrates a lo-fi framework for creating imperfect companions whose success is measured in character over efficiency.

</details>


### [6] [Bi-AQUA: Bilateral Control-Based Imitation Learning for Underwater Robot Arms via Lighting-Aware Action Chunking with Transformers](https://arxiv.org/abs/2511.16050)
*Takeru Tsunoori,Masato Kobayashi,Yuki Uranishi*

Main category: cs.RO

TL;DR: Bi-AQUA是首个基于双边控制的水下模仿学习框架，通过三级光照适应机制解决水下机器人操作中的极端光照变化问题，在真实水下拾取任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 水下机器人操作面临极端光照变化、颜色失真和能见度降低等根本性挑战，需要开发能够适应复杂水下光照条件的鲁棒控制方法。

Method: 采用分层三级光照适应机制：1）无需手动标注的照明编码器从RGB图像提取光照表示；2）通过FiLM调制视觉骨干特征进行自适应光照感知特征提取；3）在transformer编码器输入中添加显式光照标记进行任务感知条件化。

Result: 在真实水下拾取任务的各种静态和动态光照条件下，Bi-AQUA实现了鲁棒性能，显著优于没有光照建模的双边基线。消融研究证实所有三个光照感知组件都至关重要。

Conclusion: 这项工作将陆地双边控制模仿学习与水下操作连接起来，实现了在挑战性海洋环境中进行力敏感自主操作的能力。

Abstract: Underwater robotic manipulation is fundamentally challenged by extreme lighting variations, color distortion, and reduced visibility. We introduce Bi-AQUA, the first underwater bilateral control-based imitation learning framework that integrates lighting-aware visual processing for underwater robot arms. Bi-AQUA employs a hierarchical three-level lighting adaptation mechanism: a Lighting Encoder that extracts lighting representations from RGB images without manual annotation and is implicitly supervised by the imitation objective, FiLM modulation of visual backbone features for adaptive, lighting-aware feature extraction, and an explicit lighting token added to the transformer encoder input for task-aware conditioning. Experiments on a real-world underwater pick-and-place task under diverse static and dynamic lighting conditions show that Bi-AQUA achieves robust performance and substantially outperforms a bilateral baseline without lighting modeling. Ablation studies further confirm that all three lighting-aware components are critical. This work bridges terrestrial bilateral control-based imitation learning and underwater manipulation, enabling force-sensitive autonomous operation in challenging marine environments. For additional material, please check: https://mertcookimg.github.io/bi-aqua

</details>


### [7] [MagBotSim: Physics-Based Simulation and Reinforcement Learning Environments for Magnetic Robotics](https://arxiv.org/abs/2511.16158)
*Lara Bergmann,Cedric Grothues,Klaus Neumann*

Main category: cs.RO

TL;DR: 本文介绍了MagBotSim，一个用于磁悬浮系统的物理模拟器，旨在将运输和操作功能整合到磁机器人群体中，以提高制造系统的效率、适应性和紧凑性。


<details>
  <summary>Details</summary>
Motivation: 磁悬浮系统在工业自动化中具有革命性潜力，但尚未充分利用其操作能力。通过将运输和操作功能整合到协调的磁机器人群体中，可以显著提高制造系统的性能。

Method: 开发了MagBotSim，这是一个基于物理的磁悬浮系统模拟器，将磁悬浮系统框架为机器人群体，为智能算法开发提供支持。

Result: 创建了一个专用的磁悬浮系统模拟平台，包括文档、视频、实验和代码，为下一代磁机器人驱动的制造系统奠定基础。

Conclusion: 通过将磁悬浮系统视为机器人群体并提供专用模拟器，这项工作为下一代由磁机器人驱动的制造系统奠定了基础。

Abstract: Magnetic levitation is about to revolutionize in-machine material flow in industrial automation. Such systems are flexibly configurable and can include a large number of independently actuated shuttles (movers) that dynamically rebalance production capacity. Beyond their capabilities for dynamic transportation, these systems possess the inherent yet unexploited potential to perform manipulation. By merging the fields of transportation and manipulation into a coordinated swarm of magnetic robots (MagBots), we enable manufacturing systems to achieve significantly higher efficiency, adaptability, and compactness. To support the development of intelligent algorithms for magnetic levitation systems, we introduce MagBotSim (Magnetic Robotics Simulation): a physics-based simulation for magnetic levitation systems. By framing magnetic levitation systems as robot swarms and providing a dedicated simulation, this work lays the foundation for next generation manufacturing systems powered by Magnetic Robotics. MagBotSim's documentation, videos, experiments, and code are available at: https://ubi-coro.github.io/MagBotSim/

</details>


### [8] [PIPHEN: Physical Interaction Prediction with Hamiltonian Energy Networks](https://arxiv.org/abs/2511.16200)
*Kewei Chen,Yayu Long,Mingsheng Shang*

Main category: cs.RO

TL;DR: PIPHEN框架通过语义通信替代原始数据传输，将高维感知数据压缩为紧凑的物理表示，解决了多机器人系统中的"共享大脑困境"，显著降低了带宽需求和决策延迟。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人系统在复杂物理协作中面临的高维多媒体数据传输带来的带宽瓶颈和决策延迟问题，即"共享大脑困境"。

Method: 提出PIPHEN分布式物理认知控制框架：1）物理交互预测网络（PIPN）进行语义蒸馏，生成紧凑的物理表示；2）哈密顿能量网络（HEN）控制器将表示转化为协调动作。

Result: 信息表示压缩至原始数据量的5%以下，协作决策延迟从315ms降至76ms，同时显著提高任务成功率。

Conclusion: 为资源受限的多机器人系统解决"共享大脑困境"提供了根本性的高效范式。

Abstract: Multi-robot systems in complex physical collaborations face a "shared brain dilemma": transmitting high-dimensional multimedia data (e.g., video streams at ~30MB/s) creates severe bandwidth bottlenecks and decision-making latency. To address this, we propose PIPHEN, an innovative distributed physical cognition-control framework. Its core idea is to replace "raw data communication" with "semantic communication" by performing "semantic distillation" at the robot edge, reconstructing high-dimensional perceptual data into compact, structured physical representations. This idea is primarily realized through two key components: (1) a novel Physical Interaction Prediction Network (PIPN), derived from large model knowledge distillation, to generate this representation; and (2) a Hamiltonian Energy Network (HEN) controller, based on energy conservation, to precisely translate this representation into coordinated actions. Experiments show that, compared to baseline methods, PIPHEN can compress the information representation to less than 5% of the original data volume and reduce collaborative decision-making latency from 315ms to 76ms, while significantly improving task success rates. This work provides a fundamentally efficient paradigm for resolving the "shared brain dilemma" in resource-constrained multi-robot systems.

</details>


### [9] [DynaMimicGen: A Data Generation Framework for Robot Learning of Dynamic Tasks](https://arxiv.org/abs/2511.16223)
*Vincenzo Pomponi,Paolo Franceschi,Stefano Baraldo,Loris Roveda,Oliver Avram,Luca Maria Gambardella,Anna Valente*

Main category: cs.RO

TL;DR: DynaMimicGen (D-MG) 是一个可扩展的数据集生成框架，仅需少量人类演示就能训练机器人策略，特别支持动态任务设置。它通过分割演示为子任务，利用动态运动基元(DMPs)来适应和泛化到动态变化的环境。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要大量多样化数据集来学习鲁棒的操作策略，但这在动态环境中不切实际。D-MG旨在通过最少的人类监督实现策略训练，并支持动态任务设置。

Method: 给定少量人类演示，D-MG首先将演示分割为有意义的子任务，然后利用动态运动基元(DMPs)来适应和泛化演示行为到新的动态变化环境。相比依赖静态假设或简单轨迹插值的方法，D-MG生成平滑、真实且任务一致的笛卡尔轨迹。

Result: 在模仿学习中使用D-MG生成的数据训练的机器人代理，在长视野和接触丰富的基准测试中表现优异，包括堆叠立方体和将杯子放入抽屉等任务，即使在不可预测的环境变化下也能保持性能。

Conclusion: D-MG通过消除对大量人类演示的需求并支持动态环境中的泛化，为手动数据收集提供了强大而高效的替代方案，为可扩展的自主机器人学习铺平了道路。

Abstract: Learning robust manipulation policies typically requires large and diverse datasets, the collection of which is time-consuming, labor-intensive, and often impractical for dynamic environments. In this work, we introduce DynaMimicGen (D-MG), a scalable dataset generation framework that enables policy training from minimal human supervision while uniquely supporting dynamic task settings. Given only a few human demonstrations, D-MG first segments the demonstrations into meaningful sub-tasks, then leverages Dynamic Movement Primitives (DMPs) to adapt and generalize the demonstrated behaviors to novel and dynamically changing environments. Improving prior methods that rely on static assumptions or simplistic trajectory interpolation, D-MG produces smooth, realistic, and task-consistent Cartesian trajectories that adapt in real time to changes in object poses, robot states, or scene geometry during task execution. Our method supports different scenarios - including scene layouts, object instances, and robot configurations - making it suitable for both static and highly dynamic manipulation tasks. We show that robot agents trained via imitation learning on D-MG-generated data achieve strong performance across long-horizon and contact-rich benchmarks, including tasks like cube stacking and placing mugs in drawers, even under unpredictable environment changes. By eliminating the need for extensive human demonstrations and enabling generalization in dynamic settings, D-MG offers a powerful and efficient alternative to manual data collection, paving the way toward scalable, autonomous robot learning.

</details>


### [10] [FT-NCFM: An Influence-Aware Data Distillation Framework for Efficient VLA Models](https://arxiv.org/abs/2511.16233)
*Kewei Chen,Yayu Long,Shuai Li,Mingsheng Shang*

Main category: cs.RO

TL;DR: FT-NCFM是一个数据中心的生成式数据蒸馏框架，通过事实追踪引擎评估样本内在价值，然后合成模型无关、信息密集的可重用数据资产，仅用5%的蒸馏核心集就能达到85-90%的完整数据集性能。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型对大规模、冗余且价值不均数据集的过度依赖问题，现有模型中心优化方法无法从根本上解决数据层面的挑战。

Method: 使用包含因果归因和程序化对比验证的事实追踪引擎评估样本价值，然后通过对抗性NCFM过程合成模型无关、信息密集的数据资产。

Result: 在多个主流VLA基准测试中，仅使用5%的蒸馏核心集训练模型，就能达到完整数据集85-90%的成功率，同时减少80%以上的训练时间。

Conclusion: 智能数据蒸馏是构建高效、高性能VLA模型的极具前景的新路径。

Abstract: The powerful generalization of Vision-Language-Action (VLA) models is bottlenecked by their heavy reliance on massive, redundant, and unevenly valued datasets, hindering their widespread application. Existing model-centric optimization paths, such as model compression (which often leads to performance degradation) or policy distillation (whose products are model-dependent and lack generality), fail to fundamentally address this data-level challenge. To this end, this paper introduces FT-NCFM, a fundamentally different, data-centric generative data distillation framework. Our framework employs a self-contained Fact-Tracing (FT) engine that combines causal attribution with programmatic contrastive verification to assess the intrinsic value of samples. Guided by these assessments, an adversarial NCFM process synthesizes a model-agnostic, information-dense, and reusable data asset. Experimental results on several mainstream VLA benchmarks show that models trained on just 5% of our distilled coreset achieve a success rate of 85-90% compared with training on the full dataset, while reducing training time by over 80%. Our work demonstrates that intelligent data distillation is a highly promising new path for building efficient, high-performance VLA models.

</details>


### [11] [How Robot Dogs See the Unseeable](https://arxiv.org/abs/2511.16262)
*Oliver Bimber,Karl Dietrich von Ellenrieder,Michael Haller,Rakesh John Amala Arokia Nathan,Gianni Lunardi,Marco Camurri,Mohamed Youssef,Santos Miguel Orozco Soto,Jeremy E. Niven*

Main category: cs.RO

TL;DR: 该研究将动物摆头行为与合成孔径传感相结合，通过机器人执行摆头运动形成宽合成孔径，计算合成浅景深图像来消除遮挡干扰，实现复杂环境中的高级场景理解。


<details>
  <summary>Details</summary>
Motivation: 解决机器人视觉中部分遮挡的根本限制，传统相机小孔径大景深导致前景遮挡物和背景物体都清晰，遮挡关键场景信息。

Method: 让机器人执行摆头运动，相机形成宽合成孔径，通过计算整合捕获图像合成极浅景深图像，有效模糊遮挡元素同时使背景清晰。

Result: 该方法能够恢复基本场景理解，并使大型多模态模型能够进行高级视觉推理，而传统遮挡图像会失败。该技术对遮挡鲁棒、计算高效，可立即部署在任何移动机器人上。

Conclusion: 摆头运动进行合成孔径传感是复杂杂乱环境中实现高级场景理解的关键，连接了动物行为与机器人技术。

Abstract: Peering, a side-to-side motion used by animals to estimate distance through motion parallax, offers a powerful bio-inspired strategy to overcome a fundamental limitation in robotic vision: partial occlusion. Conventional robot cameras, with their small apertures and large depth of field, render both foreground obstacles and background objects in sharp focus, causing occluders to obscure critical scene information. This work establishes a formal connection between animal peering and synthetic aperture (SA) sensing from optical imaging. By having a robot execute a peering motion, its camera describes a wide synthetic aperture. Computational integration of the captured images synthesizes an image with an extremely shallow depth of field, effectively blurring out occluding elements while bringing the background into sharp focus. This efficient, wavelength-independent technique enables real-time, high-resolution perception across various spectral bands. We demonstrate that this approach not only restores basic scene understanding but also empowers advanced visual reasoning in large multimodal models, which fail with conventionally occluded imagery. Unlike feature-dependent multi-view 3D vision methods or active sensors like LiDAR, SA sensing via peering is robust to occlusion, computationally efficient, and immediately deployable on any mobile robot. This research bridges animal behavior and robotics, suggesting that peering motions for synthetic aperture sensing are a key to advanced scene understanding in complex, cluttered environments.

</details>


### [12] [Funabot-Upper: McKibben Actuated Haptic Suit Inducing Kinesthetic Perceptions in Trunk, Shoulder, Elbow, and Wrist](https://arxiv.org/abs/2511.16265)
*Haru Fukatsu,Ryoji Yasuda,Yuki Funabora,Shinji Doki*

Main category: cs.RO

TL;DR: Funabot-Upper是一种可穿戴触觉套装，通过独立刺激关节和肌肉，使用户能够感知14种上半身运动，包括躯干、肩部、肘部和手腕的运动，相比前代设计显著减少了感知混淆问题。


<details>
  <summary>Details</summary>
Motivation: 现有可穿戴触觉设备大多局限于单一身体部位的验证，很少有设备能够同时应用于多个身体部位。前代Funabot-Suit在刺激多个肌肉时出现了肩部和肘部之间的感知混淆问题。

Method: 建立了新的简化设计策略，开发了一种新型触觉套装，通过独立刺激关节和肌肉来诱导躯干、肩部、肘部和手腕的动觉感知。

Result: 实验证实Funabot-Upper成功诱导了多个关节的动觉感知，同时减少了前代设计中的感知混淆。新套装将识别准确率从前代Funabot-Suit的68.8%提升到94.6%。

Conclusion: 新设计的触觉套装在诱导动觉感知方面表现出优越性，为未来的触觉应用展示了潜力。

Abstract: This paper presents Funabot-Upper, a wearable haptic suit that enables users to perceive 14 upper-body motions, including those of the trunk, shoulder, elbow, and wrist. Inducing kinesthetic perception through wearable haptic devices has attracted attention, and various devices have been developed in the past. However, these have been limited to verifications on single body parts, and few have applied the same method to multiple body parts as well. In our previous study, we developed a technology that uses the contraction of artificial muscles to deform clothing in three dimensions. Using this technology, we developed a haptic suit that induces kinesthetic perception of 7 motions in multiple upper body. However, perceptual mixing caused by stimulating multiple human muscles has occurred between the shoulder and the elbow. In this paper, we established a new, simplified design policy and developed a novel haptic suit that induces kinesthetic perceptions in the trunk, shoulder, elbow, and wrist by stimulating joints and muscles independently. We experimentally demonstrated the induced kinesthetic perception and examined the relationship between stimulation and perceived kinesthetic perception under the new design policy. Experiments confirmed that Funabot-Upper successfully induces kinesthetic perception across multiple joints while reducing perceptual mixing observed in previous designs. The new suit improved recognition accuracy from 68.8% to 94.6% compared to the previous Funabot-Suit, demonstrating its superiority and potential for future haptic applications.

</details>


### [13] [InEKFormer: A Hybrid State Estimator for Humanoid Robots](https://arxiv.org/abs/2511.16306)
*Lasse Hohmeyer,Mihaela Popescu,Ivan Bergonzani,Dennis Mronga,Frank Kirchner*

Main category: cs.RO

TL;DR: 提出了一种结合不变扩展卡尔曼滤波器和Transformer网络的混合状态估计方法InEKFormer，用于人形机器人的状态估计，并在RH5机器人数据集上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在不同环境中的双足运动稳定性仍具挑战，状态估计对运动控制至关重要。传统方法需要专家调参，而深度学习方法在状态估计任务中显示出潜力。

Method: 提出InEKFormer混合方法，结合不变扩展卡尔曼滤波器(InEKF)和Transformer网络，形成混合状态估计框架。

Result: 在RH5人形机器人数据集上与InEKF和KalmanNet方法比较，结果显示Transformer在人形状态估计中具有潜力，但也凸显了在高维问题中需要鲁棒的自回归训练。

Conclusion: Transformer网络在人形机器人状态估计中展现出应用前景，但需要解决高维问题中的自回归训练挑战。

Abstract: Humanoid robots have great potential for a wide range of applications, including industrial and domestic use, healthcare, and search and rescue missions. However, bipedal locomotion in different environments is still a challenge when it comes to performing stable and dynamic movements. This is where state estimation plays a crucial role, providing fast and accurate feedback of the robot's floating base state to the motion controller. Although classical state estimation methods such as Kalman filters are widely used in robotics, they require expert knowledge to fine-tune the noise parameters. Due to recent advances in the field of machine learning, deep learning methods are increasingly used for state estimation tasks. In this work, we propose the InEKFormer, a novel hybrid state estimation method that incorporates an invariant extended Kalman filter (InEKF) and a Transformer network. We compare our method with the InEKF and the KalmanNet approaches on datasets obtained from the humanoid robot RH5. The results indicate the potential of Transformers in humanoid state estimation, but also highlight the need for robust autoregressive training in these high-dimensional problems.

</details>


### [14] [Safe and Optimal Variable Impedance Control via Certified Reinforcement Learning](https://arxiv.org/abs/2511.16330)
*Shreyas Kumar,Ravi Prakash*

Main category: cs.RO

TL;DR: 提出C-GMS框架，通过采样稳定增益流形学习DMP和VIC策略，保证李雅普诺夫稳定性和执行器可行性，无需奖励惩罚或事后验证。


<details>
  <summary>Details</summary>
Motivation: 传统无模型强化学习在机器人协作技能学习中存在不稳定和不安全探索风险，特别是阻抗增益时变特性带来的问题。

Method: 将策略探索重新定义为从数学定义的稳定增益流形中采样，学习结合动态运动基元和可变阻抗控制的策略。

Result: 在仿真和真实机器人上验证了有效性，确保有界跟踪误差，即使在有界模型误差和部署时不确定性下也成立。

Conclusion: C-GMS为复杂环境中可靠的自主交互铺平了道路，提供理论保证的稳定性和物理可实现性。

Abstract: Reinforcement learning (RL) offers a powerful approach for robots to learn complex, collaborative skills by combining Dynamic Movement Primitives (DMPs) for motion and Variable Impedance Control (VIC) for compliant interaction. However, this model-free paradigm often risks instability and unsafe exploration due to the time-varying nature of impedance gains. This work introduces Certified Gaussian Manifold Sampling (C-GMS), a novel trajectory-centric RL framework that learns combined DMP and VIC policies while guaranteeing Lyapunov stability and actuator feasibility by construction. Our approach reframes policy exploration as sampling from a mathematically defined manifold of stable gain schedules. This ensures every policy rollout is guaranteed to be stable and physically realizable, thereby eliminating the need for reward penalties or post-hoc validation. Furthermore, we provide a theoretical guarantee that our approach ensures bounded tracking error even in the presence of bounded model errors and deployment-time uncertainties. We demonstrate the effectiveness of C-GMS in simulation and verify its efficacy on a real robot, paving the way for reliable autonomous interaction in complex environments.

</details>


### [15] [Flow-Aided Flight Through Dynamic Clutters From Point To Motion](https://arxiv.org/abs/2511.16372)
*Bowen Xu,Zexuan Yan,Minghao Lu,Xiyu Fan,Yi Luo,Youshen Lin,Zhiqiang Chen,Yeke Chen,Qiyuan Qiao,Peng Lu*

Main category: cs.RO

TL;DR: 提出了一种基于强化学习的无人机自主飞行系统，通过单激光雷达感知直接实现从点到运动的控制，无需物体检测、跟踪和预测，在动态杂乱环境中实现安全避障。


<details>
  <summary>Details</summary>
Motivation: 传统方法在动态杂乱环境中需要显式建模障碍物运动，这在高度动态和遮挡场景中耗时且不可靠。本文旨在开发一种不依赖物体检测和运动预测的轻量级解决方案。

Method: 使用深度感知距离图和点流作为环境变化感知表示，结合强化学习策略优化。通过相对运动调制的距离场来驱动提前避障行为，采用部署友好的传感仿真和无动力学模型加速控制。

Result: 系统在动态环境中表现出优越的成功率和适应性，从仿真器推导的策略能够在真实世界四旋翼上实现安全机动。

Conclusion: 提出的变化感知传感表示和强化学习方法能够有效处理动态杂乱环境，实现从仿真到真实世界的安全部署。

Abstract: Challenges in traversing dynamic clutters lie mainly in the efficient perception of the environmental dynamics and the generation of evasive behaviors considering obstacle movement. Previous solutions have made progress in explicitly modeling the dynamic obstacle motion for avoidance, but this key dependency of decision-making is time-consuming and unreliable in highly dynamic scenarios with occlusions. On the contrary, without introducing object detection, tracking, and prediction, we empower the reinforcement learning (RL) with single LiDAR sensing to realize an autonomous flight system directly from point to motion. For exteroception, a depth sensing distance map achieving fixed-shape, low-resolution, and detail-safe is encoded from raw point clouds, and an environment change sensing point flow is adopted as motion features extracted from multi-frame observations. These two are integrated into a lightweight and easy-to-learn representation of complex dynamic environments. For action generation, the behavior of avoiding dynamic threats in advance is implicitly driven by the proposed change-aware sensing representation, where the policy optimization is indicated by the relative motion modulated distance field. With the deployment-friendly sensing simulation and dynamics model-free acceleration control, the proposed system shows a superior success rate and adaptability to alternatives, and the policy derived from the simulator can drive a real-world quadrotor with safe maneuvers.

</details>


### [16] [Robot Metacognition: Decision Making with Confidence for Tool Invention](https://arxiv.org/abs/2511.16390)
*Ajith Anil Meera,Poppy Collis,Polina Arbuzova,Abián Torres,Paul F Kinghorn,Ricardo Sanz,Pablo Lanillos*

Main category: cs.RO

TL;DR: 本文提出了一种基于信心的机器人元认知架构，使机器人能够评估自身决策的可靠性，从而提高在现实世界物理部署中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前机器人缺乏对人类智能行为至关重要的元认知能力，即反思自身认知过程和决策的能力。这种自我监控能力对于学习、决策和问题解决至关重要。

Method: 受神经科学启发，提出以信心为中心的机器人元认知架构，将信心作为机器人决策方案中的元认知度量，强调通过具身动作监控来实现更明智的决策。

Result: 在自主工具发明的用例中展示了该架构，信心知情的机器人能够评估其决策的可靠性。

Conclusion: 机器人元认知强调具身动作监控是实现更明智决策的手段，并指出了机器人元认知的潜在应用和研究方向。

Abstract: Robots today often miss a key ingredient of truly intelligent behavior: the ability to reflect on their own cognitive processes and decisions. In humans, this self-monitoring or metacognition is crucial for learning, decision making and problem solving. For instance, they can evaluate how confident they are in performing a task, thus regulating their own behavior and allocating proper resources. Taking inspiration from neuroscience, we propose a robot metacognition architecture centered on confidence (a second-order judgment on decisions) and we demonstrate it on the use case of autonomous tool invention. We propose the use of confidence as a metacognitive measure within the robot decision making scheme. Confidence-informed robots can evaluate the reliability of their decisions, improving their robustness during real-world physical deployment. This form of robotic metacognition emphasizes embodied action monitoring as a means to achieve better informed decisions. We also highlight potential applications and research directions for robot metacognition.

</details>


### [17] [Homogeneous Proportional-Integral-Derivative Controller in Mobile Robotic Manipulators](https://arxiv.org/abs/2511.16406)
*Luis Luna,Isaac Chairez,Andrey Polyakov*

Main category: cs.RO

TL;DR: 提出了一种针对移动机器人操作器的同质PID控制策略，通过同质控制理论增强系统稳定性和收敛性，在动态不确定性和外部扰动下实现鲁棒协调运动控制。


<details>
  <summary>Details</summary>
Motivation: 移动机器人操作器由于非线性动力学、欠驱动特性以及基座与机械臂子系统之间的耦合，存在显著的控制挑战，需要开发能够处理这些复杂性的控制策略。

Method: 设计了同质PID控制器结构，利用同质控制理论框架，将传统PID增益推广为非线性、状态相关的函数，采用分级同质化方法改善跟踪误差收敛性。

Result: 实验验证表明，hPID控制器在移动基座和机械臂的高精度轨迹跟踪方面优于传统线性PID控制器，在响应时间、稳态误差和模型不确定性鲁棒性方面表现更优。

Conclusion: 该研究为增强下一代移动操作系统在结构化和非结构化环境中的自主性和可靠性提供了一个可扩展且分析基础扎实的控制框架。

Abstract: Mobile robotic manipulators (MRMs), which integrate mobility and manipulation capabilities, present significant control challenges due to their nonlinear dynamics, underactuation, and coupling between the base and manipulator subsystems. This paper proposes a novel homogeneous Proportional-Integral-Derivative (hPID) control strategy tailored for MRMs to achieve robust and coordinated motion control. Unlike classical PID controllers, the hPID controller leverages the mathematical framework of homogeneous control theory to systematically enhance the stability and convergence properties of the closed-loop system, even in the presence of dynamic uncertainties and external disturbances involved into a system in a homogeneous way. A homogeneous PID structure is designed, ensuring improved convergence of tracking errors through a graded homogeneity approach that generalizes traditional PID gains to nonlinear, state-dependent functions. Stability analysis is conducted using Lyapunov-based methods, demonstrating that the hPID controller guarantees global asymptotic stability and finite-time convergence under mild assumptions. Experimental results on a representative MRM model validate the effectiveness of the hPID controller in achieving high-precision trajectory tracking for both the mobile base and manipulator arm, outperforming conventional linear PID controllers in terms of response time, steady-state error, and robustness to model uncertainties. This research contributes a scalable and analytically grounded control framework for enhancing the autonomy and reliability of next-generation mobile manipulation systems in structured and unstructured environments.

</details>


### [18] [LAOF: Robust Latent Action Learning with Optical Flow Constraints](https://arxiv.org/abs/2511.16407)
*Xizhou Bu,Jiexi Lyu,Fulei Sun,Ruichen Yang,Zhiqiang Ma,Wei Li*

Main category: cs.RO

TL;DR: LAOF是一个利用光流作为动作驱动信号的伪监督框架，通过学习光学流约束来学习对干扰物具有鲁棒性的潜在动作表示，在标签稀缺条件下显著提升下游模仿学习和强化学习任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理大规模视频中的潜在动作学习时，容易受到动作无关干扰物的影响。虽然引入动作监督可以缓解这一问题，但可用动作标签的稀缺性限制了其效果。光学流能够自然抑制背景元素并强调运动物体，因此被用作动作驱动信号。

Method: 提出LAOF框架，利用智能体的光学流作为动作驱动信号，通过光学流约束来学习对干扰物具有鲁棒性的潜在动作表示。这是一个伪监督框架，不需要大量动作标签。

Result: 实验结果显示，LAOF学习的潜在表示在下游模仿学习和强化学习任务中优于现有方法。光学流约束显著稳定了训练过程，在极低标签条件下提高了潜在表示的质量，在动作标签比例增加到10%时仍保持有效。

Conclusion: 即使没有动作监督，LAOF也能达到或超过使用1%动作标签训练的动作监督方法的性能，证明了光学流约束在潜在动作学习中的有效性和鲁棒性。

Abstract: Learning latent actions from large-scale videos is crucial for the pre-training of scalable embodied foundation models, yet existing methods often struggle with action-irrelevant distractors. Although incorporating action supervision can alleviate these distractions, its effectiveness is restricted by the scarcity of available action labels. Optical flow represents pixel-level motion between consecutive frames, naturally suppressing background elements and emphasizing moving objects. Motivated by this, we propose robust Latent Action learning with Optical Flow constraints, called LAOF, a pseudo-supervised framework that leverages the agent's optical flow as an action-driven signal to learn latent action representations robust to distractors. Experimental results show that the latent representations learned by LAOF outperform existing methods on downstream imitation learning and reinforcement learning tasks. This superior performance arises from optical flow constraints, which substantially stabilize training and improve the quality of latent representations under extremely label-scarce conditions, while remaining effective as the proportion of action labels increases to 10 percent. Importantly, even without action supervision, LAOF matches or surpasses action-supervised methods trained with 1 percent of action labels.

</details>


### [19] [From Prompts to Printable Models: Support-Effective 3D Generation via Offset Direct Preference Optimization](https://arxiv.org/abs/2511.16434)
*Chenming Wu,Xiaofan Li,Chengkai Dai*

Main category: cs.RO

TL;DR: SEG是一个支持有效生成框架，通过将带偏移的直接偏好优化(ODPO)集成到3D生成流程中，直接在模型生成阶段优化以减少支撑材料使用。


<details>
  <summary>Details</summary>
Motivation: 当前3D打印从数字模型到物理对象的转换需要支撑结构来防止悬垂特征坍塌，现有切片技术主要关注后处理优化而非在模型生成阶段解决支撑效率问题。

Method: 将支撑结构模拟集成到训练过程中，使用ODPO方法直接优化模型以减少支撑材料需求，生成固有需要较少支撑的几何形状。

Result: 在Thingi10k-Val和GPT-3DP-Val基准数据集上的实验表明，SEG在支撑体积减少和可打印性方面显著优于TRELLIS、DPO和DRO等基线模型。

Conclusion: SEG通过在生成过程中直接优化模型，具有转变3D打印实践的潜力，为更可持续和高效的数字制造实践铺平道路。

Abstract: The transition from digital 3D models to physical objects via 3D printing often requires support structures to prevent overhanging features from collapsing during the fabrication process. While current slicing technologies offer advanced support strategies, they focus on post-processing optimizations rather than addressing the underlying need for support-efficient design during the model generation phase. This paper introduces SEG (\textit{\underline{S}upport-\underline{E}ffective \underline{G}eneration}), a novel framework that integrates Direct Preference Optimization with an Offset (ODPO) into the 3D generation pipeline to directly optimize models for minimal support material usage. By incorporating support structure simulation into the training process, SEG encourages the generation of geometries that inherently require fewer supports, thus reducing material waste and production time. We demonstrate SEG's effectiveness through extensive experiments on two benchmark datasets, Thingi10k-Val and GPT-3DP-Val, showing that SEG significantly outperforms baseline models such as TRELLIS, DPO, and DRO in terms of support volume reduction and printability. Qualitative results further reveal that SEG maintains high fidelity to input prompts while minimizing the need for support structures. Our findings highlight the potential of SEG to transform 3D printing by directly optimizing models during the generative process, paving the way for more sustainable and efficient digital fabrication practices.

</details>


### [20] [MiMo-Embodied: X-Embodied Foundation Model Technical Report](https://arxiv.org/abs/2511.16518)
*Xiaoshuai Hao,Lei Zhou,Zhijian Huang,Zhiwen Hou,Yingbo Tang,Lingfeng Zhang,Guang Li,Zheng Lu,Shuhuai Ren,Xianhui Meng,Yuchen Zhang,Jing Wu,Jinghui Lu,Chenxu Dang,Jiayi Guan,Jianhua Wu,Zhiyi Hou,Hanbing Li,Shumeng Xia,Mingliang Zhou,Yinan Zheng,Zihao Yue,Shuhao Gu,Hao Tian,Yuannan Shen,Jianwei Cui,Wen Zhang,Shaoqing Xu,Bing Wang,Haiyang Sun,Zeyu Zhu,Yuncheng Jiang,Zibin Guo,Chuhong Gong,Chaofan Zhang,Wenbo Ding,Kun Ma,Guang Chen,Rui Cai,Diyun Xiang,Heng Qu,Fuli Luo,Hangjun Ye,Long Chen*

Main category: cs.RO

TL;DR: MiMo-Embodied是首个在自动驾驶和具身AI领域都取得最先进性能的跨具身基础模型，在29个基准测试中创下新记录。


<details>
  <summary>Details</summary>
Motivation: 探索自动驾驶和具身AI两个领域之间的正迁移效应，证明通过多阶段学习可以相互增强性能。

Method: 采用多阶段学习、精选数据构建以及思维链/强化学习微调的方法。

Result: 在17个具身AI基准测试（任务规划、功能预测、空间理解）和12个自动驾驶基准测试（环境感知、状态预测、驾驶规划）中均显著超越现有开源、闭源和专业基线模型。

Conclusion: 两个领域存在强正迁移效应，多阶段学习、数据构建和微调策略是实现跨领域成功的关键因素。

Abstract: We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.

</details>


### [21] [InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy](https://arxiv.org/abs/2511.16651)
*Yang Tian,Yuyin Yang,Yiman Xie,Zetao Cai,Xu Shi,Ning Gao,Hangxu Liu,Xuekun Jiang,Zherui Qiu,Feng Yuan,Yaping Li,Ping Wang,Junhao Cai,Jia Zeng,Hao Dong,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 本文首次证明仅使用合成数据就能达到最强真实数据集在视觉-语言-动作模型预训练中的性能，展示了大规模仿真的巨大价值。


<details>
  <summary>Details</summary>
Motivation: 探索真实数据和合成数据对VLA模型泛化能力的影响，当前VLA模型主要依赖大规模真实机器人预训练，而合成数据尚未展示出可比的规模化能力。

Method: 构建InternData-A1合成数据集，包含63万条轨迹和7433小时数据，涵盖4种机器人形态、18种技能、70个任务和227个场景。采用高度自主、完全解耦的组合式仿真流水线，支持长时程技能组合、灵活任务组装和异构机器人形态。

Result: 使用与π₀相同的架构，仅用InternData-A1预训练的模型在49个仿真任务、5个真实世界任务和4个长时程灵巧任务上均达到官方π₀的性能水平，并展现出惊人的零样本仿真到真实迁移能力。

Conclusion: 大规模合成数据能够替代真实数据用于VLA模型预训练，将开源数据集和生成流水线以降低具身AI研究中可扩展数据创建的壁垒。

Abstract: Recent works explore how real and synthetic data contribute to Vision-Language-Action (VLA) models' generalization. While current VLA models have shown the strong effectiveness of large-scale real-robot pre-training, synthetic data has not previously demonstrated comparable capability at scale. This paper provides the first evidence that synthetic data alone can match the performance of the strongest $π$-dataset in pre-training a VLA model, revealing the substantial value of large-scale simulation. The resulting model also exhibits surprisingly zero-shot sim-to-real transfer on several challenging tasks. Our synthetic dataset, InternData-A1, contains over 630k trajectories and 7,433 hours across 4 embodiments, 18 skills, 70 tasks, and 227 scenes, covering rigid, articulated, deformable, and fluid-object manipulation. It is generated through a highly autonomous, fully decoupled, and compositional simulation pipeline that enables long-horizon skill composition, flexible task assembly, and heterogeneous embodiments with minimal manual tuning. Using the same architecture as $π_0$, we pre-train a model entirely on InternData-A1 and find that it matches the official $π_0$ across 49 simulation tasks, 5 real-world tasks, and 4 long-horizon dexterous tasks. We release the dataset and will open-source the generation pipeline to broaden access to large-scale robotic data and to lower the barrier to scalable data creation for embodied AI research.

</details>


### [22] [Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations](https://arxiv.org/abs/2511.16661)
*Irmak Guzey,Haozhi Qi,Julen Urain,Changhao Wang,Jessica Yin,Krishna Bodduluri,Mike Lambeta,Lerrel Pinto,Akshara Rai,Jitendra Malik,Tingfan Wu,Akash Sharma,Homanga Bharadhwaj*

Main category: cs.RO

TL;DR: AINA框架通过Aria Gen 2眼镜收集人类日常任务视频，学习多指机器人策略，无需机器人数据即可直接部署


<details>
  <summary>Details</summary>
Motivation: 解决人类与机器人之间的具身差距，从自然环境中的人类视频学习通用机器人操作策略，减少对劳动密集型机器人数据收集的依赖

Method: 使用轻便的Aria Gen 2眼镜收集人类执行任务的数据，眼镜配备高分辨率RGB相机、精确的3D头部和手部姿态跟踪、立体视觉深度估计，学习基于3D点的多指手策略

Result: 在9个日常操作任务上展示结果，策略对背景变化具有鲁棒性，可直接部署而无需机器人数据、在线修正、强化学习或仿真

Conclusion: AINA框架使从任何人、任何地点、任何环境收集的人类数据学习多指机器人策略成为可能，向实现通用机器人操作迈出了重要一步

Abstract: Learning multi-fingered robot policies from humans performing daily tasks in natural environments has long been a grand goal in the robotics community. Achieving this would mark significant progress toward generalizable robot manipulation in human environments, as it would reduce the reliance on labor-intensive robot data collection. Despite substantial efforts, progress toward this goal has been bottle-necked by the embodiment gap between humans and robots, as well as by difficulties in extracting relevant contextual and motion cues that enable learning of autonomous policies from in-the-wild human videos. We claim that with simple yet sufficiently powerful hardware for obtaining human data and our proposed framework AINA, we are now one significant step closer to achieving this dream. AINA enables learning multi-fingered policies from data collected by anyone, anywhere, and in any environment using Aria Gen 2 glasses. These glasses are lightweight and portable, feature a high-resolution RGB camera, provide accurate on-board 3D head and hand poses, and offer a wide stereo view that can be leveraged for depth estimation of the scene. This setup enables the learning of 3D point-based policies for multi-fingered hands that are robust to background changes and can be deployed directly without requiring any robot data (including online corrections, reinforcement learning, or simulation). We compare our framework against prior human-to-robot policy learning approaches, ablate our design choices, and demonstrate results across nine everyday manipulation tasks. Robot rollouts are best viewed on our website: https://aina-robot.github.io.

</details>
