<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 52]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Robust control for multi-legged elongate robots in noisy environments](https://arxiv.org/abs/2506.15788)
*Baxi Chong,Juntao He,Daniel Irvine,Tianyu Wang,Esteban Flores,Daniel Soto,Jianfeng Lin,Zhaochen Xu,Vincent R Nienhusser,Grigoriy Blekherman,Daniel I. Goldman*

Main category: cs.RO

TL;DR: 提出了一种基于机械智能（MI）和计算智能（CI）的新型多足机器人控制框架，通过模拟通信理论中的前向纠错（FEC）和自动重传请求（ARQ），实现了在复杂地形中的鲁棒运动。


<details>
  <summary>Details</summary>
Motivation: 当前多足机器人依赖高带宽传感和计算，且训练过程复杂，缺乏跨平台通用性。研究旨在开发一种简单、鲁棒的控制方法，适用于杂乱、非结构化环境。

Method: 将每条腿与地面的接触视为基本主动接触（bac），模拟通信中的比特传输。通过冗余bac实现被动机械响应（MI），并结合反馈控制（CI）增强鲁棒性。

Result: 在复杂地形（噪声高度超过机器人高度的两倍）中，机器人表现出有效且可靠的运动性能（每周期约半个身长）。

Conclusion: 该研究为多足机器人控制的系统化开发奠定了基础，为极端环境中的敏捷、鲁棒机器人系统提供了新思路。

Abstract: Modern two and four legged robots exhibit impressive mobility on complex
terrain, largely attributed to advancement in learning algorithms. However,
these systems often rely on high-bandwidth sensing and onboard computation to
perceive/respond to terrain uncertainties. Further, current locomotion
strategies typically require extensive robot-specific training, limiting their
generalizability across platforms. Building on our prior research connecting
robot-environment interaction and communication theory, we develop a new
paradigm to construct robust and simply controlled multi-legged elongate robots
(MERs) capable of operating effectively in cluttered, unstructured
environments. In this framework, each leg-ground contact is thought of as a
basic active contact (bac), akin to bits in signal transmission. Reliable
locomotion can be achieved in open-loop on "noisy" landscapes via sufficient
redundancy in bacs. In such situations, robustness is achieved through passive
mechanical responses. We term such processes as those displaying mechanical
intelligence (MI) and analogize these processes to forward error correction
(FEC) in signal transmission. To augment MI, we develop feedback control
schemes, which we refer to as computational intelligence (CI) and such
processes analogize automatic repeat request (ARQ) in signal transmission.
Integration of these analogies between locomotion and communication theory
allow analysis, design, and prediction of embodied intelligence control schemes
(integrating MI and CI) in MERs, showing effective and reliable performance
(approximately half body lengths per cycle) on complex landscapes with terrain
"noise" over twice the robot's height. Our work provides a foundation for
systematic development of MER control, paving the way for terrain-agnostic,
agile, and resilient robotic systems capable of operating in extreme
environments.

</details>


### [2] [Steering Your Diffusion Policy with Latent Space Reinforcement Learning](https://arxiv.org/abs/2506.15799)
*Andrew Wagenmaker,Mitsuhiko Nakamoto,Yunchu Zhang,Seohong Park,Waleed Yagoub,Anusha Nagabandi,Abhishek Gupta,Sergey Levine*

Main category: cs.RO

TL;DR: 论文提出了一种名为DSRL的方法，通过强化学习在潜在噪声空间中调整基于行为克隆的扩散策略，实现了高效的自适应改进。


<details>
  <summary>Details</summary>
Motivation: 行为克隆策略在初始性能不足时需要额外的人类演示，而强化学习虽然能自主改进但样本效率低。DSRL旨在结合两者的优势，实现快速自适应。

Method: DSRL在扩散策略的潜在噪声空间中运行强化学习，无需修改基础策略权重，仅需黑盒访问。

Result: DSRL在模拟和真实机器人任务中表现出高样本效率和有效性能改进。

Conclusion: DSRL为行为克隆策略的快速自适应提供了一种高效且实用的解决方案。

Abstract: Robotic control policies learned from human demonstrations have achieved
impressive results in many real-world applications. However, in scenarios where
initial performance is not satisfactory, as is often the case in novel
open-world settings, such behavioral cloning (BC)-learned policies typically
require collecting additional human demonstrations to further improve their
behavior -- an expensive and time-consuming process. In contrast, reinforcement
learning (RL) holds the promise of enabling autonomous online policy
improvement, but often falls short of achieving this due to the large number of
samples it typically requires. In this work we take steps towards enabling fast
autonomous adaptation of BC-trained policies via efficient real-world RL.
Focusing in particular on diffusion policies -- a state-of-the-art BC
methodology -- we propose diffusion steering via reinforcement learning (DSRL):
adapting the BC policy by running RL over its latent-noise space. We show that
DSRL is highly sample efficient, requires only black-box access to the BC
policy, and enables effective real-world autonomous policy improvement.
Furthermore, DSRL avoids many of the challenges associated with finetuning
diffusion policies, obviating the need to modify the weights of the base policy
at all. We demonstrate DSRL on simulated benchmarks, real-world robotic tasks,
and for adapting pretrained generalist policies, illustrating its sample
efficiency and effective performance at real-world policy improvement.

</details>


### [3] [Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning](https://arxiv.org/abs/2506.15828)
*Emanuele Musumeci,Michele Brienza,Francesco Argenziano,Vincenzo Suriani,Daniele Nardi,Domenico D. Bloisi*

Main category: cs.RO

TL;DR: 论文提出了一种结合经典规划与大型语言模型（LLMs）的方法，以解决机器人任务规划中的适应性和可行性问题。


<details>
  <summary>Details</summary>
Motivation: 经典规划方法在真实场景中因感知限制和难以将感知映射到规划谓词而表现不佳，而LLMs虽能利用常识推理但常生成不可行或不安全的计划。

Method: 通过层次化方法整合经典规划与LLMs，利用LLMs提取常识知识并逐步放松目标定义，使任务可行。

Result: 方法在3D场景图中表现出适应性和高效执行能力，优于基准方法。

Conclusion: 该方法在复杂场景中表现优异，相关代码和数据集已开源。

Abstract: Classical planning in AI and Robotics addresses complex tasks by shifting
from imperative to declarative approaches (e.g., PDDL). However, these methods
often fail in real scenarios due to limited robot perception and the need to
ground perceptions to planning predicates. This often results in heavily
hard-coded behaviors that struggle to adapt, even with scenarios where goals
can be achieved through relaxed planning. Meanwhile, Large Language Models
(LLMs) lead to planning systems that leverage commonsense reasoning but often
at the cost of generating unfeasible and/or unsafe plans. To address these
limitations, we present an approach integrating classical planning with LLMs,
leveraging their ability to extract commonsense knowledge and ground actions.
We propose a hierarchical formulation that enables robots to make unfeasible
tasks tractable by defining functionally equivalent goals through gradual
relaxation. This mechanism supports partial achievement of the intended
objective, suited to the agent's specific context. Our method demonstrates its
ability to adapt and execute tasks effectively within environments modeled
using 3D Scene Graphs through comprehensive qualitative and quantitative
evaluations. We also show how this method succeeds in complex scenarios where
other benchmark methods are more likely to fail. Code, dataset, and additional
material are released to the community.

</details>


### [4] [SafeMimic: Towards Safe and Autonomous Human-to-Robot Imitation for Mobile Manipulation](https://arxiv.org/abs/2506.15847)
*Arpit Bahety,Arnav Balaji,Ben Abbatematteo,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: SafeMimic框架通过单次人类视频演示，让机器人安全自主地学习移动操作技能，减少探索需求。


<details>
  <summary>Details</summary>
Motivation: 解决机器人从人类视频中学习移动操作任务的挑战，包括视角转换、形态适应和安全自主学习的依赖。

Method: 解析视频为分段，推断语义变化和动作，转换为第一人称视角，采样候选动作并通过安全Q函数验证。

Result: 实验表明，SafeMimic能在不同环境和用户中安全高效学习，优于现有基线方法。

Conclusion: SafeMimic为机器人从单次人类演示中学习复杂任务提供了可行方案。

Abstract: For robots to become efficient helpers in the home, they must learn to
perform new mobile manipulation tasks simply by watching humans perform them.
Learning from a single video demonstration from a human is challenging as the
robot needs to first extract from the demo what needs to be done and how,
translate the strategy from a third to a first-person perspective, and then
adapt it to be successful with its own morphology. Furthermore, to mitigate the
dependency on costly human monitoring, this learning process should be
performed in a safe and autonomous manner. We present SafeMimic, a framework to
learn new mobile manipulation skills safely and autonomously from a single
third-person human video. Given an initial human video demonstration of a
multi-step mobile manipulation task, SafeMimic first parses the video into
segments, inferring both the semantic changes caused and the motions the human
executed to achieve them and translating them to an egocentric reference. Then,
it adapts the behavior to the robot's own morphology by sampling candidate
actions around the human ones, and verifying them for safety before execution
in a receding horizon fashion using an ensemble of safety Q-functions trained
in simulation. When safe forward progression is not possible, SafeMimic
backtracks to previous states and attempts a different sequence of actions,
adapting both the trajectory and the grasping modes when required for its
morphology. As a result, SafeMimic yields a strategy that succeeds in the
demonstrated behavior and learns task-specific actions that reduce exploration
in future attempts. Our experiments show that our method allows robots to
safely and efficiently learn multi-step mobile manipulation behaviors from a
single human demonstration, from different users, and in different
environments, with improvements over state-of-the-art baselines across seven
tasks

</details>


### [5] [PRISM-Loc: a Lightweight Long-range LiDAR Localization in Urban Environments with Topological Maps](https://arxiv.org/abs/2506.15849)
*Kirill Muravyev,Vasily Yuryev,Oleg Bulichev,Dmitry Yudin,Konstantin Yakovlev*

Main category: cs.RO

TL;DR: PRISM-Loc是一种基于拓扑地图的定位方法，用于大型环境中的实时定位，结合全局地点识别和局部位姿估计，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在长距离导航中，使用密集全局激光雷达地图进行实时定位可能困难且占用内存，拓扑地图提供了一种更高效的替代方案。

Method: 提出PRISM-Loc方法，采用双重定位流程：全局地点识别和局部位姿估计（基于2D特征和点优化的激光雷达扫描匹配算法）。

Result: 在3公里路线的ITLP-Campus数据集上测试，PRISM-Loc在质量和计算效率上均优于现有方法。

Conclusion: PRISM-Loc是一种高效且准确的拓扑地图定位方法，适用于大型环境中的实时导航。

Abstract: Localization in the environment is one of the crucial tasks of navigation of
a mobile robot or a self-driving vehicle. For long-range routes, performing
localization within a dense global lidar map in real time may be difficult, and
the creation of such a map may require much memory. To this end, leveraging
topological maps may be useful. In this work, we propose PRISM-Loc -- a
topological map-based approach for localization in large environments. The
proposed approach leverages a twofold localization pipeline, which consists of
global place recognition and estimation of the local pose inside the found
location. For local pose estimation, we introduce an original lidar scan
matching algorithm, which is based on 2D features and point-based optimization.
We evaluate the proposed method on the ITLP-Campus dataset on a 3 km route, and
compare it against the state-of-the-art metric map-based and place
recognition-based competitors. The results of the experiments show that the
proposed method outperforms its competitors both quality-wise and
computationally-wise.

</details>


### [6] [Semantic and Feature Guided Uncertainty Quantification of Visual Localization for Autonomous Vehicles](https://arxiv.org/abs/2506.15851)
*Qiyuan Wu,Mark Campbell*

Main category: cs.RO

TL;DR: 本文提出了一种用于自动驾驶视觉定位的不确定性量化方法，通过轻量级传感器误差模型学习测量不确定性，并在Ithaca365数据集上验证了其准确性。


<details>
  <summary>Details</summary>
Motivation: 传感器测量与深度学习网络的不确定性量化对机器人系统（如自动驾驶汽车）至关重要，尤其是在安全关键应用中。

Method: 使用轻量级传感器误差模型，将图像特征和语义信息映射到二维误差分布，以学习测量不确定性。

Result: 结果表明，在恶劣天气和光照条件下，测量误差不符合高斯分布，而高斯混合模型能更好地预测误差。

Conclusion: 该方法能够根据匹配图像对的特定上下文进行不确定性估计，并隐含捕获其他关键因素（如城市与高速公路、动态与静态场景、冬季与夏季）。

Abstract: The uncertainty quantification of sensor measurements coupled with deep
learning networks is crucial for many robotics systems, especially for
safety-critical applications such as self-driving cars. This paper develops an
uncertainty quantification approach in the context of visual localization for
autonomous driving, where locations are selected based on images. Key to our
approach is to learn the measurement uncertainty using light-weight sensor
error model, which maps both image feature and semantic information to
2-dimensional error distribution. Our approach enables uncertainty estimation
conditioned on the specific context of the matched image pair, implicitly
capturing other critical, unannotated factors (e.g., city vs highway, dynamic
vs static scenes, winter vs summer) in a latent manner. We demonstrate the
accuracy of our uncertainty prediction framework using the Ithaca365 dataset,
which includes variations in lighting and weather (sunny, night, snowy). Both
the uncertainty quantification of the sensor+network is evaluated, along with
Bayesian localization filters using unique sensor gating method. Results show
that the measurement error does not follow a Gaussian distribution with poor
weather and lighting conditions, and is better predicted by our Gaussian
Mixture model.

</details>


### [7] [Improving Robotic Manipulation: Techniques for Object Pose Estimation, Accommodating Positional Uncertainty, and Disassembly Tasks from Examples](https://arxiv.org/abs/2506.15865)
*Viral Rasik Galaiya*

Main category: cs.RO

TL;DR: 论文探讨了在非结构化环境中使用触觉传感和强化学习来提升机器人对物体姿态的感知和抓取效率。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，机器人需要更强的环境感知能力以应对不确定性。尽管摄像头常用于机器人任务，但其局限性（如遮挡、视野和信息广度）促使转向触觉传感的研究。

Method: 利用触觉传感的时序特征确定物体姿态；结合强化学习和触觉碰撞减少抓取尝试次数；通过触觉传感器信息指导强化学习代理规划轨迹，并从人类示例中迁移学习以减少训练时间。

Result: 触觉传感和强化学习的结合能够有效提升机器人对物体姿态的感知和抓取效率，同时减少训练时间。

Conclusion: 触觉传感与强化学习的结合为机器人在非结构化环境中的操作提供了更高效和适应性更强的解决方案。

Abstract: To use robots in more unstructured environments, we have to accommodate for
more complexities. Robotic systems need more awareness of the environment to
adapt to uncertainty and variability. Although cameras have been predominantly
used in robotic tasks, the limitations that come with them, such as occlusion,
visibility and breadth of information, have diverted some focus to tactile
sensing. In this thesis, we explore the use of tactile sensing to determine the
pose of the object using the temporal features. We then use reinforcement
learning with tactile collisions to reduce the number of attempts required to
grasp an object resulting from positional uncertainty from camera estimates.
Finally, we use information provided by these tactile sensors to a
reinforcement learning agent to determine the trajectory to take to remove an
object from a restricted passage while reducing training time by pertaining
from human examples.

</details>


### [8] [CooperRisk: A Driving Risk Quantification Pipeline with Multi-Agent Cooperative Perception and Prediction](https://arxiv.org/abs/2506.15868)
*Mingyue Lei,Zewei Zhou,Hongchen Li,Jia Hu,Jiaqi Ma*

Main category: cs.RO

TL;DR: 论文提出了一种基于V2X的风险量化框架CooperRisk，通过多智能体感知信息融合和未来时间戳的风险量化，生成可解释的场景风险地图，并利用基于学习的协作预测模型捕捉多智能体交互。


<details>
  <summary>Details</summary>
Motivation: 单车辆系统在复杂密集场景中的感知范围和遮挡限制风险量化能力，V2X虽能共享感知信息，但如何确保风险可解释性并理解多智能体交互仍是一个开放问题。

Method: 设计了基于Transformer的风险导向预测模型，考虑多模态和多智能体因素，确保场景一致的未来行为预测，避免冲突预测导致保守风险量化。

Result: 在V2XPnP数据集上的实验显示，CooperRisk在风险量化方面表现优异，冲突率降低了44.35%。

Conclusion: CooperRisk通过V2X技术有效提升了风险量化的准确性和可解释性，为自动驾驶安全提供了新思路。

Abstract: Risk quantification is a critical component of safe autonomous driving,
however, constrained by the limited perception range and occlusion of
single-vehicle systems in complex and dense scenarios. Vehicle-to-everything
(V2X) paradigm has been a promising solution to sharing complementary
perception information, nevertheless, how to ensure the risk interpretability
while understanding multi-agent interaction with V2X remains an open question.
In this paper, we introduce the first V2X-enabled risk quantification pipeline,
CooperRisk, to fuse perception information from multiple agents and quantify
the scenario driving risk in future multiple timestamps. The risk is
represented as a scenario risk map to ensure interpretability based on risk
severity and exposure, and the multi-agent interaction is captured by the
learning-based cooperative prediction model. We carefully design a
risk-oriented transformer-based prediction model with multi-modality and
multi-agent considerations. It aims to ensure scene-consistent future behaviors
of multiple agents and avoid conflicting predictions that could lead to overly
conservative risk quantification and cause the ego vehicle to become overly
hesitant to drive. Then, the temporal risk maps could serve to guide a model
predictive control planner. We evaluate the CooperRisk pipeline in a real-world
V2X dataset V2XPnP, and the experiments demonstrate its superior performance in
risk quantification, showing a 44.35% decrease in conflict rate between the ego
vehicle and background traffic participants.

</details>


### [9] [A Small-Scale Robot for Autonomous Driving: Design, Challenges, and Best Practices](https://arxiv.org/abs/2506.15870)
*Hossein Maghsoumi,Yaser Fallah*

Main category: cs.RO

TL;DR: 本文探讨了六分之一比例的小型自动驾驶车辆平台的设计、硬件与软件集成及其开发中的常见挑战，旨在提升其可靠性和性能，推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 小型自动驾驶车辆平台成本效益高，但特定配置的研究不足，限制了对其潜力的全面认识。

Method: 通过分析六分之一比例平台的设计、硬件与软件集成，提出解决机械和电子问题的方案，并制定改进指南。

Result: 提供了提升小型车辆平台可靠性和性能的实用方法，扩展了其在自动驾驶算法测试中的应用。

Conclusion: 通过分享经验，本文旨在促进小型自动驾驶车辆平台的研究和应用，推动该领域的发展。

Abstract: Small-scale autonomous vehicle platforms provide a cost-effective environment
for developing and testing advanced driving systems. However, specific
configurations within this scale are underrepresented, limiting full awareness
of their potential. This paper focuses on a one-sixth-scale setup, offering a
high-level overview of its design, hardware and software integration, and
typical challenges encountered during development. We discuss methods for
addressing mechanical and electronic issues common to this scale and propose
guidelines for improving reliability and performance. By sharing these
insights, we aim to expand the utility of small-scale vehicles for testing
autonomous driving algorithms and to encourage further research in this domain.

</details>


### [10] [Challenges and Research Directions from the Operational Use of a Machine Learning Damage Assessment System via Small Uncrewed Aerial Systems at Hurricanes Debby and Helene](https://arxiv.org/abs/2506.15890)
*Thomas Manzini,Priyankari Perali,Robin R. Murphy,David Merrick*

Main category: cs.RO

TL;DR: 论文总结了在飓风Debby和Helene中使用小型无人机（sUAS）进行机器学习（ML）损害评估时遇到的四个主要挑战，并提出了三个未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决实际部署中sUAS和ML系统在灾害损害评估中的操作性问题，填补了文献中实际部署案例的空白。

Method: 通过分析飓风Debby和Helene中收集的无人机图像数据（包括不同分辨率和地理空间对齐问题），评估ML模型的性能。

Result: 识别了四个主要挑战（图像分辨率变化、地理空间对齐问题、无线连接依赖性和数据格式），并提出了三个改进建议。

Conclusion: 未来研究应关注提高ML模型对分辨率变化的适应性、处理地理空间对齐问题，并减少对无线连接的依赖，以提升灾害响应效率。

Abstract: This paper details four principal challenges encountered with machine
learning (ML) damage assessment using small uncrewed aerial systems (sUAS) at
Hurricanes Debby and Helene that prevented, degraded, or delayed the delivery
of data products during operations and suggests three research directions for
future real-world deployments. The presence of these challenges is not
surprising given that a review of the literature considering both datasets and
proposed ML models suggests this is the first sUAS-based ML system for disaster
damage assessment actually deployed as a part of real-world operations. The
sUAS-based ML system was applied by the State of Florida to Hurricanes Helene
(2 orthomosaics, 3.0 gigapixels collected over 2 sorties by a Wintra WingtraOne
sUAS) and Debby (1 orthomosaic, 0.59 gigapixels collected via 1 sortie by a
Wintra WingtraOne sUAS) in Florida. The same model was applied to crewed aerial
imagery of inland flood damage resulting from post-tropical remnants of
Hurricane Debby in Pennsylvania (436 orthophotos, 136.5 gigapixels), providing
further insights into the advantages and limitations of sUAS for disaster
response. The four challenges (variationin spatial resolution of input imagery,
spatial misalignment between imagery and geospatial data, wireless
connectivity, and data product format) lead to three recommendations that
specify research needed to improve ML model capabilities to accommodate the
wide variation of potential spatial resolutions used in practice, handle
spatial misalignment, and minimize the dependency on wireless connectivity.
These recommendations are expected to improve the effective operational use of
sUAS and sUAS-based ML damage assessment systems for disaster response.

</details>


### [11] [Advancing Autonomous Racing: A Comprehensive Survey of the RoboRacer (F1TENTH) Platform](https://arxiv.org/abs/2506.15899)
*Israel Charles,Hossein Maghsoumi,Yaser Fallah*

Main category: cs.RO

TL;DR: 本文对RoboRacer（F1TENTH）平台进行了全面调查，分析了其硬件和软件架构、研究应用及教育作用，强调了其在自动驾驶研究中的重要性。


<details>
  <summary>Details</summary>
Motivation: RoboRacer平台为自动驾驶研究提供了可扩展、经济高效且社区驱动的实验环境，本文旨在总结其贡献和潜力。

Method: 通过分析平台的模块化架构、Sim2Real技术、仿真环境集成、数据集和算法进展，以及竞赛和合作研究成果。

Result: 研究发现RoboRacer是加速创新和连接理论研究与实际部署的多功能框架。

Conclusion: RoboRacer在自动驾驶赛车和机器人领域的发展中具有重要意义。

Abstract: The RoboRacer (F1TENTH) platform has emerged as a leading testbed for
advancing autonomous driving research, offering a scalable, cost-effective, and
community-driven environment for experimentation. This paper presents a
comprehensive survey of the platform, analyzing its modular hardware and
software architecture, diverse research applications, and role in autonomous
systems education. We examine critical aspects such as bridging the
simulation-to-reality (Sim2Real) gap, integration with simulation environments,
and the availability of standardized datasets and benchmarks. Furthermore, the
survey highlights advancements in perception, planning, and control algorithms,
as well as insights from global competitions and collaborative research
efforts. By consolidating these contributions, this study positions RoboRacer
as a versatile framework for accelerating innovation and bridging the gap
between theoretical research and real-world deployment. The findings underscore
the platform's significance in driving forward developments in autonomous
racing and robotics.

</details>


### [12] [Learning from Planned Data to Improve Robotic Pick-and-Place Planning Efficiency](https://arxiv.org/abs/2506.15920)
*Liang Qin,Weiwei Wan,Jun Takahashi,Ryo Negishi,Masaki Matsushita,Kensuke Harada*

Main category: cs.RO

TL;DR: 提出了一种基于能量模型的学习方法，用于加速机器人拾放任务中的共享抓取预测。


<details>
  <summary>Details</summary>
Motivation: 传统分析方法在解决共享抓取时需单独评估每个候选抓取，计算开销大。

Method: 引入能量模型（EBM），通过结合初始和目标物体位姿的可行抓取能量来预测共享抓取。

Result: 实验表明，该方法提升了抓取选择性能，数据效率更高，并能泛化到未见过的抓取和类似形状物体。

Conclusion: 该方法显著减少了搜索空间，提高了拾放任务规划的效率和泛化能力。

Abstract: This work proposes a learning method to accelerate robotic pick-and-place
planning by predicting shared grasps. Shared grasps are defined as grasp poses
feasible to both the initial and goal object configurations in a pick-and-place
task. Traditional analytical methods for solving shared grasps evaluate grasp
candidates separately, leading to substantial computational overhead as the
candidate set grows. To overcome the limitation, we introduce an Energy-Based
Model (EBM) that predicts shared grasps by combining the energies of feasible
grasps at both object poses. This formulation enables early identification of
promising candidates and significantly reduces the search space. Experiments
show that our method improves grasp selection performance, offers higher data
efficiency, and generalizes well to unseen grasps and similarly shaped objects.

</details>


### [13] [KARL: Kalman-Filter Assisted Reinforcement Learner for Dynamic Object Tracking and Grasping](https://arxiv.org/abs/2506.15945)
*Kowndinya Boyalakuntla,Abdeslam Boularias,Jingjin Yu*

Main category: cs.RO

TL;DR: KARL是一种结合卡尔曼滤波和强化学习的动态物体跟踪与抓取系统，显著提升了在复杂环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 扩展眼在手（EoH）系统在挑战性环境中的动态物体跟踪与抓取能力。

Method: 1. 六阶段强化学习课程；2. 卡尔曼滤波层集成；3. 失败恢复机制。

Result: 在仿真和实际实验中，KARL表现出更高的抓取成功率和更快的执行速度。

Conclusion: KARL在动态物体跟踪与抓取任务中优于现有系统。

Abstract: We present Kalman-filter Assisted Reinforcement Learner (KARL) for dynamic
object tracking and grasping over eye-on-hand (EoH) systems, significantly
expanding such systems capabilities in challenging, realistic environments. In
comparison to the previous state-of-the-art, KARL (1) incorporates a novel
six-stage RL curriculum that doubles the system's motion range, thereby greatly
enhancing the system's grasping performance, (2) integrates a robust Kalman
filter layer between the perception and reinforcement learning (RL) control
modules, enabling the system to maintain an uncertain but continuous 6D pose
estimate even when the target object temporarily exits the camera's
field-of-view or undergoes rapid, unpredictable motion, and (3) introduces
mechanisms to allow retries to gracefully recover from unavoidable policy
execution failures. Extensive evaluations conducted in both simulation and
real-world experiments qualitatively and quantitatively corroborate KARL's
advantage over earlier systems, achieving higher grasp success rates and faster
robot execution speed. Source code and supplementary materials for KARL will be
made available at: https://github.com/arc-l/karl.

</details>


### [14] [ViTacFormer: Learning Cross-Modal Representation for Visuo-Tactile Dexterous Manipulation](https://arxiv.org/abs/2506.15953)
*Liang Heng,Haoran Geng,Kaifeng Zhang,Pieter Abbeel,Jitendra Malik*

Main category: cs.RO

TL;DR: ViTacFormer是一种结合视觉和触觉的表示学习方法，通过跨模态编码器和自回归触觉预测头，提升机器人灵巧操作的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作是机器人系统实现类人交互的关键能力，而触觉感知在非结构化或视觉遮挡环境中尤为重要。

Method: 提出ViTacFormer架构，结合跨注意力编码器融合视觉和触觉信息，并设计由易到难的课程学习策略优化表示空间。

Result: 在真实世界基准测试中，成功率比现有技术高50%，首次实现长序列灵巧操作任务（11个阶段，持续2.5分钟）。

Conclusion: ViTacFormer通过跨模态学习和课程训练，显著提升了机器人灵巧操作的性能和适应性。

Abstract: Dexterous manipulation is a cornerstone capability for robotic systems aiming
to interact with the physical world in a human-like manner. Although
vision-based methods have advanced rapidly, tactile sensing remains crucial for
fine-grained control, particularly in unstructured or visually occluded
settings. We present ViTacFormer, a representation-learning approach that
couples a cross-attention encoder to fuse high-resolution vision and touch with
an autoregressive tactile prediction head that anticipates future contact
signals. Building on this architecture, we devise an easy-to-challenging
curriculum that steadily refines the visual-tactile latent space, boosting both
accuracy and robustness. The learned cross-modal representation drives
imitation learning for multi-fingered hands, enabling precise and adaptive
manipulation. Across a suite of challenging real-world benchmarks, our method
achieves approximately 50% higher success rates than prior state-of-the-art
systems. To our knowledge, it is also the first to autonomously complete
long-horizon dexterous manipulation tasks that demand highly precise control
with an anthropomorphic hand, successfully executing up to 11 sequential stages
and sustaining continuous operation for 2.5 minutes.

</details>


### [15] [A Low-Cost Portable Lidar-based Mobile Mapping System on an Android Smartphone](https://arxiv.org/abs/2506.15983)
*Jianzhu Huai,Yuxin Shao,Yujia Zhang,Alper Yilmaz*

Main category: cs.RO

TL;DR: 本文提出了一种低成本、便携式的移动测绘系统，结合激光雷达、Android智能手机和RTK-GNSS模块，实现了高性价比的实时测绘。


<details>
  <summary>Details</summary>
Motivation: 当前移动测绘系统成本高或性能有限，无法满足元宇宙、数字孪生和机器人等领域对低成本便携设备的需求。

Method: 系统集成了激光雷达、智能手机和RTK-GNSS模块，利用Android平台开发激光雷达惯性里程计，并记录多传感器数据。

Result: 系统总成本低于2000美元，重量约1千克，在跟踪和测绘方面表现良好。

Conclusion: 该系统为低成本便携测绘提供了可行方案，并开源了设计和软件。

Abstract: The rapid advancement of the metaverse, digital twins, and robotics
underscores the demand for low-cost, portable mapping systems for reality
capture. Current mobile solutions, such as the Leica BLK2Go and lidar-equipped
smartphones, either come at a high cost or are limited in range and accuracy.
Leveraging the proliferation and technological evolution of mobile devices
alongside recent advancements in lidar technology, we introduce a novel,
low-cost, portable mobile mapping system. Our system integrates a lidar unit,
an Android smartphone, and an RTK-GNSS stick. Running on the Android platform,
it features lidar-inertial odometry built with the NDK, and logs data from the
lidar, wide-angle camera, IMU, and GNSS. With a total bill of materials (BOM)
cost under 2,000 USD and a weight of about 1 kilogram, the system achieves a
good balance between affordability and portability. We detail the system
design, multisensor calibration, synchronization, and evaluate its performance
for tracking and mapping. To further contribute to the community, the system's
design and software are made open source at:
https://github.com/OSUPCVLab/marslogger_android/releases/tag/v2.1

</details>


### [16] [DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware Planning](https://arxiv.org/abs/2506.16012)
*Boyu Li,Siyuan He,Hang Xu,Haoqi Yuan,Yu Zang,Liwei Hu,Junpeng Yue,Zhenxiong Jiang,Pengbo Hu,Börje F. Karlsson,Yehui Tang,Zongqing Lu*

Main category: cs.RO

TL;DR: 论文提出了一种基于物理的仿真平台DualTHOR，用于复杂双臂人形机器人，旨在解决现有平台在真实世界机器人应用中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前仿真平台依赖简化的机器人形态并忽略低级执行的随机性，限制了其在真实世界中的可迁移性。

Method: 基于扩展版AI2-THOR构建DualTHOR，包含真实机器人资产、双臂协作任务套件和人形机器人逆运动学求解器，并引入基于物理的应急机制。

Result: 评估显示当前视觉语言模型在双臂协调和应急环境中的鲁棒性不足，凸显了DualTHOR的重要性。

Conclusion: DualTHOR为开发更强大的视觉语言模型提供了更全面的仿真环境，有助于提升真实世界任务的性能。

Abstract: Developing embodied agents capable of performing complex interactive tasks in
real-world scenarios remains a fundamental challenge in embodied AI. Although
recent advances in simulation platforms have greatly enhanced task diversity to
train embodied Vision Language Models (VLMs), most platforms rely on simplified
robot morphologies and bypass the stochastic nature of low-level execution,
which limits their transferability to real-world robots. To address these
issues, we present a physics-based simulation platform DualTHOR for complex
dual-arm humanoid robots, built upon an extended version of AI2-THOR. Our
simulator includes real-world robot assets, a task suite for dual-arm
collaboration, and inverse kinematics solvers for humanoid robots. We also
introduce a contingency mechanism that incorporates potential failures through
physics-based low-level execution, bridging the gap to real-world scenarios.
Our simulator enables a more comprehensive evaluation of the robustness and
generalization of VLMs in household environments. Extensive evaluations reveal
that current VLMs struggle with dual-arm coordination and exhibit limited
robustness in realistic environments with contingencies, highlighting the
importance of using our simulator to develop more capable VLMs for embodied
tasks. The code is available at https://github.com/ds199895/DualTHOR.git.

</details>


### [17] [Noise Fusion-based Distillation Learning for Anomaly Detection in Complex Industrial Environments](https://arxiv.org/abs/2506.16050)
*Jiawen Yu,Jieji Ren,Yang Chang,Qiaojun Yu,Xuan Tong,Boyang Wang,Yan Song,You Li,Xinji Mai,Wenqiang Zhang*

Main category: cs.RO

TL;DR: 提出了一种基于异构教师网络（HetNet）的新方法，用于复杂工业环境中的异常检测与定位，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂、非结构化的工业环境中检测工件缺陷时表现不佳，亟需一种更鲁棒的方法。

Method: 采用异构教师网络（HetNet）、自适应局部-全局特征融合模块和局部多元高斯噪声生成模块。

Result: 在主流基准测试中表现优异，MSC-AD指标提升约10%，并在其他数据集上达到SOTA。

Conclusion: HetNet能有效应对环境波动，提升工业异常检测系统的可靠性，适用于实时检测。

Abstract: Anomaly detection and localization in automated industrial manufacturing can
significantly enhance production efficiency and product quality. Existing
methods are capable of detecting surface defects in pre-defined or controlled
imaging environments. However, accurately detecting workpiece defects in
complex and unstructured industrial environments with varying views, poses and
illumination remains challenging. We propose a novel anomaly detection and
localization method specifically designed to handle inputs with perturbative
patterns. Our approach introduces a new framework based on a collaborative
distillation heterogeneous teacher network (HetNet), an adaptive local-global
feature fusion module, and a local multivariate Gaussian noise generation
module. HetNet can learn to model the complex feature distribution of normal
patterns using limited information about local disruptive changes. We conducted
extensive experiments on mainstream benchmarks. HetNet demonstrates superior
performance with approximately 10% improvement across all evaluation metrics on
MSC-AD under industrial conditions, while achieving state-of-the-art results on
other datasets, validating its resilience to environmental fluctuations and its
capability to enhance the reliability of industrial anomaly detection systems
across diverse scenarios. Tests in real-world environments further confirm that
HetNet can be effectively integrated into production lines to achieve robust
and real-time anomaly detection. Codes, images and videos are published on the
project website at: https://zihuatanejoyu.github.io/HetNet/

</details>


### [18] [Investigating Lagrangian Neural Networks for Infinite Horizon Planning in Quadrupedal Locomotion](https://arxiv.org/abs/2506.16079)
*Prakrut Kotecha,Aditya Shirwatkar,Shishir Kolathaya*

Main category: cs.RO

TL;DR: LNNs利用归纳偏置学习系统动力学，比传统方法更高效（10倍样本效率）和准确（2-10倍），适用于四足机器人无限时域规划。


<details>
  <summary>Details</summary>
Motivation: 传统动力学模型在长时域预测中误差累积，而LNNs能保持物理规律，实现稳定预测，适用于可持续运动。

Method: 评估四种LNNs动力学模型：全阶正向动力学、对角化质量矩阵、全阶逆向动力学、降阶模型（CoM动力学）。

Result: LNNs在样本效率和预测准确性上显著优于基线方法，对角化方法降低计算复杂度并保持可解释性。

Conclusion: LNNs能有效捕捉四足机器人动力学结构，提升运动规划与控制性能，并实现更高控制频率，适合实际部署。

Abstract: Lagrangian Neural Networks (LNNs) present a principled and interpretable
framework for learning the system dynamics by utilizing inductive biases. While
traditional dynamics models struggle with compounding errors over long
horizons, LNNs intrinsically preserve the physical laws governing any system,
enabling accurate and stable predictions essential for sustainable locomotion.
This work evaluates LNNs for infinite horizon planning in quadrupedal robots
through four dynamics models: (1) full-order forward dynamics (FD) training and
inference, (2) diagonalized representation of Mass Matrix in full order FD, (3)
full-order inverse dynamics (ID) training with FD inference, (4) reduced-order
modeling via torso centre-of-mass (CoM) dynamics. Experiments demonstrate that
LNNs bring improvements in sample efficiency (10x) and superior prediction
accuracy (up to 2-10x) compared to baseline methods. Notably, the
diagonalization approach of LNNs reduces computational complexity while
retaining some interpretability, enabling real-time receding horizon control.
These findings highlight the advantages of LNNs in capturing the underlying
structure of system dynamics in quadrupeds, leading to improved performance and
efficiency in locomotion planning and control. Additionally, our approach
achieves a higher control frequency than previous LNN methods, demonstrating
its potential for real-world deployment on quadrupeds.

</details>


### [19] [From Theory to Practice: Identifying the Optimal Approach for Offset Point Tracking in the Context of Agricultural Robotics](https://arxiv.org/abs/2506.16143)
*Stephane Ngnepiepaye Wembe,Vincent Rousseau,Johann Laconte,Roland Lenain*

Main category: cs.RO

TL;DR: 本文提出了一种针对农业机器人工具的预测控制策略，通过预测工具的运动来提升跟踪性能，解决了传统控制策略忽略工具实际工作点的问题。


<details>
  <summary>Details</summary>
Motivation: 现代农业面临劳动力短缺和环境压力，农业机器人成为解决方案，但现有控制策略多关注机器人本体，忽略了工具的实际工作点，尤其是在非直线作物行中。

Method: 提出了一种预测控制策略，专注于工具的参考点，通过预测工具的运动来优化跟踪性能。

Result: 该方法有效减少了工具在转弯时的过冲现象，提升了操作的精确性。

Conclusion: 该控制策略为农业机器人提供了更精确的工具操作能力，适应现代农业的非直线作物行需求。

Abstract: Modern agriculture faces escalating challenges: increasing demand for food,
labor shortages, and the urgent need to reduce environmental impact.
Agricultural robotics has emerged as a promising response to these pressures,
enabling the automation of precise and suitable field operations. In
particular, robots equipped with implements for tasks such as weeding or sowing
must interact delicately and accurately with the crops and soil. Unlike robots
in other domains, these agricultural platforms typically use rigidly mounted
implements, where the implement's position is more critical than the robot's
center in determining task success. Yet, most control strategies in the
literature focus on the vehicle body, often neglecting the acctual working
point of the system. This is particularly important when considering new
agriculture practices where crops row are not necessary straights. This paper
presents a predictive control strategy targeting the implement's reference
point. The method improves tracking performance by anticipating the motion of
the implement, which, due to its offset from the vehicle's center of rotation,
is prone to overshooting during turns if not properly accounted for.

</details>


### [20] [Single-Microphone-Based Sound Source Localization for Mobile Robots in Reverberant Environments](https://arxiv.org/abs/2506.16173)
*Jiang Wang,Runwu Shi,Benjamin Yen,He Kong,Kazuhiro Nakadai*

Main category: cs.RO

TL;DR: 提出了一种基于单麦克风的在线声源定位方法，适用于移动机器人在混响环境中。


<details>
  <summary>Details</summary>
Motivation: 现有声源定位方法通常需要至少两个麦克风，限制了其应用范围。

Method: 开发了一个轻量级神经网络模型（43k参数），通过提取混响信号的时域信息进行实时距离估计，并结合扩展卡尔曼滤波器实现在线定位。

Result: 实验证明了该方法的有效性和优势。

Conclusion: 这是首个在移动机器人上使用单麦克风实现在线声源定位的工作，填补了研究空白，并开源了代码。

Abstract: Accurately estimating sound source positions is crucial for robot audition.
However, existing sound source localization methods typically rely on a
microphone array with at least two spatially preconfigured microphones. This
requirement hinders the applicability of microphone-based robot audition
systems and technologies. To alleviate these challenges, we propose an online
sound source localization method that uses a single microphone mounted on a
mobile robot in reverberant environments. Specifically, we develop a
lightweight neural network model with only 43k parameters to perform real-time
distance estimation by extracting temporal information from reverberant
signals. The estimated distances are then processed using an extended Kalman
filter to achieve online sound source localization. To the best of our
knowledge, this is the first work to achieve online sound source localization
using a single microphone on a moving robot, a gap that we aim to fill in this
work. Extensive experiments demonstrate the effectiveness and merits of our
approach. To benefit the broader research community, we have open-sourced our
code at https://github.com/JiangWAV/single-mic-SSL.

</details>


### [21] [FlowRAM: Grounding Flow Matching Policy with Region-Aware Mamba Framework for Robotic Manipulation](https://arxiv.org/abs/2506.16201)
*Sen Wang,Le Wang,Sanping Zhou,Jingyi Tian,Jiayi Li,Haowen Sun,Wei Tang*

Main category: cs.RO

TL;DR: FlowRAM提出了一种基于生成模型的高效机器人操作框架，通过动态半径调度和条件流匹配优化感知与动作生成，显著提升了精度和速度。


<details>
  <summary>Details</summary>
Motivation: 解决当前扩散策略学习方法计算效率低且未充分利用生成模型在3D环境中的潜力的问题。

Method: 结合动态半径调度实现自适应感知，利用状态空间模型整合多模态信息，并通过条件流匹配学习动作位姿。

Result: 在RLBench基准测试中表现优异，平均成功率提升12%，且推理速度显著加快。

Conclusion: FlowRAM在高效性和精度上均优于现有方法，适用于高精度机器人操作任务。

Abstract: Robotic manipulation in high-precision tasks is essential for numerous
industrial and real-world applications where accuracy and speed are required.
Yet current diffusion-based policy learning methods generally suffer from low
computational efficiency due to the iterative denoising process during
inference. Moreover, these methods do not fully explore the potential of
generative models for enhancing information exploration in 3D environments. In
response, we propose FlowRAM, a novel framework that leverages generative
models to achieve region-aware perception, enabling efficient multimodal
information processing. Specifically, we devise a Dynamic Radius Schedule,
which allows adaptive perception, facilitating transitions from global scene
comprehension to fine-grained geometric details. Furthermore, we integrate
state space models to integrate multimodal information, while preserving linear
computational complexity. In addition, we employ conditional flow matching to
learn action poses by regressing deterministic vector fields, simplifying the
learning process while maintaining performance. We verify the effectiveness of
the FlowRAM in the RLBench, an established manipulation benchmark, and achieve
state-of-the-art performance. The results demonstrate that FlowRAM achieves a
remarkable improvement, particularly in high-precision tasks, where it
outperforms previous methods by 12.0% in average success rate. Additionally,
FlowRAM is able to generate physically plausible actions for a variety of
real-world tasks in less than 4 time steps, significantly increasing inference
speed.

</details>


### [22] [ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models](https://arxiv.org/abs/2506.16211)
*Puhao Li,Yingying Wu,Ziheng Xi,Wanlin Li,Yuzhe Huang,Zhiyuan Zhang,Yinghan Chen,Jianan Wang,Song-Chun Zhu,Tengyu Liu,Siyuan Huang*

Main category: cs.RO

TL;DR: ControlVLA框架通过结合预训练的VLA模型和对象中心表示，实现了在少量演示下高效微调，显著提升了机器人操作的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在少量演示下难以适应真实世界机器人操作，且依赖仿真数据或预建模块，存在仿真到现实的差距和扩展性问题。

Method: 提出ControlVLA，通过ControlNet风格架构将预训练VLA模型与对象中心表示结合，并零初始化投影层以逐步适应任务。

Result: 在6个真实任务中，仅需10-20次演示即达到76.7%成功率，优于传统方法所需的100次演示。

Conclusion: ControlVLA在少量演示下表现出高效性和扩展性，适用于长时任务和未知对象及背景。

Abstract: Learning real-world robotic manipulation is challenging, particularly when
limited demonstrations are available. Existing methods for few-shot
manipulation often rely on simulation-augmented data or pre-built modules like
grasping and pose estimation, which struggle with sim-to-real gaps and lack
extensibility. While large-scale imitation pre-training shows promise, adapting
these general-purpose policies to specific tasks in data-scarce settings
remains unexplored. To achieve this, we propose ControlVLA, a novel framework
that bridges pre-trained VLA models with object-centric representations via a
ControlNet-style architecture for efficient fine-tuning. Specifically, to
introduce object-centric conditions without overwriting prior knowledge,
ControlVLA zero-initializes a set of projection layers, allowing them to
gradually adapt the pre-trained manipulation policies. In real-world
experiments across 6 diverse tasks, including pouring cubes and folding
clothes, our method achieves a 76.7% success rate while requiring only 10-20
demonstrations -- a significant improvement over traditional approaches that
require more than 100 demonstrations to achieve comparable success. Additional
experiments highlight ControlVLA's extensibility to long-horizon tasks and
robustness to unseen objects and backgrounds.

</details>


### [23] [Probabilistic Collision Risk Estimation for Pedestrian Navigation](https://arxiv.org/abs/2506.16219)
*Amine Tourki,Paul Prevel,Nils Einecke,Tim Puphal,Alexandre Alahi*

Main category: cs.RO

TL;DR: 将自动驾驶中的风险模型技术应用于视觉障碍辅助设备，实验证明其警告准确性优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 智能视觉障碍辅助设备发展滞后于智能驾驶辅助系统，需引入更先进的技术提升其性能。

Method: 将自动驾驶中使用的风险模型技术整合到视觉障碍辅助设备中，计算物体轨迹的碰撞风险概率。

Result: 风险模型的警告准确性为67%，而距离和时间接触测量的准确性仅为51%。

Conclusion: 风险模型在视觉障碍辅助设备中表现优越，为未来技术改进提供了方向。

Abstract: Intelligent devices for supporting persons with vision impairment are
becoming more widespread, but they are lacking behind the advancements in
intelligent driver assistant system. To make a first step forward, this work
discusses the integration of the risk model technology, previously used in
autonomous driving and advanced driver assistance systems, into an assistance
device for persons with vision impairment. The risk model computes a
probabilistic collision risk given object trajectories which has previously
been shown to give better indications of an object's collision potential
compared to distance or time-to-contact measures in vehicle scenarios. In this
work, we show that the risk model is also superior in warning persons with
vision impairment about dangerous objects. Our experiments demonstrate that the
warning accuracy of the risk model is 67% while both distance and
time-to-contact measures reach only 51% accuracy for real-world data.

</details>


### [24] [CapsDT: Diffusion-Transformer for Capsule Robot Manipulation](https://arxiv.org/abs/2506.16263)
*Xiting He,Mingwu Su,Xinqi Jiang,Long Bai,Jiewen Lai,Hongliang Ren*

Main category: cs.RO

TL;DR: CapsDT是一种基于扩散变换器的模型，用于胶囊机器人胃内操作，结合视觉和文本输入生成控制信号，提升内窥镜任务效率。


<details>
  <summary>Details</summary>
Motivation: 探索VLA模型在内窥镜胶囊机器人中的应用，以改善人机交互，提高诊断和治疗效果。

Method: 设计CapsDT模型，处理视觉和文本输入生成控制信号，并开发胶囊机器人系统进行实验验证。

Result: CapsDT在多种内窥镜任务中表现优异，真实模拟操作成功率达26.25%。

Conclusion: CapsDT作为视觉语言通用模型，在内窥镜任务中具有显著潜力。

Abstract: Vision-Language-Action (VLA) models have emerged as a prominent research
area, showcasing significant potential across a variety of applications.
However, their performance in endoscopy robotics, particularly endoscopy
capsule robots that perform actions within the digestive system, remains
unexplored. The integration of VLA models into endoscopy robots allows more
intuitive and efficient interactions between human operators and medical
devices, improving both diagnostic accuracy and treatment outcomes. In this
work, we design CapsDT, a Diffusion Transformer model for capsule robot
manipulation in the stomach. By processing interleaved visual inputs, and
textual instructions, CapsDT can infer corresponding robotic control signals to
facilitate endoscopy tasks. In addition, we developed a capsule endoscopy robot
system, a capsule robot controlled by a robotic arm-held magnet, addressing
different levels of four endoscopy tasks and creating corresponding capsule
robot datasets within the stomach simulator. Comprehensive evaluations on
various robotic tasks indicate that CapsDT can serve as a robust
vision-language generalist, achieving state-of-the-art performance in various
levels of endoscopy tasks while achieving a 26.25% success rate in real-world
simulation manipulation.

</details>


### [25] [M-Predictive Spliner: Enabling Spatiotemporal Multi-Opponent Overtaking for Autonomous Racing](https://arxiv.org/abs/2506.16301)
*Nadine Imholz,Maurice Brunner,Nicolas Baumann,Edoardo Ghignone,Michele Magno*

Main category: cs.RO

TL;DR: 本文提出了一种基于KF的多对手跟踪器和GPR的方法，用于多对手赛车场景，显著提高了超车成功率和安全性。


<details>
  <summary>Details</summary>
Motivation: 多对手赛车场景中，传统方法忽略了时空信息或仅适用于单一对手，本文旨在解决这一问题。

Method: 使用KF多对手跟踪器进行对手重识别，并结合GPR预测对手轨迹，计算超车策略。

Result: 实验验证显示超车成功率达91.65%，安全性提升10.13%。

Conclusion: 该方法在高性能自主赛车中具有潜力。

Abstract: Unrestricted multi-agent racing presents a significant research challenge,
requiring decision-making at the limits of a robot's operational capabilities.
While previous approaches have either ignored spatiotemporal information in the
decision-making process or been restricted to single-opponent scenarios, this
work enables arbitrary multi-opponent head-to-head racing while considering the
opponents' future intent. The proposed method employs a KF-based multi-opponent
tracker to effectively perform opponent ReID by associating them across
observations. Simultaneously, spatial and velocity GPR is performed on all
observed opponent trajectories, providing predictive information to compute the
overtaking maneuvers. This approach has been experimentally validated on a
physical 1:10 scale autonomous racing car, achieving an overtaking success rate
of up to 91.65% and demonstrating an average 10.13%-point improvement in safety
at the same speed as the previous SotA. These results highlight its potential
for high-performance autonomous racing.

</details>


### [26] [Goal-conditioned Hierarchical Reinforcement Learning for Sample-efficient and Safe Autonomous Driving at Intersections](https://arxiv.org/abs/2506.16336)
*Yiou Huang*

Main category: cs.RO

TL;DR: 提出了一种基于目标条件碰撞预测（GCCP）的分层强化学习框架，用于解决自动驾驶任务中的样本效率和安全问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在复杂场景中难以高效且安全地训练策略。

Method: 采用分层强化学习（HRL）框架，包含GCCP模块预测碰撞风险，高层决策选择安全子目标，低层运动规划执行。

Result: 实验表明，该方法比传统RL方法收敛更快且安全性更高。

Conclusion: 提出的HRL框架在自动驾驶任务中表现出更高的样本效率和安全性。

Abstract: Reinforcement learning (RL) exhibits remarkable potential in addressing
autonomous driving tasks. However, it is difficult to train a sample-efficient
and safe policy in complex scenarios. In this article, we propose a novel
hierarchical reinforcement learning (HRL) framework with a goal-conditioned
collision prediction (GCCP) module. In the hierarchical structure, the GCCP
module predicts collision risks according to different potential subgoals of
the ego vehicle. A high-level decision-maker choose the best safe subgoal. A
low-level motion-planner interacts with the environment according to the
subgoal. Compared to traditional RL methods, our algorithm is more
sample-efficient, since its hierarchical structure allows reusing the policies
of subgoals across similar tasks for various navigation scenarios. In
additional, the GCCP module's ability to predict both the ego vehicle's and
surrounding vehicles' future actions according to different subgoals, ensures
the safety of the ego vehicle throughout the decision-making process.
Experimental results demonstrate that the proposed method converges to an
optimal policy faster and achieves higher safety than traditional RL methods.

</details>


### [27] [Comparison between External and Internal Single Stage Planetary gearbox actuators for legged robots](https://arxiv.org/abs/2506.16356)
*Aman Singh,Deepak Kapa,Prasham Chedda,Shishir N. Y. Kolathaya*

Main category: cs.RO

TL;DR: 本文提出了一种设计框架，用于优化选择执行器参数，比较了ISSPG和ESSPG两种行星齿轮箱架构的性能，并通过实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 缺乏对ISSPG和ESSPG两种行星齿轮箱架构的客观比较，且现有设计依赖启发式方法而非系统优化。

Method: 提出了一种基于性能需求和电机规格的设计框架，生成并分析了两种架构的优化齿轮箱设计。

Result: 对于T-motor U12，ISSPG在5:1至7:1的齿轮比范围内更优，而ESSPG在7:1至11:1范围内更优。实验验证了优化模型的准确性。

Conclusion: 设计框架有效，ISSPG和ESSPG各有优势，具体选择取决于齿轮比需求。

Abstract: Legged robots, such as quadrupeds and humanoids, require high-performance
actuators for efficient locomotion. Quasi-Direct-Drive (QDD) actuators with
single-stage planetary gearboxes offer low inertia, high efficiency, and
transparency. Among planetary gearbox architectures, Internal (ISSPG) and
External Single-Stage Planetary Gearbox (ESSPG) are the two predominant
designs. While ISSPG is often preferred for its compactness and high torque
density at certain gear ratios, no objective comparison between the two
architectures exists. Additionally, existing designs rely on heuristics rather
than systematic optimization. This paper presents a design framework for
optimally selecting actuator parameters based on given performance requirements
and motor specifications. Using this framework, we generate and analyze various
optimized gearbox designs for both architectures. Our results demonstrate that
for the T-motor U12, ISSPG is the superior choice within the lower gear ratio
range of 5:1 to 7:1, offering a lighter design. However, for gear ratios
exceeding 7:1, ISSPG becomes infeasible, making ESSPG the better option in the
7:1 to 11:1 range. To validate our approach, we designed and optimized two
actuators for manufacturing: an ISSPG with a 6.0:1 gear ratio and an ESSPG with
a 7.2:1 gear ratio. Their respective masses closely align with our optimization
model predictions, confirming the effectiveness of our methodology.

</details>


### [28] [CSC-MPPI: A Novel Constrained MPPI Framework with DBSCAN for Reliable Obstacle Avoidance](https://arxiv.org/abs/2506.16386)
*Leesai Park,Keunwoo Jang,Sanghyun Kim*

Main category: cs.RO

TL;DR: CSC-MPPI是一种改进的MPPI方法，通过结合原始-对偶梯度方法和DBSCAN聚类，增强轨迹优化并严格满足约束条件。


<details>
  <summary>Details</summary>
Motivation: 传统MPPI在约束满足和轨迹优化方面表现不佳，CSC-MPPI旨在解决这些问题。

Method: 结合原始-对偶梯度方法确保约束满足，并使用DBSCAN聚类选择代表性控制输入。

Result: CSC-MPPI在避障任务中优于传统MPPI，提高了可靠性和效率。

Conclusion: CSC-MPPI通过严格约束和优化轨迹选择，提升了复杂环境下的鲁棒性。

Abstract: This paper proposes Constrained Sampling Cluster Model Predictive Path
Integral (CSC-MPPI), a novel constrained formulation of MPPI designed to
enhance trajectory optimization while enforcing strict constraints on system
states and control inputs. Traditional MPPI, which relies on a probabilistic
sampling process, often struggles with constraint satisfaction and generates
suboptimal trajectories due to the weighted averaging of sampled trajectories.
To address these limitations, the proposed framework integrates a primal-dual
gradient-based approach and Density-Based Spatial Clustering of Applications
with Noise (DBSCAN) to steer sampled input trajectories into feasible regions
while mitigating risks associated with weighted averaging. First, to ensure
that sampled trajectories remain within the feasible region, the primal-dual
gradient method is applied to iteratively shift sampled inputs while enforcing
state and control constraints. Then, DBSCAN groups the sampled trajectories,
enabling the selection of representative control inputs within each cluster.
Finally, among the representative control inputs, the one with the lowest cost
is chosen as the optimal action. As a result, CSC-MPPI guarantees constraint
satisfaction, improves trajectory selection, and enhances robustness in complex
environments. Simulation and real-world experiments demonstrate that CSC-MPPI
outperforms traditional MPPI in obstacle avoidance, achieving improved
reliability and efficiency. The experimental videos are available at
https://cscmppi.github.io

</details>


### [29] [Full-Pose Tracking via Robust Control for Over-Actuated Multirotors](https://arxiv.org/abs/2506.16427)
*Mohamad Hachem,Clément Roos,Thierry Miquel,Murat Bronz*

Main category: cs.RO

TL;DR: 提出了一种针对过驱动多旋翼的级联控制架构，结合INDI和结构化H∞控制，实现精确的姿态与位置跟踪。


<details>
  <summary>Details</summary>
Motivation: 扩展INDI和结构化H∞控制的应用范围，解决过驱动多旋翼的精确控制和鲁棒性问题。

Method: 采用加权最小二乘几何引导控制分配方法，将其表述为二次优化问题，实现全姿态跟踪。

Result: 数值模拟验证了方法的有效性，展示了其适应性和实际应用潜力。

Conclusion: 该方法成功解决了不可行姿态参考和干扰鲁棒性等关键挑战，适用于多样化任务场景。

Abstract: This paper presents a robust cascaded control architecture for over-actuated
multirotors. It extends the Incremental Nonlinear Dynamic Inversion (INDI)
control combined with structured H_inf control, initially proposed for
under-actuated multirotors, to a broader range of multirotor configurations. To
achieve precise and robust attitude and position tracking, we employ a weighted
least-squares geometric guidance control allocation method, formulated as a
quadratic optimization problem, enabling full-pose tracking. The proposed
approach effectively addresses key challenges, such as preventing infeasible
pose references and enhancing robustness against disturbances, as well as
considering multirotor's actual physical limitations. Numerical simulations
with an over-actuated hexacopter validate the method's effectiveness,
demonstrating its adaptability to diverse mission scenarios and its potential
for real-world aerial applications.

</details>


### [30] [Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining](https://arxiv.org/abs/2506.16475)
*Yaru Niu,Yunzhe Zhang,Mingyang Yu,Changyi Lin,Chenhao Li,Yikai Wang,Yuxiang Yang,Wenhao Yu,Tingnan Zhang,Bingqing Chen,Jonathan Francis,Zhenzhen Li,Jie Tan,Ding Zhao*

Main category: cs.RO

TL;DR: 提出了一种跨具身模仿学习系统，用于四足机器人的多功能操作，通过人类和机器人数据联合训练，显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决四足机器人多功能自主操作的挑战，通过跨具身数据提升泛化能力。

Method: 开发了统一的遥操作和数据收集流程，提出模块化架构支持跨具身预训练和联合训练。

Result: 在六项实际任务中，平均成功率提升41.9%，OOD设置下提升79.7%；人类数据预训练贡献显著。

Conclusion: 跨具身模仿学习有效提升四足机器人操作能力，开源代码和数据促进研究发展。

Abstract: Quadrupedal robots have demonstrated impressive locomotion capabilities in
complex environments, but equipping them with autonomous versatile manipulation
skills in a scalable way remains a significant challenge. In this work, we
introduce a cross-embodiment imitation learning system for quadrupedal
manipulation, leveraging data collected from both humans and LocoMan, a
quadruped equipped with multiple manipulation modes. Specifically, we develop a
teleoperation and data collection pipeline, which unifies and modularizes the
observation and action spaces of the human and the robot. To effectively
leverage the collected data, we propose an efficient modularized architecture
that supports co-training and pretraining on structured modality-aligned data
across different embodiments. Additionally, we construct the first manipulation
dataset for the LocoMan robot, covering various household tasks in both
unimanual and bimanual modes, supplemented by a corresponding human dataset. We
validate our system on six real-world manipulation tasks, where it achieves an
average success rate improvement of 41.9% overall and 79.7% under
out-of-distribution (OOD) settings compared to the baseline. Pretraining with
human data contributes a 38.6% success rate improvement overall and 82.7% under
OOD settings, enabling consistently better performance with only half the
amount of robot data. Our code, hardware, and data are open-sourced at:
https://human2bots.github.io.

</details>


### [31] [Grounding Language Models with Semantic Digital Twins for Robotic Planning](https://arxiv.org/abs/2506.16493)
*Mehreen Naeem,Andrew Melnik,Michael Beetz*

Main category: cs.RO

TL;DR: 提出一种结合语义数字孪生（SDT）与大语言模型（LLM）的新框架，用于动态环境中机器人任务的自适应执行。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中机器人任务执行的高层推理与语义环境理解的结合问题。

Method: 将自然语言指令分解为结构化动作三元组，并通过SDT提供的环境数据实现语义接地，支持动作规划和实时适应。

Result: 在ALFRED基准测试中表现稳健，能有效应对不确定性和失败。

Conclusion: 该框架成功结合高层推理与语义环境理解，实现可靠任务完成。

Abstract: We introduce a novel framework that integrates Semantic Digital Twins (SDTs)
with Large Language Models (LLMs) to enable adaptive and goal-driven robotic
task execution in dynamic environments. The system decomposes natural language
instructions into structured action triplets, which are grounded in contextual
environmental data provided by the SDT. This semantic grounding allows the
robot to interpret object affordances and interaction rules, enabling action
planning and real-time adaptability. In case of execution failures, the LLM
utilizes error feedback and SDT insights to generate recovery strategies and
iteratively revise the action plan. We evaluate our approach using tasks from
the ALFRED benchmark, demonstrating robust performance across various household
scenarios. The proposed framework effectively combines high-level reasoning
with semantic environment understanding, achieving reliable task completion in
the face of uncertainty and failure.

</details>


### [32] [eCAV: An Edge-Assisted Evaluation Platform for Connected Autonomous Vehicles](https://arxiv.org/abs/2506.16535)
*Tyler Landle,Jordan Rapp,Dean Blank,Chandramouli Amarnath,Abhijit Chatterjee,Alex Daglis,Umakishore Ramachandran*

Main category: cs.RO

TL;DR: 提出eCAV平台，用于高效、模块化、可扩展的自动驾驶车辆控制算法评估，支持256辆车无感知模拟和64辆车有感知模拟，性能优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆普及，提升道路安全及减少事故损害需求迫切，但现有模拟框架无法有效评估大规模车辆场景。

Method: 开发eCAV平台，支持多车辆控制算法验证，包括V2X技术及未来车-边缘控制平面。

Result: eCAV可模拟256辆无感知车辆（8倍于现有方案）和64辆有感知车辆（4倍于OpenCDA，速度快1.5倍）。

Conclusion: eCAV为自动驾驶算法验证提供了高效、可扩展的解决方案，填补了大规模车辆模拟的空白。

Abstract: As autonomous vehicles edge closer to widespread adoption, enhancing road
safety through collision avoidance and minimization of collateral damage
becomes imperative. Vehicle-to-everything (V2X) technologies, which include
vehicle-to-vehicle (V2V), vehicle-to-infrastructure (V2I), and vehicle-to-cloud
(V2C), are being proposed as mechanisms to achieve this safety improvement.
  Simulation-based testing is crucial for early-stage evaluation of Connected
Autonomous Vehicle (CAV) control systems, offering a safer and more
cost-effective alternative to real-world tests. However, simulating large 3D
environments with many complex single- and multi-vehicle sensors and
controllers is computationally intensive. There is currently no evaluation
framework that can effectively evaluate realistic scenarios involving large
numbers of autonomous vehicles.
  We propose eCAV -- an efficient, modular, and scalable evaluation platform to
facilitate both functional validation of algorithmic approaches to increasing
road safety, as well as performance prediction of algorithms of various V2X
technologies, including a futuristic Vehicle-to-Edge control plane and
correspondingly designed control algorithms. eCAV can model up to 256 vehicles
running individual control algorithms without perception enabled, which is
$8\times$ more vehicles than what is possible with state-of-the-art
alternatives. %faster than state-of-the-art alternatives that can simulate
$8\times$ fewer vehicles. With perception enabled, eCAV simulates up to 64
vehicles with a step time under 800ms, which is $4\times$ more and $1.5\times$
faster than the state-of-the-art OpenCDA framework.

</details>


### [33] [Agile, Autonomous Spacecraft Constellations with Disruption Tolerant Networking to Monitor Precipitation and Urban Floods](https://arxiv.org/abs/2506.16537)
*Sreeja Roy-Singh,Alan P. Li,Vinay Ravindra,Roderick Lammers,Marc Sanchez Net*

Main category: cs.RO

TL;DR: 论文提出了一种基于小型敏捷卫星星座的算法框架，结合轨道力学、姿态控制和星间通信，显著提升对瞬态或动态现象的响应能力。


<details>
  <summary>Details</summary>
Motivation: 利用商业技术支持的完全可定向小型航天器，结合星间通信和智能预测，提高对动态现象（如降水与城市洪水）的观测效率。

Method: 开发了地面和星载算法框架，结合轨道力学、姿态控制、星间通信和智能规划，动态调度卫星星座的观测任务。

Result: 在24颗卫星的星座中，星载调度器观测到的洪水幅度比地面实现多7%，且比非敏捷星座性能提升98%。

Conclusion: 该框架通过智能规划和星间通信，显著提升了卫星星座对动态现象的观测能力和响应速度。

Abstract: Fully re-orientable small spacecraft are now supported by commercial
technologies, allowing them to point their instruments in any direction and
capture images, with short notice. When combined with improved onboard
processing, and implemented on a constellation of inter-communicable
satellites, this intelligent agility can significantly increase responsiveness
to transient or evolving phenomena. We demonstrate a ground-based and onboard
algorithmic framework that combines orbital mechanics, attitude control,
inter-satellite communication, intelligent prediction and planning to schedule
the time-varying, re-orientation of agile, small satellites in a constellation.
Planner intelligence is improved by updating the predictive value of future
space-time observations based on shared observations of evolving episodic
precipitation and urban flood forecasts. Reliable inter-satellite communication
within a fast, dynamic constellation topology is modeled in the physical,
access control and network layer. We apply the framework on a representative
24-satellite constellation observing 5 global regions. Results show
appropriately low latency in information exchange (average within 1/3rd
available time for implicit consensus), enabling the onboard scheduler to
observe ~7% more flood magnitude than a ground-based implementation. Both
onboard and offline versions performed ~98% better than constellations without
agility.

</details>


### [34] [BIDA: A Bi-level Interaction Decision-making Algorithm for Autonomous Vehicles in Dynamic Traffic Scenarios](https://arxiv.org/abs/2506.16546)
*Liyang Yu,Tianyi Wang,Junfeng Jiao,Fengwu Shan,Hongqing Chu,Bingzhao Gao*

Main category: cs.RO

TL;DR: 论文提出了一种双层交互决策算法（BIDA），结合交互式蒙特卡洛树搜索（MCTS）与深度强化学习（DRL），以提升自动驾驶车辆在动态交通场景中的交互理性、效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶车辆在复杂交通环境中因人类行为不可预测性而面临的决策挑战，特别是在多车道高速公路和无信号T型交叉口等动态场景。

Method: 采用三种DRL算法构建可靠的价值网络和策略网络，指导交互式MCTS的在线推理过程，并结合动态轨迹规划器和轨迹跟踪控制器在CARLA中实现。

Result: 实验表明，BIDA不仅提升了交互推理能力并降低了计算成本，还在安全性、效率和交互理性方面优于其他最新基准。

Conclusion: BIDA在动态交通场景中表现出优越性能，为自动驾驶车辆的交互决策提供了有效解决方案。

Abstract: In complex real-world traffic environments, autonomous vehicles (AVs) need to
interact with other traffic participants while making real-time and
safety-critical decisions accordingly. The unpredictability of human behaviors
poses significant challenges, particularly in dynamic scenarios, such as
multi-lane highways and unsignalized T-intersections. To address this gap, we
design a bi-level interaction decision-making algorithm (BIDA) that integrates
interactive Monte Carlo tree search (MCTS) with deep reinforcement learning
(DRL), aiming to enhance interaction rationality, efficiency and safety of AVs
in dynamic key traffic scenarios. Specifically, we adopt three types of DRL
algorithms to construct a reliable value network and policy network, which
guide the online deduction process of interactive MCTS by assisting in value
update and node selection. Then, a dynamic trajectory planner and a trajectory
tracking controller are designed and implemented in CARLA to ensure smooth
execution of planned maneuvers. Experimental evaluations demonstrate that our
BIDA not only enhances interactive deduction and reduces computational costs,
but also outperforms other latest benchmarks, which exhibits superior safety,
efficiency and interaction rationality under varying traffic conditions.

</details>


### [35] [An Optimization-Augmented Control Framework for Single and Coordinated Multi-Arm Robotic Manipulation](https://arxiv.org/abs/2506.16555)
*Melih Özcan,Ozgur S. Oguz*

Main category: cs.RO

TL;DR: 提出了一种结合力控制和优化运动规划的多模态控制框架，用于复杂机器人操作任务。


<details>
  <summary>Details</summary>
Motivation: 力控制和运动规划各有局限性，力控制难以维持稳定方向，运动规划不擅长动态交互。

Method: 将任务分解为子任务，动态分配三种控制模式：纯优化、纯力控制或混合控制。

Result: 在单臂、双臂和多臂操作任务中展示了方法的鲁棒性和精确性。

Conclusion: 该框架能无缝切换控制模式，适用于自由空间运动和接触丰富的操作。

Abstract: Robotic manipulation demands precise control over both contact forces and
motion trajectories. While force control is essential for achieving compliant
interaction and high-frequency adaptation, it is limited to operations in close
proximity to the manipulated object and often fails to maintain stable
orientation during extended motion sequences. Conversely, optimization-based
motion planning excels in generating collision-free trajectories over the
robot's configuration space but struggles with dynamic interactions where
contact forces play a crucial role. To address these limitations, we propose a
multi-modal control framework that combines force control and
optimization-augmented motion planning to tackle complex robotic manipulation
tasks in a sequential manner, enabling seamless switching between control modes
based on task requirements. Our approach decomposes complex tasks into
subtasks, each dynamically assigned to one of three control modes: Pure
optimization for global motion planning, pure force control for precise
interaction, or hybrid control for tasks requiring simultaneous trajectory
tracking and force regulation. This framework is particularly advantageous for
bimanual and multi-arm manipulation, where synchronous motion and coordination
among arms are essential while considering both the manipulated object and
environmental constraints. We demonstrate the versatility of our method through
a range of long-horizon manipulation tasks, including single-arm, bimanual, and
multi-arm applications, highlighting its ability to handle both free-space
motion and contact-rich manipulation with robustness and precision.

</details>


### [36] [Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control](https://arxiv.org/abs/2506.16565)
*Yuxin Chen,Jianglan Wei,Chenfeng Xu,Boyi Li,Masayoshi Tomizuka,Andrea Bajcsy,Ran Tian*

Main category: cs.RO

TL;DR: 论文提出ReOI方法，通过检测并移除视觉干扰物，提升世界模型在开放世界中的预测可靠性，显著提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 世界模型在遇到训练中罕见的视觉干扰物时表现脆弱，导致预测失效，影响机器人规划和动作验证。

Method: 提出ReOI方法：检测干扰物、修改观测、重新预测并恢复干扰物以保持视觉一致性。

Result: ReOI对分布内和分布外干扰物均有效，任务成功率提升高达3倍。

Conclusion: ReOI是一种简单有效的测试时策略，显著提升世界模型在开放世界中的鲁棒性。

Abstract: World models enable robots to "imagine" future observations given current
observations and planned actions, and have been increasingly adopted as
generalized dynamics models to facilitate robot learning. Despite their
promise, these models remain brittle when encountering novel visual distractors
such as objects and background elements rarely seen during training.
Specifically, novel distractors can corrupt action outcome predictions, causing
downstream failures when robots rely on the world model imaginations for
planning or action verification. In this work, we propose Reimagination with
Observation Intervention (ReOI), a simple yet effective test-time strategy that
enables world models to predict more reliable action outcomes in open-world
scenarios where novel and unanticipated visual distractors are inevitable.
Given the current robot observation, ReOI first detects visual distractors by
identifying which elements of the scene degrade in physically implausible ways
during world model prediction. Then, it modifies the current observation to
remove these distractors and bring the observation closer to the training
distribution. Finally, ReOI "reimagines" future outcomes with the modified
observation and reintroduces the distractors post-hoc to preserve visual
consistency for downstream planning and verification. We validate our approach
on a suite of robotic manipulation tasks in the context of action verification,
where the verifier needs to select desired action plans based on predictions
from a world model. Our results show that ReOI is robust to both
in-distribution and out-of-distribution visual distractors. Notably, it
improves task success rates by up to 3x in the presence of novel distractors,
significantly outperforming action verification that relies on world model
predictions without imagination interventions.

</details>


### [37] [DRIVE Through the Unpredictability:From a Protocol Investigating Slip to a Metric Estimating Command Uncertainty](https://arxiv.org/abs/2506.16593)
*Nicolas Samson,William Larrivée-Hardy,William Dubois,Élie Roy-Brouard,Edith Brotherton,Dominic Baril,Julien Lépine,François Pomerleau*

Main category: cs.RO

TL;DR: 论文提出使用DRIVE协议标准化数据收集，用于系统识别和滑移状态空间表征，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 越野自主导航依赖于运动模型的准确性，但运动模型受限于其对地形与无人地面车辆（UGV）交互的预测能力。

Method: 采用DRIVE协议收集数据，验证其在探索速度命令空间和识别可达速度方面的能力，并提出不可预测性度量。

Result: 实验验证了DRIVE协议的有效性，并提出了评估部署风险的不可预测性度量。

Conclusion: DRIVE协议有助于系统识别，并分享了在大型UGV上运行系统识别的经验。

Abstract: Off-road autonomous navigation is a challenging task as it is mainly
dependent on the accuracy of the motion model. Motion model performances are
limited by their ability to predict the interaction between the terrain and the
UGV, which an onboard sensor can not directly measure. In this work, we propose
using the DRIVE protocol to standardize the collection of data for system
identification and characterization of the slip state space. We validated this
protocol by acquiring a dataset with two platforms (from 75 kg to 470 kg) on
six terrains (i.e., asphalt, grass, gravel, ice, mud, sand) for a total of 4.9
hours and 14.7 km. Using this data, we evaluate the DRIVE protocol's ability to
explore the velocity command space and identify the reachable velocities for
terrain-robot interactions. We investigated the transfer function between the
command velocity space and the resulting steady-state slip for an SSMR. An
unpredictability metric is proposed to estimate command uncertainty and help
assess risk likelihood and severity in deployment. Finally, we share our
lessons learned on running system identification on large UGV to help the
community.

</details>


### [38] [History-Augmented Vision-Language Models for Frontier-Based Zero-Shot Object Navigation](https://arxiv.org/abs/2506.16623)
*Mobin Habibpour,Fatemeh Afghah*

Main category: cs.RO

TL;DR: 论文提出了一种零样本目标导航框架，通过动态历史感知提示深度整合视觉语言模型（VLM）推理，提升导航性能。


<details>
  <summary>Details</summary>
Motivation: 当前目标导航方法对VLM的利用较浅，仅用于对象-场景相似性检查，缺乏深度推理，导致上下文理解不足和重复导航行为。

Method: 采用动态历史感知提示，为VLM提供动作历史上下文，生成语义导航评分，并引入VLM辅助的路径点生成机制。

Result: 在HM3D数据集上实现了46%的成功率和24.8%的路径加权成功率（SPL），与零样本方法的最优结果相当。

Conclusion: 历史增强的VLM提示策略显著提升了导航的鲁棒性和上下文感知能力。

Abstract: Object Goal Navigation (ObjectNav) challenges robots to find objects in
unseen environments, demanding sophisticated reasoning. While Vision-Language
Models (VLMs) show potential, current ObjectNav methods often employ them
superficially, primarily using vision-language embeddings for object-scene
similarity checks rather than leveraging deeper reasoning. This limits
contextual understanding and leads to practical issues like repetitive
navigation behaviors. This paper introduces a novel zero-shot ObjectNav
framework that pioneers the use of dynamic, history-aware prompting to more
deeply integrate VLM reasoning into frontier-based exploration. Our core
innovation lies in providing the VLM with action history context, enabling it
to generate semantic guidance scores for navigation actions while actively
avoiding decision loops. We also introduce a VLM-assisted waypoint generation
mechanism for refining the final approach to detected objects. Evaluated on the
HM3D dataset within Habitat, our approach achieves a 46% Success Rate (SR) and
24.8% Success weighted by Path Length (SPL). These results are comparable to
state-of-the-art zero-shot methods, demonstrating the significant potential of
our history-augmented VLM prompting strategy for more robust and context-aware
robotic navigation.

</details>


### [39] [See What I Mean? Expressiveness and Clarity in Robot Display Design](https://arxiv.org/abs/2506.16643)
*Matthew Ebisu,Hang Yu,Reuben Aronson,Elaine Short*

Main category: cs.RO

TL;DR: 研究探讨了非语言视觉符号在人与机器人协作中的作用，发现动画显示能增强信任，而静态图标更易理解。


<details>
  <summary>Details</summary>
Motivation: 探索不同类型非语言提示在动态环境中对任务表现的影响。

Method: 设计了协作导航任务，比较动画与静态显示对用户信任和任务完成的影响。

Result: 动画显示提升信任，静态图标更易理解，静态眼睛提高任务成功率。

Conclusion: 动画增强信任，但静态图标更优，需结合两者优化人机交互。

Abstract: Nonverbal visual symbols and displays play an important role in communication
when humans and robots work collaboratively. However, few studies have
investigated how different types of non-verbal cues affect objective task
performance, especially in a dynamic environment that requires real time
decision-making. In this work, we designed a collaborative navigation task
where the user and the robot only had partial information about the map on each
end and thus the users were forced to communicate with a robot to complete the
task. We conducted our study in a public space and recruited 37 participants
who randomly passed by our setup. Each participant collaborated with a robot
utilizing either animated anthropomorphic eyes and animated icons, or static
anthropomorphic eyes and static icons. We found that participants that
interacted with a robot with animated displays reported the greatest level of
trust and satisfaction; that participants interpreted static icons the best;
and that participants with a robot with static eyes had the highest completion
success. These results suggest that while animation can foster trust with
robots, human-robot communication can be optimized by the addition of familiar
static icons that may be easier for users to interpret. We published our code,
designed symbols, and collected results online at:
https://github.com/mattufts/huamn_Cozmo_interaction.

</details>


### [40] [CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity](https://arxiv.org/abs/2506.16652)
*Guang Yin,Yitong Li,Yixuan Wang,Dale McConachie,Paarth Shah,Kunimatsu Hashimoto,Huan Zhang,Katherine Liu,Yunzhu Li*

Main category: cs.RO

TL;DR: 论文提出了一种新型机器人操作框架，通过生成可解释的中间代码解决自然语言指令的模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 现有语言条件策略因缺乏模块化和可解释性，导致性能不佳，需解决自然语言指令的模糊性。

Method: 使用视觉语言模型（VLM）解析指令并生成任务代码，结合感知模块生成3D注意力图以消除歧义。

Result: 实验表明，该方法在语言模糊性、接触丰富操作和多物体交互任务中表现优异。

Conclusion: 该框架通过可解释的中间表示有效解决了自然语言指令的模糊性问题，提升了机器人操作的性能。

Abstract: Natural language instructions for robotic manipulation tasks often exhibit
ambiguity and vagueness. For instance, the instruction "Hang a mug on the mug
tree" may involve multiple valid actions if there are several mugs and branches
to choose from. Existing language-conditioned policies typically rely on
end-to-end models that jointly handle high-level semantic understanding and
low-level action generation, which can result in suboptimal performance due to
their lack of modularity and interpretability. To address these challenges, we
introduce a novel robotic manipulation framework that can accomplish tasks
specified by potentially ambiguous natural language. This framework employs a
Vision-Language Model (VLM) to interpret abstract concepts in natural language
instructions and generates task-specific code - an interpretable and executable
intermediate representation. The generated code interfaces with the perception
module to produce 3D attention maps that highlight task-relevant regions by
integrating spatial and semantic information, effectively resolving ambiguities
in instructions. Through extensive experiments, we identify key limitations of
current imitation learning methods, such as poor adaptation to language and
environmental variations. We show that our approach excels across challenging
manipulation tasks involving language ambiguity, contact-rich manipulation, and
multi-object interactions.

</details>


### [41] [Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections](https://arxiv.org/abs/2506.16685)
*Xiaomeng Xu,Yifan Hou,Zeyi Liu,Shuran Song*

Main category: cs.RO

TL;DR: CR-DAgger通过合规干预接口和残差策略，显著提升接触密集型任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 解决DAgger在真实世界接触密集型任务中数据收集和策略更新的挑战。

Method: 引入合规干预接口和合规残差策略，结合力反馈和控制。

Result: 在书籍翻页和皮带组装任务中，成功率提升50%以上，优于从头训练和微调方法。

Conclusion: CR-DAgger为真实机器人学习任务提供了有效的DAgger实现指导。

Abstract: We address key challenges in Dataset Aggregation (DAgger) for real-world
contact-rich manipulation: how to collect informative human correction data and
how to effectively update policies with this new data. We introduce Compliant
Residual DAgger (CR-DAgger), which contains two novel components: 1) a
Compliant Intervention Interface that leverages compliance control, allowing
humans to provide gentle, accurate delta action corrections without
interrupting the ongoing robot policy execution; and 2) a Compliant Residual
Policy formulation that learns from human corrections while incorporating force
feedback and force control. Our system significantly enhances performance on
precise contact-rich manipulation tasks using minimal correction data,
improving base policy success rates by over 50\% on two challenging tasks (book
flipping and belt assembly) while outperforming both retraining-from-scratch
and finetuning approaches. Through extensive real-world experiments, we provide
practical guidance for implementing effective DAgger in real-world robot
learning tasks. Result videos are available at:
https://compliant-residual-dagger.github.io/

</details>


### [42] [VLM-Empowered Multi-Mode System for Efficient and Safe Planetary Navigation](https://arxiv.org/abs/2506.16703)
*Sinuo Cheng,Ruyi Zhou,Wenhao Feng,Huaiguang Yang,Haibo Gao,Zongquan Deng,Liang Ding*

Main category: cs.RO

TL;DR: 提出了一种基于视觉语言模型（VLM）的多模式行星漫游车导航系统，通过地形复杂度分类切换导航模式，显著提升效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 行星探索环境复杂多样，需要更灵活适应的导航策略。

Method: 利用VLM解析场景信息，根据地形复杂度分类切换导航模式（感知、建图、规划），并结合全局路径生成模块。

Result: 多模式系统在模拟环境中效率提升79.5%，同时保持对地形危险的规避能力。

Conclusion: 该系统在复杂环境中实现了高效且安全的自主导航。

Abstract: The increasingly complex and diverse planetary exploration environment
requires more adaptable and flexible rover navigation strategy. In this study,
we propose a VLM-empowered multi-mode system to achieve efficient while safe
autonomous navigation for planetary rovers. Vision-Language Model (VLM) is used
to parse scene information by image inputs to achieve a human-level
understanding of terrain complexity. Based on the complexity classification,
the system switches to the most suitable navigation mode, composing of
perception, mapping and planning modules designed for different terrain types,
to traverse the terrain ahead before reaching the next waypoint. By integrating
the local navigation system with a map server and a global waypoint generation
module, the rover is equipped to handle long-distance navigation tasks in
complex scenarios. The navigation system is evaluated in various simulation
environments. Compared to the single-mode conservative navigation method, our
multi-mode system is able to bootstrap the time and energy efficiency in a
long-distance traversal with varied type of obstacles, enhancing efficiency by
79.5%, while maintaining its avoidance capabilities against terrain hazards to
guarantee rover safety. More system information is shown at
https://chengsn1234.github.io/multi-mode-planetary-navigation/.

</details>


### [43] [Experimental Setup and Software Pipeline to Evaluate Optimization based Autonomous Multi-Robot Search Algorithms](https://arxiv.org/abs/2506.16710)
*Aditya Bhatt,Mary Katherine Corra,Franklin Merlo,Prajit KrisshnaKumar,Souma Chowdhury*

Main category: cs.RO

TL;DR: 论文提出了一种新的实验设置和开源软件管道，用于评估多机器人搜索算法，填补了模拟与物理测试之间的差距。


<details>
  <summary>Details</summary>
Motivation: 多机器人信号源定位在搜索救援和危险定位中有广泛应用，但现有算法多限于模拟测试，缺乏物理环境下的性能评估。

Method: 设计了基于声源和小型地面机器人的实验设置，开发了开源软件管道，支持分布式多机器人搜索算法的并行异步执行。

Result: 实验设置成功评估了两种先进的多机器人搜索算法（群体优化和贝叶斯优化）以及随机行走基线。

Conclusion: 该实验设置和软件管道为多机器人搜索算法的物理测试提供了实用工具，有助于缩小模拟与现实的差距。

Abstract: Signal source localization has been a problem of interest in the multi-robot
systems domain given its applications in search \& rescue and hazard
localization in various industrial and outdoor settings. A variety of
multi-robot search algorithms exist that usually formulate and solve the
associated autonomous motion planning problem as a heuristic model-free or
belief model-based optimization process. Most of these algorithms however
remains tested only in simulation, thereby losing the opportunity to generate
knowledge about how such algorithms would compare/contrast in a real physical
setting in terms of search performance and real-time computing performance. To
address this gap, this paper presents a new lab-scale physical setup and
associated open-source software pipeline to evaluate and benchmark multi-robot
search algorithms. The presented physical setup innovatively uses an acoustic
source (that is safe and inexpensive) and small ground robots (e-pucks)
operating in a standard motion-capture environment. This setup can be easily
recreated and used by most robotics researchers. The acoustic source also
presents interesting uncertainty in terms of its noise-to-signal ratio, which
is useful to assess sim-to-real gaps. The overall software pipeline is designed
to readily interface with any multi-robot search algorithm with minimal effort
and is executable in parallel asynchronous form. This pipeline includes a
framework for distributed implementation of multi-robot or swarm search
algorithms, integrated with a ROS (Robotics Operating System)-based software
stack for motion capture supported localization. The utility of this novel
setup is demonstrated by using it to evaluate two state-of-the-art multi-robot
search algorithms, based on swarm optimization and batch-Bayesian Optimization
(called Bayes-Swarm), as well as a random walk baseline.

</details>


### [44] [DRARL: Disengagement-Reason-Augmented Reinforcement Learning for Efficient Improvement of Autonomous Driving Policy](https://arxiv.org/abs/2506.16720)
*Weitao Zhou,Bo Zhang,Zhong Cao,Xiang Li,Qian Cheng,Chunyang Liu,Yaqin Zhang,Diange Yang*

Main category: cs.RO

TL;DR: 论文提出了一种基于脱钩原因增强的强化学习方法（DRARL），通过识别脱钩原因优化自动驾驶策略，避免无效数据干扰。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在开放道路上的脱钩案例增多，但数据稀缺且部分脱钩并非策略失败导致，需有效利用脱钩数据提升策略。

Method: 使用OOD状态估计模型识别脱钩原因，区分无效脱钩案例，并在原因增强的想象环境中更新策略。

Result: 实验证明，该方法能准确识别策略相关脱钩原因，提升策略性能，避免过度保守调整。

Conclusion: DRARL提供了一种高效利用脱钩数据改进自动驾驶策略的方法。

Abstract: With the increasing presence of automated vehicles on open roads under driver
supervision, disengagement cases are becoming more prevalent. While some
data-driven planning systems attempt to directly utilize these disengagement
cases for policy improvement, the inherent scarcity of disengagement data
(often occurring as a single instances) restricts training effectiveness.
Furthermore, some disengagement data should be excluded since the disengagement
may not always come from the failure of driving policies, e.g. the driver may
casually intervene for a while. To this end, this work proposes
disengagement-reason-augmented reinforcement learning (DRARL), which enhances
driving policy improvement process according to the reason of disengagement
cases. Specifically, the reason of disengagement is identified by a
out-of-distribution (OOD) state estimation model. When the reason doesn't
exist, the case will be identified as a casual disengagement case, which
doesn't require additional policy adjustment. Otherwise, the policy can be
updated under a reason-augmented imagination environment, improving the policy
performance of disengagement cases with similar reasons. The method is
evaluated using real-world disengagement cases collected by autonomous driving
robotaxi. Experimental results demonstrate that the method accurately
identifies policy-related disengagement reasons, allowing the agent to handle
both original and semantically similar cases through reason-augmented training.
Furthermore, the approach prevents the agent from becoming overly conservative
after policy adjustments. Overall, this work provides an efficient way to
improve driving policy performance with disengagement cases.

</details>


### [45] [A Scalable Post-Processing Pipeline for Large-Scale Free-Space Multi-Agent Path Planning with PiBT](https://arxiv.org/abs/2506.16748)
*Arjo Chakravarty,Michael X. Grey,M. A. Viraj J. Muthugala,Mohan Rajesh Elara*

Main category: cs.RO

TL;DR: 提出了一种结合优先级继承回溯（PiBT）和安全感知路径平滑的混合规划框架，适用于大规模自由空间多智能体路径规划。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么无法扩展到几十个智能体以上，要么依赖于网格世界假设，难以推广到连续空间。

Method: 扩展PiBT到8连通网格，结合安全感知路径平滑和局部交互感知，使用SIPP进行碰撞解决。

Result: 方法可扩展到500个智能体，运行时优于现有方法，路径接近最优。

Conclusion: 该框架是机器人系统中可扩展、实时多智能体导航的有前景的构建模块。

Abstract: Free-space multi-agent path planning remains challenging at large scales.
Most existing methods either offer optimality guarantees but do not scale
beyond a few dozen agents, or rely on grid-world assumptions that do not
generalize well to continuous space. In this work, we propose a hybrid,
rule-based planning framework that combines Priority Inheritance with
Backtracking (PiBT) with a novel safety-aware path smoothing method. Our
approach extends PiBT to 8-connected grids and selectively applies
string-pulling based smoothing while preserving collision safety through local
interaction awareness and a fallback collision resolution step based on Safe
Interval Path Planning (SIPP). This design allows us to reduce overall path
lengths while maintaining real-time performance. We demonstrate that our method
can scale to over 500 agents in large free-space environments, outperforming
existing any-angle and optimal methods in terms of runtime, while producing
near-optimal trajectories in sparse domains. Our results suggest this framework
is a promising building block for scalable, real-time multi-agent navigation in
robotics systems operating beyond grid constraints.

</details>


### [46] [Learning Dexterous Object Handover](https://arxiv.org/abs/2506.16822)
*Daniel Frau-Alfaro,Julio Castaño-Amoros,Santiago Puente,Pablo Gil,Roberto Calandra*

Main category: cs.RO

TL;DR: 该论文提出了一种基于强化学习的多指手间物体交接方法，使用双四元数奖励函数优化旋转距离，实验证明其高效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在协作环境中（如家庭），机器人需要安全高效地完成物体交接任务，这是日常人机交互的重要技能。

Method: 采用强化学习（RL）方法，设计了一种基于双四元数的奖励函数，用于最小化旋转距离，优于欧拉角和旋转矩阵等表示方法。

Result: 实验表明，训练策略在未见过物体和交接扰动情况下表现良好，最佳场景成功率达94%，扰动下性能仅下降13.8%。

Conclusion: 该方法在物体交接任务中表现出高效性和鲁棒性，适用于实际应用场景。

Abstract: Object handover is an important skill that we use daily when interacting with
other humans. To deploy robots in collaborative setting, like houses, being
able to receive and handing over objects safely and efficiently becomes a
crucial skill. In this work, we demonstrate the use of Reinforcement Learning
(RL) for dexterous object handover between two multi-finger hands. Key to this
task is the use of a novel reward function based on dual quaternions to
minimize the rotation distance, which outperforms other rotation
representations such as Euler and rotation matrices. The robustness of the
trained policy is experimentally evaluated by testing w.r.t. objects that are
not included in the training distribution, and perturbations during the
handover process. The results demonstrate that the trained policy successfully
perform this task, achieving a total success rate of 94% in the best-case
scenario after 100 experiments, thereby showing the robustness of our policy
with novel objects. In addition, the best-case performance of the policy
decreases by only 13.8% when the other robot moves during the handover, proving
that our policy is also robust to this type of perturbation, which is common in
real-world object handovers.

</details>


### [47] [Orbital Collision: An Indigenously Developed Web-based Space Situational Awareness Platform](https://arxiv.org/abs/2506.16892)
*Partha Chowdhury,Harsha M,Ayush Gupta,Sanat K Biswas*

Main category: cs.RO

TL;DR: Orbital Collision (OrCo) 是一个基于网络的平台，用于通过 TLE 数据预测空间物体的碰撞概率，以增强空间态势感知。


<details>
  <summary>Details</summary>
Motivation: 地球轨道环境日益拥挤，主要由空间碎片和失效卫星引起，增加了碰撞风险。

Method: 使用多种方法传播轨道不确定性并计算碰撞概率。

Result: 通过准确性和效率评估验证了平台的性能。

Conclusion: 该平台有助于改善空间物体跟踪，确保卫星在拥挤空间中的安全。

Abstract: This work presents an indigenous web based platform Orbital Collision (OrCo),
created by the Space Systems Laboratory at IIIT Delhi, to enhance Space
Situational Awareness (SSA) by predicting collision probabilities of space
objects using Two Line Elements (TLE) data. The work highlights the growing
challenges of congestion in the Earth's orbital environment, mainly due to
space debris and defunct satellites, which increase collision risks. It employs
several methods for propagating orbital uncertainty and calculating the
collision probability. The performance of the platform is evaluated through
accuracy assessments and efficiency metrics, in order to improve the tracking
of space objects and ensure the safety of the satellite in congested space.

</details>


### [48] [SDDiff: Boost Radar Perception via Spatial-Doppler Diffusion](https://arxiv.org/abs/2506.16936)
*Shengpeng Wang,Xin Luo,Yulong Xie,Wei Wang*

Main category: cs.RO

TL;DR: 论文提出了一种名为SDDiff的模型，首次将点云提取（PCE）和自车速度估计（EVE）任务结合，利用空间-多普勒扩散方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常独立处理PCE和EVE，忽略了雷达空间与多普勒域特征的相互作用，可能引入偏差。论文发现两者存在潜在关联，可相互促进。

Method: 设计了SDDiff模型，改进传统潜在扩散过程：1）引入包含空间占用和多普勒特征的表示；2）基于雷达先验设计定向扩散；3）提出迭代多普勒细化以增强适应性。

Result: SDDiff显著优于现有基线，EVE准确率提升59%，有效生成密度提高4倍，同时增强PCE的有效性和可靠性。

Conclusion: SDDiff通过结合PCE和EVE任务，利用空间-多普勒扩散方法，显著提升了雷达感知性能。

Abstract: Point cloud extraction (PCE) and ego velocity estimation (EVE) are key
capabilities gaining attention in 3D radar perception. However, existing work
typically treats these two tasks independently, which may neglect the interplay
between radar's spatial and Doppler domain features, potentially introducing
additional bias. In this paper, we observe an underlying correlation between 3D
points and ego velocity, which offers reciprocal benefits for PCE and EVE. To
fully unlock such inspiring potential, we take the first step to design a
Spatial-Doppler Diffusion (SDDiff) model for simultaneously dense PCE and
accurate EVE. To seamlessly tailor it to radar perception, SDDiff improves the
conventional latent diffusion process in three major aspects. First, we
introduce a representation that embodies both spatial occupancy and Doppler
features. Second, we design a directional diffusion with radar priors to
streamline the sampling. Third, we propose Iterative Doppler Refinement to
enhance the model's adaptability to density variations and ghosting effects.
Extensive evaluations show that SDDiff significantly outperforms
state-of-the-art baselines by achieving 59% higher in EVE accuracy, 4X greater
in valid generation density while boosting PCE effectiveness and reliability.

</details>


### [49] [Learning Accurate Whole-body Throwing with High-frequency Residual Policy and Pullback Tube Acceleration](https://arxiv.org/abs/2506.16986)
*Yuntao Ma,Yang Liu,Kaixian Qu,Marco Hutter*

Main category: cs.RO

TL;DR: 本文提出了一种结合学习和模型控制的框架，用于实现腿式移动机械臂的抓握投掷，展示了较高的投掷精度和成功率。


<details>
  <summary>Details</summary>
Motivation: 投掷是机器人扩展操作范围的基本技能，但现有技术难以实现高精度动态全身操作。

Method: 框架包括末端执行器的名义跟踪策略、高频残差策略和基于优化的加速度控制模块。

Result: 投掷6米远目标时平均着陆误差0.28米；与大学生对比中，系统成功率56.8%，人类仅15.2%。

Conclusion: 该框架在硬件上实现了量化精度的抓握投掷，推动了动态全身操作的发展。

Abstract: Throwing is a fundamental skill that enables robots to manipulate objects in
ways that extend beyond the reach of their arms. We present a control framework
that combines learning and model-based control for prehensile whole-body
throwing with legged mobile manipulators. Our framework consists of three
components: a nominal tracking policy for the end-effector, a high-frequency
residual policy to enhance tracking accuracy, and an optimization-based module
to improve end-effector acceleration control. The proposed controller achieved
the average of 0.28 m landing error when throwing at targets located 6 m away.
Furthermore, in a comparative study with university students, the system
achieved a velocity tracking error of 0.398 m/s and a success rate of 56.8%,
hitting small targets randomly placed at distances of 3-5 m while throwing at a
specified speed of 6 m/s. In contrast, humans have a success rate of only
15.2%. This work provides an early demonstration of prehensile throwing with
quantified accuracy on hardware, contributing to progress in dynamic whole-body
manipulation.

</details>


### [50] [Monocular One-Shot Metric-Depth Alignment for RGB-Based Robot Grasping](https://arxiv.org/abs/2506.17110)
*Teng Guo,Baichuan Huang,Jingjin Yu*

Main category: cs.RO

TL;DR: 提出了一种名为MOMA的新框架，通过单张RGB图像恢复度量深度，适用于6D物体姿态估计，尤其在透明物体处理上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前6D姿态估计依赖昂贵的深度传感器，且对透明物体效果不佳；现有单目深度估计模型（MDEMs）无法泛化。

Method: MOMA通过相机校准中的尺度-旋转-平移对齐，利用稀疏真实深度点指导，无需额外数据收集或模型重训练。

Result: 在桌面抓取和吸盘式分拣任务中，MOMA表现出高成功率，验证了其有效性。

Conclusion: MOMA为6D姿态估计提供了一种高效、泛化能力强的解决方案，尤其适用于透明物体。

Abstract: Accurate 6D object pose estimation is a prerequisite for successfully
completing robotic prehensile and non-prehensile manipulation tasks. At
present, 6D pose estimation for robotic manipulation generally relies on depth
sensors based on, e.g., structured light, time-of-flight, and stereo-vision,
which can be expensive, produce noisy output (as compared with RGB cameras),
and fail to handle transparent objects. On the other hand, state-of-the-art
monocular depth estimation models (MDEMs) provide only affine-invariant depths
up to an unknown scale and shift. Metric MDEMs achieve some successful
zero-shot results on public datasets, but fail to generalize. We propose a
novel framework, Monocular One-shot Metric-depth Alignment (MOMA), to recover
metric depth from a single RGB image, through a one-shot adaptation building on
MDEM techniques. MOMA performs scale-rotation-shift alignments during camera
calibration, guided by sparse ground-truth depth points, enabling accurate
depth estimation without additional data collection or model retraining on the
testing setup. MOMA supports fine-tuning the MDEM on transparent objects,
demonstrating strong generalization capabilities. Real-world experiments on
tabletop 2-finger grasping and suction-based bin-picking applications show MOMA
achieves high success rates in diverse tasks, confirming its effectiveness.

</details>


### [51] [Judo: A User-Friendly Open-Source Package for Sampling-Based Model Predictive Control](https://arxiv.org/abs/2506.17184)
*Albert H. Li,Brandon Hung,Aaron D. Ames,Jiuguang Wang,Simon Le Cleac'h,Preston Culbertson*

Main category: cs.RO

TL;DR: Judo是一个用于快速原型设计和评估采样基于MPC的软件包，提供标准化算法、任务和交互式GUI，支持实时性能。


<details>
  <summary>Details</summary>
Motivation: 机器人社区需要通用工具来支持采样基于MPC的原型设计、评估和部署。

Method: Judo提供常见采样基于MPC算法的实现、标准化任务、简单可扩展的接口、异步执行和交互式GUI。

Result: Judo在消费级和服务器级硬件上实现实时性能。

Conclusion: Judo为采样基于MPC的研究和应用提供了高效、易用的工具。

Abstract: Recent advancements in parallel simulation and successful robotic
applications are spurring a resurgence in sampling-based model predictive
control. To build on this progress, however, the robotics community needs
common tooling for prototyping, evaluating, and deploying sampling-based
controllers. We introduce Judo, a software package designed to address this
need. To facilitate rapid prototyping and evaluation, Judo provides robust
implementations of common sampling-based MPC algorithms and standardized
benchmark tasks. It further emphasizes usability with simple but extensible
interfaces for controller and task definitions, asynchronous execution for
straightforward simulation-to-hardware transfer, and a highly customizable
interactive GUI for tuning controllers interactively. While written in Python,
the software leverages MuJoCo as its physics backend to achieve real-time
performance, which we validate across both consumer and server-grade hardware.
Code at https://github.com/bdaiinstitute/judo.

</details>


### [52] [Dex1B: Learning with 1B Demonstrations for Dexterous Manipulation](https://arxiv.org/abs/2506.17198)
*Jianglong Ye,Keyi Wang,Chengjing Yuan,Ruihan Yang,Yiquan Li,Jiyue Zhu,Yuzhe Qin,Xueyan Zou,Xiaolong Wang*

Main category: cs.RO

TL;DR: 论文介绍了Dex1B数据集，通过生成模型创建了大规模、多样且高质量的手部操作演示数据，包含10亿个演示，并在仿真和实际机器人实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模手部操作演示数据生成的挑战，生成多样且物理可行的演示。

Method: 提出一种生成模型，结合几何约束提升可行性，并通过附加条件增强多样性。

Result: 在仿真和实际实验中显著优于现有方法，验证了模型的有效性和鲁棒性。

Conclusion: Dex1B数据集为手部操作研究提供了高质量资源，生成模型方法具有广泛潜力。

Abstract: Generating large-scale demonstrations for dexterous hand manipulation remains
challenging, and several approaches have been proposed in recent years to
address this. Among them, generative models have emerged as a promising
paradigm, enabling the efficient creation of diverse and physically plausible
demonstrations. In this paper, we introduce Dex1B, a large-scale, diverse, and
high-quality demonstration dataset produced with generative models. The dataset
contains one billion demonstrations for two fundamental tasks: grasping and
articulation. To construct it, we propose a generative model that integrates
geometric constraints to improve feasibility and applies additional conditions
to enhance diversity. We validate the model on both established and newly
introduced simulation benchmarks, where it significantly outperforms prior
state-of-the-art methods. Furthermore, we demonstrate its effectiveness and
robustness through real-world robot experiments. Our project page is at
https://jianglongye.com/dex1b

</details>
