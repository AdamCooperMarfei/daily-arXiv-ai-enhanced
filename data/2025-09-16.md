<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 54]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Large Foundation Models for Trajectory Prediction in Autonomous Driving: A Comprehensive Survey](https://arxiv.org/abs/2509.10570)
*Wei Dai,Shengen Wu,Wei Wu,Zhenhao Wang,Sisuo Lyu,Haicheng Liao,Limin Yu,Weiping Ding,Runwei Guan,Yutao Yue*

Main category: cs.RO

TL;DR: 这篇综述系统回顾了大型基础模型（LFMs）在轨迹预测领域的最新进展，特别是大型语言模型（LLMs）和多模态大语言模型（MLLMs）的应用，分析了核心方法、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在轨迹预测中存在可解释性差、依赖大规模标注数据、长尾场景泛化能力弱等局限性，而大型基础模型通过整合语言和场景语义，能够实现可解释的上下文推理，提升预测安全性和泛化能力。

Method: 提出了三种核心方法：轨迹-语言映射、多模态融合和基于约束的推理，涵盖了车辆和行人的预测任务、评估指标和数据集分析。

Result: LFMs通过语言和场景语义的整合，显著提升了复杂环境下的预测安全性和泛化能力，为轨迹预测提供了新的研究范式。

Conclusion: 尽管LFMs在轨迹预测中展现出巨大潜力，但仍面临计算延迟、数据稀缺和现实世界鲁棒性等挑战，未来研究方向包括低延迟推理、因果感知建模和运动基础模型等。

Abstract: Trajectory prediction serves as a critical functionality in autonomous
driving, enabling the anticipation of future motion paths for traffic
participants such as vehicles and pedestrians, which is essential for driving
safety. Although conventional deep learning methods have improved accuracy,
they remain hindered by inherent limitations, including lack of
interpretability, heavy reliance on large-scale annotated data, and weak
generalization in long-tail scenarios. The rise of Large Foundation Models
(LFMs) is transforming the research paradigm of trajectory prediction. This
survey offers a systematic review of recent advances in LFMs, particularly
Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for
trajectory prediction. By integrating linguistic and scene semantics, LFMs
facilitate interpretable contextual reasoning, significantly enhancing
prediction safety and generalization in complex environments. The article
highlights three core methodologies: trajectory-language mapping, multimodal
fusion, and constraint-based reasoning. It covers prediction tasks for both
vehicles and pedestrians, evaluation metrics, and dataset analyses. Key
challenges such as computational latency, data scarcity, and real-world
robustness are discussed, along with future research directions including
low-latency inference, causality-aware modeling, and motion foundation models.

</details>


### [2] [STL-Based Motion Planning and Uncertainty-Aware Risk Analysis for Human-Robot Collaboration with a Multi-Rotor Aerial Vehicle](https://arxiv.org/abs/2509.10692)
*Giuseppe Silano,Amr Afifi,Martin Saska,Antonio Franchi*

Main category: cs.RO

TL;DR: 基于信号时序逻辑的多旋翼飞行器人机协作运动规划与风险分析方法，通过优化框架生成动态可行轨迹，结合不确定性风险分析和事件触发重规划策略，实现安全高效的人机协作。


<details>
  <summary>Details</summary>
Motivation: 提升人机协作中多旋翼飞行器的安全性和效率，特别是在电力线路维护等高风险场景中，需要同时考虑安全性、时间约束和人体工程学舒适度。

Method: 使用信号时序逻辑(STL)编码任务目标，构建优化框架生成动态可行轨迹，采用平滑近似和梯度技术处理非线性非凸问题，集成不确定性风险分析和事件触发重规划策略。

Result: MATLAB和Gazebo仿真验证表明，该方法在模拟电力线路维护场景的对象交接任务中能够实现安全、高效和鲁棒的人机协作。

Conclusion: 提出的方法有效解决了多旋翼飞行器人机协作中的运动规划和风险分析问题，为高风险环境下的安全人机交互提供了可行解决方案。

Abstract: This paper presents a novel approach to motion planning and risk analysis for
enhancing human-robot collaboration using a Multi-Rotor Aerial Vehicle (MRAV).
The proposed method uses Signal Temporal Logic (STL) to encode key mission
objectives, such as safety, timing, and human preferences, with a strong focus
on ergonomics and comfort. An optimization framework generates dynamically
feasible trajectories while considering the MRAV's physical constraints. Given
the nonlinear and non-convex nature of the problem, smooth approximations and
gradient-based techniques assist in handling the problem's computational
complexity. Additionally, an uncertainty-aware risk analysis is incorporated to
assess potential deviations from the mission specifications, providing insights
into the likelihood of mission success under uncertain conditions. Further, an
event-triggered replanning strategy is implemented to respond to unforeseen
events and external disturbances. The approach is validated through MATLAB and
Gazebo simulations, using an object handover task in a mock-up environment
inspired by power line maintenance scenarios. The results highlight the
method's effectiveness in achieving safe, efficient, and resilient human-robot
collaboration.

</details>


### [3] [A Survey on LiDAR-based Autonomous Aerial Vehicles](https://arxiv.org/abs/2509.10730)
*Yunfan Ren,Yixi Cai,Haotian Li,Nan Chen,Fangcheng Zhu,Longji Yin,Fanze Kong,Rundong Li,Fu Zhang*

Main category: cs.RO

TL;DR: 激光雷达无人机技术综述：涵盖设计、感知、规划与控制策略，重点分析激光雷达在GPS拒止环境中的导航优势及应用前景


<details>
  <summary>Details</summary>
Motivation: 激光雷达技术在过去十年中成为高速、敏捷、可靠无人机导航的关键使能技术，特别是在GPS拒止环境中。本文旨在全面综述激光雷达无人机的最新进展，为研究人员和从业者提供有价值的资源

Method: 采用文献综述方法，系统分析激光雷达传感器的演进、软件组件（状态估计、建图、轨迹规划与控制方法）以及实际应用案例

Result: 激光雷达与无人机的集成显著增强了自主性，使其能够在多样化和挑战性环境中执行复杂任务，包括工业操作、多平台支持和无人机群部署

Conclusion: 激光雷达无人机系统在感知精度和环境适应性方面具有显著优势，但仍面临挑战，需要进一步研究以推动技术发展和多无人机协作

Abstract: This survey offers a comprehensive overview of recent advancements in
LiDAR-based autonomous Unmanned Aerial Vehicles (UAVs), covering their design,
perception, planning, and control strategies. Over the past decade, LiDAR
technology has become a crucial enabler for high-speed, agile, and reliable UAV
navigation, especially in GPS-denied environments. The paper begins by
examining the evolution of LiDAR sensors, emphasizing their unique advantages
such as high accuracy, long-range depth measurements, and robust performance
under various lighting conditions, making them particularly well-suited for UAV
applications. The integration of LiDAR with UAVs has significantly enhanced
their autonomy, enabling complex missions in diverse and challenging
environments. Subsequently, we explore essential software components, including
perception technologies for state estimation and mapping, as well as trajectory
planning and control methodologies, and discuss their adoption in LiDAR-based
UAVs. Additionally, we analyze various practical applications of the
LiDAR-based UAVs, ranging from industrial operations to supporting different
aerial platforms and UAV swarm deployments. The survey concludes by discussing
existing challenges and proposing future research directions to advance
LiDAR-based UAVs and enhance multi-UAV collaboration. By synthesizing recent
developments, this paper aims to provide a valuable resource for researchers
and practitioners working to push the boundaries of LiDAR-based UAV systems.

</details>


### [4] [Analytical Design and Development of a Modular and Intuitive Framework for Robotizing and Enhancing the Existing Endoscopic Procedures](https://arxiv.org/abs/2509.10735)
*Mohammad Rafiee Javazm,Yash Kulkarni,Jiaqi Xue,Naruhiko Ikoma,Farshid Alambeigi*

Main category: cs.RO

TL;DR: 这篇论文提出了一种专为内镜设备设计的模块化机电框架，包括新型捆紧机制、进给机制和直观用户界面，以减轻医生手动操作内镜的负担。


<details>
  <summary>Details</summary>
Motivation: 手动操作内镜设备对临床医生具有挑战性，导致工作负荷重、疲劳和分心，需要解决这些问题提高诊疗效果。

Method: 设计和开发包含：(1)新型嵌套头头捆紧机制控制弯曲度；(2)进给机制控制插入/撤回；(3)直观用户界面同时控制所有度的自由度。使用数学建模进行设计分析和参数优化。

Result: 通过模拟和实验研究验证了所提出的数学模型和机器人框架的性能表现。

Conclusion: 该研究提供了一种可以简化内镜操作、减轻医生负担的有效解决方案，为内镜设备自动化控制领域做出了贡献。

Abstract: Despite the widespread adoption of endoscopic devices for several cancer
screening procedures, manual control of these devices still remains challenging
for clinicians, leading to several critical issues such as increased workload,
fatigue, and distractions. To address these issues, in this paper, we introduce
the design and development of an intuitive, modular, and easily installable
mechatronic framework. This framework includes (i) a novel nested collet-chuck
gripping mechanism that can readily be integrated and assembled with the
existing endoscopic devices and control their bending degrees-of-freedom
(DoFs); (ii) a feeder mechanism that can control the insertion/retraction DoF
of a colonoscope, and (iii) a complementary and intuitive user interface that
enables simultaneous control of all DoFs during the procedure. To analyze the
design of the proposed mechanisms, we also introduce a mathematical modeling
approach and a design space for optimal selection of the parameters involved in
the design of gripping and feeder mechanisms. Our simulation and experimental
studies thoroughly demonstrate the performance of the proposed mathematical
modeling and robotic framework.

</details>


### [5] [FastTrack: GPU-Accelerated Tracking for Visual SLAM](https://arxiv.org/abs/2509.10757)
*Kimia Khabiri,Parsa Hosseininejad,Shishir Gopinath,Karthik Dantu,Steven Y. Ko*

Main category: cs.RO

TL;DR: 提出了一种利用GPU加速视觉惯性SLAM系统中跟踪模块的新方法，通过在ORB-SLAM3中实现CUDA加速，将立体特征匹配和局部地图跟踪性能提升最高2.8倍


<details>
  <summary>Details</summary>
Motivation: 视觉惯性SLAM系统的跟踪模块需要及时处理图像帧和IMU数据来估计位置，如果处理不及时会导致定位质量下降或跟踪丢失，因此需要加速耗时组件

Method: 利用GPU计算能力加速跟踪中的耗时组件，包括立体特征匹配和局部地图跟踪，在ORB-SLAM3跟踪流程中使用CUDA实现

Result: 在桌面和Jetson Xavier NX板上，使用EuRoC和TUM-VI数据集进行立体惯性模式测试，跟踪性能整体提升最高达2.8倍

Conclusion: GPU加速方法能显著提升视觉惯性SLAM系统的跟踪性能，确保实时处理能力，避免定位质量问题和跟踪丢失

Abstract: The tracking module of a visual-inertial SLAM system processes incoming image
frames and IMU data to estimate the position of the frame in relation to the
map. It is important for the tracking to complete in a timely manner for each
frame to avoid poor localization or tracking loss. We therefore present a new
approach which leverages GPU computing power to accelerate time-consuming
components of tracking in order to improve its performance. These components
include stereo feature matching and local map tracking. We implement our design
inside the ORB-SLAM3 tracking process using CUDA. Our evaluation demonstrates
an overall improvement in tracking performance of up to 2.8x on a desktop and
Jetson Xavier NX board in stereo-inertial mode, using the well-known SLAM
datasets EuRoC and TUM-VI.

</details>


### [6] [RSL-RL: A Learning Library for Robotics Research](https://arxiv.org/abs/2509.10771)
*Clemens Schwarke,Mayank Mittal,Nikita Rudin,David Hoeller,Marco Hutter*

Main category: cs.RO

TL;DR: RSL-RL是一个专为机器人社区设计的开源强化学习库，具有紧凑、易修改的代码架构，专注于机器人领域常用算法，支持GPU训练并在仿真和真实机器人实验中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 针对机器人社区对强化学习框架的特殊需求，现有通用框架过于庞大且难以修改，需要开发一个轻量级、可扩展且专注于机器人应用的强化学习库。

Method: 采用紧凑且易于修改的代码设计，专注于机器人领域广泛采用的算法和辅助技术，优化GPU训练以实现大规模仿真环境的高吞吐量性能。

Result: 在仿真基准测试和真实机器人实验中验证了有效性，证明了其作为轻量级、可扩展和实用框架的价值。

Conclusion: RSL-RL成功为机器人社区提供了一个专门优化的强化学习框架，具有轻量、易扩展和实用的特点，开源地址为https://github.com/leggedrobotics/rsl_rl。

Abstract: RSL-RL is an open-source Reinforcement Learning library tailored to the
specific needs of the robotics community. Unlike broad general-purpose
frameworks, its design philosophy prioritizes a compact and easily modifiable
codebase, allowing researchers to adapt and extend algorithms with minimal
overhead. The library focuses on algorithms most widely adopted in robotics,
together with auxiliary techniques that address robotics-specific challenges.
Optimized for GPU-only training, RSL-RL achieves high-throughput performance in
large-scale simulation environments. Its effectiveness has been validated in
both simulation benchmarks and in real-world robotic experiments, demonstrating
its utility as a lightweight, extensible, and practical framework to develop
learning-based robotic controllers. The library is open-sourced at:
https://github.com/leggedrobotics/rsl_rl.

</details>


### [7] [Follow-Bench: A Unified Motion Planning Benchmark for Socially-Aware Robot Person Following](https://arxiv.org/abs/2509.10796)
*Hanjing Ye,Weixi Situ,Jianwei Peng,Yu Zhan,Bingyi Xia,Kuanqi Cai,Hong Zhang*

Main category: cs.RO

TL;DR: 这是首个机器人跟随系统的结构化研究，包括基准测试平台Follow-Bench、六种规划器重现评测，以及安全性与舒适性的系统分析。


<details>
  <summary>Details</summary>
Motivation: 机器人跟随在个人助理、安全巡检、养老护理等领域有应用潜力，需要系统考虑跟随目标的同时确保安全性和舒适性。

Method: 构建Follow-Bench统一基准测试平台，模拟多样化场景，重新实现六种普遍RPF规划器，并在差分驱动机器上进行实际部署评测。

Result: 通过大量模拟和实际实验，提供了现有规划器在安全性与舒适性之间的交换关系的定量分析结果。

Conclusion: 研究揭示了现有技术的挑战和未来研究方向，为机器人跟随系统的安全舒适性优化提供了重要参考。

Abstract: Robot person following (RPF) -- mobile robots that follow and assist a
specific person -- has emerging applications in personal assistance, security
patrols, eldercare, and logistics. To be effective, such robots must follow the
target while ensuring safety and comfort for both the target and surrounding
people. In this work, we present the first end-to-end study of RPF, which (i)
surveys representative scenarios, motion-planning methods, and evaluation
metrics with a focus on safety and comfort; (ii) introduces Follow-Bench, a
unified benchmark simulating diverse scenarios, including various target
trajectory patterns, dynamic-crowd flows, and environmental layouts; and (iii)
re-implements six popular RPF planners, ensuring that both safety and comfort
are systematically considered. Moreover, we evaluate the two highest-performing
planners from our benchmark on a differential-drive robot to provide insights
into real-world deployment. Extensive simulation and real-world experiments
provide quantitative insights into the safety-comfort trade-offs of existing
planners, while revealing open challenges and future research directions.

</details>


### [8] [A Universal Wire Testing Machine for Enhancing the Performance of Wire-Driven Robots](https://arxiv.org/abs/2509.10862)
*Temma Suzuki,Kento Kawaharazuka,Kei Okada*

Main category: cs.RO

TL;DR: 开发了一种鸟用于测量和调整线缆特性的测试机，通过实验测量了线缆的张力传递效率和动态行为，并应用于实际线驱动机器人的力控制，降低了端执行器力错误。


<details>
  <summary>Details</summary>
Motivation: 虽然线缆作为轻量化、低摩擦传动机制有其优势，但因为线缆的灵活性质导致建模错误较大，在工业和研究机器人中的应用仍然有限。

Method: 构建了一种全能型线缆测试机，进行了三个实验：1）移除线缆的初始伸展 2）测量8种不同直径装置漆轮的张力传递效率 3）测量变长度线缆的动态行为

Result: 通过测试机获得了线缆的具体特性数据，并将这些数据应用于实际线驱动机器人的力控制系统中

Conclusion: 该测试机能够有效提升线驱动机制的性能，通过精确测量和调整线缆特性，显著降低了端执行器的力控制错误，为线驱动机器人的应用提供了重要技术支撑。

Abstract: Compared with gears and linkages, wires constitute a lightweight,
low-friction transmission mechanism. However, because wires are flexible
materials, they tend to introduce large modeling errors, and their adoption in
industrial and research robots remains limited.In this study, we built a
Universal Wire Testing Machine that enables measurement and adjustment of wire
characteristics to improve the performance of wire-driven mechanisms. Using
this testing machine, we carried out removal of initial wire stretch,
measurement of tension transmission efficiency for eight different diameters of
passive pulleys, and measurement of the dynamic behavior of variable-length
wires. Finally, we applied the data obtained from this testing machine to the
force control of an actual wire-driven robot, reducing the end-effector force
error.

</details>


### [9] [Nav-R1: Reasoning and Navigation in Embodied Scenes](https://arxiv.org/abs/2509.10884)
*Qingxiang Liu,Ting Huang,Zeyu Zhang,Hao Tang*

Main category: cs.RO

TL;DR: Nav-R1是一个统一的具身推理基础模型，通过构建大规模CoT数据集和GRPO强化学习框架，解决了现有方法推理不稳定和实时控制困难的问题，在多个基准测试中平均提升8%以上。


<details>
  <summary>Details</summary>
Motivation: 现有具身导航方法存在推理轨迹不连贯不稳定、难以平衡长时程语义推理与低延迟实时控制的问题，阻碍了在多样化环境中的泛化能力。

Method: 1) 构建Nav-CoT-110K大规模逐步推理数据集进行冷启动初始化；2) 设计GRPO强化学习框架，包含格式、理解和导航三个互补奖励；3) 提出Fast-in-Slow推理范式，将深思熟虑的语义推理与低延迟反应控制解耦。

Result: 在具身AI基准测试中持续超越强基线，推理和导航性能平均提升超过8%。在移动机器人上的真实世界部署验证了其在有限机载资源下的鲁棒性。

Conclusion: Nav-R1通过统一推理框架和创新的训练范式，有效解决了具身导航中的推理稳定性和实时控制挑战，为具身智能提供了可靠的解决方案。

Abstract: Embodied navigation requires agents to integrate perception, reasoning, and
action for robust interaction in complex 3D environments. Existing approaches
often suffer from incoherent and unstable reasoning traces that hinder
generalization across diverse environments, and difficulty balancing
long-horizon semantic reasoning with low-latency control for real-time
navigation. To address these challenges, we propose Nav-R1, an embodied
foundation model that unifies reasoning in embodied environments. We first
construct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought
(CoT) for embodied tasks, which enables cold-start initialization with
structured reasoning. Building on this foundation, we design a GRPO-based
reinforcement learning framework with three complementary rewards: format,
understanding, and navigation, to improve structural adherence, semantic
grounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow
reasoning paradigm, decoupling deliberate semantic reasoning from low-latency
reactive control for efficient yet coherent navigation. Extensive evaluations
on embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms
strong baselines, with over 8% average improvement in reasoning and navigation
performance. Real-world deployment on a mobile robot further validates its
robustness under limited onboard resources. Code:
https://github.com/AIGeeksGroup/Nav-R1. Website:
https://aigeeksgroup.github.io/Nav-R1.

</details>


### [10] [Design of scalable orthogonal digital encoding architecture for large-area flexible tactile sensing in robotics](https://arxiv.org/abs/2509.10888)
*Weijie Liu,Ziyi Qiu,Shihang Wang,Deqing Mei,Yancheng Wang*

Main category: cs.RO

TL;DR: 提出基于CDMA正交数字编码的新型触觉传感架构，通过并行信号传输大幅减少布线需求，实现单线传输和毫秒级延迟，为大规模软体触觉感知系统提供解决方案


<details>
  <summary>Details</summary>
Motivation: 解决现有柔性触觉传感器在编码效率和布线复杂度方面的瓶颈，实现类似人类皮肤的大面积、高灵敏度、快速响应的触觉感知能力

Method: 采用码分多址(CDMA)启发的正交数字编码策略，通过分布式传感节点的能量正交基码并行叠加，替代传统的串行信号传输

Result: 在16节点传感阵列上验证，仅用单根传输线实现12.8ms时间分辨率的压力分布重建，在数千节点规模下仍能保持低于20ms的延迟

Conclusion: 该架构通过重新定义软电子信号编码范式，为开发具有类人感知能力的可扩展具身智能系统开辟了新前沿

Abstract: Human-like embodied tactile perception is crucial for the next-generation
intelligent robotics. Achieving large-area, full-body soft coverage with high
sensitivity and rapid response, akin to human skin, remains a formidable
challenge due to critical bottlenecks in encoding efficiency and wiring
complexity in existing flexible tactile sensors, thus significantly hinder the
scalability and real-time performance required for human skin-level tactile
perception. Herein, we present a new architecture employing code division
multiple access-inspired orthogonal digital encoding to overcome these
challenges. Our decentralized encoding strategy transforms conventional serial
signal transmission by enabling parallel superposition of energy-orthogonal
base codes from distributed sensing nodes, drastically reducing wiring
requirements and increasing data throughput. We implemented and validated this
strategy with off-the-shelf 16-node sensing array to reconstruct the pressure
distribution, achieving a temporal resolution of 12.8 ms using only a single
transmission wire. Crucially, the architecture can maintain sub-20ms latency
across orders-of-magnitude variations in node number (to thousands of nodes).
By fundamentally redefining signal encoding paradigms in soft electronics, this
work opens new frontiers in developing scalable embodied intelligent systems
with human-like sensory capabilities.

</details>


### [11] [ViSTR-GP: Online Cyberattack Detection via Vision-to-State Tensor Regression and Gaussian Processes in Automated Robotic Operations](https://arxiv.org/abs/2509.10948)
*Navid Aftabi,Philip Samaha,Jin Ma,Long Cheng,Ramy Harik,Dan Li*

Main category: cs.RO

TL;DR: 基于V形感知和循环回归的在线检测框架ViSTR-GP，通过超负相机监控机器人运动，可及早检测数据完整性攻击，特别是细微攻击场景


<details>
  <summary>Details</summary>
Motivation: 工业机器人系统面临日益严重的网络安全风险，数据完整性攻击难以通过传统入侵检测方法发现，需要新的检测方法

Method: 开发ViSTR-GP框架，利用超负相机获取视觉估计，通过SAM-Track生成每帧遮罩、低秩张量回归映射到测量值，使用矩阵复合高斯过程建模正常残差

Result: 在真实机器人平台上验证，框架能准确恢复关节角度，比所有基线方法更早检测到数据完整性攻击，在细微攻击中收益最明显

Conclusion: 经验证，通过添加独立物理通道可以检测数据完整性攻击，突破控制器权限，无需复杂仪器

Abstract: Industrial robotic systems are central to automating smart manufacturing
operations. Connected and automated factories face growing cybersecurity risks
that can potentially cause interruptions and damages to physical operations.
Among these attacks, data-integrity attacks often involve sophisticated
exploitation of vulnerabilities that enable an attacker to access and
manipulate the operational data and are hence difficult to detect with only
existing intrusion detection or model-based detection. This paper addresses the
challenges in utilizing existing side-channels to detect data-integrity attacks
in robotic manufacturing processes by developing an online detection framework,
ViSTR-GP, that cross-checks encoder-reported measurements against a
vision-based estimate from an overhead camera outside the controller's
authority. In this framework, a one-time interactive segmentation initializes
SAM-Track to generate per-frame masks. A low-rank tensor-regression surrogate
maps each mask to measurements, while a matrix-variate Gaussian process models
nominal residuals, capturing temporal structure and cross-joint correlations. A
frame-wise test statistic derived from the predictive distribution provides an
online detector with interpretable thresholds. We validate the framework on a
real-world robotic testbed with synchronized video frame and encoder data,
collecting multiple nominal cycles and constructing replay attack scenarios
with graded end-effector deviations. Results on the testbed indicate that the
proposed framework recovers joint angles accurately and detects data-integrity
attacks earlier with more frequent alarms than all baselines. These
improvements are most evident in the most subtle attacks. These results show
that plants can detect data-integrity attacks by adding an independent physical
channel, bypassing the controller's authority, without needing complex
instrumentation.

</details>


### [12] [ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation](https://arxiv.org/abs/2509.10952)
*Yangcen Liu,Woo Chul Shin,Yunhai Han,Zhenyang Chen,Harish Ravichandar,Danfei Xu*

Main category: cs.RO

TL;DR: ImMimic是一个跨域模仿学习框架，利用人类视频和少量机器人示教数据，通过动态时间规整和混合插值来弥合人机领域差距，提升机器人操作性能


<details>
  <summary>Details</summary>
Motivation: 从丰富的人类视频中学习机器人操作可以替代昂贵的机器人数据收集，但视觉、形态和物理方面的领域差距阻碍了直接模仿

Method: 使用动态时间规整(DTW)将重定向的人类手部姿态映射到机器人关节，然后通过MixUp插值在配对的人机轨迹之间创建中间域，实现平滑的领域自适应

Result: 在四个真实世界操作任务和四种机器人平台上评估，ImMimic显著提高了任务成功率和执行流畅度

Conclusion: 该方法有效弥合了领域差距，为鲁棒的机器人操作提供了可行的解决方案

Abstract: Learning robot manipulation from abundant human videos offers a scalable
alternative to costly robot-specific data collection. However, domain gaps
across visual, morphological, and physical aspects hinder direct imitation. To
effectively bridge the domain gap, we propose ImMimic, an embodiment-agnostic
co-training framework that leverages both human videos and a small amount of
teleoperated robot demonstrations. ImMimic uses Dynamic Time Warping (DTW) with
either action- or visual-based mapping to map retargeted human hand poses to
robot joints, followed by MixUp interpolation between paired human and robot
trajectories. Our key insights are (1) retargeted human hand trajectories
provide informative action labels, and (2) interpolation over the mapped data
creates intermediate domains that facilitate smooth domain adaptation during
co-training. Evaluations on four real-world manipulation tasks (Pick and Place,
Push, Hammer, Flip) across four robotic embodiments (Robotiq, Fin Ray, Allegro,
Ability) show that ImMimic improves task success rates and execution
smoothness, highlighting its efficacy to bridge the domain gap for robust robot
manipulation. The project website can be found at
https://sites.google.com/view/immimic.

</details>


### [13] [Pogosim -- a Simulator for Pogobot robots](https://arxiv.org/abs/2509.10968)
*Leo Cazenille,Loona Macabre,Nicolas Bredeche*

Main category: cs.RO

TL;DR: Pogosim是一个专为Pogobots机器人设计的快速可扩展模拟器，旨在降低算法开发成本，实现代码在模拟和真实机器人上的无缝迁移。


<details>
  <summary>Details</summary>
Motivation: Pogobots机器人虽然成本低廉且模块化设计，但直接在机器人上测试分布式算法非常耗时，复杂问题或参数校准会消耗过多资源。

Method: 开发了Pogosim模拟器，具有相同的软件架构，支持并行运行大量模拟，提供配置文件和用户程序编写指南，并支持使用优化算法进行参数优化。

Result: 实现了模拟与实验环境的代码一致性，能够高效进行大规模仿真和参数优化，显著降低了算法开发成本。

Conclusion: Pogosim为Pogobots机器人研究提供了高效的仿真平台，解决了实际机器人测试的资源瓶颈问题，促进了群体智能算法的发展。

Abstract: Pogobots are a new type of open-source/open-hardware robots specifically
designed for swarm robotics research. Their cost-effective and modular design,
complemented by vibration-based and wheel-based locomotion, fast infrared
communication and extensive software architecture facilitate the implementation
of swarm intelligence algorithms. However, testing even simple distributed
algorithms directly on robots is particularly labor-intensive. Scaling to more
complex problems or calibrate user code parameters will have a prohibitively
high strain on available resources. In this article we present Pogosim, a fast
and scalable simulator for Pogobots, designed to reduce as much as possible
algorithm development costs. The exact same code will be used in both
simulation and to experimentally drive real robots. This article details the
software architecture of Pogosim, explain how to write configuration files and
user programs and how simulations approximate or differ from experiments. We
describe how a large set of simulations can be launched in parallel, how to
retrieve and analyze the simulation results, and how to optimize user code
parameters using optimization algorithms.

</details>


### [14] [Autonomous Close-Proximity Photovoltaic Panel Coating Using a Quadcopter](https://arxiv.org/abs/2509.10979)
*Dimitri Jacquemont,Carlo Bosio,Teaya Yang,Ruiqi Zhang,Ozgur Orun,Shuai Li,Reza Alam,Thomas M. Schutzius,Simo A. Makiharju,Mark W. Mueller*

Main category: cs.RO

TL;DR: 提出基于四旋翼无人机的光伏板保护涂层自动喷涂系统，使用机载传感器定位和模型控制器，通过室内外实验验证自主性能


<details>
  <summary>Details</summary>
Motivation: 光伏板防反射和自清洁涂层需要定期重新喷涂，传统人工方法成本高。无人机提供灵活自主的解决方案，可更频繁、低成本地实施涂层维护

Method: 使用配备液体分散机制的四旋翼无人机系统，仅依赖机载传感器进行视觉-惯性里程计定位和光伏板相对位置检测，采用考虑地面效应和质量变化的模型控制器

Result: 通过广泛的室内和室外实验验证了系统的自主能力

Conclusion: 无人机系统为光伏板保护涂层的自动化应用提供了一种有效的解决方案，相比传统方法更具灵活性和成本效益

Abstract: Photovoltaic (PV) panels are becoming increasingly widespread in the domain
of renewable energy, and thus, small efficiency gains can have massive effects.
Anti-reflective and self-cleaning coatings enhance panel performance but
degrade over time, requiring periodic reapplication. Uncrewed Aerial Vehicles
(UAVs) offer a flexible and autonomous way to apply protective coatings more
often and at lower cost compared to traditional manual coating methods. In this
letter, we propose a quadcopter-based system, equipped with a liquid dispersion
mechanism, designed to automate such tasks. The localization stack only uses
onboard sensors, relying on visual-inertial odometry and the relative position
of the PV panel detected with respect to the quadcopter. The control relies on
a model-based controller that accounts for the ground effect and the mass
decrease of the quadcopter during liquid dispersion. We validate the autonomy
capabilities of our system through extensive indoor and outdoor experiments.

</details>


### [15] [Multi-objective task allocation for electric harvesting robots: a hierarchical route reconstruction approach](https://arxiv.org/abs/2509.11025)
*Peng Chen,Jing Liang,Hui Song,Kang-Jia Qiao,Cai-Tong Yue,Kun-Jie Yu,Ponnuthurai Nagaratnam Suganthan,Witold Pedrycz*

Main category: cs.RO

TL;DR: 本文提出了一个针对农业多电机器人任务分配问题(AMERTA)的混合分层路径重构算法(HRRA)，解决了多机器人协调中的完工时间和能耗多目标优化问题。


<details>
  <summary>Details</summary>
Motivation: 农业劳动力成本上升推动了多机器人采摘系统的应用，但现有研究往往忽略了负载依赖的速度变化和电池限制等实际约束，导致协调效率低下。

Method: 提出了HRRA算法，包含分层编码结构、双阶段初始化方法、任务序列优化器和专门的路径重构操作器等创新机制。

Result: 在45个测试实例上的实验表明，HRRA算法优于7种最先进算法，统计检验证实了其竞争力和探索新解空间的独特能力。

Conclusion: 本研究通过新颖的问题表述和有效算法，为多机器人协调理论理解做出贡献，并为农业自动化提供实用见解。

Abstract: The increasing labor costs in agriculture have accelerated the adoption of
multi-robot systems for orchard harvesting. However, efficiently coordinating
these systems is challenging due to the complex interplay between makespan and
energy consumption, particularly under practical constraints like
load-dependent speed variations and battery limitations. This paper defines the
multi-objective agricultural multi-electrical-robot task allocation (AMERTA)
problem, which systematically incorporates these often-overlooked real-world
constraints. To address this problem, we propose a hybrid hierarchical route
reconstruction algorithm (HRRA) that integrates several innovative mechanisms,
including a hierarchical encoding structure, a dual-phase initialization
method, task sequence optimizers, and specialized route reconstruction
operators. Extensive experiments on 45 test instances demonstrate HRRA's
superior performance against seven state-of-the-art algorithms. Statistical
analysis, including the Wilcoxon signed-rank and Friedman tests, empirically
validates HRRA's competitiveness and its unique ability to explore previously
inaccessible regions of the solution space. In general, this research
contributes to the theoretical understanding of multi-robot coordination by
offering a novel problem formulation and an effective algorithm, thereby also
providing practical insights for agricultural automation.

</details>


### [16] [FEWT: Improving Humanoid Robot Perception with Frequency-Enhanced Wavelet-based Transformers](https://arxiv.org/abs/2509.11109)
*Jiaxin Huang,Hanyu Liu,Yunsheng Ma,Jian Shen,Yilin Zheng,Jiayi Wen,Baishu Wan,Pan Li,Zhigong Song*

Main category: cs.RO

TL;DR: 提出了基于频率增强小波变换的模仿学习框架FEWT，通过结合多尺度小波分解和残差网络，动态融合时域和频域特征，显著提升了人形机器人的感知表示和操作成功率。


<details>
  <summary>Details</summary>
Motivation: 人形机器人作为具身智能的典型物理体现，在机器人学习算法方面展现出巨大潜力。需要开发硬件平台和算法框架来实现直观的远程操作和高效的人形动作数据收集，提升机器人的感知能力。

Method: 开发了包括人形机器人和外骨骼式遥操作舱的硬件平台。提出了FEWT模仿学习框架，包含两个主要模块：频率增强高效多尺度注意力(FE-EMA)和时间序列离散小波变换(TS-DWT)，通过多尺度小波分解与残差网络结合，动态融合时域和频域特征。

Result: 实验结果表明，FEWT在仿真中将最先进算法(ACT基线)的成功率提升了高达30%，在真实世界中提升了6-12%。

Conclusion: FEWT框架通过有效捕获多尺度特征信息，显著增强了模型的鲁棒性，为人形机器人的感知表示和操作性能提供了重要改进。

Abstract: The embodied intelligence bridges the physical world and information space.
As its typical physical embodiment, humanoid robots have shown great promise
through robot learning algorithms in recent years. In this study, a hardware
platform, including humanoid robot and exoskeleton-style teleoperation cabin,
was developed to realize intuitive remote manipulation and efficient collection
of anthropomorphic action data. To improve the perception representation of
humanoid robot, an imitation learning framework, termed Frequency-Enhanced
Wavelet-based Transformer (FEWT), was proposed, which consists of two primary
modules: Frequency-Enhanced Efficient Multi-Scale Attention (FE-EMA) and
Time-Series Discrete Wavelet Transform (TS-DWT). By combining multi-scale
wavelet decomposition with the residual network, FE-EMA can dynamically fuse
features from both time-domain and frequency-domain. This fusion is able to
capture feature information across various scales effectively, thereby
enhancing model robustness. Experimental performance demonstrates that FEWT
improves the success rate of the state-of-the-art algorithm (Action Chunking
with Transformers, ACT baseline) by up to 30% in simulation and by 6-12% in
real-world.

</details>


### [17] [ManiVID-3D: Generalizable View-Invariant Reinforcement Learning for Robotic Manipulation via Disentangled 3D Representations](https://arxiv.org/abs/2509.11125)
*Zheng Li,Pei Qu,Yufei Jia,Shihui Zhou,Haizhou Ge,Jiahang Cao,Jinni Zhou,Guyue Zhou,Jun Ma*

Main category: cs.RO

TL;DR: ManiVID-3D是一个新颖的3D强化学习架构，通过自监督解耦特征学习实现视角不变表示，解决了视觉强化学习在真实世界部署中因相机视角变化导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中机器人操作部署时，相机视角变化不可避免，传统基于固定前视相机的策略在相机位置变化时会失效，现有方法依赖精确相机标定或难以处理大视角变化。

Method: 提出ViewNet轻量级模块，自动将任意视角的点云观测对齐到统一空间坐标系，无需外部标定；开发高效GPU加速批量渲染模块，每秒处理5000+帧；通过自监督解耦特征学习实现视角不变表示。

Result: 在10个模拟和5个真实世界任务中评估，相比最先进方法在视角变化下成功率提高44.7%，参数量减少80%；对严重视角变化具有鲁棒性，表现出强大的仿真到真实性能。

Conclusion: 学习几何一致表示对于在非结构化环境中实现可扩展机器人操作非常有效，ManiVID-3D为解决视角变化问题提供了有效解决方案。

Abstract: Deploying visual reinforcement learning (RL) policies in real-world
manipulation is often hindered by camera viewpoint changes. A policy trained
from a fixed front-facing camera may fail when the camera is shifted--an
unavoidable situation in real-world settings where sensor placement is hard to
manage appropriately. Existing methods often rely on precise camera calibration
or struggle with large perspective changes. To address these limitations, we
propose ManiVID-3D, a novel 3D RL architecture designed for robotic
manipulation, which learns view-invariant representations through
self-supervised disentangled feature learning. The framework incorporates
ViewNet, a lightweight yet effective module that automatically aligns point
cloud observations from arbitrary viewpoints into a unified spatial coordinate
system without the need for extrinsic calibration. Additionally, we develop an
efficient GPU-accelerated batch rendering module capable of processing over
5000 frames per second, enabling large-scale training for 3D visual RL at
unprecedented speeds. Extensive evaluation across 10 simulated and 5 real-world
tasks demonstrates that our approach achieves a 44.7% higher success rate than
state-of-the-art methods under viewpoint variations while using 80% fewer
parameters. The system's robustness to severe perspective changes and strong
sim-to-real performance highlight the effectiveness of learning geometrically
consistent representations for scalable robotic manipulation in unstructured
environments. Our project website can be found in
https://zheng-joe-lee.github.io/manivid3d/.

</details>


### [18] [RoVerFly: Robust and Versatile Learning-based Control of Quadrotor Across Payload Configurations](https://arxiv.org/abs/2509.11149)
*Mintae Kim,Jiaze Cai,Koushil Sreenath*

Main category: cs.RO

TL;DR: RoVerFly是一个基于强化学习的统一控制框架，使用RL策略作为鲁棒的跟踪控制器，适用于标准四旋翼和缆绳悬挂载荷系统，具有零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 四旋翼精确轨迹跟踪控制具有挑战性，特别是带有柔性缆绳悬挂载荷时。传统基于模型的方法需要大量调参且无法适应配置变化（如载荷增减、质量或缆长变化）。

Method: 使用强化学习策略作为跟踪控制器，通过任务和领域随机化进行训练，使控制器对干扰和动态变化具有鲁棒性。

Result: 控制器实现了跨载荷设置的强零样本泛化能力，包括无载荷、不同质量和缆长情况，无需控制器切换或重新调参，同时保持了反馈跟踪控制器的可解释性和结构。

Conclusion: RoVerFly提供了一个统一的学习型控制框架，能够处理四旋翼和缆绳悬挂载荷系统的各种配置变化，具有良好的泛化性能和鲁棒性。

Abstract: Designing robust controllers for precise, arbitrary trajectory tracking with
quadrotors is challenging due to nonlinear dynamics and underactuation, and
becomes harder with flexible cable-suspended payloads that introduce extra
degrees of freedom and hybridness. Classical model-based methods offer
stability guarantees but require extensive tuning and often do not adapt when
the configuration changes, such as when a payload is added or removed, or when
the payload mass or cable length varies. We present RoVerFly, a unified
learning-based control framework in which a reinforcement learning (RL) policy
serves as a robust and versatile tracking controller for standard quadrotors
and for cable-suspended payload systems across a range of configurations.
Trained with task and domain randomization, the controller is resilient to
disturbances and varying dynamics. It achieves strong zero-shot generalization
across payload settings, including no payload as well as varying mass and cable
length, without controller switching or re-tuning, while retaining the
interpretability and structure of a feedback tracking controller. Code and
supplementary materials are available at
https://github.com/mintaeshkim/roverfly

</details>


### [19] [SAMP: Spatial Anchor-based Motion Policy for Collision-Aware Robotic Manipulators](https://arxiv.org/abs/2509.11185)
*Kai Chen,Zhihai Bi,Guoyang Zhao,Chunxin Zheng,Yulin Li,Hang Zhao,Jun Ma*

Main category: cs.RO

TL;DR: SAMP是一个基于空间锚点的运动规划框架，通过共享空间网格上的符号距离场同时编码环境和机械臂几何形状，实现了更精确的碰撞检测和运动规划


<details>
  <summary>Details</summary>
Motivation: 现有神经运动规划方法往往依赖简化的机器人模型或主要关注障碍物表示，导致在复杂场景中碰撞检测不完整和性能下降

Method: 提出空间锚点运动策略(SAMP)框架，使用共享空间网格上的符号距离场同时编码环境和机械臂精确几何形状，通过专用机器人SDF网络捕获机械臂精确几何信息，并采用高效特征对齐策略训练神经运动策略

Result: 在模拟和真实环境实验中，SAMP相比现有方法成功率提高11%，碰撞率降低7%

Conclusion: 联合建模机器人和环境几何形状的方法具有显著优势，在挑战性真实环境中展现出实用价值

Abstract: Neural-based motion planning methods have achieved remarkable progress for
robotic manipulators, yet a fundamental challenge lies in simultaneously
accounting for both the robot's physical shape and the surrounding environment
when generating safe and feasible motions. Moreover, existing approaches often
rely on simplified robot models or focus primarily on obstacle representation,
which can lead to incomplete collision detection and degraded performance in
cluttered scenes. To address these limitations, we propose spatial anchor-based
motion policy (SAMP), a unified framework that simultaneously encodes the
environment and the manipulator using signed distance field (SDF) anchored on a
shared spatial grid. SAMP incorporates a dedicated robot SDF network that
captures the manipulator's precise geometry, enabling collision-aware reasoning
beyond coarse link approximations. These representations are fused on spatial
anchors and used to train a neural motion policy that generates smooth,
collision-free trajectories in the proposed efficient feature alignment
strategy. Experiments conducted in both simulated and real-world environments
consistently show that SAMP outperforms existing methods, delivering an 11%
increase in success rate and a 7% reduction in collision rate. These results
highlight the benefits of jointly modelling robot and environment geometry,
demonstrating its practical value in challenging real-world environments.

</details>


### [20] [DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2509.11197)
*Yunheng Wang,Yuetong Fang,Taowen Wang,Yixiao Feng,Yawen Tan,Shuning Zhang,Peiran Liu,Yiding Ji,Renjing Xu*

Main category: cs.RO

TL;DR: DreamNav是一个零样本视觉语言导航方法，通过视角校正、轨迹级规划和主动想象来解决现有方法的高成本、动作语义不对齐和短视规划问题，在VLN-CE任务上实现了新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本VLN方法依赖昂贵的感知和被动场景理解，将控制简化为点级选择，导致部署成本高、动作语义不对齐和规划短视。需要一种更高效、语义对齐且具有前瞻性的导航方法。

Method: 提出DreamNav方法，包含三个核心组件：(1)EgoView Corrector对齐视角并稳定自我中心感知；(2)Trajectory Predictor进行全局轨迹级规划；(3)Imagination Predictor赋予智能体主动思考能力，实现预期性和长时程规划。

Result: 在VLN-CE和真实世界测试中，DreamNav实现了新的零样本SOTA，在SR和SPL指标上分别比最强的自我中心基线高出7.49%和18.15%。

Conclusion: DreamNav是首个统一轨迹级规划和主动想象的零样本VLN方法，仅使用自我中心输入，为解决真实世界导航任务提供了有效解决方案。

Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE), which
links language instructions to perception and control in the real world, is a
core capability of embodied robots. Recently, large-scale pretrained foundation
models have been leveraged as shared priors for perception, reasoning, and
action, enabling zero-shot VLN without task-specific training. However,
existing zero-shot VLN methods depend on costly perception and passive scene
understanding, collapsing control to point-level choices. As a result, they are
expensive to deploy, misaligned in action semantics, and short-sighted in
planning. To address these issues, we present DreamNav that focuses on the
following three aspects: (1) for reducing sensory cost, our EgoView Corrector
aligns viewpoints and stabilizes egocentric perception; (2) instead of
point-level actions, our Trajectory Predictor favors global trajectory-level
planning to better align with instruction semantics; and (3) to enable
anticipatory and long-horizon planning, we propose an Imagination Predictor to
endow the agent with proactive thinking capability. On VLN-CE and real-world
tests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming the
strongest egocentric baseline with extra information by up to 7.49\% and
18.15\% in terms of SR and SPL metrics. To our knowledge, this is the first
zero-shot VLN method to unify trajectory-level planning and active imagination
while using only egocentric inputs.

</details>


### [21] [MEMBOT: Memory-Based Robot in Intermittent POMDP](https://arxiv.org/abs/2509.11225)
*Youzhi Liang,Eyan Noronha*

Main category: cs.RO

TL;DR: MEMBOT是一个模块化记忆架构，通过解耦信念推断和策略学习来解决机器人控制中的间歇性部分可观测性问题，在观测丢失率高达50%时仍能保持80%的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的机器人系统经常在部分和间歇性可观测条件下运行，传感器输入可能因噪声、遮挡或故障而不可用，传统基于全状态可观测假设的强化学习方法无法有效应对这些挑战。

Method: 采用两阶段训练过程：离线多任务学习预训练阶段使用重构损失学习鲁棒的任务无关潜在信念编码器，然后通过行为克隆微调任务特定策略。信念编码器使用状态空间模型(SSM)和LSTM整合观测和动作的时间序列来推断潜在状态表示。

Result: 在MetaWorld和Robomimic的10个机器人操作基准任务上测试，MEMBOT在各种观测丢失率下始终优于无记忆和简单循环基线方法，在50%观测可用性下仍能保持80%的峰值性能。

Conclusion: 显式信念建模对于实现现实世界部分可观测机器人系统的鲁棒、可迁移和数据高效策略具有显著效果。

Abstract: Robotic systems deployed in real-world environments often operate under
conditions of partial and often intermittent observability, where sensor inputs
may be noisy, occluded, or entirely unavailable due to failures or
environmental constraints. Traditional reinforcement learning (RL) approaches
that assume full state observability are ill-equipped for such challenges. In
this work, we introduce MEMBOT, a modular memory-based architecture designed to
address intermittent partial observability in robotic control tasks. MEMBOT
decouples belief inference from policy learning through a two-phase training
process: an offline multi-task learning pretraining stage that learns a robust
task-agnostic latent belief encoder using a reconstruction losses, followed by
fine-tuning of task-specific policies using behavior cloning. The belief
encoder, implemented as a state-space model (SSM) and a LSTM, integrates
temporal sequences of observations and actions to infer latent state
representations that persist even when observations are dropped. We train and
evaluate MEMBOT on 10 robotic manipulation benchmark tasks from MetaWorld and
Robomimic under varying rates of observation dropout. Results show that MEMBOT
consistently outperforms both memoryless and naively recurrent baselines,
maintaining up to 80% of peak performance under 50% observation availability.
These findings highlight the effectiveness of explicit belief modeling in
achieving robust, transferable, and data-efficient policies for real-world
partially observable robotic systems.

</details>


### [22] [CORB-Planner: Corridor as Observations for RL Planning in High-Speed Flight](https://arxiv.org/abs/2509.11240)
*Yechen Zhang,Bin Gao,Gang Wang,Jian Sun,Zhuo Li*

Main category: cs.RO

TL;DR: CORB-Planner是一个基于强化学习的实时轨迹规划框架，通过B样条轨迹生成和紧凑安全飞行走廊表示，实现无人机在异构平台上的高速自主飞行


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在无人机部署中的挑战，包括对精确动态模型的依赖和平台特定传感的限制，促进跨平台迁移

Method: 结合B样条轨迹生成和RL策略产生连续控制点，使用启发式搜索获得紧凑安全飞行走廊表示，采用基于值的软分解批评器Q算法和易到难的渐进训练管道

Result: 在仿真和真实世界测试中展示实时规划能力，支持最高8.2m/s飞行速度，兼容多种无人机配置，计算需求适中

Conclusion: CORB-Planner具有通用性和鲁棒性，适合实际部署，解决了模型不准确性和平台特定细节的过拟合问题

Abstract: Reinforcement learning (RL) has shown promise in a large number of robotic
control tasks. Nevertheless, its deployment on unmanned aerial vehicles (UAVs)
remains challenging, mainly because of reliance on accurate dynamic models and
platform-specific sensing, which hinders cross-platform transfer. This paper
presents the CORB-Planner (Corridor-as-Observations for RL B-spline planner), a
real-time, RL-based trajectory planning framework for high-speed autonomous UAV
flight across heterogeneous platforms. The key idea is to combine B-spline
trajectory generation with the RL policy producing successive control points
with a compact safe flight corridor (SFC) representation obtained via heuristic
search. The SFC abstracts obstacle information in a low-dimensional form,
mitigating overfitting to platform-specific details and reducing sensitivity to
model inaccuracies. To narrow the sim-to-real gap, we adopt an easy-to-hard
progressive training pipeline in simulation. A value-based soft
decomposed-critic Q (SDCQ) algorithm is used to learn effective policies within
approximately ten minutes of training. Benchmarks in simulation and real-world
tests demonstrate real-time planning on lightweight onboard hardware and
support maximum flight speeds up to 8.2m/s in dense, cluttered environments
without external positioning. Compatibility with various UAV configurations
(quadrotors, hexarotors) and modest onboard compute underlines the generality
and robustness of CORB-Planner for practical deployment.

</details>


### [23] [Embodied Intelligence in Disassembly: Multimodal Perception Cross-validation and Continual Learning in Neuro-Symbolic TAMP](https://arxiv.org/abs/2509.11270)
*Ziwen He,Zhigang Wang,Yanlong Peng,Pengxu Chang,Hong Yang,Ming Chen*

Main category: cs.RO

TL;DR: 通过组合神经符号任务与运动规划的持续学习框架，提升了动态环境下机器人感知的稳健性和适应性，将动力电池解体任务成功率从81.68%提升到100%


<details>
  <summary>Details</summary>
Motivation: 新能源汽车行业快速发展，动力电池的高效解体回收成为关键挑战。非结构化解体场景中环境的动态性严重限制了机器人感知的稳健性，障碍了工业应用中的自主解体

Method: 提出基于神经符号任务与运动规划(TAMP)的持续学习框架，集成多模态感知交叉验证机制到双向推理流程中：前向工作流动态精炼优化动作策略，后向学习流自主从历史任务执行中收集有效数据以支持持续系统学习

Result: 实验结果显示，该框架在动态解体场景中将任务成功率从81.68%提升到100%，并将平均感知误判次数从3.389降低到1.128

Conclusion: 该研究为提升体现智能系统在复杂工业环境中的稳健性和适应性提供了新的范式

Abstract: With the rapid development of the new energy vehicle industry, the efficient
disassembly and recycling of power batteries have become a critical challenge
for the circular economy. In current unstructured disassembly scenarios, the
dynamic nature of the environment severely limits the robustness of robotic
perception, posing a significant barrier to autonomous disassembly in
industrial applications. This paper proposes a continual learning framework
based on Neuro-Symbolic task and motion planning (TAMP) to enhance the
adaptability of embodied intelligence systems in dynamic environments. Our
approach integrates a multimodal perception cross-validation mechanism into a
bidirectional reasoning flow: the forward working flow dynamically refines and
optimizes action strategies, while the backward learning flow autonomously
collects effective data from historical task executions to facilitate continual
system learning, enabling self-optimization. Experimental results show that the
proposed framework improves the task success rate in dynamic disassembly
scenarios from 81.68% to 100%, while reducing the average number of perception
misjudgments from 3.389 to 1.128. This research provides a new paradigm for
enhancing the robustness and adaptability of embodied intelligence in complex
industrial environments.

</details>


### [24] [Policy Learning for Social Robot-Led Physiotherapy](https://arxiv.org/abs/2509.11297)
*Carl Bettosi,Lynne Ballie,Susan Shenkin,Marta Romeo*

Main category: cs.RO

TL;DR: 使用医疗专家作为患者代理，通过强化学习训练机器人自适应物理治疗指导策略，解决患者行为数据稀缺问题


<details>
  <summary>Details</summary>
Motivation: 社交机器人可自主指导物理治疗，但缺乏患者行为数据来开发鲁棒的自适应决策策略

Method: 邀请33名医疗专家作为患者代理与机器人互动，建立患者行为模型生成运动表现指标和主观用力评分，在仿真环境中训练强化学习策略

Result: 训练的策略能够根据个体用力耐受度和波动表现自适应调整运动指导，适用于不同康复阶段和运动计划的患者

Conclusion: 该方法成功解决了患者数据稀缺问题，实现了机器人对物理治疗练习的自适应指导

Abstract: Social robots offer a promising solution for autonomously guiding patients
through physiotherapy exercise sessions, but effective deployment requires
advanced decision-making to adapt to patient needs. A key challenge is the
scarcity of patient behavior data for developing robust policies. To address
this, we engaged 33 expert healthcare practitioners as patient proxies, using
their interactions with our robot to inform a patient behavior model capable of
generating exercise performance metrics and subjective scores on perceived
exertion. We trained a reinforcement learning-based policy in simulation,
demonstrating that it can adapt exercise instructions to individual exertion
tolerances and fluctuating performance, while also being applicable to patients
at different recovery stages with varying exercise plans.

</details>


### [25] [Brain-Robot Interface for Exercise Mimicry](https://arxiv.org/abs/2509.11306)
*Carl Bettosi,Emilyann Nault,Lynne Baillie,Markus Garschall,Marta Romeo,Beatrix Wais-Zechmann,Nicole Binderlehner,Theodoros Georgio*

Main category: cs.RO

TL;DR: 社交机器人通过脑机接口实时模仿患者运动动作，提高康复训练的信任和接受度


<details>
  <summary>Details</summary>
Motivation: 为了通过运动模仿构建社交机器人与患者之间的戏拟关系，维持长期康复训练的参与度

Method: 开发了新型脑机接口(BRI)系统，让社交机器能够通过患者的意图命令实时模仿其运动动作，在14名参与者(3名物理治疗师和11名偏瘫病人)中进行了探索性研究

Result: 系统在12次训练中成功展示了运动模仿能力，但准确性有所变化。参与者对机器教练呈现出积极态度，具有高度信任和接受度，且BRI技术的引入没有影响这些积极感知

Conclusion: 基于脑机接口的运动模仿技术在康复训练中具有开发潜力，能够提升患者对社交机器教练的信任和接受度

Abstract: For social robots to maintain long-term engagement as exercise instructors,
rapport-building is essential. Motor mimicry--imitating one's physical
actions--during social interaction has long been recognized as a powerful tool
for fostering rapport, and it is widely used in rehabilitation exercises where
patients mirror a physiotherapist or video demonstration. We developed a novel
Brain-Robot Interface (BRI) that allows a social robot instructor to mimic a
patient's exercise movements in real-time, using mental commands derived from
the patient's intention. The system was evaluated in an exploratory study with
14 participants (3 physiotherapists and 11 hemiparetic patients recovering from
stroke or other injuries). We found our system successfully demonstrated
exercise mimicry in 12 sessions; however, accuracy varied. Participants had
positive perceptions of the robot instructor, with high trust and acceptance
levels, which were not affected by the introduction of BRI technology.

</details>


### [26] [ActivePose: Active 6D Object Pose Estimation and Tracking for Robotic Manipulation](https://arxiv.org/abs/2509.11364)
*Sheng Liu,Zhe Li,Weiheng Wang,Han Sun,Heng Zhang,Hongpeng Chen,Yusen Qin,Arash Ajoudani,Yizhao Wang*

Main category: cs.RO

TL;DR: 提出了一种结合视觉语言模型和机器人想象力的主动位姿估计与跟踪方法，通过动态检测和解决视角引起的模糊性，显著提升了6自由度物体位姿估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决零样本方法在视角引起的模糊性下失效，以及固定摄像头在物体移动或自遮挡时表现不佳的问题，需要一种能够动态适应场景变化的主动位姿估计方法。

Method: 离线阶段渲染CAD模型密集视图并计算熵值，构建几何感知提示。运行时使用VLM检测模糊性，通过虚拟渲染生成候选相机位姿，结合VLM概率和熵值评分选择最佳视角，并引入基于扩散策略的主动跟踪模块保持物体可见性。

Result: 在仿真和真实世界实验中，该方法显著优于传统基线方法，能够有效处理视角模糊和物体移动问题。

Conclusion: 提出的主动位姿估计与跟踪框架通过结合视觉语言模型和机器人想象力，成功解决了视角模糊和物体移动带来的挑战，为可靠机器人操作提供了有效的解决方案。

Abstract: Accurate 6-DoF object pose estimation and tracking are critical for reliable
robotic manipulation. However, zero-shot methods often fail under
viewpoint-induced ambiguities and fixed-camera setups struggle when objects
move or become self-occluded. To address these challenges, we propose an active
pose estimation pipeline that combines a Vision-Language Model (VLM) with
"robotic imagination" to dynamically detect and resolve ambiguities in real
time. In an offline stage, we render a dense set of views of the CAD model,
compute the FoundationPose entropy for each view, and construct a
geometric-aware prompt that includes low-entropy (unambiguous) and high-entropy
(ambiguous) examples. At runtime, the system: (1) queries the VLM on the live
image for an ambiguity score; (2) if ambiguity is detected, imagines a discrete
set of candidate camera poses by rendering virtual views, scores each based on
a weighted combination of VLM ambiguity probability and FoundationPose entropy,
and then moves the camera to the Next-Best-View (NBV) to obtain a disambiguated
pose estimation. Furthermore, since moving objects may leave the camera's field
of view, we introduce an active pose tracking module: a diffusion-policy
trained via imitation learning, which generates camera trajectories that
preserve object visibility and minimize pose ambiguity. Experiments in
simulation and real-world show that our approach significantly outperforms
classical baselines.

</details>


### [27] [Quantum deep reinforcement learning for humanoid robot navigation task](https://arxiv.org/abs/2509.11388)
*Romerik Lokossou,Birhanu Shimelis Girma,Ozan K. Tonguz,Ahmed Biyabani*

Main category: cs.RO

TL;DR: 量子深度强化学习(QDRL)在人形机器人控制中相比经典方法实现92%更少步数下8%更高回报


<details>
  <summary>Details</summary>
Motivation: 经典强化学习方法在高维复杂环境中参数需求大且面临随机性挑战，需要更高效的训练方法

Method: 使用参数化量子电路构建混合量子-经典架构，直接在MuJoCo Humanoid-v4和Walker2d-v4等高维环境中训练，对比量子SAC与经典SAC性能

Result: 量子SAC平均回报246.40，比经典SAC(228.36)高8%，且学习步数减少92%

Conclusion: 量子计算在强化学习中展现出加速学习潜力，特别适合高维状态空间的人形机器人控制任务

Abstract: Classical reinforcement learning (RL) methods often struggle in complex,
high-dimensional environments because of their extensive parameter requirements
and challenges posed by stochastic, non-deterministic settings. This study
introduces quantum deep reinforcement learning (QDRL) to train humanoid agents
efficiently. While previous quantum RL models focused on smaller environments,
such as wheeled robots and robotic arms, our work pioneers the application of
QDRL to humanoid robotics, specifically in environments with substantial
observation and action spaces, such as MuJoCo's Humanoid-v4 and Walker2d-v4.
Using parameterized quantum circuits, we explored a hybrid quantum-classical
setup to directly navigate high-dimensional state spaces, bypassing traditional
mapping and planning. By integrating quantum computing with deep RL, we aim to
develop models that can efficiently learn complex navigation tasks in humanoid
robots. We evaluated the performance of the Soft Actor-Critic (SAC) in
classical RL against its quantum implementation. The results show that the
quantum SAC achieves an 8% higher average return (246.40) than the classical
SAC (228.36) after 92% fewer steps, highlighting the accelerated learning
potential of quantum computing in RL tasks.

</details>


### [28] [TRUST 2025: SCRITA and RTSS @ RO-MAN 2025](https://arxiv.org/abs/2509.11402)
*Alessandra Rossi,Patrick Holthaus,Gabriella Lakatos,Sílvia Moros,Ali Fallahi,Murat Kirtay,Marie Postma,Erhan Oztop*

Main category: cs.RO

TL;DR: TRUST研讨会是SCRITA和RTSS两个HRI领域成熟研讨会的合作成果，整合了从人类和机器人双重视角推进信任研究的目标


<details>
  <summary>Details</summary>
Motivation: 整合两个互补的HRI研讨会（SCRITA关注信任、接受度和社会线索，RTSS关注机器人信任与共生社会）的优势，共同推进人机信任研究

Method: 通过联合研讨会的形式，汇集两个工作坊的专家资源和研究视角，建立合作平台

Result: 成功创建了TRUST联合研讨会，为人机交互领域的信任研究提供了更全面的交流平台

Conclusion: 这种合作模式有效整合了人机信任研究的不同视角，有望推动该领域的协同发展和创新

Abstract: The TRUST workshop is the result of a collaboration between two established
workshops in the field of Human-Robot Interaction: SCRITA (Trust, Acceptance
and Social Cues in Human-Robot Interaction) and RTSS (Robot Trust for Symbiotic
Societies). This joint initiative brings together the complementary goals of
these workshops to advance research on trust from both the human and robot
perspectives.
  Website: https://scrita.herts.ac.uk/2025/

</details>


### [29] [Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations](https://arxiv.org/abs/2509.11417)
*Shresth Grover,Akshay Gopalkrishnan,Bo Ai,Henrik I. Christensen,Hao Su,Xuanlin Li*

Main category: cs.RO

TL;DR: 通过冻结视觉编码器、字符串动作标记化和多数据源兼训练策略，在保留预训练特征的同时适配机器人操作任务


<details>
  <summary>Details</summary>
Motivation: 直接对机器人数据进行微调会破坏预训练的丰富表征，限制模型的普适性。需要一种方法在适配机器人任务的同时保留这些预训练特征

Method: 1）双编码器设计：一个冻结的视觉编码器保留预训练特征，另一个可训练的编码器用于任务适配
2）字符串动作标记化器：将连续动作转换为字符序列，与模型预训练域对齐
3）协同训练策略：结合机器人示范数据和强调空间推理及支配性的视觉-语言数据集

Result: 在模拟和真实机器上的评估显示，该方法提高了对视觉干扰的稳健性、对新指令和环境的普适性，以及整体任务成功率，超过基线方法

Conclusion: 该框架有效地保留了视觉-语言模型的预训练特征，同时成功适配于机器人操作任务，为建立普适性更强的VLA模型提供了有效途径

Abstract: Vision-language-action (VLA) models finetuned from vision-language models
(VLMs) hold the promise of leveraging rich pretrained representations to build
generalist robots across diverse tasks and environments. However, direct
fine-tuning on robot data often disrupts these representations and limits
generalization. We present a framework that better preserves pretrained
features while adapting them for robot manipulation. Our approach introduces
three components: (i) a dual-encoder design with one frozen vision encoder to
retain pretrained features and another trainable for task adaptation, (ii) a
string-based action tokenizer that casts continuous actions into character
sequences aligned with the model's pretraining domain, and (iii) a co-training
strategy that combines robot demonstrations with vision-language datasets
emphasizing spatial reasoning and affordances. Evaluations in simulation and on
real robots show that our method improves robustness to visual perturbations,
generalization to novel instructions and environments, and overall task success
compared to baselines.

</details>


### [30] [A Software-Only Post-Processor for Indexed Rotary Machining on GRBL-Based CNCs](https://arxiv.org/abs/2509.11433)
*Pedro Portugal,Damian D. Venghaus,Diego Lopez*

Main category: cs.RO

TL;DR: 这是一个专门为GRBL基础CNC机床设计的软件解决方案，通过软件后处理器将平面刀具路径转换为间距旋转步骤，无需硬件改造即可实现多面分动加工。


<details>
  <summary>Details</summary>
Motivation: 解决普通桌面CNC刀前缺乏旋转轴的问题，这个问题限制了旋转对称和多面部件的制造能力。现有解决方案需要硬件改造、替代控制器或商业CAM软件，成本高且复杂。

Method: 开发了一个软件仅的框架，包含自定义后处理器，将平面刀具路径转换为离散的旋转步骤，通过浏览器基础界面执行。方法不需要修改硬件固件。

Result: 该框架实现了实用的旋辊轴加工能力，虽然不能模拟连续的4轴加工，但可以使用标准机械配件完成多面分动加工。

Conclusion: 通过降低技术和资金闯碍，该框架扩大了多轴加工在教室、制造空间和小型工作室中的应用，支持手技学习和快速原型制作。

Abstract: Affordable desktop CNC routers are common in education, prototyping, and
makerspaces, but most lack a rotary axis, limiting fabrication of rotationally
symmetric or multi-sided parts. Existing solutions often require hardware
retrofits, alternative controllers, or commercial CAM software, raising cost
and complexity. This work presents a software-only framework for indexed rotary
machining on GRBL-based CNCs. A custom post-processor converts planar toolpaths
into discrete rotary steps, executed through a browser-based interface. While
not equivalent to continuous 4-axis machining, the method enables practical
rotary-axis fabrication using only standard, off-the-shelf mechanics, without
firmware modification. By reducing technical and financial barriers, the
framework expands access to multi-axis machining in classrooms, makerspaces,
and small workshops, supporting hands-on learning and rapid prototyping.

</details>


### [31] [RAPTOR: A Foundation Policy for Quadrotor Control](https://arxiv.org/abs/2509.11481)
*Jonas Eschmann,Dario Albani,Giuseppe Loianno*

Main category: cs.RO

TL;DR: RAPTOR方法训练了一个高度自适应的四旋翼控制基础策略，仅用2084个参数的神经网络就能零样本适应10种不同的真实四旋翼无人机，通过元模仿学习和上下文学习实现快速适应。


<details>
  <summary>Details</summary>
Motivation: 解决现代机器人控制系统（如RL训练的神经网络策略）过度专业化的问题，使其能够像人类一样高效适应新环境，克服Sim2Real差距和系统微小变化带来的挑战。

Method: 使用元模仿学习算法：先采样1000个四旋翼并训练各自的教师策略，然后蒸馏成单一自适应学生策略；利用隐藏层循环实现上下文学习。

Result: 在毫秒级别内零样本适应未见过的四旋翼，成功测试了10种不同真实四旋翼（32g-2.4kg），涵盖多种电机类型、框架类型、螺旋桨类型和飞行控制器。

Conclusion: RAPTOR方法能够训练出高度自适应的基础控制策略，仅需极小参数就能实现跨平台的零样本适应，在各种条件下表现鲁棒。

Abstract: Humans are remarkably data-efficient when adapting to new unseen conditions,
like driving a new car. In contrast, modern robotic control systems, like
neural network policies trained using Reinforcement Learning (RL), are highly
specialized for single environments. Because of this overfitting, they are
known to break down even under small differences like the Simulation-to-Reality
(Sim2Real) gap and require system identification and retraining for even
minimal changes to the system. In this work, we present RAPTOR, a method for
training a highly adaptive foundation policy for quadrotor control. Our method
enables training a single, end-to-end neural-network policy to control a wide
variety of quadrotors. We test 10 different real quadrotors from 32 g to 2.4 kg
that also differ in motor type (brushed vs. brushless), frame type (soft vs.
rigid), propeller type (2/3/4-blade), and flight controller
(PX4/Betaflight/Crazyflie/M5StampFly). We find that a tiny, three-layer policy
with only 2084 parameters is sufficient for zero-shot adaptation to a wide
variety of platforms. The adaptation through In-Context Learning is made
possible by using a recurrence in the hidden layer. The policy is trained
through a novel Meta-Imitation Learning algorithm, where we sample 1000
quadrotors and train a teacher policy for each of them using Reinforcement
Learning. Subsequently, the 1000 teachers are distilled into a single, adaptive
student policy. We find that within milliseconds, the resulting foundation
policy adapts zero-shot to unseen quadrotors. We extensively test the
capabilities of the foundation policy under numerous conditions (trajectory
tracking, indoor/outdoor, wind disturbance, poking, different propellers).

</details>


### [32] [FR-Net: Learning Robust Quadrupedal Fall Recovery on Challenging Terrains through Mass-Contact Prediction](https://arxiv.org/abs/2509.11504)
*Yidan Lu,Yinzhao Dong,Jiahui Zhang,Ji Ma,Peng Lu*

Main category: cs.RO

TL;DR: FR-Net是一个基于学习的四足机器人跌倒恢复框架，通过质量-接触预测网络从有限传感器输入估计质量分布和接触状态，能在复杂地形上实现安全恢复。


<details>
  <summary>Details</summary>
Motivation: 传统控制器在复杂地形上由于地形感知不完整和交互不确定性而失败，四足机器人的跌倒恢复仍然具有挑战性。

Method: 使用质量-接触预测器网络估计机器人的质量分布和接触状态，通过精心设计的奖励函数在模拟中进行特权学习训练，无需部署时的显式地形数据。

Result: 在模拟中展示了跨不同四足平台的泛化能力，并在Go2机器人上的10个挑战性场景中通过真实世界实验验证了性能。

Conclusion: 显式的质量-接触预测是鲁棒跌倒恢复的关键，为可泛化的四足机器人技能提供了有前景的方向。

Abstract: Fall recovery for legged robots remains challenging, particularly on complex
terrains where traditional controllers fail due to incomplete terrain
perception and uncertain interactions. We present \textbf{FR-Net}, a
learning-based framework that enables quadrupedal robots to recover from
arbitrary fall poses across diverse environments. Central to our approach is a
Mass-Contact Predictor network that estimates the robot's mass distribution and
contact states from limited sensory inputs, facilitating effective recovery
strategies. Our carefully designed reward functions ensure safe recovery even
on steep stairs without dangerous rolling motions common to existing methods.
Trained entirely in simulation using privileged learning, our framework guides
policy learning without requiring explicit terrain data during deployment. We
demonstrate the generalization capabilities of \textbf{FR-Net} across different
quadrupedal platforms in simulation and validate its performance through
extensive real-world experiments on the Go2 robot in 10 challenging scenarios.
Our results indicate that explicit mass-contact prediction is key to robust
fall recovery, offering a promising direction for generalizable quadrupedal
skills.

</details>


### [33] [Design and Development of a Remotely Wire-Driven Walking Robot](https://arxiv.org/abs/2509.11506)
*Takahiro Hattori,Kento Kawaharazuka,Kei Okada*

Main category: cs.RO

TL;DR: 通过线综驱动机制实现离线控制移动机器人，解决极端环境中电子元件容易损坏的问题


<details>
  <summary>Details</summary>
Motivation: 在核电站等极端环境中，传统的自主移动机器人需要电子元件，而无电子自主机器人又无法做复杂决策。需要一种新的机制来实现离线控制的移动机器人

Method: 提出了一种称为"Remote Wire Drive"的新型机制，通过线综传动动力来远程驱动移动机器人。该机制是将用于线综驱动机械臂的解耦关节系列连接机制适配为动力传递系统

Result: 通过实验验证了该机制的可行性，成功地通过Remote Wire Drive驱动了自行开发的线综驱动四足机器人

Conclusion: Remote Wire Drive机制为极端环境下的移动机器人控制提供了一种新的解决方案，结合了线综驱动的广泛环境适用性和移动机器人的大范围到达能力

Abstract: Operating in environments too harsh or inaccessible for humans is one of the
critical roles expected of robots. However, such environments often pose risks
to electronic components as well. To overcome this, various approaches have
been developed, including autonomous mobile robots without electronics,
hydraulic remotely actuated mobile robots, and long-reach robot arms driven by
wires. Among these, electronics-free autonomous robots cannot make complex
decisions, while hydraulically actuated mobile robots and wire-driven robot
arms are used in harsh environments such as nuclear power plants. Mobile robots
offer greater reach and obstacle avoidance than robot arms, and wire mechanisms
offer broader environmental applicability than hydraulics. However, wire-driven
systems have not been used for remote actuation of mobile robots. In this
study, we propose a novel mechanism called Remote Wire Drive that enables
remote actuation of mobile robots via wires. This mechanism is a series
connection of decoupled joints, a mechanism used in wire-driven robot arms,
adapted for power transmission. We experimentally validated its feasibility by
actuating a wire-driven quadruped robot, which we also developed in this study,
through Remote Wire Drive.

</details>


### [34] [PaiP: An Operational Aware Interactive Planner for Unknown Cabinet Environments](https://arxiv.org/abs/2509.11516)
*Chengjin Wang,Zheng Yan,Yanmin Zhou,Runjie Shen,Zhipeng Wang,Bin Cheng,Bin He*

Main category: cs.RO

TL;DR: 提出了PaiP交互运动规划器，通过多模态触觉感知实现实时闭环规划，解决箱柜场景中堆叠物体的视觉遮挡和空间受限问题


<details>
  <summary>Details</summary>
Motivation: 传统无碰撞轨迹规划方法在箱柜堆叠场景中经常失败，因为不存在无碰撞路径，甚至可能因不可见物体导致灾难性碰撞

Method: 利用多模态触觉感知自主推断物体交互特征，将交互特征融入栅格地图生成操作代价图，在采样规划方法基础上同时优化路径代价和操作代价

Result: 实验结果表明PaiP在狭窄空间中实现了鲁棒的运动

Conclusion: 该框架通过交互感知有效解决了视觉遮挡环境下的运动规划挑战

Abstract: Box/cabinet scenarios with stacked objects pose significant challenges for
robotic motion due to visual occlusions and constrained free space. Traditional
collision-free trajectory planning methods often fail when no collision-free
paths exist, and may even lead to catastrophic collisions caused by invisible
objects. To overcome these challenges, we propose an operational aware
interactive motion planner (PaiP) a real-time closed-loop planning framework
utilizing multimodal tactile perception. This framework autonomously infers
object interaction features by perceiving motion effects at interaction
interfaces. These interaction features are incorporated into grid maps to
generate operational cost maps. Building upon this representation, we extend
sampling-based planning methods to interactive planning by optimizing both path
cost and operational cost. Experimental results demonstrate that PaiP achieves
robust motion in narrow spaces.

</details>


### [35] [Shape control of simulated multi-segment continuum robots via Koopman operators with per-segment projection](https://arxiv.org/abs/2509.11567)
*Eron Ristich,Jiahe Wang,Lei Zhang,Sultan Haidar Ali,Wanxin Jin,Yi Ren,Jiefeng Sun*

Main category: cs.RO

TL;DR: 基于Koopman算子的数据驱动方法，通过模型预测控制实现软连续体机器人的形状控制，解决了高计算复杂度问题


<details>
  <summary>Details</summary>
Motivation: 当前软连续体机器人只能实现实时端点控制，而无法实现全形状控制，主要因为无限自由度导致的高计算成本

Method: 采用数据驱动的Koopman算子方法，通过模拟软机器人收集数据，进行每段投影方案，识别控制伴随Koopman模型，使用线性模型预测控制(MPC)实现形状控制

Result: 方法实现了计算效率高的闭环控制，模型准确性比无投影方案提高一个数量级，证明了软机器人实时形状控制的可行性

Conclusion: 这项工作为软连续体机器人的实用形状控制推广了道路

Abstract: Soft continuum robots can allow for biocompatible yet compliant motions, such
as the ability of octopus arms to swim, crawl, and manipulate objects. However,
current state-of-the-art continuum robots can only achieve real-time task-space
control (i.e., tip control) but not whole-shape control, mainly due to the high
computational cost from its infinite degrees of freedom. In this paper, we
present a data-driven Koopman operator-based approach for the shape control of
simulated multi-segment tendon-driven soft continuum robots with the Kirchhoff
rod model. Using data collected from these simulated soft robots, we conduct a
per-segment projection scheme on the state of the robots allowing for the
identification of control-affine Koopman models that are an order of magnitude
more accurate than without the projection scheme. Using these learned Koopman
models, we use a linear model predictive control (MPC) to control the robots to
a collection of target shapes of varying complexity. Our method realizes
computationally efficient closed-loop control, and demonstrates the feasibility
of real-time shape control for soft robots. We envision this work can pave the
way for practical shape control of soft continuum robots.

</details>


### [36] [GBPP: Grasp-Aware Base Placement Prediction for Robots via Two-Stage Learning](https://arxiv.org/abs/2509.11594)
*Jizhuo Chen,Diwen Liu,Jiaming Wang,Harold Soh*

Main category: cs.RO

TL;DR: GBPP是一种基于快速学习的评分器，通过单次RGB-D快照为抓取任务选择机器人基座位姿。该方法采用两阶段课程学习：低成本自动标注大量数据，然后通过少量高保真仿真试验精调模型以匹配真实抓取结果。


<details>
  <summary>Details</summary>
Motivation: 解决机器人抓取任务中基座位姿选择的问题，传统方法需要复杂的任务和运动优化，计算成本高且效率低。

Method: 使用两阶段课程学习：1) 基于距离-可见性规则的简单启发式方法自动标注大规模数据集；2) 通过少量高保真仿真试验精调模型。采用PointNet++风格的点云编码器和MLP对候选位姿密集网格进行评分。

Result: 在仿真和真实移动机械臂上，GBPP优于仅基于接近度和几何的基线方法，选择更安全、更可达的站位，在错误时能够优雅降级。

Conclusion: 提供了一种数据高效、几何感知的基座位姿选择实用方案：使用廉价启发式方法实现覆盖，然后通过针对性仿真进行校准。

Abstract: GBPP is a fast learning based scorer that selects a robot base pose for
grasping from a single RGB-D snapshot. The method uses a two stage curriculum:
(1) a simple distance-visibility rule auto-labels a large dataset at low cost;
and (2) a smaller set of high fidelity simulation trials refines the model to
match true grasp outcomes. A PointNet++ style point cloud encoder with an MLP
scores dense grids of candidate poses, enabling rapid online selection without
full task-and-motion optimization. In simulation and on a real mobile
manipulator, GBPP outperforms proximity and geometry only baselines, choosing
safer and more reachable stances and degrading gracefully when wrong. The
results offer a practical recipe for data efficient, geometry aware base
placement: use inexpensive heuristics for coverage, then calibrate with
targeted simulation.

</details>


### [37] [AssemMate: Graph-Based LLM for Robotic Assembly Assistance](https://arxiv.org/abs/2509.11617)
*Qi Zheng,Chaoran Zhang,Zijian Liang,EnTe Lin,Shubo Cui,Qinghongbing Xie,Zhaobo Xu,Long Zeng*

Main category: cs.RO

TL;DR: AssemMate是一个基于知识图谱的LLM系统，用于机器人装配辅助，通过图结构表示知识，比传统文本方法更高效准确，支持问答、任务规划和抓取操作


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言文本的知识表示方法存在上下文过长、内容冗余问题，无法满足机器人实时精确推理的需求，需要更简洁准确的知识表示形式

Method: 使用知识图谱作为输入，通过自监督图卷积网络(GCN)将知识图谱实体和关系编码到潜在空间，并与LLM表示对齐，使LLM能够理解图信息；采用视觉增强策略处理堆叠场景抓取

Result: 在训练评估中表现优于现有方法：准确率提高6.4%，推理速度快3倍，上下文长度缩短28倍，在随机图谱上展示强泛化能力；在模拟和真实机器人抓取实验中表现优越

Conclusion: 图结构的知识表示相比自然语言文本更适用于机器人装配辅助任务，能够提供更实时、精确的推理能力，AssemMate系统在准确率、速度和泛化性方面都有显著提升

Abstract: Large Language Model (LLM)-based robotic assembly assistance has gained
significant research attention. It requires the injection of domain-specific
knowledge to guide the assembly process through natural language interaction
with humans. Despite some progress, existing methods represent knowledge in the
form of natural language text. Due to the long context and redundant content,
they struggle to meet the robots' requirements for real-time and precise
reasoning. In order to bridge this gap, we present AssemMate, which utilizes
the graph\textemdash a concise and accurate form of knowledge
representation\textemdash as input. This graph-based LLM enables knowledge
graph question answering (KGQA), supporting human-robot interaction and
assembly task planning for specific products. Beyond interactive QA, AssemMate
also supports sensing stacked scenes and executing grasping to assist with
assembly. Specifically, a self-supervised Graph Convolutional Network (GCN)
encodes knowledge graph entities and relations into a latent space and aligns
them with LLM's representation, enabling the LLM to understand graph
information. In addition, a vision-enhanced strategy is employed to address
stacked scenes in grasping. Through training and evaluation, AssemMate
outperforms existing methods, achieving 6.4\% higher accuracy, 3 times faster
inference, and 28 times shorter context length, while demonstrating strong
generalization ability on random graphs. And our approach further demonstrates
superiority through robotic grasping experiments in both simulated and
real-world settings. More details can be found on the project page:
https://github.com/cristina304/AssemMate.git

</details>


### [38] [Inference-stage Adaptation-projection Strategy Adapts Diffusion Policy to Cross-manipulators Scenarios](https://arxiv.org/abs/2509.11621)
*Xiangtong Yao,Yirui Zhou,Yuan Meng,Yanwen Liu,Liangyu Dong,Zitao Zhang,Zhenshan Bing,Kai Huang,Fuchun Sun,Alois Knoll*

Main category: cs.RO

TL;DR: 提出了一种零样本适应策略，使扩散策略能够在推理时无需重新训练即可适应新机械臂和动态任务设置


<details>
  <summary>Details</summary>
Motivation: 扩散策略在机器人操作中表现强大，但难以泛化到未见过的机械臂和末端执行器，且难以适应新的任务需求，通常需要昂贵的数据重新收集和策略重新训练

Method: 采用适应-投影策略：首先在SE(3)空间中训练基础扩散策略，然后在在线部署时通过投影将生成的轨迹调整以满足新硬件和任务的运动学和任务特定约束

Result: 在真实世界的拾取放置、推动和倾倒任务中，跨多个机械臂（Franka Panda和Kuka iiwa 14）和多种末端执行器上实现了持续高成功率

Conclusion: 该方法证明了适应-投影策略在跨机械臂场景中的有效性和实用性，能够实现零样本适应而不需要重新训练

Abstract: Diffusion policies are powerful visuomotor models for robotic manipulation,
yet they often fail to generalize to manipulators or end-effectors unseen
during training and struggle to accommodate new task requirements at inference
time. Addressing this typically requires costly data recollection and policy
retraining for each new hardware or task configuration. To overcome this, we
introduce an adaptation-projection strategy that enables a diffusion policy to
perform zero-shot adaptation to novel manipulators and dynamic task settings,
entirely at inference time and without any retraining. Our method first trains
a diffusion policy in SE(3) space using demonstrations from a base manipulator.
During online deployment, it projects the policy's generated trajectories to
satisfy the kinematic and task-specific constraints imposed by the new hardware
and objectives. Moreover, this projection dynamically adapts to physical
differences (e.g., tool-center-point offsets, jaw widths) and task requirements
(e.g., obstacle heights), ensuring robust and successful execution. We validate
our approach on real-world pick-and-place, pushing, and pouring tasks across
multiple manipulators, including the Franka Panda and Kuka iiwa 14, equipped
with a diverse array of end-effectors like flexible grippers, Robotiq 2F/3F
grippers, and various 3D-printed designs. Our results demonstrate consistently
high success rates in these cross-manipulator scenarios, proving the
effectiveness and practicality of our adaptation-projection strategy. The code
will be released after peer review.

</details>


### [39] [ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and Answering](https://arxiv.org/abs/2509.11663)
*Haisheng Wang,Weiming Zhi*

Main category: cs.RO

TL;DR: 这篇论文提出了并行体验问题答案（EQsA）问题，建立了PAEQs标准数据集，并设计了ParaEQsA系统来实现紧急性感知和并行调度，在多问题场景下显著提升了效率和响应能力。


<details>
  <summary>Details</summary>
Motivation: 传统体验问题答案（EQA）通常只处理单个问题，而实际部署常需要同时处理多个可能异步到来且具有不同紧急程度的问题。

Method: 提出ParaEQsA框架，包含一个共享的组内存模块来减少重复探索，以及一个优先级规划模块来动态调度问题。建立了PAEQs标准数据集，包含40个室内场景和每个场景5个问题，具有异步跟进问题和紧急标签。

Result: ParaEQsA在效率和响应能力方面一赴超过了来自最新EQA系统的强大顺序基线，同时减少了探索和延迟。实验评估调查了优先级、紧急急急急模型、空间范围、奖励估计和依赖性推理在框架内的相对贡献。

Conclusion: 紧急急急急感知和并行调度是使体验代理在现实多问题工作负荷下具有响应能力和高效性的关键。

Abstract: This paper formulates the Embodied Questions Answering (EQsA) problem,
introduces a corresponding benchmark, and proposes a system to tackle the
problem. Classical Embodied Question Answering (EQA) is typically formulated as
answering one single question by actively exploring a 3D environment. Real
deployments, however, often demand handling multiple questions that may arrive
asynchronously and carry different urgencies. We formalize this setting as
Embodied Questions Answering (EQsA) and present ParaEQsA, a framework for
parallel, urgency-aware scheduling and answering. ParaEQsA leverages a group
memory module shared among questions to reduce redundant exploration, and a
priority-planning module to dynamically schedule questions. To evaluate this
setting, we contribute the Parallel Asynchronous Embodied Questions (PAEQs)
benchmark containing 40 indoor scenes and five questions per scene (200 in
total), featuring asynchronous follow-up questions and urgency labels. We
further propose metrics for EQsA performance: Direct Answer Rate (DAR), and
Normalized Urgency-Weighted Latency (NUWL), which jointly measure efficiency
and responsiveness of this system. ParaEQsA consistently outperforms strong
sequential baselines adapted from recent EQA systems, while reducing
exploration and delay. Empirical evaluations investigate the relative
contributions of priority, urgency modeling, spatial scope, reward estimation,
and dependency reasoning within our framework. Together, these results
demonstrate that urgency-aware, parallel scheduling is key to making embodied
agents responsive and efficient under realistic, multi-question workloads.

</details>


### [40] [Tensor Invariant Data-Assisted Control and Dynamic Decomposition of Multibody Systems](https://arxiv.org/abs/2509.11688)
*Mostafa Eslami,Maryam Babazadeh*

Main category: cs.RO

TL;DR: 提出了一种基于张量力学的坐标无关多体动力学模型与数据辅助控制相结合的框架，解决了机器人系统在协作空间中学习的数据效率低下和泛化问题。


<details>
  <summary>Details</summary>
Motivation: 传统坐标依赖模型导致数据效率低下，无法在不同参考系间泛化物理交互，迫使学习算法在每个新方向重新发现物理原理，人为增加了学习复杂度。

Method: 开发了基于张量力学的坐标无关非简化多体动力学和运动学模型，结合数据辅助控制架构，使用增强矩阵形式的非递归闭式牛顿-欧拉模型，并通过李雅普诺夫分析证明稳定性。

Result: 通过仿真验证了模型和闭环系统的有效性，为数据高效的框架不变学习算法提供了理想输入，提高了可解释性和鲁棒性。

Conclusion: 该框架直接解决了数据效率问题，提高了系统可解释性，为交互环境中更鲁棒和可泛化的机器人控制铺平了道路。

Abstract: The control of robotic systems in complex, shared collaborative workspaces
presents significant challenges in achieving robust performance and safety when
learning from experienced or simulated data is employed in the pipeline. A
primary bottleneck is the reliance on coordinate-dependent models, which leads
to profound data inefficiency by failing to generalize physical interactions
across different frames of reference. This forces learning algorithms to
rediscover fundamental physical principles in every new orientation,
artificially inflating the complexity of the learning task. This paper
introduces a novel framework that synergizes a coordinate-free, unreduced
multibody dynamics and kinematics model based on tensor mechanics with a
Data-Assisted Control (DAC) architecture. A non-recursive, closed-form
Newton-Euler model in an augmented matrix form is derived that is optimized for
tensor-based control design. This structure enables a principled decomposition
of the system into a structurally certain, physically grounded part and an
uncertain, empirical, and interaction-focused part, mediated by a virtual port
variable. Then, a complete, end-to-end tensor-invariant pipeline for modeling,
control, and learning is proposed. The coordinate-free control laws for the
structurally certain part provide a stable and abstract command interface,
proven via Lyapunov analysis. Eventually, the model and closed-loop system are
validated through simulations. This work provides a naturally ideal input for
data-efficient, frame-invariant learning algorithms, such as equivariant
learning, designed to learn the uncertain interaction. The synergy directly
addresses the data-inefficiency problem, increases explainability and
interpretability, and paves the way for more robust and generalizable robotic
control in interactive environments.

</details>


### [41] [From Pixels to Shelf: End-to-End Algorithmic Control of a Mobile Manipulator for Supermarket Stocking and Fronting](https://arxiv.org/abs/2509.11740)
*Davide Peron,Victor Nan Fernandez-Ayala,Lukas Segelmark*

Main category: cs.RO

TL;DR: 这篇论文提出了一种用于超市自主架上补货的端到端机器人系统，集成了商用硬件和高效算法，在实验室环境中达到了超过98%的成功率，但仍较人类工作者性能差距。


<details>
  <summary>Details</summary>
Motivation: 超市自主补货面临动态人机交互、空间约束和多样化商品形状的挑战，需要开发高效可部署的机器人系统。

Method: 集成商用硬件和ROS2基础的感知、规划和控制系统，采用行为树进行任务规划，精细调整的视觉模型进行物体检测，以及基于ArUco标记的两步模型预测控制框架进行精确架上导航。

Result: 在模拟现实超市条件的实验室环境中，系统在超过700次补货事件中达到了超过98%的摘放操作成功率。

Conclusion: 当前自主系统的性能和成本效益仍低于人类工作者，需要进一步改进才能实现大规模商业部署。

Abstract: Autonomous stocking in retail environments, particularly supermarkets,
presents challenges due to dynamic human interactions, constrained spaces, and
diverse product geometries. This paper introduces an efficient end-to-end
robotic system for autonomous shelf stocking and fronting, integrating
commercially available hardware with a scalable algorithmic architecture. A
major contribution of this work is the system integration of off-the-shelf
hardware and ROS2-based perception, planning, and control into a single
deployable platform for retail environments. Our solution leverages Behavior
Trees (BTs) for task planning, fine-tuned vision models for object detection,
and a two-step Model Predictive Control (MPC) framework for precise shelf
navigation using ArUco markers. Laboratory experiments replicating realistic
supermarket conditions demonstrate reliable performance, achieving over 98%
success in pick-and-place operations across a total of more than 700 stocking
events. However, our comparative benchmarks indicate that the performance and
cost-effectiveness of current autonomous systems remain inferior to that of
human workers, which we use to highlight key improvement areas and quantify the
progress still required before widespread commercial deployment can
realistically be achieved.

</details>


### [42] [Adaptive Motorized LiDAR Scanning Control for Robust Localization with OpenStreetMap](https://arxiv.org/abs/2509.11742)
*Jianping Li,Kaisong Zhu,Zhongyuan Liu,Rui Jin,Xinhang Xu,Pengfei Wan,Lihua Xie*

Main category: cs.RO

TL;DR: 提出了一种基于OpenStreetMap引导的自适应LiDAR扫描框架，通过结合全局地图先验和局部可观测性预测来提升定位鲁棒性，相比恒定速度扫描显著降低了轨迹误差。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR-to-OSM定位方法面临OSM数据不完整或过时的问题，同时恒定速度扫描的LiDAR系统在特征稀疏区域浪费扫描资源，导致定位精度下降。

Method: 采用不确定性感知的模型预测控制，增加OSM感知项，根据场景依赖的可观测性和OSM特征空间分布自适应分配扫描资源，在ROS中实现电机化LiDAR里程计后端。

Result: 在校园道路、室内走廊和城市环境中的实验表明，相比恒定速度基线方法，轨迹误差显著降低，同时保持了扫描完整性。

Conclusion: 将开源地图与自适应LiDAR扫描相结合，能够在复杂环境中实现鲁棒高效的定位。

Abstract: LiDAR-to-OpenStreetMap (OSM) localization has gained increasing attention, as
OSM provides lightweight global priors such as building footprints. These
priors enhance global consistency for robot navigation, but OSM is often
incomplete or outdated, limiting its reliability in real-world deployment.
Meanwhile, LiDAR itself suffers from a limited field of view (FoV), where
motorized rotation is commonly used to achieve panoramic coverage. Existing
motorized LiDAR systems, however, typically employ constant-speed scanning that
disregards both scene structure and map priors, leading to wasted effort in
feature-sparse regions and degraded localization accuracy. To address these
challenges, we propose Adaptive LiDAR Scanning with OSM guidance, a framework
that integrates global priors with local observability prediction to improve
localization robustness. Specifically, we augment uncertainty-aware model
predictive control with an OSM-aware term that adaptively allocates scanning
effort according to both scene-dependent observability and the spatial
distribution of OSM features. The method is implemented in ROS with a motorized
LiDAR odometry backend and evaluated in both simulation and real-world
experiments. Results on campus roads, indoor corridors, and urban environments
demonstrate significant reductions in trajectory error compared to
constant-speed baselines, while maintaining scan completeness. These findings
highlight the potential of coupling open-source maps with adaptive LiDAR
scanning to achieve robust and efficient localization in complex environments.

</details>


### [43] [Igniting VLMs toward the Embodied Space](https://arxiv.org/abs/2509.11766)
*Andy Zhai,Brae Liu,Bruno Fang,Chalse Cai,Ellie Ma,Ethan Yin,Hao Wang,Hugo Zhou,James Wang,Lights Shi,Lucy Liang,Make Wang,Qian Wang,Roy Gan,Ryan Yu,Shalfun Li,Starrick Liu,Sylas Chen,Vincent Chen,Zach Xu*

Main category: cs.RO

TL;DR: WALL-OSS是一个端到端的具身基础模型，通过大规模多模态预训练实现具身感知的视觉语言理解、强大的语言-动作关联和鲁棒的操控能力，在复杂长时程操作任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在空间和具身理解方面有限，转移到具身领域存在模态、预训练分布和训练目标的不匹配问题，动作理解和生成成为AGI路径上的核心瓶颈。

Method: 采用紧密耦合的架构和多策略训练课程，通过统一跨级CoT（思维链）方法，在单一可微分框架内无缝统一指令推理、子目标分解和细粒度动作合成。

Result: WALL-OSS在复杂长时程操作任务上取得高成功率，展现出强大的指令跟随能力、复杂理解和推理能力，并优于强基线模型。

Conclusion: 该模型为从视觉语言模型到具身基础模型提供了可靠且可扩展的路径，解决了具身智能中的核心挑战。

Abstract: While foundation models show remarkable progress in language and vision,
existing vision-language models (VLMs) still have limited spatial and
embodiment understanding. Transferring VLMs to embodied domains reveals
fundamental mismatches between modalities, pretraining distributions, and
training objectives, leaving action comprehension and generation as a central
bottleneck on the path to AGI.
  We introduce WALL-OSS, an end-to-end embodied foundation model that leverages
large-scale multimodal pretraining to achieve (1) embodiment-aware
vision-language understanding, (2) strong language-action association, and (3)
robust manipulation capability.
  Our approach employs a tightly coupled architecture and multi-strategies
training curriculum that enables Unified Cross-Level CoT-seamlessly unifying
instruction reasoning, subgoal decomposition, and fine-grained action synthesis
within a single differentiable framework.
  Our results show that WALL-OSS attains high success on complex long-horizon
manipulations, demonstrates strong instruction-following capabilities, complex
understanding and reasoning, and outperforms strong baselines, thereby
providing a reliable and scalable path from VLMs to embodied foundation models.

</details>


### [44] [Augmented Reality-Enhanced Robot Teleoperation for Collecting User Demonstrations](https://arxiv.org/abs/2509.11783)
*Shiqi Gong,Sebastian Zudaire,Chi Zhang,Zhen Li*

Main category: cs.RO

TL;DR: 基于增强现实的机器人远程操作系统，通过空间点云渲染实现直观的无接触示范收集，显著提升任务性能和用户体验


<details>
  <summary>Details</summary>
Motivation: 解决传统工业机器人编程复杂耗时的问题，小化程序编写中的直观控制和示范收集接口挑战

Method: 开发AR增强的机器人远程操作系统，集成AR基于控制与空间点云渲染技术，支持无需进入工作区或使用传统教学垂的远程操作

Result: 实时环境感知显著提高任务完成准确性和效率28%，用户体验提升12%（SUS评分），在ABB IRB 1200和GoFa 5机器人上验证成功

Conclusion: 该系统推进了直观机器人远程操作技术，为工业环境中的示范收集提供了安全有效的解决方案，收集的示范数据可作为机器学习应用的价值资源

Abstract: Traditional industrial robot programming is often complex and time-consuming,
typically requiring weeks or even months of effort from expert programmers.
Although Programming by Demonstration (PbD) offers a more accessible
alternative, intuitive interfaces for robot control and demonstration
collection remain challenging. To address this, we propose an Augmented Reality
(AR)-enhanced robot teleoperation system that integrates AR-based control with
spatial point cloud rendering, enabling intuitive, contact-free demonstrations.
This approach allows operators to control robots remotely without entering the
workspace or using conventional tools like the teach pendant. The proposed
system is generally applicable and has been demonstrated on ABB robot
platforms, specifically validated with the IRB 1200 industrial robot and the
GoFa 5 collaborative robot. A user study evaluates the impact of real-time
environmental perception, specifically with and without point cloud rendering,
on task completion accuracy, efficiency, and user confidence. Results indicate
that enhanced perception significantly improves task performance by 28% and
enhances user experience, as reflected by a 12% increase in the System
Usability Scale (SUS) score. This work contributes to the advancement of
intuitive robot teleoperation, AR interface design, environmental perception,
and teleoperation safety mechanisms in industrial settings for demonstration
collection. The collected demonstrations may serve as valuable training data
for machine learning applications.

</details>


### [45] [Synthetic vs. Real Training Data for Visual Navigation](https://arxiv.org/abs/2509.11791)
*Lauri Suomela,Sasanka Kuruppu Arachchige,German F. Torres,Harry Edelman,Joni-Kristian Kämäräinen*

Main category: cs.RO

TL;DR: 研究表明，在模拟环境中训练的视觉导航策略通过利用预训练视觉表示和实时运行架构，能够超越真实世界训练策略31%的性能，并比现有最佳方法提高50%的导航成功率。


<details>
  <summary>Details</summary>
Motivation: 解决模拟训练策略在真实世界中性能显著下降的sim-to-real差距问题，证明模拟训练策略可以达到甚至超越真实世界训练策略的性能。

Method: 采用能够桥接sim-to-real外观差距的导航策略架构，利用预训练视觉表示，并在机器人硬件上实时运行。在轮式移动机器人和无人机上进行部署验证。

Result: 模拟训练的策略比真实世界训练版本性能高出31%，比现有最先进方法提高50%的导航成功率。同一模型在无人机上也表现出良好的泛化能力。

Conclusion: 多样化的图像编码器预训练对sim-to-real泛化至关重要，模拟训练的on-policy学习相比真实数据训练具有关键优势。

Abstract: This paper investigates how the performance of visual navigation policies
trained in simulation compares to policies trained with real-world data.
Performance degradation of simulator-trained policies is often significant when
they are evaluated in the real world. However, despite this well-known
sim-to-real gap, we demonstrate that simulator-trained policies can match the
performance of their real-world-trained counterparts.
  Central to our approach is a navigation policy architecture that bridges the
sim-to-real appearance gap by leveraging pretrained visual representations and
runs real-time on robot hardware. Evaluations on a wheeled mobile robot show
that the proposed policy, when trained in simulation, outperforms its
real-world-trained version by 31% and the prior state-of-the-art methods by 50%
in navigation success rate. Policy generalization is verified by deploying the
same model onboard a drone.
  Our results highlight the importance of diverse image encoder pretraining for
sim-to-real generalization, and identify on-policy learning as a key advantage
of simulated training over training with real data.

</details>


### [46] [UniPilot: Enabling GPS-Denied Autonomy Across Embodiments](https://arxiv.org/abs/2509.11793)
*Mihir Kulkarni,Mihir Dharmadhikari,Nikhil Khedekar,Morten Nissov,Mohit Singh,Philipp Weiss,Kostas Alexis*

Main category: cs.RO

TL;DR: UniPilot是一个紧凑的硬件-软件自主载荷系统，可在GPS拒止环境中为多种机器人提供自主操作能力，集成了多模态感知、路径规划和基于学习的导航策略。


<details>
  <summary>Details</summary>
Motivation: 解决在GPS拒止环境下单一模态感知方法容易失效的问题，为多样化机器人平台提供统一的自主操作解决方案。

Method: 集成LiDAR、雷达、视觉和惯性传感的多模态感知套件，运行完整的自主软件包括多模态感知、探索检查路径规划和基于学习的导航策略。

Result: 在多样化环境和多种机器人平台上进行了大量实验，验证了系统在建图、规划和安全导航方面的能力。

Conclusion: UniPilot提供了一个可在广泛平台上部署的单一单元，具备鲁棒的定位、建图、规划以及安全控制能力。

Abstract: This paper presents UniPilot, a compact hardware-software autonomy payload
that can be integrated across diverse robot embodiments to enable autonomous
operation in GPS-denied environments. The system integrates a multi-modal
sensing suite including LiDAR, radar, vision, and inertial sensing for robust
operation in conditions where uni-modal approaches may fail. UniPilot runs a
complete autonomy software comprising multi-modal perception, exploration and
inspection path planning, and learning-based navigation policies. The payload
provides robust localization, mapping, planning, and safety and control
capabilities in a single unit that can be deployed across a wide range of
platforms. A large number of experiments are conducted across diverse
environments and on a variety of robot platforms to validate the mapping,
planning, and safe navigation capabilities enabled by the payload.

</details>


### [47] [TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning](https://arxiv.org/abs/2509.11839)
*Jiacheng Liu,Pengxiang Ding,Qihang Zhou,Yuxuan Wu,Da Huang,Zimian Peng,Wei Xiao,Weinan Zhang,Lixin Yang,Cewu Lu,Donglin Wang*

Main category: cs.RO

TL;DR: 提出了KORR框架，通过Koopman算子理论在潜在空间中建立线性时不变结构，为残差策略学习提供全局动态建模指导，显著提升了长时程精细控制任务的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习在长时程任务和高精度控制中存在误差累积问题，现有残差策略方法主要关注局部修正，缺乏对状态演化的全局理解，限制了在未见场景中的鲁棒性和泛化能力。

Method: 提出KORR框架，利用Koopman算子理论在学习的潜在空间中施加线性时不变结构，使残差修正基于Koopman预测的潜在状态，实现全局信息指导的稳定动作精炼。

Result: 在长时程精细机器人家具装配任务的各种扰动下进行评估，结果显示相比强基线方法在性能、鲁棒性和泛化能力方面都有持续提升。

Conclusion: 研究结果凸显了基于Koopman的建模在连接现代学习方法与经典控制理论方面的潜力，为残差策略学习提供了有效的全局动态指导框架。

Abstract: Imitation learning (IL) enables efficient skill acquisition from
demonstrations but often struggles with long-horizon tasks and high-precision
control due to compounding errors. Residual policy learning offers a promising,
model-agnostic solution by refining a base policy through closed-loop
corrections. However, existing approaches primarily focus on local corrections
to the base policy, lacking a global understanding of state evolution, which
limits robustness and generalization to unseen scenarios. To address this, we
propose incorporating global dynamics modeling to guide residual policy
updates. Specifically, we leverage Koopman operator theory to impose linear
time-invariant structure in a learned latent space, enabling reliable state
transitions and improved extrapolation for long-horizon prediction and unseen
environments. We introduce KORR (Koopman-guided Online Residual Refinement), a
simple yet effective framework that conditions residual corrections on
Koopman-predicted latent states, enabling globally informed and stable action
refinement. We evaluate KORR on long-horizon, fine-grained robotic furniture
assembly tasks under various perturbations. Results demonstrate consistent
gains in performance, robustness, and generalization over strong baselines. Our
findings further highlight the potential of Koopman-based modeling to bridge
modern learning methods with classical control theory. For more details, please
refer to https://jiachengliu3.github.io/TrajBooster.

</details>


### [48] [Tenma: Robust Cross-Embodiment Robot Manipulation with Diffusion Transformer](https://arxiv.org/abs/2509.11865)
*Travis Davies,Yiqi Huang,Yunxin Liu,Xiang Chen,Huxian Liu,Luhui Hu*

Main category: cs.RO

TL;DR: Tenma是一个轻量级扩散-Transformer策略，用于双臂控制，通过跨具身标准化器、联合状态-时间编码器和扩散动作解码器，在异构多模态机器人数据上实现了88.95%的平均成功率，显著超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer策略和扩散模型的扩展已经推动了机器人操作的发展，但在轻量级、跨具身学习设置中结合这些技术仍然具有挑战性。研究旨在探索在异构多模态机器人数据上训练扩散-Transformer策略时，最影响稳定性和性能的设计选择。

Method: 提出Tenma框架：1）跨具身标准化器将不同的状态/动作空间映射到共享潜在空间；2）联合状态-时间编码器实现时间对齐的观察学习并提升推理速度；3）扩散动作解码器针对训练稳定性和学习能力进行优化。整合多视角RGB、本体感知和语言信息。

Result: 在匹配计算资源下，Tenma在分布内测试中达到88.95%的平均成功率，在物体和场景变化下仍保持强劲性能，显著超越基线方法（最佳分布内平均仅为18.12%）。

Conclusion: 尽管使用中等规模数据，Tenma提供了鲁棒的操作和泛化能力，表明多模态和跨具身学习策略在进一步增强基于Transformer的模仿学习策略能力方面具有巨大潜力。

Abstract: Scaling Transformer policies and diffusion models has advanced robotic
manipulation, yet combining these techniques in lightweight, cross-embodiment
learning settings remains challenging. We study design choices that most affect
stability and performance for diffusion-transformer policies trained on
heterogeneous, multimodal robot data, and introduce Tenma, a lightweight
diffusion-transformer for bi-manual arm control. Tenma integrates multiview
RGB, proprioception, and language via a cross-embodiment normalizer that maps
disparate state/action spaces into a shared latent space; a Joint State-Time
encoder for temporally aligned observation learning with inference speed
boosts; and a diffusion action decoder optimized for training stability and
learning capacity. Across benchmarks and under matched compute, Tenma achieves
an average success rate of 88.95% in-distribution and maintains strong
performance under object and scene shifts, substantially exceeding baseline
policies whose best in-distribution average is 18.12%. Despite using moderate
data scale, Tenma delivers robust manipulation and generalization, indicating
the great potential for multimodal and cross-embodiment learning strategies for
further augmenting the capacity of transformer-based imitation learning
policies.

</details>


### [49] [VH-Diffuser: Variable Horizon Diffusion Planner for Time-Aware Goal-Conditioned Trajectory Planning](https://arxiv.org/abs/2509.11930)
*Ruijia Liu,Ancheng Hou,Shaoyuan Li,Xiang Yin*

Main category: cs.RO

TL;DR: 提出了Variable Horizon Diffuser (VHD)框架，将规划时域作为学习变量而非固定超参数，通过长度预测器预测实例特定时域，提升扩散规划器在不同几何和动力学难度任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散规划器依赖固定的预定义时域，导致长度不匹配问题（轨迹过短或过长），在不同实例间表现脆弱，无法适应变化的几何或动力学难度。

Method: 使用学习的长度预测器模型预测实例特定时域，通过初始噪声整形和随机裁剪子轨迹训练来控制轨迹长度，无需架构修改即可与现有扩散规划器兼容。

Result: 在迷宫导航和机械臂控制基准测试中提高了成功率和路径效率，对时域不匹配和未见长度表现出更强鲁棒性，同时保持训练简单且仅需离线训练。

Conclusion: VHD框架通过将时域作为学习变量，有效解决了固定时域规划器的长度不匹配问题，提升了扩散规划器在不同任务中的适应性和性能。

Abstract: Diffusion-based planners have gained significant recent attention for their
robustness and performance in long-horizon tasks. However, most existing
planners rely on a fixed, pre-specified horizon during both training and
inference. This rigidity often produces length-mismatch (trajectories that are
too short or too long) and brittle performance across instances with varying
geometric or dynamical difficulty. In this paper, we introduce the Variable
Horizon Diffuser (VHD) framework, which treats the horizon as a learned
variable rather than a fixed hyperparameter. Given a start-goal pair, we first
predict an instance-specific horizon using a learned Length Predictor model,
which guides a Diffusion Planner to generate a trajectory of the desired
length. Our design maintains compatibility with existing diffusion planners by
controlling trajectory length through initial noise shaping and training on
randomly cropped sub-trajectories, without requiring architectural changes.
Empirically, VHD improves success rates and path efficiency in maze-navigation
and robot-arm control benchmarks, showing greater robustness to horizon
mismatch and unseen lengths, while keeping training simple and offline-only.

</details>


### [50] [E2-BKI: Evidential Ellipsoidal Bayesian Kernel Inference for Uncertainty-aware Gaussian Semantic Mapping](https://arxiv.org/abs/2509.11964)
*Junyoung Kim,Minsik Jeon,Jihong Min,Kiho Kwak,Junwon Seo*

Main category: cs.RO

TL;DR: 提出了一种不确定性感知的语义建图框架，通过证据深度学习估计语义预测不确定性，并整合到贝叶斯核推理中，在复杂户外环境中实现鲁棒的语义建图。


<details>
  <summary>Details</summary>
Motivation: 现有语义建图方法在挑战性户外环境中面临多种不确定性来源，严重影响建图性能，需要处理这些不确定性来提升建图质量。

Method: 使用证据深度学习估计语义预测不确定性，将其整合到贝叶斯核推理中；将噪声观测聚合成高斯表示以减轻不可靠点的影响；采用几何对齐核适应复杂场景结构。

Result: 在多种越野和城市户外环境中的综合评估显示，在建图质量、不确定性校准、表示灵活性和鲁棒性方面均有一致改进，同时保持实时效率。

Conclusion: 该方法通过有效处理多种不确定性来源，实现了在复杂户外场景中鲁棒、不确定性感知的语义建图，显著提升了建图性能。

Abstract: Semantic mapping aims to construct a 3D semantic representation of the
environment, providing essential knowledge for robots operating in complex
outdoor settings. While Bayesian Kernel Inference (BKI) addresses
discontinuities of map inference from sparse sensor data, existing semantic
mapping methods suffer from various sources of uncertainties in challenging
outdoor environments. To address these issues, we propose an uncertainty-aware
semantic mapping framework that handles multiple sources of uncertainties,
which significantly degrade mapping performance. Our method estimates
uncertainties in semantic predictions using Evidential Deep Learning and
incorporates them into BKI for robust semantic inference. It further aggregates
noisy observations into coherent Gaussian representations to mitigate the
impact of unreliable points, while employing geometry-aligned kernels that
adapt to complex scene structures. These Gaussian primitives effectively fuse
local geometric and semantic information, enabling robust, uncertainty-aware
mapping in complex outdoor scenarios. Comprehensive evaluation across diverse
off-road and urban outdoor environments demonstrates consistent improvements in
mapping quality, uncertainty calibration, representational flexibility, and
robustness, while maintaining real-time efficiency.

</details>


### [51] [Time-Constrained Intelligent Adversaries for Automation Vulnerability Testing: A Multi-Robot Patrol Case Study](https://arxiv.org/abs/2509.11971)
*James C. Ward,Alex Bott,Connor York,Edmund R. Hunt*

Main category: cs.RO

TL;DR: 提出基于机器学习的对抗模型来评估多机器人巡逻系统的安全性，该模型通过观察巡逻行为来尝试在限定时间内无检测地进入安全区域，相比现有基线表现更优。


<details>
  <summary>Details</summary>
Motivation: 通过模拟物理自主系统的敌对攻击来检验其抗攻击鲁棒性，为漏洞感知设计提供信息，特别是在多机器人巡逻场景中评估巡逻系统对抗现实潜在对手的能力。

Method: 开发机器学习驱动的对抗模型，该模型观察机器人巡逻行为，在有限时间窗口内尝试无检测地渗透安全环境，用于评估巡逻策略的有效性。

Result: 新模型在性能上超越了现有基线方法，提供了更严格的测试标准，并在多个领先的分散式多机器人巡逻策略上进行了验证。

Conclusion: 该对抗模型为多机器人巡逻系统提供了更真实的安全性评估工具，能够为未来巡逻策略设计提供有价值的见解，有助于提高系统的抗攻击能力。

Abstract: Simulating hostile attacks of physical autonomous systems can be a useful
tool to examine their robustness to attack and inform vulnerability-aware
design. In this work, we examine this through the lens of multi-robot patrol,
by presenting a machine learning-based adversary model that observes robot
patrol behavior in order to attempt to gain undetected access to a secure
environment within a limited time duration. Such a model allows for evaluation
of a patrol system against a realistic potential adversary, offering insight
into future patrol strategy design. We show that our new model outperforms
existing baselines, thus providing a more stringent test, and examine its
performance against multiple leading decentralized multi-robot patrol
strategies.

</details>


### [52] [Gesture-Based Robot Control Integrating Mm-wave Radar and Behavior Trees](https://arxiv.org/abs/2509.12008)
*Yuqing Song,Cesare Tonola,Stefano Savazzi,Sanaz Kianoush,Nicola Pedrocchi,Stephan Sigg*

Main category: cs.RO

TL;DR: 基于mm波激光雷达的手势识别系统，用于无接触控制机器人手臂，支持9种手势实时命令映射


<details>
  <summary>Details</summary>
Motivation: 解决相机视觉系统存在的隐私泄露、光照条件限制等问题，提供更保护隐私、更稳定的无接触人机交互方式

Method: 采用mm波激光雷达感知技术，建立了一个统一的实时处理流水线，将手势识别与机器人控制相结合

Result: 系统能够精确识别9种不同手势，并实时映射为机器人控制命令，通过案例研究验证了系统的实用性、性能和可靠性

Conclusion: 该方案为人机交互提供了一种更保护隐私、更稳定可靠的无接触控制方式，充分利用了雷达感知在复杂环境下的优势

Abstract: As robots become increasingly prevalent in both homes and industrial
settings, the demand for intuitive and efficient human-machine interaction
continues to rise. Gesture recognition offers an intuitive control method that
does not require physical contact with devices and can be implemented using
various sensing technologies. Wireless solutions are particularly flexible and
minimally invasive. While camera-based vision systems are commonly used, they
often raise privacy concerns and can struggle in complex or poorly lit
environments. In contrast, radar sensing preserves privacy, is robust to
occlusions and lighting, and provides rich spatial data such as distance,
relative velocity, and angle. We present a gesture-controlled robotic arm using
mm-wave radar for reliable, contactless motion recognition. Nine gestures are
recognized and mapped to real-time commands with precision. Case studies are
conducted to demonstrate the system practicality, performance and reliability
for gesture-based robotic manipulation. Unlike prior work that treats gesture
recognition and robotic control separately, our system unifies both into a
real-time pipeline for seamless, contactless human-robot interaction.

</details>


### [53] [Embodied Navigation Foundation Model](https://arxiv.org/abs/2509.12129)
*Jiazhao Zhang,Anqi Li,Yunpeng Qi,Minghan Li,Jiahang Liu,Shaoan Wang,Haoran Liu,Gengze Zhou,Yuze Wu,Xingxing Li,Yuxin Fan,Wenjun Li,Zhibo Chen,Fei Gao,Qi Wu,Zhizheng Zhang,He Wang*

Main category: cs.RO

TL;DR: NavFoM是一个跨具身和跨任务的导航基础模型，使用统一架构处理多模态导航输入，在多个导航任务和具身形式上实现SOTA性能，无需任务特定微调。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉语言模型在通用视觉语言任务上表现出色，但在具身导航中的泛化能力仍局限于狭窄的任务设置和特定具身架构，需要开发更通用的导航基础模型。

Method: 使用800万导航样本训练，涵盖四足机器人、无人机、轮式机器人和车辆等多种具身形式。采用统一架构处理不同相机配置和导航视野，引入标识符令牌嵌入相机视角信息和任务时间上下文，使用动态调整采样策略控制观测令牌。

Result: 在公共基准测试中实现了最先进或极具竞争力的性能，无需任务特定微调。真实世界实验进一步证实了强大的泛化能力和实际应用性。

Conclusion: NavFoM展示了跨具身和跨任务的强大导航能力，为实际部署提供了有效的解决方案，证明了统一导航基础模型的可行性和优越性。

Abstract: Navigation is a fundamental capability in embodied AI, representing the
intelligence required to perceive and interact within physical environments
following language instructions. Despite significant progress in large
Vision-Language Models (VLMs), which exhibit remarkable zero-shot performance
on general vision-language tasks, their generalization ability in embodied
navigation remains largely confined to narrow task settings and
embodiment-specific architectures. In this work, we introduce a
cross-embodiment and cross-task Navigation Foundation Model (NavFoM), trained
on eight million navigation samples that encompass quadrupeds, drones, wheeled
robots, and vehicles, and spanning diverse tasks such as vision-and-language
navigation, object searching, target tracking, and autonomous driving. NavFoM
employs a unified architecture that processes multimodal navigation inputs from
varying camera configurations and navigation horizons. To accommodate diverse
camera setups and temporal horizons, NavFoM incorporates identifier tokens that
embed camera view information of embodiments and the temporal context of tasks.
Furthermore, to meet the demands of real-world deployment, NavFoM controls all
observation tokens using a dynamically adjusted sampling strategy under a
limited token length budget. Extensive evaluations on public benchmarks
demonstrate that our model achieves state-of-the-art or highly competitive
performance across multiple navigation tasks and embodiments without requiring
task-specific fine-tuning. Additional real-world experiments further confirm
the strong generalization capability and practical applicability of our
approach.

</details>


### [54] [Learning Contact Dynamics for Control with Action-conditioned Face Interaction Graph Networks](https://arxiv.org/abs/2509.12151)
*Zongyao Yi,Joachim Hertzberg,Martin Atzmueller*

Main category: cs.RO

TL;DR: 基于GNN的可学习物理模拟器，通过新的节点和边类型扩展，在探索性操作任务中提供准确的运动和力矩预测


<details>
  <summary>Details</summary>
Motivation: 需要一种能够准确预测机器人末端执行器在接触丰富操作中的运动和力矩的物理模拟器，以支持控制和状态估计任务

Method: 扩展现有的FIGNet GNN模型，引入新的节点和边类型，支持动作条件预测，并使用MPC控制器进行验证

Result: 在模拟中，使用该模型的MPC控制器在高难度栏塞插入任务中达到了与真实动力学模型相同的性能；在实际实验中，运动预测准确度提升50%，力矩预测精度提高3倍

Conclusion: 该可学习物理模拟器能够在接触丰富的操作任务中提供准确的动力学预测，为机器人控制和状态估计提供了有效的解决方案

Abstract: We present a learnable physics simulator that provides accurate motion and
force-torque prediction of robot end effectors in contact-rich manipulation.
The proposed model extends the state-of-the-art GNN-based simulator (FIGNet)
with novel node and edge types, enabling action-conditional predictions for
control and state estimation tasks. In simulation, the MPC agent using our
model matches the performance of the same controller with the ground truth
dynamics model in a challenging peg-in-hole task, while in the real-world
experiment, our model achieves a 50% improvement in motion prediction accuracy
and 3$\times$ increase in force-torque prediction precision over the baseline
physics simulator. Source code and data are publicly available.

</details>
