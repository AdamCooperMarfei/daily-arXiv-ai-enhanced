<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 22]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [MimicDroid: In-Context Learning for Humanoid Robot Manipulation from Human Play Videos](https://arxiv.org/abs/2509.09769)
*Rutav Shah,Shuijing Liu,Qi Wang,Zhenyu Jiang,Sateesh Kumar,Mingyo Seo,Roberto Martín-Martín,Yuke Zhu*

Main category: cs.RO

TL;DR: MimicDroid利用人类玩耍视频作为训练数据，通过轨迹配对和动作预测使仿人机器人具备上下文学习能力，在测试时能快速适应新物体和环境，实现了比现有方法更高的成功率。


<details>
  <summary>Details</summary>
Motivation: 解决当前上下文学习方法依赖人工遥操作数据的可扩展性问题，利用人类玩耍视频作为更丰富、可扩展的训练数据源，使仿人机器人能够从少量视频示例中学习新操作任务。

Method: 1) 从人类玩耍视频中提取相似操作行为的轨迹对；2) 训练策略以预测一个轨迹的动作条件于另一个轨迹；3) 通过运动学相似性将人类手腕姿态重定向到仿人机器人；4) 使用随机补丁掩码减少对人类特定线索的过拟合并提高视觉差异的鲁棒性。

Result: MimicDroid在开源仿真基准测试中优于最先进方法，在现实世界中实现了接近两倍的成功率提升。

Conclusion: 人类玩耍视频可以作为有效的训练数据源，使仿人机器人获得上下文学习能力，实现从少量示例中快速适应新任务，为解决机器人操作任务提供了可扩展的解决方案。

Abstract: We aim to enable humanoid robots to efficiently solve new manipulation tasks
from a few video examples. In-context learning (ICL) is a promising framework
for achieving this goal due to its test-time data efficiency and rapid
adaptability. However, current ICL methods rely on labor-intensive teleoperated
data for training, which restricts scalability. We propose using human play
videos -- continuous, unlabeled videos of people interacting freely with their
environment -- as a scalable and diverse training data source. We introduce
MimicDroid, which enables humanoids to perform ICL using human play videos as
the only training data. MimicDroid extracts trajectory pairs with similar
manipulation behaviors and trains the policy to predict the actions of one
trajectory conditioned on the other. Through this process, the model acquired
ICL capabilities for adapting to novel objects and environments at test time.
To bridge the embodiment gap, MimicDroid first retargets human wrist poses
estimated from RGB videos to the humanoid, leveraging kinematic similarity. It
also applies random patch masking during training to reduce overfitting to
human-specific cues and improve robustness to visual differences. To evaluate
few-shot learning for humanoids, we introduce an open-source simulation
benchmark with increasing levels of generalization difficulty. MimicDroid
outperformed state-of-the-art methods and achieved nearly twofold higher
success rates in the real world. Additional materials can be found on:
ut-austin-rpl.github.io/MimicDroid

</details>


### [2] [MIMo grows! Simulating body and sensory development in a multimodal infant model](https://arxiv.org/abs/2509.09805)
*Francisco M. López,Miles Lenz,Marco G. Fedozzi,Arthur Aubret,Jochen Triesch*

Main category: cs.RO

TL;DR: MIMo v2是一个多模态婴儿模型，通过模拟从出生到24个月的身体生长、运动能力发展、视觉发育和感觉运动延迟，提高了发育机器人建模的真实性。


<details>
  <summary>Details</summary>
Motivation: 现有的发育机器人和仿真平台通常只针对特定年龄段设计，无法捕捉婴儿发育过程中不断变化的能力和限制。

Method: 开发了MIMo v2模型，包含：生长的身体和增强的驱动强度、具有发展性视觉敏锐度的中央凹视觉、模拟信号传输延迟的感觉运动延迟模块、逆向运动学模块、随机环境生成器，并更新了与第三方仿真和学习库的兼容性。

Result: 新版本的MIMo能够更真实地模拟各种感觉运动发育的各个方面，覆盖从出生到24个月的年龄范围。

Conclusion: MIMo v2为发育机器人研究提供了更真实的仿真平台，代码已在官方仓库开源。

Abstract: Infancy is characterized by rapid body growth and an explosive change of
sensory and motor abilities. However, developmental robots and simulation
platforms are typically designed in the image of a specific age, which limits
their ability to capture the changing abilities and constraints of developing
infants. To address this issue, we present MIMo v2, a new version of the
multimodal infant model. It includes a growing body with increasing actuation
strength covering the age range from birth to 24 months. It also features
foveated vision with developing visual acuity as well as sensorimotor delays
modeling finite signal transmission speeds to and from an infant's brain.
Further enhancements of this MIMo version include an inverse kinematics module,
a random environment generator and updated compatiblity with third-party
simulation and learning libraries. Overall, this new MIMo version permits
increased realism when modeling various aspects of sensorimotor development.
The code is available on the official repository
(https://github.com/trieschlab/MIMo).

</details>


### [3] [Using the Pepper Robot to Support Sign Language Communication](https://arxiv.org/abs/2509.09889)
*Giulia Botta,Marco Botta,Cristina Gena,Alessandro Mazzei,Massimo Donini,Alberto Lillo*

Main category: cs.RO

TL;DR: 研究表明商用社交机器人Pepper能够生成可理解的意大利手语(LIS)手势，但完整句子识别率较低，需要多模态增强和参与式设计来提升机器人表达能力。


<details>
  <summary>Details</summary>
Motivation: 社交机器人在公共和辅助环境中应用日益增多，但针对聋人用户的可访问性研究不足。意大利手语(LIS)作为成熟自然语言，具有复杂的手势和非手势成分，让机器人使用LIS交流可以促进更包容的人机交互。

Method: 与聋人学生和LIS专家合作，通过手动动画技术和MATLAB逆向运动学求解器在Pepper机器人上实现了52个LIS手势。对12名LIS熟练者(包括聋人和听力正常者)进行探索性用户研究，包含15个视频选择题和2个开放式问题。

Result: 大多数孤立手势被正确识别，但由于机器人关节限制和时间约束，完整句子识别率显著较低。

Conclusion: 即使是商用社交机器人如Pepper也能执行部分可理解的LIS手势，为更包容的交互设计提供机会。未来需要多模态增强(如屏幕支持或表情化头像)并让聋人用户参与设计以改进机器人表达能力和可用性。

Abstract: Social robots are increasingly experimented in public and assistive settings,
but their accessibility for Deaf users remains quite underexplored. Italian
Sign Language (LIS) is a fully-fledged natural language that relies on complex
manual and non-manual components. Enabling robots to communicate using LIS
could foster more inclusive human robot interaction, especially in social
environments such as hospitals, airports, or educational settings. This study
investigates whether a commercial social robot, Pepper, can produce
intelligible LIS signs and short signed LIS sentences. With the help of a Deaf
student and his interpreter, an expert in LIS, we co-designed and implemented
52 LIS signs on Pepper using either manual animation techniques or a MATLAB
based inverse kinematics solver. We conducted a exploratory user study
involving 12 participants proficient in LIS, both Deaf and hearing.
Participants completed a questionnaire featuring 15 single-choice video-based
sign recognition tasks and 2 open-ended questions on short signed sentences.
Results shows that the majority of isolated signs were recognized correctly,
although full sentence recognition was significantly lower due to Pepper's
limited articulation and temporal constraints. Our findings demonstrate that
even commercially available social robots like Pepper can perform a subset of
LIS signs intelligibly, offering some opportunities for a more inclusive
interaction design. Future developments should address multi-modal enhancements
(e.g., screen-based support or expressive avatars) and involve Deaf users in
participatory design to refine robot expressivity and usability.

</details>


### [4] [Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision](https://arxiv.org/abs/2509.09893)
*Hanbit Oh,Masaki Murooka,Tomohiro Motoda,Ryoichi Nakajo,Yukiyasu Domae*

Main category: cs.RO

TL;DR: SART框架通过单次人类演示和自主轨迹增强，实现高效安全的模仿学习，显著减少人工干预和碰撞风险


<details>
  <summary>Details</summary>
Motivation: 标准模仿学习方法需要大量演示数据或随机探索，存在安全性差、碰撞频繁、需要人工重置环境等问题，特别是在空间受限任务中

Method: 两阶段框架：1)单次人类演示并标注精度边界球体；2)机器人在边界内自主生成多样化的无碰撞轨迹并与原始演示重新连接

Result: 在仿真和真实机器人操作任务中，SART相比仅使用人类演示训练的策略获得了显著更高的成功率

Conclusion: SART通过最小化人工干预同时确保安全性，有效提高了数据收集效率，为模仿学习提供了更实用的解决方案

Abstract: Imitation learning is a promising paradigm for training robot agents;
however, standard approaches typically require substantial data acquisition --
via numerous demonstrations or random exploration -- to ensure reliable
performance. Although exploration reduces human effort, it lacks safety
guarantees and often results in frequent collisions -- particularly in
clearance-limited tasks (e.g., peg-in-hole) -- thereby, necessitating manual
environmental resets and imposing additional human burden. This study proposes
Self-Augmented Robot Trajectory (SART), a framework that enables policy
learning from a single human demonstration, while safely expanding the dataset
through autonomous augmentation. SART consists of two stages: (1) human
teaching only once, where a single demonstration is provided and precision
boundaries -- represented as spheres around key waypoints -- are annotated,
followed by one environment reset; (2) robot self-augmentation, where the robot
generates diverse, collision-free trajectories within these boundaries and
reconnects to the original demonstration. This design improves the data
collection efficiency by minimizing human effort while ensuring safety.
Extensive evaluations in simulation and real-world manipulation tasks show that
SART achieves substantially higher success rates than policies trained solely
on human-collected demonstrations. Video results available at
https://sites.google.com/view/sart-il .

</details>


### [5] [Detection of Anomalous Behavior in Robot Systems Based on Machine Learning](https://arxiv.org/abs/2509.09953)
*Mahfuzul I. Nissan,Sharmin Aktar*

Main category: cs.RO

TL;DR: 基于机器学习的机器人系统日志异常检测方法，在两种不同机器人场景中比较了逻辑回归、支持向量机和自编码器的性能，发现最优模型选择取决于具体应用场景。


<details>
  <summary>Details</summary>
Motivation: 确保机器人系统的安全可靠运行至关重要，尽管有严格的设计和工程实践，系统仍可能出现故障导致安全风险，因此需要有效的异常检测方法来增强安全性。

Method: 使用CoppeliaSim收集两种不同场景（四旋翼无人机和Pioneer机器人）的系统日志，并比较评估了逻辑回归、支持向量机和自编码器三种机器学习模型的异常检测性能。

Result: 在四旋翼无人机场景中逻辑回归表现最佳，而在Pioneer机器人场景中自编码器最为有效，表明最优模型选择具有场景依赖性，可能由于不同机器人平台的异常复杂度不同。

Conclusion: 研究强调了比较方法的价值，并证明了自编码器在检测机器人系统中复杂异常方面的特殊优势，模型选择应基于具体应用场景的需求。

Abstract: Ensuring the safe and reliable operation of robotic systems is paramount to
prevent potential disasters and safeguard human well-being. Despite rigorous
design and engineering practices, these systems can still experience
malfunctions, leading to safety risks. In this study, we present a machine
learning-based approach for detecting anomalies in system logs to enhance the
safety and reliability of robotic systems. We collected logs from two distinct
scenarios using CoppeliaSim and comparatively evaluated several machine
learning models, including Logistic Regression (LR), Support Vector Machine
(SVM), and an Autoencoder. Our system was evaluated in a quadcopter context
(Context 1) and a Pioneer robot context (Context 2). Results showed that while
LR demonstrated superior performance in Context 1, the Autoencoder model proved
to be the most effective in Context 2. This highlights that the optimal model
choice is context-dependent, likely due to the varying complexity of anomalies
across different robotic platforms. This research underscores the value of a
comparative approach and demonstrates the particular strengths of autoencoders
for detecting complex anomalies in robotic systems.

</details>


### [6] [Gaussian path model library for intuitive robot motion programming by demonstration](https://arxiv.org/abs/2509.10007)
*Samuli Soutukorva,Markku Suomalainen,Martin Kollingbaum,Tapio Heikkilä*

Main category: cs.RO

TL;DR: 提出了基于高斯路径模型的系统，可从教学数据生成路径形状模型，并用于分类人类演示路径，实现直观的机器人运动编程


<details>
  <summary>Details</summary>
Motivation: 通过建立多种形状的高斯路径模型库，使人类演示能够用于直观的机器人运动编程，提高编程效率和自然性

Method: 从教学数据生成高斯路径模型，建立多形状模型库，通过几何分析修改现有模型，并用于分类人类演示路径

Result: 开发了完整的系统，能够生成、分类和修改高斯路径模型，为机器人运动编程提供直观的演示学习方法

Conclusion: 高斯路径模型系统有效支持基于人类演示的机器人运动编程，通过模型库和几何分析方法实现了直观的路径生成和修改

Abstract: This paper presents a system for generating Gaussian path models from
teaching data representing the path shape. In addition, methods for using these
path models to classify human demonstrations of paths are introduced. By
generating a library of multiple Gaussian path models of various shapes, human
demonstrations can be used for intuitive robot motion programming. A method for
modifying existing Gaussian path models by demonstration through geometric
analysis is also presented.

</details>


### [7] [Towards simulation-based optimization of compliant fingers for high-speed connector assembly](https://arxiv.org/abs/2509.10012)
*Richard Matthias Hartisch,Alexander Rother,Jörg Krüger,Kevin Haninger*

Main category: cs.RO

TL;DR: 这篇论文提出了一种基于模拟的设计工具，用于优化软体机器人的结构程度参数，以提高插入任务的容对性和稳健性。


<details>
  <summary>Details</summary>
Motivation: 软体机器人结构的设计参数选择影响操作性能和稳健性，但目前的设计方法需要大量硬件迭代或使用简化模型，无法处理复杂的操作任务目标。

Method: 利用动态模拟技术（包括接触和摩擦模型）开发了一种模拟基础的设计工具，用于优化结构程度手指的设计参数，以提高插入任务的成功率。

Result: 优化后的参数可以将容对范围提高2.29倍，能够补偿工件辅度8.6mm的变化。但趋势具有任务特异性：某些任务中最高程度表现最好，而另一些任务中则相反。

Conclusion: 结构程度设计需要考虑具体应用场景的几何和动力学特性，模拟基础的设计工具能够有效地优化软体机器人的性能和稳健性。

Abstract: Mechanical compliance is a key design parameter for dynamic contact-rich
manipulation, affecting task success and safety robustness over contact
geometry variation. Design of soft robotic structures, such as compliant
fingers, requires choosing design parameters which affect geometry and
stiffness, and therefore manipulation performance and robustness. Today, these
parameters are chosen through either hardware iteration, which takes
significant development time, or simplified models (e.g. planar), which can't
address complex manipulation task objectives. Improvements in dynamic
simulation, especially with contact and friction modeling, present a potential
design tool for mechanical compliance. We propose a simulation-based design
tool for compliant mechanisms which allows design with respect to task-level
objectives, such as success rate. This is applied to optimize design parameters
of a structured compliant finger to reduce failure cases inside a tolerance
window in insertion tasks. The improvement in robustness is then validated on a
real robot using tasks from the benchmark NIST task board. The finger stiffness
affects the tolerance window: optimized parameters can increase tolerable
ranges by a factor of 2.29, with workpiece variation up to 8.6 mm being
compensated. However, the trends remain task-specific. In some tasks, the
highest stiffness yields the widest tolerable range, whereas in others the
opposite is observed, motivating need for design tools which can consider
application-specific geometry and dynamics.

</details>


### [8] [Design and Evaluation of Two Spherical Systems for Mobile 3D Mapping](https://arxiv.org/abs/2509.10032)
*Marawan Khalil,Fabian Arzberger,Andreas Nüchter*

Main category: cs.RO

TL;DR: 这篇论文研究了两种球形地图构建系统，发现球形运动的高动态性会导致LIO算法性能恶化，产生全局不一致地图和偏移问题。


<details>
  <summary>Details</summary>
Motivation: 球形机器人在危险或窄窄环境中具有保护壳体和全向移动的优势，适合地图构建应用，需要研究其地图构建性能。

Method: 开发了轻量化非驱动和驱动型两种球形地图系统，配备Livox Mid-360固态销輹光雷达和IMU，在资源受限硬件上运行LIO算法，并与真实地图进行准确性对比。

Result: 结果显示LIO算法在球形高动态运动下性能显著恶化，导致全局不一致的地图和无法恢复的偏移。

Conclusion: 球形机器人的高动态运动特性对当前最优LIO算法构成挑战，需要进一步研究优化方案来提高地图构建的稳定性和准确性。

Abstract: Spherical robots offer unique advantages for mapping applications in
hazardous or confined environments, thanks to their protective shells and
omnidirectional mobility. This work presents two complementary spherical
mapping systems: a lightweight, non-actuated design and an actuated variant
featuring internal pendulum-driven locomotion. Both systems are equipped with a
Livox Mid-360 solid-state LiDAR sensor and run LiDAR-Inertial Odometry (LIO)
algorithms on resource-constrained hardware. We assess the mapping accuracy of
these systems by comparing the resulting 3D point-clouds from the LIO
algorithms to a ground truth map. The results indicate that the performance of
state-of-the-art LIO algorithms deteriorates due to the high dynamic movement
introduced by the spherical locomotion, leading to globally inconsistent maps
and sometimes unrecoverable drift.

</details>


### [9] [TwinTac: A Wide-Range, Highly Sensitive Tactile Sensor with Real-to-Sim Digital Twin Sensor Model](https://arxiv.org/abs/2509.10063)
*Xiyan Huang,Zhe Xu,Chenxi Xiao*

Main category: cs.RO

TL;DR: TwinTac系统结合物理触觉传感器与其数字孪生模型，通过真实到仿真的方法解决触觉感知在机器人技能学习中的仿真缺失问题。


<details>
  <summary>Details</summary>
Motivation: 机器人技能学习中依赖仿真生成交互数据，但触觉传感器的仿真模型缺失阻碍了触觉感知在策略学习中的应用，限制了基于触觉感知的有效策略开发。

Method: 设计高灵敏度和宽测量范围的硬件传感器，采用真实到仿真方法开发数字孪生模型：收集同步跨域数据（有限元法结果和物理传感器输出），训练神经网络将仿真数据映射到真实传感器响应。

Result: 实验评估表征了物理传感器的灵敏度，证明了数字孪生在复制物理传感器输出方面的一致性。通过物体分类任务显示，数字孪生传感器生成的仿真数据能有效增强真实数据，提高分类准确率。

Conclusion: TwinTac有潜力弥合跨域学习任务的差距，为触觉感知驱动的机器人技能学习提供有效的仿真解决方案。

Abstract: Robot skill acquisition processes driven by reinforcement learning often rely
on simulations to efficiently generate large-scale interaction data. However,
the absence of simulation models for tactile sensors has hindered the use of
tactile sensing in such skill learning processes, limiting the development of
effective policies driven by tactile perception. To bridge this gap, we present
TwinTac, a system that combines the design of a physical tactile sensor with
its digital twin model. Our hardware sensor is designed for high sensitivity
and a wide measurement range, enabling high quality sensing data essential for
object interaction tasks. Building upon the hardware sensor, we develop the
digital twin model using a real-to-sim approach. This involves collecting
synchronized cross-domain data, including finite element method results and the
physical sensor's outputs, and then training neural networks to map simulated
data to real sensor responses. Through experimental evaluation, we
characterized the sensitivity of the physical sensor and demonstrated the
consistency of the digital twin in replicating the physical sensor's output.
Furthermore, by conducting an object classification task, we showed that
simulation data generated by our digital twin sensor can effectively augment
real-world data, leading to improved accuracy. These results highlight
TwinTac's potential to bridge the gap in cross-domain learning tasks.

</details>


### [10] [Prespecified-Performance Kinematic Tracking Control for Aerial Manipulation](https://arxiv.org/abs/2509.10065)
*Hauzi Cao,Jiahao Shen,Zhengzhen Li,Qinquan Ren,Shiyu Zhao*

Main category: cs.RO

TL;DR: 提出了一种新型的空中机械臂运动学跟踪控制框架，通过预设轨迹和二次规划参考分配，确保末端执行器在规定时间内到达目标位置并满足物理约束


<details>
  <summary>Details</summary>
Motivation: 现有运动学跟踪控制方法（如比例微分反馈或跟踪误差反馈策略）无法在规定时间约束内实现跟踪目标，需要解决这一局限性

Method: 采用包含两个关键组件的控制框架：基于用户定义预设轨迹的末端执行器跟踪控制，以及基于二次规划的参考分配方法，同时考虑空中机械臂的物理约束

Result: 通过三个实验验证，结果表明该方法能有效确保末端执行器在预设时间内到达目标位置，且跟踪误差保持在反映任务要求的性能包络内

Conclusion: 所提出的控制方法相比现有技术具有显著优势，能够保证在规定时间内完成跟踪任务，同时避免违反物理限制的解决方案

Abstract: This paper studies the kinematic tracking control problem for aerial
manipulators. Existing kinematic tracking control methods, which typically
employ proportional-derivative feedback or tracking-error-based feedback
strategies, may fail to achieve tracking objectives within specified time
constraints. To address this limitation, we propose a novel control framework
comprising two key components: end-effector tracking control based on a
user-defined preset trajectory and quadratic programming-based reference
allocation. Compared with state-of-the-art approaches, the proposed method has
several attractive features. First, it ensures that the end-effector reaches
the desired position within a preset time while keeping the tracking error
within a performance envelope that reflects task requirements. Second,
quadratic programming is employed to allocate the references of the quadcopter
base and the Delta arm, while considering the physical constraints of the
aerial manipulator, thus preventing solutions that may violate physical
limitations. The proposed approach is validated through three experiments.
Experimental results demonstrate the effectiveness of the proposed algorithm
and its capability to guarantee that the target position is reached within the
preset time.

</details>


### [11] [HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario](https://arxiv.org/abs/2509.10096)
*Saeed Saadatnejad,Reyhaneh Hosseininejad,Jose Barreiros,Katherine M. Tsui,Alexandre Alahi*

Main category: cs.RO

TL;DR: 该论文提出了HHI-Assist数据集和基于条件Transformer的扩散模型，用于预测人机交互场景中的人体运动，以提升辅助机器人的安全性。


<details>
  <summary>Details</summary>
Motivation: 劳动力短缺和人口老龄化加剧了对辅助机器人的需求，但机器人需要准确预测物理交互中的人体运动，这仍然是一个具有挑战性的任务。

Method: 提出了两个关键贡献：(1) HHI-Assist数据集，包含人类在辅助任务中的运动捕捉片段；(2) 基于条件Transformer的去噪扩散模型，用于预测交互代理的姿态。

Result: 模型有效捕捉了护理者和被护理者之间的耦合动态，相比基线方法有所改进，并在未见场景中表现出良好的泛化能力。

Conclusion: 通过推进交互感知的运动预测和引入新数据集，这项工作有潜力显著增强机器人辅助策略，数据集和代码已公开。

Abstract: The increasing labor shortage and aging population underline the need for
assistive robots to support human care recipients. To enable safe and
responsive assistance, robots require accurate human motion prediction in
physical interaction scenarios. However, this remains a challenging task due to
the variability of assistive settings and the complexity of coupled dynamics in
physical interactions. In this work, we address these challenges through two
key contributions: (1) HHI-Assist, a dataset comprising motion capture clips of
human-human interactions in assistive tasks; and (2) a conditional
Transformer-based denoising diffusion model for predicting the poses of
interacting agents. Our model effectively captures the coupled dynamics between
caregivers and care receivers, demonstrating improvements over baselines and
strong generalization to unseen scenarios. By advancing interaction-aware
motion prediction and introducing a new dataset, our work has the potential to
significantly enhance robotic assistance policies. The dataset and code are
available at: https://sites.google.com/view/hhi-assist/home

</details>


### [12] [Efficient Learning-Based Control of a Legged Robot in Lunar Gravity](https://arxiv.org/abs/2509.10128)
*Philip Arm,Oliver Fischer,Joseph Church,Adrian Fuhrer,Hendrik Kolvenbach,Marco Hutter*

Main category: cs.RO

TL;DR: 提出基于强化学习的足式机器人控制方法，通过重力缩放功率优化奖励函数，实现从月球重力到超地球重力的多重力环境高效运动控制，功耗降低23-36%。


<details>
  <summary>Details</summary>
Motivation: 行星探测机器人面临严格的功率和热预算限制，需要能在多种重力环境下高效工作的能量优化控制方法。

Method: 使用强化学习开发重力缩放功率优化的奖励函数，设计恒力弹簧卸载系统进行月球重力环境下的真实实验验证。

Result: 在地球重力下功耗23.4W（比基线提升23%），月球重力下功耗12.2W（比基线降低36%），成功验证了从1.62m/s²到19.62m/s²多种重力环境的控制效果。

Conclusion: 该方法为足式机器人提供了跨多重力级别的功率优化运动控制的可扩展解决方案，适用于行星探测任务。

Abstract: Legged robots are promising candidates for exploring challenging areas on
low-gravity bodies such as the Moon, Mars, or asteroids, thanks to their
advanced mobility on unstructured terrain. However, as planetary robots' power
and thermal budgets are highly restricted, these robots need energy-efficient
control approaches that easily transfer to multiple gravity environments. In
this work, we introduce a reinforcement learning-based control approach for
legged robots with gravity-scaled power-optimized reward functions. We use our
approach to develop and validate a locomotion controller and a base pose
controller in gravity environments from lunar gravity (1.62 m/s2) to a
hypothetical super-Earth (19.62 m/s2). Our approach successfully scales across
these gravity levels for locomotion and base pose control with the
gravity-scaled reward functions. The power-optimized locomotion controller
reached a power consumption for locomotion of 23.4 W in Earth gravity on a
15.65 kg robot at 0.4 m/s, a 23 % improvement over the baseline policy.
Additionally, we designed a constant-force spring offload system that allowed
us to conduct real-world experiments on legged locomotion in lunar gravity. In
lunar gravity, the power-optimized control policy reached 12.2 W, 36 % less
than a baseline controller which is not optimized for power efficiency. Our
method provides a scalable approach to developing power-efficient locomotion
controllers for legged robots across multiple gravity levels.

</details>


### [13] [CaR1: A Multi-Modal Baseline for BEV Vehicle Segmentation via Camera-Radar Fusion](https://arxiv.org/abs/2509.10139)
*Santiago Montiel-Marín,Angel Llamazares,Miguel Antunes-García,Fabio Sánchez-García,Luis M. Bergasa*

Main category: cs.RO

TL;DR: CaR1是一个新颖的相机-雷达融合架构，用于BEV车辆分割，在nuScenes数据集上达到57.6 IoU的竞争性性能


<details>
  <summary>Details</summary>
Motivation: 相机-雷达融合提供了一种比LiDAR更具鲁棒性和成本效益的自动驾驶系统替代方案，相机提供丰富的语义信息但深度不可靠，雷达提供稀疏但可靠的位置和运动信息

Method: 基于BEVFusion构建，采用网格化雷达编码将点云离散化为结构化BEV特征，以及自适应融合机制动态平衡传感器贡献

Result: 在nuScenes数据集上实现了竞争性的分割性能（57.6 IoU），与最先进方法相当

Conclusion: CaR1展示了相机-雷达融合在BEV车辆分割任务中的有效性，代码已公开

Abstract: Camera-radar fusion offers a robust and cost-effective alternative to
LiDAR-based autonomous driving systems by combining complementary sensing
capabilities: cameras provide rich semantic cues but unreliable depth, while
radar delivers sparse yet reliable position and motion information. We
introduce CaR1, a novel camera-radar fusion architecture for BEV vehicle
segmentation. Built upon BEVFusion, our approach incorporates a grid-wise radar
encoding that discretizes point clouds into structured BEV features and an
adaptive fusion mechanism that dynamically balances sensor contributions.
Experiments on nuScenes demonstrate competitive segmentation performance (57.6
IoU), on par with state-of-the-art methods. Code is publicly available
\href{https://www.github.com/santimontiel/car1}{online}.

</details>


### [14] [DiffAero: A GPU-Accelerated Differentiable Simulation Framework for Efficient Quadrotor Policy Learning](https://arxiv.org/abs/2509.10247)
*Xinhong Zhang,Runqing Wang,Yunfan Ren,Jian Sun,Hao Fang,Jie Chen,Gang Wang*

Main category: cs.RO

TL;DR: DiffAero是一个轻量级、GPU加速的完全可微分四旋翼控制策略学习仿真框架，支持环境级和智能体级并行，集成多种动力学模型和传感器，通过GPU原生并行化实现数量级的仿真吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 现有仿真器存在CPU-GPU数据传输瓶颈，无法高效支持可微分和混合学习算法的研究，需要开发一个高性能、完全可微分的仿真平台来加速四旋翼控制策略的学习。

Method: 开发了完全GPU并行的物理和渲染引擎，支持多种动力学模型（IMU、深度相机、LiDAR等传感器），提供统一的GPU原生训练接口，实现环境级和智能体级并行。

Result: DiffAero在消费级硬件上实现了数量级的仿真吞吐量提升，结合混合学习算法能够在数小时内学习到鲁棒的飞行策略，并通过真实飞行实验验证了有效性。

Conclusion: DiffAero不仅是一个高性能仿真器，更是一个研究可微分和混合学习算法的平台，为四旋翼控制策略学习提供了高效的解决方案，代码已开源。

Abstract: This letter introduces DiffAero, a lightweight, GPU-accelerated, and fully
differentiable simulation framework designed for efficient quadrotor control
policy learning. DiffAero supports both environment-level and agent-level
parallelism and integrates multiple dynamics models, customizable sensor stacks
(IMU, depth camera, and LiDAR), and diverse flight tasks within a unified,
GPU-native training interface. By fully parallelizing both physics and
rendering on the GPU, DiffAero eliminates CPU-GPU data transfer bottlenecks and
delivers orders-of-magnitude improvements in simulation throughput. In contrast
to existing simulators, DiffAero not only provides high-performance simulation
but also serves as a research platform for exploring differentiable and hybrid
learning algorithms. Extensive benchmarks and real-world flight experiments
demonstrate that DiffAero and hybrid learning algorithms combined can learn
robust flight policies in hours on consumer-grade hardware. The code is
available at https://github.com/flyingbitac/diffaero.

</details>


### [15] [GundamQ: Multi-Scale Spatio-Temporal Representation Learning for Robust Robot Path Planning](https://arxiv.org/abs/2509.10305)
*Yutong Shen,Ruizhe Xia,Bokai Yan,Shunqi zhang,Pengrui Xiang,Sicheng He,Yixin Xu*

Main category: cs.RO

TL;DR: GundamQ是一个用于机器人路径规划的多尺度时空Q网络，通过时空感知模块和自适应策略优化模块，解决了动态环境中多尺度时间依赖建模不足和探索-利用平衡效率低的问题，在成功率和路径质量上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度强化学习的路径规划方法存在两个根本局限：(1)多尺度时间依赖建模不足，导致动态场景适应性差；(2)探索-利用平衡效率低，导致路径质量下降。需要在动态不确定环境中实现准确的时空环境理解和鲁棒决策。

Method: 提出GundamQ框架，包含两个关键模块：(i)时空感知模块：分层提取多粒度空间特征和多尺度时间依赖（从瞬时到扩展时间范围）；(ii)自适应策略优化模块：在训练中平衡探索与利用，通过约束策略更新优化平滑度和碰撞概率。

Result: 在动态环境实验中，GundamQ实现了15.3%的成功率提升和21.7%的整体路径质量提升，显著优于现有最先进方法。

Conclusion: GundamQ通过有效的多尺度时空建模和自适应策略优化，显著提高了动态环境中机器人路径规划的性能，为解决当前方法的局限性提供了有效解决方案。

Abstract: In dynamic and uncertain environments, robotic path planning demands accurate
spatiotemporal environment understanding combined with robust decision-making
under partial observability. However, current deep reinforcement learning-based
path planning methods face two fundamental limitations: (1) insufficient
modeling of multi-scale temporal dependencies, resulting in suboptimal
adaptability in dynamic scenarios, and (2) inefficient exploration-exploitation
balance, leading to degraded path quality. To address these challenges, we
propose GundamQ: A Multi-Scale Spatiotemporal Q-Network for Robotic Path
Planning. The framework comprises two key modules: (i) the Spatiotemporal
Perception module, which hierarchically extracts multi-granularity spatial
features and multi-scale temporal dependencies ranging from instantaneous to
extended time horizons, thereby improving perception accuracy in dynamic
environments; and (ii) the Adaptive Policy Optimization module, which balances
exploration and exploitation during training while optimizing for smoothness
and collision probability through constrained policy updates. Experiments in
dynamic environments demonstrate that GundamQ achieves a 15.3\% improvement in
success rate and a 21.7\% increase in overall path quality, significantly
outperforming existing state-of-the-art methods.

</details>


### [16] [Robot guide with multi-agent control and automatic scenario generation with LLM](https://arxiv.org/abs/2509.10317)
*Elizaveta D. Moskovskaya,Anton D. Moscowsky*

Main category: cs.RO

TL;DR: 开发了一种结合多智能体资源管理和基于大语言模型的自动行为场景生成的混合控制架构，用于人形导游机器人，以克服传统手动调整行为场景系统的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统导游机器人系统依赖手动调整行为场景，存在配置繁琐、灵活性低、行为不自然等局限性，需要更自动化和自然的控制方法。

Method: 采用两阶段生成过程：首先生成风格化叙述，然后将非语言动作标签集成到文本中；使用多智能体系统确保并行动作执行时的协调和冲突解决，并在主要操作完成后维持默认行为。

Result: 试验结果表明，该方法在自动化和社会机器人控制系统扩展方面具有潜力，能够实现更自然的机器人行为。

Conclusion: 提出的混合控制架构成功克服了传统系统的局限性，为社交机器人控制系统的自动化和规模化提供了有效解决方案。

Abstract: The work describes the development of a hybrid control architecture for an
anthropomorphic tour guide robot, combining a multi-agent resource management
system with automatic behavior scenario generation based on large language
models. The proposed approach aims to overcome the limitations of traditional
systems, which rely on manual tuning of behavior scenarios. These limitations
include manual configuration, low flexibility, and lack of naturalness in robot
behavior. The process of preparing tour scenarios is implemented through a
two-stage generation: first, a stylized narrative is created, then non-verbal
action tags are integrated into the text. The multi-agent system ensures
coordination and conflict resolution during the execution of parallel actions,
as well as maintaining default behavior after the completion of main
operations, contributing to more natural robot behavior. The results obtained
from the trial demonstrate the potential of the proposed approach for
automating and scaling social robot control systems.

</details>


### [17] [Acetrans: An Autonomous Corridor-Based and Efficient UAV Suspended Transport System](https://arxiv.org/abs/2509.10349)
*Weiyan Lu,Huizhe Li,Yuhao Fang,Zhexuan Zhou,Junda Wu,Yude Li,Youmin Gong,Jie Mei*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Unmanned aerial vehicles (UAVs) with suspended payloads offer significant
advantages for aerial transportation in complex and cluttered environments.
However, existing systems face critical limitations, including unreliable
perception of the cable-payload dynamics, inefficient planning in large-scale
environments, and the inability to guarantee whole-body safety under cable
bending and external disturbances. This paper presents Acetrans, an Autonomous,
Corridor-based, and Efficient UAV suspended transport system that addresses
these challenges through a unified perception, planning, and control framework.
A LiDAR-IMU fusion module is proposed to jointly estimate both payload pose and
cable shape under taut and bent modes, enabling robust whole-body state
estimation and real-time filtering of cable point clouds. To enhance planning
scalability, we introduce the Multi-size-Aware Configuration-space Iterative
Regional Inflation (MACIRI) algorithm, which generates safe flight corridors
while accounting for varying UAV and payload geometries. A spatio-temporal,
corridor-constrained trajectory optimization scheme is then developed to ensure
dynamically feasible and collision-free trajectories. Finally, a nonlinear
model predictive controller (NMPC) augmented with cable-bending constraints
provides robust whole-body safety during execution. Simulation and experimental
results validate the effectiveness of Acetrans, demonstrating substantial
improvements in perception accuracy, planning efficiency, and control safety
compared to state-of-the-art methods.

</details>


### [18] [Self-supervised Learning Of Visual Pose Estimation Without Pose Labels By Classifying LED States](https://arxiv.org/abs/2509.10405)
*Nicholas Carlotti,Mirko Nava,Alessandro Giusti*

Main category: cs.RO

TL;DR: 提出一种无需位姿标签或机器人先验知识的单目RGB相对位姿估计模型，通过LED状态预测任务学习机器人位置、距离和相对方位


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要位姿监督或机器人CAD模型的问题，实现完全自监督的单目相对位姿估计

Method: 使用带多个LED的机器人，训练时已知LED状态和大致视角方向，通过预测LED状态学习位姿估计；推理时LED状态未知但不影响性能

Result: 与需要监督的SoA方法竞争力相当，具有良好的领域泛化能力，支持多机器人位姿估计

Conclusion: 该方法证明了通过简单的LED状态预测任务可以实现有效的自监督相对位姿估计，无需外部基础设施或人工监督

Abstract: We introduce a model for monocular RGB relative pose estimation of a ground
robot that trains from scratch without pose labels nor prior knowledge about
the robot's shape or appearance. At training time, we assume: (i) a robot
fitted with multiple LEDs, whose states are independent and known at each
frame; (ii) knowledge of the approximate viewing direction of each LED; and
(iii) availability of a calibration image with a known target distance, to
address the ambiguity of monocular depth estimation. Training data is collected
by a pair of robots moving randomly without needing external infrastructure or
human supervision. Our model trains on the task of predicting from an image the
state of each LED on the robot. In doing so, it learns to predict the position
of the robot in the image, its distance, and its relative bearing. At inference
time, the state of the LEDs is unknown, can be arbitrary, and does not affect
the pose estimation performance. Quantitative experiments indicate that our
approach: is competitive with SoA approaches that require supervision from pose
labels or a CAD model of the robot; generalizes to different domains; and
handles multi-robot pose estimation.

</details>


### [19] [TASC: Task-Aware Shared Control for Teleoperated Manipulation](https://arxiv.org/abs/2509.10416)
*Ze Fu,Pinhao Song,Yutong Hu,Renaud Detry*

Main category: cs.RO

TL;DR: TASC是一个任务感知共享控制框架，通过视觉输入构建开放词汇交互图来推断任务级用户意图，为远程操作提供旋转辅助，无需预定义知识即可支持日常任务。


<details>
  <summary>Details</summary>
Motivation: 解决通用、长时程共享控制中的两个关键挑战：(1)理解和推断任务级用户意图，(2)跨不同对象和任务的辅助泛化。

Method: 构建开放词汇交互图表示功能对象关系，通过视觉语言模型预测空间约束，在抓取和对象交互期间提供旋转辅助的共享控制策略。

Result: 在仿真和真实世界实验中，TASC相比先前方法提高了任务效率并减少了用户输入工作量。

Conclusion: 这是第一个支持零样本泛化的日常操作任务的共享控制框架，具有零样本泛化能力。

Abstract: We present TASC, a Task-Aware Shared Control framework for teleoperated
manipulation that infers task-level user intent and provides assistance
throughout the task. To support everyday tasks without predefined knowledge,
TASC constructs an open-vocabulary interaction graph from visual input to
represent functional object relationships, and infers user intent accordingly.
A shared control policy then provides rotation assistance during both grasping
and object interaction, guided by spatial constraints predicted by a
vision-language model. Our method addresses two key challenges in
general-purpose, long-horizon shared control: (1) understanding and inferring
task-level user intent, and (2) generalizing assistance across diverse objects
and tasks. Experiments in both simulation and the real world demonstrate that
TASC improves task efficiency and reduces user input effort compared to prior
methods. To the best of our knowledge, this is the first shared control
framework that supports everyday manipulation tasks with zero-shot
generalization. The code that supports our experiments is publicly available at
https://github.com/fitz0401/tasc.

</details>


### [20] [DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training](https://arxiv.org/abs/2509.10426)
*Jianxin Shi,Zengqi Peng,Xiaolong Chen,Tianyu Wo,Jun Ma*

Main category: cs.RO

TL;DR: DECAMP是一个解耦的上下文感知预训练框架，用于多智能体运动预测，通过分离行为模式学习和潜在特征重建，在自动驾驶中实现了优越的性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在标注数据稀缺和多智能体预测场景中表现不佳的问题，提升自动驾驶的安全性和效率。

Method: 采用解耦的上下文感知预训练框架，将行为模式学习与潜在特征重建分离，结合上下文感知表示学习和协作空间运动预训练任务。

Result: 在Argoverse 2基准测试中展现出优越性能，证明了在多智能体运动预测中的有效性。

Conclusion: DECAMP是首个用于自动驾驶多智能体运动预测的上下文自编码器框架，具有优异的预测能力和可解释性。

Abstract: Trajectory prediction is a critical component of autonomous driving,
essential for ensuring both safety and efficiency on the road. However,
traditional approaches often struggle with the scarcity of labeled data and
exhibit suboptimal performance in multi-agent prediction scenarios. To address
these challenges, we introduce a disentangled context-aware pre-training
framework for multi-agent motion prediction, named DECAMP. Unlike existing
methods that entangle representation learning with pretext tasks, our framework
decouples behavior pattern learning from latent feature reconstruction,
prioritizing interpretable dynamics and thereby enhancing scene representation
for downstream prediction. Additionally, our framework incorporates
context-aware representation learning alongside collaborative spatial-motion
pretext tasks, which enables joint optimization of structural and intentional
reasoning while capturing the underlying dynamic intentions. Our experiments on
the Argoverse 2 benchmark showcase the superior performance of our method, and
the results attained underscore its effectiveness in multi-agent motion
forecasting. To the best of our knowledge, this is the first context
autoencoder framework for multi-agent motion forecasting in autonomous driving.
The code and models will be made publicly available.

</details>


### [21] [Coordinated Motion Planning of a Wearable Multi-Limb System for Enhanced Human-Robot Interaction](https://arxiv.org/abs/2509.10444)
*Chaerim Moon,Joohyung Kim*

Main category: cs.RO

TL;DR: 提出了一种减少超数机器人肢体操作时产生力矩的运动规划层，通过调整轨迹的角加速度和位置偏差来增强人机交互


<details>
  <summary>Details</summary>
Motivation: 超数机器人肢体作为可穿戴设备，其操作产生的力矩会作用于人体，当力矩增大时需要更多肌肉单元来平衡，导致肌肉零空间减少，影响人机交互效果

Method: 设计运动规划层来修改给定轨迹，设置理想的角加速度和位置偏差限制，通过简化的人体和机器人系统模型进行仿真验证

Result: 仿真结果表明该方法能够有效减少操作时产生的力矩

Conclusion: 提出的运动规划层概念能够减少超数机器人肢体操作时的力矩输出，从而改善人机交互性能

Abstract: Supernumerary Robotic Limbs (SRLs) can enhance human capability within close
proximity. However, as a wearable device, the generated moment from its
operation acts on the human body as an external torque. When the moments
increase, more muscle units are activated for balancing, and it can result in
reduced muscular null space. Therefore, this paper suggests a concept of a
motion planning layer that reduces the generated moment for enhanced
Human-Robot Interaction. It modifies given trajectories with desirable angular
acceleration and position deviation limits. Its performance to reduce the
moment is demonstrated through the simulation, which uses simplified human and
robotic system models.

</details>


### [22] [GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation](https://arxiv.org/abs/2509.10454)
*Hang Yin,Haoyu Wei,Xiuwei Xu,Wenxuan Guo,Jie Zhou,Jiwen Lu*

Main category: cs.RO

TL;DR: 这篇论文提出了一种无需训练的视觉-语言导航框架，通过将导航指令分解为显式空间约束并采用图约束优化方法，实现了在连续环境中的零样本适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本VLN方法主要为离散环境设计，或需要在连续模拟器环境中进行无监督训练，这给真实世界场景中的部署带来挑战。需要开发一种无需训练的框架来提高通用性和可部署性。

Method: 将导航指导形式化为图约束优化问题，通过分解指令为显式空间约束。构建了覆盖VLN指令中所有空间关系类型的空间约束库。将人类指令分解为有向无环图，通过查询约束库构建图约束，使用约束求解器解决优化问题。为处理无解或多解情况，构建导航树并采用回溯机制。

Result: 在标准测试集上进行了大量实验，与最先进的零样本VLN方法相比，在成功率和导航效率方面都取得了显著提升。真实世界实验证明了该框架能够有效地适应新环境和指令集。

Conclusion: 该无训练框架通过约束驱动的方法实现了在连续环境中的零样本适应，为建立更稳健和自主的导航框架摩平了道路。

Abstract: In this paper, we propose a training-free framework for vision-and-language
navigation (VLN). Existing zero-shot VLN methods are mainly designed for
discrete environments or involve unsupervised training in continuous simulator
environments, which makes it challenging to generalize and deploy them in
real-world scenarios. To achieve a training-free framework in continuous
environments, our framework formulates navigation guidance as graph constraint
optimization by decomposing instructions into explicit spatial constraints. The
constraint-driven paradigm decodes spatial semantics through constraint
solving, enabling zero-shot adaptation to unseen environments. Specifically, we
construct a spatial constraint library covering all types of spatial
relationship mentioned in VLN instructions. The human instruction is decomposed
into a directed acyclic graph, with waypoint nodes, object nodes and edges,
which are used as queries to retrieve the library to build the graph
constraints. The graph constraint optimization is solved by the constraint
solver to determine the positions of waypoints, obtaining the robot's
navigation path and final goal. To handle cases of no solution or multiple
solutions, we construct a navigation tree and the backtracking mechanism.
Extensive experiments on standard benchmarks demonstrate significant
improvements in success rate and navigation efficiency compared to
state-of-the-art zero-shot VLN methods. We further conduct real-world
experiments to show that our framework can effectively generalize to new
environments and instruction sets, paving the way for a more robust and
autonomous navigation framework.

</details>
