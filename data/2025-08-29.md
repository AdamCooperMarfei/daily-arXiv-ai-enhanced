<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Learning Fast, Tool aware Collision Avoidance for Collaborative Robots](https://arxiv.org/abs/2508.20457)
*Joonho Lee,Yunho Kim,Seokjoon Kim,Quan Nguyen,Youngjin Heo*

Main category: cs.RO

TL;DR: 一种基于强化学习的工具感知碰撞避免系统，能够在动态环境中进行实时调整以避免碰撞，同时保持高精度和低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决协作机器人在动态人类环境中的安全运行问题，当前控制器偏依赖全可视性和固定工具偏置，导致碰撞或保守行为。

Method: 使用学习感知模型过滤点云数据，推理遮挡区域，预测部分可观测情况下的碰撞风险，通过约束强化学习训练控制策略生成平滑的避免机动。

Result: 在模拟和实际测试中表现超过传统方法(APF, MPPI)，维持毫米级精度，计算成本比GPU基冶规划器低60%，处理时间在10毫秒以内。

Conclusion: 该系统为动态环境中的机器人提供了模块化、高效和有效的碰撞避免方案，实现了安全和响应快速的协作操作。

Abstract: Ensuring safe and efficient operation of collaborative robots in human
environments is challenging, especially in dynamic settings where both obstacle
motion and tasks change over time. Current robot controllers typically assume
full visibility and fixed tools, which can lead to collisions or overly
conservative behavior. In our work, we introduce a tool-aware collision
avoidance system that adjusts in real time to different tool sizes and modes of
tool-environment interaction. Using a learned perception model, our system
filters out robot and tool components from the point cloud, reasons about
occluded area, and predicts collision under partial observability. We then use
a control policy trained via constrained reinforcement learning to produce
smooth avoidance maneuvers in under 10 milliseconds. In simulated and
real-world tests, our approach outperforms traditional approaches (APF, MPPI)
in dynamic environments, while maintaining sub-millimeter accuracy. Moreover,
our system operates with approximately 60% lower computational cost compared to
a state-of-the-art GPU-based planner. Our approach provides modular, efficient,
and effective collision avoidance for robots operating in dynamic environments.
We integrate our method into a collaborative robot application and demonstrate
its practical use for safe and responsive operation.

</details>


### [2] [SPGrasp: Spatiotemporal Prompt-driven Grasp Synthesis in Dynamic Scenes](https://arxiv.org/abs/2508.20547)
*Yunpeng Mei,Hongjie Cao,Yinqiu Xia,Wei Xiao,Zhaohan Feng,Gang Wang,Jie Chen*

Main category: cs.RO

TL;DR: SPGrasp是一个基于SAMv2的实时动态抓取合成框架，通过整合用户提示和时空上下文，实现59ms低延迟推理，在多个基准测试中达到90%以上的抓取准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法在保持可提示性的同时实现低延迟推理，实时交互式动态物体抓取合成仍面临挑战。

Method: 扩展SAMv2模型用于视频流抓取估计，整合用户提示与时空上下文信息，确保动态物体的时间一致性。

Result: 在OCID上达到90.6%实例级抓取准确率，Jacquard上93.8%，GraspNet-1Billion连续跟踪下92.0%准确率，延迟73.1ms，比RoG-SAM降低58.5%。真实实验中13个移动物体交互抓取成功率94.8%。

Conclusion: SPGrasp有效解决了动态抓取合成中的延迟-交互性权衡问题，实现了实时交互式动态物体抓取。

Abstract: Real-time interactive grasp synthesis for dynamic objects remains challenging
as existing methods fail to achieve low-latency inference while maintaining
promptability. To bridge this gap, we propose SPGrasp (spatiotemporal
prompt-driven dynamic grasp synthesis), a novel framework extending segment
anything model v2 (SAMv2) for video stream grasp estimation. Our core
innovation integrates user prompts with spatiotemporal context, enabling
real-time interaction with end-to-end latency as low as 59 ms while ensuring
temporal consistency for dynamic objects. In benchmark evaluations, SPGrasp
achieves instance-level grasp accuracies of 90.6% on OCID and 93.8% on
Jacquard. On the challenging GraspNet-1Billion dataset under continuous
tracking, SPGrasp achieves 92.0% accuracy with 73.1 ms per-frame latency,
representing a 58.5% reduction compared to the prior state-of-the-art
promptable method RoG-SAM while maintaining competitive accuracy. Real-world
experiments involving 13 moving objects demonstrate a 94.8% success rate in
interactive grasping scenarios. These results confirm SPGrasp effectively
resolves the latency-interactivity trade-off in dynamic grasp synthesis. Code
is available at https://github.com/sejmoonwei/SPGrasp.

</details>


### [3] [SimShear: Sim-to-Real Shear-based Tactile Servoing](https://arxiv.org/abs/2508.20561)
*Kipp McAdam Freud,Yijiong Lin,Nathan F. Lepora*

Main category: cs.RO

TL;DR: SimShear是一个从仿真到现实的触觉控制管道，通过shPix2pix GAN将无剪切信息的仿真触觉图像转换为包含剪切变形的真实图像，成功应用于桌面机械臂的触觉跟踪和协作举升任务。


<details>
  <summary>Details</summary>
Motivation: 剪切信息对于动态物体交互任务至关重要，但在仿真中难以准确建模。现有方法无法有效模拟剪切动力学，限制了触觉控制在真实环境中的应用。

Method: 提出shPix2pix剪切条件U-Net GAN，将无剪切信息的仿真触觉图像与剪切编码向量结合，生成包含剪切变形的真实触觉图像。使用低成本桌面机械臂和视觉触觉传感器进行验证。

Result: 该方法在触觉图像仿真和位姿/剪切预测方面优于基线pix2pix方法。在触觉跟踪和协作举升任务中，接触误差保持在1-2毫米范围内，验证了仿真到现实剪切建模的可行性。

Conclusion: SimShear证明了使用刚体仿真器进行仿真到现实剪切建模的可行性，为触觉机器人学的仿真开辟了新方向，能够有效处理需要剪切感知的各种轨迹任务。

Abstract: We present SimShear, a sim-to-real pipeline for tactile control that enables
the use of shear information without explicitly modeling shear dynamics in
simulation. Shear, arising from lateral movements across contact surfaces, is
critical for tasks involving dynamic object interactions but remains
challenging to simulate. To address this, we introduce shPix2pix, a
shear-conditioned U-Net GAN that transforms simulated tactile images absent of
shear, together with a vector encoding shear information, into realistic
equivalents with shear deformations. This method outperforms baseline pix2pix
approaches in simulating tactile images and in pose/shear prediction. We apply
SimShear to two control tasks using a pair of low-cost desktop robotic arms
equipped with a vision-based tactile sensor: (i) a tactile tracking task, where
a follower arm tracks a surface moved by a leader arm, and (ii) a collaborative
co-lifting task, where both arms jointly hold an object while the leader
follows a prescribed trajectory. Our method maintains contact errors within 1
to 2 mm across varied trajectories where shear sensing is essential, validating
the feasibility of sim-to-real shear modeling with rigid-body simulators and
opening new directions for simulation in tactile robotics.

</details>


### [4] [Traversing the Narrow Path: A Two-Stage Reinforcement Learning Framework for Humanoid Beam Walking](https://arxiv.org/abs/2508.20661)
*TianChen Huang,Wei Gao,Runchen Xu,Shiwu Zhang*

Main category: cs.RO

TL;DR: 提出一个两阶段框架，结合XCoM/LIPM步态模板与轻量级残差规划器，使双足机器人能够可靠地穿越窄梁


<details>
  <summary>Details</summary>
Motivation: 解决双足机器人在窄梁上行走的挑战，包括稀疏的安全关键接触点和纯学习策略的脆弱性

Method: 两阶段方法：第一阶段在平地上训练跟踪器跟随脚步目标；第二阶段在梁上训练高层规划器预测摆动脚的残差，优化模板步态以确保安全精确的落脚点

Result: 在Unitree G1机器人上成功穿越0.2米宽、3米长的梁，在仿真和真实环境中残差优化方法在成功率、中心线跟随和安全裕度方面均优于基准方法

Conclusion: 该结构化框架通过物理基础的方法实现了可靠的窄梁穿越，同时保持了可解释性和低摩擦的仿真到现实迁移

Abstract: Traversing narrow beams is challenging for humanoids due to sparse,
safety-critical contacts and the fragility of purely learned policies. We
propose a physically grounded, two-stage framework that couples an XCoM/LIPM
footstep template with a lightweight residual planner and a simple low-level
tracker. Stage-1 is trained on flat ground: the tracker learns to robustly
follow footstep targets by adding small random perturbations to heuristic
footsteps, without any hand-crafted centerline locking, so it acquires stable
contact scheduling and strong target-tracking robustness. Stage-2 is trained in
simulation on a beam: a high-level planner predicts a body-frame residual
(Delta x, Delta y, Delta psi) for the swing foot only, refining the template
step to prioritize safe, precise placement under narrow support while
preserving interpretability. To ease deployment, sensing is kept minimal and
consistent between simulation and hardware: the planner consumes compact,
forward-facing elevation cues together with onboard IMU and joint signals. On a
Unitree G1, our system reliably traverses a 0.2 m-wide, 3 m-long beam. Across
simulation and real-world studies, residual refinement consistently outperforms
template-only and monolithic baselines in success rate, centerline adherence,
and safety margins, while the structured footstep interface enables transparent
analysis and low-friction sim-to-real transfer.

</details>


### [5] [Task-Oriented Edge-Assisted Cross-System Design for Real-Time Human-Robot Interaction in Industrial Metaverse](https://arxiv.org/abs/2508.20664)
*Kan Chen,Zhen Meng,Xiangmin Xu,Jiaming Yang,Emma Li,Philip G. Zhao*

Main category: cs.RO

TL;DR: 提出基于数字孪生的边缘辅助跨系统框架，通过预测操作员动作实现主动元界渲染和远程设备预控制，在工业元宇宙中解决高计算负载、带宽限制和严格延迟问题。


<details>
  <summary>Details</summary>
Motivation: 工业元宇宙中实时人机交互面临高计算负载、有限带宽和严格延迟等挑战，需要一种能够确保空间精度和视觉保真度的响应式交互解决方案。

Method: 采用任务导向的边缘辅助跨系统框架，将数字孪生解耦为视觉显示和机器人控制两个虚拟功能，并引入HITL-MAML算法动态调整预测范围以增强泛化能力。

Result: 在轨迹绘制控制任务中将加权RMSE从0.0712m降低到0.0101m；在核退役实时3D场景表示任务中实现PSNR 22.11、SSIM 0.8729和LPIPS 0.1298的优异性能。

Conclusion: 该框架能够在实时高风险工业环境中有效确保空间精度和视觉保真度，为工业元宇宙中的实时人机交互提供了可行的解决方案。

Abstract: Real-time human-device interaction in industrial Metaverse faces challenges
such as high computational load, limited bandwidth, and strict latency. This
paper proposes a task-oriented edge-assisted cross-system framework using
digital twins (DTs) to enable responsive interactions. By predicting operator
motions, the system supports: 1) proactive Metaverse rendering for visual
feedback, and 2) preemptive control of remote devices. The DTs are decoupled
into two virtual functions-visual display and robotic control-optimizing both
performance and adaptability. To enhance generalizability, we introduce the
Human-In-The-Loop Model-Agnostic Meta-Learning (HITL-MAML) algorithm, which
dynamically adjusts prediction horizons. Evaluation on two tasks demonstrates
the framework's effectiveness: in a Trajectory-Based Drawing Control task, it
reduces weighted RMSE from 0.0712 m to 0.0101 m; in a real-time 3D scene
representation task for nuclear decommissioning, it achieves a PSNR of 22.11,
SSIM of 0.8729, and LPIPS of 0.1298. These results show the framework's
capability to ensure spatial precision and visual fidelity in real-time,
high-risk industrial environments.

</details>


### [6] [Task Allocation for Autonomous Machines using Computational Intelligence and Deep Reinforcement Learning](https://arxiv.org/abs/2508.20688)
*Thanh Thi Nguyen,Quoc Viet Hung Nguyen,Jonathan Kua,Imran Razzak,Dung Nguyen,Saeid Nahavandi*

Main category: cs.RO

TL;DR: 本文是一份关于多自主机器协同控制算法的综述，重点分析了计算智能和深度强化学习在任务分配中的应用。


<details>
  <summary>Details</summary>
Motivation: 为了提高多自主机器系统的可靠性和性能，需要发展高效的协同控制算法，特别是在复杂环境中的任务分配方法。

Method: 采用综述方法，详细评估了计算智能(CI)和深度强化学习(deep RL)在任务分配中的应用，分析各种方法的优缺点。

Result: 调查结果显示CI和deep RL方法能够有效解决动态和不确定环境中的复杂任务分配问题，深度强化学习已成为该领域的发展趋势。

Conclusion: 本文为研究人员和工程师提供了自主机器相关机器学习研究的全面概览，指出了未充分探索的领域，识别了新兴方法论，并为未来研究提供了新的研究方向。

Abstract: Enabling multiple autonomous machines to perform reliably requires the
development of efficient cooperative control algorithms. This paper presents a
survey of algorithms that have been developed for controlling and coordinating
autonomous machines in complex environments. We especially focus on task
allocation methods using computational intelligence (CI) and deep reinforcement
learning (RL). The advantages and disadvantages of the surveyed methods are
analysed thoroughly. We also propose and discuss in detail various future
research directions that shed light on how to improve existing algorithms or
create new methods to enhance the employability and performance of autonomous
machines in real-world applications. The findings indicate that CI and deep RL
methods provide viable approaches to addressing complex task allocation
problems in dynamic and uncertain environments. The recent development of deep
RL has greatly contributed to the literature on controlling and coordinating
autonomous machines, and it has become a growing trend in this area. It is
envisaged that this paper will provide researchers and engineers with a
comprehensive overview of progress in machine learning research related to
autonomous machines. It also highlights underexplored areas, identifies
emerging methodologies, and suggests new avenues for exploration in future
research within this domain.

</details>


### [7] [Non-expert to Expert Motion Translation Using Generative Adversarial Networks](https://arxiv.org/abs/2508.20740)
*Yuki Tanaka,Seiichiro Katsura*

Main category: cs.RO

TL;DR: 通过生成对抗网络实现灵活的动作翻译方法，允许用户通过输入数据教学任务，通过训练模型教学技能


<details>
  <summary>Details</summary>
Motivation: 解决专业技工人才减少问题，实现专家技能向机器人的传输。现有方法难以根据人类意图切换任务，标签限制多

Method: 使用生成对抗网络（GAN）实现灵活的动作翻译方法，能够根据人类意图进行任务切换

Result: 在3自由度书法机器人上进行了评估，验证了方法的有效性

Conclusion: 提出的方法能够灵活地实现任务切换，为机器人任务教学提供了新的解决方案

Abstract: Decreasing skilled workers is a very serious problem in the world. To deal
with this problem, the skill transfer from experts to robots has been
researched. These methods which teach robots by human motion are called
imitation learning. Experts' skills generally appear in not only position data,
but also force data. Thus, position and force data need to be saved and
reproduced. To realize this, a lot of research has been conducted in the
framework of a motion-copying system. Recent research uses machine learning
methods to generate motion commands. However, most of them could not change
tasks by following human intention. Some of them can change tasks by
conditional training, but the labels are limited. Thus, we propose the flexible
motion translation method by using Generative Adversarial Networks. The
proposed method enables users to teach robots tasks by inputting data, and
skills by a trained model. We evaluated the proposed system with a 3-DOF
calligraphy robot.

</details>


### [8] [Uncertainty Aware-Predictive Control Barrier Functions: Safer Human Robot Interaction through Probabilistic Motion Forecasting](https://arxiv.org/abs/2508.20812)
*Lorenzo Busellato,Federico Cunico,Diego Dall'Alba,Marco Emporio,Andrea Giachetti,Riccardo Muradore,Marco Cristani*

Main category: cs.RO

TL;DR: 提出了一种不确定性感知预测控制屏障函数（UA-PCBFs）框架，将概率性人体运动预测与控制屏障函数的形式化安全保证相结合，实现更流畅的人机协作


<details>
  <summary>Details</summary>
Motivation: 解决人机协作环境中机器人因过度保守的安全策略而导致的不必要制动和任务停滞问题，提高人机交互的流畅性和效率

Method: 融合概率性人手运动预测和控制屏障函数，通过动态调整安全边界来适应人体运动的不确定性，使用预测模块提供不确定性估计

Result: 在真实世界实验中，相比最先进的人机交互架构，UA-PCBFs在任务关键指标上表现更好，显著减少了机器人安全空间被侵犯的次数

Conclusion: UA-PCBFs框架通过不确定性感知的预测控制，为人机协作提供了更智能、更流畅的安全保障，提升了人机交互的质量和效率

Abstract: To enable flexible, high-throughput automation in settings where people and
robots share workspaces, collaborative robotic cells must reconcile stringent
safety guarantees with the need for responsive and effective behavior. A
dynamic obstacle is the stochastic, task-dependent variability of human motion:
when robots fall back on purely reactive or worst-case envelopes, they brake
unnecessarily, stall task progress, and tamper with the fluidity that true
Human-Robot Interaction demands. In recent years, learning-based human-motion
prediction has rapidly advanced, although most approaches produce worst-case
scenario forecasts that often do not treat prediction uncertainty in a
well-structured way, resulting in over-conservative planning algorithms,
limiting their flexibility. We introduce Uncertainty-Aware Predictive Control
Barrier Functions (UA-PCBFs), a unified framework that fuses probabilistic
human hand motion forecasting with the formal safety guarantees of Control
Barrier Functions. In contrast to other variants, our framework allows for
dynamic adjustment of the safety margin thanks to the human motion uncertainty
estimation provided by a forecasting module. Thanks to uncertainty estimation,
UA-PCBFs empower collaborative robots with a deeper understanding of future
human states, facilitating more fluid and intelligent interactions through
informed motion planning. We validate UA-PCBFs through comprehensive real-world
experiments with an increasing level of realism, including automated setups (to
perform exactly repeatable motions) with a robotic hand and direct human-robot
interactions (to validate promptness, usability, and human confidence).
Relative to state-of-the-art HRI architectures, UA-PCBFs show better
performance in task-critical metrics, significantly reducing the number of
violations of the robot's safe space during interaction with respect to the
state-of-the-art.

</details>


### [9] [A Soft Fabric-Based Thermal Haptic Device for VR and Teleoperation](https://arxiv.org/abs/2508.20831)
*Rui Chen,Domenico Chiaradia,Antonio Frisoli,Daniele Leonardis*

Main category: cs.RO

TL;DR: 这篇论文提出了一种新颖的基于纺织物的热触觉界面，通过空气动力和导电纺织集成，以超轻设计实现了虚拟现实和遥操作中的动态热触觉反馈。


<details>
  <summary>Details</summary>
Motivation: 为了在虚拟现实和遥操作中提供更丰富的触觉体验，需要开发能够同时提供压力和温度刺激的轻便触觉界面。

Method: 采用空气动力和导电纺织集成技术，在纺织空气室内嵌入加热元件，构建全软体穿戴式界面，每个指头单元仅重2克。

Result: 系统实现了迅速的温度调节（最高加热速率3°C/s），最大压力8.93N，温度识别准确率0.98，在虚拟摘放任务中成功率从88.5%提升到96.4%，力控制精度显著提高。

Conclusion: 这种集成热触觉方法有效提升了人机交互效果，为高级人机交互应用提供了有效的解决方案。

Abstract: This paper presents a novel fabric-based thermal-haptic interface for virtual
reality and teleoperation. It integrates pneumatic actuation and conductive
fabric with an innovative ultra-lightweight design, achieving only 2~g for each
finger unit. By embedding heating elements within textile pneumatic chambers,
the system delivers modulated pressure and thermal stimuli to fingerpads
through a fully soft, wearable interface.
  Comprehensive characterization demonstrates rapid thermal modulation with
heating rates up to 3$^{\circ}$C/s, enabling dynamic thermal feedback for
virtual or teleoperation interactions. The pneumatic subsystem generates forces
up to 8.93~N at 50~kPa, while optimization of fingerpad-actuator clearance
enhances cooling efficiency with minimal force reduction. Experimental
validation conducted with two different user studies shows high temperature
identification accuracy (0.98 overall) across three thermal levels, and
significant manipulation improvements in a virtual pick-and-place tasks.
Results show enhanced success rates (88.5\% to 96.4\%, p = 0.029) and improved
force control precision (p = 0.013) when haptic feedback is enabled, validating
the effectiveness of the integrated thermal-haptic approach for advanced
human-machine interaction applications.

</details>


### [10] [Model-Free Hovering and Source Seeking via Extremum Seeking Control: Experimental Demonstration](https://arxiv.org/abs/2508.20836)
*Ahmed A. Elgohary,Rohan Palanikumar,Sameh A. Eisa*

Main category: cs.RO

TL;DR: 本文首次实验验证了极值搜索控制(ESC)在扑翼机器人中实现无模型实时悬停和源追踪的潜力，结果证实了ESC作为扑翼飞行自然控制方法和仿生机制的有效性。


<details>
  <summary>Details</summary>
Motivation: 模仿昆虫和蜂鸟的悬停和源追踪现象，为扑翼系统提供无模型、实时的仿生控制设计机会。

Method: 采用新型极值搜索控制(ESC)方法，在扑翼机器人上进行实验测试和验证。

Result: 实验结果(虽然限于1D)证实了ESC作为扑翼飞行自然控制方法和仿生机制的前提。

Conclusion: ESC被证明是扑翼机器人和飞行领域中有效的无模型实时控制方法，具有重要的仿生应用价值。

Abstract: In a recent effort, we successfully proposed a categorically novel approach
to mimic the phenomenoa of hovering and source seeking by flapping insects and
hummingbirds using a new extremum seeking control (ESC) approach. Said ESC
approach was shown capable of characterizing the physics of hovering and source
seeking by flapping systems, providing at the same time uniquely novel
opportunity for a model-free, real-time biomimicry control design. In this
paper, we experimentally test and verify, for the first time in the literature,
the potential of ESC in flapping robots to achieve model-free, real-time
controlled hovering and source seeking. The results of this paper, while being
restricted to 1D, confirm the premise of introducing ESC as a natural control
method and biomimicry mechanism to the field of flapping flight and robotics.

</details>


### [11] [Learning Primitive Embodied World Models: Towards Scalable Robotic Learning](https://arxiv.org/abs/2508.20840)
*Qiao Sun,Liujia Yang,Wei Tang,Wei Huang,Kaixin Xu,Yongchao Chen,Mingyu Liu,Jiange Yang,Haoyi Zhu,Yating Wang,Tong He,Yilun Chen,Xili Dai,Nanyang Ye,Qinying Gu*

Main category: cs.RO

TL;DR: 本文提出了一种新的体验世界模型范式（PEWM），通过将视频生成限制在短时间范围内，解决了体验数据稀缺和高维度的问题，实现了语言与动作的细粒度对齐和高效数据利用。


<details>
  <summary>Details</summary>
Motivation: 体验数据的稀缺性、收集困难和高维度性限制了语言与动作的对齐细度，并加剧了长期望视频生成的挑战，阻碍了生成模型在体验领域达到"GPT时刻"的进步。

Method: 提出PEWM方法，将视频生成限制在固定短时间范围内，结合模块化视觉-语言模型（VLM）规划器和起始-目标热力图导航机制（SGG），支持组合性推广和闭环控制。

Result: 该方法实现了语言概念与机器人动作视觉表征的细粒度对齐，降低了学习复杂度，提高了数据利用效率，减少了推理延迟，支持长期复杂任务的组合性推广。

Conclusion: PEWM模型利用视频模型的时空视觉先验知和VLM的语义识别能力，桥接了细粒度物理交互与高级推理之间的差距，为可扩展、可解释和通用的体验智能探索了新路径。

Abstract: While video-generation-based embodied world models have gained increasing
attention, their reliance on large-scale embodied interaction data remains a
key bottleneck. The scarcity, difficulty of collection, and high dimensionality
of embodied data fundamentally limit the alignment granularity between language
and actions and exacerbate the challenge of long-horizon video
generation--hindering generative models from achieving a "GPT moment" in the
embodied domain. There is a naive observation: the diversity of embodied data
far exceeds the relatively small space of possible primitive motions. Based on
this insight, we propose a novel paradigm for world modeling--Primitive
Embodied World Models (PEWM). By restricting video generation to fixed short
horizons, our approach 1) enables fine-grained alignment between linguistic
concepts and visual representations of robotic actions, 2) reduces learning
complexity, 3) improves data efficiency in embodied data collection, and 4)
decreases inference latency. By equipping with a modular Vision-Language Model
(VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further
enables flexible closed-loop control and supports compositional generalization
of primitive-level policies over extended, complex tasks. Our framework
leverages the spatiotemporal vision priors in video models and the semantic
awareness of VLMs to bridge the gap between fine-grained physical interaction
and high-level reasoning, paving the way toward scalable, interpretable, and
general-purpose embodied intelligence.

</details>


### [12] [Genetic Informed Trees (GIT*): Path Planning via Reinforced Genetic Programming Heuristics](https://arxiv.org/abs/2508.20871)
*Liding Zhang,Kuanqi Cai,Zhenshan Bing,Chaoqun Wang,Alois Knoll*

Main category: cs.RO

TL;DR: GIT*算法通过整合多种环境数据和强化遗传编程来优化启发函数，在R^4到R^16的高维路径规划问题中超越了现有方法


<details>
  <summary>Details</summary>
Motivation: 现有路径规划方法往往忽略可用环境数据并简化启发函数结构，导致搜索效率和解决方案质量受限

Method: 提出遗传启发树(GIT*)，在EIT*基础上整合障碍物排斥力、顶点动态重要性等环境数据；采用强化遗传编程(RGP)结合遗传编程和奖励反馈来生成启发函数

Result: GIT*在R^4到R^16维度问题中超越现有单查询采样规划器，并在真实移动操作任务中验证有效

Conclusion: 整合多种环境数据和RGP方法能显著提升路径规划的计算效率和解决方案质量

Abstract: Optimal path planning involves finding a feasible state sequence between a
start and a goal that optimizes an objective. This process relies on heuristic
functions to guide the search direction. While a robust function can improve
search efficiency and solution quality, current methods often overlook
available environmental data and simplify the function structure due to the
complexity of information relationships. This study introduces Genetic Informed
Trees (GIT*), which improves upon Effort Informed Trees (EIT*) by integrating a
wider array of environmental data, such as repulsive forces from obstacles and
the dynamic importance of vertices, to refine heuristic functions for better
guidance. Furthermore, we integrated reinforced genetic programming (RGP),
which combines genetic programming with reward system feedback to mutate
genotype-generative heuristic functions for GIT*. RGP leverages a multitude of
data types, thereby improving computational efficiency and solution quality
within a set timeframe. Comparative analyses demonstrate that GIT* surpasses
existing single-query, sampling-based planners in problems ranging from R^4 to
R^16 and was tested on a real-world mobile manipulation task. A video
showcasing our experimental results is available at
https://youtu.be/URjXbc_BiYg

</details>


### [13] [Deep Fuzzy Optimization for Batch-Size and Nearest Neighbors in Optimal Robot Motion Planning](https://arxiv.org/abs/2508.20884)
*Liding Zhang,Qiyang Zong,Yu Zhang,Zhenshan Bing,Alois Knoll*

Main category: cs.RO

TL;DR: LIT*是一种基于深度模糊学习的采样规划器，通过动态调整批处理大小和最近邻参数来适应环境障碍分布，在8-14维空间中实现了更快的收敛速度和更好的路径质量


<details>
  <summary>Details</summary>
Motivation: 现有运动规划算法缺乏环境适应性，需要优化关键参数如批处理大小和最近邻选择来提升性能

Method: 采用深度模糊神经网络方法，通过编码全局和局部有效/无效状态比例来区分障碍稀疏和密集区域，动态调整参数

Result: 在高维空间实验中，LIT*实现了更快的收敛速度、更低的计算时间和更优的路径质量，在R^8到R^14环境中优于现有单查询采样规划器

Conclusion: LIT*通过深度模糊学习方法成功实现了环境自适应运动规划，在复杂高维环境中表现出色，并在双臂机器人操作任务中得到验证

Abstract: Efficient motion planning algorithms are essential in robotics. Optimizing
essential parameters, such as batch size and nearest neighbor selection in
sampling-based methods, can enhance performance in the planning process.
However, existing approaches often lack environmental adaptability. Inspired by
the method of the deep fuzzy neural networks, this work introduces
Learning-based Informed Trees (LIT*), a sampling-based deep fuzzy
learning-based planner that dynamically adjusts batch size and nearest neighbor
parameters to obstacle distributions in the configuration spaces. By encoding
both global and local ratios via valid and invalid states, LIT* differentiates
between obstacle-sparse and obstacle-dense regions, leading to lower-cost paths
and reduced computation time. Experimental results in high-dimensional spaces
demonstrate that LIT* achieves faster convergence and improved solution
quality. It outperforms state-of-the-art single-query, sampling-based planners
in environments ranging from R^8 to R^14 and is successfully validated on a
dual-arm robot manipulation task. A video showcasing our experimental results
is available at: https://youtu.be/NrNs9zebWWk

</details>


### [14] [CoCoL: A Communication Efficient Decentralized Collaborative Method for Multi-Robot Systems](https://arxiv.org/abs/2508.20898)
*Jiaxi Huang,Yan Huang,Yixian Zhao,Wenchao Meng,Jinming Xu*

Main category: cs.RO

TL;DR: CoCoL是一种面向多机器人系统的通信高效去中心化协作学习方法，通过镜像下降框架和近似牛顿更新显著减少通信开销，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 多机器人协作学习面临高通信开销和数据异构性的挑战，需要设计高效的通信方法来解决这些问题。

Method: 采用镜像下降框架实现近似牛顿型更新，利用梯度跟踪方案增强对数据异构性的鲁棒性，并通过不精确子问题解降低计算成本。

Result: 在三个典型多机器人协作学习任务中，CoCoL显著减少了通信轮数和总带宽消耗，同时保持了最先进的准确性，在非IID数据、流数据和时变网络拓扑等挑战性场景中表现优异。

Conclusion: CoCoL为多机器人系统提供了一种通信高效的协作学习解决方案，有效解决了通信开销和数据异构性问题，具有很好的实用价值。

Abstract: Collaborative learning enhances the performance and adaptability of
multi-robot systems in complex tasks but faces significant challenges due to
high communication overhead and data heterogeneity inherent in multi-robot
tasks. To this end, we propose CoCoL, a Communication efficient decentralized
Collaborative Learning method tailored for multi-robot systems with
heterogeneous local datasets. Leveraging a mirror descent framework, CoCoL
achieves remarkable communication efficiency with approximate Newton-type
updates by capturing the similarity between objective functions of robots, and
reduces computational costs through inexact sub-problem solutions. Furthermore,
the integration of a gradient tracking scheme ensures its robustness against
data heterogeneity. Experimental results on three representative multi robot
collaborative learning tasks show the superiority of the proposed CoCoL in
significantly reducing both the number of communication rounds and total
bandwidth consumption while maintaining state-of-the-art accuracy. These
benefits are particularly evident in challenging scenarios involving non-IID
(non-independent and identically distributed) data distribution, streaming
data, and time-varying network topologies.

</details>


### [15] [Language-Enhanced Mobile Manipulation for Efficient Object Search in Indoor Environments](https://arxiv.org/abs/2508.20899)
*Liding Zhang,Zeqi Li,Kuanqi Cai,Qian Huang,Zhenshan Bing,Alois Knoll*

Main category: cs.RO

TL;DR: 语言增强的层次导航框架GODHS，利用大语言模型进行场景语义推理和空间搜索，通过多级决策层并结合启发式运动规划，在本地化环境中实现更高效的物体搜索。


<details>
  <summary>Details</summary>
Motivation: 解决传统场景表征只能捕获静态语义、缺乏可解释性上下文推理的问题，以支持机器人在完全不熟悉的复杂环境中高效搜索物体的需求。

Method: 提出GODHS方法，统一集成语义感知和空间推理，利用大语言模型推断场景语义并通过多级决策层导搜索过程。采用结构化提示和逻辑约束确保推理可靠性，并为移动操作特别设计了基于极坐标角度排序和距离优先级的启发式运动规划器。

Result: 在Isaac Sim环境中的综合评估显示，GODHS能够更高效地定位目标物体，搜索效率显著高于传统非语义搜索策略。

Conclusion: 该框架通过语言增强的层次搜索方法，有效地结合了语义感知和空间推理，为机器人在本地化环境中的高效物体搜索提供了可靠的解决方案。

Abstract: Enabling robots to efficiently search for and identify objects in complex,
unstructured environments is critical for diverse applications ranging from
household assistance to industrial automation. However, traditional scene
representations typically capture only static semantics and lack interpretable
contextual reasoning, limiting their ability to guide object search in
completely unfamiliar settings. To address this challenge, we propose a
language-enhanced hierarchical navigation framework that tightly integrates
semantic perception and spatial reasoning. Our method, Goal-Oriented
Dynamically Heuristic-Guided Hierarchical Search (GODHS), leverages large
language models (LLMs) to infer scene semantics and guide the search process
through a multi-level decision hierarchy. Reliability in reasoning is achieved
through the use of structured prompts and logical constraints applied at each
stage of the hierarchy. For the specific challenges of mobile manipulation, we
introduce a heuristic-based motion planner that combines polar angle sorting
with distance prioritization to efficiently generate exploration paths.
Comprehensive evaluations in Isaac Sim demonstrate the feasibility of our
framework, showing that GODHS can locate target objects with higher search
efficiency compared to conventional, non-semantic search strategies. Website
and Video are available at: https://drapandiger.github.io/GODHS

</details>


### [16] [PLUME: Procedural Layer Underground Modeling Engine](https://arxiv.org/abs/2508.20926)
*Gabriel Manuel Garcia,Antoine Richard,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: PLUME是一个程序化生成框架，用于创建3D地下环境，支持AI训练、机器人算法评估和太空探索算法开发


<details>
  <summary>Details</summary>
Motivation: 随着太空探索发展，地下环境因其提供庇护、资源获取和科研机会的潜力而备受关注，但地球上的地下环境难以准确模拟太阳系多样性

Method: 开发了PLUME程序化生成框架，具有灵活结构可不断扩展地下环境特征，与太阳系认知发展保持同步

Result: PLUME已与机器人模拟器集成使用，能够生成用于AI训练、机器人算法评估和3D渲染的地下环境

Conclusion: PLUME作为开源框架，为地下环境模拟提供了有效工具，支持太空探索相关技术的快速迭代开发

Abstract: As space exploration advances, underground environments are becoming
increasingly attractive due to their potential to provide shelter, easier
access to resources, and enhanced scientific opportunities. Although such
environments exist on Earth, they are often not easily accessible and do not
accurately represent the diversity of underground environments found throughout
the solar system. This paper presents PLUME, a procedural generation framework
aimed at easily creating 3D underground environments. Its flexible structure
allows for the continuous enhancement of various underground features, aligning
with our expanding understanding of the solar system. The environments
generated using PLUME can be used for AI training, evaluating robotics
algorithms, 3D rendering, and facilitating rapid iteration on developed
exploration algorithms. In this paper, it is demonstrated that PLUME has been
used along with a robotic simulator. PLUME is open source and has been released
on Github. https://github.com/Gabryss/P.L.U.M.E

</details>


### [17] [Scaling Fabric-Based Piezoresistive Sensor Arrays for Whole-Body Tactile Sensing](https://arxiv.org/abs/2508.20959)
*Curtis C. Johnson,Daniel Webb,David Hill,Marc D. Killpack*

Main category: cs.RO

TL;DR: 提出了一种可扩展的触觉传感架构，通过硬件设计和新颖的SPI总线拓扑解决了布线复杂性和数据吞吐量问题，实现了超过8000个触觉传感器在50FPS下的同步数据流，并在全身抓取任务中验证了实时触觉反馈的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决触觉传感在全身操作中的扩展性挑战，包括布线复杂性、数据吞吐量和系统可靠性限制，为实时控制和物理人机交互研究提供可靠平台。

Method: 采用开源织物传感器与定制读取电子设备配对，通过硬件缓解将信号串扰降低至3.3%以下，并引入新颖的菊花链SPI总线拓扑结构，避免了无线协议的局限性和USB集线器系统的复杂布线问题。

Result: 系统能够从1平方米传感区域的8000多个触觉传感器以超过50FPS的更新率流式传输同步数据，在全身抓取任务中，实时触觉反馈使机器人能够实现轻柔稳定的抓取，而不会造成结构损坏。

Conclusion: 该工作提供了一个稳健且充分表征的平台，为先进的全身控制和物理人机交互的未来研究奠定了基础，展示了实时触觉反馈在机器人操作中的关键作用。

Abstract: Scaling tactile sensing for robust whole-body manipulation is a significant
challenge, often limited by wiring complexity, data throughput, and system
reliability. This paper presents a complete architecture designed to overcome
these barriers. Our approach pairs open-source, fabric-based sensors with
custom readout electronics that reduce signal crosstalk to less than 3.3%
through hardware-based mitigation. Critically, we introduce a novel,
daisy-chained SPI bus topology that avoids the practical limitations of common
wireless protocols and the prohibitive wiring complexity of USB hub-based
systems. This architecture streams synchronized data from over 8,000 taxels
across 1 square meter of sensing area at update rates exceeding 50 FPS,
confirming its suitability for real-time control. We validate the system's
efficacy in a whole-body grasping task where, without feedback, the robot's
open-loop trajectory results in an uncontrolled application of force that
slowly crushes a deformable cardboard box. With real-time tactile feedback, the
robot transforms this motion into a gentle, stable grasp, successfully
manipulating the object without causing structural damage. This work provides a
robust and well-characterized platform to enable future research in advanced
whole-body control and physical human-robot interaction.

</details>


### [18] [ActLoc: Learning to Localize on the Move via Active Viewpoint Selection](https://arxiv.org/abs/2508.20981)
*Jiajie Li,Boyang Sun,Luca Di Giammarino,Hermann Blum,Marc Pollefeys*

Main category: cs.RO

TL;DR: ActLoc是一个主动视角感知规划框架，通过大尺度训练的注意力模型选择最优视角来提升机器人定位精度，在单视角选择和完整轨迹规划中都取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有定位系统假设所有视角方向信息量相同，但实际上观察未映射、模糊或无信息区域时定位不可靠，需要主动选择信息丰富的视角来提升定位鲁棒性。

Method: 使用大尺度训练的基于注意力的模型，编码度量地图和建图时的相机位姿，预测任意3D位置在不同偏航和俯仰角度的定位精度分布，并将这些精度分布整合到路径规划器中。

Result: 在单视角选择和完整轨迹规划任务中达到了最先进的性能，能够有效提升定位鲁棒性。

Conclusion: ActLoc的模块化设计使其能够广泛应用于各种机器人导航和检测任务，通过主动视角选择显著提升了定位系统的可靠性。

Abstract: Reliable localization is critical for robot navigation, yet most existing
systems implicitly assume that all viewing directions at a location are equally
informative. In practice, localization becomes unreliable when the robot
observes unmapped, ambiguous, or uninformative regions. To address this, we
present ActLoc, an active viewpoint-aware planning framework for enhancing
localization accuracy for general robot navigation tasks. At its core, ActLoc
employs a largescale trained attention-based model for viewpoint selection. The
model encodes a metric map and the camera poses used during map construction,
and predicts localization accuracy across yaw and pitch directions at arbitrary
3D locations. These per-point accuracy distributions are incorporated into a
path planner, enabling the robot to actively select camera orientations that
maximize localization robustness while respecting task and motion constraints.
ActLoc achieves stateof-the-art results on single-viewpoint selection and
generalizes effectively to fulltrajectory planning. Its modular design makes it
readily applicable to diverse robot navigation and inspection tasks.

</details>


### [19] [UltraTac: Integrated Ultrasound-Augmented Visuotactile Sensor for Enhanced Robotic Perception](https://arxiv.org/abs/2508.20982)
*Junhao Gong,Kit-Wa Sou,Shoujie Li,Changqing Guo,Yan Huang,Chuqiao Lyu,Ziwu Song,Wenbo Ding*

Main category: cs.RO

TL;DR: UltraTac传感器通过同轴光声架构将视觉触觉成像与超声传感相结合，解决了传统视觉触觉传感器无法感知物体材料特性的问题，实现了接近感知、材料分类和纹理-材料双模式物体识别三大功能。


<details>
  <summary>Details</summary>
Motivation: 传统视觉触觉传感器虽然能提供高分辨率触觉信息，但无法感知物体的材料特性，这限制了机器人在复杂环境中的感知能力。

Method: 采用同轴光声架构设计，共享结构组件并实现两种模态的一致传感区域；在传统视觉触觉传感器结构中融入声学匹配，在不影响视觉触觉性能的前提下集成超声传感模块；通过触觉反馈动态调整超声模块的工作状态。

Result: 实现了3-8厘米范围内的接近感知（R²=0.90）、材料分类（平均准确率99.20%）以及纹理-材料双模式物体识别（15类任务准确率92.11%）；成功集成到机器人操作系统中，能同时检测容器表面图案和内部内容物。

Conclusion: UltraTac传感器展示了在高级人机交互和精确机器人操作中的潜力，为多模态感知提供了有效的解决方案。

Abstract: Visuotactile sensors provide high-resolution tactile information but are
incapable of perceiving the material features of objects. We present UltraTac,
an integrated sensor that combines visuotactile imaging with ultrasound sensing
through a coaxial optoacoustic architecture. The design shares structural
components and achieves consistent sensing regions for both modalities.
Additionally, we incorporate acoustic matching into the traditional
visuotactile sensor structure, enabling integration of the ultrasound sensing
modality without compromising visuotactile performance. Through tactile
feedback, we dynamically adjust the operating state of the ultrasound module to
achieve flexible functional coordination. Systematic experiments demonstrate
three key capabilities: proximity sensing in the 3-8 cm range ($R^2=0.90$),
material classification (average accuracy: 99.20%), and texture-material
dual-mode object recognition achieving 92.11% accuracy on a 15-class task.
Finally, we integrate the sensor into a robotic manipulation system to
concurrently detect container surface patterns and internal content, which
verifies its potential for advanced human-machine interaction and precise
robotic manipulation.

</details>


### [20] [Rapid Mismatch Estimation via Neural Network Informed Variational Inference](https://arxiv.org/abs/2508.21007)
*Mateusz Jaszczuk,Nadia Figueroa*

Main category: cs.RO

TL;DR: 提出RME（快速失配估计）框架，通过神经网络和变分推理在线估计末端执行器动力学失配，无需外部力传感器即可在约400毫秒内适应质量和质心变化


<details>
  <summary>Details</summary>
Motivation: 随着机器人在人机环境中的广泛应用，需要确保柔软安全的物理交互。现有阻抗控制方法依赖精确的动力学模型，模型失配会导致任务失败和不安全行为

Method: 使用神经网络模型失配估计器生成先验，结合变分推理求解器快速收敛到未知参数并量化不确定性，仅需本体感受反馈，无需外部力-扭矩传感器

Result: 在7自由度机械臂上，RME能在静态和动态设置中约400毫秒内适应末端执行器的质量和质心突变，在协作场景中展示了对动态变化的快速安全适应

Conclusion: RME提供了一个控制器无关的概率框架，能够在线快速估计动力学失配，实现安全的人机物理交互，无需额外传感器系统

Abstract: With robots increasingly operating in human-centric environments, ensuring
soft and safe physical interactions, whether with humans, surroundings, or
other machines, is essential. While compliant hardware can facilitate such
interactions, this work focuses on impedance controllers that allow
torque-controlled robots to safely and passively respond to contact while
accurately executing tasks. From inverse dynamics to quadratic
programming-based controllers, the effectiveness of these methods relies on
accurate dynamics models of the robot and the object it manipulates. Any model
mismatch results in task failures and unsafe behaviors. Thus, we introduce
Rapid Mismatch Estimation (RME), an adaptive, controller-agnostic,
probabilistic framework that estimates end-effector dynamics mismatches online,
without relying on external force-torque sensors. From the robot's
proprioceptive feedback, a Neural Network Model Mismatch Estimator generates a
prior for a Variational Inference solver, which rapidly converges to the
unknown parameters while quantifying uncertainty. With a real 7-DoF manipulator
driven by a state-of-the-art passive impedance controller, RME adapts to sudden
changes in mass and center of mass at the end-effector in $\sim400$ ms, in
static and dynamic settings. We demonstrate RME in a collaborative scenario
where a human attaches an unknown basket to the robot's end-effector and
dynamically adds/removes heavy items, showcasing fast and safe adaptation to
changing dynamics during physical interaction without any external sensory
system.

</details>


### [21] [HITTER: A HumanoId Table TEnnis Robot via Hierarchical Planning and Learning](https://arxiv.org/abs/2508.21043)
*Zhi Su,Bike Zhang,Nima Rahmanian,Yuman Gao,Qiayuan Liao,Caitlin Regan,Koushil Sreenath,S. Shankar Sastry*

Main category: cs.RO

TL;DR: 提出分层框架实现人形机器人乒乓球对打，结合模型规划器和强化学习控制器，实现亚秒级反应控制，最高连续击球106次


<details>
  <summary>Details</summary>
Motivation: 人形机器人在动态环境交互方面仍受限，乒乓球运动需要亚秒级反应时间，兼具敏捷性和精确性，是极具挑战性的任务

Method: 分层框架：模型规划器负责球轨迹预测和球拍目标规划，强化学习控制器生成协调的全身动作，训练中融入人类运动参考

Result: 在通用人形机器人上验证，最高实现106次连续击球，能与人类对手持续对打，展示亚秒级反应控制能力

Conclusion: 实现了真实世界的人形机器人乒乓球运动，为敏捷交互式人形行为发展迈出重要一步

Abstract: Humanoid robots have recently achieved impressive progress in locomotion and
whole-body control, yet they remain constrained in tasks that demand rapid
interaction with dynamic environments through manipulation. Table tennis
exemplifies such a challenge: with ball speeds exceeding 5 m/s, players must
perceive, predict, and act within sub-second reaction times, requiring both
agility and precision. To address this, we present a hierarchical framework for
humanoid table tennis that integrates a model-based planner for ball trajectory
prediction and racket target planning with a reinforcement learning-based
whole-body controller. The planner determines striking position, velocity and
timing, while the controller generates coordinated arm and leg motions that
mimic human strikes and maintain stability and agility across consecutive
rallies. Moreover, to encourage natural movements, human motion references are
incorporated during training. We validate our system on a general-purpose
humanoid robot, achieving up to 106 consecutive shots with a human opponent and
sustained exchanges against another humanoid. These results demonstrate
real-world humanoid table tennis with sub-second reactive control, marking a
step toward agile and interactive humanoid behaviors.

</details>


### [22] [Prompt-to-Product: Generative Assembly via Bimanual Manipulation](https://arxiv.org/abs/2508.21063)
*Ruixuan Liu,Philip Huang,Ava Pun,Kangle Deng,Shobhit Aggarwal,Kevin Tang,Michelle Liu,Deva Ramanan,Jun-Yan Zhu,Jiaoyang Li,Changliu Liu*

Main category: cs.RO

TL;DR: Prompt-to-Product是一个自动化流水线，能够从自然语言提示生成可物理构建的乐高积木设计，并使用双臂机器人系统自动组装真实产品。


<details>
  <summary>Details</summary>
Motivation: 传统装配产品制造需要大量人工努力和专业知识，包括设计装配和构建产品两个主要阶段。本文旨在降低从创意想法到实际装配产品的门槛和人工成本。

Method: 利用乐高积木作为装配平台，通过自然语言提示生成可物理构建的积木设计，然后使用双臂机器人系统自动构建真实的装配产品。

Result: 通过全面的用户研究表明，Prompt-to-Product显著降低了从想象创意创建装配产品的门槛，并减少了人工努力。

Conclusion: 该系统成功实现了从用户想象到现实世界的自动化装配产品创建，为创意实现提供了高效的技术解决方案。

Abstract: Creating assembly products demands significant manual effort and expert
knowledge in 1) designing the assembly and 2) constructing the product. This
paper introduces Prompt-to-Product, an automated pipeline that generates
real-world assembly products from natural language prompts. Specifically, we
leverage LEGO bricks as the assembly platform and automate the process of
creating brick assembly structures. Given the user design requirements,
Prompt-to-Product generates physically buildable brick designs, and then
leverages a bimanual robotic system to construct the real assembly products,
bringing user imaginations into the real world. We conduct a comprehensive user
study, and the results demonstrate that Prompt-to-Product significantly lowers
the barrier and reduces manual effort in creating assembly products from
imaginative ideas.

</details>


### [23] [Learning on the Fly: Rapid Policy Adaptation via Differentiable Simulation](https://arxiv.org/abs/2508.21065)
*Jiahe Pan,Jiaxu Xing,Rudolf Reiter,Yifan Zhai,Elie Aljalbout,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 提出了一种在线自适应学习框架，通过可微分仿真统一残差动力学学习和实时策略适应，能够在5秒内快速适应未见干扰，显著提升四旋翼无人机在真实环境中的控制性能。


<details>
  <summary>Details</summary>
Motivation: 解决仿真到现实（sim-to-real）迁移中的策略性能退化问题，现有方法如域随机化和Real2Sim2Real要么难以处理分布外条件，要么需要昂贵的离线重新训练。

Method: 结合残差动力学学习和实时策略适应的在线自适应学习框架，使用可微分仿真实现梯度反向传播，持续用真实世界数据精炼动力学模型以捕捉未建模效应和干扰。

Result: 在仿真和真实世界的四旋翼敏捷控制中验证，相比L1-MPC减少81%的悬停误差，相比DATT减少55%的误差，且在无显式状态估计的视觉控制中表现出鲁棒性。

Conclusion: 该框架能够快速适应真实世界中的未建模动力学和环境干扰，在5秒内完成策略更新，为机器人控制提供了一种高效的在线自适应解决方案。

Abstract: Learning control policies in simulation enables rapid, safe, and
cost-effective development of advanced robotic capabilities. However,
transferring these policies to the real world remains difficult due to the
sim-to-real gap, where unmodeled dynamics and environmental disturbances can
degrade policy performance. Existing approaches, such as domain randomization
and Real2Sim2Real pipelines, can improve policy robustness, but either struggle
under out-of-distribution conditions or require costly offline retraining. In
this work, we approach these problems from a different perspective. Instead of
relying on diverse training conditions before deployment, we focus on rapidly
adapting the learned policy in the real world in an online fashion. To achieve
this, we propose a novel online adaptive learning framework that unifies
residual dynamics learning with real-time policy adaptation inside a
differentiable simulation. Starting from a simple dynamics model, our framework
refines the model continuously with real-world data to capture unmodeled
effects and disturbances such as payload changes and wind. The refined dynamics
model is embedded in a differentiable simulation framework, enabling gradient
backpropagation through the dynamics and thus rapid, sample-efficient policy
updates beyond the reach of classical RL methods like PPO. All components of
our system are designed for rapid adaptation, enabling the policy to adjust to
unseen disturbances within 5 seconds of training. We validate the approach on
agile quadrotor control under various disturbances in both simulation and the
real world. Our framework reduces hovering error by up to 81% compared to
L1-MPC and 55% compared to DATT, while also demonstrating robustness in
vision-based control without explicit state estimation.

</details>
